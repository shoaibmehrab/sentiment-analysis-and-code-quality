id,original_comment,processed_comment,source,sentiment_VADER,sentiment_textblob,sentiment_pattern,sentiment_bert,sentiment_spacy,max_voted_sentiment
2005264175,"> @scottjlee Premerge is failing

Looks like databuild is failing, just kicked off retry",failing like failing retry,issue,negative,neutral,neutral,neutral,neutral,neutral
2005160380,"discussed with team - we'll timebox all of these py39 failures and revisit wednesday. if it is a similar cause let's just fix it for all of them; else if it's more involved maybe we defer failing tests due to python39 to later.

cc @jjyao @can-anyscale ",team revisit similar cause let fix else involved maybe defer failing due python later,issue,negative,negative,neutral,neutral,negative,negative
2005123063,@fishbone no thing for the new issues created by automation yeah; this one is created by human so let's close it,fishbone thing new yeah one human let close,issue,negative,positive,neutral,neutral,positive,positive
2005120400,@can-anyscale do we need to do anything if the flakey test was fixed?,need anything test fixed,issue,negative,positive,neutral,neutral,positive,positive
2005052984,@gramhagen let me know if you would be able to review this one; I am also happy to break out some of the changes if you think that not all of them are necessary!,let know would able review one also happy break think necessary,issue,positive,positive,positive,positive,positive,positive
2005009407,"Docs Pages from PR:
* [Loading Data](https://anyscale-ray--44093.com.readthedocs.build/en/44093/data/loading-data.html)
* [Inspecting Data](https://anyscale-ray--44093.com.readthedocs.build/en/44093/data/inspecting-data.html)
* [Transforming Data](https://anyscale-ray--44093.com.readthedocs.build/en/44093/data/transforming-data.html)
* [Iterating Over Data](https://anyscale-ray--44093.com.readthedocs.build/en/44093/data/iterating-over-data.html)
* [Saving Data](https://anyscale-ray--44093.com.readthedocs.build/en/44093/data/saving-data.html)",loading data data transforming data data saving data,issue,negative,neutral,neutral,neutral,neutral,neutral
2005001814,"> Can you link the GH issue when you get a chance @jobh ?

@anyscalesam I think this is the one: https://github.com/ray-project/ray/issues/44083
",link issue get chance think one,issue,negative,neutral,neutral,neutral,neutral,neutral
2004980998,"> I see a possibly related failure, using `get_if_exists` in a tight loop on a single thread.
> 
> It looks like it may by a race condition where the returned actor is not pinned properly because it's already marked for deletion.
> 
> Should I create a separate report for this, or do you think it's the same underlying issue?
> 
> ```
>   Traceback (most recent call last):
>   File ""/home/jobh/src/simula/transact/Reference_route_service/test_ray_lock.py"", line 20, in <module>
>     test_lock()
>   File ""/home/jobh/src/simula/transact/Reference_route_service/test_ray_lock.py"", line 15, in test_lock
>     ray.get(actor.release.remote())
>   File ""/home/jobh/mambaforge/envs/transact/lib/python3.10/site-packages/ray/_private/auto_init_hook.py"", line 22, in auto_init_wrapper
>     return fn(*args, **kwargs)
>   File ""/home/jobh/mambaforge/envs/transact/lib/python3.10/site-packages/ray/_private/client_mode_hook.py"", line 103, in wrapper
>     return func(*args, **kwargs)
>   File ""/home/jobh/mambaforge/envs/transact/lib/python3.10/site-packages/ray/_private/worker.py"", line 2626, in get
>     raise value
> ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
>         class_name: LockActor
>         actor_id: f41165a4b8f1cca43c60623001000000
>         pid: 1728191
>         name: test
>         namespace: locks
>         ip: 172.16.0.146
> The actor is dead because all references to the actor were removed.
> ```
> 
> Reproducer. It may not work for you, the problem goes away if I remove the unused `import numpy` statement so it's probably very timing dependent:
> 
> ```
> import asyncio
> 
> import numpy as np
> import ray
> 
> 
> @ray.remote(namespace=""locks"", num_cpus=0)
> class LockActor:
>     def release(self):
>         ...
> 
> 
> def test_lock():
>     actor = LockActor.options(name=""test"", get_if_exists=True).remote()
>     ray.get(actor.release.remote())
> 
> 
> ray.init()
> for i in range(100):
>     test_lock()
> ```

Can you link the GH issue when you get a chance @jobh ?",see possibly related failure tight loop single thread like may race condition returned actor pinned properly already marked deletion create separate report think underlying issue recent call last file line module file line file line return file line wrapper return file line get raise value actor unexpectedly finishing task name test actor dead actor removed reproducer may work problem go away remove unused import statement probably timing dependent import import import ray class release self actor test range link issue get chance,issue,negative,negative,neutral,neutral,negative,negative
2004963625,"> (Let's set a max width of the whole container so that it doesn't look odd on really big screen sizes)

@simran-2797 Any update here? I'm working on this currently.

Since the content of the tab widget looks like it is changing, would you also be able to provide the text that you'd like displayed for each tab?",let set width whole container look odd really big screen size update working currently since content tab like would also able provide text like displayed tab,issue,negative,positive,positive,positive,positive,positive
2004934068,Mark it as P2 since it's a Ray client test.,mark since ray client test,issue,negative,neutral,neutral,neutral,neutral,neutral
2004902765,"I think the reason is that for gpu tasks, we by default don't reuse worker processes, so the gap is waiting for a new worker process to be started. You can reenable worker process reusing by setting `max_calls=0` in the `ray.remote()` decorator. See https://docs.ray.io/en/releases-2.9.3/ray-core/scheduling/accelerators.html#workers-not-releasing-gpu-resources",think reason default reuse worker gap waiting new worker process worker process setting decorator see,issue,negative,positive,positive,positive,positive,positive
2004856441,"Assigning P0 assumption basic Jax + Ray doesn't work. If it turns out not to be the case, we can downgrade.",assumption basic ray work turn case downgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
2004821231,"yes, it's a critical stability fix",yes critical stability fix,issue,negative,neutral,neutral,neutral,neutral,neutral
2004800342,"> Is this in scope for Data GA? If so we should pick (even at the cost of delaying the release)

Yes, let's cherry pick the PR.",scope data ga pick even cost delaying release yes let cherry pick,issue,negative,neutral,neutral,neutral,neutral,neutral
2004791338,@c21 @raulchen Is this in scope for Data GA? If so we should pick (even at the cost of delaying the release),scope data ga pick even cost delaying release,issue,negative,neutral,neutral,neutral,neutral,neutral
2004783886,"> @stephanie-wang we are pretty late into the release cycle. How critical is it tp idc this one into 2.10?

We need it for Ray Data recovery. If we don't cherry-pick, I think we need to do a minor release at the very least. @c21 @raulchen can weigh in too.",pretty late release cycle critical one need ray data recovery think need minor release least weigh,issue,negative,negative,neutral,neutral,negative,negative
2004707539,"Looks like there is a race condition when accessing actor metadata in GCS. Full stack trace:
```
2024-03-18 10:43:06,446	WARNING worker.py:1422 -- SIGTERM handler is not set because current thread is not the main thread.
2024-03-18 10:43:08,121	INFO worker.py:1743 -- Started a local Ray instance. View the dashboard at 127.0.0.1:8265
[2024-03-18 10:43:08,122 I 74426 315464] logging.cc:230: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1
2024-03-18 10:43:08,509	INFO function_manager.py:663 -- qr: class_name = None
2024-03-18 10:43:08,509	INFO function_manager.py:664 -- qr: type(class_name) = <class 'NoneType'>
Exception in thread Thread-130:
Traceback (most recent call last):
  File ""/Users/ruiqiao/anaconda3/envs/env-ray/lib/python3.9/threading.py"", line 980, in _bootstrap_inner
2024-03-18 10:43:08,509	INFO function_manager.py:663 -- qr: class_name = bar
2024-03-18 10:43:08,509	INFO function_manager.py:663 -- qr: class_name = bar
2024-03-18 10:43:08,509	INFO function_manager.py:664 -- qr: type(class_name) = <class 'str'>
2024-03-18 10:43:08,510	INFO function_manager.py:663 -- qr: class_name = None
2024-03-18 10:43:08,510	INFO function_manager.py:664 -- qr: type(class_name) = <class 'NoneType'>
Exception in thread Thread-347:
Traceback (most recent call last):
  File ""/Users/ruiqiao/repos/ray/python/ray/actor.py"", line 926, in _remote
2024-03-18 10:43:08,509	INFO function_manager.py:664 -- qr: type(class_name) = <class 'str'>
(raylet) [2024-03-18 10:43:08,442 I 74444 316646] logging.cc:230: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1
    self.run()
  File ""/Users/ruiqiao/anaconda3/envs/env-ray/lib/python3.9/threading.py"", line 917, in run
    return ray.get_actor(name, namespace=namespace)
  File ""/Users/ruiqiao/repos/ray/python/ray/_private/auto_init_hook.py"", line 21, in auto_init_wrapper
    self._target(*self._args, **self._kwargs)
  File ""/Users/ruiqiao/repos/ray/python/threads.py"", line 13, in foo
    return fn(*args, **kwargs)
  File ""/Users/ruiqiao/repos/ray/python/ray/_private/client_mode_hook.py"", line 103, in wrapper
    bar.options(name=""bar"", namespace=""bar_name"", get_if_exists=True, lifetime=""detached"").remote()
  File ""/Users/ruiqiao/repos/ray/python/ray/actor.py"", line 830, in remote
    return actor_cls._remote(args=args, kwargs=kwargs, **updated_options)
  File ""/Users/ruiqiao/repos/ray/python/ray/_private/auto_init_hook.py"", line 21, in auto_init_wrapper
    return func(*args, **kwargs)
  File ""/Users/ruiqiao/repos/ray/python/ray/_private/worker.py"", line 2927, in get_actor
    return fn(*args, **kwargs)
  File ""/Users/ruiqiao/repos/ray/python/ray/util/tracing/tracing_helper.py"", line 388, in _invocation_actor_class_remote_span
    return worker.core_worker.get_named_actor_handle(name, namespace or """")
  File ""python/ray/_raylet.pyx"", line 4370, in ray._raylet.CoreWorker.get_named_actor_handle
    return method(self, args, kwargs, *_args, **_kwargs)
  File ""/Users/ruiqiao/repos/ray/python/ray/actor.py"", line 926, in _remote
  File ""python/ray/_raylet.pyx"", line 574, in ray._raylet.check_status
    return ray.get_actor(name, namespace=namespace)
  File ""/Users/ruiqiao/repos/ray/python/ray/_private/auto_init_hook.py"", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File ""/Users/ruiqiao/repos/ray/python/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
  File ""/Users/ruiqiao/repos/ray/python/ray/_private/worker.py"", line 2927, in get_actor
ValueError: Failed to look up actor with name 'bar'. This could because 1. You are trying to look up a named actor you didn't create. 2. The named actor died. 3. You did not use a namespace matching the namespace of the actor.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/ruiqiao/anaconda3/envs/env-ray/lib/python3.9/threading.py"", line 980, in _bootstrap_inner
    return worker.core_worker.get_named_actor_handle(name, namespace or """")
  File ""python/ray/_raylet.pyx"", line 4372, in ray._raylet.CoreWorker.get_named_actor_handle
    self.run()
  File ""/Users/ruiqiao/anaconda3/envs/env-ray/lib/python3.9/threading.py"", line 917, in run
  File ""python/ray/_raylet.pyx"", line 4312, in ray._raylet.CoreWorker.make_actor_handle
    self._target(*self._args, **self._kwargs)
  File ""/Users/ruiqiao/repos/ray/python/threads.py"", line 13, in foo
  File ""/Users/ruiqiao/repos/ray/python/ray/_private/function_manager.py"", line 567, in load_actor_class
    bar.options(name=""bar"", namespace=""bar_name"", get_if_exists=True, lifetime=""detached"").remote()
  File ""/Users/ruiqiao/repos/ray/python/ray/actor.py"", line 830, in remote
    return actor_cls._remote(args=args, kwargs=kwargs, **updated_options)
  File ""/Users/ruiqiao/repos/ray/python/ray/_private/auto_init_hook.py"", line 21, in auto_init_wrapper
    actor_class = self._load_actor_class_from_gcs(
  File ""/Users/ruiqiao/repos/ray/python/ray/_private/function_manager.py"", line 666, in _load_actor_class_from_gcs
    return fn(*args, **kwargs)
  File ""/Users/ruiqiao/repos/ray/python/ray/util/tracing/tracing_helper.py"", line 388, in _invocation_actor_class_remote_span
    class_name = ensure_str(class_name)
  File ""/Users/ruiqiao/repos/ray/python/ray/_private/utils.py"", line 243, in ensure_str
    return method(self, args, kwargs, *_args, **_kwargs)
  File ""/Users/ruiqiao/repos/ray/python/ray/actor.py"", line 932, in _remote
    assert isinstance(s, bytes)
AssertionError
    return self._remote(args, kwargs, **updated_options)
  File ""/Users/ruiqiao/repos/ray/python/ray/_private/auto_init_hook.py"", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File ""/Users/ruiqiao/repos/ray/python/ray/util/tracing/tracing_helper.py"", line 388, in _invocation_actor_class_remote_span
    return method(self, args, kwargs, *_args, **_kwargs)
  File ""/Users/ruiqiao/repos/ray/python/ray/actor.py"", line 997, in _remote
    ray.get_actor(name, namespace=namespace)
  File ""/Users/ruiqiao/repos/ray/python/ray/_private/auto_init_hook.py"", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File ""/Users/ruiqiao/repos/ray/python/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
  File ""/Users/ruiqiao/repos/ray/python/ray/_private/worker.py"", line 2927, in get_actor
    return worker.core_worker.get_named_actor_handle(name, namespace or """")
  File ""python/ray/_raylet.pyx"", line 4372, in ray._raylet.CoreWorker.get_named_actor_handle
  File ""python/ray/_raylet.pyx"", line 4312, in ray._raylet.CoreWorker.make_actor_handle
  File ""/Users/ruiqiao/repos/ray/python/ray/_private/function_manager.py"", line 567, in load_actor_class
    actor_class = self._load_actor_class_from_gcs(
  File ""/Users/ruiqiao/repos/ray/python/ray/_private/function_manager.py"", line 666, in _load_actor_class_from_gcs
    class_name = ensure_str(class_name)
  File ""/Users/ruiqiao/repos/ray/python/ray/_private/utils.py"", line 243, in ensure_str
    assert isinstance(s, bytes)
AssertionError
```",like race condition actor full stack trace warning handler set current thread main thread local ray instance view dashboard set ray log level environment variable none type class exception thread recent call last file line bar bar type class none type class exception thread recent call last file line type class raylet set ray log level environment variable file line run return name file line file line foo return file line wrapper bar detached file line remote return file line return file line return file line return name file line return method self file line file line return name file line return file line wrapper return file line look actor name could trying look actor create actor use matching actor handling exception another exception recent call last file line return name file line file line run file line file line foo file line bar detached file line remote return file line file line return file line file line return method self file line assert return file line return file line return method self file line name file line return file line wrapper return file line return name file line file line file line file line file line assert,issue,negative,positive,neutral,neutral,positive,positive
2004674738,@stephanie-wang we are pretty late into the release cycle. How critical is it tp idc this one into 2.10?,pretty late release cycle critical one,issue,negative,negative,neutral,neutral,negative,negative
2004655134,"> This makes `root_exception.__cause__ == StartTraceback`, which makes it a circular reference.
> ```
> except Exception as start_traceback:
>         raise skip_exceptions(start_traceback)
> ```

Minor correction here, in this case `root_exection.__context__ == StartTraceback`, but similarly this would create the circular reference!

",circular reference except exception raise minor correction case similarly would create circular reference,issue,negative,negative,neutral,neutral,negative,negative
2004612882,@anyscalesam @aslonnie  Kserve is depenedent on Ray[serve] and we need to deploy kserve on s390x. Thats why we need s390x support on ray & its trivial for us . Please guide me regarding the changes which might impact protobuf & grpc.  ,ray serve need deploy thats need support ray trivial u please guide regarding might impact,issue,positive,neutral,neutral,neutral,neutral,neutral
2004568090,"I also face this issue. I am just doing single agent PPO. High severity: this blocks me.

```
config = (
      'PPO',
      .get_default_config()
      .environment(
          cli_args.env,
          env_config=get_env_config_from_cli(cli_args)
      )
      .training(
          grad_clip=1
      )
      .framework('torch')
      .rollouts(
          num_rollout_workers=cli_args.workers,
      )
)

stop = {
      ""training_iteration"": cli_args.stop_iters,
      ""timesteps_total"": cli_args.stop_timesteps,
      ""episode_reward_mean"": cli_args.stop_reward,
}
    
tuner = tune.Tuner(
        'PPO',
        param_space=config.to_dict(),
        run_config=air.RunConfig(
            stop=stop,
            checkpoint_config=train.CheckpointConfig(checkpoint_frequency=4, num_to_keep=2),
            callbacks=[
                WandbLoggerCallback(
                    project=""project""
                )
            ]
        ),
    )
    results = tuner.fit()
```

",also face issue single agent high severity stop tuner project,issue,negative,positive,neutral,neutral,positive,positive
2004537707,@sven1977 confirmed the test works when kicked off manually,confirmed test work manually,issue,negative,positive,positive,positive,positive,positive
2004491863,Closing as unable to repro since no response from @sfriedowitz  - ty @ruisearch42 for looking into it.,unable since response looking,issue,negative,negative,negative,negative,negative,negative
2003746003,"A simpler/more reliable reproducer:
```
import ray


@ray.remote
class Actor:
    def act(self):
        ...


ray.init()
for i in range(100):
    actor = Actor.options(name=""test"", namespace=""test"", get_if_exists=True).remote()
    ray.get(actor.act.remote())
    del actor
```",reliable reproducer import ray class actor act self range actor test test actor,issue,negative,neutral,neutral,neutral,neutral,neutral
2003715479,"CI test **linux://rllib:examples/rock_paper_scissors_multiagent_tf** is flaky. Recent failures: 
	- https://buildkite.com/ray-project/postmerge/builds/3578#018e4bdc-a0d5-4184-a60c-cd7f1d54fea7
	- https://buildkite.com/ray-project/postmerge/builds/3511#018e3df8-dda0-4538-bcc8-3aee60ced92e
	- https://buildkite.com/ray-project/postmerge/builds/3497#018e3a0d-677f-4660-9215-9fb778699caf
	- https://buildkite.com/ray-project/postmerge/builds/3474#018e3743-20ad-4ae8-869d-3c8141af1619
	- https://buildkite.com/ray-project/postmerge/builds/3448#018e344d-a9fa-43b3-b321-e033195212de

DataCaseName-linux://rllib:examples/rock_paper_scissors_multiagent_tf-END
Managed by OSS Test Policy",test flaky recent test policy,issue,negative,neutral,neutral,neutral,neutral,neutral
2003052600,"Running `python setup.py XXX` is deprecated.
* https://packaging.python.org/en/latest/discussions/setup-py-deprecated/#

> Running command `python setup.py egg_info`
```
[2024-03-18T04:29:00Z] #15 74.96 Using pip 24.0 from /opt/miniconda/lib/python3.9/site-packages/pip (python 3.9)
[2024-03-18T04:29:00Z] #15 75.15 Obtaining file:///rayci/python
[2024-03-18T04:29:00Z] #15 75.15   Preparing metadata (setup.py): started
[2024-03-18T04:29:00Z] #15 75.15   Running command python setup.py egg_info
[2024-03-18T04:29:00Z] #15 75.48   /opt/miniconda/lib/python3.9/site-packages/setuptools/__init__.py:80: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.
[2024-03-18T04:29:00Z] #15 75.48   !!
[2024-03-18T04:29:00Z] #15 75.48
[2024-03-18T04:29:00Z] #15 75.48           ********************************************************************************
[2024-03-18T04:29:00Z] #15 75.48           Requirements should be satisfied by a PEP 517 installer.
[2024-03-18T04:29:00Z] #15 75.48           If you are using pip, you can try `pip install --use-pep517`.
[2024-03-18T04:29:00Z] #15 75.48           ********************************************************************************
[2024-03-18T04:29:00Z] #15 75.48
[2024-03-18T04:29:00Z] #15 75.48   !!
[2024-03-18T04:29:00Z] #15 75.48     dist.fetch_build_eggs(dist.setup_requires)
```",running python running command python pip python file running command python satisfied pep installer pip try pip install,issue,negative,positive,positive,positive,positive,positive
2002785449,Please follow https://github.com/ray-project/ray/issues/42343 first for the support of ml/rllib for python 3.11. Note that python 3.11 is already supported for ray-core. We'll work on supoprting python 3.12 after that.,please follow first support python note python already work python,issue,positive,positive,positive,positive,positive,positive
2002691126,"+1, I wish to move to ray rllib for its efficiency in RL training. But my project needs Python>=3.12. ",wish move ray efficiency training project need python,issue,positive,neutral,neutral,neutral,neutral,neutral
2002682620,"Although `(func pid=20855)` in the driver log disrespects the logging configurations, the log file located in `/tmp/ray/session_latest/logs` adheres to the configurations. You can stream the log files to your logging infrastructure.

<img width=""491"" alt=""Screen Shot 2024-03-17 at 5 22 51 PM"" src=""https://github.com/ray-project/ray/assets/20109646/c1f0b718-7e26-40cb-ad7f-df61b67ad838"">
",although driver log logging log file stream log logging infrastructure screen shot,issue,negative,neutral,neutral,neutral,neutral,neutral
2002669169,"Actually, is the intent that each reader process has its own channel? In other words, each channel is private to a single reader?",actually intent reader process channel channel private single reader,issue,negative,negative,neutral,neutral,negative,negative
2002664001,This is a duplicate of #37803. I have already opened #44073 to update the links.,duplicate already update link,issue,negative,neutral,neutral,neutral,neutral,neutral
2002663544,"It's not immediately clear to me why a semaphore is the right synchronization primitive to use for the mutable object manager. For example, consider this code:
```
sem_wait(&channel.reader_semaphore);

int64_t version_read = 0;
RAY_LOG(DEBUG) << ""ReadAcquire "" << object_id
               << "" version: "" << channel.next_version_to_read;
RAY_RETURN_NOT_OK(channel.mutable_object->header->ReadAcquire(
   channel.next_version_to_read, &version_read));
RAY_CHECK(version_read > 0);
channel.next_version_to_read = version_read;
channel.read_acquired = true;
```

If the reader semaphore is initialized with a value greater than 1, multiple readers in parallel will be able to advance beyond `sem_wait()` in `ReadAcquire()`. As a result, multiple readers will race to set `channel->next_version_to_read` and `channel->read_acquire`. Additionally, any readers in `ReadRelease()` will also race to set these two variables.

I am making the assumption that multiple readers will each call `ReadAcquire()` and `ReadRelease()`. Is my assumption incorrect or am I otherwise overlooking something?",immediately clear semaphore right synchronization primitive use mutable object manager example consider code version true reader semaphore value greater multiple parallel able advance beyond result multiple race set additionally also race set two making assumption multiple call assumption incorrect otherwise something,issue,positive,positive,positive,positive,positive,positive
2002663339,I am currently going through observability issues and have opened a PR at https://github.com/ray-project/ray/pull/44073 to fix this issue.,currently going observability fix issue,issue,negative,neutral,neutral,neutral,neutral,neutral
2002632126,"No, that build should work well, but the first time you run it you will have to allow firewall exceptions via a giu pop-up box. Did you see that?",build work well first time run allow via box see,issue,negative,positive,positive,positive,positive,positive
2002621454,"Hey folks, I was able to debug the issue. I was not setting up the `nsight` logging arguments in the correct place to be included in runtime argument. A good way to check is to search the ray logs and see if `nsys` if successfully registered for runtime argument. Thanks @jjyao @cadedaniel  for your help and looking at the PR implementation #19631 really helped me.",hey able issue setting logging correct place included argument good way check search ray see successfully registered argument thanks help looking implementation really,issue,positive,positive,positive,positive,positive,positive
2002531272,"Apologies if it wasn't clear that was understood. The problem description and (1) were more intended to note that the printed message, given the use of a 'Warning' descriptor, felt rather disingenuous. Pragmatically, if a message is listed as a Warning, then it would be commonly expected by users to be filterable...?

Regardless, would there be any issue with the proposal/suggestion described in (2) exposing the warning threshold declaration (python/ray/_private/ray_constants.py)? That only some of the constants have been exposed as environmental variables, suggested that there might be non-obvious issue(s) with doing so...

changing line 144: 
`FUNCTION_SIZE_WARN_THRESHOLD = 10**7`
to: 
`FUNCTION_SIZE_WARN_THRESHOLD = env_integer(""FUNCTION_SIZE_WARN_THRESHOLD "", (10**7)) `",clear understood problem description intended note printed message given use felt rather disingenuous pragmatically message listed warning would commonly filterable regardless would issue warning threshold declaration exposed environmental might issue line,issue,negative,negative,neutral,neutral,negative,negative
2002473621,"Currently, users can propagate logging configuration to all worker processes within a job via `worker_process_setup_hook`. See [this doc](https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html) for more details. This issue is also duplicate with #39712 #23385.
 
```python
# driver.py
def logging_setup_func():
    logger = logging.getLogger(""ray"")
    logger.setLevel(logging.DEBUG)
    warnings.simplefilter(""always"")

ray.init(runtime_env={""worker_process_setup_hook"": logging_setup_func})

logging_setup_func()
```",currently propagate logging configuration worker within job via see doc issue also duplicate python logger ray always,issue,negative,neutral,neutral,neutral,neutral,neutral
2002438679,"I still see this typo every day :/

On Sun, 17 Mar 2024 at 09:05, stale[bot] ***@***.***> wrote:

> This pull request has been automatically marked as stale because it has
> not had recent activity. It will be closed in 14 days if no further
> activity occurs. Thank you for your contributions.
>
>    - If you'd like to keep this open, just leave any comment, and the
>    stale label will be removed.
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/ray-project/ray/pull/42963#issuecomment-2002353609>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AF7PX3EI7FAYLC2KPS5IRNLYYVFERAVCNFSM6AAAAABCYMEZWCVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMBSGM2TGNRQHE>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>
",still see typo every day sun mar stale bot wrote pull request automatically marked stale recent activity closed day activity thank like keep open leave comment stale label removed reply directly view id,issue,positive,negative,negative,negative,negative,negative
2002371873,"This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 14 days if no further activity occurs. Thank you for your contributions.
- If you'd like to keep this open, just leave any comment, and the stale label will be removed.
",pull request automatically marked stale recent activity closed day activity thank like keep open leave comment stale label removed,issue,positive,negative,negative,negative,negative,negative
2002368297,This test still failed on releases/2.10.0 branch even after the fix was cherry picked in @sven1977 ,test still branch even fix cherry picked,issue,negative,neutral,neutral,neutral,neutral,neutral
2002351300,"The warning message is printed by Ray Core (C++), so the logging level in Python can't hide the warning message.",warning message printed ray core logging level python ca hide warning message,issue,negative,neutral,neutral,neutral,neutral,neutral
2002344184,"* [test.py](https://gist.github.com/kevin85421/34baa982c5ce5647ff36be367cc61b28): I've modified the script you provided slightly.

```sh
# Ray 2.9.0
serve run test:model_server
curl -X GET 127.0.0.1:9000/health
```

Both ""I'm healthy!"" and ""Healthy"" are shown in the STDOUT. See the following screenshot for more details.

<img width=""1439"" alt=""Screen Shot 2024-03-17 at 12 22 54 AM"" src=""https://github.com/ray-project/ray/assets/20109646/a3a96d70-de13-437c-93fe-0dd9f3b3bf37"">

",script provided slightly sh ray serve run test curl get healthy healthy shown see following screen shot,issue,positive,positive,positive,positive,positive,positive
2002336955,Test has been failing for far too long. Jailing.,test failing far long,issue,negative,positive,neutral,neutral,positive,positive
2002021468,@fishbone @rkooo567 plz take a look this issue.,fishbone take look issue,issue,negative,neutral,neutral,neutral,neutral,neutral
2001944557,Blamed commit: 82e6c40cbd236ef17ac16fd1131e71b3ed81187a found by bisect job https://buildkite.com/ray-project/release-tests-bisect/builds/947,blamed commit found bisect job,issue,negative,neutral,neutral,neutral,neutral,neutral
2001944393,"It may not be a bug, but I need your help to fix it. Thank you so much
",may bug need help fix thank much,issue,positive,positive,positive,positive,positive,positive
2000576025,@zhe-thoughts just removing a spammy warning message. No logic changes.,removing warning message logic,issue,negative,neutral,neutral,neutral,neutral,neutral
2000460818,@khluu the linkcheck is failing due to rllib issue that was fixed in master,failing due issue fixed master,issue,negative,negative,neutral,neutral,negative,negative
2000145101,agree yes let monitor in postmerge for now,agree yes let monitor,issue,positive,neutral,neutral,neutral,neutral,neutral
2000012068,"Merged in https://github.com/ray-project/ray/pull/44049 so CI would pass, codeowners please ignore",would pas please ignore,issue,negative,neutral,neutral,neutral,neutral,neutral
1999976696,The `random_agent` one had already been removed in an earlier PR merged today. Could you pull again?,one already removed today could pull,issue,negative,neutral,neutral,neutral,neutral,neutral
1999957275,"I think there's another in `rllib-dev` that needs to get fixed (from [failed build](https://buildkite.com/ray-project/premerge/builds/22063#018e42aa-df5f-46bd-91c3-361cd141e4a6/183-4494)):

```
[2024-03-15T15:17:58Z] (rllib/rllib-concepts: line   13) broken    https://github.com/ray-project/ray/blob/master/rllib/algorithms/pg/pg_tf_policy.py - 404 Client Error: Not Found for url: https://github.com/ray-project/ray/blob/master/rllib/algorithms/pg/pg_tf_policy.py
--
  | [2024-03-15T15:17:58Z] ( rllib/rllib-dev: line   71) broken    https://github.com/ray-project/ray/tree/master/rllib/algorithms/random_agent/random_agent.py - 404 Client Error: Not Found for url: https://github.com/ray-project/ray/tree/master/rllib/algorithms/random_agent/random_agent.py
```",think another need get fixed build line broken client error found line broken client error found,issue,negative,negative,negative,negative,negative,negative
1999948287,The `random_agent` link had already been removed in a PR we merged earlier today.,link already removed today,issue,negative,neutral,neutral,neutral,neutral,neutral
1999703510,"I debugged this issue by ssh'ing into the worker nodes, and learning that the default volume size in the cluster launcher is too low for some common docker container images (like nvcr.io/nvidia/pytorch:24.02-py3, with a few additional dependencies).

Similarly, the default timeout might be low, so worker nodes might be initialized and then stuck on an endless loop of being killed after they hit the timeout limit while also running out of memory due to default volume size limits.",issue worker learning default volume size cluster launcher low common docker container like additional similarly default might low worker might stuck endless loop hit limit also running memory due default volume size,issue,negative,negative,neutral,neutral,negative,negative
1999481673,Re-opening issue as test is still failing. Latest run: https://buildkite.com/ray-project/release/builds/11364#018e40a9-0d08-4a0d-a61f-727e7786ca6a,issue test still failing latest run,issue,negative,positive,positive,positive,positive,positive
1999285771,"Hi @liuxsh9 , you are welcome to add support for NPU accelerator in Ray Train: ) If you already have the designs, feel free to post a PR and we can discuss the implementation details. ",hi welcome add support accelerator ray train already feel free post discus implementation,issue,positive,positive,positive,positive,positive,positive
1999240810,"Do we need some special treatments for BaseException? Consider this code:

```
@ray.remote
def exits():
    print(""some work..."")
    import sys
    sys.exit()


ray.get(exits.remote())
```

where `sys.exit()` raises a `SystemExit`. If we just forward it, does it make the driver code to exit? We may need a policy on what to do in raised worker BaseException.",need special consider code print work import forward make driver code exit may need policy raised worker,issue,negative,positive,positive,positive,positive,positive
1999166735,"I print out a hex representation of both floats. For `0.4f`, I see `0x1.99999ap-2`. For `usage_threshold`, I see `0x1.99999cp-2`. Thus, the two values are different.",print hex representation see see thus two different,issue,negative,neutral,neutral,neutral,neutral,neutral
1999138951,"I'm seeing the following issue when running `ci/lint/format.sh`:
```
$ ci/lint/format.sh
From github.com:ray-project/ray
 * branch                  master     -> FETCH_HEAD
Traceback (most recent call last):
  File ""/home/ubuntu/anaconda3/lib/python3.11/site-packages/flake8/checker.py"", line 478, in run_ast_checks
    ast = self.processor.build_ast()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ubuntu/anaconda3/lib/python3.11/site-packages/flake8/processor.py"", line 225, in build_ast
    return ast.parse("""".join(self.lines))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ubuntu/anaconda3/lib/python3.11/ast.py"", line 50, in parse
    return compile(source, filename, mode, flags,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""<unknown>"", line 7
    from cpython.exc cimport PyErr_CheckSignals
                     ^^^^^^^
SyntaxError: invalid syntax

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/ubuntu/anaconda3/bin/flake8"", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File ""/home/ubuntu/anaconda3/lib/python3.11/site-packages/flake8/main/cli.py"", line 22, in main
    app.run(argv)
  File ""/home/ubuntu/anaconda3/lib/python3.11/site-packages/flake8/main/application.py"", line 363, in run
    self._run(argv)
  File ""/home/ubuntu/anaconda3/lib/python3.11/site-packages/flake8/main/application.py"", line 351, in _run
    self.run_checks()
  File ""/home/ubuntu/anaconda3/lib/python3.11/site-packages/flake8/main/application.py"", line 264, in run_checks
    self.file_checker_manager.run()
  File ""/home/ubuntu/anaconda3/lib/python3.11/site-packages/flake8/checker.py"", line 323, in run
    self.run_serial()
  File ""/home/ubuntu/anaconda3/lib/python3.11/site-packages/flake8/checker.py"", line 307, in run_serial
    checker.run_checks()
  File ""/home/ubuntu/anaconda3/lib/python3.11/site-packages/flake8/checker.py"", line 589, in run_checks
    self.run_ast_checks()
  File ""/home/ubuntu/anaconda3/lib/python3.11/site-packages/flake8/checker.py"", line 480, in run_ast_checks
    row, column = self._extract_syntax_information(e)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ubuntu/anaconda3/lib/python3.11/site-packages/flake8/checker.py"", line 465, in _extract_syntax_information
    lines = physical_line.rstrip(""\n"").split(""\n"")
            ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'int' object has no attribute 'rstrip'
```

This error seems to appear for *any* change to _raylet.pyx, including just adding an empty line. I am still trying to figure out the cause of this.",seeing following issue running branch master recent call last file line ast file line return file line parse return compile source mode file unknown line invalid syntax handling exception another exception recent call last file line module main file line main file line run file line file line file line run file line file line file line row column file line object attribute error appear change empty line still trying figure cause,issue,negative,positive,neutral,neutral,positive,positive
1999116786,Always nice to delete code! ,always nice delete code,issue,negative,positive,positive,positive,positive,positive
1999031812,"It's great to see that Ray Train has successfully extended its support beyond GPUs to other acceleration hardware. Interestingly, we are also preparing to contribute support for Huawei NPU. The adaptation process for torch_npu is very similar to torch_hpu. 
However, we are concerned that following the HPU approach to add NPU support might make the code more complex and introduce scattered device type and library checks. 
Therefore, in terms of design, we would like to separate the hardware-specific components, similar to the abstraction of `accelerators`, to make it easier for Ray Train (even RLlib) to support third-party devices. 
We have already made some designs that ensure it won't affect the usage of GPUs and HPUs, and we provide references for integrating third-party devices with APIs like `prepare_model` and `prepare_data_loader`. We would love to hear the community's opinions on this matter.
Should we initiate a PR to showcase the implementation (including some code migration for HPU), or would it be better to initiate an issue first to discuss this matter? @kira-lin @woshiyyya @matthewdeng ",great see ray train successfully extended support beyond acceleration hardware interestingly also contribute support adaptation process similar however concerned following approach add support might make code complex introduce scattered device type library therefore design would like separate similar abstraction make easier ray train even support already made ensure wo affect usage provide like would love hear community matter initiate showcase implementation code migration would better initiate issue first discus matter,issue,positive,positive,positive,positive,positive,positive
1998955951,"Btw, it'd be nicer to think about how to safely enable thread-safety for APIs. right now, my impression is it is kind of happened to work (and prone to be broken). ",think safely enable right impression kind work prone broken,issue,positive,positive,positive,positive,positive,positive
1998820579,"I see. So it's because:


1. This makes `StartTraceback.__cause__ == root_exception`
```
    except Exception as root_exception:
            raise StartTraceback from root_exception
```

2. `skip_exceptions(start_traceback)` returns the original `root_exception`.


3. This makes `root_exception.__cause__ == StartTraceback`, which makes it a circular reference.

```
except Exception as start_traceback:
        raise skip_exceptions(start_traceback)
```",see except exception raise original circular reference except exception raise,issue,negative,positive,positive,positive,positive,positive
1998682354,@jobh Thanks for reporting. Can you open a separate issue and link this one? We can combine them when investigations provide more evidence they are the same.,thanks open separate issue link one combine provide evidence,issue,negative,positive,neutral,neutral,positive,positive
1998671836,"I think this is related. I also faced this issue when search_space contains a nested dict.

https://github.com/microsoft/FLAML/issues/1244",think related also faced issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1998625533,@simonguo-cohere I think you found the cause of your issue. Could you reply what your fix is. This might help the other user as well.,think found cause issue could reply fix might help user well,issue,positive,neutral,neutral,neutral,neutral,neutral
1998517120,This test is now considered as flaky because it has been failing on postmerge for too long. Flaky tests do not run on premerge.,test considered flaky failing long flaky run,issue,negative,negative,neutral,neutral,negative,negative
1998514396,"The test is marked `skip_flaky_core_test_premerge`. After unskipping it, the failure did not reproduce when run many times on HEAD, even when I made the `time.sleep(20)` to be sure the item was (potentially) wrongly collected. The test was added in #30415 with a fix for issue #30341. It was flaky on windows, but I can't get it to fail. Maybe something in the timing of the call to `reference_counter_->AddNestedObjectIds` has changed for the better and the objects are not being collected.",test marked failure reproduce run many time head even made sure item potentially wrongly collected test added fix issue flaky ca get fail maybe something timing call better collected,issue,negative,positive,neutral,neutral,positive,positive
1998502251,p2 as it doesn't block weekly or release for ray210; but @mattip please continue prioritizing,block weekly release ray please continue,issue,negative,neutral,neutral,neutral,neutral,neutral
1998490537,@rkooo567 @jjyao Could you confirm this is OK to cherry pick?,could confirm cherry pick,issue,negative,neutral,neutral,neutral,neutral,neutral
1998467098,"Test failure:

```

024-03-14T20:41:21Z] collecting ... collected 9 items / 3 errors / 6 selected
--
  | [2024-03-14T20:41:21Z]
  | [2024-03-14T20:41:21Z] ==================================== ERRORS ====================================
  | [2024-03-14T20:41:21Z] ________________________ ERROR collecting test session _________________________
  | [2024-03-14T20:41:21Z] python/ray/autoscaler/launch_and_verify_cluster.py:29: in <module>
  | [2024-03-14T20:41:21Z]     from ray.autoscaler._private.constants import RAY
  | [2024-03-14T20:41:21Z] E   ImportError: cannot import name 'RAY' from 'ray.autoscaler._private.constants' (/rayci/python/ray/autoscaler/_private/constants.py)
  | [2024-03-14T20:41:21Z] ________________________ ERROR collecting test session _________________________
  | [2024-03-14T20:41:21Z] python/ray/autoscaler/launch_and_verify_cluster.py:29: in <module>
  | [2024-03-14T20:41:21Z]     from ray.autoscaler._private.constants import RAY
  | [2024-03-14T20:41:21Z] E   ImportError: cannot import name 'RAY' from 'ray.autoscaler._private.constants' (/rayci/python/ray/autoscaler/_private/constants.py)


```",test failure collected selected error test session module import ray import name error test session module import ray import name,issue,negative,negative,negative,negative,negative,negative
1998458487,"> @edoakes can I have your opinion whether this is 1) severe enough as a release blocker; 2) low risk enough at this stage of the release?

1. Yeah this can completely break user workflows if they rely on importing things from the directory where they `ray start`. It's not the ""golden workflow"" but I would be _very_ surprised if there aren't quite a few people doing it.
2. To me it seems low risk but to be honest I don't fully understand the motivation behind the breaking change (it is a core PR). We should get @rynewang / @rkooo567 to chime in.",opinion whether severe enough release blocker low risk enough stage release yeah completely break user rely directory ray start golden would quite people low risk honest fully understand motivation behind breaking change core get chime,issue,negative,positive,neutral,neutral,positive,positive
1998323660,Marking this as ready for review - @simran-2797 can you take a look now that the community examples are in?,marking ready review take look community,issue,negative,positive,positive,positive,positive,positive
1998307931,@mjd3 would love to learn more about your use case if you'd be game to share; Slack me on Ray Slack @anyscalesam  if you have a chance!,would love learn use case game share slack ray slack chance,issue,positive,positive,neutral,neutral,positive,positive
1998276914,@edoakes can I have your opinion whether this is 1) severe enough as a release blocker; 2) low risk enough at this stage of the release?,opinion whether severe enough release blocker low risk enough stage release,issue,negative,neutral,neutral,neutral,neutral,neutral
1998092010,"Rebased, there are now two community examples visible in the example gallery.

If we want to have some badge or icon next to community examples in the individual library example pages then I'm open to that too. I'd also be happy to add that in a subsequent PR if it makes review simpler. @angelinalg thoughts?",two community visible example gallery want badge icon next community individual library example open also happy add subsequent review simpler,issue,positive,positive,positive,positive,positive,positive
1998065928,"> Question for @zcin and @edoakes : why wasn't this caught earlier in tests?

Compact scheduling is best effort, so even when our calculations were incorrect because of this bug, it goes unnoticed in end-to-end tests because after best effort on Serve's side, we hand it off to Core for final scheduling decisions. This could have been caught by flushing out some unit tests more, which has room for improvement because of the rush for branch cut.",question caught compact best effort even incorrect bug go unnoticed best effort serve side hand core final could caught flushing unit room improvement rush branch cut,issue,positive,positive,positive,positive,positive,positive
1998056935,"Thanks. However, the original model for llama2 7b is only 13.5G, does it only has the model weights?

How can I only store the weights in the checkpoint? I assume that I need to implement a customized callback by passing the parameter weights_only=True?",thanks however original model llama model store assume need implement passing parameter,issue,positive,positive,positive,positive,positive,positive
1997965933,"@liqiangsz have you considered following this approach?

```
IMPORTANT: DreamerV3 out-of-the-box only supports image observation spaces of shape 64x64x3 as well as any vector observations (1D float32 Box spaces). Should you require a special world model encoder- and decoder for other observation spaces (e.g. a text embedding or images of other dimensions), you will have to subclass [DreamerV3's catalog class](https://github.com/ray-project/ray/blob/master/rllib/algorithms/dreamerv3/dreamerv3_catalog.py) and then configure this new catalog via your DreamerV3Config object as follows:
```

https://github.com/ray-project/ray/blob/master/rllib/algorithms/dreamerv3/dreamerv3_catalog.py

",considered following approach important image observation shape well vector float box require special world model observation text subclass class configure new via object,issue,positive,positive,positive,positive,positive,positive
1997930507,"> We're working on a similar system to fine-tune some small models for users. Our current solution:
> 
> 1. Run a dispatcher deployment which accepts requests to begin training.
> 2. Dispatcher verifies the request and then calls a new actor with the required training code.
> 3. This Actor's task is called async
> 4. We return the TaskID and make calls to ray_state.get_task for scheduling status.
> 
> In actuallity, the Task is tied to an S3 object associted with the fine-tuning run which we poll for persistent status.
> 
> Roughly:
> 
> ```
> @ray.remote(
>     num_gpus=1.0,
>     num_cpus=4.0
> )
> class Trainer:
>     def train(self):
>         *long-running task here*
>         *save to s3 etc.*
> 
> @serve.deployment()
> @serve.ingress(app)
> class Dispatcher:
>     @app.post(""/train"")
>     async def call_trainer(self, http_request: Request):
>         body = await http_request.json()
> 
>         trainer = Trainer.remote()
>         ref = trainer.train.remote()
> 
>         return str(ref.task_id())
> ```
> 
> The major issue here is the persistence is reliant on something like S3, and importantly, Serve will downscale Dispatcher replicas with active training runs (as they have already returned), resulting in Ray ending the Train call before completion. So the Dispatcher cannot be autoscaled safely.
> 
> Both suggestions above work, but I prefer the second option as se have a deployment graph for pre-processing and option 2 seems easier to work around. It's also more similar to the workflow of Ray core.

How do you handle multiple training requests?",working similar system small current solution run dispatcher deployment begin training dispatcher request new actor training code actor task return make status task tied object run poll persistent status roughly class trainer train self task save class dispatcher self request body await trainer ref return major issue persistence reliant something like importantly serve dispatcher active training already returned resulting ray ending train call completion dispatcher safely work prefer second option se deployment graph option easier work around also similar ray core handle multiple training,issue,positive,positive,neutral,neutral,positive,positive
1997910403,Looks like there are two Train community examples that have been merged. I'll rebase this PR and modify them to use the new machinery - stand by!,like two train community rebase modify use new machinery stand,issue,negative,positive,positive,positive,positive,positive
1997892769,"Nice! In the `All Examples` drop down, should we say `Maintained by the Ray Team` @woshiyyya @matthewdeng @simran-2797? Also, please note the capitalization of `Team` to be consistent with the rest of the docs.",nice drop say ray team also please note capitalization team consistent rest,issue,negative,positive,positive,positive,positive,positive
1997867235,Love the purging and maintenance! Thank you!,love purging maintenance thank,issue,positive,positive,positive,positive,positive,positive
1997734041,"@stephanie-wang @jjyao I'd like to help with this issue. 

Looking at the code I wonder if the fix would be located around these places: 

* `resource_spec.resolve` (./python/ray/_private/resource_spec.py)
* `accelerator.get_current_node_accelerator_type` (./python/ray/_private/accelerators/accelerator.py)
	* implementations for nvidia (./python/ray/_private/accelerators/nvidia_gpu.py)
	* intel (./python/ray/_private/accelerators/intel_gpu.py)
	* and so on
* Particularly, `get_current_node_accelerator_type` in most implementations return None if there are issues. (ex: `return None  # pynvml init failed`)

Would the fix require changes in all the these provider specific implementations to return error vs no-accelerator detected responses ? or would the fix be located in other modules ?",like help issue looking code wonder fix would around particularly return none ex return none would fix require provider specific return error would fix,issue,negative,positive,neutral,neutral,positive,positive
1997733532,If someone stumbles upon this discussion: Consider checking out the flyte project which has a lot of features around caching. It is also able to integrate with ray.,someone upon discussion consider project lot around also able integrate ray,issue,negative,positive,positive,positive,positive,positive
1997639238,"@edoakes if you have sometime today, can you help taking another look again so we can get this merged and cherry picked🙏",sometime today help taking another look get cherry picked,issue,negative,neutral,neutral,neutral,neutral,neutral
1997461800,"[debug_state_gcs.txt](https://github.com/ray-project/ray/files/14602807/debug_state_gcs.txt)

hi, @jjyao follow the file to validate.",hi follow file validate,issue,negative,neutral,neutral,neutral,neutral,neutral
1996686446,"> upgrading the bazel rules would be pretty complicated given how ray uses bazel today, it will likely be intertwined with protobuf and grpc rules upgrade, and even if we managed to upgrade that, it will not be trivial to support a new architecture.

I will be working on supporting ray on s390x architecture.",would pretty complicated given ray today likely upgrade even upgrade trivial support new architecture working supporting ray architecture,issue,positive,positive,neutral,neutral,positive,positive
1996521489,@anyscalesam I am already on slack. Lets connect today . My slack id :- Mudassar Rana,already slack connect today slack id rana,issue,negative,neutral,neutral,neutral,neutral,neutral
1996394829,"@edoakes @jjyao Impact of this bug: the resource dict and placement group bundles in `on_deployment_deployed` are already flattened, so passing them into `from_ray_resource_dict` actually *drops* all custom resources (anything except cpu, gpu, memory). So anywhere that accesses `DeploymentSchedulingInfo.required_resources` (currently only `DeploymentScheduler._get_available_resources_per_node`) is only getting the cpu/gpu/memory resources required to schedule a replica, but not the custom resources.",impact bug resource placement group already passing actually custom anything except memory anywhere currently getting schedule replica custom,issue,negative,neutral,neutral,neutral,neutral,neutral
1996330506,"@anyscalesam Would it make sense to add a brief note to the Ray documentation about this? It's not just VS Code, PyCharm has a similar functionality (see https://github.com/ray-project/ray/issues/14005#issuecomment-802842126 ), but it's not apparent at all right now from the documentation that it's possible to debug Ray through any kind of IDE. ",would make sense add brief note ray documentation code similar functionality see apparent right documentation possible ray kind ide,issue,positive,positive,positive,positive,positive,positive
1996229374,"> Caught up more on the issue ##43964, approving (infinite backoff is a release blocker)

Yes without this there is an integer overflow error that causes Serve to pile up requests. Rare and under only some special circumstances, but very bad failure mode.",caught issue infinite release blocker yes without integer overflow error serve pile rare special bad failure mode,issue,negative,negative,neutral,neutral,negative,negative
1996163884,"@jjyao I made this PR to address [this comment](https://github.com/ray-project/ray/pull/43930#issuecomment-1995720594). I'm unsure how to test it though. [Serve already validates that there's enough resources](https://github.com/ray-project/ray/blob/0086eb5616f63901c5bfd5f1e57aa4b53589fb3d/python/ray/serve/_private/config.py#L629), and #43930 runs the placement group's validation logic before at definition time. When else might the placement group creation raise an exception?

Or is this intended to guard against unexpected exceptions that may be raised in the future?",made address comment unsure test though serve already enough placement group validation logic definition time else might placement group creation raise exception intended guard unexpected may raised future,issue,negative,positive,neutral,neutral,positive,positive
1996041304,Closing since tests are passing now; let's continue to work on the clean up separately,since passing let continue work clean separately,issue,negative,positive,positive,positive,positive,positive
1996013612,"let's pick then, CC: @zhe-thoughts ci/cd only change to make the system more stable",let pick change make system stable,issue,negative,neutral,neutral,neutral,neutral,neutral
1995969824,"**Mar 13 updates**

Done:
- ray py3.8 is now completed deprecated in master branch (after ray 2.10 branch cut)
- dependencies are successfully resolved to minimize the number of test failures we need to fix forward. Ray seems OK itself, but we need to update our tests

In progress:
- non-test breaking initial dependency upgrade (https://github.com/ray-project/ray/pull/42104, ETA: merge tomorrow or early next week)
- test-breaking dependency (tensorflow and friends) upgrade + ray.ml.311 image built (https://github.com/ray-project/ray/pull/43402, ETA: merge next week)

Remaining work:
- March 20-31: fixing forward test failures for python 3.11
- April 1st: ray 2.11 release",mar done ray master branch ray branch cut successfully resolved minimize number test need fix forward ray need update progress breaking initial dependency upgrade eta merge tomorrow early next week dependency upgrade image built eta merge next week work march fixing forward test python st ray release,issue,positive,positive,positive,positive,positive,positive
1995925831,we can pick this in never the less. this fixes some other flaky issues.,pick never le flaky,issue,negative,neutral,neutral,neutral,neutral,neutral
1995925214,"hmm.. if they are not related to the google sdk or apt things, it seems unlikely the cause.",related apt unlikely cause,issue,negative,positive,neutral,neutral,positive,positive
1995726134,"I think it's because it has both model weights and optimizer states.

In total it would be 7B (model weights) * 2 (bf16)  + 7B * 2 (optimizer states) * 4 (fp32)  = 70G",think model total would model,issue,negative,neutral,neutral,neutral,neutral,neutral
1995720594,"> @jjyao Is it possible for the placement group bundles to be valid format-wise, but ray.util.placement_group still fails to create the placement group (e.g. not enough resources or custom resources don't exist)?

Yea, it's possible so ideally we should also handle exceptions inside `deployment_scheduler` as well but that can be a separate PR.
",possible placement group valid still create placement group enough custom exist yea possible ideally also handle inside well separate,issue,positive,positive,positive,positive,positive,positive
1995710440,"Perf looks good. Those with > 10% regressions are noise.

Let's update this PR after all cherry picks are done.",good noise let update cherry done,issue,negative,positive,positive,positive,positive,positive
1995577420,"removing release-blocker per review with @sven1977 cc @zhe-thoughts.

Sven, should this be a weekly release blocker?",removing per review weekly release blocker,issue,negative,neutral,neutral,neutral,neutral,neutral
1995551497,@modassarrana89 could we connect over Slack more about your use case? Ask for invite here: https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform,could connect slack use case ask invite,issue,negative,neutral,neutral,neutral,neutral,neutral
1995519527,"some doc tests are failing; otherwise ci change looks great, thanks for doing this matt",doc failing otherwise change great thanks,issue,positive,positive,positive,positive,positive,positive
1995462941,"Was there traction on incorporating the MultiAgentWrappers into the rllib main?
Searching through the code base it doesn't seem like it was ever implemented?",traction main searching code base seem like ever,issue,negative,negative,negative,negative,negative,negative
1995435460,"upgrading the bazel rules would be pretty complicated given how ray uses bazel today, it will likely be intertwined with protobuf and grpc rules upgrade, and even if we managed to upgrade that, it will not be trivial to support a new architecture.",would pretty complicated given ray today likely upgrade even upgrade trivial support new architecture,issue,positive,negative,neutral,neutral,negative,negative
1995212894,@mjd3 would it make sense to add documentation here? https://docs.ray.io/en/latest/cluster/vms/references/ray-cluster-configuration.html#cluster-configuration-provider-type,would make sense add documentation,issue,negative,neutral,neutral,neutral,neutral,neutral
1995210566,"Hi @gramhagen, would you mind reviewing this PR if you have time?",hi would mind time,issue,negative,neutral,neutral,neutral,neutral,neutral
1995143455,@ericl @architkulkarni @hongchaodeng let me know if there's anything else I need to do for this PR; I don't believe I can assign anyone to review it.,let know anything else need believe assign anyone review,issue,negative,neutral,neutral,neutral,neutral,neutral
1995005787,"@jjyao can you please review and merge @mattip PR; you are listed as reviewer.
Matti as discused please cut a separate ticket with labels core, windows, p1, enhancement for solving for the holsitic case of all special characters in http urls as discussed

here is allof them https://chat.openai.com/share/4fd49922-f27a-4169-af91-2a7345971609",please review merge listed reviewer matti please cut separate ticket core enhancement case special,issue,positive,positive,positive,positive,positive,positive
1994940283,"<img width=""1632"" alt=""image"" src=""https://github.com/ray-project/ray/assets/116198444/5ff7612f-641b-4a8d-b7b5-daa3adc0cffb"">
looks passing... closing for now. @can-anyscale are you running windows test gh issues where they open/close so we can eventually gather metadata on flakiness and prioritize test rewrite for the ones that keep opening/closing?",image passing running test eventually gather flakiness test rewrite keep,issue,negative,neutral,neutral,neutral,neutral,neutral
1994939668,"@jjyao Is it possible for the placement group bundles to be valid format-wise, but `ray.util.placement_group` still fails to create the placement group (e.g. not enough resources or custom resources don't exist)?",possible placement group valid still create placement group enough custom exist,issue,negative,neutral,neutral,neutral,neutral,neutral
1994935507,ah yes it's probably because gene's PR changed the name :) ,ah yes probably gene name,issue,negative,neutral,neutral,neutral,neutral,neutral
1994930438,ah yes probably my bad I think this test has been deleted/renamed; let's confirmed but otherwise don't worry about the automation,ah yes probably bad think test let confirmed otherwise worry,issue,negative,negative,negative,negative,negative,negative
1994927045,@erlan-11 can you respond to @mattip's clarifying question above? Are you on py39? Matti can you try to repro on py39 while we wait on erlan's response?,respond question matti try wait response,issue,negative,neutral,neutral,neutral,neutral,neutral
1994922711,"> To unblock the release, perhaps it's enough to just delete some old security groups manually, and then we can take our time with this PR afterwards. Thoughts?

Ah, just realized you already did this!",unblock release perhaps enough delete old security manually take time afterwards ah already,issue,negative,positive,neutral,neutral,positive,positive
1994912212,"Thanks @Fokko , great comments! Glad this PR has some traction - I'm going to look into your review soon!",thanks great glad traction going look review soon,issue,positive,positive,positive,positive,positive,positive
1994890584,"@jjyao If those are just accidentally left out I think you also need to change the following:
- serve_long_poll_host_transmission_counter 
- serve_num_http_requests
- serve_num_grpc_requests
- serve_num_http_error_requests
- serve_num_grpc_error_requests
- serve_num_deployment_http_error_requests
- serve_num_deployment_grpc_error_requests
- serve_num_router_requests

Not sure if I left out any, but those are the ones I found missing in the PR",accidentally left think also need change following sure left found missing,issue,negative,positive,neutral,neutral,positive,positive
1994752007,"The test passed because I deleted some unused security groups.
To completely solve this issue, we need to merge PR https://github.com/ray-project/ray/pull/43949 which changes the test to delete security groups after each run.",test unused security completely solve issue need merge test delete security run,issue,positive,positive,neutral,neutral,positive,positive
1994679872,"In fixing this, let's please also add some log statements when the backoff index gets high to aid with debugging.",fixing let please also add log index high aid,issue,negative,positive,positive,positive,positive,positive
1994664355,"Ok, seems to be a wrong machine config on our end. 4 GPUs where 8 needed. Will fix asap ...",wrong machine end fix,issue,negative,negative,negative,negative,negative,negative
1994650997,"I see a possibly related failure, using `get_if_exists` in a tight loop on a single thread.

It looks like it may by a race condition where the returned actor is not pinned properly because it's already marked for deletion.

Should I create a separate report for this, or do you think it's the same underlying issue?

```
  Traceback (most recent call last):
  File ""/home/jobh/src/simula/transact/Reference_route_service/test_ray_lock.py"", line 20, in <module>
    test_lock()
  File ""/home/jobh/src/simula/transact/Reference_route_service/test_ray_lock.py"", line 15, in test_lock
    ray.get(actor.release.remote())
  File ""/home/jobh/mambaforge/envs/transact/lib/python3.10/site-packages/ray/_private/auto_init_hook.py"", line 22, in auto_init_wrapper
    return fn(*args, **kwargs)
  File ""/home/jobh/mambaforge/envs/transact/lib/python3.10/site-packages/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
  File ""/home/jobh/mambaforge/envs/transact/lib/python3.10/site-packages/ray/_private/worker.py"", line 2626, in get
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
        class_name: LockActor
        actor_id: f41165a4b8f1cca43c60623001000000
        pid: 1728191
        name: test
        namespace: locks
        ip: 172.16.0.146
The actor is dead because all references to the actor were removed.
```

Reproducer. It may not work for you, the problem goes away if I remove the unused `import numpy` statement so it's probably very timing dependent:
```
import asyncio

import numpy as np
import ray


@ray.remote(namespace=""locks"", num_cpus=0)
class LockActor:
    def release(self):
        ...


def test_lock():
    actor = LockActor.options(name=""test"", get_if_exists=True).remote()
    ray.get(actor.release.remote())


ray.init()
for i in range(100):
    test_lock()
```",see possibly related failure tight loop single thread like may race condition returned actor pinned properly already marked deletion create separate report think underlying issue recent call last file line module file line file line return file line wrapper return file line get raise value actor unexpectedly finishing task name test actor dead actor removed reproducer may work problem go away remove unused import statement probably timing dependent import import import ray class release self actor test range,issue,negative,negative,neutral,neutral,negative,negative
1994548019,I downloaded the installer for `3.10.11` from [here](https://www.python.org/downloads/windows/). If there is another version that you know has no issues I can try that,installer another version know try,issue,negative,neutral,neutral,neutral,neutral,neutral
1994364230,"@woshiyyya sure. I am using the llama 2 7b, the size of the original model is about 7gb, however, after fine tuning it with 12 GPU, each shard in the checking point has about 6.3GB, and the total is 76GB. see the output below

```
(base) sh-4.2$ ls -lh ./TorchTrainer_df11b_00000_0_2024-03-07_16-51-36/checkpoint_000000/checkpoint.ckpt/checkpoint
total 76G
-rw-r--r-- 1 root root 6.3G Mar  7 17:31 bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
-rw-r--r-- 1 root root 6.3G Mar  7 17:22 bf16_zero_pp_rank_10_mp_rank_00_optim_states.pt
-rw-r--r-- 1 root root 6.3G Mar  7 17:25 bf16_zero_pp_rank_11_mp_rank_00_optim_states.pt
-rw-r--r-- 1 root root 6.3G Mar  7 17:36 bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
-rw-r--r-- 1 root root 6.3G Mar  7 17:36 bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
-rw-r--r-- 1 root root 6.3G Mar  7 17:36 bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
-rw-r--r-- 1 root root 6.3G Mar  7 17:26 bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
-rw-r--r-- 1 root root 6.3G Mar  7 17:27 bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
-rw-r--r-- 1 root root 6.3G Mar  7 17:26 bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt
-rw-r--r-- 1 root root 6.3G Mar  7 17:25 bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt
-rw-r--r-- 1 root root 6.3G Mar  7 17:25 bf16_zero_pp_rank_8_mp_rank_00_optim_states.pt
-rw-r--r-- 1 root root 6.3G Mar  7 17:24 bf16_zero_pp_rank_9_mp_rank_00_optim_states.pt
-rw-r--r-- 1 root root 173K Mar  7 17:25 zero_pp_rank_0_mp_rank_00_model_states.pt
-rw-r--r-- 1 root root 173K Mar  7 17:19 zero_pp_rank_10_mp_rank_00_model_states.pt
-rw-r--r-- 1 root root 173K Mar  7 17:19 zero_pp_rank_11_mp_rank_00_model_states.pt
-rw-r--r-- 1 root root 173K Mar  7 17:31 zero_pp_rank_1_mp_rank_00_model_states.pt
-rw-r--r-- 1 root root 173K Mar  7 17:25 zero_pp_rank_2_mp_rank_00_model_states.pt
-rw-r--r-- 1 root root 173K Mar  7 17:25 zero_pp_rank_3_mp_rank_00_model_states.pt
-rw-r--r-- 1 root root 173K Mar  7 17:19 zero_pp_rank_4_mp_rank_00_model_states.pt
-rw-r--r-- 1 root root 173K Mar  7 17:19 zero_pp_rank_5_mp_rank_00_model_states.pt
-rw-r--r-- 1 root root 173K Mar  7 17:19 zero_pp_rank_6_mp_rank_00_model_states.pt
-rw-r--r-- 1 root root 173K Mar  7 17:19 zero_pp_rank_7_mp_rank_00_model_states.pt
-rw-r--r-- 1 root root 173K Mar  7 17:19 zero_pp_rank_8_mp_rank_00_model_states.pt
-rw-r--r-- 1 root root 173K Mar  7 17:19 zero_pp_rank_9_mp_rank_00_model_states.pt
```",sure llama size original model however fine tuning shard point total see output base total root root mar root root mar root root mar root root mar root root mar root root mar root root mar root root mar root root mar root root mar root root mar root root mar root root mar root root mar root root mar root root mar root root mar root root mar root root mar root root mar root root mar root root mar root root mar root root mar,issue,positive,positive,neutral,neutral,positive,positive
1994256868,"CI test **windows://python/ray/tests:test_task_events** is consistently_failing. Recent failures: 
	- https://buildkite.com/ray-project/postmerge/builds/3474#018e3743-2163-468a-9111-357b5472b735
	- https://buildkite.com/ray-project/postmerge/builds/3472#018e365f-7bd9-4582-95b7-bf6e12124a0f
	- https://buildkite.com/ray-project/postmerge/builds/3439#018e332f-9f64-43f3-bc86-9dcb1dd97d2d
	- https://buildkite.com/ray-project/postmerge/builds/3438#018e321c-f9c9-4ebf-b507-2e2f325c009e

DataCaseName-windows://python/ray/tests:test_task_events-END
Managed by OSS Test Policy",test recent test policy,issue,negative,neutral,neutral,neutral,neutral,neutral
1993740055,Any update on this PR? It would be a nice feature to have.,update would nice feature,issue,negative,positive,positive,positive,positive,positive
1993618982,"For anyone who might end up here from google: It is actually possible to debug Ray actors from VS Code! (Albeit not through Ray Debug.) Following the [debugpy documentation](https://github.com/microsoft/debugpy?tab=readme-ov-file), put the following into your code, somewhere  early on in the ray actor you want to debug (e.g., inside the constructor):
```
import debugpy
debugpy.listen((""localhost"", 5678))
```
Then, create a ""Python: Remote Attach"" debug configuration in VS Code and point it to port 5678. If you have multiple copies of that actor, figure out some sort of local rank and add that to the port number, or similar.",anyone might end actually possible ray code albeit ray following documentation put following code somewhere early ray actor want inside constructor import create python remote attach configuration code point port multiple actor figure sort local rank add port number similar,issue,negative,negative,neutral,neutral,negative,negative
1993284258,"@stephanie-wang kindly check the changes,
and let me know if any more changes needed",kindly check let know,issue,negative,positive,positive,positive,positive,positive
1992880474,@bveeramani  This test starts failing again on release branch: https://buildkite.com/ray-project/release/builds/10874#018e222b-411d-4478-b4e6-be92f30bb07b,test failing release branch,issue,negative,neutral,neutral,neutral,neutral,neutral
1992861102,Start failing as we now run with python 3.9,start failing run python,issue,negative,neutral,neutral,neutral,neutral,neutral
1992831690,"Hi there @jtm101-code. not sure if you're still having this problem as years have passed -- but I will share in case this helps anyone else. You need to: 
1. make sure you install ray[default] on whatever instance/environment is running the ray up command. installing without the [default] declaration is a minimal installation that does not include everything needed for the cluster launcher.
2. in your `setup_commands`, you will also need to pip install ray[default]. If you include anything for that parameter, my understanding is that you are overriding the default installation. 

One additional recommendation is to not leave this dependency unpinned or implicit on the setup_commands as it (at least as of version 2.9.3) will install an unstable nightly release of ray. The documentation indicates otherwise but I have observed this in the logs. ",hi sure still problem share case anyone else need make sure install ray default whatever running ray command without default declaration minimal installation include everything cluster launcher also need pip install ray default include anything parameter understanding default installation one additional recommendation leave dependency unpinned implicit least version install unstable nightly release ray documentation otherwise,issue,negative,positive,positive,positive,positive,positive
1992728554,"The code is initial prototype, will create a new PR.",code initial prototype create new,issue,negative,positive,neutral,neutral,positive,positive
1992649846,"As I explained above, this script is showing a different problem.

```python
@ray.remote
def f_wrapper(large_shared_object_ref): # when corrent == false, Large input is not GCed until ray stop.
    return f.remote(large_shared_object_ref)
```

Here, `f_wrapper` actually gets the value of `large_shared_object_ref`, not the ref. When `f.remote(...)` is called, it copies the value and implicitly calls `ray.put()` to create a new object. This input is not GCed because the ref returned by f.remote() is still in scope, and the ref was created by `ray.put`. If you want to prevent this from happening, you can mark the nested `f` as non-retryable by doing:
```python
f.options(max_retries=0).remote(large_shared_object_ref)`
```

Or you can `ray.wait` until the task has finished, then call the internal API `free`.

But ideally, you should instead use the modified script that I posted above, which avoids the unnecessary copy.",script showing different problem python false large input ray stop return actually value ref value implicitly create new object input ref returned still scope ref want prevent happening mark python task finished call internal free ideally instead use script posted unnecessary copy,issue,positive,positive,neutral,neutral,positive,positive
1992624051,"On Ray Train side, there's [a PR](https://github.com/ray-project/ray/pull/43343/files) that is pending to be merged. 

One question is: can we add a `contributor: community` tag in that PR before merging @peytondmurray 's PR?",ray train side pending one question add contributor community tag,issue,negative,neutral,neutral,neutral,neutral,neutral
1992542295,"HI, here are the codes:
```import time

import numpy as np
import ray


@ray.remote
def f(large_shared_object_ref): # Large input can be GCed in all cases.
    time.sleep(10)
    return np.random.rand(25000)  # 200 KB


@ray.remote
def f_wrapper(large_shared_object_ref): # when corrent == false, Large input is not GCed until ray stop.
    return f.remote(large_shared_object_ref)


def runner(is_correct: bool):
    ray.init(include_dashboard=True)
    max_num_pending_tasks = 2

    # This large shared object takes up 400 MB.
    large_shared_object = np.random.rand(50000000)
    large_shared_object_ref = ray.put(large_shared_object)

    pending_task_list, all_refs, outputs = [], [], []

    for _ in range(50):
        if len(pending_task_list) >= max_num_pending_tasks:
            ready_refs, pending_task_list = ray.wait(pending_task_list, num_returns=1)
            outputs += ray.get(ready_refs)

        if is_correct:
            # In the correct case, we simply call the unnested remote function.
            new_task = f.remote(large_shared_object_ref)
            # We're holding onto a full list of these promises.
            all_refs.append(new_task)
            pending_task_list.append(new_task)
        else:
            # In the incorrect case, we call the wrapper remote function.
            new_task = f_wrapper.remote(large_shared_object_ref)
            # Since we now have nested remote calls, we unwrap the outer promise with `ray.get()`.
            new_task_inner_promise = ray.get(new_task)
            all_refs.append(new_task_inner_promise)
            pending_task_list.append(new_task_inner_promise)

    print(outputs)
    ray.shutdown()


if __name__ == ""__main__"":
    # runner(is_correct=True)
    runner(is_correct=False)
```",hi import time import import ray large input return false large input ray stop return runner bool large object range correct case simply call remote function holding onto full list else incorrect case call wrapper remote function since remote unwrap outer promise print runner runner,issue,negative,positive,neutral,neutral,positive,positive
1992522525,"Reassigning to core label and removing data, as this looks to be a Ray Core related issue. Please feel free to reassign if that's not the case",core label removing data ray core related issue please feel free reassign case,issue,positive,positive,positive,positive,positive,positive
1992513689,"> But in this issue https://github.com/ray-project/ray/issues/37912, when correct = True, the object can correctly get evicted without free.

In the original script that you posted, the large object would not get evicted until the end of the script, once all dependent tasks have finished and their refs have gone out of scope.

> Also based on my test, the outer object (by ray.get) is correctly evict but input for the father object is not evict as the lineage ref = 1 after father task finished. Unless we call relase submitted object(not the results, it;s the args.)

Sorry, I don't follow this. Can you post a minimal script and leave comments in the code to explain which object is not getting cleaned up as expected? The original script in the PR description has the issue that I described above.",issue correct true object correctly get without free original script posted large object would get end script dependent finished gone scope also based test outer object correctly evict input father object evict lineage ref father task finished unless call object sorry follow post minimal script leave code explain object getting original script description issue,issue,negative,positive,positive,positive,positive,positive
1992502386,Umm. I think @angelinalg or @woshiyyya  might have a community example ready to add here? ,think might community example ready add,issue,negative,positive,positive,positive,positive,positive
1992483532,@jjyao have https://github.com/anyscale/runtime/issues/472 to discuss next sprint planning on Mon.,discus next sprint mon,issue,negative,neutral,neutral,neutral,neutral,neutral
1992464660,"Hi @robinsonmhj , can you show us more details? Like the list of checkpoints, which files you were looking at, and how did you determine the size.",hi show u like list looking determine size,issue,negative,neutral,neutral,neutral,neutral,neutral
1992437274,"```



10806:C 05 Mar 2024 15:22:05.412 # WARNING: Changing databases number from 16 to 1 since we are in cluster mode
  | 2024-03-05 07:34:07 PDT | 10806:M 05 Mar 2024 15:22:05.413 # WARNING: The TCP backlog setting of 511 cannot be enforced because kern.ipc.somaxconn is set to the lower value of 128.
  | 2024-03-05 07:34:07 PDT | 10806:M 05 Mar 2024 15:22:05.487 # Warning: Could not create server TCP listening socket ::*:49160: bind: Address already in use
  | 2024-03-05 07:34:07 PDT | 10806:M 05 Mar 2024 15:22:05.487 # Failed listening on port 49160 (tcp), aborting.
 
....


Waiting for redis to be up Error 61 connecting to localhost:49160. Connection refused.
  | 2024-03-05 07:34:07 PDT | Waiting for redis to be up Error 61 connecting to localhost:49160. Connection refused.
  | 2024-03-05 07:34:07 PDT | Waiting for redis to be up Error 61 connecting to localhost:49160. Connection refused.
  | 2024-03-05 07:34:07 PDT | Waiting for redis to be up Error 61 connecting to localhost:49160. Connection refused.
  | 2024-03-05 07:34:07 PDT | Waiting for redis to be up Error 61 connecting to localhost:49160. Connection refused.
  | 2024-03-05 07:34:07 PDT | Waiting for redis to be up Error 61 connecting to localhost:49160. Connection refused.
  | 2024-03-05 07:34:07 PDT | Waiting for redis to be up Error 61 connecting to localhost:49160. Connection refused.
  | 2024-03-05 07:34:07 PDT | Waiting for redis to be up Error 61 connecting to localhost:49160. Connection refused.
  | 2024-03-05 07:34:07 PDT | Waiting for redis to be up Error 61 connecting to localhost:49160. Connection refused.
  | 2024-03-05 07:34:07 PDT | Waiting for redis to be up Error 61 connecting to localhost:49160. Connection refused.
  | 2024-03-05 07:34:07 PDT | Waiting for redis to be up Error 61 connecting to localhost:49160. Connection refused.
 ```

Seems due to environment not being cleaned up.",mar warning number since cluster mode mar warning backlog setting enforced set lower value mar warning could create server listening socket bind address already use mar listening port waiting error connection waiting error connection waiting error connection waiting error connection waiting error connection waiting error connection waiting error connection waiting error connection waiting error connection waiting error connection waiting error connection due environment,issue,negative,negative,negative,negative,negative,negative
1992311800,"I'm going to close this, I assume it was an env issue for me somehow, but I moved to building in a docker container and it's successful. Thanks",going close assume issue somehow building docker container successful thanks,issue,positive,positive,positive,positive,positive,positive
1991928842,"that's true yeah, bug yes let's pick both changes together",true yeah bug yes let pick together,issue,positive,positive,positive,positive,positive,positive
1991916255,hmm.. this was failing because the new anyscale layer does not have the google source list file anymore. so the release branch might not be affected.,failing new layer source list file release branch might affected,issue,negative,positive,positive,positive,positive,positive
1991842998,@edoakes this is ready for another round of review. Changed to just log a warning and added a test for it ,ready another round review log warning added test,issue,negative,neutral,neutral,neutral,neutral,neutral
1991627271,"> I'm not sure I understand the exact need for this additional option to the API. My main argument against this would be: When stuff gets stored to a checkpoint, it should be stored in a device-independent fashion. So the issue at hand here is NOT the loading from the checkpoint, but the saving to the checkpoint beforehand, which - I'm guessing - probably happened in torch.cuda tensors, NOT in numpy format.
> 
> Can we rather take the opposite approach to keep the mental model of what a checkpoint should be clean? Always save weights (and other tensor/matrix states) as numpy arrays, never as torch or tf tensors. When loading from a checkpoint, the sequence should be something like: 0) RLModule.from_checkpoint(dir=...)
> 
> 1. load numpy arrays from `dir` file (pickle?)
> 2. pack these numpy arrays into a state dict, mapping
> 3. Call RLModule.set_state(state_dict=...) -> This automatically converts the numpy contents of `state_dict` into torch tensors (with `self.device` as the device of the already existing RLModule object), then performs a `torch.nn.Module.load_state_dict()` operation using these (cuda?) tensors.

I agree with your argument that we should ensure that checkpointing is device-independent. This should be the cleanest way of doing this. We should investigate, where exactly this device-dependent checkpointing takes place and fix the problem there. 

I am, however, not so sure, if 3. describes how the workflow runs right now. Here is what makes me wonder: If the `torch.nn.Module.load_state_dict()` would use the `self.device` then it should not matter, if the state contains `numpy.array`s or `torch.tensor` (with their own device attribute) as it uses always the module's device. But no matter what - checkpointing always `numpy.array` will avoid this error anyways.",sure understand exact need additional option main argument would stuff fashion issue hand loading saving beforehand guessing probably format rather take opposite approach keep mental model clean always save never torch loading sequence something like load file pickle pack state call automatically content torch device already object operation agree argument ensure way investigate exactly place fix problem however sure right wonder would use matter state device attribute always module device matter always avoid error anyways,issue,positive,positive,positive,positive,positive,positive
1991599947,"I'm not sure I understand the exact need for this additional option to the API. My main argument against this would be: When stuff gets stored to a checkpoint, it should be stored in a device-independent fashion. So the issue at hand here is NOT the loading from the checkpoint, but the saving to the checkpoint beforehand, which - I'm guessing - probably happened in torch.cuda tensors, NOT in numpy format.

Can we rather take the opposite approach to keep the mental model of what a checkpoint should be clean? Always save weights (and other tensor/matrix states) as numpy arrays, never as torch or tf tensors. When loading from a checkpoint, the sequence should be something like:
0) RLModule.from_checkpoint(dir=...)
1) load numpy arrays from `dir` file (pickle?)
2) pack these numpy arrays into a state dict, mapping 
3) Call RLModule.set_state(state_dict=...) -> This automatically converts the numpy contents of `state_dict` into torch tensors (with `self.device` as the device of the already existing RLModule object), then performs a `torch.nn.Module.load_state_dict()` operation using these (cuda?) tensors.",sure understand exact need additional option main argument would stuff fashion issue hand loading saving beforehand guessing probably format rather take opposite approach keep mental model clean always save never torch loading sequence something like load file pickle pack state call automatically content torch device already object operation,issue,positive,positive,positive,positive,positive,positive
1991406640,"Hi @jjyao, can this PR be merged? Happy to give a hand to fix broken tests if it can help.",hi happy give hand fix broken help,issue,positive,positive,positive,positive,positive,positive
1991224608,"> @simonsays1980 Does this error get logged happen very consistently throughout the run? I'm thinking this might be a race condition of wandb deleting some file, while that file is trying to be synced to cloud storage.
> 
> Does the run eventually succeed? The syncing should get retried throughout the run.

Hi @justinvyu thanks for taking a look. It does get logged consistently during the run. I later had to stop because of  many `_QueueActor`s and `WandBActor`s started and stopped again - it looked like a mess. Regrettably I have no worker logs stored. I think it should be reproducable given the cluster YAML and the run script. The runs did not succeed because of these stopping and restarting actions. These took away the memory (156GB) and left no space for the runs - I got a OOM. 

I ran another experiment then with double resources (24 -> 48 CPUs, 156GB -> 312GB, 2 T4 NVIDIA) and turned off `WandB`. This run did not succeed either with GPUs after ~100 iterations not anymore detected (see #43866 ) ",error get logged happen consistently throughout run thinking might race condition file file trying cloud storage run eventually succeed get throughout run hi thanks taking look get logged consistently run later stop many stopped like mess regrettably worker think given cluster run script succeed stopping took away memory left space got ran another experiment double turned run succeed either see,issue,negative,positive,positive,positive,positive,positive
1991185929,"> @simonsays1980 are you able to reproduce it with latest Ray version?

Hi @jjyao, thanks for coming back to this. I have to retry this - its a year ago. I ping you as soon as I have results.",able reproduce latest ray version hi thanks coming back retry year ago ping soon,issue,negative,positive,positive,positive,positive,positive
1991048093,"@jjyao  I collected logs for 2.10.0 and updated regression result in the PR description
Logs for 2.10.0 is based from run on commit `d8b3d6732cf90fbaa4b30fcce2806ac3021b9f1f`",collected regression result description based run commit,issue,negative,neutral,neutral,neutral,neutral,neutral
1990569692,"Yea, we should catch the error. Ideally we should validate pg in the @serve.deployment so we can throw errors early.",yea catch error ideally validate throw early,issue,negative,positive,positive,positive,positive,positive
1990512365,@khluu could you run `compare_perf_metrics` script and paste the result?,could run script paste result,issue,negative,neutral,neutral,neutral,neutral,neutral
1990211156,"@simran-2797 This is ready for review, but we don't yet have any community examples yet. Are there some you want to include here? I think that would make the review process easier.",ready review yet community yet want include think would make review process easier,issue,positive,positive,positive,positive,positive,positive
1989735993,"````
botocore.exceptions.ClientError: An error occurred (SecurityGroupLimitExceeded) when calling the CreateSecurityGroup operation: The maximum number of security groups has been reached.
````

cc @rickyyx I recall we have seen this before.",error calling operation maximum number security recall seen,issue,negative,neutral,neutral,neutral,neutral,neutral
1989691893,@sven1977 any update on this; we need confirmation that this is/is-not a release-blocker for ray210,update need confirmation ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1989670199,"Also based on my test, the outer object (by ray.get) is correctly evict but input for the father object is not evict as the lineage ref = 1 after father task finished. Unless we call relase submitted object(not the results, it;s the args.)",also based test outer object correctly evict input father object evict lineage ref father task finished unless call object,issue,negative,neutral,neutral,neutral,neutral,neutral
1989666923,"But in this issue https://github.com/ray-project/ray/issues/37912, when correct = True, the object can correctly get evicted without free. ",issue correct true object correctly get without free,issue,negative,positive,positive,positive,positive,positive
1989659987,"The object is likely not getting evicted because it was created by `ray.put`.
> Also, one note is that objects created by ray.put are GCed a bit differently, because we cannot reconstruct them. So we keep the object around until its lineage ref count goes to 0, not just its normal ref count. You can workaround this by using the private API ray._private.internal_api.free.",object likely getting also one note bit differently reconstruct keep object around lineage ref count go normal ref count private,issue,negative,positive,neutral,neutral,positive,positive
1989655745,"1. True, that is what I expected, but when I check the delete, there is a parameter that controls the release of the lineage object. So when normal ref count is 0, but lineage reference is 1, it will not release the lineage object and keep it in memory. 
2. I think the original one is my intention for this bug as what we want to pass to the child task is a large object. And that will not be cleared after child task finished currentlly and keeps the memory high.

So. I think there are two problems here:
1. The lineage foot prints should have no relation with the object thus the evict object when lineage foot prints is high doen't make sense in `TaskManager::CompletePendingTask`. 
2. The object value is in fact not delelted when normal ref is 0 so we need to change the behavior of the lineage_pinned object. As currently the only way to clear the lineage_pinned object is to trigger the lineage foot print high and evict the lineage_pinned object.",true check delete parameter release lineage object normal ref count lineage reference release lineage object keep memory think original one intention bug want pas child task large object child task finished memory high think two lineage foot relation object thus evict object lineage foot high doe make sense object value fact normal ref need change behavior object currently way clear object trigger lineage foot print high evict object,issue,positive,positive,positive,positive,positive,positive
1989653400,"After more debugging, the issue seems to come from this section of the test
```python
  base = ray.data.range(1000, override_num_blocks=100)
  ds1 = base.map_batches(
      RandomExit, compute=ray.data.ActorPoolStrategy(size=4), max_task_retries=999
  )
  ds1.take_all()
```

Some of the time (likely due to the exits), tasks do not become ready causing the process to hang. [This call](https://github.com/ray-project/ray/blob/13e0767d713c95e26aa970fbb4ac6908549d8653/python/ray/data/_internal/execution/streaming_executor_state.py#L392) to `ray.wait` is where the hung tasks do not ever become ready.",issue come section test python base time likely due become ready causing process call hung ever become ready,issue,positive,negative,negative,negative,negative,negative
1989608625,"Ach, I need to separately handle the `sync` case, will fix in the morning",ach need separately handle sync case fix morning,issue,negative,neutral,neutral,neutral,neutral,neutral
1989598926,"This one was opened due to a bug in CI (it fails only one so it's not that flaky), let's close to see if it happens again",one due bug one flaky let close see,issue,negative,negative,negative,negative,negative,negative
1989592066,"Lineage footprint refers to metadata size, not data size of objects. When the normal ref count is 0, we delete the object value but may still keep the metadata around. When the *lineage ref count* is 0, we delete the metadata.

Can you send a different code example? The original one in the issue description copies the object values so it doesn't show the intended bug, I think.

Also, one note is that objects created by `ray.put` are GCed a bit differently, because we cannot reconstruct them. So we keep the object around until its lineage ref count goes to 0, not just its normal ref count. You can workaround this by using the private API `ray._private.internal_api.free`.",lineage footprint size data size normal ref count delete object value may still keep around lineage ref count delete send different code example original one issue description object show intended bug think also one note bit differently reconstruct keep object around lineage ref count go normal ref count private,issue,positive,positive,positive,positive,positive,positive
1989574055,"In my example, I just want to show that after the father task finished, the input for the children task should be cleared. However, it's not cleared currently. As the children task retryable is set to be true(Even the father task retryable is false), the lineage_pin is also set to be true, so the lineage count is still 1 even after father task finished and obj is not cleared. Other ways can try is to change this task to none retryable, or we count the lineage object size.",example want show father task finished input task however currently task set true even father task false also set true lineage count still even father task finished way try change task none count lineage object size,issue,positive,positive,neutral,neutral,positive,positive
1989561613,"Object values (which can be large) are GCed separately from their lineage metadata (a few KB/ObjectRef). MMAP_SHM in the grafana screenshot that you included is for object data, not metadata. The lineage eviction that you linked to in the code manages metadata, not data. ObjectRefs are GCed when they go out of scope, whereas their lineage metadata is GCed if:
1. The task that produced the object is retryable. AND
2. There is a task downstream to the object that is still in scope.

The reason you see MMAP_SHM increasing is because of the script. When you call `f_wrapper.remote(large_shared_object_ref)`, `def f_wrapper` receives the object's *value* not the ObjectRef. So when you call `f.remote(large_shared_object_ref)`, it creates a copy of the object instead of passing the ObjectRef. Here is a modified version of the script that works as expected:
```python
import time

import numpy as np
import ray


@ray.remote
def f(large_shared_object_ref):
    time.sleep(0.1)
    return np.random.rand(25000)  # 200 KB


@ray.remote
def f_wrapper(large_shared_object_ref):
    return f.remote(large_shared_object_ref[0])


def runner(is_correct: bool):
    ray.init(include_dashboard=True)
    max_num_pending_tasks = 2

    # This large shared object takes up 400 MB.
    large_shared_object = np.random.rand(50000000)
    large_shared_object_ref = ray.put(large_shared_object)

    pending_task_list, all_refs, outputs = [], [], []

    for _ in range(1000):
        if len(pending_task_list) >= max_num_pending_tasks:
            ready_refs, pending_task_list = ray.wait(pending_task_list, num_returns=1)
            outputs += ray.get(ready_refs)

        if is_correct:
            # In the correct case, we simply call the unnested remote function.
            new_task = f.remote(large_shared_object_ref)
            # We're holding onto a full list of these promises.
            all_refs.append(new_task)
            print(len(all_refs), ""tasks"")
            pending_task_list.append(new_task)
        else:
            # In the incorrect case, we call the wrapper remote function.
            new_task = f_wrapper.remote([large_shared_object_ref])
            # Since we now have nested remote calls, we unwrap the outer promise with `ray.get()`.
            new_task_inner_promise = ray.get(new_task)
            all_refs.append(new_task_inner_promise)
            print(len(all_refs), ""tasks"")
            pending_task_list.append(new_task_inner_promise)

    print(len(outputs), ""outputs"")
    ray.shutdown()


if __name__ == ""__main__"":
    #runner(is_correct=True)
    runner(is_correct=False)
```",object large separately lineage included object data lineage eviction linked code data go scope whereas lineage task produced object task downstream object still scope reason see increasing script call object value call copy object instead passing version script work python import time import import ray return return runner bool large object range correct case simply call remote function holding onto full list print else incorrect case call wrapper remote function since remote unwrap outer promise print print runner runner,issue,negative,positive,neutral,neutral,positive,positive
1989554358,"But when it reaches at limit 1GB,it calls release the lineage object data. Originally I found this and I suspect it's the meta data. But the later code that release the lineage object data makes me believe it also record the lineage object size. Maybe here we just need another counter for the lineage object size? And I think it's bacally what lineage_footprint_bytes what to achieves?",limit release lineage object data originally found suspect meta data later code release lineage object data believe also record lineage object size maybe need another counter lineage object size think,issue,negative,positive,positive,positive,positive,positive
1989546159,"The lineage footprint refers to how much driver heap memory is used to store task *metadata*, not object *data*. The grafana screenshot shows the memory footprint of object data. So I don't think this change makes sense. But I'll take a look at the original issue and try to understand what's going on.",lineage footprint much driver heap memory used store task object data memory footprint object data think change sense take look original issue try understand going,issue,negative,positive,positive,positive,positive,positive
1989537277,@shixianc Talked with the team about this. We will just log a warning to let the user know `max_concurrent_queries` should be reconfigured. Will not change the behavior for the code above. ,team log warning let user know change behavior code,issue,negative,neutral,neutral,neutral,neutral,neutral
1989534149,"@jjyao Seems like if the placement group bundles are invalid, we try to create them with `ray.util.placement_group(bundles)` and it fails, then the controller will just busy spin while failing repeatedly while calling `deployment_scheduler.schedule()`.",like placement group invalid try create controller busy spin failing repeatedly calling,issue,negative,positive,neutral,neutral,positive,positive
1989532646,"@stevenhubhub could you copy-paste text not pictures? Please provide a reproducer or at least some idea of what your code looks like. What version(s) of ray did you try? Where did you get them (it seems there are hints of anaconda, or are you using a miniconda variant)? How did you install ray?",could text please provide reproducer least idea code like version ray try get anaconda variant install ray,issue,positive,negative,negative,negative,negative,negative
1989521975,"@zcin and I debugged this further. It doesn't look like a problem with Ray tasks in general. See repro below.

```python
# File name: tasks.py

import os
import ray
from ray._private.utils import import_attr

ray.init(address=""auto"")

@ray.remote
def try_import():
   print(""Current working directory: "", os.getcwd())
   print(""Files in cwd: "", os.listdir("".""))
   print(import_attr(""repro:app""))

ray.get(try_import.remote())
```

Start a long-lived Ray cluster in the `tld` directory (which contains `repro.py`):

```
ray start --head
```

Run the Python script:

```
python tasks.py
```

The script prints the current directory and the files inside, confirming that it's running in the expected directory. It successfully imports Python objects from the file in the local directory.

```
% python tasks.py              
2024-03-11 15:00:02,644	INFO worker.py:1567 -- Connecting to existing Ray cluster at address: 10.103.209.171:6379...
2024-03-11 15:00:02,648	INFO worker.py:1743 -- Connected to Ray cluster. View the dashboard at http://127.0.0.1:8265 
(try_import pid=96678) Current working directory:  /Users/shrekris/Desktop/scratch
(try_import pid=96678) Files in cwd:  ['try.py', 'tasks.py', 'check.py', '.pytest_cache', 'repro.py', 'test_batching.py', 'empty.py', 'config.yaml', '__pycache__', 'make_errors.py', 'basic.py', 'kill.py', 'test_err.py', 'dev.py']
(try_import pid=96678) <ray.serve.deployment.Application object at 0x104fed670>
(try_import pid=96678) The default value for `max_ongoing_requests` is currently 100, but will change to 5 in the next upcoming release.
```
",look like problem ray general see python file name import o import ray import auto print current working directory print print start ray cluster directory ray start head run python script python script current directory inside confirming running directory successfully python file local directory python ray cluster address connected ray cluster view dashboard current working directory object default value currently change next upcoming release,issue,positive,positive,neutral,neutral,positive,positive
1989509707,Someone needs to reproduce it on our side. I don't think it's related to cpu affinity.,someone need reproduce side think related affinity,issue,negative,neutral,neutral,neutral,neutral,neutral
1989506013,"Forgot the context sorry - is this still failing? If not, I think we could close this. ",forgot context sorry still failing think could close,issue,negative,negative,negative,negative,negative,negative
1989505908,This is not something we officially support. We currently only assign gpu ids to different Ray workers.,something officially support currently assign different ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1989501188,@simonsays1980 are you able to reproduce it with latest Ray version?,able reproduce latest ray version,issue,negative,positive,positive,positive,positive,positive
1989493384,"@simonsays1980 Does this error get logged happen very consistently throughout the run? I'm thinking this might be a race condition of wandb deleting some file, while that file is trying to be synced to cloud storage.

Does the run eventually succeed? The syncing should get retried throughout the run.",error get logged happen consistently throughout run thinking might race condition file file trying cloud storage run eventually succeed get throughout run,issue,negative,positive,positive,positive,positive,positive
1989489132,Someone should read the related code and see how it can possibly happen.,someone read related code see possibly happen,issue,negative,neutral,neutral,neutral,neutral,neutral
1989478761,"@MahdiALLALA thanks for reporting this. Given it's working with normal Python interpreter and we don't officially support Cx_freeze, I'll make it as P2 for now.",thanks given working normal python interpreter officially support make,issue,positive,positive,positive,positive,positive,positive
1989472506,"Hey @crypdick a few questions:

1. do you have a repro for this? 
2. Is it a deterministic issue? 
3. Does it happen only with `tuner`?",hey deterministic issue happen tuner,issue,negative,neutral,neutral,neutral,neutral,neutral
1989471000,this is a windows test so probably not a weekly release blocker,test probably weekly release blocker,issue,negative,neutral,neutral,neutral,neutral,neutral
1989468566,"actually passed on release branch, let's close",actually release branch let close,issue,negative,neutral,neutral,neutral,neutral,neutral
1989467753,@woshiyyya the produce fix has been merged; should we unjail this test and try again? thankkks ,produce fix test try,issue,negative,neutral,neutral,neutral,neutral,neutral
1989432838,To close this - i think we need to track the memory regression specifically. Linked PRs only fixed the regression at one time instance. ,close think need track memory regression specifically linked fixed regression one time instance,issue,negative,positive,neutral,neutral,positive,positive
1989430633,looking at go/flaky it seems to be missing a bunch; @jjyao should this remain a weekly-release-blocker?,looking missing bunch remain,issue,negative,negative,negative,negative,negative,negative
1989320661,Triggered a postmerge run to run data flaky tests [here](https://buildkite.com/ray-project/postmerge/builds/3425#018e2f15-5809-410e-bd5b-b93dabd0710c).,triggered run run data flaky,issue,negative,neutral,neutral,neutral,neutral,neutral
1989233673,"This test is failing because of a [product change](https://github.com/anyscale/product/pull/26665), which decreased the amount of available memory on the head node. Pending on product-side fix.  ",test failing product change amount available memory head node pending fix,issue,negative,positive,positive,positive,positive,positive
1989149575,"Hi @stephanie-wang 
I have added the PR description. Let me know what else changes are needed from my side? Thanks!",hi added description let know else side thanks,issue,negative,positive,positive,positive,positive,positive
1989148370,"My code had the following line in the `train_loop` to report metrics:
```py
if i % 20 == 0:
    ray.train.report(metrics={""loss"": loss.detach().cpu().item()})
```

But as the docs says [here](https://docs.ray.io/en/latest/train/user-guides/monitoring-logging.html#train-monitoring-and-logging), `train.report()` has to be called on each worker. If I am not wrong, the condition statement prevents `train.report` on each worker as it gets called only in the worker where `i % 20 == 0`. Removing the condition statement fixes the issue.",code following line report metric loss worker wrong condition statement worker worker removing condition statement issue,issue,negative,negative,negative,negative,negative,negative
1989132773,Some macos tests are failing on python 3.9; they are likely pre-existing test failures with the current state of ray code (not exposed previously with python 3.8),failing python likely test current state ray code exposed previously python,issue,negative,negative,neutral,neutral,negative,negative
1989068381,"> @ruisearch42 also if I change a little code in C++, then I want to replace the module installed in my node, and do one round performance testing, can I just replace binary file? is there any guide for this?

I think yoiu can just ran `bazel build`: https://docs.ray.io/en/latest/ray-contribute/development.html#fast-debug-and-optimized-builds",also change little code want replace module node one round performance testing replace binary file guide think ran build,issue,negative,negative,negative,negative,negative,negative
1989049134,"Seems like the consistent failures are coming from what you linked @bveeramani, put up a PR to fix that [here](https://github.com/ray-project/ray/pull/43875). Still likely have some intermittent failures from the `test_streaming_fault_tolerance`, I will also spend a bit of time trying to deflake that.",like consistent coming linked put fix still likely intermittent also spend bit time trying,issue,negative,positive,positive,positive,positive,positive
1989039532,"Some (4?) of the macosx are exposed in this upgrade to not working for python 3.9. @jjyao need your advise here whether we have bandwidth to fix them in master branch (probably for ray 2.11), thankkks
- https://buildkite.com/ray-project/premerge/builds/21593#018e2e01-3cbe-4dc7-a9a5-def4686ac536/3136-7633
- https://buildkite.com/ray-project/premerge/builds/21593#018e2e01-5159-45ae-a405-5a056436df22/3136-5729",exposed upgrade working python need advise whether fix master branch probably ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1989029733,"> @ruisearch42 buddy, I managed install bazel6.4, now previous error gone, I saw new error below
> 
> ```
> (base) [root@cassiopeia ray]# bazel build -c fastbuild //:*
> Extracting Bazel installation...
> Starting local Bazel server and connecting to it...
> DEBUG: /mnt/nvme1n1/pfray/ray/bazel/ray_deps_setup.bzl:67:14: No implicit mirrors used because urls were explicitly provided
> DEBUG: /mnt/nvme1n1/pfray/ray/bazel/ray_deps_setup.bzl:67:14: No implicit mirrors used because urls were explicitly provided
> ERROR: /root/.cache/bazel/_bazel_root/86c0b2da2f7232f44660b8e2d43c00fd/external/bazel_tools/platforms/BUILD:89:6: in alias rule @bazel_tools//platforms:windows: Constraints from @bazel_tools//platforms have been removed. Please use constraints from @platforms repository embedded in Bazel, or preferably declare dependency on https://github.com/bazelbuild/platforms. See https://github.com/bazelbuild/bazel/issues/8622 for details.
> ERROR: /root/.cache/bazel/_bazel_root/86c0b2da2f7232f44660b8e2d43c00fd/external/bazel_tools/platforms/BUILD:89:6: Analysis of target '@bazel_tools//platforms:windows' failed
> ERROR: /root/.cache/bazel/_bazel_root/86c0b2da2f7232f44660b8e2d43c00fd/external/com_github_google_flatbuffers/BUILD:74:10: While resolving toolchains for target @com_github_google_flatbuffers//:flatc: invalid registered toolchain '@bazel_skylib//toolchains/unittest:cmd_toolchain': 
> ERROR: Analysis of target '//:ray/raylet/format/node_manager_generated.h' failed; build aborted: 
> INFO: Elapsed time: 35.111s
> INFO: 0 processes.
> FAILED: Build did NOT complete successfully (61 packages loaded, 1438 targets configured)
> ```

This file looks outdated and ""@bazel_tools//platforms"" is not supported. Here is what I saw on my platform:

```
alias(
    name = ""windows"",
    actual = ""@platforms//os:windows"",
)
```
Maybe clean up bazel completely and retry?",buddy install previous error gone saw new error base root ray build installation starting local server implicit used explicitly provided implicit used explicitly provided error alias rule removed please use repository preferably declare dependency see error analysis target error target invalid registered error analysis target build aborted time build complete successfully loaded file outdated saw platform alias name actual maybe clean completely retry,issue,negative,positive,neutral,neutral,positive,positive
1989013907,"Seems like `test_pipelined_execution` is also failing:
```
return DatasetStatsSummary(
--
  | operators_stats,
  | iter_stats,
  | stats_summary_parents,
  | self.number,
  | self.dataset_uuid,
  | self.time_total_s,
  | self.base_name,
  | self.extra_metrics,
  | self.global_bytes_spilled,
  | self.global_bytes_restored,
  | self.dataset_bytes_spilled,
  | >           self.streaming_exec_schedule_s.get(),
  | )
  | E       AttributeError: 'NoneType' object has no attribute 'get'
  |  
  | /rayci/python/ray/data/_internal/stats.py:791: AttributeError
```

https://buildkite.com/ray-project/postmerge/builds/3419#018e2e21-413c-4179-bbfd-6792f458f968",like also failing return object attribute,issue,negative,neutral,neutral,neutral,neutral,neutral
1989004075,"I can take a look at this, I think the root cause is potentially one of the tests in test_streaming_integration (more in this [issue](https://github.com/ray-project/ray/issues/43798)).",take look think root cause potentially one issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1988997937,"Looks like https://github.com/ray-project/ray/pull/43790 might've caused the regression (cc @omatthew98)
<img width=""1598"" alt=""image"" src=""https://github.com/ray-project/ray/assets/26107013/d099c53e-5643-43ea-8934-e17fd74f0edf"">
",like might regression image,issue,negative,neutral,neutral,neutral,neutral,neutral
1988938216,It's ray client and not a regression so I think it doesn't need to be a release blocker.,ray client regression think need release blocker,issue,negative,neutral,neutral,neutral,neutral,neutral
1988934878,"@angelinalg could you also review this? Thanks!

Let's take this as a release blocker",could also review thanks let take release blocker,issue,negative,positive,positive,positive,positive,positive
1988929198,"Yea, it's not high priority given user can join subprocesses themself.",yea high priority given user join,issue,negative,positive,positive,positive,positive,positive
1988867889,"When will this be released? I am seeing that it is still not part of the latest release 2.9.3:

https://github.com/ray-project/ray/blob/ray-2.9.3/java/dependencies.bzl#L7",seeing still part latest release,issue,negative,positive,positive,positive,positive,positive
1988815469,I was able to reproduce the issue. Looking further.,able reproduce issue looking,issue,negative,positive,positive,positive,positive,positive
1988802663,"Here is the error, recording it here for convenience:

```python-traceback
subprocess.CalledProcessError: Command '['ray', 'down', '-v', '-y', '/tmp/tmpsv_rby5w.yaml']' returned non-zero exit status 1.
stdout:
2024-03-09 21:30:18,624 INFO commands.py:388 -- Checking AWS environment settings
2024-03-09 21:30:18,624 VINFO utils.py:149 -- Creating AWS resource `ec2` in `us-west-2`
2024-03-09 21:30:20,584 VINFO utils.py:149 -- Creating AWS resource `iam` in `us-west-2`
2024-03-09 21:30:20,988 VINFO utils.py:149 -- Creating AWS resource `ec2` in `us-west-2`

stderr:
2024-03-09 21:30:18,511 INFO util.py:382 -- setting max workers for head node type to 0
Traceback (most recent call last):
  File ""/home/ray/anaconda3/bin/ray"", line 8, in <module>
    sys.exit(main())
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/scripts/scripts.py"", line 2610, in main
    return cli()
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/click/core.py"", line 1157, in __call__
    return self.main(*args, **kwargs)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/click/core.py"", line 1078, in main
    rv = self.invoke(ctx)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/click/core.py"", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/click/core.py"", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/click/core.py"", line 783, in invoke
    return __callback(*args, **kwargs)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/cli_logger.py"", line 856, in wrapper
    return f(*args, **kwargs)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/scripts/scripts.py"", line 1356, in down
    teardown_cluster(
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/commands.py"", line 441, in teardown_cluster
    config = _bootstrap_config(config)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/commands.py"", line 416, in _bootstrap_config
    resolved_config = provider_cls.bootstrap_config(config)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/aws/node_provider.py"", line 606, in bootstrap_config
    return bootstrap_aws(cluster_config)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/aws/config.py"", line 259, in bootstrap_aws
    config = _configure_security_group(config)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/aws/config.py"", line 676, in _configure_security_group
    security_groups = _upsert_security_groups(config, node_types_to_configure)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/aws/config.py"", line 713, in _upsert_security_groups
    security_groups = _get_or_create_vpc_security_groups(config, node_types)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/aws/config.py"", line 754, in _get_or_create_vpc_security_groups
    return {
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/aws/config.py"", line 755, in <dictcomp>
    node_type: vpc_to_sg[vpc_id] for node_type, vpc_id in node_type_to_vpc.items()
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/aws/utils.py"", line 32, in __missing__
    self[key] = self.default_factory(key)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/aws/config.py"", line 807, in _create_security_group
    client.create_security_group(
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/botocore/client.py"", line 530, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/botocore/client.py"", line 960, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.exceptions.ClientError: An error occurred (SecurityGroupLimitExceeded) when calling the CreateSecurityGroup operation: The maximum number of security groups has been reached.
```",error recording convenience command returned exit status environment resource resource resource setting head node type recent call last file line module main file line main return file line return file line main file line invoke return file line invoke return file line invoke return file line wrapper return file line file line file line file line return file line file line file line file line return file line file line self key key file line file line return file line raise error calling operation maximum number security,issue,negative,positive,neutral,neutral,positive,positive
1988641376,"We ended up re-writing this portion of our stack in pure C++/gRPC with Pybind11. I'm not a gRPC expert, but here were some anecdotal observations:
- Implementing with gRPC in pure Python resulted in *much worse performance* than just using Ray. I don't remember where I recorded benchmarks, but I recall that the same benchmark resulted in ~1.2s latency with Python:gRPC vs ~400ms with Ray
- The C++ based approach resulted in ~100ms on the same benchmark, with more potential for improvement.

I think everything is working as intended so I'll close this out.",ended portion stack pure expert anecdotal pure python much worse performance ray remember recall latency python ray based approach potential improvement think everything working intended close,issue,negative,positive,neutral,neutral,positive,positive
1988199874,Hello... @Bye-legumes  I'll be back to work tomorrow and is there any way to do the tests with the values ​​provided? I will also send the requested file @jjyao .,hello back work tomorrow way also send file,issue,negative,neutral,neutral,neutral,neutral,neutral
1988175206,"@mattip Thank you for the detailed explanation. I will have to go through it to understand it better, but this sure has been very helpful.",thank detailed explanation go understand better sure helpful,issue,positive,positive,positive,positive,positive,positive
1988158012,"TL;DR: Maybe you are running WSL headless and missing allowing ray through the firewall?

And for the whole triage report:

I get a different error when I did the following on WSL Ubuntu 22.04. Note I got grpcio 1.62.1. I did get some firewall popup windows that required user intervention the first time I ran `ray.init()`, so the process cannot be run headless:
```
$ python3 -m venv /tmp/venv310
$ source /tmp/venv310/bin/activate

(venv310) $ pip install ray==2.6.3

(venv310) $ pip list | grep ""\(grpcio\)\|\(ray\)""
grpcio                    1.62.1
ray                       2.6.3
 
(venv310) $ python
>>> import ray
>>> ray.init()
2024-03-11 09:10:47,742 INFO worker.py:1621 -- Started a local Ray instance.
E0311 09:10:48.908024500     306 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: \
    {""created"":""@1710141048.908010200"",""description"":""Protocol not available"",\
     ""errno"":92,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc"",""file_line"":202,\
     ""os_error"":""Protocol not available"",""syscall"":""getsockopt(SO_REUSEPORT)""\
    }
RayContext(dashboard_url='', python_version='3.10.12', ray_version='2.6.3', ray_commit='8a434b4ee7cd48e60fa1531315d39901fac5d79e', protocol_version=None)
(raylet) E0311 09:10:48.881503200     410 socket_utils_common_posix.cc:223] check for SO_REUSEPORT:\
     {""created"":""@1710141048.881474900"",""description"":""Protocol not available"",\
      ""errno"":92,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc"",""file_line"":202,\
      ""os_error"":""Protocol not available"",""syscall"":""getsockopt(SO_REUSEPORT)""
```

So it seems `SO_REUSEPORT` is not available on WSL. If I force grpcio 1.58.0, I get the same result.

But this does not seem to stop ray from working:

<details><summary>test_ray.py</summary>

```
$ cat /tmp/test_ray.py
import ray
ray.init()


# Define the square task.
@ray.remote
def square(x):
    return x * x


# Launch four parallel square tasks.
futures = [square.remote(i) for i in range(4)]


# Retrieve results.
print(ray.get(futures))
```

</details>

```
$ python /tmp/test_ray.py
2024-03-11 09:37:12,120 INFO worker.py:1621 -- Started a local Ray instance.
E0311 09:37:13.347362400     942 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {""created"":""@1710142633.347321900"",""description"":""Protocol not available"",""errno"":92,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc"",""file_line"":202,""os_error"":""Protocol not available"",""syscall"":""getsockopt(SO_REUSEPORT)""}
[0, 1, 4, 9]
```
. I do see an unguarded call in ray, maybe it should be conditional on `SO_REUSEPORT` working https://github.com/ray-project/ray/blob/4d2be8f35a805d251fa139838064a435e53fa2a2/src/ray/rpc/grpc_server.cc#L63-L67

I would think this use case is niche enough that fixing the error message is not a high priority. ",maybe running headless missing ray whole triage report get different error following note got get user intervention first time ran process run headless python source pip install pip list ray python import ray local ray instance check description protocol available file protocol available raylet check description protocol available file protocol available available force get result seem stop ray working summary cat import ray define square task square return launch four parallel square range retrieve print python local ray instance check description protocol available file protocol available see unguarded call ray maybe conditional working would think use case niche enough fixing error message high priority,issue,negative,positive,positive,positive,positive,positive
1987751805,@jjyao was this abandoned; still relevant to pick up and close out?,abandoned still relevant pick close,issue,negative,positive,positive,positive,positive,positive
1987726599,@jjyao let's prioritize this later unless you tell me an internal team needs this; focus on p0/p1 internal team reported issues seems bigger bang for buck,let later unless tell internal team need focus internal team bigger bang buck,issue,negative,neutral,neutral,neutral,neutral,neutral
1987725694,@rynewang @jjyao should we not bring this into scope for current sprint and pick up a p0/p1 of stability instead?,bring scope current sprint pick stability instead,issue,negative,neutral,neutral,neutral,neutral,neutral
1987433710,Closing this as complete - haven't heard of complaints about this yet with the new docs site. Please reopen if this is an issue.,complete yet new site please reopen issue,issue,negative,positive,positive,positive,positive,positive
1987204718,"I have some problems like this.
My python version is 3.7.16
For me, I need downgrade tensorflow==1.14.0 and gym==0.19.0
And then find the compatibility version of tensorflow_probability==0.7.0. You can refer [tensorflow_probability reslease](https://github.com/tensorflow/probability/releases?page=4)",like python version need downgrade find compatibility version refer,issue,negative,neutral,neutral,neutral,neutral,neutral
1987124459,"Hi @stephanie-wang @scottjlee , 
Is this issue assigned to anyone?
I'm interested in contributing to Ray and would like to take this up.",hi issue assigned anyone interested ray would like take,issue,positive,positive,positive,positive,positive,positive
1987011232,"Ah we found out the issue, we had an override on the starlette exception handler and this was the culprit.

Thanks",ah found issue override exception handler culprit thanks,issue,negative,positive,positive,positive,positive,positive
1986990899,"For what it's worth, I just ran into this issue again, only this time in the context of `Dataset.groupby(col)`. It's the same error message, and presumably the same code under the hood. Just a bummer.",worth ran issue time context col error message presumably code hood bummer,issue,negative,positive,positive,positive,positive,positive
1986955902,Not sure if it is a release blocker or not but it's failing on release 2.10 branch cut,sure release blocker failing release branch cut,issue,negative,positive,positive,positive,positive,positive
1986883161,Seeing the test able to run in postmerge again https://buildkite.com/ray-project/postmerge/builds/3407#018e220c-260d-45ce-a03c-d616dd28e405/6-11802,seeing test able run,issue,negative,positive,positive,positive,positive,positive
1986881259,I have a similar or maybe same issue in #43856. It affects all Ray tasks too.,similar maybe issue ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1986709987,@aslonnie I'm going to create a new branch. We can abandon and close this PR,going create new branch abandon close,issue,negative,positive,positive,positive,positive,positive
1986669070,I don't think they are even running. Someone has to remove these conditions https://github.com/ray-project/ray/pull/43100/files from re-test.,think even running someone remove,issue,negative,neutral,neutral,neutral,neutral,neutral
1986583898,I'll merge this for now. @angelinalg could you still review the doc related changes and we will address comments in the follow up PR.,merge could still review doc related address follow,issue,negative,neutral,neutral,neutral,neutral,neutral
1986557490,"Looks like the regression was a one-off. On the latest release test run based off `d4121f8eea02982f6bc9a9603df7cd960b5bd34d` (merged 3/8), the runtime is 113.43 seconds, which is in the same range as other previous runtimes. In addition, re-ran the commit on which the regression was observed (`b1f622df6f3eb64672069d46c2cdbd551f0c7a71`), and got a runtime of 112.26 seconds. So seems like a spotty delay.",like regression latest release test run based range previous addition commit regression got like spotty delay,issue,positive,positive,positive,positive,positive,positive
1986548112,"Let's mark this P2. And we have a plan for this issue:

1. merge #42992. This makes the worker to collect all  all processes to be killed on worker exit.
2. add this: on task exit, in the core worker we kill all subprocesses recursively, before accepting new task",let mark plan issue merge worker collect worker exit add task exit core worker kill new task,issue,negative,positive,positive,positive,positive,positive
1986512818,Closing for now in favor of #43772 but we should revisit once the TaskManager <> RefCounter deadlock situation is resolved. The merged PR is less ideal because it requires periodic GC and may have edge cases depending on order of task completion vs references out of scope.,favor revisit deadlock situation resolved le ideal periodic may edge depending order task completion scope,issue,positive,positive,positive,positive,positive,positive
1986495369,"@shixianc thanks for reporting this issue! `max_concurrent_queries` (will be renamed as `max_ongoing_requests` after Ray 2.10 and up) seems to be bounding the requests. If you do something like the below, then you should start seeing ""batching_issue.py:16 - length called: 256"" in the logs.

```
from ray import serve
import logging
from starlette.requests import Request


@serve.deployment(max_concurrent_queries=256)
class Model:
    def __init__(self):
        self.logger = logging.getLogger(""ray.serve"")

    @serve.batch(max_batch_size=256, batch_wait_timeout_s=5)
    async def batch_predict(self, inputs: list[str]):
        self.logger.info(f""length called: {len(inputs)}"")
        return [f""batch_predict({input})"" for input in inputs]


@serve.deployment
class Inference:
    def __init__(self, _model):
        self.model = _model

    async def __call__(self, request: Request) -> list[any]:
        request_json = await request.json()
        prompts = request_json.get(""prompts"")
        responses = [self.model.batch_predict.remote(prompt) for prompt in prompts]
        return await asyncio.gather(*responses)


model = Model.bind()
inference = Inference.bind(model)
```
I will continue to look into a fix so user doesn't need to override unnecessary  `max_concurrent_queries`. 
",thanks issue ray bounding something like start seeing length ray import serve import logging import request class model self self list length return input input class inference self self request request list await prompt prompt return await model inference model continue look fix user need override unnecessary,issue,positive,negative,neutral,neutral,negative,negative
1986330475,"> Will remove `serve_controller_num_starts` for now since it doesn't seem like it's being exported by Serve. Created an issue here: #43829

NP. This is somewhat redundant with the control loop counter anyways",remove since seem like serve issue somewhat redundant control loop counter anyways,issue,negative,negative,negative,negative,negative,negative
1986326491,Will remove `serve_controller_num_starts` for now since it doesn't seem like it's being exported by Serve. Created an issue here: https://github.com/ray-project/ray/issues/43829,remove since seem like serve issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1986267083,"
> Can't reproduce it on current release `2.9.3`.


This is probably not so specific to a ray release - what cuda versions are you using? ",ca reproduce current release probably specific ray release,issue,negative,neutral,neutral,neutral,neutral,neutral
1986263321,@jjyao I assume you are going to follow up on a separate PR w/ documentation? Please make sure to update the serve monitoring page and docs for the serve metric wrappers `serve/metrics.py`,assume going follow separate documentation please make sure update serve page serve metric,issue,positive,positive,positive,positive,positive,positive
1986254733,resolved here https://ray-distributed.slack.com/archives/CN2RGCHRR/p1709920701664179?thread_ts=1709231231.652879&cid=CN2RGCHRR but leaving this ticket open as a concrete example on how we can clean up the ray docker image story more stable.,resolved leaving ticket open concrete example clean ray docker image story stable,issue,positive,positive,positive,positive,positive,positive
1986208494,"> IIUC the issue is if any of coroutine is failed (cancelled), it raises an exception before all tasks in the thread pool is finished -> ExecuteTask finishes before task inside thread pool finishes right?

> Hmm actually I am curious why don't we just use gather(return_exceptions=True)?

> In this case, gather will not return until everything in the coroutine is finished, and the race condition won't happen?

That's not sufficient unfortunately, b/c of cancellation -- in this case there will be `CancelledError` thrown at `await asyncio.gather(...)` which we have to handle. 

On top of that -- cancelling future of the task already running in the executor won't interrupt the task, hence we have to ""enforce"" the interruption via signal to guarantee that the vector isn't being pushed into once we return from this method",issue exception thread pool finished task inside thread pool right actually curious use gather case gather return everything finished race condition wo happen sufficient unfortunately cancellation case thrown await handle top future task already running executor wo interrupt task hence enforce interruption via signal guarantee vector return method,issue,negative,positive,neutral,neutral,positive,positive
1986118445,"> Thanks for the reviews, I'll update the PR soon.
> 
> @edoakes any thoughts on other metrics that should be included here?

Oh nice, this is looking good! Could we please also add `serve_controller_num_control_loops`? This detects if the controller is ""stuck"" (counter is not increasing)",thanks update soon metric included oh nice looking good could please also add controller stuck counter increasing,issue,positive,positive,positive,positive,positive,positive
1986096814,"Hey @vickytsang, looks like the commit history got a bit messed up here. Could you create a new PR that's based off the current head of master? Thanks!",hey like commit history got bit could create new based current head master thanks,issue,positive,positive,positive,positive,positive,positive
1986094813,"Thanks for the reviews, I'll update the PR soon.

@edoakes any thoughts on other metrics that should be included here?",thanks update soon metric included,issue,negative,positive,positive,positive,positive,positive
1986056654,"@kira-lin can you check what's already inside test_hpu.py and just add extra tests to that file. Tests inside that file don't need real HPU hardware.

Concretely, you don't need real HPU hardware to run `ray.init(resources={""HPU"": 1})` since Ray resource is logical: https://docs.ray.io/en/latest/ray-core/scheduling/resources.html",check already inside add extra file inside file need real hardware concretely need real hardware run since ray resource logical,issue,negative,positive,positive,positive,positive,positive
1985945627,Thanks for creating the the issue @can-anyscale It's fixed by https://github.com/ray-project/ray/pull/43780 and seeing it passing when I manually trigger it https://buildkite.com/ray-project/postmerge/builds/3357#018e1b0e-e563-44d1-8762-bf3a1fc29d9e/191-1776,thanks issue fixed seeing passing manually trigger,issue,negative,positive,positive,positive,positive,positive
1985935920,Thanks for merging in master @edoakes should be able to merge now 😄,thanks master able merge,issue,negative,positive,positive,positive,positive,positive
1985746263,"also before merge, let's make sure to run mac test/window test/release test ",also merge let make sure run mac test,issue,negative,positive,positive,positive,positive,positive
1985734662,@fishbone we will turn it on by default. It is off because it is merged in the last minute and a bit of breaking change. ,fishbone turn default last minute bit breaking change,issue,negative,neutral,neutral,neutral,neutral,neutral
1985710048,"Here's an example stacktrace with some stuff ***'d out:

```
Read progress 0: 100%|█████████▉| 3711/3712 [00:39<00:00, 49.63it/s]Traceback (most recent call last):
  File ""/tmp/ray/session_2024-03-05_11-09-55_961367_273/runtime_resources/working_dir_files/_ray_pkg_ddef4fad01df9951/src/my_file.py"", line 277, in <module>
    ***
  File ""/tmp/ray/session_2024-03-05_11-09-55_961367_273/runtime_resources/working_dir_files/_ray_pkg_ddef4fad01df9951/src/my_file.py"", line 273, in ***
  File ""/tmp/ray/session_2024-03-05_11-09-55_961367_273/runtime_resources/working_dir_files/_ray_pkg_ddef4fad01df9951/src/my_file.py"", line 152, in ***
    print('size of prev_batches:', prev_batches.count())
  File ""/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/dataset.py"", line 2606, in count
    [get_num_rows.remote(block) for block in self.get_internal_block_refs()]
  File ""/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/dataset.py"", line 4779, in get_internal_block_refs
    blocks = self._plan.execute().get_blocks()
  File ""/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/lazy_block_list.py"", line 293, in get_blocks
    blocks, _ = self._get_blocks_with_metadata()
  File ""/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/lazy_block_list.py"", line 327, in _get_blocks_with_metadata
    meta = ray.get(refs_list.pop(-1))
  File ""/home/ray/anaconda3/lib/python3.10/site-packages/ray/_private/auto_init_hook.py"", line 22, in auto_init_wrapper
    return fn(*args, **kwargs)
  File ""/home/ray/anaconda3/lib/python3.10/site-packages/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
  File ""/home/ray/anaconda3/lib/python3.10/site-packages/ray/_private/worker.py"", line 2624, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(OSError): [36mray::_execute_read_task_split()[39m (pid=25144, ip=10.83.226.216)
  File ""/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/lazy_block_list.py"", line 637, in _execute_read_task_split
    for block in blocks:
  File ""/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/datasource/datasource.py"", line 237, in __call__
    yield from result
  File ""/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py"", line 430, in __call__
    for block in blocks:
  File ""/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py"", line 371, in __call__
    for data in iter:
  File ""/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/datasource/parquet_datasource.py"", line 508, in _read_fragments
    for batch in batches:
  File ""pyarrow/_dataset.pyx"", line 3414, in _iterator
  File ""pyarrow/_dataset.pyx"", line 3032, in pyarrow._dataset.TaggedRecordBatchIterator.__next__
  File ""pyarrow/error.pxi"", line 144, in pyarrow.lib.pyarrow_internal_check_status
  File ""pyarrow/error.pxi"", line 115, in pyarrow.lib.check_status
OSError: When reading information for key '***_000000.parquet' in bucket 'MY_BUCKET': AWS Error NETWORK_CONNECTION during HeadObject operation: curlCode: 28, Timeout was reached
```",example stuff read progress recent call last file line module file line file line print file line count block block file line file line file line meta file line return file line wrapper return file line get raise file line block file line yield result file line block file line data iter file line batch file line file line file line file line reading information key bucket error operation,issue,negative,neutral,neutral,neutral,neutral,neutral
1985691360,"Hi there,

I am seeing the same issue and it is a major blocker.

I am using `read_parquet` for 512 or more parquet chunks at a time  and read_binary_files for millions of files. In each case all files are in a common root prefix on S3. I am using about 1000 CPUs, but sometimes more. More CPUs seems to cause the error more often.

Sometimes its error getting information for key, sometimes it's libcurl was given bad argument, sometimes it's timeout like above, however, it's always caused by transient network errors that need to be retried.

I see some fixes for similar problems have been applied in the past, but there are still many gaps.

Some previous related works:
- https://github.com/ray-project/ray/pull/42252
- https://github.com/ray-project/ray/pull/42492
- https://github.com/ray-project/ray/pull/42806

The closest fix is this one: https://github.com/ray-project/ray/pull/42027
while it says it fixed for reads, inspecting the codebase in version 2.9.3 it looks like it's only actually fixed for writes.

The error I am seeing is `AWS Error NETWORK_CONNECTION` which is transient and must be retried. It is on the actual read tasks, as well as metadata tasks. Some gaps are here:
- https://github.com/ray-project/ray/blob/releases/2.9.3/python/ray/data/datasource/file_based_datasource.py#L68 should probably include `""AWS Error NETWORK_CONNECTION""`
- errors get thrown from these places (there may be more but I've caught these ones so far):
  - https://github.com/ray-project/ray/blob/releases/2.9.3/python/ray/data/datasource/parquet_datasource.py#L508
  - https://github.com/ray-project/ray/blob/releases/2.9.3/python/ray/data/datasource/parquet_datasource.py#L545
  - https://github.com/ray-project/ray/blob/releases/2.9.3/python/ray/data/datasource/file_meta_provider.py#L557

A bit of custom hackery into the codebase has improved the reliability greatly for me, although my solutions are too messy to PR. These code references should hopefully be helpful enough to indicate gaps in the current retry strategies though.",hi seeing issue major blocker parquet time million case common root prefix sometimes cause error often sometimes error getting information key sometimes given bad argument sometimes like however always transient network need see similar applied past still many previous related work fix one fixed version like actually fixed error seeing error transient must actual read well probably include error get thrown may caught far bit custom hackery reliability greatly although messy code hopefully helpful enough indicate current retry though,issue,negative,positive,neutral,neutral,positive,positive
1985264959,"@kevin85421  Thanks a lot for the support so far!
I see https://github.com/ray-project/ray/pull/43117 is approved but there are some concerns regarding its merge.
If that is the case, Is there any workaround I can do on my side with `IterableDataset` to make it work with Python 3.9+?",thanks lot support far see regarding merge case side make work python,issue,positive,positive,positive,positive,positive,positive
1985218655,"@khluu  please try again, and separate checking of uploading to private ecr vs uploading to docker hub.",please try separate private docker hub,issue,negative,neutral,neutral,neutral,neutral,neutral
1985217371,"this is breaking release tests run on master, which relies on the tags being pushed to the private ecr.",breaking release run master private,issue,negative,neutral,neutral,neutral,neutral,neutral
1985089291,"Seems like it can be flaky, but at least it's able to run on windows now https://buildkite.com/ray-project/postmerge/builds/3369#018e1c5f-28c6-4243-82f3-52dd51a6b77a/6-11590",like flaky least able run,issue,negative,positive,neutral,neutral,positive,positive
1985031256,"@zcin Actually you know what it is, could just be file name too long. Let's also try this before disable it https://github.com/ray-project/ray/pull/43812",actually know could file name long let also try disable,issue,negative,negative,neutral,neutral,negative,negative
1985025797,Change moved to #43767 which had correct signoff message. Discarding this one. ,change correct message one,issue,negative,neutral,neutral,neutral,neutral,neutral
1985013299,@zcin This seems to be newly added test. It's gonna be a release blocker for 2.10. Can you help taking a look🙏,newly added test gon na release blocker help taking look,issue,negative,positive,positive,positive,positive,positive
1984961656,"@jjyao have to revisit the approach:

 - TPE is used cooperatively by multiple requests (hence can't shut it down)
 - Running multiple TPEs proves costly in the benchmarks (`streaming_core_throughput.py`)
 - Instead, revisited approach to use an external interruption (using `threading.Event`) that only causes very minor effect on performance (see below)


```
# Before (master)
Core Actors streaming throughput (ASYNC) (num_replicas=1, tokens_per_request=1000, batch_size=10): 13052.62 +- 372.43 tokens/s
(CallerActor pid=56100) Individual request quantiles:
(CallerActor pid=56100) 	P50=497.5112084999988
(CallerActor pid=56100) 	P75=648.4424480000001
(CallerActor pid=56100) 	P99=807.8154376599998

# After 
Core Actors streaming throughput (ASYNC) (num_replicas=1, tokens_per_request=1000, batch_size=10): 12782.83 +- 216.97 tokens/s
(CallerActor pid=51634) Individual request quantiles:
(CallerActor pid=51634) 	P50=504.2683960000014
(CallerActor pid=51634) 	P75=667.4028960000006
(CallerActor pid=51634) 	P99=811.2643177199991
```",revisit approach used multiple hence ca shut running multiple costly instead approach use external interruption minor effect performance see master core streaming throughput individual request core streaming throughput individual request,issue,negative,negative,neutral,neutral,negative,negative
1984856830,"Hey @jennifgcrl, could you share a full traceback? Also, what does your cluster look like (type and number of nodes), and how did you choose between `read_parquet` and `read_parquet_bulk`?",hey could share full also cluster look like type number choose,issue,positive,positive,positive,positive,positive,positive
1984817734,"> LGTM
> 
> Re the example
> 
> ```
> Runtime Metrics:
> * ReadParquet->SplitBlocks(24): 1.42s (80.860%)
> * Map(f): 290.89ms (16.539%)
> * Sort: 0us (0.000%)
> * Filter(g): 39.68ms (2.256%)
> * Scheduling: 166.04ms (9.440%)
> * Total: 1.76s (100%)
> ```
> 
> Any reason why `Sort` took 0us in metrics here?

Hmm I think it might be a quirk of Sort having two sub-operators. From a little pdb investigation it seems like the SortMap and SortReduct `OperatorStatsSummary`s have their `time_total_s` set to 0 after execution. I think I have a potential fix for why this would be happening and will put in a separate PR.",example metric map sort u filter total reason sort took u metric think might quirk sort two little investigation like set execution think potential fix would happening put separate,issue,negative,negative,neutral,neutral,negative,negative
1984806804,@kevin85421 - btw could you point me to where you think is the best place to mention this in KubeRay's doc? ,could point think best place mention doc,issue,positive,positive,positive,positive,positive,positive
1984796464,Can't reproduce it on current release `2.9.3`.,ca reproduce current release,issue,negative,neutral,neutral,neutral,neutral,neutral
1984795498,"> btw other configs in `DataContext` w/ `bool(...)` are all right?

yes, they use `bool(int(...))`",bool right yes use bool,issue,negative,positive,positive,positive,positive,positive
1984780043,"Hi @dioptre I tested with your code running using Ray Serve
```
from ray import serve
from starlette.requests import Request


@serve.deployment
async def read_protected(request: Request):
    return {""msg"": ""pong""}

app = read_protected.bind()
```
To start serve with your application can run `serve run deployment:app` 

Using exactly your client code runs without issue. Can you give it a try and let us know if this works for you??",hi tested code running ray serve ray import serve import request request request return pong start serve application run serve run deployment exactly client code without issue give try let u know work,issue,negative,positive,positive,positive,positive,positive
1984773408,"Awesome, thanks for getting this out so quickly! The dashboard page looks good.

It's a little odd that ray_serve_controller_num_starts, ray_serve_num_scheduling_tasks, and ray_serve_num_scheduling_tasks_in_backoff aren't showing up. ray_serve_controller_num_starts should show up soon after the controller initializes for the first time. ray_serve_num_scheduling_tasks and ray_serve_num_scheduling_tasks_in_backoff should show up after a few requests on a hello world app.

Is there any chance that the query is filtering for an incorrect/unexpected tag?",awesome thanks getting quickly dashboard page good little odd showing show soon controller first time show hello world chance query filtering tag,issue,positive,positive,positive,positive,positive,positive
1984706615,"After digging into the cluster's logs, I found that there are many `Deadline Exceeded` errors in dashboard logs. There are no obvious errors or exceptions from GCS server and Raylet. Might be transient network issues.

This is also transient on release tests. No such failures from recent test results.

My suggestion is to downgrade it to P1/P2.",digging cluster found many deadline dashboard obvious server raylet might transient network also transient release recent test suggestion downgrade,issue,negative,positive,positive,positive,positive,positive
1984686313,"im removing the release-blocker here since its just a doc change cc @angelinalg 
",removing since doc change,issue,negative,neutral,neutral,neutral,neutral,neutral
1984630779,"Hey @vovochka404, thanks for filing the issue; I can understand the frustration here. It's pretty unlikely that we will bring back the old replica scheduling technique because it had some fundamental incompatibilities with the API, i.e., `max_concurrent_queries` was enforced per-caller instead of per-replica which was very confusing and led to unintuitive & hard-to-configure autoscaling behaviors.

I have been working to improve the efficiency of the new scheduling technique to reduce the overheads that you mentioned by adding caching that reduces the number of RTTs in the fast path to be equivalent to the old technique: https://github.com/ray-project/ray/pull/42943. This will come out in the upcoming Ray 2.10 release (branch cut is tomorrow, so optimistically out by end of next week), but of course you can test it out using the [nightly wheels](https://docs.ray.io/en/latest/ray-overview/installation.html#daily-releases-nightlies).

Please give it a go and let me know if it makes a difference for you. I believe we are also going to allocate time for the 2.11 release to reduce the overheads in the proxy in general which should provide more benefit.",hey thanks filing issue understand frustration pretty unlikely bring back old replica technique fundamental enforced instead led unintuitive working improve efficiency new technique reduce number fast path equivalent old technique come upcoming ray release branch cut tomorrow optimistically end next week course test nightly please give go let know difference believe also going allocate time release reduce proxy general provide benefit,issue,positive,positive,neutral,neutral,positive,positive
1984620887,@khluu looks like this test has been passing for the past several nightly runs. Should we close this Issue?,like test passing past several nightly close issue,issue,negative,negative,negative,negative,negative,negative
1984419113,"New output:
```
2024-03-07 12:48:30,003	WARNING api.py:408 -- The default value for `max_ongoing_requests` is currently 100, but will change to 5 in the next upcoming release.
2024-03-07 12:48:31,929	INFO worker.py:1743 -- Started a local Ray instance. View the dashboard at 127.0.0.1:8265
(ServeController pid=19410) INFO 2024-03-07 12:48:34,052 controller 19410 deployment_state.py:1581 - Deploying new version of Deployment(name='MyModelDeployment', app='default') (initial target replicas: 1).
(ProxyActor pid=19414) INFO 2024-03-07 12:48:33,997 proxy 10.104.98.7 proxy.py:1160 - Proxy starting on node a74e5397cb9025c1bccdb1a7e1c3a74a0ada0b5b7be4be922d9608e9 (HTTP port: 8000).
(ServeController pid=19410) INFO 2024-03-07 12:48:34,153 controller 19410 deployment_state.py:1883 - Adding 1 replica to Deployment(name='MyModelDeployment', app='default').
2024-03-07 12:48:35,033	INFO api.py:585 -- Deployed app 'default' successfully.
{'result': 'Hello world!'}
(ServeReplica:default:MyModelDeployment pid=19415) INFO 2024-03-07 12:48:35,067 default_MyModelDeployment gzhdjwcw 20888cca-8c0b-43c4-9a18-2c7282530190 / replica.py:366 - __CALL__ OK 2.3ms
```

The deprecation warning is gone. The warning about the default value changing is expected. @architkulkarni LMK if this is good to close.",new output warning default value currently change next upcoming release local ray instance view dashboard controller new version deployment initial target proxy proxy starting node port controller replica deployment successfully world default deprecation warning gone warning default value good close,issue,positive,positive,positive,positive,positive,positive
1984234012,"> should we also indicate this method as deprecated? (inside the function itself) https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.to_random_access_dataset.html

yes, updated.",also indicate method inside function yes,issue,negative,neutral,neutral,neutral,neutral,neutral
1984214719,"Doc tabs look correct. (it's purple in the second one because my mouse was hovering on it)

<img width=""1145"" alt=""Screenshot 2024-03-07 at 10 52 04 AM"" src=""https://github.com/ray-project/ray/assets/5459654/60fd4e70-4558-4d62-bf83-0814214679ad"">
<img width=""1129"" alt=""Screenshot 2024-03-07 at 10 52 12 AM"" src=""https://github.com/ray-project/ray/assets/5459654/449498f2-852b-4fb5-aacf-c4a713fb437b"">
",doc look correct purple second one mouse hovering,issue,negative,neutral,neutral,neutral,neutral,neutral
1984206206,should we also indicate this method as deprecated? (inside the function itself) https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.to_random_access_dataset.html,also indicate method inside function,issue,negative,neutral,neutral,neutral,neutral,neutral
1983262179,"With vllm now running off pydantic 2.0 without the v1 compat layer, is there a working branch/fork for ray where we can contribute to get ray and by extension ray-llm running? I'm willing to put aside time if its helpful, just not sure on the best starting point :)

Cheers!",running without layer working ray contribute get ray extension running willing put aside time helpful sure best starting point,issue,positive,positive,positive,positive,positive,positive
1983250124,We have the same issue as markgoodhead. Are there any plans for when python 3.11 will no longer be tagged as experimental?,issue python longer tagged experimental,issue,negative,positive,neutral,neutral,positive,positive
1982313636,"We have added this feature into our databricks runtime instead of in Ray OSS side.

In future, once we decide to opensource the feature, I will come back and make this PR to go into Ray OSS.

Thanks everyone's help! 
",added feature instead ray side future decide feature come back make go ray thanks everyone help,issue,positive,positive,neutral,neutral,positive,positive
1982301654,"Alright, i think after staring at the code for Nth time, i have a pretty good candidate for why this is happening:

 - This issue is another edge-case of the issue @edoakes fixed in #40122

The summary of the problem is like following:

 - When we start streaming back we handle reporting individual response back in a event-loop’s ThreadPoolExecutor [here](https://github.com/ray-project/ray/blob/fbcc51128b30a0402369cb30078fba00a78bc8c9/python/ray/_raylet.pyx#L1406)

 - When request is cancelled, we only cancel the coroutine iterating over the generator, but not the tasks that are already scheduled in the TPE

 - If CoreWorker::ExecuteTask (running boost::fiber) is able to complete before the task in the TPE, it will deallocate some of the vectors (like dynamic_return_objects)

 - *While* the reporting task (scheduled in TPE) will attempt to push_back into that vector [here](https://github.com/ray-project/ray/blob/fbcc51128b30a0402369cb30078fba00a78bc8c9/python/ray/_raylet.pyx#L1298)


I was able to reproduce this condition using following branch: https://github.com/ray-project/ray/pull/43765

```
(EndpointActor pid=39167) >>> 2024-03-07 03:58:15.000985 [run_async_func_or_coro_in_event_loop] Before awaiting coro (None, None)
(EndpointActor pid=39167) >>> 2024-03-07 03:58:15.136913 [run_async_func_or_coro_in_event_loop] After awaiting coro (None, None)
(EndpointActor pid=39167) >>> 2024-03-07 03:58:15.136948 [run_async_func_or_coro_in_event_loop] Notifying fiber-event (None, None)
(EndpointActor pid=39167) >>> 2024-03-07 03:58:15.137230 [run_async_func_or_coro_in_event_loop] Before awaiting coro (TaskID(ffffffffffffffffc588185e903f91baae3050f101000000), TaskID(ffffffffffffffffc588185e903f91baae3050f101000000))
(EndpointActor pid=39167) >>> 2024-03-07 03:58:15.137271 [run_async_func_or_coro_in_event_loop] After awaiting coro (TaskID(ffffffffffffffffc588185e903f91baae3050f101000000), TaskID(ffffffffffffffffc588185e903f91baae3050f101000000))
(EndpointActor pid=39167) >>> 2024-03-07 03:58:15.137284 [run_async_func_or_coro_in_event_loop] Notifying fiber-event (TaskID(ffffffffffffffffc588185e903f91baae3050f101000000), TaskID(ffffffffffffffffc588185e903f91baae3050f101000000))
(EndpointActor pid=39167) >>> [CoreWorkerDirectTaskReceiver::HandleTask[accept_callback]] Left task handler: Type=ACTOR_CREATION_TASK, Language=PYTHON, Resources: {CPU: 1, }, function_descriptor={type=PythonFunctionDescriptor, module_name=test_streaming_generator_regression, class_name=EndpointActor, function_name=__init__, function_hash=14aa7a080ebf4da4bad7c9589696c4dc}, task_id=ffffffffffffffffc588185e903f91baae3050f101000000, task_name=EndpointActor.__init__, job_id=01000000, num_args=4, num_returns=1, max_retries=0, depth=1, attempt_number=0, actor_creation_task_spec={actor_id=c588185e903f91baae3050f101000000, max_restarts=0, max_concurrency=1000, is_asyncio_actor=1, is_detached=0}
(EndpointActor pid=39167) >>> [CoreWorkerDirectTaskReceiver::HandleTask[accept_callback]] Completed
(EndpointActor pid=39167) >>> 2024-03-07 03:58:15.165190 [run_async_func_or_coro_in_event_loop] Before awaiting coro (TaskID(d339719ef4992055c588185e903f91baae3050f101000000), TaskID(d339719ef4992055c588185e903f91baae3050f101000000))
(EndpointActor pid=39167) >>> 2024-03-07 03:58:15.165266 [execute_streaming_generator_async] Scheduling report_streaming_generator_output (TaskID(d339719ef4992055c588185e903f91baae3050f101000000))
(EndpointActor pid=39167) >>> [DelayedThreadPoolExecutor] Starting executing function with delay 2s>>> 2024-03-07 03:58:15.165405 [execute_streaming_generator_async] Scheduled report_streaming_generator_output (TaskID(d339719ef4992055c588185e903f91baae3050f101000000))
(EndpointActor pid=39167)
(CallerActor pid=39168) >>> [CoreWorkerDirectTaskReceiver::HandleTask[accept_callback]] Left task handler: Type=ACTOR_CREATION_TASK, Language=PYTHON, Resources: {CPU: 1, }, function_descriptor={type=PythonFunctionDescriptor, module_name=test_streaming_generator_regression, class_name=CallerActor, function_name=__init__, function_hash=ac980b41fbca4c0db87d57953b1d9368}, task_id=ffffffffffffffff9b4c8bcf8be0b2cdca2a292201000000, task_name=CallerActor.__init__, job_id=01000000, num_args=2, num_returns=1, max_retries=0, depth=1, attempt_number=0, actor_creation_task_spec={actor_id=9b4c8bcf8be0b2cdca2a292201000000, max_restarts=0, max_concurrency=1000, is_asyncio_actor=1, is_detached=0}
(CallerActor pid=39168) >>> [Caller] Starting consuming stream
(EndpointActor pid=39167) >>> 2024-03-07 03:58:17.170524 [report_streaming_generator_output] Reporting value of (0) (None)
(EndpointActor pid=39167) >>> 2024-03-07 03:58:17.170839 [report_streaming_generator_output] Pushing into vector (of size 0) (None)
(EndpointActor pid=39167) >>> 2024-03-07 03:58:17.171527 [report_streaming_generator_output] Finished reporting (None)
(EndpointActor pid=39167) >>> 2024-03-07 03:58:17.171784 [execute_streaming_generator_async] Completed report_streaming_generator_output (TaskID(d339719ef4992055c588185e903f91baae3050f101000000)):  True False False
(EndpointActor pid=39167) >>> 2024-03-07 03:58:17.171926 [execute_streaming_generator_async] Returning (None) (TaskID(d339719ef4992055c588185e903f91baae3050f101000000))
(EndpointActor pid=39167) >>> 2024-03-07 03:58:17.171969 [execute_streaming_generator_async] Scheduling report_streaming_generator_output (TaskID(d339719ef4992055c588185e903f91baae3050f101000000))
(EndpointActor pid=39167) >>> 2024-03-07 03:58:17.172118 [execute_streaming_generator_async] Scheduled report_streaming_generator_output (TaskID(d339719ef4992055c588185e903f91baae3050f101000000))
(EndpointActor pid=39167) >>> [DelayedThreadPoolExecutor] Starting executing function with delay 2s
(CallerActor pid=39168) >>> [Caller] Received 0
(EndpointActor pid=39167) >>> 2024-03-07 03:58:19.174500 [report_streaming_generator_output] Reporting value of (1) (None)
(EndpointActor pid=39167) >>> 2024-03-07 03:58:19.175000 [report_streaming_generator_output] Pushing into vector (of size 1) (None)
(EndpointActor pid=39167) >>> 2024-03-07 03:58:19.175490 [report_streaming_generator_output] Finished reporting (None)
(EndpointActor pid=39167) >>> 2024-03-07 03:58:19.175875 [execute_streaming_generator_async] Completed report_streaming_generator_output (TaskID(d339719ef4992055c588185e903f91baae3050f101000000)):  True False False
(EndpointActor pid=39167) >>> 2024-03-07 03:58:19.176105 [execute_streaming_generator_async] Returning (None) (TaskID(d339719ef4992055c588185e903f91baae3050f101000000))
(EndpointActor pid=39167) >>> 2024-03-07 03:58:19.176186 [execute_streaming_generator_async] Scheduling report_streaming_generator_output (TaskID(d339719ef4992055c588185e903f91baae3050f101000000))
(EndpointActor pid=39167) >>> 2024-03-07 03:58:19.176517 [execute_streaming_generator_async] Scheduled report_streaming_generator_output (TaskID(d339719ef4992055c588185e903f91baae3050f101000000))
(EndpointActor pid=39167) >>> [DelayedThreadPoolExecutor] Starting executing function with delay 2s
(EndpointActor pid=39167) >>> 2024-03-07 03:58:19.178079 [cancel_async_task] Cancelling task TaskID(d339719ef4992055c588185e903f91baae3050f101000000)
(EndpointActor pid=39167) >>> [execute_streaming_generator_async] Exception:  CancelledError() True
(EndpointActor pid=39167) >>> 2024-03-07 03:58:19.180081 [execute_streaming_generator_async] Completed report_streaming_generator_output (TaskID(d339719ef4992055c588185e903f91baae3050f101000000)):  True True False
(EndpointActor pid=39167) >>> 2024-03-07 03:58:19.180275 [run_async_func_or_coro_in_event_loop] Caught CancelledError
(EndpointActor pid=39167) >>> [CoreWorkerDirectTaskReceiver::HandleTask[accept_callback]] Left task handler: Type=ACTOR_TASK, Language=PYTHON, Resources: {}, function_descriptor={type=PythonFunctionDescriptor, module_name=test_streaming_generator_regression, class_name=EndpointActor, function_name=aio_stream, function_hash=}, task_id=d339719ef4992055c588185e903f91baae3050f101000000, task_name=EndpointActor.aio_stream, job_id=01000000, num_args=0, num_returns=1, max_retries=0, depth=2, attempt_number=0, actor_task_spec={actor_id=c588185e903f91baae3050f101000000, actor_caller_id=ffffffffffffffff9b4c8bcf8be0b2cdca2a292201000000, actor_counter=0, retry_exceptions=0}
(EndpointActor pid=39167) Traceback (most recent call last):
(EndpointActor pid=39167)   File ""python/ray/_raylet.pyx"", line 1440, in ray._raylet.execute_streaming_generator_async
(EndpointActor pid=39167)     done = await fut
(EndpointActor pid=39167) asyncio.exceptions.CancelledError
(CallerActor pid=39168) >>> [Caller] Cancelling generator
(CallerActor pid=39168) >>> [Caller] **Sleeping** 2s
(raylet) A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffc588185e903f91baae3050f101000000 Worker ID: 7c84c15bd8ad241e8445503c962ba788ae26262bfa433b593ec9517b Node ID: c15b8bde7b0ac1a3e70e4b848931e01e05f6bbdc7848eb66a53c7a29 Worker IP address: 127.0.0.1 Worker port: 50231 Worker PID: 39167 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
(CallerActor pid=39168) >>> 2024-03-07 03:58:15.023446 [run_async_func_or_coro_in_event_loop] Before awaiting coro (None, None)
(CallerActor pid=39168) >>> 2024-03-07 03:58:15.161518 [run_async_func_or_coro_in_event_loop] After awaiting coro (None, None)
(CallerActor pid=39168) >>> 2024-03-07 03:58:15.161555 [run_async_func_or_coro_in_event_loop] Notifying fiber-event (None, None)
(CallerActor pid=39168) >>> 2024-03-07 03:58:15.163570 [run_async_func_or_coro_in_event_loop] Before awaiting coro (TaskID(c2668a65bda616c19b4c8bcf8be0b2cdca2a292201000000), TaskID(c2668a65bda616c19b4c8bcf8be0b2cdca2a292201000000)) [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)
(CallerActor pid=39168) >>> 2024-03-07 03:58:15.161809 [run_async_func_or_coro_in_event_loop] After awaiting coro (TaskID(ffffffffffffffff9b4c8bcf8be0b2cdca2a292201000000), TaskID(ffffffffffffffff9b4c8bcf8be0b2cdca2a292201000000))
(EndpointActor pid=39167) >>> 2024-03-07 03:58:19.180174 [run_async_func_or_coro_in_event_loop] Notifying fiber-event (TaskID(d339719ef4992055c588185e903f91baae3050f101000000), TaskID(d339719ef4992055c588185e903f91baae3050f101000000)) [repeated 2x across cluster]
(EndpointActor pid=39167) >>> [CoreWorkerDirectTaskReceiver::HandleTask[accept_callback]] Completed [repeated 2x across cluster]
>>> Workers state after:  [('01000000ffffffffffffffffffffffffffffffffffffffffffffffff', None), ('1316713ab579a1d22143694971686b9a6b617c738051e8c870c87421', 'INTENDED_SYSTEM_EXIT'), ('6b5bad30194d7907417eccc82ef563a4f13a1fc23da131c4dbc59cb8', None), ('7c84c15bd8ad241e8445503c962ba788ae26262bfa433b593ec9517b', 'SYSTEM_ERROR')]
(EndpointActor pid=39167) >>> 2024-03-07 03:58:21.177161 [report_streaming_generator_output] Reporting value of (2) (None)
(EndpointActor pid=39167) >>> 2024-03-07 03:58:21.177686 [report_streaming_generator_output] Pushing into vector (of size 3489917329260249840) (None)
(EndpointActor pid=39167) *** SIGSEGV received at time=1709783901 ***
(EndpointActor pid=39167) PC: @        0x105590c0c  (unknown)  __pyx_f_3ray_7_raylet_report_streaming_generator_output()
(EndpointActor pid=39167)     @        0x1061956b4  (unknown)  absl::lts_20230125::WriteFailureInfo()
(EndpointActor pid=39167)     @        0x106195400  (unknown)  absl::lts_20230125::AbslFailureSignalHandler()
(EndpointActor pid=39167)     @        0x186ad3a24  (unknown)  _sigtramp
(EndpointActor pid=39167)     @        0x105590bac  (unknown)  __pyx_f_3ray_7_raylet_report_streaming_generator_output()
(EndpointActor pid=39167)     @        0x1055f2668  (unknown)  __pyx_pw_11cfunc_dot_to_py_74__Pyx_CFunc_object____object____StreamingGeneratorExecutionContext___to_py_1wrap()
(EndpointActor pid=39167)     @        0x1024e9dd0  (unknown)  _PyObject_Call
(EndpointActor pid=39167)     @        0x1025d5ef4  (unknown)  _PyEval_EvalFrameDefault
(EndpointActor pid=39167)     @        0x1025cee98  (unknown)  _PyEval_EvalCode
(EndpointActor pid=39167)     @        0x1024e9fe4  (unknown)  _PyFunction_Vectorcall
(EndpointActor pid=39167)     @        0x1025d5ef4  (unknown)  _PyEval_EvalFrameDefault
(EndpointActor pid=39167)     @        0x1024ea0a0  (unknown)  function_code_fastcall
(EndpointActor pid=39167)     @        0x1025d93cc  (unknown)  call_function
(EndpointActor pid=39167)     @        0x1025d5ba8  (unknown)  _PyEval_EvalFrameDefault
(EndpointActor pid=39167)     @        0x1024ea0a0  (unknown)  function_code_fastcall
(EndpointActor pid=39167)     @        0x1025d5ef4  (unknown)  _PyEval_EvalFrameDefault
(EndpointActor pid=39167)     @        0x1024ea0a0  (unknown)  function_code_fastcall
(EndpointActor pid=39167)     @        0x1025d93cc  (unknown)  call_function
(EndpointActor pid=39167)     @        0x1025d5ba8  (unknown)  _PyEval_EvalFrameDefault
(EndpointActor pid=39167)     @        0x1024ea0a0  (unknown)  function_code_fastcall
(EndpointActor pid=39167)     @        0x1025d93cc  (unknown)  call_function
(EndpointActor pid=39167)     @        0x1025d5ba8  (unknown)  _PyEval_EvalFrameDefault
(EndpointActor pid=39167)     @        0x1024ea0a0  (unknown)  function_code_fastcall
(EndpointActor pid=39167)     @        0x1024ece70  (unknown)  method_vectorcall
(EndpointActor pid=39167)     @        0x10269f0cc  (unknown)  t_bootstrap
(EndpointActor pid=39167)     @        0x102638574  (unknown)  pythread_wrapper
(EndpointActor pid=39167)     @        0x186aa5034  (unknown)  _pthread_start
(EndpointActor pid=39167)     @        0x186a9fe3c  (unknown)  thread_start
(EndpointActor pid=39167) [2024-03-06 19:58:21,180 E 39167 43803399] logging.cc:361: *** SIGSEGV received at time=1709783901 ***
(EndpointActor pid=39167) [2024-03-06 19:58:21,181 E 39167 43803399] logging.cc:361: PC: @        0x105590c0c  (unknown)  __pyx_f_3ray_7_raylet_report_streaming_generator_output()
(EndpointActor pid=39167) [2024-03-06 19:58:21,181 E 39167 43803399] logging.cc:361:     @        0x1061956b4  (unknown)  absl::lts_20230125::WriteFailureInfo()
(EndpointActor pid=39167) [2024-03-06 19:58:21,181 E 39167 43803399] logging.cc:361:     @        0x106195418  (unknown)  absl::lts_20230125::AbslFailureSignalHandler()
(EndpointActor pid=39167) [2024-03-06 19:58:21,181 E 39167 43803399] logging.cc:361:     @        0x186ad3a24  (unknown)  _sigtramp
(EndpointActor pid=39167) [2024-03-06 19:58:21,181 E 39167 43803399] logging.cc:361:     @        0x105590bac  (unknown)  __pyx_f_3ray_7_raylet_report_streaming_generator_output()
(EndpointActor pid=39167) [2024-03-06 19:58:21,181 E 39167 43803399] logging.cc:361:     @        0x1055f2668  (unknown)  __pyx_pw_11cfunc_dot_to_py_74__Pyx_CFunc_object____object____StreamingGeneratorExecutionContext___to_py_1wrap()
(EndpointActor pid=39167) [2024-03-06 19:58:21,181 E 39167 43803399] logging.cc:361:     @        0x1024e9dd0  (unknown)  _PyObject_Call
(EndpointActor pid=39167) [2024-03-06 19:58:21,181 E 39167 43803399] logging.cc:361:     @        0x1025d5ef4  (unknown)  _PyEval_EvalFrameDefault
(EndpointActor pid=39167) [2024-03-06 19:58:21,181 E 39167 43803399] logging.cc:361:     @        0x1025cee98  (unknown)  _PyEval_EvalCode
(EndpointActor pid=39167) [2024-03-06 19:58:21,182 E 39167 43803399] logging.cc:361:     @        0x1024e9fe4  (unknown)  _PyFunction_Vectorcall
(EndpointActor pid=39167) [2024-03-06 19:58:21,182 E 39167 43803399] logging.cc:361:     @        0x1025d5ef4  (unknown)  _PyEval_EvalFrameDefault
(EndpointActor pid=39167) [2024-03-06 19:58:21,182 E 39167 43803399] logging.cc:361:     @        0x1024ea0a0  (unknown)  function_code_fastcall
(EndpointActor pid=39167) [2024-03-06 19:58:21,182 E 39167 43803399] logging.cc:361:     @        0x1025d93cc  (unknown)  call_function
(EndpointActor pid=39167) [2024-03-06 19:58:21,182 E 39167 43803399] logging.cc:361:     @        0x1025d5ba8  (unknown)  _PyEval_EvalFrameDefault
(EndpointActor pid=39167) [2024-03-06 19:58:21,182 E 39167 43803399] logging.cc:361:     @        0x1024ea0a0  (unknown)  function_code_fastcall
(EndpointActor pid=39167) [2024-03-06 19:58:21,182 E 39167 43803399] logging.cc:361:     @        0x1025d5ef4  (unknown)  _PyEval_EvalFrameDefault
(EndpointActor pid=39167) [2024-03-06 19:58:21,182 E 39167 43803399] logging.cc:361:     @        0x1024ea0a0  (unknown)  function_code_fastcall
(EndpointActor pid=39167) [2024-03-06 19:58:21,182 E 39167 43803399] logging.cc:361:     @        0x1025d93cc  (unknown)  call_function
(EndpointActor pid=39167) [2024-03-06 19:58:21,182 E 39167 43803399] logging.cc:361:     @        0x1025d5ba8  (unknown)  _PyEval_EvalFrameDefault
(EndpointActor pid=39167) [2024-03-06 19:58:21,182 E 39167 43803399] logging.cc:361:     @        0x1024ea0a0  (unknown)  function_code_fastcall
(EndpointActor pid=39167) [2024-03-06 19:58:21,182 E 39167 43803399] logging.cc:361:     @        0x1025d93cc  (unknown)  call_function
(EndpointActor pid=39167) [2024-03-06 19:58:21,182 E 39167 43803399] logging.cc:361:     @        0x1025d5ba8  (unknown)  _PyEval_EvalFrameDefault
(EndpointActor pid=39167) [2024-03-06 19:58:21,182 E 39167 43803399] logging.cc:361:     @        0x1024ea0a0  (unknown)  function_code_fastcall
(EndpointActor pid=39167) [2024-03-06 19:58:21,182 E 39167 43803399] logging.cc:361:     @        0x1024ece70  (unknown)  method_vectorcall
(EndpointActor pid=39167) [2024-03-06 19:58:21,182 E 39167 43803399] logging.cc:361:     @        0x10269f0cc  (unknown)  t_bootstrap
(EndpointActor pid=39167) [2024-03-06 19:58:21,182 E 39167 43803399] logging.cc:361:     @        0x102638574  (unknown)  pythread_wrapper
(EndpointActor pid=39167) [2024-03-06 19:58:21,182 E 39167 43803399] logging.cc:361:     @        0x186aa5034  (unknown)  _pthread_start
(EndpointActor pid=39167) [2024-03-06 19:58:21,182 E 39167 43803399] logging.cc:361:     @        0x186a9fe3c  (unknown)  thread_start
(EndpointActor pid=39167) Fatal Python error: Segmentation fault
(EndpointActor pid=39167)
(EndpointActor pid=39167) Stack (most recent call first):
(EndpointActor pid=39167)   File ""/Users/ak/code/ray-project/ray/python/ray/tests/test_streaming_generator_regression.py"", line 35 in __slowed_fn
(EndpointActor pid=39167)   File ""/Users/ak/miniconda3/envs/ray/lib/python3.9/concurrent/futures/thread.py"", line 58 in run
(EndpointActor pid=39167)   File ""/Users/ak/miniconda3/envs/ray/lib/python3.9/concurrent/futures/thread.py"", line 83 in _worker
(EndpointActor pid=39167)   File ""/Users/ak/miniconda3/envs/ray/lib/python3.9/threading.py"", line 917 in run
(EndpointActor pid=39167)   File ""/Users/ak/miniconda3/envs/ray/lib/python3.9/threading.py"", line 980 in _bootstrap_inner
(EndpointActor pid=39167)   File ""/Users/ak/miniconda3/envs/ray/lib/python3.9/threading.py"", line 937 in _bootstrap
(CallerActor pid=39168) >>> [CoreWorkerDirectTaskReceiver::HandleTask[accept_callback]] Left task handler: Type=ACTOR_TASK, Language=PYTHON, Resources: {}, function_descriptor={type=PythonFunctionDescriptor, module_name=test_streaming_generator_regression, class_name=CallerActor, function_name=run, function_hash=}, task_id=c2668a65bda616c19b4c8bcf8be0b2cdca2a292201000000, task_name=CallerActor.run, job_id=01000000, num_args=0, num_returns=1, max_retries=0, depth=1, attempt_number=0, actor_task_spec={actor_id=9b4c8bcf8be0b2cdca2a292201000000, actor_caller_id=ffffffffffffffffffffffffffffffffffffffff01000000, actor_counter=0, retry_exceptions=0}
```",alright think staring code nth time pretty good candidate happening issue another issue fixed summary problem like following start streaming back handle individual response back request cancel generator already running boost able complete task like task attempt vector able reproduce condition following branch none none none none none none left task handler starting function delay left task handler caller starting consuming stream value none pushing vector size none finished none true false false none starting function delay caller received value none pushing vector size none finished none true false false none starting function delay task exception true true true false caught left task handler recent call last file line done await fut caller generator caller sleeping raylet worker task unexpected system error problem check dead worker id worker id node id worker address worker port worker worker exit type worker exit detail worker unexpectedly connection error code end file potential root process killer due high memory usage ray stop force worker unexpectedly due unexpected none none none none none none repeated across cluster ray default set disable log deduplication see repeated across cluster repeated across cluster state none none value none pushing vector size none received unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown received unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown fatal python error segmentation fault stack recent call first file line file line run file line file line run file line file line left task handler,issue,negative,negative,neutral,neutral,negative,negative
1982220564,"> For my own understanding, what change/when did `stats()` because a not consumption API?

I believe now that datasets are lazily executed, calling `ds.stats()` on a non-executed / materialized dataset will use the empty `DatasetStats` object (gives empty string).",understanding consumption believe lazily executed calling use empty object empty string,issue,negative,negative,negative,negative,negative,negative
1982205608,"> I think you can refer to this user guide: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html

I actually do not have one account in AWS S3, to run this test, I have to have one account on AWS S3?",think refer user guide actually one account run test one account,issue,negative,neutral,neutral,neutral,neutral,neutral
1982189659,"Remaining items in the issue are to be done after Ray 2.10 (awaiting Core fix, P2 feature).",issue done ray core fix feature,issue,negative,neutral,neutral,neutral,neutral,neutral
1982146519,"Hi @raulchen , thanks for pushing this fix -- this actually fixed a NCCL timeout error that we were seeing when doing multi-node distributed training. The behavior there was that sometimes randomly at the start of a train epoch, we would hit a NCCL timeout error because all of the ranks except one were trying to allreduce the gradients.

I'm also confused by the deadlock explanation though. Have you / the team thought more about how exactly this would have created a deadlock with gradient synchronization? We iterate over our data using the `iter_torch_batches` function, and I thought this would call into `streaming_split` with `equal=True` (so each worker gets an equal number of rows).

If it's helpful, we only started seeing this issue when we scaled up the model size (probably because gradient synchronization took longer).",hi thanks pushing fix actually fixed error seeing distributed training behavior sometimes randomly start train epoch would hit error except one trying also confused deadlock explanation though team thought exactly would deadlock gradient synchronization iterate data function thought would call worker equal number helpful seeing issue scaled model size probably gradient synchronization took longer,issue,negative,negative,neutral,neutral,negative,negative
1982019840,pls update TCO and ask @jjyao to pull the merge,update ask pull merge,issue,negative,neutral,neutral,neutral,neutral,neutral
1982003270,"> @zcin you're right that this can only happen if we have multiple threads accessing the `MetricsPusher`, in which case I'm OK without this fix. Previously I thought this could happen just due to ordering differences between when the stop event and timer task finished. I'm OK skipping it since this class is certainly not thread-safe.

cc: @alexeykudinkin Since we also discussed this offline. Are you OK with omitting this fix or you still think we should defensively add this fix?",right happen multiple case without fix previously thought could happen due stop event timer task finished skipping since class certainly since also fix still think defensively add fix,issue,negative,positive,neutral,neutral,positive,positive
1982000131,this feels like it only introduces a memory stability issue in more niche cases; should we label this as p2 @jjyao @rynewang ?,like memory stability issue niche label,issue,negative,neutral,neutral,neutral,neutral,neutral
1981997472,@rynewang would you consider this a stability or usability issue?,would consider stability usability issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1981995049,"A second try gave me the same result:

```
(remote_task pid=35768) 2024-03-06 12:43:52.959127: Task going to sleep...
(remote_task pid=35768) 2024-03-06 12:44:02.960327: Task going to sleep...
(remote_task pid=35768) 2024-03-06 12:44:12.966625: Task going to sleep...
Obtained remote result: 1709758690.668441
(remote_task pid=35768) Task finished sleeping for 7200 seconds -- returning.
```

@sfriedowitz Can you try with the latest Ray version?",second try gave result task going sleep task going sleep task going sleep remote result task finished sleeping try latest ray version,issue,negative,positive,positive,positive,positive,positive
1981985962,"Still some mystery:

```
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py"", line 826, in reducer_override
    if sys.version_info[:2] < (3, 7) and _is_parametrized_type_hint(
```

The python is obviously 3.9 so the boolean after `and` should not run...?",still mystery file line python obviously run,issue,negative,neutral,neutral,neutral,neutral,neutral
1981984633,This should no longer happen since latest Ray already updated cloudpickle and no longer has _is_parametrized_type_hint.,longer happen since latest ray already longer,issue,negative,positive,positive,positive,positive,positive
1981959852,"@jjyao after some investigation I believe this PR is the best way to fix it, pls review and merge",investigation believe best way fix review merge,issue,positive,positive,positive,positive,positive,positive
1981944660,"So the solution:

- give a flag to direct_task_submitter_ before `local_raylet_client_->Disconnect` to avoid any retries.",solution give flag disconnect avoid,issue,negative,neutral,neutral,neutral,neutral,neutral
1981875393,Note this task is big and may take multiple PRs. Hopefully we can also add some more unit tests to the previously untestable cython code.,note task big may take multiple hopefully also add unit previously untestable code,issue,negative,negative,neutral,neutral,negative,negative
1981815057,"I encountered this problem when trying to make an evaluation worker : 
I simply had to specify 
`create_env_on_local_worker= True
`
hope this helps ",problem trying make evaluation worker simply specify true hope,issue,negative,positive,positive,positive,positive,positive
1981799768,@hanyuyangddgh please re-open if this is an issue on latest ray.,please issue latest ray,issue,negative,positive,positive,positive,positive,positive
1981799639,"Where did you get python, ~how are you installing ray~? We have seen some problems with the python from the windows app store.",get python seen python store,issue,negative,neutral,neutral,neutral,neutral,neutral
1981790332,@mattip please take a look and determine priority; reach out to core anyscaler team if you have questions,please take look determine priority reach core team,issue,negative,neutral,neutral,neutral,neutral,neutral
1981715323,@rdabane I never really adopted Ray so unfortunately I can't help!,never really adopted ray unfortunately ca help,issue,negative,negative,negative,negative,negative,negative
1981701246,"Previous time flow is not detailed. Here is what happened

# core worker shutdown process

In core worker code:
- `CoreWorker::Exit`
    - `task_execution_service_.post` to do this:

```cpp
Disconnect(exit_type, detail, creation_task_exception_pb_bytes);    // 2.a below
Shutdown();    // 4.a below
```

so first calls `Disconnect` then `Shutdown`.

Then:

1. core worker in `task_execution_service_`
    a. `CoreWorker::Disconnect`
    b. `local_raylet_client_->Disconnect`
2. raylet receives socket msg
    a. `NodeManager::ProcessDisconnectClientMessage`
    b. `NodeManager::DisconnectClient`
    d. `cluster_task_manager_->CancelTaskForOwner(worker->GetAssignedTaskId());`
    e. `ReplyCancelled`
3. core worker receives RPC in event loop `io_service_`
    a. callback in `CoreWorkerDirectTaskSubmitter::RequestNewWorkerIfNeeded`
    b. calls itself to retry
4. core worker runs in event loop `task_execution_service_`:
    a. `CoreWorker::Shutdown`
    b. `io_service_.stop();`
    c. after this, it no longer handles RPC replies

3 and 4 are in different threads and racing.
if 3 happens-before 4, we got the leak.
if 4 happens-before 3, the RPC reply is never run (`io_service_` stopped) so no leak.
",previous time flow detailed core worker shutdown process core worker code disconnect detail shutdown first disconnect shutdown core worker disconnect raylet socket core worker event loop retry core worker event loop longer different racing got leak reply never run stopped leak,issue,negative,positive,positive,positive,positive,positive
1981700828,"@demitri 
How did you solve this issue?
I'm also interesting in this.",solve issue also interesting,issue,positive,positive,positive,positive,positive,positive
1981698980,"@geekboood Thanks for reporting the issue. Generally we are discouraging the use of advanced api such as `_to_object_ref`. In this case, `_to_object_ref` should only be used on short lived Ray Actor and/or Task. See doc: https://docs.ray.io/en/latest/serve/model_composition.html#advanced-convert-a-deploymentresponse-to-a-ray-objectref

Since you are calling methods on Serve deployments, I would suggest you to change the code to something like 
```
from starlette.requests import Request
from ray import serve

@serve.deployment()
class Dep1:
    def get_length(self, params: str):
        return len(params)
    
    def multiple_by_two(self, length: int):
        return length*2

@serve.deployment()
class Gateway:
    def __init__(self, dep1):
        self.dep1: Dep1 = dep1

    async def __call__(self, http_request: Request) -> str:
        raw_data = await http_request.body()
        length = self.dep1.get_length.remote(raw_data)
        # `length` being a `DeploymentResponse` can be passed into another 
        # `DeploymentHandle` directly to achieve model composition. 
        return await self.dep1.multiple_by_two.remote(length)

if __name__ == ""__main__"":
    deployment = Gateway.bind(Dep1.bind())
    handle = serve.run(deployment)

```

I would continue to look into possible fixes for the memory leak when using `_to_object_ref`. But please also let us know if this solves your usecase. ",thanks issue generally discouraging use advanced case used short lived ray actor task see doc since calling serve would suggest change code something like import request ray import serve class self return self length return length class gateway self self request await length length another directly achieve model composition return await length deployment handle deployment would continue look possible memory leak please also let u know,issue,positive,positive,positive,positive,positive,positive
1981691554,"Ok, manually tested, and it runs with the workloads I've thrown at it, updated to latest master branch",manually tested thrown latest master branch,issue,negative,positive,positive,positive,positive,positive
1981631693,"Ready to review now, I'll merge after 2.10 branch cut",ready review merge branch cut,issue,negative,positive,positive,positive,positive,positive
1981606355,"> @jjyao There is no related issue. It is just some improvement that I think of as I read the code.
> 
> Do you want me to create an issue or do you only want PR that has an explicit issue?

It's okay if there's no issue for small improvements. But let's add a PR description - it helps for reviewing and we use it as the commit message on master.",related issue improvement think read code want create issue want explicit issue issue small let add description use commit message master,issue,positive,negative,negative,negative,negative,negative
1981592934,"I see, thank you for the clarification! I'll close the issue since it has already been fixed.",see thank clarification close issue since already fixed,issue,negative,positive,neutral,neutral,positive,positive
1981571097,Ah I had thought about that too but decided against it in case `_forward_module.` was contained in the middle of the key (e.g. if it was somehow there in a nested attribute) and not just the start.,ah thought decided case middle key somehow attribute start,issue,negative,neutral,neutral,neutral,neutral,neutral
1981560297,"Hi @matthewdeng Thank you so much for the update and for resolving the issue. However, I noticed that the accepted fix in  https://github.com/ray-project/ray/pull/43594 involves checking if each key starts with ""_forward_module."" and then performing string splicing if the condition is true, which seems tedious. In contrast, the solution I propose, using the `replace` method, is more straightforward and requires modifying only one line of code. Do you think it would be worthwhile to make the code more concise and readable using the proposed solution? If so, I can submit a new PR. Thank you!",hi thank much update issue however accepted fix key string splicing condition true tedious contrast solution propose replace method straightforward one line code think would make code concise readable solution submit new thank,issue,positive,positive,neutral,neutral,positive,positive
1981518675,"Hi @naity, thanks for the clean issue report. This should be fixed in https://github.com/ray-project/ray/pull/43594, which is available in nightly wheels and will be available in the upcoming Ray 2.10 release.",hi thanks clean issue report fixed available nightly available upcoming ray release,issue,positive,positive,positive,positive,positive,positive
1981509367,"@edoakes Merge conflict fixed, tests passing",merge conflict fixed passing,issue,negative,positive,neutral,neutral,positive,positive
1981492422,We should be good now. If it's flaky again automation will tell us. Thanks both!,good flaky tell u thanks,issue,positive,positive,positive,positive,positive,positive
1981474564,"> Is this specific to windows?

Just checked - nope I also get this on my linux machine.

First process:
```
username@host:~# python
Python 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import ray
>>> ray.init()
>>> exit()
username@host:~#
```

Second process:
```
username@host:~# python
Python 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import ray
>>> ray.init(address='auto')
>>> gcs_rpc_client.h:552: Failed to
connect to GCS within 60 seconds. GCS may have been killed. It's either GCS is
terminated by `ray stop` or is killed unexpectedly. If it is killed unexpectedly, see
the log file gcs_server.out. https://docs.ray.io/en/master/ray-observability/ray-
logging.html#logging-directory-structure. The program will terminate.
username@host:~#
```",specific checked nope also get machine first process host python python main type help copyright license information import ray exit host second process host python python main type help copyright license information import ray connect within may either ray stop unexpectedly unexpectedly see log file program terminate host,issue,negative,positive,positive,positive,positive,positive
1981465845,"I ran the script with (num_cpus=1, num_gpus=0) and was not able to reproduce:
```
(remote_task pid=8024) 2024-03-06 00:31:12.447930: Task going to sleep...
(remote_task pid=8024) 2024-03-06 00:31:22.454542: Task going to sleep...
(remote_task pid=8024) 2024-03-06 00:31:32.463040: Task going to sleep...
(remote_task pid=8024) 2024-03-06 00:31:42.471519: Task going to sleep...
(remote_task pid=8024) 2024-03-06 00:31:52.480599: Task going to sleep...
Obtained remote result: 1709714301.033468
(remote_task pid=8024) Task finished sleeping for 7200 seconds -- returning.
```

I used Ray 2.9.3 and modified the script a little (for more debugging info):
```
import ray
import time
import datetime


@ray.remote
def remote_task(task_length: float) -> float:
    task_end_time = time.time() + task_length
    while True:
        if time.time() > task_end_time:
            break
        c = datetime.datetime.now()
        print(f""{c}: Task going to sleep..."")
        time.sleep(10)
    print(f""Task finished sleeping for {task_length} seconds -- returning."")
    return task_end_time


if __name__ == ""__main__"":
    task_length = datetime.timedelta(hours=2).seconds

    # The reason for using `.options` to specify the `num_cpus` and `num_gpus`
    # is that in the live application, we pass these values dynamically to the entrypoint.
    remote_func = remote_task.options(num_cpus=1, num_gpus=0)

    print(""Waiting on remote..."")
    result = ray.get(remote_func.remote(task_length))
    print(f""Obtained remote result: {result}"")
```
",ran script able reproduce task going sleep task going sleep task going sleep task going sleep task going sleep remote result task finished sleeping used ray script little import ray import time import float float true break print task going sleep print task finished sleeping return reason specify live application pas dynamically print waiting remote result print remote result result,issue,positive,positive,neutral,neutral,positive,positive
1981443840,"Closing. I see some windows tests are flaky on https://flaky-tests.ray.io/, but not `test_task_metrics`. Please reopen if I am mistaken.",see flaky please reopen mistaken,issue,negative,neutral,neutral,neutral,neutral,neutral
1981427959,"Can you check on latest ray 2.9.3, where redis-server is no longer required?",check latest ray longer,issue,negative,positive,positive,positive,positive,positive
1981421980,"Closing, please open a new issue if the problem re-appears",please open new issue problem,issue,negative,positive,neutral,neutral,positive,positive
1981405179,@anyscalesam It was closed before I had a chance to look. Most likely it's just flaky,closed chance look likely flaky,issue,negative,negative,neutral,neutral,negative,negative
1981401200,@GeneDer did we root cause why it failed or is it just flaky in general?,root cause flaky general,issue,negative,positive,neutral,neutral,positive,positive
1981258137,"Sorry for the long silence, I intend to add a test the next days.",sorry long silence intend add test next day,issue,negative,negative,negative,negative,negative,negative
1981156792,"> moving on, we will build nightly images with dates and commit digests in the tag name, like `nightly.240306.123abc`, every weekday night, but just not going to build a full set of images for _every single commit_ (old behavior). The old behavior is spamming the ray docker hub, and buries the stable releases in the deep of unstable per-commit builds.

Thanks for clarification @aslonnie ! ",moving build nightly commit tag name like nightly every weekday night going build full set single old behavior old behavior ray docker hub stable deep unstable thanks clarification,issue,positive,positive,positive,positive,positive,positive
1980533834,"fwiw, we will keep building per commit wheels from master branch (and save them on s3). that behavior is not changed (so far).",keep building per commit master branch save behavior far,issue,positive,positive,neutral,neutral,positive,positive
1980530589,"we will document the new behavior once the code is merged. we stop promising the old behavior (and it can break any time).

if there is a strong need, we (Anyscale) can consider providing a service for people to build and download images from an arbitrary recent commit.",document new behavior code stop promising old behavior break time strong need consider providing service people build arbitrary recent commit,issue,positive,positive,positive,positive,positive,positive
1980516067,"moving on, we will build nightly images with dates and commit digests in the tag name, like `nightly.240306.123abc`, every weekday night, but just not going to build a full set of images for *every single commit* (old behavior). The old behavior is spamming the ray docker hub, and buries the stable releases in the deep of unstable per-commit builds.",moving build nightly commit tag name like nightly every weekday night going build full set every single commit old behavior old behavior ray docker hub stable deep unstable,issue,positive,positive,neutral,neutral,positive,positive
1980493810,I think you can refer to this user guide: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html,think refer user guide,issue,negative,neutral,neutral,neutral,neutral,neutral
1980411420,"I have a dumb question: How are images from the master (not nightlies) identified? I understood images with a SHA are image builds from the master that - in contrast to a nightly - do not change. 

Use case: A user needs some hot fixes that are in the master and not the release, but wants to avoid to continuously adapt her own code to the master (which might change too quickly).",dumb question master understood sha image master contrast nightly change use case user need hot master release avoid continuously adapt code master might change quickly,issue,negative,positive,neutral,neutral,positive,positive
1980314984,"> > ```
> > OSError: When reading information for key '20G-image-data-synthetic-raw/dog_7520.jpg' in bucket 'air-example-data-2': AWS Error NETWORK_CONNECTION during HeadObject operation: curlCode: 28, Timeout was reached
> > ```
> > 
> > 
> >     
> >       
> >     
> > 
> >       
> >     
> > 
> >     
> >   
> > Hi @gaowayne , it seems to be a AWS s3 permission issue. Please ensure that you've properly set the aws credentials on all nodes.
> 
> thank you so much!~ could you please share me how to do this? and how to check?

also, I only have one node to do a simple test for now.
",reading information key bucket error operation hi permission issue please ensure properly set thank much could please share check also one node simple test,issue,positive,positive,neutral,neutral,positive,positive
1980307981,"@ruisearch42 also if I change a little code in C++, then I want to replace the module installed in my node, and do one round performance testing, can I just replace binary file? is there any guide for this?",also change little code want replace module node one round performance testing replace binary file guide,issue,negative,negative,negative,negative,negative,negative
1980306356,"@ruisearch42 buddy, I managed install bazel6.4, now previous error gone, I saw new error below
```
(base) [root@cassiopeia ray]# bazel build -c fastbuild //:*
Extracting Bazel installation...
Starting local Bazel server and connecting to it...
DEBUG: /mnt/nvme1n1/pfray/ray/bazel/ray_deps_setup.bzl:67:14: No implicit mirrors used because urls were explicitly provided
DEBUG: /mnt/nvme1n1/pfray/ray/bazel/ray_deps_setup.bzl:67:14: No implicit mirrors used because urls were explicitly provided
ERROR: /root/.cache/bazel/_bazel_root/86c0b2da2f7232f44660b8e2d43c00fd/external/bazel_tools/platforms/BUILD:89:6: in alias rule @bazel_tools//platforms:windows: Constraints from @bazel_tools//platforms have been removed. Please use constraints from @platforms repository embedded in Bazel, or preferably declare dependency on https://github.com/bazelbuild/platforms. See https://github.com/bazelbuild/bazel/issues/8622 for details.
ERROR: /root/.cache/bazel/_bazel_root/86c0b2da2f7232f44660b8e2d43c00fd/external/bazel_tools/platforms/BUILD:89:6: Analysis of target '@bazel_tools//platforms:windows' failed
ERROR: /root/.cache/bazel/_bazel_root/86c0b2da2f7232f44660b8e2d43c00fd/external/com_github_google_flatbuffers/BUILD:74:10: While resolving toolchains for target @com_github_google_flatbuffers//:flatc: invalid registered toolchain '@bazel_skylib//toolchains/unittest:cmd_toolchain': 
ERROR: Analysis of target '//:ray/raylet/format/node_manager_generated.h' failed; build aborted: 
INFO: Elapsed time: 35.111s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (61 packages loaded, 1438 targets configured)
```",buddy install previous error gone saw new error base root ray build installation starting local server implicit used explicitly provided implicit used explicitly provided error alias rule removed please use repository preferably declare dependency see error analysis target error target invalid registered error analysis target build aborted time build complete successfully loaded,issue,negative,positive,neutral,neutral,positive,positive
1980301309,"LGTM.
We have reached a consensus offline and now we will unify the approach using the extension of the `opencensus::stats::StatsExporter::Handler` interface.

",consensus unify approach extension interface,issue,negative,neutral,neutral,neutral,neutral,neutral
1980253337,"> @gaowayne The error message complains the bazel version is too low:
> 
> ```
> Error in fail: Current Bazel version is 3.5.1- (@non-git); expected at least 5.4.1
> ```
> 
> Can you double check the bazel is installed from the official script ci/env/install-bazel.sh , or does it already exist on your system?

the enviroment is reinstalled. I will check again. 
now I use FedoraOS to build I met below error to install python depends
```
(myenv) [root@cassiopeia ray]# pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-3.0.0.dev0-cp38-cp38-manylinux2014_x86_64.whl
ERROR: ray-3.0.0.dev0-cp38-cp38-manylinux2014_x86_64.whl is not a supported wheel on this platform.
```
(myenv) [root@cassiopeia ray]# cat /etc/release-os
cat: /etc/release-os: No such file or directory
(myenv) [root@cassiopeia ray]# cat /etc/os-release
NAME=""Fedora Linux""
VERSION=""37 (Server Edition)""
ID=fedora
VERSION_ID=37
VERSION_CODENAME=""""
PLATFORM_ID=""platform:f37""
PRETTY_NAME=""Fedora Linux 37 (Server Edition)""
ANSI_COLOR=""0;38;2;60;110;180""
LOGO=fedora-logo-icon
CPE_NAME=""cpe:/o:fedoraproject:fedora:37""
HOME_URL=""https://fedoraproject.org/""
DOCUMENTATION_URL=""https://docs.fedoraproject.org/en-US/fedora/f37/system-administrators-guide/""
SUPPORT_URL=""https://ask.fedoraproject.org/""
BUG_REPORT_URL=""https://bugzilla.redhat.com/""
REDHAT_BUGZILLA_PRODUCT=""Fedora""
REDHAT_BUGZILLA_PRODUCT_VERSION=37
REDHAT_SUPPORT_PRODUCT=""Fedora""
REDHAT_SUPPORT_PRODUCT_VERSION=37
SUPPORT_END=2023-12-05
VARIANT=""Server Edition""
VARIANT_ID=server
",error message version low error fail current version least double check official script already exist system check use build met error install python root ray pip install error wheel platform root ray cat cat file directory root ray cat server edition platform server edition server edition,issue,negative,negative,negative,negative,negative,negative
1980157333,"## default behavior
<img width=""1295"" alt=""image"" src=""https://github.com/ray-project/ray/assets/38552291/44a93837-0cd6-477b-a809-c5c4f6c29ab5"">


## with this PR
<img width=""1361"" alt=""image"" src=""https://github.com/ray-project/ray/assets/38552291/ba052b73-7b18-4d69-b7f1-25a0b8e171c9"">
",default behavior image image,issue,negative,neutral,neutral,neutral,neutral,neutral
1980139298,any update here? is this going to be part of the 2.10 release? cc @rkooo567 ,update going part release,issue,negative,neutral,neutral,neutral,neutral,neutral
1980076773,"@stephanie-wang is this a data issue? i think the function in the reproducible has the name `ray_data`, but does not use ray data part of the library",data issue think function reproducible name use ray data part library,issue,negative,neutral,neutral,neutral,neutral,neutral
1979961982,"@gaowayne The error message complains the bazel version is too low:

```
Error in fail: Current Bazel version is 3.5.1- (@non-git); expected at least 5.4.1
```

Can you double check the bazel is installed from the official script ci/env/install-bazel.sh , or does it already exist on your system?",error message version low error fail current version least double check official script already exist system,issue,negative,negative,negative,negative,negative,negative
1979941242,"> ```
> OSError: When reading information for key '20G-image-data-synthetic-raw/dog_7520.jpg' in bucket 'air-example-data-2': AWS Error NETWORK_CONNECTION during HeadObject operation: curlCode: 28, Timeout was reached
> ```
> 
> Hi @gaowayne , it seems to be a AWS s3 permission issue. Please ensure that you've properly set the aws credentials on all nodes.

thank you so much!~ could you please share me how to do this? and how to check? ",reading information key bucket error operation hi permission issue please ensure properly set thank much could please share check,issue,positive,positive,neutral,neutral,positive,positive
1979924603,"> oh wow good catch. how did you find it

i had a test that broke it but I think it's this test issue. ",oh wow good catch find test broke think test issue,issue,positive,positive,positive,positive,positive,positive
1979912177,"https://github.com/ray-project/ray/pull/43360 and https://github.com/ray-project/ray/pull/43735 cleans up a lot of the Ray Data logging to stdout. Full logs will still be written to the Ray Data log file.

You can try out the latest nightly, or wait for the upcoming Ray 2.10 release which will include this change. Please feel free to re-open this issue for further followups questions or requests!",lot ray data logging full still written ray data log file try latest nightly wait upcoming ray release include change please feel free issue,issue,positive,positive,positive,positive,positive,positive
1979894883,"```
OSError: When reading information for key '20G-image-data-synthetic-raw/dog_7520.jpg' in bucket 'air-example-data-2': AWS Error NETWORK_CONNECTION during HeadObject operation: curlCode: 28, Timeout was reached
```

Hi @gaowayne , it seems to be a AWS s3 permission issue. Please ensure that you've properly set the aws credentials on all nodes.",reading information key bucket error operation hi permission issue please ensure properly set,issue,negative,neutral,neutral,neutral,neutral,neutral
1979885843,"Hi @frazierprime, seems that you have a column with string datatype in the `ray_datasets `, which is not able to convert to torch tensor. ",hi column string able convert torch tensor,issue,negative,positive,positive,positive,positive,positive
1979882543,it is conditioned to the change. you would need to change some c++ code to trigger cpp tests.,conditioned change would need change code trigger,issue,negative,neutral,neutral,neutral,neutral,neutral
1979855763,"@justinvyu yeah, I've asked core team and they told we can use it as an additional resource in Ray Train. I'm trying to avoid combining and splitting accelerator_type_key back and forth with the current solution.",yeah core team told use additional resource ray train trying avoid combining splitting back forth current solution,issue,negative,neutral,neutral,neutral,neutral,neutral
1979797816,Test build: https://buildkite.com/ray-project/release-automation/builds/201 that shows the trigger step and its block,test build trigger step block,issue,negative,neutral,neutral,neutral,neutral,neutral
1979776002,"This is how it looks if we just use serve logger (I didn't change back to info, but can easily do that)
<img width=""1509"" alt=""image"" src=""https://github.com/ray-project/ray/assets/7553988/a7c6056b-f320-48ce-a021-961e10cd4dde"">
",use serve logger change back easily image,issue,negative,positive,positive,positive,positive,positive
1979773039,"Oh I see @GeneDer. What does it look like if we use the serve logger? I think a lot of the fields we usually set there aren't applicable on the driver...

Maybe we should have an API logger (could maybe just pull in the `cli_logger` that's used in the `CLI` code).",oh see look like use serve logger think lot usually set applicable driver maybe logger could maybe pull used code,issue,negative,negative,negative,negative,negative,negative
1979772167,"> > @edoakes this is how it looks now. It's no longer green and doesn't have the file name and line number. Also, noticed this is not using `ray.serve` logger. Do you think it makes sense to reuse serve logger for this entire file? <img alt=""image"" width=""1520"" src=""https://private-user-images.githubusercontent.com/7553988/310291209-ec8c3c82-6ff4-4707-8744-cb478c352291.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDk2NzkyODIsIm5iZiI6MTcwOTY3ODk4MiwicGF0aCI6Ii83NTUzOTg4LzMxMDI5MTIwOS1lYzhjM2M4Mi02ZmY0LTQ3MDctODc0NC1jYjQ3OGMzNTIyOTEucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDMwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDAzMDVUMjI0OTQyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZDNlMWQ2MmEwNmVjZDU5NjdjNjc3ZDI3YmVjN2Y3YTI3NjAwZjg4MTBkYjkwMWVkNTFkODE5ZmMwY2Q4NTBkYSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.-bELiy6s3Vkw1lDTrWxAksBQR2_D_aoIok7oEF4FqFM"">
> 
> Do you know why it doesn't have any log line formatting like `INFO api.py ...` even though you're using the logger? That part looks out of place.

Yes, bc it's not using serve's logger, it's doing `logger = logging.getLogger(__file__)`😅",longer green file name line number also logger think sense reuse serve logger entire file image know log line like even though logger part place yes serve logger logger,issue,positive,negative,neutral,neutral,negative,negative
1979770295,"> @edoakes this is how it looks now. It's no longer green and doesn't have the file name and line number. Also, noticed this is not using `ray.serve` logger. Do you think it makes sense to reuse serve logger for this entire file? <img alt=""image"" width=""1520"" src=""https://private-user-images.githubusercontent.com/7553988/310291209-ec8c3c82-6ff4-4707-8744-cb478c352291.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDk2NzkyODIsIm5iZiI6MTcwOTY3ODk4MiwicGF0aCI6Ii83NTUzOTg4LzMxMDI5MTIwOS1lYzhjM2M4Mi02ZmY0LTQ3MDctODc0NC1jYjQ3OGMzNTIyOTEucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDMwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDAzMDVUMjI0OTQyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZDNlMWQ2MmEwNmVjZDU5NjdjNjc3ZDI3YmVjN2Y3YTI3NjAwZjg4MTBkYjkwMWVkNTFkODE5ZmMwY2Q4NTBkYSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.-bELiy6s3Vkw1lDTrWxAksBQR2_D_aoIok7oEF4FqFM"">

Do you know why it doesn't have any log line formatting like `INFO api.py ...` even though you're using the logger? That part looks out of place.",longer green file name line number also logger think sense reuse serve logger entire file image know log line like even though logger part place,issue,negative,negative,neutral,neutral,negative,negative
1979729464,Done! This code path is test covered too.,done code path test covered,issue,negative,neutral,neutral,neutral,neutral,neutral
1979705640,"@edoakes this is how it looks now. It's no longer green and doesn't have the file name and line number. Also, noticed this is not using `ray.serve` logger. Do you think it makes sense to reuse serve logger for this entire file? 
<img width=""1520"" alt=""image"" src=""https://github.com/ray-project/ray/assets/7553988/ec8c3c82-6ff4-4707-8744-cb478c352291"">
",longer green file name line number also logger think sense reuse serve logger entire file image,issue,negative,negative,neutral,neutral,negative,negative
1979704590,@can-anyscale; confirmed not release-blocker for ray210 but need-to-fix for ray weekly releases,confirmed ray ray weekly,issue,negative,positive,positive,positive,positive,positive
1979689752,"> > @GeneDer I don't really want to add a callback to the public API (to reduce the API surface to maintain). It's probably useful to log this when folks call `serve.run` too, how about we just add a logging statement there by default?
> 
> Partial bc there are 2 separate messages that we log ""Deployed app successfully."" and ""Redeployed app successfully."" and also the cli logger works different than our serve logger with some custom formatting and stuff. But yea let me simply by just always log a ""Deployed app successfully."" and drop the callback

Got it, the original motivation makes sense. Let's just simplify and go w/ the single message.",really want add public reduce surface maintain probably useful log call add logging statement default partial separate log successfully successfully also logger work different serve logger custom stuff yea let simply always log successfully drop got original motivation sense let simplify go single message,issue,positive,positive,positive,positive,positive,positive
1979687096,"> @GeneDer I don't really want to add a callback to the public API (to reduce the API surface to maintain). It's probably useful to log this when folks call `serve.run` too, how about we just add a logging statement there by default?

Partial bc there are 2 separate messages that we log ""Deployed app successfully."" and ""Redeployed app successfully."" and also the cli logger works different than our serve logger with some custom formatting and stuff. But yea let me simply by just always log a ""Deployed app successfully."" and drop the callback
",really want add public reduce surface maintain probably useful log call add logging statement default partial separate log successfully successfully also logger work different serve logger custom stuff yea let simply always log successfully drop,issue,positive,positive,positive,positive,positive,positive
1979680373,"@GeneDer I don't really want to add a callback to the public API (to reduce the API surface to maintain). It's probably useful to log this when folks call `serve.run` too, how about we just add a logging statement there by default?",really want add public reduce surface maintain probably useful log call add logging statement default,issue,positive,positive,positive,positive,positive,positive
1979636512,"Also tested manually seeing those new fields are now logged
<img width=""1728"" alt=""image"" src=""https://github.com/ray-project/ray/assets/7553988/bfcac10e-9b9c-43ad-9254-d65b4502b2a8"">
",also tested manually seeing new logged image,issue,negative,positive,positive,positive,positive,positive
1979540357,"> Yes, I am testing in a multi-node cluster. Must've missed it in the docs, but how can I ensure that files are accessible to all worker nodes?

In general, you use a distributed filesystem like NFS (or cloud storage if you're reading files directly). For Ray Client, though, I'm not sure if I have any concrete recommendations.

> I am running into a similar issue with this mechanism where I am able to push a local module using the runtime environment, but when the code gets executed on the workers, I get module not found errors.

That's weird. If you specify modules in your runtime environment, I'd expect them to be available on your worker nodes. @rynewang do you have any hunches about what might be happening?",yes testing cluster must ensure accessible worker general use distributed like cloud storage reading directly ray client though sure concrete running similar issue mechanism able push local module environment code executed get module found weird specify environment expect available worker might happening,issue,positive,positive,positive,positive,positive,positive
1979538589,"My understanding is that it's not explicitly deprecated, but we haven't maintained it for a very long time.",understanding explicitly long time,issue,negative,negative,neutral,neutral,negative,negative
1979512936,"yes, we are investigating as a release blocker.",yes investigating release blocker,issue,negative,neutral,neutral,neutral,neutral,neutral
1979498634,Pending permission to access secrets from branch queue IAM role,pending permission access branch queue role,issue,negative,neutral,neutral,neutral,neutral,neutral
1979481490,@bveeramani - relating to your comment about Ray Client; has this been deprecated? We're using Ray version 2.7.0 in our remote clusters due to this particular issue (I am able to run jobs successfully with the older version).,comment ray client ray version remote due particular issue able run successfully older version,issue,negative,positive,positive,positive,positive,positive
1979461972,"@anyscalesam Yes, I think it should be a release blocker. I will take a look today",yes think release blocker take look today,issue,negative,neutral,neutral,neutral,neutral,neutral
1979460129,"Also tested manually and seeing ""Deployed app successfully."" is logged again
<img width=""1156"" alt=""image"" src=""https://github.com/ray-project/ray/assets/7553988/a99e8035-b4f8-44d1-8247-6effd34ce157"">
",also tested manually seeing successfully logged image,issue,negative,positive,positive,positive,positive,positive
1979398501,@c21 should this be considered a ray 2.10 release blocker?,considered ray release blocker,issue,negative,neutral,neutral,neutral,neutral,neutral
1979396315,@GeneDer @shrekris-anyscale - failing release test - can y'all please take a look and make a decision on whether this should be a ray 2.10 release blocker?,failing release test please take look make decision whether ray release blocker,issue,negative,neutral,neutral,neutral,neutral,neutral
1979383771,"@architkulkarni , can you take a look at this one after your current task?",take look one current task,issue,negative,neutral,neutral,neutral,neutral,neutral
1979374867,"Here's a simple repo:

1. Start ray with some custom resources:
```
ray start --head --resources=""{\""special_hardware\"":1, \""custom_label\"":1}""
```


2. Run the below script
```
import ray

@ray.remote
def f():
    import time
    time.sleep(9999)

f.options(resources={""special_hardware"": 0.0001}).remote()
f.options(resources={""special_hardware"": 0.00999999999}).remote()

```


3. Run `ray status` in another terminal
```
======== Autoscaler status: 2024-03-05 18:19:37.883731 ========
Node status
---------------------------------------------------------------
Active:
 1 node_9d771b3ce9aab26c7989bf68b064ea0f8fbd86994b2db4bd33c89928
Pending:
 (no pending nodes)
Recent failures:
 (no failures)

Resources
---------------------------------------------------------------
Usage:
 2.0/32.0 CPU
 0.0/1.0 custom_label
 0B/33.46GiB memory
 0B/16.73GiB object_store_memory
 0.010000000000000009/1.0 special_hardware

Demands:
 (no resource demands)

```
",simple start ray custom ray start head run script import ray import time run ray status another terminal status node status active pending pending recent usage memory resource,issue,negative,negative,neutral,neutral,negative,negative
1979249120,"Hey, I really like this idea! I hope this can be rolled into the core API soon :)

I am willing to work on a PR for this if you would like... but my guess is that it would probably be best to wait until the new API stack is finished.",hey really like idea hope rolled core soon willing work would like guess would probably best wait new stack finished,issue,positive,positive,positive,positive,positive,positive
1979155392,"I was looking for some return curves on CartPole-v1 thanks! If that's helpful to you I'm getting similar performances from [my implementation](https://github.com/Armandpl/minidream) (~200k steps to convergence, 50 min wall clock time on an rtx 3090, [wandb run here](https://wandb.ai/armandpl/minidream_dev/runs/jnsk7vn1?workspace=user-armandpl)) 
![W B Chart 3_5_2024, 5_19_33 PM](https://github.com/ray-project/ray/assets/14967758/403e01be-54f5-4df3-9a0c-093d29093050)
",looking return thanks helpful getting similar implementation convergence min wall clock time run chart,issue,positive,positive,neutral,neutral,positive,positive
1978715595,"> Hey @frazierprime, are you running this program on a multi-node cluster? If the file exists on the head node and you create the dataset in a remote task, then the remote task might be scheduled on a worker node and the file will be inaccessible.
> 
> Re: the global node not initialized thing -- we don't really support Ray Client. If possible, I'd recommend not using it.

Yes, I am testing in a multi-node cluster. Must've missed it in the docs, but how can I ensure that files are accessible to all worker nodes? 

Same question for local packages that I have pushed in with my runtime environment? I am running into a similar issue with this mechanism where I am able to push a local module using the runtime environment, but when the code gets executed on the workers, I get module not found errors.",hey running program cluster file head node create remote task remote task might worker node file inaccessible global node thing really support ray client possible recommend yes testing cluster must ensure accessible worker question local environment running similar issue mechanism able push local module environment code executed get module found,issue,positive,positive,neutral,neutral,positive,positive
1978466591,Triggered a build: https://buildkite.com/ray-project/release-automation/builds/191 and it worked with dependency as expected,triggered build worked dependency,issue,negative,neutral,neutral,neutral,neutral,neutral
1978363640,"My team uses Ray with Jupyter notebooks and this issue is a big problem in this use case. 

The official documentation says that [you can configure the logging level of different ray components](https://docs.ray.io/en/latest/ray-observability/user-guides/configure-logging.html#using-rays-logger), but that doesn't seem to work. `ray.init(..., logging_level=logging.ERROR)` doesn't work either.

![image](https://github.com/ray-project/ray/assets/69429/366cd062-6c6a-4df6-a294-f0b5e6fe733d)

We use devcontainers so workarounds like monkey-patching or deleting ray logging lines aren't great.",team ray issue big problem use case official documentation configure logging level different ray seem work work either image use like ray logging great,issue,positive,positive,positive,positive,positive,positive
1978259843,"Test: a successful run was here: https://buildkite.com/ray-project/premerge/builds/21106#018e0d50-8370-4c00-9c69-f27a17fd0cc1

Current branch premerge would fail since it depends on a few other PRs in-flight. ",test successful run current branch would fail since,issue,negative,positive,neutral,neutral,positive,positive
1978211187,"`train_multinode_persistence` release test passed here: https://buildkite.com/ray-project/release/builds/10131#018e0d55-eb73-42aa-abe9-e3ba603a61f5

I removed the `tune_cloud_durable_upload` test because it's basically a duplicate of the test above, but also relies on ""local directory"" implementation details. I need to add back a tune multi-node persistence test as a follow-up.",release test removed test basically duplicate test also local directory implementation need add back tune persistence test,issue,negative,neutral,neutral,neutral,neutral,neutral
1978056447,"This can be closed, the issue does not persist in Python >= 3.10",closed issue persist python,issue,negative,negative,neutral,neutral,negative,negative
1977990269,"@jjyao  There is no related issue. It is just some improvement that I think of as I read the code.

Do you want me to create an issue or do you only want PR that has an explicit issue?",related issue improvement think read code want create issue want explicit issue,issue,positive,neutral,neutral,neutral,neutral,neutral
1977934882,LG on my end - leaving it to @larrylian if we wanna do this. ,end leaving wan na,issue,negative,negative,negative,negative,negative,negative
1977853363,Could you update the PR description: what's the issue and what's the fix? You can also link the GH issue under `Related issue number`,could update description issue fix also link issue related issue number,issue,negative,neutral,neutral,neutral,neutral,neutral
1977845864,"@aslonnie  @can-anyscale  I updated this PR with new implementation that only checks for `os.environ[""RAYCI_SCHEDULE""]` when `upload` flag is one because that's the only time it matters (whether we tag and push nightly tags or not).",new implementation flag one time whether tag push nightly,issue,negative,positive,positive,positive,positive,positive
1977830835,"For now, `ray.train.get_context().get_storage().storage_fs_path` is the way to access that path, but we're working to expose this in a better way. (Right now it's through an unstable developer API.)",way access path working expose better way right unstable developer,issue,negative,positive,positive,positive,positive,positive
1977794470,"Hey @frazierprime, are you running this program on a multi-node cluster? If the file exists on the head node and you create the dataset in a remote task, then the remote task might be scheduled on a worker node and the file will be inaccessible.

Re: the global node not initialized thing -- we don't really support Ray Client. If possible, I'd recommend not using it.",hey running program cluster file head node create remote task remote task might worker node file inaccessible global node thing really support ray client possible recommend,issue,positive,neutral,neutral,neutral,neutral,neutral
1977785292,Hi @HAPAVAN22 Can you share the full stack trace with us? Also it'd be better to have a reproducible script.,hi share full stack trace u also better reproducible script,issue,positive,positive,positive,positive,positive,positive
1977780074,"The time inversion comes from the core worker's retry:

1. core worker: wants to schedule a task in `CoreWorkerDirectTaskSubmitter::RequestNewWorkerIfNeeded`
2. raylet: received a schedule request, put it to infeasible list
3. core worker: shutdown
4. raylet: `ClusterTaskManager` cancels the task from the worker
5. core worker: task cancelled, retry by calling the function again `CoreWorkerDirectTaskSubmitter::RequestNewWorkerIfNeeded`
6. raylet: received a schedule request, put it to infeasible list

and after this time series, core worker is gone, while the raylet gets a task that is never cancelled.

I made a PR that fixes this by changing step 5: on core worker shutdown, the `CoreWorkerDirectTaskSubmitter` now checks a boolean and to not retry if already shutdown.",time inversion come core worker retry core worker schedule task raylet received schedule request put infeasible list core worker shutdown raylet task worker core worker task retry calling function raylet received schedule request put infeasible list time series core worker gone raylet task never made step core worker shutdown retry already shutdown,issue,negative,neutral,neutral,neutral,neutral,neutral
1977741829,"> The Time Series and Reinforcement Learning Use Cases filters return an empty list. We should remove those in a follow-up PR.

We can do that, although there are examples from the other libraries that will get filtered by these, so maybe it just makes sense to update the other libraries to follow the same pattern as Data, Train, and Serve?

> Also, the first link in the Train Examples page under the advanced section is still broken. Could you do a check of all the links in the example pages, please?

All links tested working.",time series reinforcement learning use return empty list remove although get maybe sense update follow pattern data train serve also first link train page advanced section still broken could check link example please link tested working,issue,negative,positive,neutral,neutral,positive,positive
1977701396,"In https://github.com/ray-project/ray/pull/31175 we cancel all tasks called from a worker if the worker's dead. However here we have a time inversion:

```
[2024-03-04 23:13:30,165 D 1344915 1344915] (raylet) node_manager.cc:1018: Worker 01000000ffffffffffffffffffffffffffffffffffffffffffffffff failed
[2024-03-04 23:13:30,165 D 1344915 1344915] (raylet) worker_pool.cc:1309: PrestartWorkers, num_available_cpus 4 backlog_size 1 task spec Type=NORMAL_TASK, Language=PYTHON, Resources: {CPU: 1000, }, function_descriptor={type=PythonFunctionDescriptor, module_name=repro2, class_name=, function_name=cant_schedule, function_hash=415bafb0af364c0f817c28dd2e89f438}, task_id=d7575ad6000bfed245fbbe91537db73efc5fca8701000000, task_name=cant_schedule, job_id=01000000, num_args=2, num_returns=1, max_retries=3, depth=1, attempt_number=0 has runtime env 0
[2024-03-04 23:13:30,165 D 1344915 1344915] (raylet) cluster_task_manager.cc:50: Queuing and scheduling task d7575ad6000bfed245fbbe91537db73efc5fca8701000000
[2024-03-04 23:13:30,165 D 1344915 1344915] (raylet) cluster_task_manager.cc:146: Scheduling pending task d7575ad6000bfed245fbbe91537db73efc5fca8701000000
```

The first line is when the worker's considered dead by raylet; second lines = trying to schedule, although the worker's already gone.",cancel worker worker dead however time inversion raylet worker raylet task spec raylet task raylet pending task first line worker considered dead raylet second trying schedule although worker already gone,issue,negative,negative,neutral,neutral,negative,negative
1977698787,"Raylet log, repeated even after all jobs died:

```
[2024-03-04 23:43:17,922 D 1344915 1344915] (raylet) cluster_task_manager.cc:240: Check if the infeasible task is schedulable in any node. task_id:d7575ad6000bfed245fbbe91537db73efc5fca8701000000
[2024-03-04 23:43:17,922 D 1344915 1344915] (raylet) cluster_resource_scheduler.cc:209: Scheduling decision. forcing spillback: 0. Best node: -1 NIL_ID, is infeasible: 1
[2024-03-04 23:43:17,922 D 1344915 1344915] (raylet) cluster_task_manager.cc:254: No feasible node found for task d7575ad6000bfed245fbbe91537db73efc5fca8701000000
```

This task is from a python-core-worker.*.log:

```
[2024-03-04 23:13:30,164 D 1345039 1345096] direct_task_transport.cc:405: Requesting lease from raylet 96e2083b646ce2ea81d2a0467c20fe61cbc534320d3c3e80671b2502 for task d7575ad6000bfed245fbbe91537db73efc5fca8701000000
```

and write after this, the worker went down, and never printed a callback log.
",raylet log repeated even raylet check infeasible task node raylet decision forcing best node infeasible raylet feasible node found task task lease raylet task write worker went never printed log,issue,positive,positive,positive,positive,positive,positive
1977692074,"Removing as release blocker since it is not a regression for streaming generator. Ray Data has a workaround for now. The problem with the workaround is that for very long Data jobs, the workaround causes a slow memory leak. So we will try to get the fix in for 2.10 but it's not blocking the release.",removing release blocker since regression streaming generator ray data problem long data slow memory leak try get fix blocking release,issue,negative,negative,negative,negative,negative,negative
1977630828,"> Can we call out which of these users are likely to want to configure and which ones are advanced? The best way would be by grouping them, and if that's not possible, put an (advanced) next to the advanced ones.

@pcmoritz would you be opposed to us addressing this in a subsequent PR? I agree that the descriptions aren't very helpful right now. That said, before we invest time into improving the documentation, I was thinking we should go through the list of settings and remove unnecessary ones.

> The reference to retry_exceptions should be a link.

Fixed.

",call likely want configure advanced best way would grouping possible put advanced next advanced would opposed u subsequent agree helpful right said invest time improving documentation thinking go list remove unnecessary reference link fixed,issue,positive,positive,positive,positive,positive,positive
1977617117,We should test and make it clear that whether Ray docker works with Apple silicon.,test make clear whether ray docker work apple silicon,issue,negative,positive,positive,positive,positive,positive
1977614006,At least we should improve the error message.,least improve error message,issue,negative,negative,negative,negative,negative,negative
1977610382,Can you try the latest Ray. It should automatically retry if gcs is not ready yet.,try latest ray automatically retry ready yet,issue,negative,positive,positive,positive,positive,positive
1977606914,"> could you also remove [this](https://github.com/ray-project/ray/blob/ccf7e732adce14944cf9705c6e0404be4dd4e24c/python/ray/data/_internal/execution/operators/map_operator.py#L79-L83) in this PR? It's a data-level workaround for this issue

It would be better if we can keep the changes separate. This PR is risky as is.

Also, if there is a workaround in Data, should we still mark this as a release blocker? It's not complete yet and the amount of refactoring needed is going to be significant.",could also remove issue would better keep separate risky also data still mark release blocker complete yet amount going significant,issue,negative,positive,positive,positive,positive,positive
1977598668,"@shiyuc6688,

If you are using spot instance, then you should expect that it may fail and actors on it will fail as well and you need to handle this case: https://docs.ray.io/en/latest/ray-core/fault-tolerance.html",spot instance expect may fail fail well need handle case,issue,negative,negative,negative,negative,negative,negative
1977591793,@XavierGeerinck could you update the PR to include a test case?,could update include test case,issue,negative,neutral,neutral,neutral,neutral,neutral
1977581803,"Update: the issue happens even if there's no repeated runs. I tried to run loop: each loop starts a fresh ray cluster and run the script and see if it leaks. In round 5 it leaks.

repro2.sh
```
#!/bin/bash

# Initialize the attempt counter
attempt=1

while true; do
  # Print the attempt number
  echo ""This is attempt number $attempt""

  # Start the Ray head node
  ray start --head

  # Run your Python script
  python repro2.py

  # Check Ray status for a specific condition
  if ray status | grep ""1000""; then
    echo ""Finished""
    break  # Exit the loop
  else
    # Stop the Ray cluster if the condition is not met
    ray stop
  fi

  # Increment the attempt counter
  ((attempt++))
done
```

repro2.py
```
#!/usr/bin/env python3
import ray
import time

# Hypothesis: when restarting the code, it connected to the old client which caused issues.
ray.init(""ray://127.0.0.1:10001"")
# ray.init()


@ray.remote(num_cpus=1000)
def cant_schedule(x):
    return x

def main():
    ref = cant_schedule.remote(200)
    time.sleep(10)

 
if __name__ == ""__main__"": 
    main()
```",update issue even repeated tried run loop loop fresh ray cluster run script see round initialize attempt counter true print attempt number echo attempt number attempt start ray head node ray start head run python script python check ray status specific condition ray status echo finished break exit loop else stop ray cluster condition met ray stop fi increment attempt counter done python import ray import time hypothesis code connected old client ray return main ref main,issue,negative,positive,positive,positive,positive,positive
1977572020,@marwan116 can you wrap OpenAI Authentication Error and make it serializable/deserializable?,wrap authentication error make,issue,negative,neutral,neutral,neutral,neutral,neutral
1977566677,"I think `ray stop` doesn't handle this case from the very beginning.

> If I remember correctly, in the earlier version of ray, we did not have this issue

@wguocbp could you tell us which Ray version doesn't have the issue?


For now, I'll treat this as a feature request.",think ray stop handle case beginning remember correctly version ray issue could tell u ray version issue treat feature request,issue,negative,neutral,neutral,neutral,neutral,neutral
1977562930,"Can you change to

```
@ray.remote
def f():
    time.sleep(0.001)
    # Return node id.
    return ray.get_runtime_context().get_node_id()
```

Also you can try to increase `time.sleep(xxx)`. It's possible that the task is too short and they just reuse the worker processes on the head node.",change return node id return also try increase possible task short reuse worker head node,issue,negative,neutral,neutral,neutral,neutral,neutral
1977524218,"@larrylian Is `MetricPoint` a standard format? One issue I found with `MetricPoint` is that it only contains subset of the data of `opencensus::stats::ViewData` (i.e. when you convert `opencensus::stats::ViewData` to `MetricPoint`, some data is lost, especially for the `Distribution` data) and that's why `OpenCensusProtoExporter` doesn't inherit from `MetricExporterClient`. Given `opencensus::stats::ViewData` is the source-of-truth, I think every exporter should directly consume, process and export it.

Happy to chat offline on this.

",standard format one issue found subset data convert data lost especially distribution data inherit given think every exporter directly consume process export happy chat,issue,negative,positive,positive,positive,positive,positive
1977396934,All the error catching have been changed to just throwing error straight. @aslonnie  @can-anyscale ,error catching throwing error straight,issue,negative,positive,positive,positive,positive,positive
1977377275,"Hi @dabauxi , The reason why we did this is because Lightning previously have a `_LightningModuleWrapperBase`, which added the extra ""_forward_module."" prefix to the state_dict keys. Users need to manually trim it to correctly load the checkpoint.  https://github.com/Lightning-AI/pytorch-lightning/issues/16526 

In the recent versions, lightning trimmed the prefix internally so no need to do it ourselves.

Thanks for the fix! ",hi reason lightning previously added extra prefix need manually trim correctly load recent lightning prefix internally need thanks fix,issue,negative,positive,neutral,neutral,positive,positive
1977370871,"> @edoakes would you be able to merge in this or if docs has to pass too?

merged",would able merge pas,issue,negative,positive,positive,positive,positive,positive
1977329621,Update: Remove logic to check for .whl files since it's already done in https://github.com/ray-project/ray/pull/43579,update remove logic check since already done,issue,negative,neutral,neutral,neutral,neutral,neutral
1977268649,Added comments for the exit code 42. The exit code is intended behavior only for the bazel test run command.,added exit code exit code intended behavior test run command,issue,negative,neutral,neutral,neutral,neutral,neutral
1977238241,This is the same python 3.11-specific error we've been seeing in CI. Should be fixed by @edoakes PRs https://github.com/ray-project/ray/pull/43616 and https://github.com/ray-project/ray/pull/43674,python error seeing fixed,issue,negative,positive,neutral,neutral,positive,positive
1977237272,this will convert all into exit code 42 ? is that the intended behavior?,convert exit code intended behavior,issue,negative,neutral,neutral,neutral,neutral,neutral
1977204833,"Can we call out which of these users are likely to want to configure and which ones are advanced? The best way would be by grouping them, and if that's not possible, put an `(advanced)` next to the advanced ones.

Also some more detailed questions: What's the difference between `enable_progress_bars` and `use_ray_tqdm`? That's currently not clear from the docs.

For many of these, it is impossible for the users to know what they are doing, like `actor_prefetcher_enabled`, `pipeline_push_based_shuffle_reduce_tasks` (not documented), `enable_get_object_locations_for_metrics`, `write_file_retry_on_errors` (not documented)

The reference to `retry_exceptions` should be a link.",call likely want configure advanced best way would grouping possible put advanced next advanced also detailed difference currently clear many impossible know like reference link,issue,positive,positive,positive,positive,positive,positive
1977191095,"Ah yes, i changed the name of the tag to trigger python 3.11 version. Added and rerunning tests now.",ah yes name tag trigger python version added,issue,negative,neutral,neutral,neutral,neutral,neutral
1977155583,"> @sven1977 Not sure if it's related, I also noticed readthedocs complains about ""exception: no module named ray.rllib.Policy""

Yeah, this is unrelated. I'm talking to @angelinalg and @peytondmurray about this rn ...",sure related also exception module yeah unrelated talking,issue,positive,positive,positive,positive,positive,positive
1977149479,"@sven1977 Not sure if it's related, I also noticed readthedocs complains about ""exception: no module named ray.rllib.Policy""",sure related also exception module,issue,negative,positive,positive,positive,positive,positive
1977107534,The lint error in `premerge` and the `docs/readthedocs.com` error are caused by RLlib and have no relationship with this PR.,lint error error relationship,issue,negative,neutral,neutral,neutral,neutral,neutral
1977101261,"The lint error is related to RLLib and has no relationship with this PR. @architkulkarni or @angelinalg, would you mind merging this PR? Thanks
<img width=""1290"" alt=""Screen Shot 2024-03-04 at 9 24 37 AM"" src=""https://github.com/ray-project/ray/assets/20109646/301cfcdb-ed15-4d99-87b3-920a6ff827b7"">
",lint error related relationship would mind thanks screen shot,issue,negative,positive,neutral,neutral,positive,positive
1977091261,Would like to confirm this fixes the issue before merging but need to run python 3.11 tests against this PR somehow. @can-anyscale any pointers?,would like confirm issue need run python somehow,issue,negative,neutral,neutral,neutral,neutral,neutral
1977049190,"The Time Series and Reinforcement Learning Use Cases filters return an empty list. We should remove those in a follow-up PR. 

Also, the first link in the Train Examples page under the advanced section is still broken. Could you do a check of all the links in the example pages, please?",time series reinforcement learning use return empty list remove also first link train page advanced section still broken could check link example please,issue,negative,positive,neutral,neutral,positive,positive
1976634545,"> LGTM. Only two questions:
> 
> 1. Why do we deprecate via `@OldStack` the `BaseEnv` when the multi-agent wrapper still needs it?
> 2. We should probably expect a certain interface to be implemented such that each `env` we wrap here has a `close` method.

The MultiAgentWrapper is also only there b/c of BaseEnv.
Note also that OldAPIStack does not deprecate things yet. It merely marks that these classes belong to a set of APIs that we will no longer advance and develop and - at some point in the future - will be deprecated.",two deprecate via wrapper still need probably expect certain interface wrap close method also note also deprecate yet merely class belong set longer advance develop point future,issue,negative,negative,neutral,neutral,negative,negative
1975978737,"@jjyao 
Your current solution, I feel, would require other developers to write a lot more code. The original solution was to have `MetricPointExporter` extend `opencensus::stats::StatsExporter::Handler`, and then parse the `opencensus::stats::ViewData` data format into the `MetricPoint` format. Subsequently, other developers only need to implement the `MetricExporterClient`, responsible for reporting `MetricPoint` data to other time-series databases. If the `MetricPointExporter `is now removed, then developers will need to re-implement a similar functionality to `MetricPointExporter`.

Additionally, I want to mention the implementation of `OpenCensusProtoExporter`. I think it could totally inherit from `MetricExporterClient`, converting `MetricPoint` into `pc::ReportOCMetricsRequest` and then reporting it to the time-series database.",current solution feel would require write lot code original solution extend parse data format format subsequently need implement responsible data removed need similar functionality additionally want mention implementation think could totally inherit converting,issue,positive,positive,neutral,neutral,positive,positive
1975574697,"OK, thanks mattip, i'll look into it. If you know how to apply it to ray later, please contact me thanks.",thanks look know apply ray later please contact thanks,issue,positive,positive,positive,positive,positive,positive
1975557979,@rickyyx Could you take a look at this problem?,could take look problem,issue,negative,neutral,neutral,neutral,neutral,neutral
1975501239,The transmission under the tunnel network has a significant impact on performance. I have readjusted the network architecture so that it can be improved @ArturNiederfahrenhorst @anyscalesam ,transmission tunnel network significant impact performance network architecture,issue,negative,positive,positive,positive,positive,positive
1975386293,"Have ran into this issue trying to run remote functions inside TorchTrainer train_func
`TypeError: Remote functions cannot be called directly. Instead of running ... `

Resolved with the solution proposed in https://github.com/ray-project/ray/issues/32856#issuecomment-1446882245 but this also doesn't strike me as an amazing solution. 

",ran issue trying run remote inside remote directly instead running resolved solution also strike amazing solution,issue,positive,positive,positive,positive,positive,positive
1975184975,"The error is coming from pytorch [here](https://github.com/pytorch/pytorch/blob/b7f25226929e70187a9f36c393665abad0b25190/torch/csrc/distributed/c10d/socket.cpp#L873). Searching around for such errors in pytorch, I see https://github.com/pytorch/pytorch/issues/77523 with no solution, and https://github.com/pytorch/pytorch/issues/80638 that does have a solution but I am not sure how to apply it to ray.",error coming searching around see solution solution sure apply ray,issue,negative,positive,positive,positive,positive,positive
1975145017,"Why is there no python in your dependencies list? Maybe you got python via the app store, we have had reports that version is problematic.",python list maybe got python via store version problematic,issue,negative,neutral,neutral,neutral,neutral,neutral
1975000416,"yes. I prefer to ask users to workaround than adding dogpatches unless the issue is widely exists (and critical). We've actually suffered from some dogpatches we made in the past multiple times and had to revert at the end. And the fact that Python doesn't try fix this issue also makes me a little concern (there could be a reason why it is not fixed in their layer like there's an anti pattern) though for this part, we can do a bit more sesearch. 

",yes prefer ask unless issue widely critical actually made past multiple time revert end fact python try fix issue also little concern could reason fixed layer like anti pattern though part bit,issue,negative,negative,neutral,neutral,negative,negative
1974919056,cc @Yicheng-Lu-llll would you mind taking a look at this PR?,would mind taking look,issue,negative,neutral,neutral,neutral,neutral,neutral
1974833673,"I am getting this error while running custom env form examples. 
Please help me resolve.
ValueError: Env must be of one of the following supported types: BaseEnv, gym.Env, MultiAgentEnv, VectorEnv, RemoteBaseEnv, ExternalMultiAgentEnv, ExternalEnv, but instead is of type <class '__main__.SimpleCorridor'>.",getting error running custom form please help resolve must one following instead type class,issue,positive,neutral,neutral,neutral,neutral,neutral
1974820122,@edoakes @jjyao Still adding more tests but this is ready for an early first pass!,still ready early first pas,issue,negative,positive,positive,positive,positive,positive
1974785794,"Hi ipsec, getting the same results with rllib implementation, failing to converge. However, the autor's codebase is missing a vector input example (instead of 64x64 image) so having trouble testing cartpole there.

Edit: Got it working with their modified `example.py` with 


```
config = embodied.Config(dreamerv3.configs['defaults'])
config = config.update(dreamerv3.configs['small'])


'encoder.mlp_keys': '',
'decoder.mlp_keys': '',
'encoder.cnn_keys': '',
'decoder.cnn_keys': '',

env = from_gym.FromGym('CartPole-v1', obs_key='vector')
```

It gets to 500 reward at about step 8000-10000 and gets 500 a few times there, and quickly unlearns and then stagnates around 120-150 and never gets back to 500. Not so sure what's going on there.

Edit again: 
forgot to close it and it ran until 300k steps, it got better with time
![image](https://github.com/ray-project/ray/assets/15318348/bb614734-379a-4a45-b8dd-b5a5da3e3393)

still not sure why it decides to get bad from time to time, maybe because of the empty encoder and decoder...
",hi getting implementation failing converge however missing vector input example instead image trouble testing edit got working reward step time quickly around never back sure going edit forgot close ran got better time image still sure get bad time time maybe empty,issue,negative,positive,neutral,neutral,positive,positive
1974385073,"hmm this PR is getting larger now - let me know if you want me to break it down cc @jjyao 

UPDATE: split some of the refactoring into previous PRs. ",getting let know want break update split previous,issue,negative,negative,negative,negative,negative,negative
1974185744,"Confirmed `ray[serve]==2.9.3` works fine with `fastapi==0.110.0`. However,  `ray 2.9.3` has pinned `fastapi<=1.108.0` in the serve extras config in `setup.py`, which opens users up to https://nvd.nist.gov/vuln/detail/CVE-2024-24762.
Please also see:
https://github.com/ray-project/ray/commit/570932614301fa80b98635fbe97792a2e9608f98
",confirmed ray serve work fine however ray pinned serve please also see,issue,negative,positive,positive,positive,positive,positive
1974160830,"Closed by https://github.com/ray-project/ray/pull/42892, please merge in latest master and give it a try. If you continue to run into problems, feel free to re-open this issue.",closed please merge latest master give try continue run feel free issue,issue,positive,positive,positive,positive,positive,positive
1974160586,"Dupe of https://github.com/ray-project/ray/issues/42842, which was fixed by https://github.com/ray-project/ray/pull/42892.
Please try merging in latest master, and feel free to re-surface the discussion in the above issue if you continue to run into the problem. Thanks!",dupe fixed please try latest master feel free discussion issue continue run problem thanks,issue,positive,positive,positive,positive,positive,positive
1974104232,"Discussed with @scottsun94 there are three areas where we need to make the changes...

1. Update the Ray Dashboard > Clusters tooltip for Object Store Memory on the various types
2. Update the same for Grafana dashboard
3. Update the System Metrics documentation as well

",three need make update ray dashboard object store memory various update dashboard update system metric documentation well,issue,negative,neutral,neutral,neutral,neutral,neutral
1974045811,"> Is this reasonable we just ask for the workaround (creating an instance inside an actor instead of directly adding .remote)?

Do you mean something like below? If so, it makes sense to me.

```python
@ray.remote
class SyncDataCollector:
    def __init__(self):
        self.xxxx = xxxxx # something inherited from IterableDataset.
    def __iter__(self):
        # call self.xxxx
```",reasonable ask instance inside actor instead directly mean something like sense python class self something self call,issue,negative,negative,neutral,neutral,negative,negative
1974008106,please remove the label when it is ready to review again! I will take a look asap,please remove label ready review take look,issue,positive,positive,positive,positive,positive,positive
1973998343,"Btw, I am very comfortable merging this PR. I feel a bit hacky that we are monkey patching the method. The CI failure also makes me feel like this is not a good direction. 

Is this reasonable we just ask for the workaround (creating an instance inside an actor instead of directly adding .remote)? Wdyt @kevin85421 ? The issue was also originally p2",comfortable feel bit hacky monkey method failure also feel like good direction reasonable ask instance inside actor instead directly issue also originally,issue,positive,positive,positive,positive,positive,positive
1973993550,Feel free to merge after addressing last comments! ,feel free merge last,issue,positive,positive,positive,positive,positive,positive
1973965810,"> what's the issue being fixed here? not quite understanding

@edoakes Addition/subtraction was not being handled correctly for custom resources because I was mixing pure dicts and ray resource dicts (which has a nested `{""resources"": ...}` field). So I'm adding a `from_ray_resource_dict` which should get used almost all of the time instead of the normal constructor.",issue fixed quite understanding handled correctly custom pure ray resource field get used almost time instead normal constructor,issue,negative,positive,positive,positive,positive,positive
1973965586,@aslonnie Having a `RAYCI_ROUTINE` which default to `daytime` and updated to `night` on scheduled nightly runs sounds good. Can you help making that change on `rayci`? I can take on that too with some guidance.,default daytime night nightly good help making change take guidance,issue,positive,positive,positive,positive,positive,positive
1973833209,"@edoakes Addressed comments, please take another look when you get the chance!",please take another look get chance,issue,positive,neutral,neutral,neutral,neutral,neutral
1973811712,"Howdy again! Sorry for the delay on this. It's been a busy few months 😵‍💫

I'll aim to implement a fix for this during this upcoming week.",howdy sorry delay busy aim implement fix upcoming week,issue,negative,negative,negative,negative,negative,negative
1973796473,"@edoakes Thanks for those great suggestions! The log prefix are working now!!! 😄
<img width=""1639"" alt=""image"" src=""https://github.com/ray-project/ray/assets/7553988/d108be8b-ef9a-48c7-90d4-cfea06634f5d"">
",thanks great log prefix working image,issue,positive,positive,positive,positive,positive,positive
1973771496,"Do we have a fix for this?

we ran into similar error  ```RuntimeError: CUDA error: unspecified launch failure``` when using partial GPU workers. It also happens when we do tensors.to(cuda)",fix ran similar error error unspecified launch failure partial also,issue,negative,negative,negative,negative,negative,negative
1973751181,I can confirm that it works with some minor issues but we're then migrating to the new way.,confirm work minor new way,issue,negative,positive,neutral,neutral,positive,positive
1973740982,"Here is the cluster configuration if needed: 

```
cluster_name: debug_p3_instances_with_ray

upscaling_speed: 1.0
max_workers: 10
idle_timeout_minutes: 5

docker:
    image: ""rayproject/ray:2.9.1-py39-cu121""
    container_name: ""ray_container""
    pull_before_run: True
    run_options: 
        - --ulimit nofile=65536:65536

initialization_commands: []

provider:
    type: aws
    region: us-east-1
    availability_zone: us-east-1a, us-east-1b, us-east-1c, us-east-1d, us-east-1e, us-east-1e

auth:
    ssh_user: ubuntu

available_node_types:
    ray.head.default:
        resources: {}
        node_config:
            InstanceType: p3.2xlarge
            ImageId: ami-041855406987a648b
            BlockDeviceMappings:
                - DeviceName: /dev/sda1
                  Ebs:
                      VolumeSize: 140
                      VolumeType: gp3
                      Throughput: 500
    ray.worker.default:
        min_workers: 4
        max_workers: 10
        resources: {}
        node_config:
            InstanceType: p3.2xlarge
            ImageId: ami-041855406987a648b
            BlockDeviceMappings:
                - DeviceName: /dev/sda1
                  Ebs:
                      VolumeSize: 140
                      VolumeType: gp3
                      Throughput: 500

head_node_type: ray.head.default
file_mounts: {}
cluster_synced_files: []
file_mounts_sync_continuously: False

setup_commands:
    - pip3 install --upgrade pip
    - pip3 uninstall -y torch torchvision torchaudio
    - pip3 install torch torchvision torchaudio
    - pip3 install numpy
    - pip3 install torcheval 
    - pip3 install jaxtyping 
    - pip3 install beartype 
    - pip3 install omegaconf
    - pip3 install wandb
    - pip3 install tokenizers
    - pip3 install scikit-learn
    - pip3 install scipy 
    - pip3 install transformers 
    - pip3 install boto3
    - python -c ""import torch; print('Torch version - ', torch.__version__)""
    - python -c ""import torch; assert torch.cuda.is_available(), 'CUDA not available'""
head_setup_commands: []
worker_setup_commands: []
head_start_ray_commands:
    - ray stop
    - ray start --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml --dashboard-host=0.0.0.0
worker_start_ray_commands:
    - ray stop
    - ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076
```

This fails at the assert statement for `torch.cuda.is_available()`.",cluster configuration docker image true provider type region throughput throughput false pip install upgrade pip pip torch pip install torch pip install pip install pip install pip install pip install pip install pip install pip install pip install pip install pip install python import torch print version python import torch assert available ray stop ray start head ray stop ray start assert statement,issue,negative,positive,positive,positive,positive,positive
1973707475,"@peytondmurray The error in linkcheck was already fixed by https://github.com/ray-project/ray/pull/43500, just need to merge master.",error already fixed need merge master,issue,negative,positive,neutral,neutral,positive,positive
1973640914,"the old way still kind of works (or at least we did not intentionally break it), but we are not using it to build anything in the release process anymore.",old way still kind work least intentionally break build anything release process,issue,positive,positive,positive,positive,positive,positive
1973625314,"Ah I was referring to the non-kuberay Ray job submission docs actually, but it'll be great to have a link in the kuberay doc as well!",ah ray job submission actually great link doc well,issue,positive,positive,positive,positive,positive,positive
1973621057,"@architkulkarni I will add a link to the ""Next Steps"" section in #43590 after this PR is merged.",add link next section,issue,negative,neutral,neutral,neutral,neutral,neutral
1973413258,"> @edoakes I think I remember seeing an error about trying to use an `asyncio.Event` object from a different event loop/thread before, which is why I changed it to `asyncio.Event(loop=event_loop)`. I think since the `Router`, and therefore `RouterMetricsManager` and `MetricsPusher`, is initialized in the main thread, while the asyncio.Event needs to be used in the special router event loop?

Oh... if this is the case then we need to lazily initialize it. Let me update.",think remember seeing error trying use object different event think since router therefore main thread need used special router event loop oh case need lazily initialize let update,issue,negative,positive,neutral,neutral,positive,positive
1973410200,"@edoakes I think I remember seeing an error about trying to use an `asyncio.Event` object from a different event loop/thread before, which is why I changed it to `asyncio.Event(loop=event_loop)`. I think since the `Router`, and therefore `RouterMetricsManager` and `MetricsPusher`, is initialized in the main thread, while the asyncio.Event needs to be used in the special router event loop?",think remember seeing error trying use object different event think since router therefore main thread need used special router event loop,issue,negative,positive,positive,positive,positive,positive
1973339299,Still need to add a test,still need add test,issue,negative,neutral,neutral,neutral,neutral,neutral
1973335036,"> But if MetricPointExporter is deleted , exporter_to_use will not be used by OpenCensusProtoExporter

@larrylian `OpenCensusProtoExporter` doesn't use `exporter_to_use` (see your screenshot) even before this PR. `OpenCensusProtoExporter` directly implements the `opencensus::stats::StatsExporter::Handler` interface and has nothing to do with `MetricPointExporter`",used use see even directly interface nothing,issue,negative,positive,neutral,neutral,positive,positive
1973286287,@GeneDer looks pretty solid. One more thing I think we can address is to fix the `logging_utils.py` as the source of the log line. I think you should be able to rewind the stack trace in order to get the actual line of code that initiated the write.,pretty solid one thing think address fix source log line think able rewind stack trace order get actual line code write,issue,positive,positive,positive,positive,positive,positive
1972994403,"@rynewang the workers are indeed never dead. From my understanding this is intended and the workers should stay alive for the entire duration of the workload.
The MRE that I posted is not problematic if executed as is because the workers (and therefore also the zombie children) will be killed at the end of the workload.
However, the actual problem arises if tasks need to run after this on the same cluster. In extreme cases, this can lead to OOM because all the memory will be occupied by the zombie processes.

```
import os

import ray
import psutil
from multiprocessing import Process
import time

@ray.remote
def start_subprocess():
    print(f""start_subprocess pid: {os.getpid()}"")
    p = Process(target=lambda: None)
    p.start()
    print(f""subprocess: {p.pid}"")
    # not joining the process will create a zombie process
    # p.join()

refs = [start_subprocess.remote() for _ in range(10)]
ray.wait(refs)

# wait, otherwise processes won't be visible in psutil yet
time.sleep(10)

for p in psutil.process_iter():
    if ""ray::"" in p.name() and p.status() == ""zombie"":
        print(f""zombie: {p}, ppid: {p.ppid()}"")
    if ""ray::"" in p.name() and p.status() != ""zombie"":
        print(f""not zombie: {p}"")

# some more tasks
# these tasks will have less memory available as the zombie processes are still consuming memory
# function_x.remote()
# function_y.remote()
# ...
```

```
(start_subprocess pid=4062894) start_subprocess pid: 4062894
(start_subprocess pid=4062894) subprocess: 4063113
(start_subprocess pid=4062897) start_subprocess pid: 4062897
(start_subprocess pid=4062897) subprocess: 4063114
(start_subprocess pid=4062901) start_subprocess pid: 4062901
(start_subprocess pid=4062901) subprocess: 4063112
(start_subprocess pid=4062898) start_subprocess pid: 4062898
(start_subprocess pid=4062898) subprocess: 4063111
(start_subprocess pid=4062900) start_subprocess pid: 4062900
(start_subprocess pid=4062900) subprocess: 4063109
(start_subprocess pid=4062896) start_subprocess pid: 4062896
(start_subprocess pid=4062896) subprocess: 4063110
(start_subprocess pid=4062899) start_subprocess pid: 4062899
(start_subprocess pid=4062899) subprocess: 4063108
(start_subprocess pid=4062895) start_subprocess pid: 4062895
(start_subprocess pid=4062895) subprocess: 4063115
(start_subprocess pid=4062899) start_subprocess pid: 4062899
(start_subprocess pid=4062899) subprocess: 4063131
(start_subprocess pid=4062899) start_subprocess pid: 4062899
(start_subprocess pid=4062899) subprocess: 4063133
not zombie: psutil.Process(pid=4062894, name='ray::IDLE', status='sleeping', started='10:57:05')
not zombie: psutil.Process(pid=4062895, name='ray::IDLE', status='sleeping', started='10:57:05')
not zombie: psutil.Process(pid=4062896, name='ray::IDLE', status='sleeping', started='10:57:05')
not zombie: psutil.Process(pid=4062897, name='ray::IDLE', status='sleeping', started='10:57:05')
not zombie: psutil.Process(pid=4062898, name='ray::IDLE', status='sleeping', started='10:57:05')
not zombie: psutil.Process(pid=4062899, name='ray::IDLE', status='sleeping', started='10:57:05')
not zombie: psutil.Process(pid=4062900, name='ray::IDLE', status='sleeping', started='10:57:05')
not zombie: psutil.Process(pid=4062901, name='ray::IDLE', status='sleeping', started='10:57:05')
zombie: psutil.Process(pid=4063109, name='ray::start_subp', status='zombie', started='10:57:06'), ppid: 4062900
zombie: psutil.Process(pid=4063110, name='ray::start_subp', status='zombie', started='10:57:06'), ppid: 4062896
zombie: psutil.Process(pid=4063111, name='ray::start_subp', status='zombie', started='10:57:06'), ppid: 4062898
zombie: psutil.Process(pid=4063112, name='ray::start_subp', status='zombie', started='10:57:06'), ppid: 4062901
zombie: psutil.Process(pid=4063113, name='ray::start_subp', status='zombie', started='10:57:06'), ppid: 4062894
zombie: psutil.Process(pid=4063114, name='ray::start_subp', status='zombie', started='10:57:06'), ppid: 4062897
zombie: psutil.Process(pid=4063115, name='ray::start_subp', status='zombie', started='10:57:06'), ppid: 4062895
zombie: psutil.Process(pid=4063131, name='ray::start_subp', status='zombie', started='10:57:06'), ppid: 4062899
zombie: psutil.Process(pid=4063133, name='ray::start_subp', status='zombie', started='10:57:06'), ppid: 4062899
```

",indeed never dead understanding intended stay alive entire duration posted problematic executed therefore also zombie end however actual problem need run cluster extreme lead memory zombie import o import ray import import process import time print process none print joining process create zombie process range wait otherwise wo visible yet ray zombie print zombie ray zombie print zombie le memory available zombie still consuming memory zombie zombie zombie zombie zombie zombie zombie zombie zombie zombie zombie zombie zombie zombie zombie zombie zombie,issue,negative,positive,neutral,neutral,positive,positive
1972638882,"Clarification on my earlier comment -- by cycle time, I mean the time in seconds it takes for a task to produce an output. For example, a map operator that performs inference might take a few seconds to produce each output.

This metric can help you understand where the bottleneck is.

We don't currently track it. ",clarification comment cycle time mean time task produce output example map operator inference might take produce output metric help understand bottleneck currently track,issue,negative,negative,negative,negative,negative,negative
1972596629,"@larrylian Got it.

I think the customization interface should be `opencensus::stats::StatsExporter::Handler` instead of `MetricExporterClient`. Like if someone wants to implement a different exporter, they should implement a subclass of `opencensus::stats::StatsExporter::Handler` instead of `MetricExporterClient`. `OpenCensusProtoExporter` is the default exporter implemented in Ray that exports metrics to the metrics agent. `MetricPointExporter` should be a customized one developed by Ant (the code should be in Ant's repo).

`ray::stats::Init` interface can be 

```
ray::stats::Init(
....
std::shared_ptr<opencensus::stats::StatsExporter::Handler> exporter_to_use = nullptr,
) {
  if exporter_to_use == nullptr:
     // Use the default exporter provided by Ray
     exporter_to_use = std::make_shared<>(new OpenCensusProtoExporter())
}
```


In the Ant's code, you can do `ray::stats::Init(exporter_to_use=MetricPointCeresDBExporter())`


cc @rkooo567 @rickyyx ",got think interface instead like someone implement different exporter implement subclass instead default exporter ray metric metric agent one ant code ant ray interface ray use default exporter provided ray new ant code ray,issue,negative,positive,neutral,neutral,positive,positive
1972540899,"In spirit of not breaking anything, set the flag to default=false and added back `decouple`.",spirit breaking anything set flag added back,issue,negative,neutral,neutral,neutral,neutral,neutral
1972501154,"@anyscalesam I believe it is an issue with WSL. When I ran the same code on pure Linux, it started without any error",believe issue ran code pure without error,issue,negative,positive,positive,positive,positive,positive
1972477961,"Premerge failure due to it adds a read to gcs on core worker creation. It can happen very slow (as slow as 3 second in bad cases) and makes some unit tests to timeout. Fixed by removing the read, instead, populate driver_node_id in JobConfig in driver's core worker ctor.",failure due read core worker creation happen slow slow second bad unit fixed removing read instead populate driver core worker,issue,negative,negative,negative,negative,negative,negative
1972452787,"@jjyao  Yes, this is still used by Ant.  This is very important. Ant Group uses a time-series database called CeresDB. By extending MetricExporterDecorator, we have implemented own OpenCensusProtoExporter that reports metric data via the HTTP protocol.
```
auto ceresdb_exporter = InitCeresdbExporter(exporter);
MetricPointExporter::Register(ceresdb_exporter, metrics_report_batch_size);
```

If you want to remove it and use a unified Exporter for reporting metric data, you need to refactor to allow other developers to customize the exporter client.


",yes still used ant important ant group extending metric data via protocol auto exporter want remove use unified exporter metric data need allow exporter client,issue,positive,positive,positive,positive,positive,positive
1972304477,Sending this PR to draft while waiting to merge the Python version of this script,sending draft waiting merge python version script,issue,negative,neutral,neutral,neutral,neutral,neutral
1972259115,@larrylian @wumuzi520 @SongGuyang is this something still used by Ant?,something still used ant,issue,negative,neutral,neutral,neutral,neutral,neutral
1972222116,"@ericl Yes I'll try to merge this PR to land the accelerator_type support.

Actually the users now can already specify `accelerator_type` in ScalingConfig, e.g. 
```
ScalingConfig(
    ...,
    resources_per_worker={""accelerator_type:A100"": 0.01, ...}. 
)
```
This PR provides a better user interface without the awkward 0.01 workaround.
",yes try merge land support actually already specify better user interface without awkward,issue,positive,negative,neutral,neutral,negative,negative
1972212117,Closing since this script will be incorporated into a PyPI library in another PR,since script incorporated library another,issue,negative,neutral,neutral,neutral,neutral,neutral
1972188810,Do we still need this PR to land the `accelerator_type` support?,still need land support,issue,negative,neutral,neutral,neutral,neutral,neutral
1972155893,"Tested few cases and seeing it behave as expected

- Error trace 
<img width=""1710"" alt=""image"" src=""https://github.com/ray-project/ray/assets/7553988/83f9ee6d-c1be-4d2e-b746-cc39d653c159"">


- Multiline logs
<img width=""1271"" alt=""image"" src=""https://github.com/ray-project/ray/assets/7553988/1fbbc1e4-b7a8-48e5-916c-db83d998f381"">

- Directly logging through stdout and stderr
<img width=""1541"" alt=""image"" src=""https://github.com/ray-project/ray/assets/7553988/c616f736-1cb1-4c88-aba6-01023d0b1254"">

- Initializing resnet from pytorch hub and print some labels and objects
<img width=""1174"" alt=""image"" src=""https://github.com/ray-project/ray/assets/7553988/de380930-9f32-47af-80f8-5cc99afd2634"">
<img width=""1425"" alt=""image"" src=""https://github.com/ray-project/ray/assets/7553988/7f1a9b9c-7542-4d88-b86a-6d9c1234ee5a"">
<img width=""1556"" alt=""image"" src=""https://github.com/ray-project/ray/assets/7553988/1339f05a-cf16-47a1-800f-13c05f344f74"">

- Running sentiment analysis example with hugging face
<img width=""1101"" alt=""image"" src=""https://github.com/ray-project/ray/assets/7553988/adca54ad-5d20-4edb-b792-0860c9715b05"">
<img width=""1218"" alt=""image"" src=""https://github.com/ray-project/ray/assets/7553988/94315c73-2487-4fce-8cf4-bfcfa99b5e77"">
<img width=""1395"" alt=""image"" src=""https://github.com/ray-project/ray/assets/7553988/967b7caf-0b37-4180-afeb-c88b5aea96c0"">



",tested seeing behave error trace image image directly logging image hub print image image image running sentiment analysis example hugging face image image image,issue,negative,positive,neutral,neutral,positive,positive
1972155686,"```
some of our tasks were using a module that creates daemon subprocesses for parallel reading and writing but never joins them. when running the module in a stand-alone application this happened to be no problem as the daemon processes were killed once the main process terminated. however, as ray is reusing its worker processes, this means that the daemon subprocesses stay around as zombie processes on the ray nodes, slowly eating up more and more memory with every task. this also explains why we saw so many processes in the traceback of the out of memory errors.
it looks like this is not an issue in ray itself but a bug in the respective module.
however, do you think it could be a valuable feature for ray to join left over zombie processes after a worker finishes a task in order to prevent these kind of situations for modules that were not built with multiprocessing in mind?
```",module daemon parallel reading writing never running module application problem daemon main process however ray worker daemon stay around zombie ray slowly eating memory every task also saw many memory like issue ray bug respective module however think could valuable feature ray join left zombie worker task order prevent kind built mind,issue,positive,positive,positive,positive,positive,positive
1972146147,Part of this should be publishing a Serve scalability envelope similar to the Ray one,part serve envelope similar ray one,issue,negative,neutral,neutral,neutral,neutral,neutral
1972111148,"@matthewdeng ml have 1 windows test, haven't talked to you about this but let me know if you see any red flags, thankks",test let know see red,issue,negative,neutral,neutral,neutral,neutral,neutral
1972047820,When will this fix be released. It introduced installation [issue](https://github.com/autogluon/autogluon/issues/3807) for AutoGluon on Windows platform with Python 3.11,fix installation issue platform python,issue,negative,neutral,neutral,neutral,neutral,neutral
1971988100,"From the error message, this appears to be caused by yesterday's huggingface_hub outage. It should have been recovered now.

```
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like hf-internal-testing/tiny-random-gpt2 is not the path to a directory containing a config.json file.
--
  | Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
```",error message yesterday outage could connect load model could find like path directory file connection see run library mode,issue,negative,neutral,neutral,neutral,neutral,neutral
1971894779,"- we should not change the pre-command hook. related env vars should just passthrough. otherwise, we will have a lot of env var manipulation in pre-command hook over time, which is a misuse of the pre-command hook.
- for the env vars to work inside the steps, it needs to be added into rayci so that it goes through the docker plugin layer
- rayci env vars should prefix with `RAYCI_`
- premerge/postmerge can already be distinguished from the pipeline slug or pipeline id
- ""continuous"" probably should be changed to ""routine"" or ""routinely"". ""continuous"" means non-stop / never quits

I think we can:
- add an env var called `RAYCI_ROUTINE`, and make it passthrough docker plugin layer.
- and its value can be ""night"" or ""daytime""
- we can add more `RAYCI_` env vars if we need, but we should be disciplined about it.",change hook related otherwise lot manipulation hook time misuse hook work inside need added go docker layer prefix already distinguished pipeline slug pipeline id continuous probably routine routinely continuous never quits think add make docker layer value night daytime add need,issue,negative,neutral,neutral,neutral,neutral,neutral
1971835710,Printing the token has risks of exposing the credentials... Sending this PR to draft while I work on another communication method between this script and the bash script used in pipeline,printing token sending draft work another communication method script bash script used pipeline,issue,negative,neutral,neutral,neutral,neutral,neutral
1971734390,"Bad merge, randomly tagged people please ignore ",bad merge randomly tagged people please ignore,issue,negative,negative,negative,negative,negative,negative
1971716210,@jjyao confirm here that this is a bug in the core stack and something we need to fix for ray210 release?,confirm bug core stack something need fix ray release,issue,negative,neutral,neutral,neutral,neutral,neutral
1971638948,"Yeah. So, I do not want to remove the gradient, I wanted to make it lighter. 
But, for some reason, it was not happening. 
So, I commented out the code so that we solve the issue asap and get this in 2.10 and later can play with adding or modifying the gradient. 
Wdyt @angelinalg? Or should I just remove it? ",yeah want remove gradient make lighter reason happening code solve issue get later play gradient remove,issue,positive,neutral,neutral,neutral,neutral,neutral
1971594721,"Is there a reason to comment out the code instead of removing it? If you are keeping the code in, wdyt about adding a comment as to why it's commented out and why we are keeping it in the code?",reason comment code instead removing keeping code comment keeping code,issue,negative,neutral,neutral,neutral,neutral,neutral
1971591945,@hongchaodeng Hello! I think you can close my PR and finish this task in your PR. ,hello think close finish task,issue,negative,neutral,neutral,neutral,neutral,neutral
1971249601,"My cause of this problem may be that when `trl` uses `packing`, it will generate a dynamic length dataset `ConstantLengthDataset`, which will lead to inconsistent data lengths on different nodes.
Hope that can help you.

You can use `packing=False` or pass the dataset directly to the training func",cause problem may generate dynamic length lead inconsistent data different hope help use pas directly training,issue,positive,positive,neutral,neutral,positive,positive
1970287109,"Hi @simonsays1980, I've tried to use the `local_policy_inference` method, but it will raise an exception on the second step. I think the problem is caused here:

https://github.com/ray-project/ray/blob/3546c41b9996c36d0f1bba497dc660d3fcfc59a7/rllib/evaluation/collectors/agent_collector.py#L364-L366

It seems like on the first call, some missing data (e.g. ""rewards"") will be padded. However, on the second call it is simply `[[]]`, which caused index error when doing this caching.

- Ray version: 2.8.1
- Python version: 3.10.13
- Environment: Humanoid-v4
- Algorithm: PPO (rl-module)

UPDATE:
I found out this is likely caused by `rl_module`? If I forced PPO to use ModelV2 (`PPOConfig().rl_module(_enable_rl_module_api=False).training(_enable_learner_api=False)`), the code can run without a problem. However, since I already trained the model, is there a way to fix it?",hi tried use method raise exception second step think problem like first call missing data however second call simply index error ray version python version environment algorithm update found likely forced use code run without problem however since already trained model way fix,issue,negative,negative,neutral,neutral,negative,negative
1970273891,"> > It's just a better name than the RAY_task_events_report_interval, and it provides a way to workers get default behaviour of if task events should be reported.
> 
> Hmm I feel like we should make it just orthogonal. It makes things confusing if we start coupling user-level config and system level config?
> 
> * True by default.
> * False if you don't want
> * although it is True, if user sets systems to not report task, it is disabled. (just use existing flag)
> 
> Btw, it is not a hard blocker.

This is actually exactly how it's behaved in the PR, with the exception of not using existing flag (i.e. RAY_task_events_report_interval):
If users to set system config for not reporting, they would:
- In PR: set `RAY_enable_tasks_events=False`
- In master: set `RAY_task_events_report_interval=0` 


I am also open to keeping with existing flags. 
",better name way get default behaviour task feel like make orthogonal start coupling system level true default false want although true user report task disabled use flag hard blocker actually exactly exception flag set system would set master set also open keeping,issue,positive,positive,neutral,neutral,positive,positive
1970270756,"@peytondmurray Let's try it out and see how it looks? 
I created a PR to remove it for now. Whenever you have time, we can try out some options here, not urgent. ",let try see remove whenever time try urgent,issue,negative,neutral,neutral,neutral,neutral,neutral
1970222283,"> It's just a better name than the RAY_task_events_report_interval, and it provides a way to workers get default behaviour of if task events should be reported.

Hmm I feel like we should make it just orthogonal. It makes things confusing if we start coupling user-level config and system level config? 

- True by default.
- False if you don't want
- although it is True, if user sets systems to not report task, it is disabled. (just use existing flag)

Btw, it is not a hard blocker. ",better name way get default behaviour task feel like make orthogonal start coupling system level true default false want although true user report task disabled use flag hard blocker,issue,positive,positive,neutral,neutral,positive,positive
1970203898,"
> Hmm I don't understand why we need this?

It's just a better name than the `RAY_task_events_report_interval`, and it provides a way to workers get default behaviour of if task events should be reported. 
",understand need better name way get default behaviour task,issue,negative,positive,positive,positive,positive,positive
1970186143,@simran-2797 I don't even see the gradient anymore in the light theme mockup. Is it even there? In the dark theme it looks like it's faintly there.,even see gradient light theme even dark theme like faintly,issue,negative,negative,neutral,neutral,negative,negative
1970186078,"@can-anyscale checked. one is an existing flaky test, the other is because hugging face is down now.",checked one flaky test hugging face,issue,negative,neutral,neutral,neutral,neutral,neutral
1970182871,Can you help check that the failing tests on premerge are same as ones failing. in go/flaky. Thankks,help check failing failing,issue,negative,neutral,neutral,neutral,neutral,neutral
1970178246,@aslonnie  @can-anyscale I think we can just keep it as `small_branch` to specify it's a branch runner queue (it's the only one in this pipeline) and not confuse it with the rest which are PR queues,think keep specify branch runner queue one pipeline confuse rest,issue,negative,neutral,neutral,neutral,neutral,neutral
1970177402,"> ActorHandle has to have this enforce_task_events flag as well otherwise remote actors would not work

You mean enable_task_events right? This makes sense

> Add a global RAY_enable_task_events to enforce if task events would be reported from the entire ray cluster (rather than relying on RAY_task_events_report_interval) -> we could maybe refactor all task events related things into a config struct in the future.

Hmm I don't understand why we need this? 
",flag well otherwise remote would work mean right sense add global enforce task would entire ray cluster rather could maybe task related future understand need,issue,negative,negative,neutral,neutral,negative,negative
1970163018,"let me know if it is ready to merge (like if you want to use ""small"" instead)

",let know ready merge like want use small instead,issue,positive,negative,neutral,neutral,negative,negative
1970153907,"oh yes, the test hasn't been running for a while because the os mac build is broken (investigating by @iycheng currently). Before the build is broken, the test still seems to be flaky ",oh yes test running o mac build broken investigating currently build broken test still flaky,issue,negative,negative,negative,negative,negative,negative
1970148585,"@can-anyscale why did the bot not re-open https://github.com/ray-project/ray/issues/42537 and instead open this one instead? Did it start failing again (confused because if you look at #42537 it was closed 5d ago after the test started passing from the latest PR #43268  that @rkooo567 merged to master).

Looking at go/flaky it looks like the test isn't being run?",bot instead open one instead start failing confused look closed ago test passing latest master looking like test run,issue,negative,neutral,neutral,neutral,neutral,neutral
1970112015,"btw, we do want to deprecate this template in favor of the latest fine-tuning template right? @kouroshHakha @shawnpanda 
This fix is probably not needed any more right?",want deprecate template favor latest template right fix probably right,issue,negative,positive,positive,positive,positive,positive
1970103636,"@peytondmurray 
Thanks for helping out with this. 
I just updated the designs here to make the gradient lighter. - https://www.figma.com/file/DjtgroLeO6BQSD6Si5dedw/Ray-docs-design-system?type=design&node-id=20%3A2017&mode=design&t=2GKZcgcLjNF6BrO8-1
",thanks helping make gradient lighter,issue,positive,positive,positive,positive,positive,positive
1970043016,"@aslonnie you can review https://github.com/ray-project/ray/pull/43465 yess, i'll merge them together after approval (to save some rebase + rerun of CI))",review merge together approval save rebase rerun,issue,positive,neutral,neutral,neutral,neutral,neutral
1970041459,(ready to merge? I can review the next PR on top of this now?),ready merge review next top,issue,positive,positive,positive,positive,positive,positive
1970029912,"@scottjlee @anyscalesam Hi folks, what's the progress on this issue? Is it on track to be fixed in 2.10?",hi progress issue track fixed,issue,negative,positive,neutral,neutral,positive,positive
1969990272,@scottsun94 do we merge master > branch than back up to master to close this out?,merge master branch back master close,issue,negative,neutral,neutral,neutral,neutral,neutral
1969981489,Fixed by Justin. Recent weekly test passed: https://buildkite.com/ray-project/release/builds/9367,fixed recent weekly test,issue,negative,positive,neutral,neutral,positive,positive
1969980004,Data team is deprioritizing the fix for this until after branch cut: https://anyscaleteam.slack.com/archives/C01DVKB5SHE/p1708724977618549?thread_ts=1708718038.180029&cid=C01DVKB5SHE,data team fix branch cut,issue,negative,neutral,neutral,neutral,neutral,neutral
1969972685,"I found it's just the info stored in the gcs, when I set export RAY_task_events_max_num_task_in_gcs=10000 (default is 100000), the memory will not increase to 1.5G but keeps at 700MB for VIRT and 138MB for RES.
![image](https://github.com/ray-project/ray/assets/121425509/386c2bcd-8167-4d57-b859-f3f74abe7564)

![image](https://github.com/ray-project/ray/assets/121425509/109bff22-bdbb-4661-871b-4fd872a73ca4)

",found set export default memory increase image image,issue,negative,neutral,neutral,neutral,neutral,neutral
1969917491,"1. I test the script above and I found the memory of gcs will increase will increase  from 600M to 1.5G total, but it will not increase when I submit later task. Just keep at 1.5G when later task is submit.
2. I tried to replace the in memory store to redis store, it will still increase. 
I suspect it is the REF in the object memory store?
![image](https://github.com/ray-project/ray/assets/121425509/9127215e-8371-463a-a894-8059b4d3e696)
",test script found memory increase increase total increase submit later task keep later task submit tried replace memory store store still increase suspect ref object memory store image,issue,positive,neutral,neutral,neutral,neutral,neutral
1969859177,Closing this PR since I'll be using this script as a step in the `release-automation` pipeline in another PR.,since script step pipeline another,issue,negative,neutral,neutral,neutral,neutral,neutral
1969857571,"@khluu sound good, i just mean the manual e2e tests which you already did",sound good mean manual already,issue,negative,positive,positive,positive,positive,positive
1969857002,"> Where exactly should I set up the environmental variables? Local machine? Head node? Worker nodes? All of them?

Set them as a variable in the `ray.init(runtime_env={""env_vars"": {""<variable>"": ""<value>""}})`",exactly set environmental local machine head node worker set variable variable value,issue,negative,positive,positive,positive,positive,positive
1969853101,"@can-anyscale  I don't have an e2e test right now (was planning for that once I put up the PR for the binary file that calls these functions), but I manually tested them and the crane binary from bazel runfiles did work",test right put binary file manually tested crane binary work,issue,negative,positive,positive,positive,positive,positive
1969698451,"This PR has a bug, as detailed in https://github.com/ray-project/ray/pull/43505. Reverting this first while I'm working on a fix.",bug detailed first working fix,issue,negative,positive,positive,positive,positive,positive
1969676110,"Thanks for the fix!


@rynewang  Do you only want to add those notes to the documentation? Shall we also show them in the error message directly?

```
Failed to execute `['sudo', '-n', '/home/ray/anaconda3/bin/py-spy', 'record', '-o', PosixPath('/tmp/ray/session_2024-01-31_09-17-15_037017_341/logs/flamegraph_33695_cpu_profiling.svg'), '-p', '33695', '-d', '5', '-f', 'flamegraph']`.
- if you see ""No such file or direction"" in the stderr below, your worker process may have been exited. 
- If you see ""No stack counts found"" in the stderr below, your worker process may be sleeping and did not have activity in the last 5s.

=== stderr ===
[2024-01-31T22:37:27.932972718Z ERROR inferno::flamegraph] No stack counts found
Error: Failed to write flamegraph: I/O error: No stack counts found


=== stdout ===
py-spy> Sampling process 100 times a second for 5 seconds. Press Control-C to exit.
```",thanks fix want add documentation shall also show error message directly execute see file direction worker process may see stack found worker process may sleeping activity last error inferno stack found error write error stack found sampling process time second press exit,issue,negative,positive,neutral,neutral,positive,positive
1969605551,"Since we haven't heard from you, I have cherry pick your commit and make another PR with the commented changes: https://github.com/ray-project/ray/pull/43507

Let me know if you still want to work on this. Thanks for your contribution and good will!",since cherry pick commit make another let know still want work thanks contribution good,issue,positive,positive,positive,positive,positive,positive
1969567635,can we hold on reverting but let Data team investigate first? This is a giant and important PR that is 2.10 release blocker.,hold let data team investigate first giant important release blocker,issue,negative,positive,positive,positive,positive,positive
1969544597,"Checked some multi-node release tests: `torch_batch_inference_16_gpu_300gb_parquet, read_parquet_train_16_gpu, read_images_train_16_gpu, stable_diffusion_benchmark`.  No obvious perf regression. Will consider this PR safe to merge.",checked release obvious regression consider safe merge,issue,negative,positive,positive,positive,positive,positive
1969544467,"Some CI test failures to fix:

```
_____________________ test_read_tfrecords[False-True-GZIP] _____________________
--
  |  
  | with_tf_schema = False, tfx_read = True, compression = 'GZIP'
  | ray_start_regular_shared = RayContext(dashboard_url='127.0.0.1:8265', python_version='3.9.16', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')
  | tmp_path = PosixPath('/tmp/pytest-of-root/pytest-1/test_read_tfrecords_False_True1')
  |  
  | @pytest.mark.parametrize(
  | ""with_tf_schema,tfx_read,compression"",
  | [
  | (True, True, None),
  | (True, True, ""GZIP""),
  | (True, False, None),
  | (False, True, None),
  | (False, True, ""GZIP""),
  | (False, False, None),
  | ],
  | )
  | def test_read_tfrecords(
  | with_tf_schema,
  | tfx_read,
  | compression,
  | ray_start_regular_shared,
  | tmp_path,
  | ):
  | import pandas as pd
  | import tensorflow as tf
  |  
  | example = tf_records_empty()[0]
  |  
  | tf_schema = None
  | if with_tf_schema:
  | tf_schema = _features_to_schema(example.features)
  |  
  | path = os.path.join(tmp_path, ""data.tfrecords"")
  | with tf.io.TFRecordWriter(
  | path=path, options=tf.io.TFRecordOptions(compression_type=compression)
  | ) as writer:
  | writer.write(example.SerializeToString())
  |  
  | arrow_open_stream_args = None
  | if compression:
  | arrow_open_stream_args = {""compression"": compression}
  |  
  | ds = read_tfrecords_with_tfx_read_override(
  | path,
  | tf_schema=tf_schema,
  | tfx_read=tfx_read,
  | arrow_open_stream_args=arrow_open_stream_args,
  | )
  |  
  | >       df = ds.to_pandas()
  |  
  | python/ray/data/tests/test_tfrecords.py:386:
  | _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
  | /rayci/python/ray/data/dataset.py:4372: in to_pandas
  | count = self.count()
  | /rayci/python/ray/data/dataset.py:2486: in count
  | [get_num_rows.remote(block) for block in self.get_internal_block_refs()]
  | /rayci/python/ray/data/dataset.py:4628: in get_internal_block_refs
  | blocks = self._plan.execute().get_blocks()
  | /rayci/python/ray/data/_internal/lazy_block_list.py:293: in get_blocks
  | blocks, _ = self._get_blocks_with_metadata()
  | /rayci/python/ray/data/_internal/lazy_block_list.py:327: in _get_blocks_with_metadata
  | meta = ray.get(refs_list.pop(-1))
  | /rayci/python/ray/_private/auto_init_hook.py:21: in auto_init_wrapper
  | return fn(*args, **kwargs)
  | /rayci/python/ray/_private/auto_init_hook.py:21: in auto_init_wrapper
  | return fn(*args, **kwargs)
  | /rayci/python/ray/_private/client_mode_hook.py:103: in wrapper
  | return func(*args, **kwargs)
  | /rayci/python/ray/_private/worker.py:2647: in get
  | values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  | _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
  |  
  | self = <ray._private.worker.Worker object at 0x7fae7ab34e20>
  | object_refs = [ObjectRef(c76a79b2875a7251ffffffffffffffffffffffff0100000002e1f505)]
  | timeout = None
  |  
  | def get_objects(
  | self,
  | object_refs: list,
  | timeout: Optional[float] = None,
  | ):
  | """"""Get the values in the object store associated with the IDs.
  |  
  | Return the values from the local object store for object_refs. This
  | will block until all the values for object_refs have been written to
  | the local object store.
  |  
  | Args:
  | object_refs: A list of the object refs
  | whose values should be retrieved.
  | timeout: The maximum amount of time in
  | seconds to wait before returning.
  | Returns:
  | list: List of deserialized objects
  | bytes: UUID of the debugger breakpoint we should drop
  | into or b"""" if there is no breakpoint.
  | """"""
  | # Make sure that the values are object refs.
  | for object_ref in object_refs:
  | if not isinstance(object_ref, ObjectRef):
  | raise TypeError(
  | f""Attempting to call `get` on the value {object_ref}, ""
  | ""which is not an ray.ObjectRef.""
  | )
  |  
  | timeout_ms = int(timeout * 1000) if timeout is not None else -1
  | data_metadata_pairs = self.core_worker.get_objects(
  | object_refs,
  | self.current_task_id,
  | timeout_ms,
  | )
  | debugger_breakpoint = b""""
  | for data, metadata in data_metadata_pairs:
  | if metadata:
  | metadata_fields = metadata.split(b"","")
  | if len(metadata_fields) >= 2 and metadata_fields[1].startswith(
  | ray_constants.OBJECT_METADATA_DEBUG_PREFIX
  | ):
  | debugger_breakpoint = metadata_fields[1][
  | len(ray_constants.OBJECT_METADATA_DEBUG_PREFIX) :
  | ]
  | values = self.deserialize_objects(data_metadata_pairs, object_refs)
  | for i, value in enumerate(values):
  | if isinstance(value, RayError):
  | if isinstance(value, ray.exceptions.ObjectLostError):
  | global_worker.core_worker.dump_object_store_memory_usage()
  | if isinstance(value, RayTaskError):
  | >                   raise value.as_instanceof_cause()
  | E                   ray.exceptions.RayTaskError(ModuleNotFoundError): ray::_execute_read_task_split() (pid=96061, ip=172.16.0.3)
  | E                     File ""/rayci/python/ray/data/_internal/lazy_block_list.py"", line 637, in _execute_read_task_split
  | E                       for block in blocks:
  | E                     File ""/rayci/python/ray/data/datasource/datasource.py"", line 164, in __call__
  | E                       yield from result
  | E                     File ""/rayci/python/ray/data/_internal/execution/operators/map_transformer.py"", line 430, in __call__
  | E                       for block in blocks:
  | E                     File ""/rayci/python/ray/data/_internal/execution/operators/map_transformer.py"", line 371, in __call__
  | E                       for data in iter:
  | E                     File ""/rayci/python/ray/data/datasource/file_based_datasource.py"", line 267, in read_task_fn
  | E                       yield from read_files(read_paths)
  | E                     File ""/rayci/python/ray/data/datasource/file_based_datasource.py"", line 236, in read_files
  | E                       for block in read_stream(f, read_path):
  | E                     File ""/rayci/python/ray/data/datasource/tfrecords_datasource.py"", line 74, in _read_stream
  | E                       yield from self._tfx_read_stream(f, path)
  | E                     File ""/rayci/python/ray/data/datasource/tfrecords_datasource.py"", line 102, in _tfx_read_stream
  | E                       from tfx_bsl.cc.tfx_bsl_extension.coders import ExamplesToRecordBatchDecoder
  | E                   ModuleNotFoundError: No module named 'tfx_bsl'


```",test fix false true compression dev compression true true none true true true false none false true none false true false false none compression import import example none path writer none compression compression compression path count count block block meta return return wrapper return get self object none self list optional float none get object store associated return local object store block written local object store list object whose maximum amount time wait list list drop make sure object raise call get value none else data value enumerate value value value raise ray file line block file line yield result file line block file line data iter file line yield file line block file line yield path file line import module,issue,positive,positive,neutral,neutral,positive,positive
1969154255,"Lmk when it is ready again. Per private discussion
- we will make both raylet and worker subreaper
- this means granchildren can be zomebie in some circumstances. we should document this well
- off by default
- timer based approach",ready per private discussion make raylet worker document well default timer based approach,issue,positive,positive,neutral,neutral,positive,positive
1969008989,"I am still seeing that del is called erroneously on the ActorClass object, which raises exceptions like the one in the example script.",still seeing erroneously object like one example script,issue,negative,negative,negative,negative,negative,negative
1968810975,@jjyao - I'd like to ask if there are any upcoming plans for this bug to be fixed? ,like ask upcoming bug fixed,issue,negative,positive,neutral,neutral,positive,positive
1968542649,"After further investigation, I've found out this error shows up if you use `minibatch_size` as part of hyperparameter mutations dict with PBT or PB2.  I'm not sure if this affects other search algorithms. #43467 would solve this",investigation found error use part sure search would solve,issue,negative,positive,positive,positive,positive,positive
1968376890,"`cloudpickle.dumps` raise PicklingError with Worker actor class, but if `@ray.remote` removed, the original class can be pickled by cloudpickle.
![image](https://github.com/ray-project/ray/assets/48577571/93fc23a7-2a23-40a2-930c-5ba2177dddfc)
",raise worker actor class removed original class image,issue,negative,positive,positive,positive,positive,positive
1968331933,"@jjyao Oh, I didn't get it. You were right, these tests can be mocked. We add these tests to increase coverage. What do you think, mock these tests or wait until HPU is available in CI?",oh get right add increase coverage think mock wait available,issue,negative,positive,positive,positive,positive,positive
1968219370,"@anyscalesam I need to run it on ray2.6.3 due to some limitations, so version upgrade is not the solution for me",need run ray due version upgrade solution,issue,negative,negative,negative,negative,negative,negative
1968143902,"Cuurently ray workspace is not pointing to s390x for python repo under bazel . So i am using the latest code ( 0.31.0 ) which has s390x support already as shown below 
```
[root@m1305001 ray]# git diff WORKSPACE 
diff --git a/WORKSPACE b/WORKSPACE
index 2283b4aaf8..1ee491fef5 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -32,9 +32,9 @@ hedron_compile_commands_setup()
 
 http_archive(
     name = ""rules_python"",
-    sha256 = ""94750828b18044533e98a129003b6a68001204038dc4749f40b195b24c38f49f"",
-    strip_prefix = ""rules_python-0.21.0"",
-    url = ""https://github.com/bazelbuild/rules_python/releases/download/0.21.0/rules_python-0.21.0.tar.gz"",
+    sha256 = ""c68bdc4fbec25de5b5493b8819cfc877c4ea299c0dcb15c244c5a00208cde311"",
+    strip_prefix = ""rules_python-0.31.0"",
+    url = ""https://github.com/bazelbuild/rules_python/releases/download/0.31.0/rules_python-0.31.0.tar.gz"",
 )
```
 But still it is taking older version during bazel build operation. do i have to change somewhere else as well ",ray pointing python latest code support already shown root ray git git index name sha sha still taking older version build operation change somewhere else well,issue,positive,positive,positive,positive,positive,positive
1968090710,"> @modassarrana89 did you try latest ray?

Yes , I have .it didn't work.  I am currently looking at ray files to support s390x. It would be great if you can guide me what all file should i look apart from docker image 
```
ubuntu/focal
└── base-deps:cpu
    └── ray-deps:cpu
        └── ray:cpu
            └── ray-ml:cpu
```",try latest ray yes work currently looking ray support would great guide file look apart docker image ray,issue,positive,positive,positive,positive,positive,positive
1967996080,"@rkooo567 can you take a quick look at the delta in the commits? 

I made a few changes to make tests passed
- ActorHandle has to have this `enforce_task_events` flag as well otherwise remote actors would not work 
- Add a global RAY_enable_task_events to enforce if task events would be reported from the entire ray cluster (rather than relying on `RAY_task_events_report_interval`) -> we could maybe refactor all task events related things into a config struct in the future. ",take quick look delta made make flag well otherwise remote would work add global enforce task would entire ray cluster rather could maybe task related future,issue,negative,positive,neutral,neutral,positive,positive
1967964024,Closing because we no longer expose a glossary for Ray Data,longer expose glossary ray data,issue,negative,neutral,neutral,neutral,neutral,neutral
1967951544,"Here is the call path:

(scripts.py) status() -> debug_status() -> format_info_string() -> get_usage_report() -> parse_usage()

If it goes into the `else` statement, it will print it without formatting:

https://github.com/ray-project/ray/blob/783da640a20ddbd3b41b893485abf187f0f27223/python/ray/autoscaler/_private/util.py#L626-L647

My conclusion is we can just update the following line:

```
 line = f""{used}/{total} {resource}""
```",call path status go else statement print without conclusion update following line line used total resource,issue,negative,neutral,neutral,neutral,neutral,neutral
1967881774,cc @woshiyyya @matthewdeng I am removing these for now because they're just testing `xgboost_ray`/`lightgbm_ray` but will add back 2 release tests in the next PR.,removing testing add back release next,issue,negative,neutral,neutral,neutral,neutral,neutral
1967851457,"@aslonnie yes, that logic already exists the global pipeline configuration level; this PR just adds it to the per test configuration level.",yes logic already global pipeline configuration level per test configuration level,issue,negative,neutral,neutral,neutral,neutral,neutral
1967740216,"> Discussed offline w/ @martinbomio. For 2.10 release, let's introduce 3 configs in DataContext, and disable the tfx read by default (to be safe), and we can enable the tfx read in future release.
> 
> ```
> enable_tfrecords_tfx_read
> tfrecords_tfx_read_batch_size
> tfrecords_tfx_read_auto_infer_schema
> ```

actually instead of adding flags in `DataContext`. I think it's better to just add them in `read_tfrecords`. that would be more self-contained and easier to manage. and I'm also concerned of adding too many flags in `DataContext`. 

Also, since we are adding 3 parameters, it'd be cleaner to combine them in a single one e.g. `tfx_read_options: TfxReadOptions`, and mark it as experimental.",release let introduce disable read default safe enable read future release actually instead think better add would easier manage also concerned many also since cleaner combine single one mark experimental,issue,positive,positive,positive,positive,positive,positive
1967616265,"I've also noticed the problem you mentioned. In my case, it seems that leaving ```name=""test_experiment""``` out fixes the issue.",also problem case leaving issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1967614850,The problem seems somewhat related to [43092](https://github.com/ray-project/ray/issues/43092). but not exactly the same.,problem somewhat related exactly,issue,negative,positive,positive,positive,positive,positive
1967559796,"Discussed offline w/ @martinbomio. For 2.10 release, let's introduce 3 configs in DataContext, and disable the tfx read by default (to be safe), and we can enable the tfx read in future release.

```
enable_tfrecords_tfx_read
tfrecords_tfx_read_batch_size
tfrecords_tfx_read_auto_infer_schema
```",release let introduce disable read default safe enable read future release,issue,negative,positive,positive,positive,positive,positive
1967496233,"> @zcin please link the relevant issue this closes. And any way for us to validate that it's fixed?

The original issues were all related to creating new threads and different threads accessing the same memory causing race conditions. We aren't starting any new threads here (using the same asyncio event loop as router, replica etc) so they should be fixed.",please link relevant issue way u validate fixed original related new different memory causing race starting new event loop router replica fixed,issue,positive,positive,positive,positive,positive,positive
1967493572,"No updates. It's a blocker, but we're deprioritizing it to after the branch cut so that we can focus on the stability changes.",blocker branch cut focus stability,issue,negative,neutral,neutral,neutral,neutral,neutral
1967470407,@zcin please link the relevant issue this closes. And any way for us to validate that it's fixed?,please link relevant issue way u validate fixed,issue,positive,positive,positive,positive,positive,positive
1967430756,"@edoakes @GeneDer Still trying to figure out a test failure + cleaning up some tests, but PTAL!",still trying figure test failure cleaning,issue,negative,negative,negative,negative,negative,negative
1967428840,"Yes let's put it as release blocker, https://github.com/ray-project/ray/pull/43125 is target fix.",yes let put release blocker target fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1967414148,"> doc build test failed.

Yep as mentioned, should be solved by https://github.com/ray-project/ray/pull/43438. I'll rebase onto latest master, which should fix the issue.",doc build test yep rebase onto latest master fix issue,issue,negative,positive,positive,positive,positive,positive
1967408572,"If I remember correctly, in the earlier version of ray, we did not have this issue",remember correctly version ray issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1967378491,@max-509 Would you update the PR: 1. fix lint check; 2. rebase master?,would update fix lint check rebase master,issue,negative,neutral,neutral,neutral,neutral,neutral
1967329349,"> Q: Will you plan to continue on this PR @zcin ?

Yes, will continue after 2.10.",plan continue yes continue,issue,negative,neutral,neutral,neutral,neutral,neutral
1967305341,@jjyao im assuming here this sort of scenario isn't that well supported currently so enhancement and not bug? full implementation of multi-tenancy support would support these sort of use cases.,assuming sort scenario well currently enhancement bug full implementation support would support sort use,issue,positive,positive,positive,positive,positive,positive
1967259965,"> @rickyyx should we merge this?

See the above message for memory impact. Also, I think the front end 10K limit might make this change less significant: if users can query max 10K tasks, even if we have 1M in GCS, they still couldn't see much of them. ",merge see message memory impact also think front end limit might make change le significant query even still could see much,issue,negative,positive,positive,positive,positive,positive
1966600184,Please make sure to change docstring to the consistent style!,please make sure change consistent style,issue,positive,positive,positive,positive,positive,positive
1966548458,"@rickyyx let's make sure this is merged asap. It is required for 2.10. (cc @jjyao)

I think the blocker is the doc approval. ",let make sure think blocker doc approval,issue,positive,positive,positive,positive,positive,positive
1965915037,"@simonsays1980  In particular I have constructed the following environment as an example. Note that this environment has variable size in the inputs!
```
class ModuloComputationEnv(gym.Env):
    """"""Environment in which an agent must learn to output mod 2,3,4 of the sum of
       seen observations.

    Observations are squences of integer numbers ,
    e.g. (1,3,4,5)

    The action space is just 3 values first for the sum of inputs till now %2, second %3 
    and third %4.

    Rewards are r=-abs(self.ac1-action[0]) - abs(self.ac2-action[1]) - abs(self.ac3-action[2]), 
    for all steps.
    """"""

    def __init__(self, config):
        
        #the input sequence can have any number from 0,99
        self.observation_space = Sequence(Discrete(100), seed=2)

        #the action is a vector of 3, [%2, %3, %4], of the sum of the input sequence
        self.action_space = MultiDiscrete([2,3,4])

        self.cur_obs = None
        
        #this variable maintains the episode_length
        self.episode_len = 0

        #this variable maintains %2
        self.ac1 = 0
        
        #this variable maintains %3
        self.ac2 = 0

        #this variable maintains %4
        self.ac3 = 0

    def reset(self, *, seed=None, options=None):
        """"""Resets the episode and returns the initial observation of the new one.
        """"""

        # Reset the episode len.
        self.episode_len = 0
        
        # Sample a random sequence from our observation space.
        self.cur_obs = self.observation_space.sample()

        #take the sum of the initial observation
        sum_obs = sum(self.cur_obs)

        #consider the %2, %3, and %4 of the initial observation
        self.ac1 = sum_obs%2
        self.ac2 = sum_obs%3
        self.ac3 = sum_obs%4

        # Return initial observation.
        return self.cur_obs, {}

    def step(self, action):
        """"""Takes a single step in the episode given `action`

        Returns:
            New observation, reward, done-flag, info-dict (empty).
        """"""
        # Set `truncated` flag after 10 steps.
        self.episode_len += 1
        truncated = False
        terminated = self.episode_len >= 10

        #the reward is the negative of further away from computing the individual values
        reward = abs(self.ac1-action[0]) + abs(self.ac2-action[1]) + abs(self.ac3-action[2])
        reward = -reward


        # Set a new observation (random sample).
        self.cur_obs = self.observation_space.sample()

        #recompute the %2, %3 and %4 values
        self.ac1 = (self.cur_obs+self.ac1)%2
        self.ac2 = (self.cur_obs+self.ac2)%3
        self.ac3 = (self.cur_obs+self.ac3)%4
        
        return self.cur_obs, reward, terminated, truncated, {}

```
I would like to use the RLLib library for training some RL algorithm, is it possible? Some help in this regards would be great!
",particular following environment example note environment variable size class environment agent must learn output sum seen integer action space first sum till second third self input sequence number sequence discrete action vector sum input sequence none variable variable variable variable reset self episode initial observation new reset episode sample random sequence observation space take sum initial observation sum consider initial observation return initial observation return step self action single step episode given action new observation reward empty set truncated flag truncated false reward negative away individual reward reward set new observation random sample recompute return reward truncated would like use library training algorithm possible help would great,issue,positive,positive,neutral,neutral,positive,positive
1965670452,"> @MissiontoMars do you mind creating a PR to fix it?

OK,  i will try to fix it.",mind fix try fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1965655321,"@simonsays1980 Thank you for your quick message. This example seems to be of Tensor Flow, it would be great to see an example of PyTorch. Also, I am not very sure about the terminology. I am not super familiar with RNNs, I just have an MDP defined, and a state is represented by a `variable input size` and an action is represeted by size 3 output of the RNN. How easy would it be to use RLLib for this? I was hoping to see some example of this.",thank quick message example tensor flow would great see example also sure terminology super familiar defined state variable input size action size output easy would use see example,issue,positive,positive,positive,positive,positive,positive
1965644704,"> > which install less dependencies doesn't run into the issues
> 
> feels like this can be resolved by using the hermetic python? no?

ah, ok. I think I know what you mean now.",install le run like resolved hermetic python ah think know mean,issue,negative,negative,negative,negative,negative,negative
1965643192,"> which install less dependencies doesn't run into the issues

feels like this can be resolved by using the hermetic python? no?",install le run like resolved hermetic python,issue,negative,neutral,neutral,neutral,neutral,neutral
1965622295,We are going to make this a databricks private feature for now. So that I propose to rename the method to `from_databricks_spark`.,going make private feature propose rename method,issue,negative,neutral,neutral,neutral,neutral,neutral
1965612984,"Added the 5th check, and a unit test. It should work now, @rkooo567 ",added th check unit test work,issue,negative,neutral,neutral,neutral,neutral,neutral
1965592068,"> Please also make sure:
> 
> * this is reflected in appropriate docstrings.
> * a warning will get logged if users use the REST API.

@edoakes Added both of these! PTAL when you get the chance.",please also make sure reflected appropriate warning get logged use rest added get chance,issue,negative,positive,positive,positive,positive,positive
1965480741,As a workaround users can manually sync logs to a third party log system like datadog.,manually sync third party log system like,issue,positive,neutral,neutral,neutral,neutral,neutral
1965469082,Then you need to install Ray Java following the instructions.,need install ray following,issue,negative,neutral,neutral,neutral,neutral,neutral
1965466358,@brombaut are you able to build Ray from source code by following https://docs.ray.io/en/releases-2.9.3/ray-contribute/development.html?,able build ray source code following,issue,negative,positive,positive,positive,positive,positive
1965458753,@iycheng could you try to reproduce it on Mac as oncall?,could try reproduce mac,issue,negative,neutral,neutral,neutral,neutral,neutral
1965394167,I'll keep the [copy_build_artifacts script](https://github.com/ray-project/ray/blob/master/ci/build/copy_build_artifacts.sh) as a black box to rayci. It uploads to buildkite artifact on both premerge and postmerge; and only the release s3 bucket on postmerge; but yeah i keep that logic as a blackbox to rayci.,keep script black box artifact release bucket yeah keep logic,issue,negative,negative,negative,negative,negative,negative
1965383230,@aslonnie the [.buildkite/windows_ci.sh](https://github.com/ray-project/ray/blob/master/.buildkite/windows_ci.sh) calls `bash ci/ci.sh init` and `bash ci/ci.sh build` which installs dependencies that run into conflicts on windows (https://buildkite.com/ray-project/premerge/builds/20068#018dde20-be58-4569-b33a-18248f52586e/1523-2045). The wanda `windowbuilds` which install less dependencies doesn't run into the issues https://buildkite.com/ray-project/premerge/builds/20068#018dddd1-25f0-45d2-9878-c6398a44d484. I'm working on subsequent PRs to use this windowsbuilds to build windows wheel instead.,bash bash build run install le run working subsequent use build wheel instead,issue,negative,neutral,neutral,neutral,neutral,neutral
1965370052,decision is made that we still need to support windows.. so let's continue the work and get this wheel verification thing done.,decision made still need support let continue work get wheel verification thing done,issue,negative,neutral,neutral,neutral,neutral,neutral
1965368451,"> I feel that it is better to use and test the real thing rather than mocking.

Testing and using the Ray CLI is a separate concern. This PR affects how documentation is built, not how code is used or tested. We do not need to be able to _run_ the CLI to build the CLI docs, and therefore we do not need the Ray CLI dependencies. We're just reading some docstrings and generating HTML from them - we really _shouldn't_ be installing all the heavy ML/AI/distributed computing dependencies needed to actually run Ray to do that.

I'm happy to talk more about this if there are still some doubts about this; for context, we are already doing this for every other Ray dependency, and we [have a note about not adding extra dependencies in the requirements-doc.txt already](https://github.com/ray-project/ray/blob/bca48a386e989726ff090d47d0b4d839b87fbc3c/doc/requirements-doc.txt#L38C1-L39C80) so this isn't introducing any new patterns here.",feel better use test real thing rather testing ray separate concern documentation built code used tested need able build therefore need ray reading generating really heavy actually run ray happy talk still context already every ray dependency note extra already new,issue,positive,positive,positive,positive,positive,positive
1965337636,"ahh yess this is my bad, i'll put up a PR to fix shortly",bad put fix shortly,issue,negative,negative,negative,negative,negative,negative
1965336574,"Looks like the fix was successful, the `**` is correctly rendered here:

![image](https://github.com/ray-project/ray/assets/14017872/30b8a2ee-55ba-4062-b2ac-8f39ab0c6853)
",like fix successful correctly image,issue,positive,positive,positive,positive,positive,positive
1965327256,"Thank you @martinbomio. Can you address the CI test failure?

<img width=""1507"" alt=""Screen Shot 2024-02-26 at 1 30 09 PM"" src=""https://github.com/ray-project/ray/assets/4629931/49ac052f-016d-4802-8f8a-9b198097e159"">

<img width=""1494"" alt=""Screen Shot 2024-02-26 at 1 30 13 PM"" src=""https://github.com/ray-project/ray/assets/4629931/019f5770-0726-4146-9fbb-70bed36fab6e"">
",thank address test failure screen shot screen shot,issue,negative,negative,negative,negative,negative,negative
1965250982,"> ah yesss, it runs in the 'flaky test' jobs still in postmerge and not running in premerge (https://buildkite.com/ray-project/postmerge/builds/3106#018de410-2b74-48c8-8352-0c9440c3e978). Did we add the 'no_windows' tag back to the test bazel rule? If we need that to skip it completely.

i think i did it here - or at least i thought i did it here https://github.com/ray-project/ray/commit/a10011db7a66ee70896f4dacc07b07d277fd45a4",ah test still running add tag back test rule need skip completely think least thought,issue,negative,negative,neutral,neutral,negative,negative
1965241269,"# addressed 

> decouple

Intention of the decouple was to make the spawned process be reparented to 1 (init), however if we have subreaper the double forked subprocess will be reparented to raylet so it won't work.

> how we track child process' health

Yes I'm keeping it

> Only handle sigchld for unowned children

sigchld actually is triggered from the direct children death and we need to take care of all children deaths, which we do via `waitpid`. For the killing, yes we kill onowned children only.

> We can remove owned children pid when a worker is killed (there must be some sort of hook here)

Right, we remove owned children in `waitpid` where Linux tells us the child is gone.

> Make sure things are logged properly with pid with INFO.

Yes we have the logs on child exited and child killed.


> Probably we can remove parent core worker killing child workers and convert it to this mechanism?

Maybe. but let's at least keep it in this PR.

# Not addressed 

> The behavior should be clearly specified from core doc.

ok, will write doc in this PR

> premerge

It's strange, can't repro on my linux devbox. will fix",intention make process however double forked raylet wo work track child process health yes keeping handle unowned actually triggered direct death need take care via killing yes kill remove worker must sort hook right remove u child gone make sure logged properly yes child child probably remove parent core worker killing child convert mechanism maybe let least keep behavior clearly core doc write doc strange ca fix,issue,negative,positive,neutral,neutral,positive,positive
1965062990,"ah yesss, it runs in the 'flaky test' jobs still in postmerge and not running in premerge (https://buildkite.com/ray-project/postmerge/builds/3106#018de410-2b74-48c8-8352-0c9440c3e978). Did we add the 'no_windows' tag back to the test bazel rule? If we need that to skip it completely.",ah test still running add tag back test rule need skip completely,issue,negative,positive,neutral,neutral,positive,positive
1965054367,"oh, might be a flaky dashboard issue? 
<img width=""1145"" alt=""image"" src=""https://github.com/ray-project/ray/assets/11676094/a913febc-7065-4c02-859f-d056285c7130"">
",oh might flaky dashboard issue image,issue,negative,neutral,neutral,neutral,neutral,neutral
1965039078,"Hey @WeichenXu123 can we make this work in general for OSS Spark clusters? Seems like it probably shouldn't be hard (perhaps supporting a subset of functionality).

If not, maybe it makes more sense to call the method `from_databricks_df` or something more specific.",hey make work general spark like probably hard perhaps supporting subset functionality maybe sense call method something specific,issue,positive,positive,neutral,neutral,positive,positive
1965015263,@can-anyscale why is this still being run on the post merge? ,still run post merge,issue,negative,neutral,neutral,neutral,neutral,neutral
1964869466,"I encountered this issue as well. I have some relatively expensive state to initialize in an actor. However, unlike the operators `map_batches`, `map`, or `flat_map`, I realize that `map_groups` op does not support a `fn_constructor_kwargs` arg.

The workaround w/o using an actor means that I have to initialize such as state per call. Really look forward to ray 2.10 for this feature.",issue well relatively expensive state initialize actor however unlike map realize support actor initialize state per call really look forward ray feature,issue,positive,negative,negative,negative,negative,negative
1964841870,"> The mentioned script runs into issues when I try to upgrade it to python 3.11

could you talk about what specific issues this runs into? why moving it into the build container fixes the issue?",script try upgrade python could talk specific moving build container issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1964760651,Object store memory is already reserved. `memory` is calculated from available memory - object store memory.,object store memory already reserved memory calculated available memory object store memory,issue,negative,positive,positive,positive,positive,positive
1964759620,Seems object store memory is already reserved.,object store memory already reserved,issue,negative,neutral,neutral,neutral,neutral,neutral
1964754763,There are some lint errors but otherwise exciting! Let's run some release tests to validate. Thankks.,lint otherwise exciting let run release validate,issue,positive,positive,positive,positive,positive,positive
1964589511,"Found the solution. In the checking point, there is a python script named zero_to_fp32.py. I use the script to combine the zero shard model to a single file model

`python zero_to_fp32.py checkpoint_000002/  pytorch_model.bin`


",found solution point python script use script combine zero shard model single file model python,issue,negative,negative,neutral,neutral,negative,negative
1964439665,"Used different `storage_kwargs` to solve this issue:

> The filesystem can be instantiated for different use cases based on a variety of storage_options combinations. The following list describes some common use cases utilizing AzureBlobFileSystem, i.e. protocols abfsor az. Note that all cases require the account_name argument to be provided:
> Anonymous connection to public container: storage_options={'account_name': ACCOUNT_NAME, 'anon': True} will assume the ACCOUNT_NAME points to a public container, and attempt to use an anonymous login. Note, the default value for anon is True.
> Auto credential solving using Azure's DefaultAzureCredential() library: storage_options={'account_name': ACCOUNT_NAME, 'anon': False} will use [DefaultAzureCredential](https://learn.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) to get valid credentials to the container ACCOUNT_NAME. DefaultAzureCredential attempts to authenticate via the [mechanisms and order visualized here](https://learn.microsoft.com/en-us/python/api/overview/azure/identity-readme?view=azure-python#defaultazurecredential).
> Azure ServicePrincipal: tenant_id, client_id, and client_secret are all used as credentials for an Azure ServicePrincipal: e.g. storage_options={'account_name': ACCOUNT_NAME, 'tenant_id': TENANT_ID, 'client_id': CLIENT_ID, 'client_secret': CLIENT_SECRET}.

https://github.com/fsspec/adlfs/blob/main/README.md",used different solve issue different use based variety following list common use note require argument provided anonymous connection public container true assume public container attempt use anonymous login note default value anon true auto credential azure library false use get valid container authenticate via order azure used azure,issue,positive,negative,neutral,neutral,negative,negative
1964365405,"@kouroshHakha and @ArturNiederfahrenhorst Can help take a look or answer my questions? 
",help take look answer,issue,negative,neutral,neutral,neutral,neutral,neutral
1964354670,"I am using the above example on 5 node with 8*A100(32GB GPU Memory) and 92 CPU(RAM 670GB), I always get OOM on CPU. Based on the ray document, [training_fucntion](https://github.com/ray-project/ray/blob/master/doc/source/templates/04_finetuning_llms_with_deepspeed/finetune_hf_llm.py#L219) needs to run on each worker, and each worker has 1 GPU and they share the CPU and RAM on the node, which means that there are 8 workers per node see the [configuration](https://github.com/ray-project/ray/blob/master/doc/source/templates/04_finetuning_llms_with_deepspeed/finetune_hf_llm.py#L759). For the llama2 70b model, it needs 720GB RAM to load the model, see the calculation below, 
```
Model parameters: 70(billion parameters) * 2(FP16) ≈ 160GB
Optimizer states: 70(billion parameters) * 2(momentums per param) * 4 (FP32) ≈ 560GB
```
The calculation above is based on this [ray document](https://docs.ray.io/en/latest/train/examples/lightning/vicuna_13b_lightning_deepspeed_finetune.html)

Here are my questions:

1. Does each worker run the [training_fucntion](https://github.com/ray-project/ray/blob/master/doc/source/templates/04_finetuning_llms_with_deepspeed/finetune_hf_llm.py#L219)? 
1.1) if yes, how can the model loaded into the CPU using g5.48xlarge instance as there is only 760GB RAM? Because there are 8 workers on each node, it needs 560GB*8=4480GB, because AutoModelForCausalLM.from_pretrained() load the model into CPU. 
1.2) if no. Which worker run the function? How does the data shared between workers, in this case, the data is the model? 
",example node memory ram always get based ray document need run worker worker share ram node per node see configuration llama model need ram load model see calculation model billion billion per param calculation based ray document worker run yes model loaded instance ram node need load model worker run function data case data model,issue,positive,neutral,neutral,neutral,neutral,neutral
1963763181,"@anirjoshi Basically you just set `""use_lstm"": True` in the `model` dictionary in `AlgorithmConfig.training()`. See here for a more elaborate example: https://github.com/ray-project/ray/blob/master/rllib/examples/custom_recurrent_rnn_tokenizer.py 

You don't need the custom tokenizer shown there, just set `""use_lstm"": True`.",basically set true model dictionary see elaborate example need custom shown set true,issue,positive,positive,positive,positive,positive,positive
1963705288,"Regarding the original issue, it seems that (roughly, I haven't thoroughly tested every possibility):
1. `storage_path` sets where the checkpoints are stored
2. `local_dir` sets where logs and some other stuff stored
3. `_get_defaults_results_dir` is called before `os.environ[""RAY_AIR_LOCAL_CACHE_DIR""] = local_dir `, notably [here](https://github.com/ray-project/ray/blob/c1535c0bc5f94737cfd228736b781b1b872e10f0/python/ray/tune/impl/tuner_internal.py#L135), which will store the `tuner.pkl` file in the wrong place
4. if `local_dir` is not specified, then that duplicates the logs, but not the checkpoints

In any case, as a workaround to have the logs in the right place I have:
```python
logdir = ....

os.environ[""TUNE_RESULT_DIR""] = logdir # should work with RAY_AIR_LOCAL_CACHE_DIR or TEST_TMPDIR too
tuner  = Tuner(
  ...
  storage_path = logdir,
  local_dir = logdir,
)

tuner.fit()
```

For example, leaving `local_dir` as `None` will result in the following in the CLI logs:
```
View detailed results here: /home/user/project-dir/PPO.2024-02-26.09:13:50
To visualize your results with TensorBoard, run: `tensorboard --logdir /home/user/ray_results/PPO.2024-02-26.09:13:50`
```
",regarding original issue roughly thoroughly tested every possibility stuff notably store file wrong place case right place python work tuner tuner example leaving none result following view detailed visualize run,issue,negative,positive,positive,positive,positive,positive
1963365883,"@kira-lin I checked your tests, I don't think you need real HPU hardware to run them",checked think need real hardware run,issue,negative,positive,positive,positive,positive,positive
1963301634,"> I think the easiest way would be to save both stdout_logger and logger in _initialize_logger() as attributes of the DatasetLogger. Then we can modify the stdout_logger's info and debug methods to not do anything, so we do not log to stdout in these cases.

Got it, @scottjlee, @bveeramani - how about leaving this as a followup? Two loggers sound complicated to me, and I want us to be able to clear up logging in 2.10.
",think easiest way would save logger modify anything log got leaving two sound complicated want u able clear logging,issue,positive,positive,positive,positive,positive,positive
1963238947,"I'm sorry for opening this issue, I now understand what is happening... I was thinking as if a copy of the list was being passed in each submit call, but that's not the case...",sorry opening issue understand happening thinking copy list submit call case,issue,negative,negative,negative,negative,negative,negative
1963202109,"> What's the difference between these tests and the existing tests in test_hpu.py? Seems you don't really need HPU hardware to run these tests.

These tests require actual HPUs to run. We need this to make sure Ray scheduling is working for HPU. These are adapted from some GPU tests.",difference really need hardware run require actual run need make sure ray working,issue,negative,positive,positive,positive,positive,positive
1963191960,"@mattip Hi mattip, the windows computer A and windows computer B are on the same LAN. For example, the A's ip is 192.168.1.2, the B's ip is 192.168.1.3. I set the computer A as the head node.

On A,  i set the variable RAY_ENABLE_WINDOWS_OR_OSX_CLUSTER=1, execute the command ""ray start --head --node-ip-address=localhost --port=6666 "".

On B, i set the variable RAY_ENABLE_WINDOWS_OR_OSX_CLUSTER=1, execute the command ""ray start --address='192.168.1.2:6666' "" (B has joined the cluster).
Then computer A run this code.

Tips: only use the CPU to run the demo script. In my case, run the ""ray status"" command, computer A shows 8 cpus, computer B shows 8 cpus too, the cluster shows total 16 cpus. So i set the ""ScalingConfig(num_workers=8(>=8), use_gpu=False), try to call the two computers' cpus at the same time.
",hi computer computer lan example set computer head node set variable execute command ray start head set variable execute command ray start cluster computer run code use run script case run ray status command computer computer cluster total set try call two time,issue,negative,neutral,neutral,neutral,neutral,neutral
1962779368,@viirya This will be included in 2.10 (release in the next month). You can use our [nightly build](https://docs.ray.io/en/master/ray-overview/installation.html#daily-releases-nightlies) if you want to include this fix asap. ,included release next month use nightly build want include fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1962778333,"Hi, thanks for fixing this. I also encountered this bug today with ray 2.9.2. When I tried to fix it and just found it is already fixed in latest branch.

But I just checked ray 2.9.3 which is released 2 days ago. This fix seems not included there. Actually this commit looks like merged in 2/13, it should be in the time range of ray 2.9.2 - ray 2.9.3 (https://github.com/ray-project/ray/compare/ray-2.9.2...ray-2.9.3, where latest commit is 2/15).

Any idea why this fix isn't included and when a new release will include this fix? Thanks.",hi thanks fixing also bug today ray tried fix found already fixed latest branch checked ray day ago fix included actually commit like time range ray ray latest commit idea fix included new release include fix thanks,issue,positive,positive,positive,positive,positive,positive
1962735036,"If this is an ipv6 issue, is the fix something like below maybe?  If someone can confirm this seems right-ish I can figure out how to build things locally and get a PR open if it works.  I imagine if this does work then all the other calls to socket.gethostbyname would get replaced with the utility helper too?  I understand that this may not be all of what's needed for ipv6 support, but it also might be close.

In node.py:125:
`self._localhost = socket.gethostbyname(""localhost"")`
should be
`self._localhost = get_host_addr(""localhost"")`

with get_host_addr added to _private/utils.py:

```
def get_host_addr(name: str) -> str:
    """"""Returns the host address of the node.""""""
    # Get a list of address info tuples for 'name'
    info = socket.getaddrinfo(name, None)

    # Filter out the IPv4 and IPv6 addresses
    ipv4 = [addr[4][0] for addr in info if addr[0] == socket.AF_INET]
    ipv6 = [addr[4][0] for addr in info if addr[0] == socket.AF_INET6]

    # Choose an address to use, preferring IPv6 if available
    if ipv6:
        return ipv6[0]
    elif ipv4:
        return ipv4[0]
    else:
        raise Exception(""No IPv4 or IPv6 address found for "" + name)
```",issue fix something like maybe someone confirm figure build locally get open work imagine work would get utility helper understand may support also might close added name host address node get list address name none filter choose address use available return return else raise exception address found name,issue,positive,positive,positive,positive,positive,positive
1962392867,"Hi I see that #41194 has been merged to master. However, looking through the changelog on https://github.com/ray-project/ray/releases, I was unable to find any mention of this new feature. Is this a supported capability now to set retry_exceptions on actor methods?
",hi see master however looking unable find mention new feature capability set actor,issue,negative,negative,negative,negative,negative,negative
1962296441,"After I upgraded to ray 2.9.2, the issue looks like resolved. Ray tasks can scheduled evenly to nodes under `SPREAD` placement strategy.  I also don't see `RuntimeError: Connection reset by peer` error now.",ray issue like resolved ray evenly spread placement strategy also see connection reset peer error,issue,negative,neutral,neutral,neutral,neutral,neutral
1962232999,old pipeline deprecated. new pipeline has been reworked.,old pipeline new pipeline reworked,issue,negative,positive,positive,positive,positive,positive
1962215092,remove blocker tag for flaky window tests,remove blocker tag flaky window,issue,negative,neutral,neutral,neutral,neutral,neutral
1962213510,"> It may also be helpful to look at the logs in %TEMP%\ray to see if the worker is failing to start for some reason

I think it's probably because I'm using win11 that I can run correctly on Ubuntu",may also helpful look temp see worker failing start reason think probably win run correctly,issue,positive,positive,positive,positive,positive,positive
1962195915,this was a manual postmerge run for testing purposes. closing since not relevant.,manual run testing since relevant,issue,negative,positive,positive,positive,positive,positive
1962060330,"> @peytondmurray looks like the RTD build failed -- would you mind fixing it so that we can review the rendered documentation?

Will do. The build is broken by f880528fd9, I think. I'm working on an issue and fix for it now.",like build would mind fixing review documentation build broken think working issue fix,issue,negative,negative,negative,negative,negative,negative
1961897546,"ok, I am merging this. nothing that requires blocking. we can also fix small things afterwards.",nothing blocking also fix small afterwards,issue,negative,negative,negative,negative,negative,negative
1961683730,"Turns out it's because Open3d has a cuda and cpu version. It doesn't by trying to import it twice. Ray doesn't allow the gpu to be used twice and multi threaded unlike torch.multiprocessing easily or I haven't seen any guide that does it properly. I moved to cpu only version and it works. I try to set the number of gpus in the ray but it works but doesn't make it distributed which torch multiprocessing does easily. Inconvenience, but appears one need to do this when using ray to get it to work properly.

The GPU not working with multiprocessing and blocking when I set num_gpus=1  may be a feature instead of a issue, so I will just close it here. 

Edit:
You have to set a fixed capacity that matches up to each GPU to make it parralelized",turn version trying import twice ray allow used twice threaded unlike easily seen guide properly version work try set number ray work make distributed torch easily inconvenience one need ray get work properly working blocking set may feature instead issue close edit set fixed capacity make,issue,negative,positive,positive,positive,positive,positive
1961634637,I am experiencing similar issue too. In the ray log directly I don't see any nsys file being generated. I also ssh it into the worker and tried `nsys sessions list` but don't see any active nsys session. I might be missing something and would really appreciate your help!,similar issue ray log directly see file also worker tried session list see active session might missing something would really appreciate help,issue,positive,negative,neutral,neutral,negative,negative
1961564252,"> The speed with which you crank out these algo implementations on the new stack is breathtaking :) Great work.
> 
> * A few comment-related nits.
> * One bigger item still to complete: Could we fix the synchronous sample utility to work on the new stack in this PR? This would close all these open TODOs for good.

I take another look into the PR for the `synchronous_parallel_sample` - two tests are still failing. Let's fix these and merge. ",speed crank new stack great work one bigger item still complete could fix synchronous sample utility work new stack would close open good take another look two still failing let fix merge,issue,positive,positive,positive,positive,positive,positive
1961354103,"Is there any update already? @rkooo567, @anyscalesam we are quite dependent on this as we would need async support and are limited to using cythonized code only",update already quite dependent would need support limited code,issue,negative,negative,neutral,neutral,negative,negative
1961072700,"There's one more thing that I've just noticed, that Ray is logging also to `/tmp/ray` AND a lot of randomly generated `/tmp/tmp....` directories. I've just cleaned up 100GB of space from temp dirs of previous experiments. 
With this I need to change the severity to HIGH, because can't run experiments without filling up the disk.",one thing ray logging also lot randomly space temp previous need change severity high ca run without filling disk,issue,negative,negative,negative,negative,negative,negative
1960941048,"> maybe consider splitting this PR to smaller ones? is that possible?

Good question. I thought about this PR together because of the large code owner impact. As I see it most of the changes are:

1. Design and styling of the example gallery
2. Changes to the build machinery
3. Updates to the individual examples

We can't really break apart 2 and 3 very easily - the updates to the examples are driven by the changes to the sphinx build tooling. However, they actually have very little substance (and therefore I hope a light review burden): most of the changes are adding `orphan` or updating internal references, but unfortunately this is where the bulk of the changes by number of files have been made (42 example files). Remembering that these changes have no content beyond reference updates (checked by linkcheck as part of CI) or adding `orphan` front matter, I think this cuts down the amount of actual review needed by a lot. Adding an examples configuration file for each library (data, train, serve) constitutes another 3 files.

Modifications to the build machinery account for the substance of this PR. Accommodating the large changes to the example gallery in the design mockups meant that we couldn't use a static `examples.rst`, but instead we need to pull in examples at build time from each of the individual Ray libraries. I do this using `BeautifulSoup` following existing design patterns, but what that means is that the impact of that is that the design/styling changes were affected by the build machinery. So for these reasons I haven't broken it down further.

I will mention that we have already broken this down somewhat - changes to the example pages for the rest of the Ray libraries remain. So there will be work following this PR, but it's going to be significantly less. I'm open to ideas about how to further break this down, though.",maybe consider splitting smaller possible good question thought together large code owner impact see design styling example gallery build machinery individual ca really break apart easily driven sphinx build tooling however actually little substance therefore hope light review burden orphan internal unfortunately bulk number made example content beyond reference checked part orphan front matter think amount actual review lot configuration file library data train serve another build machinery account substance accommodating large example gallery design meant could use static instead need pull build time individual ray following design impact affected build machinery broken mention already broken somewhat example rest ray remain work following going significantly le open break though,issue,negative,positive,neutral,neutral,positive,positive
1960788518,@rkooo567 flaky client mode test are not release blocking if i recall correctly?,flaky client mode test release blocking recall correctly,issue,negative,neutral,neutral,neutral,neutral,neutral
1960776588,"@aslonnie Sorry, I originally did not assign you as this is docs only. Will do in the future!",sorry originally assign future,issue,negative,negative,neutral,neutral,negative,negative
1960776419,maybe consider splitting this PR to smaller ones? is that possible?,maybe consider splitting smaller possible,issue,negative,neutral,neutral,neutral,neutral,neutral
1960773993,"@peytondmurray , when a PR needs me, could you add me as reviewer instead of assignee in the future?

@angelinalg , please ping @aslonnie rather than ""@ lonnie"" (which is not me)",need could add reviewer instead assignee future please ping rather,issue,negative,neutral,neutral,neutral,neutral,neutral
1960757819,"> > Rather than manually specifying log_to_stdout=False, would it be difficult to configure the logger so that we don't print info and debug levels by default?
> 
> We need to make sure they are still logging to log file though, and not interfere w/ `warn`/`error`-level. @scottjlee do you know if we have an easy way to do that? Thanks.

I think the easiest way would be to save both `stdout_logger` and `logger` in `_initialize_logger()` as attributes of the `DatasetLogger`. Then we can modify the `stdout_logger`'s `info` and `debug` methods to not do anything, so we do not log to stdout in these cases.",rather manually would difficult configure logger print default need make sure still logging log file though interfere warn error know easy way thanks think easiest way would save logger modify anything log,issue,positive,positive,positive,positive,positive,positive
1960730170,"This is resources on the cluster:

```
>>> pprint(ray.available_resources())
{'CPU': 30.0,
 'CPU_group_1_dd1738c1a458ad5395c3805f1b7d02000000': 1.0,
 'CPU_group_2_dd1738c1a458ad5395c3805f1b7d02000000': 1.0,
 'CPU_group_3_dd1738c1a458ad5395c3805f1b7d02000000': 1.0,
 'CPU_group_4_dd1738c1a458ad5395c3805f1b7d02000000': 1.0,
 'CPU_group_5_dd1738c1a458ad5395c3805f1b7d02000000': 1.0,
 'bundle_group_0_dd1738c1a458ad5395c3805f1b7d02000000': 1000.0,
 'bundle_group_1_dd1738c1a458ad5395c3805f1b7d02000000': 1000.0,
 'bundle_group_2_dd1738c1a458ad5395c3805f1b7d02000000': 1000.0,
 'bundle_group_3_dd1738c1a458ad5395c3805f1b7d02000000': 1000.0,
 'bundle_group_4_dd1738c1a458ad5395c3805f1b7d02000000': 1000.0,
 'bundle_group_5_dd1738c1a458ad5395c3805f1b7d02000000': 1000.0,
 'bundle_group_dd1738c1a458ad5395c3805f1b7d02000000': 6000.0,
 'memory': 420000000000.0,
 'node:240.56.163.77': 1.0,
 'node:240.56.165.185': 1.0,
 'node:240.56.175.167': 1.0,
 'node:240.56.180.252': 1.0,
 'node:240.56.184.189': 1.0,
 'node:240.56.185.90': 1.0,
 'node:240.56.187.109': 1.0,
 'node:__internal_head__': 1.0,
 'object_store_memory': 125711252970.0}
```

```
>>> pprint(ray.nodes())
...
 {'Alive': True,
  'Labels': {'ray.io/node_id': '4a1b1e88bf2e55cd9c238ca74b0db3d41667754b3964d60ebb38d239'},
  'MetricsExportPort': 8080,
  'NodeID': '4a1b1e88bf2e55cd9c238ca74b0db3d41667754b3964d60ebb38d239',
  'NodeManagerAddress': '240.56.175.167',
  'NodeManagerHostname': 'tune-t5-small-raycluster-vgjj8-worker-small-group-k8d22',
  'NodeManagerPort': 43285,
  'NodeName': '240.56.175.167',
  'ObjectManagerPort': 46635,
  'ObjectStoreSocketName': '/tmp/ray/session_2024-02-21_23-17-16_864422_8/sockets/plasma_store',
  'RayletSocketName': '/tmp/ray/session_2024-02-21_23-17-16_864422_8/sockets/raylet',
  'Resources': {'CPU': 6.0,
                'memory': 60000000000.0,
                'node:240.56.175.167': 1.0,
                'object_store_memory': 17969018880.0},
  'RuntimeEnvAgentPort': 62986,
  'alive': True},
 {'Alive': True,
  'Labels': {'ray.io/node_id': '2b749db89d675b739244e62254b9541a2b32387621732a941701122b'},
  'MetricsExportPort': 8080,
  'NodeID': '2b749db89d675b739244e62254b9541a2b32387621732a941701122b',
  'NodeManagerAddress': '240.56.180.252',
  'NodeManagerHostname': 'rayjob-finetune-t5-small-raycluster-vgjj8-head-v94c6',
  'NodeManagerPort': 40283,
  'NodeName': '240.56.180.252',
  'ObjectManagerPort': 34131,
  'ObjectStoreSocketName': '/tmp/ray/session_2024-02-21_23-17-16_864422_8/sockets/plasma_store',
  'RayletSocketName': '/tmp/ray/session_2024-02-21_23-17-16_864422_8/sockets/raylet',
  'Resources': {'memory': 60000000000.0,
                'node:240.56.180.252': 1.0,
                'node:__internal_head__': 1.0,
                'object_store_memory': 17898461184.0},
  'RuntimeEnvAgentPort': 60554,
  'alive': True},
 {'Alive': True,
  'Labels': {'ray.io/node_id': 'ddb8af2ba2b22140064e667e6e5ef6dc98fde358d75575966e26a48a'},
  'MetricsExportPort': 8080,
  'NodeID': 'ddb8af2ba2b22140064e667e6e5ef6dc98fde358d75575966e26a48a',
  'NodeManagerAddress': '240.56.163.77',
  'NodeManagerHostname': 'tune-t5-small-raycluster-vgjj8-worker-small-group-sb94f',
  'NodeManagerPort': 44221,
  'NodeName': '240.56.163.77',
  'ObjectManagerPort': 44875,
  'ObjectStoreSocketName': '/tmp/ray/session_2024-02-21_23-17-16_864422_8/sockets/plasma_store',
  'RayletSocketName': '/tmp/ray/session_2024-02-21_23-17-16_864422_8/sockets/raylet',
  'Resources': {'CPU': 6.0,
                'memory': 60000000000.0,
                'node:240.56.163.77': 1.0,
                'object_store_memory': 17968978329.0},
  'RuntimeEnvAgentPort': 49162,
  'alive': True}]
```

",cluster true true true true true true,issue,positive,positive,positive,positive,positive,positive
1960729954,"If this is due to my misunderstanding of what `ray.cluster_resources()` shows, please let me know.

I'm also wondering if you will make the document clear? As I can find, for example for the API `ray.available_resources `, its document https://docs.ray.io/en/latest/ray-core/api/doc/ray.available_resources.html doesn't explain the output.

As I quoted in https://github.com/ray-project/ray/issues/43261#issuecomment-1958865726, there are several `'CPU_group_1_dd1738c1a458ad5395c3805f1b7d02000000': 1.0`, `'node:240.56.163.77': 1.0`. What does it exactly mean?

I have 6 CPU per node, so obviously the CPU group is not same as CPU. I have 6 worker nodes, there are only 5 CPU groups?

",due misunderstanding please let know also wondering make document clear find example document explain output several exactly mean per node obviously group worker,issue,negative,negative,neutral,neutral,negative,negative
1960643925,"Hmm, something's up with the RTD configuration. I can't tell exactly what is wrong without spending a little more time troubleshooting, but I think the changes in #43189 have something to do with it. Will investigate more and possibly make a separate issue/PR.",something configuration ca tell exactly wrong without spending little time think something investigate possibly make separate,issue,negative,negative,negative,negative,negative,negative
1960584029,"Hi @stephanie-wang, you mentioned that you tested the code on an Intel machine.  Would it be possible to share the CPU info, and the version of *openmp* packages in your env?  On my end I have different Xeon Golds to choose from.  Would like to replicate your run to the best I can.  Thanks.",hi tested code machine would possible share version end different choose would like replicate run best thanks,issue,positive,positive,positive,positive,positive,positive
1960566632,Just fixed a merge conflict; thanks for your patience while this builds!,fixed merge conflict thanks patience,issue,negative,positive,positive,positive,positive,positive
1960372865,Boop @raulchen can you squash and re-run on head; customer is still waiting on a response < 7d. Please confirm internally as well to see if it solves the whitespace Jobs issue before closing the loop with the customer.,squash head customer still waiting response please confirm internally well see issue loop customer,issue,positive,neutral,neutral,neutral,neutral,neutral
1960362252,"TODO: think about how to handle the ""filter"" area at the top",think handle filter area top,issue,negative,positive,positive,positive,positive,positive
1960361970,Oh sorry I took this over and it's fixed now,oh sorry took fixed,issue,negative,negative,negative,negative,negative,negative
1960333275,"> Rather than manually specifying log_to_stdout=False, would it be difficult to configure the logger so that we don't print info and debug levels by default?

We need to make sure they are still logging to log file though, and not interfere w/ `warn`/`error`-level. @scottjlee do you know if we have an easy way to do that? Thanks.",rather manually would difficult configure logger print default need make sure still logging log file though interfere warn error know easy way thanks,issue,negative,positive,positive,positive,positive,positive
1960272334,"@jjyao Do we need to include this issue in Ray 2.10? If not, I can take the issue.",need include issue ray take issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1960258917,"Sorry, I am a slow learner.
Can you provide instructions exactly how to reproduce the problem (only what doesn't work, not what does work)? Something like:
On windows computer A I set these environment variables and run this code. Then on computer B I set these other variables and run this other code.",sorry slow learner provide exactly reproduce problem work work something like computer set environment run code computer set run code,issue,negative,negative,negative,negative,negative,negative
1960248317,@stephanie-wang is this on track to make branch cut for Ray 2.10?,track make branch cut ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1959995390,"Hi @WeichenXu123 - seeing the followed unit tests failure from CI, could you take a look?

```
python/ray/data/tests/test_raydp.py::test_raydp_roundtrip FAILED         [ 20%]
  | python/ray/data/tests/test_raydp.py::test_raydp_to_spark PASSED          [ 40%]
  | python/ray/data/tests/test_raydp.py::test_from_spark_e2e FAILED          [ 60%]
  | python/ray/data/tests/test_raydp.py::test_raydp_to_torch_iter FAILED     [ 80%]
  | python/ray/data/tests/test_raydp.py::test_to_pandas FAILED               [100%]
  |  
  | =================================== FAILURES ===================================
  | _____________________________ test_raydp_roundtrip _____________________________
  |  
  | spark = <pyspark.sql.session.SparkSession object at 0x7f3c50259370>
  |  
  | def test_raydp_roundtrip(spark):
  | spark_df = spark.createDataFrame([(1, ""a""), (2, ""b""), (3, ""c"")], [""one"", ""two""])
  | rows = [(r.one, r.two) for r in spark_df.take(3)]
  | >       ds = ray.data.from_spark(spark_df)
  |  
  | python/ray/data/tests/test_raydp.py:28:
  | _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
  | /rayci/python/ray/data/read_api.py:2557: in from_spark
  | return raydp.spark.spark_dataframe_to_ray_dataset(df, parallelism)
  | /opt/miniconda/lib/python3.9/site-packages/raydp/spark/dataset.py:201: in spark_dataframe_to_ray_dataset
  | df = df.repartition(parallelism)
  | /opt/miniconda/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1659: in repartition
  | return DataFrame(self._jdf.repartition(numPartitions), self.sparkSession)
  | /opt/miniconda/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
  | return_value = get_return_value(
  | _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
  |  
  | a = ('xro69', <py4j.clientserver.JavaClient object at 0x7f3c501d7e50>, 'o64', 'repartition')
  | kw = {}, converted = IllegalArgumentException()
  |  
  | def deco(*a: Any, **kw: Any) -> Any:
  | try:
  | return f(*a, **kw)
  | except Py4JJavaError as e:
  | converted = convert_exception(e.java_exception)
  | if not isinstance(converted, UnknownException):
  | # Hide where the exception came from that shows a non-Pythonic
  | # JVM exception message.
  | >               raise converted from None
  | E               pyspark.errors.exceptions.captured.IllegalArgumentException: requirement failed: Number of partitions (-1) must be positive.
  |  
  | /opt/miniconda/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:175: IllegalArgumentException
  | ---------------------------- Captured stderr setup -----------------------------
  | 2024-02-21 09:23:14,592	WARNING services.py:1995 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 2684354560 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=4.70gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.
  | 2024-02-21 09:23:14,718	INFO worker.py:1759 -- Started a local Ray instance.
  | Setting default log level to ""WARN"".
  | To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
  | 24/02/21 09:23:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  | ----------------------------- Captured stderr call -----------------------------
  |  
  | _____________________________ test_from_spark_e2e ______________________________
  |  
  | spark = <pyspark.sql.session.SparkSession object at 0x7f3aea631640>
  |  
  | def test_from_spark_e2e(spark):
  | spark_df = spark.createDataFrame([(1, ""a""), (2, ""b""), (3, ""c"")], [""one"", ""two""])
  |  
  | rows = [(r.one, r.two) for r in spark_df.take(3)]
  | >       ds = ray.data.from_spark(spark_df)
  |  
  | python/ray/data/tests/test_raydp.py:49:
  | _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
  | /rayci/python/ray/data/read_api.py:2557: in from_spark
  | return raydp.spark.spark_dataframe_to_ray_dataset(df, parallelism)
  | /opt/miniconda/lib/python3.9/site-packages/raydp/spark/dataset.py:201: in spark_dataframe_to_ray_dataset
  | df = df.repartition(parallelism)
  | /opt/miniconda/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1659: in repartition
  | return DataFrame(self._jdf.repartition(numPartitions), self.sparkSession)
  | /opt/miniconda/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
  | return_value = get_return_value(
  | _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
  |  
  | a = ('xro210', <py4j.clientserver.JavaClient object at 0x7f3c501d7e50>, 'o205', 'repartition')
  | kw = {}, converted = IllegalArgumentException()
  |  
  | def deco(*a: Any, **kw: Any) -> Any:
  | try:
  | return f(*a, **kw)
  | except Py4JJavaError as e:
  | converted = convert_exception(e.java_exception)
  | if not isinstance(converted, UnknownException):
  | # Hide where the exception came from that shows a non-Pythonic
  | # JVM exception message.
  | >               raise converted from None
  | E               pyspark.errors.exceptions.captured.IllegalArgumentException: requirement failed: Number of partitions (-1) must be positive.
  |  
  | /opt/miniconda/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:175: IllegalArgumentException
  | ---------------------------- Captured stderr setup -----------------------------
  | 2024-02-21 09:23:52,968	WARNING services.py:1995 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 2684350464 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=4.53gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.
  | 2024-02-21 09:23:53,096	INFO worker.py:1759 -- Started a local Ray instance.
  | ----------------------------- Captured stderr call -----------------------------
  |  
  | ___________________________ test_raydp_to_torch_iter ___________________________
  |  
  | spark = <pyspark.sql.session.SparkSession object at 0x7f3aea6f83d0>
  |  
  | def test_raydp_to_torch_iter(spark):
  | spark_df = spark.createDataFrame([(1, 0), (2, 0), (3, 1)], [""feature"", ""label""])
  | data_size = spark_df.count()
  | features = [r[""feature""] for r in spark_df.take(data_size)]
  | features = torch.tensor(features).reshape(data_size, 1)
  | labels = [r[""label""] for r in spark_df.take(data_size)]
  | labels = torch.tensor(labels).reshape(data_size, 1)
  | >       ds = ray.data.from_spark(spark_df)
  |  
  | python/ray/data/tests/test_raydp.py:68:
  | _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
  | /rayci/python/ray/data/read_api.py:2557: in from_spark
  | return raydp.spark.spark_dataframe_to_ray_dataset(df, parallelism)
  | /opt/miniconda/lib/python3.9/site-packages/raydp/spark/dataset.py:201: in spark_dataframe_to_ray_dataset
  | df = df.repartition(parallelism)
  | /opt/miniconda/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1659: in repartition
  | return DataFrame(self._jdf.repartition(numPartitions), self.sparkSession)
  | /opt/miniconda/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
  | return_value = get_return_value(
  | _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
  |  
  | a = ('xro284', <py4j.clientserver.JavaClient object at 0x7f3c501d7e50>, 'o277', 'repartition')
  | kw = {}, converted = IllegalArgumentException()
  |  
  | def deco(*a: Any, **kw: Any) -> Any:
  | try:
  | return f(*a, **kw)
  | except Py4JJavaError as e:
  | converted = convert_exception(e.java_exception)
  | if not isinstance(converted, UnknownException):
  | # Hide where the exception came from that shows a non-Pythonic
  | # JVM exception message.
  | >               raise converted from None
  | E               pyspark.errors.exceptions.captured.IllegalArgumentException: requirement failed: Number of partitions (-1) must be positive.
  |  
  | /opt/miniconda/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:175: IllegalArgumentException
  | ---------------------------- Captured stderr setup -----------------------------
  | 2024-02-21 09:24:08,259	WARNING services.py:1995 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 2684350464 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=4.52gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.
  | 2024-02-21 09:24:08,393	INFO worker.py:1759 -- Started a local Ray instance.
  | ----------------------------- Captured stderr call -----------------------------
  |  
  | ________________________________ test_to_pandas ________________________________
  |  
  | spark = <pyspark.sql.session.SparkSession object at 0x7f3c06bf3d90>
  |  
  | def test_to_pandas(spark):
  | df = spark.range(100)
  | >       ds = ray.data.from_spark(df)
  |  
  | python/ray/data/tests/test_raydp.py:76:
  | _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
  | /rayci/python/ray/data/read_api.py:2557: in from_spark
  | return raydp.spark.spark_dataframe_to_ray_dataset(df, parallelism)
  | /opt/miniconda/lib/python3.9/site-packages/raydp/spark/dataset.py:201: in spark_dataframe_to_ray_dataset
  | df = df.repartition(parallelism)
  | /opt/miniconda/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1659: in repartition
  | return DataFrame(self._jdf.repartition(numPartitions), self.sparkSession)
  | /opt/miniconda/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
  | return_value = get_return_value(
  | _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
  |  
  | a = ('xro338', <py4j.clientserver.JavaClient object at 0x7f3c501d7e50>, 'o335', 'repartition')
  | kw = {}, converted = IllegalArgumentException()
  |  
  | def deco(*a: Any, **kw: Any) -> Any:
  | try:
  | return f(*a, **kw)
  | except Py4JJavaError as e:
  | converted = convert_exception(e.java_exception)
  | if not isinstance(converted, UnknownException):
  | # Hide where the exception came from that shows a non-Pythonic
  | # JVM exception message.
  | >               raise converted from None
  | E               pyspark.errors.exceptions.captured.IllegalArgumentException: requirement failed: Number of partitions (-1) must be positive.
  |  
  | /opt/miniconda/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:175: IllegalArgumentException
```",hi seeing unit failure could take look spark object spark one two return parallelism parallelism repartition return object converted try return except converted converted hide exception came exception message raise converted none requirement number must positive setup warning warning object store instead available harm performance may able free space inside docker container increase size passing run add list ray cluster make sure set available ram local ray instance setting default log level warn adjust logging level use use warn unable load library platform class applicable call spark object spark one two return parallelism parallelism repartition return object converted try return except converted converted hide exception came exception message raise converted none requirement number must positive setup warning warning object store instead available harm performance may able free space inside docker container increase size passing run add list ray cluster make sure set available ram local ray instance call spark object spark feature label feature label return parallelism parallelism repartition return object converted try return except converted converted hide exception came exception message raise converted none requirement number must positive setup warning warning object store instead available harm performance may able free space inside docker container increase size passing run add list ray cluster make sure set available ram local ray instance call spark object spark return parallelism parallelism repartition return object converted try return except converted converted hide exception came exception message raise converted none requirement number must positive,issue,positive,positive,positive,positive,positive,positive
1959927318,what about in a simple trainable function that only takes a config as argument.,simple trainable function argument,issue,negative,neutral,neutral,neutral,neutral,neutral
1959911348,Thanks @joncarter1 for reporting this. I'll fix it in the next release.,thanks fix next release,issue,negative,positive,neutral,neutral,positive,positive
1959684017,not anymore if I submit the job to a cluster. You can close it,submit job cluster close,issue,negative,neutral,neutral,neutral,neutral,neutral
1959257896,"@aslonnie  Here is my draft for the script. Once the PyPI credential storage method gets figured out, I think we can port this in the `release-automation` BK pipeline, like running it as a step/group before running wheel validation group?",draft script credential storage method figured think port pipeline like running running wheel validation group,issue,negative,neutral,neutral,neutral,neutral,neutral
1958983943,How can I do this anyways? I just have a small local cluster for which it doesnt make sense to add a cloud provider. Any recommendations for ways to for example set the head node up as a NFS server as well?,anyways small local cluster doesnt make sense add cloud provider way example set head node server well,issue,negative,negative,negative,negative,negative,negative
1958964587,"Thanks for checking this out!

My use case involves running a system where Ray actors are defined in multiple different environments, and these environments are built and deployed either via Docker or via Nix. I'd like to mix and match the way the environments are built and deployed while having them all coexist in the one Ray cluster.

My feeling is that this flexibility can be permitted without adding much complexity, but I defer to you on this.

Re interface, I agree this change is a hacky way to go about it. Ideally the top-level `container_name` would set the default, and the per-node `container_name` would override the default when provided. If you think it's a good idea I can look at making this change in this MR.",thanks use case running system ray defined multiple different built either via docker via nix like mix match way built coexist one ray cluster feeling flexibility permitted without much complexity defer interface agree change hacky way go ideally would set default would override default provided think good idea look making change,issue,positive,positive,positive,positive,positive,positive
1958868156,"Hmm, so looks like the cpu setting is correct. And what `ray.cluster_resources()` shows `'node:240.56.105.101': 1.0` is not cpu?

But assume the resources are correct, the Ray cluster still cannot schedule tasks to multiple nodes: #43265",like setting correct assume correct ray cluster still schedule multiple,issue,negative,neutral,neutral,neutral,neutral,neutral
1958865726,"```
>>> pprint(ray.available_resources())
{'CPU': 30.0,
 'CPU_group_1_dd1738c1a458ad5395c3805f1b7d02000000': 1.0,
 'CPU_group_2_dd1738c1a458ad5395c3805f1b7d02000000': 1.0,
 'CPU_group_3_dd1738c1a458ad5395c3805f1b7d02000000': 1.0,
 'CPU_group_4_dd1738c1a458ad5395c3805f1b7d02000000': 1.0,
 'CPU_group_5_dd1738c1a458ad5395c3805f1b7d02000000': 1.0,
 'bundle_group_0_dd1738c1a458ad5395c3805f1b7d02000000': 1000.0,
 'bundle_group_1_dd1738c1a458ad5395c3805f1b7d02000000': 1000.0,
 'bundle_group_2_dd1738c1a458ad5395c3805f1b7d02000000': 1000.0,
 'bundle_group_3_dd1738c1a458ad5395c3805f1b7d02000000': 1000.0,
 'bundle_group_4_dd1738c1a458ad5395c3805f1b7d02000000': 1000.0,
 'bundle_group_5_dd1738c1a458ad5395c3805f1b7d02000000': 1000.0,
 'bundle_group_dd1738c1a458ad5395c3805f1b7d02000000': 6000.0,
 'memory': 420000000000.0,
 'node:240.56.163.77': 1.0,
 'node:240.56.165.185': 1.0,
 'node:240.56.175.167': 1.0,
 'node:240.56.180.252': 1.0,
 'node:240.56.184.189': 1.0,
 'node:240.56.185.90': 1.0,
 'node:240.56.187.109': 1.0,
 'node:__internal_head__': 1.0,
 'object_store_memory': 125711252970.0}
```

```
>>> pprint(ray.nodes())
...
 {'Alive': True,
  'Labels': {'ray.io/node_id': '4a1b1e88bf2e55cd9c238ca74b0db3d41667754b3964d60ebb38d239'},
  'MetricsExportPort': 8080,
  'NodeID': '4a1b1e88bf2e55cd9c238ca74b0db3d41667754b3964d60ebb38d239',
  'NodeManagerAddress': '240.56.175.167',
  'NodeManagerHostname': 'tune-t5-small-raycluster-vgjj8-worker-small-group-k8d22',
  'NodeManagerPort': 43285,
  'NodeName': '240.56.175.167',
  'ObjectManagerPort': 46635,
  'ObjectStoreSocketName': '/tmp/ray/session_2024-02-21_23-17-16_864422_8/sockets/plasma_store',
  'RayletSocketName': '/tmp/ray/session_2024-02-21_23-17-16_864422_8/sockets/raylet',
  'Resources': {'CPU': 6.0,
                'memory': 60000000000.0,
                'node:240.56.175.167': 1.0,
                'object_store_memory': 17969018880.0},
  'RuntimeEnvAgentPort': 62986,
  'alive': True},
 {'Alive': True,
  'Labels': {'ray.io/node_id': '2b749db89d675b739244e62254b9541a2b32387621732a941701122b'},
  'MetricsExportPort': 8080,
  'NodeID': '2b749db89d675b739244e62254b9541a2b32387621732a941701122b',
  'NodeManagerAddress': '240.56.180.252',
  'NodeManagerHostname': 'rayjob-finetune-t5-small-raycluster-vgjj8-head-v94c6',
  'NodeManagerPort': 40283,
  'NodeName': '240.56.180.252',
  'ObjectManagerPort': 34131,
  'ObjectStoreSocketName': '/tmp/ray/session_2024-02-21_23-17-16_864422_8/sockets/plasma_store',
  'RayletSocketName': '/tmp/ray/session_2024-02-21_23-17-16_864422_8/sockets/raylet',
  'Resources': {'memory': 60000000000.0,
                'node:240.56.180.252': 1.0,
                'node:__internal_head__': 1.0,
                'object_store_memory': 17898461184.0},
  'RuntimeEnvAgentPort': 60554,
  'alive': True},
 {'Alive': True,
  'Labels': {'ray.io/node_id': 'ddb8af2ba2b22140064e667e6e5ef6dc98fde358d75575966e26a48a'},
  'MetricsExportPort': 8080,
  'NodeID': 'ddb8af2ba2b22140064e667e6e5ef6dc98fde358d75575966e26a48a',
  'NodeManagerAddress': '240.56.163.77',
  'NodeManagerHostname': 'tune-t5-small-raycluster-vgjj8-worker-small-group-sb94f',
  'NodeManagerPort': 44221,
  'NodeName': '240.56.163.77',
  'ObjectManagerPort': 44875,
  'ObjectStoreSocketName': '/tmp/ray/session_2024-02-21_23-17-16_864422_8/sockets/plasma_store',
  'RayletSocketName': '/tmp/ray/session_2024-02-21_23-17-16_864422_8/sockets/raylet',
  'Resources': {'CPU': 6.0,
                'memory': 60000000000.0,
                'node:240.56.163.77': 1.0,
                'object_store_memory': 17968978329.0},
  'RuntimeEnvAgentPort': 49162,
  'alive': True}]
```


",true true true true true true,issue,positive,positive,positive,positive,positive,positive
1958835688,"This is the parameter of `ray start` on worker node:

```
ray            1       0  0 22:46 ?        00:00:00 /bin/bash -lc -- ulimit -n 65536; ray start  --address=rayjob-finetune-t5-small-raycluster-blmnv-head-svc.ray-test.svc.cluster.local:6379  --metrics-export-port=8080  --block  --dashboard-agent-listen-port=52365  --memory=60000000000  --num-cpus=6  --num-gpus=0 
```

So looks like `--num-cpus` is correct, although it doesn't reflect in cluster resources.

Any idea about this?
",parameter ray start worker node ray ray start block like correct although reflect cluster idea,issue,negative,neutral,neutral,neutral,neutral,neutral
1958831740,"If I set placement strategy as `SPREAD`, it will also get the same error as above. So seems Ray cluster cannot schedule tasks across nodes.",set placement strategy spread also get error ray cluster schedule across,issue,negative,neutral,neutral,neutral,neutral,neutral
1958829430,"The full stack trace:

```
│ 2024-02-21 23:04:42,647    ERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_a4e75_00000                                                                                                                    │
│ Traceback (most recent call last):                                                                                                                                                                                                  │
│   File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py"", line 110, in resolve_future                                                                                                  │
│     result = ray.get(future)                                                                                                                                                                                                        │
│   File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/auto_init_hook.py"", line 22, in auto_init_wrapper                                                                                                              │
│     return fn(*args, **kwargs)                                                                                                                                                                                                      │
│   File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/client_mode_hook.py"", line 103, in wrapper                                                                                                                     │
│     return func(*args, **kwargs)                                                                                                                                                                                                    │
│   File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/worker.py"", line 2624, in get                                                                                                                                  │
│     raise value.as_instanceof_cause()                                                                                                                                                                                               │
│ ray.exceptions.RayTaskError(RuntimeError): ray::_Inner.train() (pid=375, ip=240.56.182.229, actor_id=a7fe1f7025fe1480cc3d753202000000, repr=TorchTrainer)                                                                           │
│   File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/trainable.py"", line 342, in train                                                                                                                        │
│     raise skipped from exception_cause(skipped)                                                                                                                                                                                     │
│   File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/air/_internal/util.py"", line 88, in run                                                                                                                                 │
│     self._ret = self._target(*self._args, **self._kwargs)                                                                                                                                                                           │
│   File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py"", line 115, in <lambda>                                                                                                            │
│     training_func=lambda: self._trainable_func(self.config),                                                                                                                                                                        │
│   File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/base_trainer.py"", line 819, in _trainable_func                                                                                                                    │
│     super()._trainable_func(self._merged_config)                                                                                                                                                                                    │
│   File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py"", line 332, in _trainable_func                                                                                                     │
│     output = fn()                                                                                                                                                                                                                   │
│   File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/base_trainer.py"", line 729, in train_func                                                                                                                         │
│     trainer.training_loop()                                                                                                                                                                                                         │
│   File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/data_parallel_trainer.py"", line 464, in training_loop                                                                                                             │
│     backend_executor.start()                                                                                                                                                                                                        │
│   File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/_internal/backend_executor.py"", line 190, in start                                                                                                                │
│     self._backend.on_start(self.worker_group, self._backend_config)                                                                                                                                                                 │
│   File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/torch/config.py"", line 198, in on_start                                                                                                                           │
│     ray.get(setup_futures)                                                                                                                                                                                                          │
│ ray.exceptions.RayTaskError(RuntimeError): ray::_RayTrainWorker__execute._setup_torch_process_group() (pid=426, ip=240.56.187.134, actor_id=37cded263cc790ef67e124a202000000, repr=<ray.train._internal.worker_group.RayTrainWorker │
│   File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/_internal/worker_group.py"", line 33, in __execute                                                                                                                 │
│     raise skipped from exception_cause(skipped)                                                                                                                                                                                     │
│   File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/_internal/worker_group.py"", line 30, in __execute                                                                                                                 │
│     return func(*args, **kwargs)                                                                                                                                                                                                    │
│   File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/torch/config.py"", line 106, in _setup_torch_process_group                                                                                                         │
│     dist.init_process_group(                                                                                                                                                                                                        │
│   File ""/home/ray/anaconda3/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py"", line 595, in init_process_group                                                                                                     │
│     store, rank, world_size = next(rendezvous_iterator)                                                                                                                                                                             │
│   File ""/home/ray/anaconda3/lib/python3.9/site-packages/torch/distributed/rendezvous.py"", line 232, in _env_rendezvous_handler                                                                                                      │
│     store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)                                                                                                                                                 │
│   File ""/home/ray/anaconda3/lib/python3.9/site-packages/torch/distributed/rendezvous.py"", line 160, in _create_c10d_store                                                                                                           │
│     return TCPStore(                                                                                                                                                                                                                │
│ RuntimeError: Connection reset by peer       
```",full stack trace error trial task trial recent call last file line result future file line return file line wrapper return file line get raise ray file line train raise file line run file line lambda file line super file line output file line file line file line start file line ray file line raise file line return file line file line store rank next file line store rank file line return connection reset peer,issue,negative,negative,negative,negative,negative,negative
1958596360,"I think we can use the same columns and add an ""Expand collapse"" button in front of each application to show the deployments in that application.

For the existing columns, each of them make sense for either application or deployment. Route_prefix we can choose to make that an application level concept only.",think use add expand collapse button front application show application make sense either application deployment choose make application level concept,issue,negative,neutral,neutral,neutral,neutral,neutral
1958595311,"The issue right now is that if an application fails in a way where a deployment does not start up, the status of that update is hard to find. IT can be found in logs, but it would be much much better to see the application metadata in the main table.",issue right application way deployment start status update hard find found would much much better see application main table,issue,negative,positive,positive,positive,positive,positive
1958573066,"> > Potentially breaking train and tune tests?
> 
> I think it's caused by a power outage trying to fetch https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz: <img alt=""Screenshot 2024-02-21 at 5 26 09 PM"" width=""1726"" src=""https://private-user-images.githubusercontent.com/15851518/306822456-a204cdfc-5d28-4ffd-940b-9baa068be0bf.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDg1NzEzMzksIm5iZiI6MTcwODU3MTAzOSwicGF0aCI6Ii8xNTg1MTUxOC8zMDY4MjI0NTYtYTIwNGNkZmMtNWQyOC00ZmZkLTk0MGItOWJhYTA2OGJlMGJmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAyMjIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMjIyVDAzMDM1OVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA0MTU4MjI0Y2RkNTAyOTQ4NzZjOTQzYmIxMDZkMGRhZDA0MDQxMmM3ZTE5YTMwYmVlZDVkYTY5ZmZkYTMzZjcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.OjCP0EQO7xdG9-CpJAj5Eu232Xb9cEX0veOyG-kd8ak"">
> 
> Seen in failing test logs:
> 
> ```
> Exception: URL fetch failure on https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz: 503 -- Service Unavailable
> ```

Haha! I thought it was a typo when you said power outage :)",potentially breaking train tune think power outage trying fetch seen failing test exception fetch failure service unavailable thought typo said power outage,issue,negative,negative,negative,negative,negative,negative
1958512141,I think this functionality is working as expected. See [my comment here](https://github.com/ray-project/kuberay/issues/710#issuecomment-1958510200).,think functionality working see comment,issue,negative,neutral,neutral,neutral,neutral,neutral
1958496603,Ah good catch thanks @zcin,ah good catch thanks,issue,positive,positive,positive,positive,positive,positive
1958495123,"> Potentially breaking train and tune tests?

I think it's caused by a power outage trying to fetch https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz:
<img width=""1726"" alt=""Screenshot 2024-02-21 at 5 26 09 PM"" src=""https://github.com/ray-project/ray/assets/15851518/a204cdfc-5d28-4ffd-940b-9baa068be0bf"">

Seen in failing test logs:
```
Exception: URL fetch failure on https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz: 503 -- Service Unavailable
```
",potentially breaking train tune think power outage trying fetch seen failing test exception fetch failure service unavailable,issue,negative,negative,negative,negative,negative,negative
1958491722,We've switched to the new directory-based checkpoint implementation without OOM issue. Close it now.,switched new implementation without issue close,issue,negative,positive,positive,positive,positive,positive
1958489476,"Hi @jakemdaly , I think this is a limitation of gRPC. When you are passing a large dataset through train_func arguments, ray internally serialize the dataset and ship it to the remote worker nodes. I am afraid it won't be faster than initialize a new dataset on each worker from scratch.

My suggestion is to define a dataset initialization function, and call this function at the beginning of your training function. This will get around the gRPC limit issue and may also accelerate your training.",hi think limitation passing large ray internally serialize ship remote worker afraid wo faster initialize new worker scratch suggestion define function call function beginning training function get around limit issue may also accelerate training,issue,negative,negative,neutral,neutral,negative,negative
1958487712,"Potentially breaking train and tune tests?

<img width=""1807"" alt=""Screenshot 2024-02-21 at 5 17 39 PM"" src=""https://github.com/ray-project/ray/assets/128072568/73cdf0d7-ac3e-4271-9019-2f9aa661d45b"">
",potentially breaking train tune,issue,negative,neutral,neutral,neutral,neutral,neutral
1958483593,Sorry that we are not planning to support it in the near future. Can you find some workaround for it?,sorry support near future find,issue,negative,negative,negative,negative,negative,negative
1958480153,Close it since we've switched to the new directory-based checkpoint and let the users to determine the checkpointing behavior.,close since switched new let determine behavior,issue,negative,positive,positive,positive,positive,positive
1958478519,Close it now since we already have a Ray Data + Ray Train user guides: https://docs.ray.io/en/master/train/user-guides/data-loading-preprocessing.html,close since already ray data ray train user,issue,negative,neutral,neutral,neutral,neutral,neutral
1958477461,"Close it now. 

Update: We've deprecated `ray.tune.integration.wandb.WandbLogger `. Users should directly import and use wandb library in your training function.",close update directly import use library training function,issue,negative,positive,neutral,neutral,positive,positive
1958474997,Done. We have a diagram now in this user guide: https://docs.ray.io/en/master/train/overview.html,done diagram user guide,issue,negative,neutral,neutral,neutral,neutral,neutral
1958471585,"Close it since we've deprecated `LightningTrainer` and switched to `TorchTrainer`, which no longer have the duplicated ScalingConfig issue.",close since switched longer issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1958470617,"Close it now since we've migrated to TorchTrainer API, and we have an onboarding user guide for Lightning.",close since user guide lightning,issue,negative,neutral,neutral,neutral,neutral,neutral
1958469906,"Close this issue since we've migrated to the TorchTrainer API, which provides supports for Lightning 2.0.",close issue since lightning,issue,negative,neutral,neutral,neutral,neutral,neutral
1958452317,"Premerge failure appears network related.

:point_up: Core examples will be included in a later PR.",failure network related core included later,issue,negative,negative,negative,negative,negative,negative
1958452217,This is fixed from Ray 2.7 onward. See https://docs.ray.io/en/latest/train/user-guides/persistent-storage.html,fixed ray onward see,issue,negative,positive,neutral,neutral,positive,positive
1958439342,"@jakemdaly This will be fixed in Ray 2.10, but for now you can get around it by setting the environment variable `RAY_AIR_LOCAL_CACHE_DIR` to be the same as your `storage_path`.",fixed ray get around setting environment variable,issue,negative,positive,neutral,neutral,positive,positive
1958251873,"> 3. If there is no deadline, wait indefinitely for the new replacement replica to start before gracefully terminating the old replica.

@zcin I think we should have a conservative default value here (controlled by environment variable). Otherwise we could end up in a ""stuck"" state indefinitely if there's a deadlock scenario (e.g., constrained resources).

Perhaps 5min by default?",deadline wait indefinitely new replacement replica start gracefully old replica think conservative default value environment variable otherwise could end stuck state indefinitely deadlock scenario constrained perhaps min default,issue,negative,positive,positive,positive,positive,positive
1958245335,"> Do we also need to change `constants.py` to turn it on by default?

lol! somehow that got overwritten when I merged master (had some stacked PRs). Updating.",also need change turn default somehow got master,issue,negative,neutral,neutral,neutral,neutral,neutral
1958216242,"@kentropy got it.

I think it's an issue with cloudpickle which is used by Ray for serialization and deserialization.

If you change pickle to cloudpickle in your code, it also fails. 

Could you file a GH issue against https://github.com/cloudpipe/cloudpickle?",got think issue used ray serialization change pickle code also could file issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1958128900,"Currently we have multiple ways to do the initialization before calling the users' `train_func`.
- Using this backend-specific context manager
- Use the predefined training loop (e.g. LightGBM, XGBoost)
- `Backend.on_start` + `Backend.on_training_start`

In the future design, we need a more unified way to do the initialization. One possible way is to store all states in a global context, wrap the initialization logic with context manager around train_func. This ensures that initialization logics executes in the same thread as `train_func`.",currently multiple way calling context manager use training loop future design need unified way one possible way store global context wrap logic context manager around thread,issue,negative,neutral,neutral,neutral,neutral,neutral
1958096222,"I updated the PR description.

The BackendConfig is a developer API so it'd be safe and won't be exposed to the users. ",description developer safe wo exposed,issue,negative,positive,positive,positive,positive,positive
1958000399,"Hi. Yes, sorry I'm caught up with something. Feel free to take it from me",hi yes sorry caught something feel free take,issue,positive,negative,neutral,neutral,negative,negative
1957881548,"@m-harmonic Yes, it will be part of Ray 2.10 release.",yes part ray release,issue,negative,neutral,neutral,neutral,neutral,neutral
1957865998,"Sorry for late response. So I created a new virtual environment with python `3.10.11` and did a `pip install ray` which installed version `2.9.2`. Then I did a `import ray` and then a `ray.init(num_cpus=4)` and it gave the following error message

```
>>> ray.init(num_cpus=4)
Traceback (most recent call last):
  File ""F:\AvisPaul\venvs\ray_3_10_test\lib\site-packages\ray\_private\node.py"", line 312, in __init__
    ray._private.services.wait_for_node(
  File ""F:\AvisPaul\venvs\ray_3_10_test\lib\site-packages\ray\_private\services.py"", line 463, in wait_for_node
    raise TimeoutError(
TimeoutError: Timed out after 60 seconds while waiting for node to startup. Did not find socket name tcp://127.0.0.1:63251 in the list of object store socket names.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""F:\AvisPaul\venvs\ray_3_10_test\lib\site-packages\ray\_private\client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
  File ""F:\AvisPaul\venvs\ray_3_10_test\lib\site-packages\ray\_private\worker.py"", line 1618, in init
    _global_node = ray._private.node.Node(
  File ""F:\AvisPaul\venvs\ray_3_10_test\lib\site-packages\ray\_private\node.py"", line 317, in __init__
    raise Exception(
Exception: The current node timed out during startup. This could happen because some of the Ray processes failed to startup.
```
I exited the python environment and tried it again and it seemed to work with the following message

```
2024-02-21 15:29:34,099 INFO worker.py:1724 -- Started a local Ray instance.
RayContext(dashboard_url='', python_version='3.10.11', ray_version='2.9.2', ray_commit='fce7a361807580953364e2da964f9498f3123bf9', protocol_version=None)
```

However if I do only `ray.init()` was leaving it at `2024-02-21 15:33:16,903 INFO worker.py:1724 -- Started a local Ray instance.` indefinitely. So I did a `ray.init(logging_level='debug')` and this is what I got

```
2024-02-21 15:33:11,105 DEBUG worker.py:1483 -- Could not import resource module (on Windows)
2024-02-21 15:33:13,029 DEBUG node.py:1303 -- Process STDOUT and STDERR is being redirected to C:\Users\paula\AppData\Local\Temp\ray\session_2024-02-21_15-33-12_966592_144860\logs.
2024-02-21 15:33:16,404 DEBUG node.py:1332 -- Process STDOUT and STDERR is being redirected to C:\Users\paula\AppData\Local\Temp\ray\session_2024-02-21_15-33-12_966592_144860\logs.
2024-02-21 15:33:16,410 DEBUG npu.py:58 -- Could not import AscendCL: No module named 'acl'
2024-02-21 15:33:16,411 DEBUG tpu.py:119 -- Failed to detect number of TPUs: [WinError 3] The system cannot find the path specified: '/dev/vfio'
2024-02-21 15:33:16,624 DEBUG services.py:2064 -- Determine to start the Plasma object store with 92.53 GB memory using C:\Users\paula\AppData\Local\Temp.
2024-02-21 15:33:16,903 INFO worker.py:1724 -- Started a local Ray instance.
```",sorry late response new virtual environment python pip install ray version import ray gave following error message recent call last file line file line raise timed waiting node find socket name list object store socket exception direct cause following exception recent call last file line module file line wrapper return file line file line raise exception exception current node timed could happen ray python environment tried work following message local ray instance however leaving local ray indefinitely got could import resource module process process could import module detect number system find path determine start plasma object store memory local ray instance,issue,negative,negative,neutral,neutral,negative,negative
1957821228,"Hello, am I understanding correctly that this fix is not yet merged into any release? Thanks",hello understanding correctly fix yet release thanks,issue,negative,positive,positive,positive,positive,positive
1957731854,"Another data point:
I was able to create datasets (without the remote wrapper) by reverting my local Ray version to `2.7.0`, but now I cannot run a training job against the cluster because the versions do not match.
Cluster: 2.9.1
Local SDK: 2.7.0

This is the error I am observing: https://stackoverflow.com/questions/75403232/attributeerror-cant-get-attribute-tunerinternal-validate-overwrite-trainable",another data point able create without remote wrapper local ray version run training job cluster match cluster local error observing,issue,negative,positive,neutral,neutral,positive,positive
1957699300,"@zhangjian94cn The code is pretty clean. Thanks for the contribution! 

Here are several improvements:

For the code change:
- Is there's any environment variables need to be set for the hccl backend?
- Fix the CI tests?
    - run `ci/lint/format.sh` to fix the lint issue

For examples:
- Can you add the cluster configuration (e.g. num hpu cores, num of nodes, instance type) at the beginning?
- Also can you include the required packages and environment configuration for users to run the example?
- Just to confirm it works out of the box: can you also run the notebook on hpu and keep the output cells? ",code pretty clean thanks contribution several code change environment need set fix run fix lint issue add cluster configuration instance type beginning also include environment configuration run example confirm work box also run notebook keep output,issue,positive,positive,positive,positive,positive,positive
1957681612,"> But for get_assigned_ids(). it is more accurate to include bundles:0.001. Should we always include this to the output. Wdyt?

IMO, this is implementation detail and should not expose to the end user via a public API.",accurate include always include output implementation detail expose end user via public,issue,negative,positive,positive,positive,positive,positive
1957229210,"I don't think `rllib_contrib` release tests have ever run or been setup properly. We would need to change the byod file for these to make sure the correct ray version is installed first.

Fair enough, I can spend some more (limited) time to try make this work.",think release ever run setup properly would need change file make sure correct ray version first fair enough spend limited time try make work,issue,negative,positive,positive,positive,positive,positive
1957110655,"Hey @man2machine , sorry, but APPO does NOT support Box action spaces per-se (this has nothing to do with the particular shape). If you want to use continuous action spaces, you should use another algo, e.g. PPO or SAC.",hey sorry support box action nothing particular shape want use continuous action use another sac,issue,negative,negative,neutral,neutral,negative,negative
1957085020,"Hey @FuBaoLoong , thanks for raising this issue. I can reproduce it. I don't think leaving `normalize_actions=True` should be a problem. I actually couldn't think of a case where you should switch this off. As the docstring says: RLlib will automatically ""unsquash"" the NN output to make sure the computed actions match your environment's space. Whether that space is from -500 to +6 or from -1 to 1 or from 0 to 1 (in your case).

",hey thanks raising issue reproduce think leaving problem actually could think case switch automatically output make sure match environment space whether space case,issue,negative,positive,positive,positive,positive,positive
1957015824,"Hey @jakecyr , I understand your pain here, I just tried the described procedure from our docs again on my local setup and ran into a different issue with tf not having a 2.11 version anymore (which is required by the A3C requirements.txt file). I guess one could try changing this to 2.13 (and tf probability to 0.21.0).

@Poiuytrezay1 , thanks for posting a workaround here (even though it's hacky and painful). Is there a way you could simply use an older version of Ray (that still has full support for A3C)? Note also that these contrib algos are NOT meant to be kept up to date with the current ray versions. For example, A3C's Ray version is pinned to 2.3.1, see here: https://github.com/ray-project/ray/blob/master/rllib_contrib/a3c/pyproject.toml#L15

We will talk about this whole rllib_contrib concept internally now, but we will not bring these back into the regular rllib for sure. Instead, we will - once the new API stack has been completely rolled out by fall 2024 - add newer algos to this new API stack and will encourage users to write their own from scratch (which will be much much simpler on the new stack than this was on the old).",hey understand pain tried procedure local setup ran different issue version file guess one could try probability thanks posting even though hacky painful way could simply use older version ray still full support note also meant kept date current ray example ray version pinned see talk whole concept internally bring back regular sure instead new stack completely rolled fall add new stack encourage write scratch much much simpler new stack old,issue,positive,positive,neutral,neutral,positive,positive
1956976176,"not a release blocker, the test should not run",release blocker test run,issue,negative,neutral,neutral,neutral,neutral,neutral
1956945562,"We are cleaning these up as we speak :) Thanks for raising this @can-anyscale .

PR is here:
https://github.com/ray-project/ray/pull/43278",cleaning speak thanks raising,issue,negative,positive,positive,positive,positive,positive
1956940434,"Thanks for raising this issue @simonsays1980 , PR (by yourself!) :) is in review.
https://github.com/ray-project/ray/pull/43084",thanks raising issue review,issue,negative,positive,positive,positive,positive,positive
1956928866,"Hey @JingDang99 , thanks for raising this question. You seem to be working with a very old code-snippet from somewhere.

The `agents` packags in rllib has been moves into `algorithms` a long time ago. Could you try to replace your line with:
```
from ray import tune

my_class = tune.get_trainable_class(""PPO"")  # or another RLlib algo name
my_config_class = my_class.get_default_config()
```",hey thanks raising question seem working old somewhere long time ago could try replace line ray import tune another name,issue,negative,positive,neutral,neutral,positive,positive
1956920546,"Hey @zhubenchao , thanks for raising this issue. In your output, does the ""PENDING"" ever turn into a ""RUNNING""?
If not, then the job has not even started and is possibly blocked by something else. Maybe the GPU is not available or blocked by another process. If there is no GPU at all on your machine, you should see a Ray error, though, so that's seems to be not the case.",hey thanks raising issue output pending ever turn running job even possibly blocked something else maybe available blocked another process machine see ray error though case,issue,negative,positive,positive,positive,positive,positive
1956909716,"Hey @man2machine  thanks for raising this issue. This is really interesting. I actually did not know, you could set a `start` arg in a Discrete space :) 

As a simple solution, could you make the extra 1-shift inside your env's `step()` code?

Like so:
```

def step(self, action_dict):
    action_dict = OrderedDict({k: a+1 for k, a in action_dict.items()})
    ...  # continue with this shifted dict
```

I'm trying to PR a better solution in the meantime. I tried gym.ActionWrapper around your env, but RLlib's env checker and also the multi-agent env does not allow this b/c a gym.ActionWrapper is NOT an RLlib BaseEnv or an RLlib MultiAgentEnv, so more issues will surface.
",hey thanks raising issue really interesting actually know could set start discrete space simple solution could make extra inside step code like step self continue trying better solution tried around checker also allow surface,issue,positive,positive,positive,positive,positive,positive
1956875507,"@jjyao after this PR, all the scheduling request will include ""bundle"": 0.001. 

All the test failures happen because when you call `get_assigned_ids()` API, it returns now ""bundles"": 0.001. Also, when we call ray status, the pending req also contains bundle: 0.001

1. I hided this info from `ray status` demand output.
2. But for get_assigned_ids(). it is more accurate to include bundles:0.001. Should we always include this to the output. Wdyt? ",request include bundle test happen call also call ray status pending also bundle hided ray status demand output accurate include always include output,issue,negative,positive,positive,positive,positive,positive
1956820481,"Hey @brenting thanks for filing this issue! I agree, this is not ideal and requires a fix!

In the meantime, can you add this small workaround to your script to make it work? It should not interfere with the rest of your experiment.

Add
```
        self._obs_space_in_preferred_format = True
        self._action_space_in_preferred_format = True
```
to your Env's c'tor. This tells MultiAgentEnv that the spaces are already provided per-agent (and maybe not every agent is using the same spaces).

Add:
`config.environment(disable_env_checking=True)`
to your config, to disable the env checker entirely for now.


The following reproduction is working for me:
```
import ray
from gymnasium.spaces import Dict, Discrete
from ray.rllib.algorithms.ppo import PPOConfig
from ray.rllib.env.multi_agent_env import MultiAgentEnv


class DummyEnv(MultiAgentEnv):
    def __init__(self, env_config: dict = None):
        self._agent_ids = {""agent""}
        self.observation_space = Dict({""agent"": Discrete(2)})
        self.action_space = Dict({""agent"": Discrete(2)})
        super().__init__()

        self._obs_space_in_preferred_format = True
        self._action_space_in_preferred_format = True

    def reset(self, seed=None, options=None):
        super().reset(seed=seed, options=options)
        return self.observation_space.sample(), {}

    def step(self, action_dict):
        return (
            self.observation_space.sample(),
            {""agent"": 1.0},
            {""__all__"": True},
            {""__all__"": True},
            {},
        )
    

class CommonInfoEnv(DummyEnv):
    def step(self, action_dict):
        ret = list(super().step(action_dict))
        ret[-1] = {""__common__"": {""test"": 0}}
        return tuple(ret)


ray.init()
from ray import tune
tune.register_env(""env"", lambda cfg: CommonInfoEnv())


config = (
    PPOConfig()
    .environment(""env"", disable_env_checking=True)
    .rollouts(num_rollout_workers=0)
    .training(train_batch_size=1, sgd_minibatch_size=1, num_sgd_iter=1)
)

config.build().train()

```
",hey thanks filing issue agree ideal fix add small script make work interfere rest experiment add true true already provided maybe every agent add disable checker entirely following reproduction working import ray import discrete import import class self none agent agent discrete agent discrete super true true reset self super return step self return agent true true class step self ret list super ret test return ret ray import tune lambda,issue,positive,positive,positive,positive,positive,positive
1956796975,"I saw the same error with kuberay 1.0.0 creating K8s RayClusters running Ray 2.9.2 with gpustat 1.1.1 installed. This error causes the head Pod's ray-head Container to fail the K8s liveness and readiness probes because the Ray dashboard isn't listening on the assigned port.

Pinning gpustat to 1.0.0 fixed it.

```
❯ kubectl get pods

NAME                                                      READY   STATUS                  RESTARTS   AGE
dxia-ray-29-test-head-mkksc                               1/2     Running                 0          156m
dxia-ray-29-test-worker-worker-2vl55                      0/2     Init:1/2                0          156m

❯ kubectl logs -f dxia-ray-29-test-head-mkksc

Defaulted container ""ray-head"" out of: ray-head, fluentbit
2024-02-20 08:27:09,343	ERROR services.py:1329 -- Failed to start the dashboard , return code 1
2024-02-20 08:27:09,343	ERROR services.py:1354 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure' to find where the log file is.
2024-02-20 08:27:09,343	ERROR services.py:1398 --
The last 20 lines of /tmp/ray/session_2024-02-20_08-27-07_182583_8/logs/dashboard.log (it contains the error message from the dashboard):
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/dashboard/utils.py"", line 121, in get_all_modules
    importlib.import_module(name)
  File ""/home/ray/anaconda3/lib/python3.9/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1030, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 1007, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 986, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 680, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 850, in exec_module
  File ""<frozen importlib._bootstrap>"", line 228, in _call_with_frames_removed
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/dashboard/modules/reporter/reporter_agent.py"", line 52, in <module>
    import gpustat.core as gpustat
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/gpustat/__init__.py"", line 16, in <module>
    from .core import GPUStat, GPUStatCollection
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/gpustat/core.py"", line 24, in <module>
    from gpustat.nvml import pynvml as N
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/gpustat/nvml.py"", line 57, in <module>
    _original_nvmlGetFunctionPointer = pynvml._nvmlGetFunctionPointer
AttributeError: module 'pynvml' has no attribute '_nvmlGetFunctionPointer'
2024-02-20 08:27:07,181	INFO usage_lib.py:423 -- Usage stats collection is disabled.
2024-02-20 08:27:07,181	INFO scripts.py:744 -- Local node IP: 10.169.18.195
2024-02-20 08:27:09,471	SUCC scripts.py:781 -- --------------------
2024-02-20 08:27:09,471	SUCC scripts.py:782 -- Ray runtime started.
2024-02-20 08:27:09,472	SUCC scripts.py:783 -- --------------------
2024-02-20 08:27:09,472	INFO scripts.py:785 -- Next steps
2024-02-20 08:27:09,472	INFO scripts.py:788 -- To add another node to this Ray cluster, run
2024-02-20 08:27:09,472	INFO scripts.py:791 --   ray start --address='10.169.18.195:6379'
2024-02-20 08:27:09,472	INFO scripts.py:800 -- To connect to this Ray cluster:
2024-02-20 08:27:09,472	INFO scripts.py:802 -- import ray
2024-02-20 08:27:09,472	INFO scripts.py:803 -- ray.init()
2024-02-20 08:27:09,472	INFO scripts.py:834 -- To terminate the Ray runtime, run
2024-02-20 08:27:09,472	INFO scripts.py:835 --   ray stop
2024-02-20 08:27:09,472	INFO scripts.py:838 -- To view the status of the cluster, use
2024-02-20 08:27:09,472	INFO scripts.py:839 --   ray status
2024-02-20 08:27:09,473	INFO scripts.py:952 -- --block
2024-02-20 08:27:09,474	INFO scripts.py:953 -- This command will now block forever until terminated by a signal.
2024-02-20 08:27:09,474	INFO scripts.py:956 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.

❯ kubectl describe pods dxia-ray-29-test-head-mkksc

Name:                 dxia-ray-29-test-head-mkksc
Namespace:            hyperkube
Events:
  Type     Reason     Age                    From     Message
  ----     ------     ----                   ----     -------
  Warning  Unhealthy  3m52s (x24 over 143m)  kubelet  Readiness probe errored: command ""bash -c wget -T 2 -q -O- http://localhost:52365/api/local_raylet_healthz | grep success && wget -T 2 -q -O- http://localhost:8265/api/gcs_healthz | grep success"" timed out
  Warning  Unhealthy  3m32s (x24 over 142m)  kubelet  Liveness probe errored: command ""bash -c wget -T 2 -q -O- http://localhost:52365/api/local_raylet_healthz | grep success && wget -T 2 -q -O- http://localhost:8265/api/gcs_healthz | grep success"" timed out
```",saw error running ray error head pod container fail liveness readiness ray dashboard listening assigned port pinning fixed get name ready status age running container error start dashboard return code error error written printing last see find log file error last error message dashboard file line name file line return name level package level file frozen line file frozen line file frozen line file frozen line file frozen line file frozen line file line module import file line module import file line module import file line module module attribute usage collection disabled local node ray next add another node ray cluster run ray start connect ray cluster import ray terminate ray run ray stop view status cluster use ray status block command block forever signal running message printed terminate unexpectedly exit graceful thus describe name type reason age message warning unhealthy readiness probe command bash success success timed warning unhealthy liveness probe command bash success success timed,issue,negative,positive,neutral,neutral,positive,positive
1956755702,"Hey @garymm , thanks for raising this issue!
You are absolutely right, this is causing a problem and needs a fix. We usually don't run anything with the Algorithm's default implementation of `training_step` (let alone multi-agent stuff) so this slipped through.

In PPO's training_step method, we do something like:
```
policies_to_update = set(train_results.keys()) - {ALL_MODULES}  # <- ALL_MODULES == ""__all__""
```
and then pass that as `policies` into the `sync_weights` call. This is similar to your suggestion.

We'll provide a fix-PR ...

In the meantime, you can also take a look at this currently-in-review PR, which brings self-play and league-based self-play into the new API stack, including example scripts (for PPO): https://github.com/ray-project/ray/pull/43276

But this PR will not fix your problem. I'll create a new one.",hey thanks raising issue absolutely right causing problem need fix usually run anything algorithm default implementation let alone stuff slipped method something like set pas call similar suggestion provide also take look new stack example fix problem create new one,issue,negative,positive,neutral,neutral,positive,positive
1956741235,"Thanks for filing this issue @haidaodeweixiao !
PR has been put into review: https://github.com/ray-project/ray/pull/43313",thanks filing issue put review,issue,negative,positive,positive,positive,positive,positive
1956733982,"@rickyyx yes, you removed the 'no_windows' tag from the bazel config https://github.com/ray-project/ray/commit/a76fca6f1de7c5bb564c578787303b01a0c7d2d8; I think this test should not run on windows",yes removed tag think test run,issue,negative,neutral,neutral,neutral,neutral,neutral
1956725285,"PR by @simonsays1980 (DQN on new API stack related) has been merged :)

Closing this issue",new stack related issue,issue,negative,positive,neutral,neutral,positive,positive
1956282482,"```sh
pip install -U ""ray[data]""
```

```sh
# workaround for https://github.com/ray-project/ray/issues/42842

pip uninstall pandas
pip install pandas==
pip install -Iv pandas==2.1.4
```",sh pip install ray data sh pip pip install pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
1956190073,"@aslonnie  I have split the code so that this PR only contains helper functions.
#43309  is code change for deleting old commit tags
#43308  is for back up release tags
I have another pending branch for the binary file to run the task with Bazel. Will put up a PR for that after these PRs are merged in.",split code helper code change old commit back release another pending branch binary file run task put,issue,positive,positive,neutral,neutral,positive,positive
1956092582,"But with a subclass of `TaskBase`, the code no longer work.
```python
class TaskA(TaskBase, task_ids=(1, 2, 3)):
    pass
```",subclass code longer work python class pas,issue,negative,neutral,neutral,neutral,neutral,neutral
1955924350,"@kentropy I tried

```
import ray

ray.init()

class TaskBase:
    _registry = {}
    def __init__(self, task_id: int):
        self.task_id = task_id

    def __init_subclass__(cls, task_ids, **kwargs):
        super().__init_subclass__(**kwargs)
        for _id in task_ids:
            cls._registry[_id] = cls

    def __new__(cls, task_id: int, **kwargs):
        subcls = cls._registry.get(task_id, cls)
        obj = super().__new__(subcls)
        return obj

    def __getnewargs__(self):
        return (self.task_id,)

def main():
    task = TaskBase(task_id=1)
    print(task)
    #print(inspect_serializability(task))
    task_ref = ray.put(task)
    print(ray.get(task_ref))


if __name__ == ""__main__"":
    main()
```

and it worked.",tried import ray class self super super return self return main task print task print task task print main worked,issue,positive,positive,positive,positive,positive,positive
1955796443,"FYI @rkooo567 added some C++ changes to enable thread-safe begin_read / end_read, lost them accidentally in the merge earlier.",added enable lost accidentally merge,issue,negative,neutral,neutral,neutral,neutral,neutral
1955736402,I can take over if @akshara08 is away. @anyscalesam please assign me.,take away please assign,issue,negative,neutral,neutral,neutral,neutral,neutral
1955735590,Can you explain how `__getnewargs__` can work in ray? I added the function to my initial reproduction script and it still produce a type error.,explain work ray added function initial reproduction script still produce type error,issue,negative,neutral,neutral,neutral,neutral,neutral
1955690615,"@zcin a minor fix for hpu doc, please review",minor fix doc please review,issue,negative,negative,neutral,neutral,negative,negative
1955682327,@can-anyscale  Reverted it back to use `bash -i` and export build env vars into the linux script now. https://buildkite.com/ray-project/release-automation/builds/126 passed for all envs.,back use bash export build script,issue,negative,neutral,neutral,neutral,neutral,neutral
1955681219,"@matthewdeng  thanks for your reply. I added the configuration in deepspeed, unfortunately, it doesn't generate a whole model. ",thanks reply added configuration unfortunately generate whole model,issue,negative,negative,neutral,neutral,negative,negative
1955391359,"> Is this basically a bug from inspect module? 

Yes. See https://github.com/ray-project/ray/pull/43117#issuecomment-1947459842 for more details.

> Do you know what's the timeline for the fix in Python level? (or is it fixed in the latest versions)?

No, I don't think there is a timeline for that. It has lasted for more than 3 years.

> Also this method = method.__init__ is only applied to __class_getitme__ right? Other methods are not affected

I believe so because I can still get `3 4 5 6 7 8 9` in the following example.

```python
from torch.utils.data import IterableDataset
import ray

@ray.remote
class SyncDataCollector(IterableDataset):
    def __iter__(self):
        return iter(range(3, 10))

ray.init()
sdc = SyncDataCollector.remote()
obj_ref = sdc.__iter__.remote()
it = ray.get(obj_ref)
for i in it:
    print(i)
# 3 4 5 6 7 8 9
```",basically bug inspect module yes see know fix python level fixed latest think also method applied right affected believe still get following example python import import ray class self return iter range print,issue,negative,positive,positive,positive,positive,positive
1955337505,@FrancescoMandru do you still see this error with latest Ray?,still see error latest ray,issue,negative,positive,positive,positive,positive,positive
1955317686,Your latest code snippet (including `__getnerargs__`) works with Ray as well.,latest code snippet work ray well,issue,negative,positive,positive,positive,positive,positive
1955296615,Please follow https://docs.ray.io/en/latest/ray-contribute/development.html to bulid Ray from source code.,please follow ray source code,issue,negative,neutral,neutral,neutral,neutral,neutral
1955283385,"@hhq326781458 since we don't have a repro-script, I'm closing this one for now. Feel free to reopen with a repro script.",since one feel free reopen script,issue,positive,positive,positive,positive,positive,positive
1955259968,This should have been fixed since we no longer use `grpcio`.,fixed since longer use,issue,negative,positive,neutral,neutral,positive,positive
1955184707,"Let's give @lonnie a chance to review, before we merge.",let give chance review merge,issue,negative,neutral,neutral,neutral,neutral,neutral
1955132978,Unfortunately it's hard to help here without a proper repro script. Can you update the ticket when a shareable repro is possible?,unfortunately hard help without proper script update ticket shareable possible,issue,negative,negative,neutral,neutral,negative,negative
1955095772,"nope, I reverted unnecessary refactoring. I think there are some failures I need to handle still. I will finish it today. 

",nope unnecessary think need handle still finish today,issue,negative,negative,negative,negative,negative,negative
1955035589,"```
(base) ruiyangwang@ryw-anyscale-macbook release_logs % python compare_perf_metrics 2.9.2old 2.9.2
REGRESSION 90.69%: actors_per_second (THROUGHPUT) regresses from 651.1270434383275 to 60.616879340497654 (90.69%) in 2.9.2/benchmarks/many_actors.json
REGRESSION 82.65%: dashboard_p95_latency_ms (LATENCY) regresses from 30.418 to 55.558 (82.65%) in 2.9.2/benchmarks/many_nodes.json
REGRESSION 28.39%: dashboard_p99_latency_ms (LATENCY) regresses from 133.785 to 171.764 (28.39%) in 2.9.2/benchmarks/many_nodes.json
```",base python old regression throughput regression latency regression latency,issue,negative,negative,negative,negative,negative,negative
1954963539,"I originally tried calling the `from_huggingface` function without the `remote` wrapper, but I get an error;
```
RuntimeError: Global node is not initialized.
```

I found this ticket, where your team indicates that it should be wrapped in a `ray.remote` context, hence the code included in the issue above.
https://github.com/ray-project/ray/issues/41333",originally tried calling function without remote wrapper get error global node found ticket team wrapped context hence code included issue,issue,negative,positive,neutral,neutral,positive,positive
1954936838,"oops, I mistakenly pushed the change directly to master https://github.com/ray-project/ray/commit/c6094a96aa9e08b43cfdd15a7c2a1d07eddd2dad",mistakenly change directly master,issue,negative,positive,neutral,neutral,positive,positive
1954932216,I'll put up a PR to mark this test as unstable to keep the record ;),put mark test unstable keep record,issue,negative,neutral,neutral,neutral,neutral,neutral
1954930571,"> Thanks @edoakes; how should we think about this test in with respect to getting to a weekly green build from master? Is it important enough that it needs to stay within that scope?

No, as written it's not, we can even just disable this test.",thanks think test respect getting weekly green build master important enough need stay within scope written even disable test,issue,positive,positive,neutral,neutral,positive,positive
1954912084,Where exactly should I set up the environmental variables? Local machine? Head node? Worker nodes? All of them?,exactly set environmental local machine head node worker,issue,negative,positive,positive,positive,positive,positive
1954904965,"@justinvyu I completely agree to the issues that can arise from trying to over-engineer the automatic resolution of relative path into absolute path, given the distributed nature of the task. I would suggest that, instead, it'd be better to just detect that a relative path was passed by a user, and to print a meaningful error message asking the user to pass in the absolute path instead. Besides, in most cases, getting the absolute path isn't that difficult of a thing to do either (`pwd`). 

What do you think?
",completely agree arise trying automatic resolution relative path absolute path given distributed nature task would suggest instead better detect relative path user print meaningful error message user pas absolute path instead besides getting absolute path difficult thing either think,issue,negative,positive,positive,positive,positive,positive
1954896589,"Hey @Dj-Polyester, we made this change in Ray 2.7 when simplifying the Ray Train/Tune storage backend to only use `pyarrow` with as few ""layers"" on top as possible. So, this `storage_path` gets fed directly into a `pyarrow.fs.FileSystem`, without extra checks -- and the pyarrow API doesn't allow for a relative path by itself.

**I am thinking of this solution:** Documentation change PLUS raising a better exception telling you to call `Path(...).resolve()` before passing it into `storage_path`.

The main issue with auto-resolving relative paths into absolute paths is that the behavior on workers is a little ambiguous:
* The current working directory between workers may be different from the process that you launch the training job from. (For example, Ray Tune will change your worker cwd to the trial directory by default.)
* In this case, if the user storage path is `""./relpath""`, then it's unclear whether you should use `{driver_cwd}/relpath` or `{worker_cwd}/relpath` as the result/checkpoint destination.

What do you think?",hey made change ray ray storage use top possible fed directly without extra allow relative path thinking solution documentation change plus raising better exception telling call path passing main issue relative absolute behavior little ambiguous current working directory may different process launch training job example ray tune change worker trial directory default case user storage path unclear whether use destination think,issue,positive,positive,positive,positive,positive,positive
1954886400,"Samuel Chan
:spiral_calendar_pad:  [10:07 PM](https://anyscaleteam.slack.com/archives/D04TY7D0BLY/p1708322833395689)
hey justin - can you please take a look at https://github.com/orgs/anyscale/projects/76/views/1?pane=issue&itemId=53589070 and update the end date of when it will be fixed?


Justin Yu
:bulb:  [10:50 AM](https://anyscaleteam.slack.com/archives/D04TY7D0BLY/p1708455016281699)
Will be fixed today!
:ty:
1",hey please take look update end date fixed bulb fixed today,issue,negative,positive,neutral,neutral,positive,positive
1954872074,"To consolidate your model, you can set `zero_optimization.stage3_gather_16bit_weights_on_model_save` to True in the Deepspeed config.",consolidate model set true,issue,negative,positive,positive,positive,positive,positive
1954745390,Thanks @edoakes; how should we think about this test in with respect to getting to a weekly green build from master? Is it important enough that it needs to stay within that scope?,thanks think test respect getting weekly green build master important enough need stay within scope,issue,positive,positive,neutral,neutral,positive,positive
1954727039,"@anyscalesam this was pushed back due to Cindy prioritizing draining/compaction improvements instead. Release tests won't get reworked until 2.11.

This test failure shouldn't be a blocker for 2.10, however (it's archaic).",back due instead release wo get reworked test failure blocker however archaic,issue,negative,negative,negative,negative,negative,negative
1954718077,Can confirm that using absolute path fixes the issue.,confirm absolute path issue,issue,negative,positive,positive,positive,positive,positive
1954701547,"@datalee I don't think this is the right forum for this question. But to answer it, there can be multiple objects in Ray's object store and you can also have multiple Ray actors. But there can only be one ray instance running which is what this PR is addressing. If you have further questions I would suggest you follow this page to https://docs.ray.io/en/latest/ ask in slack or in the discuss forum https://discuss.ray.io/",think right forum question answer multiple ray object store also multiple ray one ray instance running would suggest follow page ask slack discus forum,issue,negative,positive,neutral,neutral,positive,positive
1954629659,I'm trying to revert to unblock test failures,trying revert unblock test,issue,negative,neutral,neutral,neutral,neutral,neutral
1954465724,"Yes, that's true, but it happened almost every time when I launch my cluster, I believe launching spot instances cluster should be allowed? is there some configurations I can change to allow ray to work with spot instances cluster? Also, I used to be able to use spot cluster with python3.8.",yes true almost every time launch cluster believe spot cluster change allow ray work spot cluster also used able use spot cluster python,issue,positive,positive,positive,positive,positive,positive
1954436741,"hi @Akshi22 , don't let me get in your way! though it looks like @ujjawal-khare-27 has already submitted a pr to fix this issue. maybe you can help there?",hi let get way though like already fix issue maybe help,issue,positive,neutral,neutral,neutral,neutral,neutral
1953939440,@jjyao since I can't repro this anymore I have closed the issue,since ca closed issue,issue,negative,negative,neutral,neutral,negative,negative
1953875732,"Btw, can we accelerate the progress of this PR as the branch cut is coming? We'd like to merge this by this week to meet 2/29",accelerate progress branch cut coming like merge week meet,issue,negative,neutral,neutral,neutral,neutral,neutral
1953827248,"Has this now become possible?
For instance, can I specify how much vram a task needs such that ray reserves that amount on a given gpu?",become possible instance specify much task need ray amount given,issue,negative,positive,neutral,neutral,positive,positive
1953704353,@rkooo567 Could you help triage this issue?,could help triage issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1953633902,This looks like an infra issue. @scottjlee could you help confirm? Thanks.,like infra issue could help confirm thanks,issue,positive,positive,positive,positive,positive,positive
1953632343,@raulchen - can you help take a look? thanks.,help take look thanks,issue,positive,positive,positive,positive,positive,positive
1953631866,@bveeramani - can you help take a look this week? Thanks.,help take look week thanks,issue,positive,positive,positive,positive,positive,positive
1953631039,Hi @scottjlee can you help take a look?,hi help take look,issue,negative,neutral,neutral,neutral,neutral,neutral
1953484881,"In my debugging I noticed that the native model saved to a checkpoint in my case is around ~116 MB, but when I use `Algorithm.from_checkpoint()` to reinitialize the model (i.e. load the weights and state parameters from the state file) and then immediately use `Algorithm.save_checkpoint()` without any training, the native model saved to the new checkpoint is ~66 MB. The `tower_stats` or  _last_output` attributes of the new model were not the same as the original one, but even after modifying `torch_policy.py` to save those to the state file and load them from it, the model was smaller (~88 MB) compared to the original one, indicating some information is still missing. The only solution I found that could solve the problem was doing this:
```Python3
ray.init(num_cpus=12, num_gpus=2)
  
register_env('carla', env_creator)
  
os.system('nvidia-smi')
  
if not os.path.exists(os.path.join(args.directory, args.name)):
    os.mkdir(os.path.join(args.directory, args.name))
  
if not args.restore:
    sac_config = SACConfig().framework(**args.config['framework']) \
        .environment(**args.config['environment']) \
        .callbacks(**args.config['callbacks']) \
        .rollouts(**args.config['rollouts']) \
        .fault_tolerance(**args.config['fault_tolerance']) \
        .resources(**args.config['resources']) \
        .debugging(**args.config['debugging']) \
        .checkpointing(**args.config['checkpointing']) \
        .reporting(**args.config['reporting']) \
        .training(**args.config['training'])
    
    sac_algo = sac_config.build()
else:
    sac_algo = Algorithm.from_checkpoint(os.path.join(args.directory, args.name))
    
    model = torch.load(os.path.join(args.directory, args.name, 'policies', 'default_policy', 'model', 'model.pt'))
    
    sac_algo.get_policy().model = copy.deepcopy(model)
    sac_algo.get_policy().target_model = copy.deepcopy(model)
  
    gpu_ids = list(range(torch.cuda.device_count()))
  
    devices = [
        torch.device(""cuda:{}"".format(i))
        for i, id_ in enumerate(gpu_ids)
        if i < args.config['resources']['num_gpus']
    ]
  
    sac_algo.get_policy().model_gpu_towers = []
  
    for i, _ in enumerate(gpu_ids):
        model_copy = copy.deepcopy(model)
        sac_algo.get_policy().model_gpu_towers.append(model_copy.to(devices[i]))
  
    sac_algo.get_policy().model_gpu_towers[0] = sac_algo.get_policy().model
    
    sac_algo.get_policy().target_models = {
        m: copy.deepcopy(sac_algo.get_policy().target_model).to(devices[i])
        for i, m in enumerate(sac_algo.get_policy().model_gpu_towers)
    }
  
    sac_algo.get_policy()._state_inputs = sac_algo.get_policy().model.get_initial_state()
  
    sac_algo.get_policy()._is_recurrent = len(sac_algo.get_policy()._state_inputs) > 0
  
    sac_algo.get_policy()._update_model_view_requirements_from_init_state()
  
    sac_algo.get_policy().view_requirements.update(sac_algo.get_policy().model.view_requirements)
  
    sac_algo.get_policy().unwrapped_model = model
  
    sac_algo.get_policy()._optimizers = force_list(sac_algo.get_policy().optimizer())
  
    sac_algo.get_policy().multi_gpu_param_groups = []
  
    main_params = {p: i for i, p in enumerate(sac_algo.get_policy().model.parameters())}
    
    for o in sac_algo.get_policy()._optimizers:
        param_indices = []
        
        for pg_idx, pg in enumerate(o.param_groups):
            for p in pg[""params""]:
                param_indices.append(main_params[p])
        
        sac_algo.get_policy().multi_gpu_param_groups.append(set(param_indices))
  
for i in range(32768):
    print(f'Iteration: {i}')
  
    sac_algo.train()
  
    if i % 8 == 0:
        sac_algo.save_checkpoint(os.path.join(args.directory, args.name))
```
I’m essentially plugging the old model back in and re-initializing the model-based stuff (like the optimizers).",native model saved case around use model load state state file immediately use without training native model saved new new model original one even save state file load model smaller original one information still missing solution found could solve problem python else model model model list range enumerate enumerate model enumerate model enumerate enumerate set range print essentially plugging old model back stuff like,issue,positive,positive,positive,positive,positive,positive
1953301572,"Hello burton, I'd like to work on this issue! TIA.",hello burton like work issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1953101965,"@kira-lin The docs build is failing because the new doc isn't added to any toctree. Could you add it to the toctree [here](https://docs.ray.io/en/latest/serve/tutorials/index.html), near [this tutorial](https://docs.ray.io/en/latest/serve/tutorials/aws-neuron-core-inference.html)?",build failing new doc added could add near tutorial,issue,negative,positive,positive,positive,positive,positive
1953077218,"some one in @ray-project/ray-docs need to approve.

@angelinalg maybe?",one need approve maybe,issue,negative,neutral,neutral,neutral,neutral,neutral
1953073653,"Hey folks, is there anything else that needs to get done to get this merged?",hey anything else need get done get,issue,negative,neutral,neutral,neutral,neutral,neutral
1952914903,"

facing same issue but  key error is coming for '1' , compute_single_action does not work  ( version 2.9.2 ray)

Running this code



```
done = False
obs = env.reset()
state = algo.get_policy().model.get_initial_state()

while not done:
    action = algo.compute_single_action(obs, prev_action=0, prev_reward=0.0,state=state)
    obs, reward, done, info = env.step(action)
```




getting this error 

```
KeyError                                  Traceback (most recent call last)
Cell In[11], line 64
     61 obs = env.reset()
     63 while not done:
---> 64     action = algo.compute_single_action(obs)
     65     obs, reward, done, info = env.step(action)
     66     episode_reward += reward

File ~/Projects/ving/code/pynotebook/ving_venv/lib/python3.11/site-packages/ray/util/tracing/tracing_helper.py:467, in _inject_tracing_into_class.<locals>.span_wrapper.<locals>._resume_span(self, _ray_trace_ctx, *_args, **_kwargs)
    465 # If tracing feature flag is not on, perform a no-op
    466 if not _is_tracing_enabled() or _ray_trace_ctx is None:
--> 467     return method(self, *_args, **_kwargs)
    469 tracer: _opentelemetry.trace.Tracer = _opentelemetry.trace.get_tracer(
    470     __name__
    471 )
    473 # Retrieves the context from the _ray_trace_ctx dictionary we
    474 # injected.

File ~/Projects/ving/code/pynotebook/ving_venv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:1836, in Algorithm.compute_single_action(self, observation, state, prev_action, prev_reward, info, input_dict, policy_id, full_fetch, explore, timestep, episode, unsquash_action, clip_action, **kwargs)
   1828     action, state, extra = policy.compute_single_action(
   1829         input_dict=input_dict,
   1830         explore=explore,
   1831         timestep=timestep,
   1832         episode=episode,
   1833     )
   1834 # Individual args.
   1835 else:
-> 1836     action, state, extra = policy.compute_single_action(
   1837         obs=observation,
   1838         state=state,
   1839         prev_action=prev_action,
   1840         prev_reward=prev_reward,
   1841         info=info,
   1842         explore=explore,
   1843         timestep=timestep,
   1844         episode=episode,
   1845     )
   1847 # If we work in normalized action space (normalize_actions=True),
   1848 # we re-translate here into the env's action space.
   1849 if unsquash_action:

File ~/Projects/ving/code/pynotebook/ving_venv/lib/python3.11/site-packages/ray/rllib/policy/policy.py:559, in Policy.compute_single_action(self, obs, state, prev_action, prev_reward, info, input_dict, episode, explore, timestep, **kwargs)
    556 if episode is not None:
    557     episodes = [episode]
--> 559 out = self.compute_actions_from_input_dict(
    560     input_dict=SampleBatch(input_dict),
    561     episodes=episodes,
    562     explore=explore,
    563     timestep=timestep,
    564 )
    566 # Some policies don't return a tuple, but always just a single action.
    567 # E.g. ES and ARS.
    568 if not isinstance(out, tuple):

File ~/Projects/ving/code/pynotebook/ving_venv/lib/python3.11/site-packages/ray/rllib/policy/torch_policy_v2.py:572, in TorchPolicyV2.compute_actions_from_input_dict(self, input_dict, explore, timestep, **kwargs)
    565 if state_batches:
    566     seq_lens = torch.tensor(
    567         [1] * len(state_batches[0]),
    568         dtype=torch.long,
    569         device=state_batches[0].device,
    570     )
--> 572 return self._compute_action_helper(
    573     input_dict, state_batches, seq_lens, explore, timestep
    574 )

File ~/Projects/ving/code/pynotebook/ving_venv/lib/python3.11/site-packages/ray/rllib/utils/threading.py:24, in with_lock.<locals>.wrapper(self, *a, **k)
     22 try:
     23     with self._lock:
---> 24         return func(self, *a, **k)
     25 except AttributeError as e:
     26     if ""has no attribute '_lock'"" in e.args[0]:

File ~/Projects/ving/code/pynotebook/ving_venv/lib/python3.11/site-packages/ray/rllib/policy/torch_policy_v2.py:1293, in TorchPolicyV2._compute_action_helper(self, input_dict, state_batches, seq_lens, explore, timestep)
   1291 else:
   1292     dist_class = self.dist_class
-> 1293     dist_inputs, state_out = self.model(input_dict, state_batches, seq_lens)
   1295 if not (
   1296     isinstance(dist_class, functools.partial)
   1297     or issubclass(dist_class, TorchDistributionWrapper)
   1298 ):
   1299     raise ValueError(
   1300         ""`dist_class` ({}) not a TorchDistributionWrapper ""
   1301         ""subclass! Make sure your `action_distribution_fn` or ""
   1302         ""`make_model_and_action_dist` return a correct ""
   1303         ""distribution class."".format(dist_class.__name__)
   1304     )

File ~/Projects/ving/code/pynotebook/ving_venv/lib/python3.11/site-packages/ray/rllib/models/modelv2.py:263, in ModelV2.__call__(self, input_dict, state, seq_lens)
    260         restored[""obs_flat""] = input_dict[""obs""]
    262 with self.context():
--> 263     res = self.forward(restored, state or [], seq_lens)
    265 if isinstance(input_dict, SampleBatch):
    266     input_dict.accessed_keys = restored.accessed_keys - {""obs_flat""}

File ~/Projects/ving/code/pynotebook/ving_venv/lib/python3.11/site-packages/ray/rllib/models/torch/complex_input_net.py:211, in ComplexInputNetwork.forward(self, input_dict, state, seq_lens)
    209         outs.append(one_hot_out)
    210     else:
--> 211         nn_out, _ = self.flatten[i](
    212             SampleBatch(
    213                 {
    214                     SampleBatch.OBS: torch.reshape(
    215                         component, [-1, self.flatten_dims[i]]
    216                     )
    217                 }
    218             )
    219         )
    220         outs.append(nn_out)
    222 # Concat all outputs and the non-image inputs.

File ~/Projects/ving/code/pynotebook/ving_venv/lib/python3.11/site-packages/torch/nn/modules/container.py:461, in ModuleDict.__getitem__(self, key)
    459 @_copy_to_script_wrapper
    460 def __getitem__(self, key: str) -> Module:
--> 461     return self._modules[key]

KeyError: '1'

```
",facing issue key error coming work version ray running code done false state done action reward done action getting error recent call last cell line done action reward done action reward file self tracing feature flag perform none return method self tracer context dictionary file self observation state explore episode action state extra individual else action state extra work action space action space file self state episode explore episode none episode return always single action e file self explore return explore file self try return self except attribute file self explore else raise subclass make sure return correct distribution class file self state state file self state else component file self key self key module return key,issue,positive,positive,neutral,neutral,positive,positive
1952573271,"I can also confirm that I faced the mentioned issue when using Python 3.10 from the Microsoft Store.

By **downloading Python 3.10 from the official website**, I was able to use Ray without any problem (so far).",also confirm faced issue python store python official able use ray without problem far,issue,negative,positive,positive,positive,positive,positive
1952203464,"We  also have this problem two years later. Running date on the head node in our Ray Cluster on EKS gives ""Mon Feb 19 02:58:13 PST 2024"" even though server is in eu-west-1. Shouldn't it be UTC?",also problem two later running date head node ray cluster mon pst even though server,issue,negative,neutral,neutral,neutral,neutral,neutral
1952080248,"@mattip  hi, mattip. Thank you for your comments.

""On one host, the code can keep going on, while on multi node, it will stuck.""  means,  if i use only one windows host to run the demo script,  the 10049 error will also appear, but the training process can continue and the training process was normal. When i use another windows host to join the cluster( ray start --address='xxxxxx') and then run the code on head node, the error will appear and the running process will stuck, the training process doesn't seem to have started.

I set the environment variable RAY_ENABLE_WINDOWS_OR_OSX_CLUSTER=1 and the head node execute the command "" ray start --head --node-ip-address=localhost --port='xxxx' "",  another host node execute the command "" ray start --address='xxxxxx' to join the cluster. And then the head node execute the python demo script. Then the error occurred. BTW, i only use the CPU to run the demo script.

Thanks!",hi thank one host code keep going node stuck use one host run script error also appear training process continue training process normal use another host join cluster ray start run code head node error appear running process stuck training process seem set environment variable head node execute command ray start head another host node execute command ray start join cluster head node execute python script error use run script thanks,issue,negative,positive,positive,positive,positive,positive
1952076003,Hmm isn't that just the actor died because spot instance is interrupted then? ,actor spot instance interrupted,issue,negative,neutral,neutral,neutral,neutral,neutral
1952061272,"Can I initialize multiple ray objects on the same machine?
",initialize multiple ray machine,issue,negative,neutral,neutral,neutral,neutral,neutral
1951456410,"Doesn't look like this got resolved. 

python 3.10 // ubunutu 22.04 // CUDA 12.2",look like got resolved python,issue,negative,neutral,neutral,neutral,neutral,neutral
1951393194,"@jjyao can you please help here, I am not sure why these tests are failing.",please help sure failing,issue,negative,positive,positive,positive,positive,positive
1950613439,"I used https://github.com/ray-project/ray/pull/43117/commits/5186b34951fb317beea59db55997e408d1184e76 to verify that the CI has actually entered the following `if` statement as expected. The repro CI currently doesn't support pre-merge. I have already spent a lot of time trying to reproduce this strange issue. I will reach out to the CI team to discuss how to reproduce it.

```python
if GenericAlias and any(
    (
        method is GenericAlias,
        getattr(method, ""__func__"", None) is GenericAlias,
    )
):
    method = method.__init__
```",used verify actually following statement currently support already spent lot time trying reproduce strange issue reach team discus reproduce python method method none method,issue,negative,negative,neutral,neutral,negative,negative
1950599213,"fixing issue in https://github.com/ray-project/ray/pull/43183 

the testing-tips file is moved to a different location, and now auto included in the `doctest`. however, the code snippets in that file are not really testable, or at least do not work well with other tests.

I am excluding the file from `doctest` glob for now.",fixing issue file different location auto included however code file really testable least work well excluding file,issue,negative,negative,neutral,neutral,negative,negative
1949886843,@aslonnie Here is the workflow for the docker tags clean up. It's quite a long PR so please let me know if you'd like a few parts from the PR description to be moved to another PR. ,docker clean quite long please let know like description another,issue,positive,positive,positive,positive,positive,positive
1949687014,The PR looks good to me! Thank you! @rkooo567  do you want to give another look at this one?,good thank want give another look one,issue,positive,positive,positive,positive,positive,positive
1949544118,"I installed the Python 3.9 artifact on [the Buildkite CI](https://buildkite.com/ray-project/premerge/builds/19301#018daef5-dd55-417a-9bab-f2f6c273ca36), and then replaced the Ray Python scripts with my local ones. I still can't reproduce the CI error.

<img width=""1440"" alt=""Screen Shot 2024-02-16 at 4 45 47 PM"" src=""https://github.com/ray-project/ray/assets/20109646/48e1b733-6e23-4b8c-9ebc-b9516408bf48"">
",python artifact ray python local still ca reproduce error screen shot,issue,negative,neutral,neutral,neutral,neutral,neutral
1949499069,Agreed to make the schema validation to be lazy during groupby map task execution. Looks like a low hanging fruit.,agreed make schema validation lazy map task execution like low hanging fruit,issue,negative,negative,negative,negative,negative,negative
1949483266,seems like issue is resolved (unserializable class). We can probably make error message better. ,like issue resolved class probably make error message better,issue,positive,positive,positive,positive,positive,positive
1949476996,"`ml/core-requirements` is not included in these release tests for some reason. See my attempt here: https://buildkite.com/ray-project/release/builds/8456#018da94d-2df2-4022-a383-63293e561427

I'm not going to spend time to figure it out since I am trying to remove this dependency within a week.",included release reason see attempt going spend time figure since trying remove dependency within week,issue,negative,neutral,neutral,neutral,neutral,neutral
1949471033,"## Workaround 1: Configure framework logging directories

One workaround is to set the log directory for these frameworks to some path outside the Ray Train experiment directory. (The default behavior for a lot of these is the current working directory in the training worker, which is in the experiment dir.)

**Huggingface Transformers Trainer:**

```python
TrainingArguments(output_dir=""/tmp/path"")
```

**Lightning Trainer:**

```python
pl.Trainer(default_root_dir=""/tmp/path"")
```

**wandb:**

```python
wandb.init(dir=""/tmp/path"")
```

## Workaround 2: Disable CWD change behavior

Another workaround is to run with the environment variable `RAY_CHDIR_TO_TRIAL_DIR=0`.

See https://docs.ray.io/en/master/train/user-guides/persistent-storage.html#keep-the-original-current-working-directory.",configure framework logging one set log directory path outside ray train experiment directory default behavior lot current working directory training worker experiment trainer python lightning trainer python python disable change behavior another run environment variable see,issue,negative,neutral,neutral,neutral,neutral,neutral
1949446173,@stephanie-wang Pinging you in case you got busy! Thanks in advance 😃,case got busy thanks advance,issue,negative,positive,positive,positive,positive,positive
1949418590,"**Performance microbenchmark results:**
[unstable] local put, single channel calls per second 228815.06 +- 598.59
[unstable] local put:local get, single channel calls per second 39922.33 +- 1655.47
[unstable] local put:1 remote get, single channel calls per second 39389.14 +- 421.81
Testing multiple readers/channels, n=16
[unstable] local put:n remote get, single channel calls per second 8921.89 +- 484.99
[unstable] local put:1 remote get, n channels calls per second 2611.82 +- 12.54
[unstable] local put:n remote get, n channels calls per second 6607.68 +- 422.72
[unstable] single-actor DAG calls per second 888.7 +- 9.93
[unstable] compiled single-actor DAG calls per second 8464.56 +- 147.99
[unstable] scatter-gather DAG calls, n={n_cpu} actors per second 152.29 +- 3.59
[unstable] compiled scatter-gather DAG calls, n=16 actors per second 2008.02 +- 90.34
[unstable] chain DAG calls, n=16 actors per second 67.49 +- 0.72
[unstable] compiled chain DAG calls, n=16 actors per second 982.68 +- 18.57",performance unstable local put single channel per second unstable local put local get single channel per second unstable local put remote get single channel per second testing multiple unstable local put remote get single channel per second unstable local put remote get per second unstable local put remote get per second unstable dag per second unstable dag per second unstable dag per second unstable dag per second unstable chain dag per second unstable chain dag per second,issue,negative,negative,neutral,neutral,negative,negative
1949413166,"I am also seeing a similar issue with using Ray 2.9. Notably, it looks like the default result directory is determined by one of three variables: https://github.com/ray-project/ray/blob/9641c72d80dc3fd24db62ba5ca9dcfa1bd4e7e3e/python/ray/train/constants.py#L13-L24

where `RAY_AIR_LOCAL_CACHE_DIR` is set by `local_dir`: https://github.com/ray-project/ray/blob/9641c72d80dc3fd24db62ba5ca9dcfa1bd4e7e3e/python/ray/tune/tune.py#L647-L650

Is this intentional? I thought `storage_path` was the replacement for `local_dir`",also seeing similar issue ray notably like default result directory determined one three set intentional thought replacement,issue,positive,positive,positive,positive,positive,positive
1949408428,Running release test to verify fix: https://buildkite.com/ray-project/release/builds/8691,running release test verify fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1949331277,@aslonnie Addressed your comment. Could I get another review / approval? This is to fix broken release tests.,comment could get another review approval fix broken release,issue,negative,negative,negative,negative,negative,negative
1949263383,I have already requested Ray doc team to review this PR.,already ray doc team review,issue,negative,neutral,neutral,neutral,neutral,neutral
1949174579,@akshara08 did you get a chance to work on that PR? Can you link it here?,get chance work link,issue,negative,neutral,neutral,neutral,neutral,neutral
1949155680,"Hi @jjyao,

Thanks for the clarification! Either I missed it, or it its absent from most of the documentation that there is a functional difference between client and job submission. 
May I ask why you don't want to support the client anymore? I find the interactive usage a great feature, because I don't need to ssh to my node and can directly do stuff on my machine too. (As likereviewing the results or saving results to a network storages my node has no access to, because of network policies).",hi thanks clarification either absent documentation functional difference client job submission may ask want support client find interactive usage great feature need node directly stuff machine saving network node access network,issue,positive,positive,positive,positive,positive,positive
1949120911,Also ensured the `blocking` option is in the doc https://anyscale-ray--43227.com.readthedocs.build/en/43227/serve/api/doc/ray.serve.run.html#ray.serve.run,also blocking option doc,issue,negative,neutral,neutral,neutral,neutral,neutral
1948972545,"I have a simplified repo:

```
import ray
from typing import Iterable

@ray.remote
class SyncDataCollector(Iterable):
    def __iter__(self):
        pass

```",simplified import ray import iterable class iterable self pas,issue,negative,neutral,neutral,neutral,neutral,neutral
1948864380,"Just want to add that I have all the logs ready, just let me know which one is required and I can provide them, thanks for helping",want add ready let know one provide thanks helping,issue,positive,positive,positive,positive,positive,positive
1947958154,"> Hello, you should not re-install the tree package but only the dm-tree one.
> 
> Do
> 
> ```shell
> pip uninstall tree
> pip uninstall dm-tree
> 
> pip install --upgrade ray
> 
> #### Here we do not install tree but only dm-tree
> pip install dm-tree
> ```

Thank you very much, now I got it. 

Since dm_tree generates a tree folder and a dm_tree folder describing it, 

I have taken to 

import dm_tree as tree 

which is incorrect since the main files generated in the installation are in the tree folder rather than dm_tree. 

Thank you very much for your reply, as I have just started writing code and my knowledge about it was too weak to understand what you said quickly, now it all makes sense, thank you very much.",hello tree package one shell pip tree pip pip install upgrade ray install tree pip install thank much got since tree folder folder taken import tree incorrect since main installation tree folder rather thank much reply writing code knowledge weak understand said quickly sense thank much,issue,positive,positive,positive,positive,positive,positive
1947929021,It may also be helpful to look at the logs in %TEMP%\ray to see if the worker is failing to start for some reason,may also helpful look temp see worker failing start reason,issue,negative,neutral,neutral,neutral,neutral,neutral
1947792769,"Python 3.9.0 still can't reproduce the issue.


<img width=""1439"" alt=""Screen Shot 2024-02-15 at 9 41 48 PM"" src=""https://github.com/ray-project/ray/assets/20109646/b7c27c94-2cbc-4f48-9ccf-5ecab031bd33"">
",python still ca reproduce issue screen shot,issue,negative,neutral,neutral,neutral,neutral,neutral
1947721878,"The premerge fails on `test_classmethod_genericalias` and `test_placement_group_4.py`. I tried to reproduce locally with Python 3.8 and Python 3.10, but I can't.

* Python 3.10
  <img width=""1438"" alt=""Screen Shot 2024-02-15 at 7 53 29 PM"" src=""https://github.com/ray-project/ray/assets/20109646/da533916-12ea-435a-b1fb-a4e7c2eea708"">

* Python 3.8
  <img width=""1440"" alt=""Screen Shot 2024-02-15 at 5 49 13 PM"" src=""https://github.com/ray-project/ray/assets/20109646/41d92433-0443-43ea-ae62-69a76e89eca1"">
",tried reproduce locally python python ca python screen shot python screen shot,issue,negative,neutral,neutral,neutral,neutral,neutral
1947656755,"Oh forget one more thing, please add the new API to documentation - https://github.com/ray-project/ray/blob/master/doc/source/data/api/data_iterator.rst . Thanks.",oh forget one thing please add new documentation thanks,issue,positive,positive,positive,positive,positive,positive
1947593189,Thanks for starting the thread. I've just followed up with the issue I encountered when building Ray from source (not Python only) in the [discuss thread](https://discuss.ray.io/t/both-cpp-and-python-tests-cannot-be-built-and-fail-with-a-successful-build-from-source/13566/5?u=nemocbb),thanks starting thread issue building ray source python discus thread,issue,negative,positive,neutral,neutral,positive,positive
1947459842,"> IIUC, this is a python 3.9 bug. Do you know if it's fixed in 3.10+?

No, Python 3.10 still has the issue. I am not sure whether to call it a bug or a limitation. The official documentation for `inspect.signature` already acknowledges that ""Note Some callables may not be introspectable in certain implementations of Python. For example, in CPython, some built-in functions defined in C provide no metadata about their arguments"".",python bug know fixed python still issue sure whether call bug limitation official documentation already note may introspectable certain python example defined provide,issue,negative,positive,positive,positive,positive,positive
1947337861,"The per task serialization occurs in Ray Core code (`_raylet.pyx` [here](https://github.com/ray-project/ray/blob/3fe0c07a6e68102fe6d3b2e9d2548efc8d1dd4c1/python/ray/_raylet.pyx#L1647)), which will make it difficult to time this code and attribute it to datasets or operators. At the data level, we will focus on timing the total time of the Ray Data tasks and then the time spent in UDFs. At a later point, Ray Core could add metrics to track this overhead although it still might not be easily attributable to Ray Data tasks.",per task serialization ray core code make difficult time code attribute data level focus timing total time ray data time spent later point ray core could add metric track overhead although still might easily attributable ray data,issue,negative,negative,neutral,neutral,negative,negative
1947262767,"I think they have S3-compatible APIs. then should already be supported. Ray Data uses PyArrow under the hood for accessing cloud storage. So if PyArrow supports them, Ray Data will as well. ",think already ray data hood cloud storage ray data well,issue,negative,neutral,neutral,neutral,neutral,neutral
1947252279,there is no plan to support Hive natively so far. but you can probably use `read_sql` and `write_sql`,plan support hive natively far probably use,issue,negative,positive,neutral,neutral,positive,positive
1946760119,"Hi @Famok,

Is it possible to not use Ray client but [Ray job submission](https://docs.ray.io/en/releases-2.9.2/cluster/running-applications/job-submission/index.html)? We currently don't encourage people to use Ray client anymore so we didn't spend effort to support streaming generator for ray client.

I'll update the doc to explicit mention that Ray client is not supported.",hi possible use ray client ray job submission currently encourage people use ray client spend effort support streaming generator ray client update doc explicit mention ray client,issue,positive,neutral,neutral,neutral,neutral,neutral
1946683810,Moving testing-tips to ray-contribute/ per discussion with @jjyao. Remember to add redirect in RTD.,moving per discussion remember add redirect,issue,negative,neutral,neutral,neutral,neutral,neutral
1946668428,"Hello, you should not re-install the tree package but only the dm-tree one.

Do 

```sh
pip uninstall tree
pip uninstall dm-tree

pip install --upgrade ray

#### Here we do not install tree but only dm-tree
pip install dm-tree
```",hello tree package one sh pip tree pip pip install upgrade ray install tree pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
1946474651,"@ratnopamc just in case you missed the notification, can you address a few comments suggested by the docs team?",case notification address team,issue,negative,neutral,neutral,neutral,neutral,neutral
1946241500,This seems like a pretty bad issue. I will mark P0 for now (we can probably handle it in 2.11),like pretty bad issue mark probably handle,issue,negative,negative,negative,negative,negative,negative
1946239694,@rickyyx have we eventually figured out the root cause? ,eventually figured root cause,issue,negative,neutral,neutral,neutral,neutral,neutral
1946237987,"For port issue, it can happen if you use the default option. For robust port allocation, I recommend you to manually set all ports based on https://docs.ray.io/en/master/ray-core/configure.html#ports-configurations. 

For the first issue, it seems like a real issue. But it also looks like env specific. Can you reproduce the issue if you use conda? ",port issue happen use default option robust port allocation recommend manually set based first issue like real issue also like specific reproduce issue use,issue,positive,positive,positive,positive,positive,positive
1946230400,"The size includes metadata of the object (that's appended by Ray), so it is not guranteed to be exactly same as the original object size. ",size object ray exactly original object size,issue,negative,positive,positive,positive,positive,positive
1946217576,This is the long lasting enhancement request. This requires to synchornize config across the cluster which could be quite complex. P2 until we find compelling use cases,long lasting enhancement request across cluster could quite complex find compelling use,issue,negative,negative,neutral,neutral,negative,negative
1946216519,@iycheng is this class even used? Why isn't this caught by any CI? ,class even used caught,issue,negative,neutral,neutral,neutral,neutral,neutral
1946213329,Let's use tests here for unit test for your PR @rynewang ,let use unit test,issue,negative,neutral,neutral,neutral,neutral,neutral
1946163128,"Hello developers, I also encountered the ""no module named dm_tree"" warning while using the ray library, and after reading the forum discussions, tried

pip uninstall tree
pip uninstall dm-tree

pip install --upgrade ray

pip install tree
pip install dm-tree

but it still didn't work, I went back to my virtual environment's site-package and realized that I only had the
dm_tree-0.1.8.dist-info file, not dm_tree-0.1.8 file.

So I 

pipuninstalled the tree
pip install dm-tree 

again 
and found the same situation.

I think this is very strange and I don't really understand what's going on here, I'm still trying to fix it, if you could give me some advice it would be greatly appreciated!

",hello also module warning ray library reading forum tried pip tree pip pip install upgrade ray pip install tree pip install still work went back virtual environment file file tree pip install found situation think strange really understand going still trying fix could give advice would greatly,issue,negative,positive,positive,positive,positive,positive
1945958261,"Sorry for digging up this issue, I'm still confused:
I still want to use generators. I've read https://docs.ray.io/en/latest/ray-core/ray-generator.html#generators it does not mention it doesn't work as a client.

I can run the examples when executed using a local machine (using ray.init() to start one) but does not work when connection to the same or remote machine using ray.init('ray://...').
```
@ray.remote
def task():
    for i in range(5):
        time.sleep(5)
        yield i

task.remote()
```
It should yield a <ray._raylet.ObjectRefGenerator at 0x24c784784f0>.
but executing it as a client it throws  `AttributeError: 'ObjectRefGenerator' object has no attribute 'binary'`.

trying 'gen = task.remote() is even weirder, because this won't throw an exception but when I try iterating:
```
for g in gen:
    print(g)
```

it will tell me: 'TypeError: 'ClientObjectRef' object is not iterable'

Complete Traceback from task.remote():
```
Traceback (most recent call last):
  File ""c:\environments\ray\lib\site-packages\IPython\core\formatters.py"", line 223, in catch_format_error
    r = method(self, *args, **kwargs)
  File ""c:\environments\ray\lib\site-packages\IPython\core\formatters.py"", line 708, in __call__
    printer.pretty(obj)
  File ""c:\environments\ray\lib\site-packages\IPython\lib\pretty.py"", line 410, in pretty
    return _repr_pprint(obj, self, cycle)
  File ""c:\environments\ray\lib\site-packages\IPython\lib\pretty.py"", line 778, in _repr_pprint
    output = repr(obj)
  File ""python\ray\includes/unique_ids.pxi"", line 79, in ray._raylet.BaseID.__repr__
  File ""c:\environments\ray\lib\site-packages\ray\util\client\common.py"", line 121, in hex
    self._wait_for_id()
  File ""c:\environments\ray\lib\site-packages\ray\util\client\common.py"", line 194, in _wait_for_id
    self._set_id(self._id_future.result(timeout=timeout))
  File ""c:\environments\ray\lib\concurrent\futures\_base.py"", line 458, in result
    return self.__get_result()
  File ""c:\environments\ray\lib\concurrent\futures\_base.py"", line 403, in __get_result
    raise self._exception
AttributeError: 'ObjectRefGenerator' object has no attribute 'binary'
```",sorry digging issue still confused still want use read mention work client run executed local machine start one work connection remote machine task range yield yield client object attribute trying even wo throw exception try gen print tell object complete recent call last file line method self file line file line pretty return self cycle file line output file line file line hex file line file line result return file line raise object attribute,issue,negative,negative,neutral,neutral,negative,negative
1945760407,"There is no config file so reproducing is difficult. I see that the list of packages includes `grpcio==1.6.0` and `pandas==2.2.0`. That version of pandas should be blocked due to #42842, but I think this issue might be caused by `grpcio`. Where are you getting the dependencies, from PyPI (`pip install...`) or from conda/anaconda (`conda install ...`)? Could you try installing `grpcio==1.57`, like the version [ray is built with](https://github.com/ray-project/ray/blob/c6ab40374eb6bdb2bfd2984d039a962946cdc318/bazel/ray_deps_setup.bzl#L237)",file difficult see list version blocked due think issue might getting pip install install could try like version ray built,issue,negative,negative,negative,negative,negative,negative
1945611174,"Discussed offline. The potential solution is to keep the mechanism when there's no working dir. And we can add working dir's path when there's a working dir.

Besides, I feel like it is a bit bad assumption (that's easy to be broken), and it'd be great to programmatically figure out if dependencies are available properly. For example, if the worker node path is not the same, it should raise an exception or warning",potential solution keep mechanism working add working path working besides feel like bit bad assumption easy broken great programmatically figure available properly example worker node path raise exception warning,issue,negative,positive,neutral,neutral,positive,positive
1945593081,"I can reproduce this, from a simple installation of `ray 2.9.1` and `torchaudio==2.1.2`. 

~The [error 89](https://learn.microsoft.com/en-us/windows/win32/debug/system-error-codes--0-499-) rasied by `SymInitialize` (a windows-specific function to enable easier debugging) is `The system cannot start another process at this time.`.~

The [error 87](https://learn.microsoft.com/en-us/windows/win32/debug/system-error-codes--0-499-) rasied by `SymInitialize` (a windows-specific function to enable easier debugging) is `The parameter is incorrect.` which is a bit strange.

Maybe a duplicate of #29037, which was closed without solution since it did not include a reproducer.  I see the symbol in `_raylet.pyd`, `raylet.exe`, and `gcs_server.exe`, as well as [in `pytorch`](https://github.com/peterjc123/pytorch/blob/77a7dc3f6a486f5d3f366161dcc240b8c2663883/c10/util/Backtrace.cpp#L162)  and `rpds-py` (a wrapper for the rust `rpds` crate for persistent data). In the documentation for [`SymInitialize`](https://learn.microsoft.com/en-us/windows/win32/api/dbghelp/nf-dbghelp-syminitialize) it states 
> A process that calls SymInitialize should not call it again unless it calls [SymCleanup](https://learn.microsoft.com/en-us/windows/desktop/api/dbghelp/nf-dbghelp-symcleanup) first. ... All DbgHelp functions, such as this one, are single threaded. Therefore, calls from more than one thread to this function will likely result in unexpected behavior or memory corruption. To avoid this, call SymInitialize only when your process starts and [SymCleanup](https://learn.microsoft.com/en-us/windows/desktop/api/dbghelp/nf-dbghelp-symcleanup) only when your process ends.

I think the call is coming from [absl](https://github.com/abseil/abseil-cpp/blob/797501d12ea767dabdc8d36674e083869e62ee7d/absl/debugging/symbolize_win32.inc#L48), the filename matches. The function `InitializeSymbolizer` is used in [`src/ray/logging.cc`](https://github.com/ray-project/ray/blob/c6ab40374eb6bdb2bfd2984d039a962946cdc318/src/ray/util/logging.cc#L386). This was added in #19423 by @mwtian with the comment
> Also, try to initialize symbolizer in GCS, Raylet and core worker. This is a no-op on MacOS and some Linux environments (e.g. Ray on Ubuntu 20.04 already produces symbolized stack traces), but should make Ray more likely to have symbolized stack traces on other platforms.

It would be nice to not call `absl::InitializeSymbolizer` on Windows if `SymInitialize` has already been called (I don't know how to detect this, and the windows documentation is not helpful) or if the current thread is not the main one. Another alternative, since it seems the call to `absl::InitializeSymbolizer` is ""nice to have"" and not a hard requirement, is to not call it at all on Windows.
",reproduce simple installation ray error function enable easier system start another process error function enable easier parameter bit strange maybe duplicate closed without solution since include reproducer see symbol well wrapper rust crate persistent data documentation process call unless first one single threaded therefore one thread function likely result unexpected behavior memory corruption avoid call process process think call coming function used added comment also try initialize symbolizer raylet core worker ray already stack make ray likely stack would nice call already know detect documentation helpful current thread main one another alternative since call nice hard requirement call,issue,positive,positive,neutral,neutral,positive,positive
1945553012,"In general, clusters are [not supported by ray using windows](https://docs.ray.io/en/master/cluster/getting-started.html#where-can-i-deploy-ray-clusters). 

I need more information to reproduce. @mct211 what exactly do you mean by
> On one host, the code can keep going on, while on multi node, it will stuck.

How are you running the reproducer script? How are you setting up the cluster hinted at by the error line
```
(RayTrainWorker pid=48872) [W C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] \ 
    The client socket has failed to connect to [kubernetes.docker.internal]:65151 (system error: 10049 - 在其上下文中，该请求的地址无效。).
```

System error 10049 is `The requested address is not valid in its context.`, which would hint at a network configuration problem with the `kubernetes.docker.internal` node, so as much detail as possible about how you set things up is required in order to help you work through this (unsupported) mode of running ray.",general ray need information reproduce exactly mean one host code keep going node stuck running reproducer script setting cluster error line client socket connect system error system error address valid would hint network configuration problem node much detail possible set order help work unsupported mode running ray,issue,negative,negative,neutral,neutral,negative,negative
1945478150,"- moved `NodeKind` to protobuf for usage from other places. 
- added missing support of `fake_multiple` node provider imports and v2 compatible flags.  ",usage added missing support node provider compatible,issue,negative,negative,negative,negative,negative,negative
1945454643,"@thestick6123: please open a new issue with all the required information to reproduce (operating system, version(s) of software used, where you got ray and how you installed it ...). Re-closing since you have not responded to the request for more information.",please open new issue information reproduce operating system version used got ray since request information,issue,negative,positive,neutral,neutral,positive,positive
1945205329,"> @aslonnie @can-anyscale can we delay this to ray2.11 since we're delaying our py upgrade plans (py38 base and py311 ceiling) one release?

oss / core team should decide. REEf team does not own the decision. people can always choose to disable/ignore the test. REEf team just wants the decision to be expressed explicitly on CI/CD process.",delay ray since delaying upgrade base ceiling one release core team decide reef team decision people always choose test reef team decision expressed explicitly process,issue,negative,negative,negative,negative,negative,negative
1945197177,"btw to be more clear about feedback (not 100% sure if it is possible);

- Keep everything (decouple or how we track child process' health) as it is. 
- Ignore sigchld from owned children 
- Only handle sigchld from unonwned children and sigkill. 
- We can remove owned children pid when a worker is killed (there must be some sort of hook here)
- Make sure things are logged properly with pid with INFO. 
- The behavior should be clearly specified from core doc. 
- Probably we can remove parent core worker killing child workers and convert it to this mechanism? 

Some other comments;

- Do you think we need a way to exclude subprocesses from being killed? E.g., if an actor start a new job (.sh file) and exits, is there a way to not kill it? My guess is it is  probably not a requirement given we already kill child procs and no one complained (meaning no regression)
- I wonder if we want to do this for subprocesses started from agent.py. Maybe it is okay (because agent.py fate share with raylet). ",clear feedback sure possible keep everything track child process health ignore handle remove worker must sort hook make sure logged properly behavior clearly core doc probably remove parent core worker killing child convert mechanism think need way exclude actor start new job file way kill guess probably requirement given already kill child one meaning regression wonder want maybe fate share raylet,issue,negative,positive,positive,positive,positive,positive
1945182141,"@danielezhu the code looks correct on master. 

https://github.com/ray-project/ray/blob/e973f2ed034834457e4aa5084dc7c1f8240923a8/python/ray/data/datasource/file_based_datasource.py#L123-L130

We merged https://github.com/ray-project/ray/pull/42308 after the branch cut for the 2.9.x releases, so we'd expect 2.9.1 and 2.9.2 to not contain the fix.",code correct master branch cut expect contain fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1945175572,"This comes from how Ray adds to `sys.path`. Prepended this order (last ones most significant)

1. basic python sys.path
2. $PYTHONPATH
3. /home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/workers
4. /home/ray/anaconda3/lib/python3.9/site-packages/ray/thirdparty_files
5. '/home/ray/default' (???)

so the last sys.path entry comes from `job_config.py_driver_sys_path` which is added with `os.path.dirname(os.path.realpath(sys.argv[0]))` if it's SCRIPT_MODE.  https://github.com/ray-project/ray/blob/b3febcdd6be52e9893ef5d617beabf27165885f2/python/ray/_private/worker.py#L2355

The comment is suspicious:
```
        # Add the directory containing the script that is running to the Python
        # paths of the workers. Also add the current directory. Note that this
        # assumes that the directory structures on the machines in the clusters
        # are the same.
```
",come ray order last significant basic python last entry come added comment suspicious add directory script running python also add current directory note directory,issue,negative,positive,neutral,neutral,positive,positive
1945065193,"Hi @bveeramani , it looks like the fix you introduced in #42308 got reverted sometime later, because if we look at `file_based_datasource.py` from release `2.9.1` or `2.9.2`, the old code is being used instead (i.e. `_resolve_paths_and_filesystem(paths, filesystem)` is being called before `_is_local_scheme(paths)`).

[Version 2.9.1 code](https://github.com/ray-project/ray/blob/cfbf98c315cfb2710c56039a3c96477d196de049/python/ray/data/datasource/file_based_datasource.py)

[Version 2.9.2 code](https://github.com/ray-project/ray/blob/fce7a361807580953364e2da964f9498f3123bf9/python/ray/data/datasource/file_based_datasource.py)",hi like fix got sometime later look release old code used instead version code version code,issue,negative,positive,neutral,neutral,positive,positive
1944959036,@aslonnie @can-anyscale can we delay this to ray2.11 since we're delaying our py upgrade plans (py38 base and py311 ceiling) one release?,delay ray since delaying upgrade base ceiling one release,issue,negative,negative,negative,negative,negative,negative
1944574936,I'd like to add the Core examples to this migration. Could you include those examples once I get you a curated list with skill levels?,like add core migration could include get list skill,issue,negative,neutral,neutral,neutral,neutral,neutral
1944529873,"@can-anyscale I specify the `xgboost_ray` / `lightgbm_ray` dependencies in `ml/core-requirements.txt`:

https://github.com/ray-project/ray/blob/c812e31cbd458553a819968ceadae5be34c85e27/python/requirements/ml/core-requirements.txt#L8

Is that enough for these packages to be included in the release test BYOD?

This test includes a post build script that reinstalls the package -- is this not necessary?

https://github.com/ray-project/ray/blob/c812e31cbd458553a819968ceadae5be34c85e27/release/ray_release/byod/byod_xgboost_test.sh#L7",specify enough included release test test post build script package necessary,issue,negative,neutral,neutral,neutral,neutral,neutral
1944446535,"Fixed this by switching to a different AMI - was using the NVIDIA base deep learning AMI (ami-041855406987a648b). 

Switching to the Amazon OSS AMI from January 2024 solved this issue (ami-07252019da3a8acc8).

Not sure if Ray is able to throw errors for incompatible AMIs? 

Posted here in case others also spend inordinate amounts of time debugging this :)

https://forums.developer.nvidia.com/t/nvidia-base-ami-does-not-work-with-ray-on-aws-gpus/282861",fixed switching different ami base deep learning ami switching ami issue sure ray able throw incompatible posted case also spend inordinate time,issue,negative,positive,neutral,neutral,positive,positive
1944403855,"> @mgerstgrasser when you say "" at least 2 cores for the slurm job"" do you refer to ""#SBATCH --cpus-per-task=2"" or decorator @ray.remote(num_cpus=2) for the task inside the code itself? Thank you!

@Pkulyte The former - I don't recall if it was `--cpus-per-task` or one of the equivalent slurm options, but shouldn't make a difference. Note that it still wasn't 100% for me, it just greatly reduced the frequency of failures.",say least job refer decorator task inside code thank former recall one equivalent make difference note still greatly reduced frequency,issue,negative,positive,positive,positive,positive,positive
1944304422,@mattip can you please attempt repro and root cause this?,please attempt root cause,issue,negative,neutral,neutral,neutral,neutral,neutral
1944296558,@mattip can you please triage and take a look,please triage take look,issue,negative,neutral,neutral,neutral,neutral,neutral
1944270694,"I am encountering potentially related error when running a `Tune` job. This also happens very flakily. This happens for some portion of my trials, most of which were successfully saving checkpoints up until the point of failure.  

```bash
File ""/usr/local/lib/python3.10/site-packages/ray/train/lightning/_lightning_utils.py"", line 270, in on_train_epoch_end      
(TunerInternal pid=2285)     train.report(metrics=metrics, checkpoint=checkpoint)                                                                       
(TunerInternal pid=2285)   File ""/usr/local/lib/python3.10/site-packages/ray/train/_internal/session.py"", line 644, in wrapper                          
(TunerInternal pid=2285)     return fn(*args, **kwargs)                                                                                                 
(TunerInternal pid=2285)   File ""/usr/local/lib/python3.10/site-packages/ray/train/_internal/session.py"", line 706, in report                           
(TunerInternal pid=2285)     _get_session().report(metrics, checkpoint=checkpoint)                                                                      
(TunerInternal pid=2285)   File ""/usr/local/lib/python3.10/site-packages/ray/train/_internal/session.py"", line 417, in report                           
(TunerInternal pid=2285)     persisted_checkpoint = self.storage.persist_current_checkpoint(checkpoint)                                                 
(TunerInternal pid=2285)   File ""/usr/local/lib/python3.10/site-packages/ray/train/_internal/storage.py"", line 558, in persist_current_checkpoint       
(TunerInternal pid=2285)     _pyarrow_fs_copy_files(                                                                                                    
(TunerInternal pid=2285)   File ""/usr/local/lib/python3.10/site-packages/ray/train/_internal/storage.py"", line 110, in _pyarrow_fs_copy_files           
(TunerInternal pid=2285)     return pyarrow.fs.copy_files(                                                                                              
(TunerInternal pid=2285)   File ""/usr/local/lib/python3.10/site-packages/pyarrow/fs.py"", line 272, in copy_files                                        
(TunerInternal pid=2285)     _copy_files_selector(source_fs, source_sel,                                                                                
(TunerInternal pid=2285)   File ""pyarrow/_fs.pyx"", line 1627, in pyarrow._fs._copy_files_selector                                                       
(TunerInternal pid=2285)   File ""pyarrow/error.pxi"", line 91, in pyarrow.lib.check_status  
OSError: When uploading part for key 'helm-asha-3/TorchTrainer_2024-02-14_16-02-39/TorchTrainer_7f103_00001_1_data_mute_prob=0.
0592,data_swap_prob=0.1236,data_waveform_prob=0.6707,model_learning_rate=0.0707,model__2024-02-14_16-03-03/checkpoint_000004/checkpoint.ckpt' in bucket 
'aframe-test': AWS Error ACCESS_DENIED during UploadPart operation
```

What is the quickest way to workaround this / is this being investigated? It is mentioned that directly using `s3fs` resolves the problem. How can I setup my tune run to do so? Thanks",potentially related error running tune job also flakily portion successfully saving point failure bash file line file line wrapper return file line report metric file line report file line file line return file line file line file line part key bucket error operation way directly problem setup tune run thanks,issue,negative,positive,positive,positive,positive,positive
1944258513,@zcin I'd still like to add more unit tests but this is ready for review,still like add unit ready review,issue,positive,positive,positive,positive,positive,positive
1944070424,"Thanks @matthewdeng , if I specify `storage_path` in `ray.train.RunConfig`, it helps with the issue (files are no longer erased, and it doesn't print out that output). However, it also still logs some ray specific files to `~/ray_results`

```python
    tuner = tune.Tuner(
        tune.with_resources(
                train,
                resources={""cpu"": 16, ""gpu"": 1}
        ),
        tune_config=tune.TuneConfig(
            metric=METRIC,
            mode=METRICMODE,
            scheduler=scheduler,
            num_samples = 1, 
            reuse_actors=False
        ),
        run_config=RunConfig(storage_path=perf_dir, name='test'),
        param_space=config,
    )
```
Is there anyway to not have ray duplicate all of these ray files into ~/ray_results?

Also is this is correct way to get the storage_path from inside of the training session? Seems a little more burried than I would have expected?
`ray.train.context.TrainContext().get_storage().storage_fs_path`
I previously was using air's `session.get_trial_dir()` but this still points to the location at `~/ray_results` even with specifying `storage_path`

",thanks specify issue longer erased print output however also still ray specific python tuner train anyway ray duplicate ray also correct way get inside training session little would previously air still location even,issue,negative,negative,neutral,neutral,negative,negative
1943935520,"Hi @wpwpxpxp, I'm currently working on a problem using rllib and AIRL. Just wanted to know if you opened source your code in the end.

Thanks",hi currently working problem know source code end thanks,issue,negative,positive,neutral,neutral,positive,positive
1943618718,"Hi @anyscalesam, I have updated the issue for a cleaner reproduction of how to trigger this issue. I can confirm that this is still happening in Ray on all the versions.

Note: I also opened a separate issue for the decorator (#43162) which is not supported either (only when using `ray.remote(MyClass).remote()` I can create actors)",hi issue cleaner reproduction trigger issue confirm still happening ray note also separate issue decorator either create,issue,negative,neutral,neutral,neutral,neutral,neutral
1943205260,Looks like this is not the root cause. Will not merge (for now) and Ricky is doing further investigation.,like root cause merge investigation,issue,negative,neutral,neutral,neutral,neutral,neutral
1943199391,Need to handle drain vs `ray stop` properly here. ,need handle drain ray stop properly,issue,negative,neutral,neutral,neutral,neutral,neutral
1942984948,"> LGTM thanks! Are there e2e tests we can add that tests the retry logic?

The test case I added will test the retry logic non-deterministically (because requests will try to schedule to the same replica).",thanks add retry logic test case added test retry logic try schedule replica,issue,negative,positive,positive,positive,positive,positive
1942909739,"One thing to note is there wasn't any code changes when we started to see this failure, so I initially thought it was more of a dep issue, let's see if this works. 
<img width=""1621"" alt=""image"" src=""https://github.com/ray-project/ray/assets/11676094/3b23aeb6-d75f-4333-b755-c54b6fcaf1d8"">
",one thing note code see failure initially thought issue let see work image,issue,negative,negative,negative,negative,negative,negative
1942854911,"@zhe-thoughts this is a cherry pick to fix postmerge test failures. Test only, no functional change to prod code. Please reveiw.",cherry pick fix test test functional change prod code please,issue,negative,neutral,neutral,neutral,neutral,neutral
1942770932,"Does not build. I noticed there's difference between this and https://github.com/ray-project/ray/commit/149e400f8d66a3de0aef0b8a81fd5f7640fc0d41 in `direct_task_transport.cc`. Please fix this @alexeykudinkin 

```
src/ray/core_worker/transport/direct_task_transport.cc: In member function 'void ray::core::CoreWorkerDirectTaskSubmitter::HandleGetTaskFailureCause(const ray::Status&, bool, const ray::TaskID&, const ray::rpc::WorkerAddress&, const ray::Status&, const ray::rpc::GetTaskFailureCauseReply&)':
--
  | 2024-02-13 14:45:04 PST | src/ray/core_worker/transport/direct_task_transport.cc:715:56: error: no match for call to '(const ray::NodeID) ()'
  | 2024-02-13 14:45:04 PST | 715 \|                      << "" node id: "" << addr.raylet_id() << "" ip: "" << addr.ip_address();
  | 2024-02-13 14:45:04 PST | \|                                                        ^
  | 2024-02-13 14:45:04 PST | src/ray/core_worker/transport/direct_task_transport.cc:715:88: error: no match for call to '(const string {aka const std::basic_string<char>}) ()'
  | 2024-02-13 14:45:04 PST | 715 \|                      << "" node id: "" << addr.raylet_id() << "" ip: "" << addr.ip_address();
  | 2024-02-13 14:45:04 PST | \|
```",build difference please fix member function ray ray bool ray ray ray ray pst error match call ray pst node id pst pst error match call string aka char pst node id pst,issue,negative,neutral,neutral,neutral,neutral,neutral
1942750813,@zhe-thoughts Could you review this cherry pick and approve it? Thanks,could review cherry pick approve thanks,issue,negative,positive,positive,positive,positive,positive
1942723466,I rebased against `master` and this is ready for another review.,master ready another review,issue,negative,positive,positive,positive,positive,positive
1942709676,"Hi all, to add this to the new example gallery that we are building we need the following information:

- Skill level (beginner, intermediate, advanced)
- Frameworks (pytorch, deepspeed, etc)
- Use case (see the use cases section on the primary sidebar on the left of this page: https://docs.ray.io/en/latest/ray-overview/examples.html)

Thank you in advance!",hi add new example gallery building need following information skill level beginner intermediate advanced use case see use section primary left page thank advance,issue,positive,positive,positive,positive,positive,positive
1942598063,btw @alexeykudinkin @rynewang can you check logs of one of random release tests and make sure this is not spammy in logs? I assume there's a possibility this was debug because it can happen under normal condition (though I am not 100% sure). Should take only a couple min to verify it. (maybe check logs from a random scalability tests),check one random release make sure assume possibility happen normal condition though sure take couple min verify maybe check random,issue,positive,positive,neutral,neutral,positive,positive
1942208624,"@aslonnie  I have addressed the comments and triggered another run at https://buildkite.com/ray-project/release-automation/builds/50 to confirm everything's working properly. Once it passes, it's ready to merge.",triggered another run confirm everything working properly ready merge,issue,negative,positive,positive,positive,positive,positive
1942104882,"For light mode: 
1. There's a slight shadow in the design file, is it possible to add that? 
2. Can we make the icon color white (for both light mode and dark mode)
3. Can we increase the side of the icons, maybe to 28x28?

For dark mode: 
1. Can we make the icon color white (for both light mode and dark mode)
2.  Can we increase the side of the icons, maybe to 28x28?
3. The background of the card is transparent. 
Can you try using the dev mode in the same figma file now? I think it should work now. Please let me know if it doesn't. ",light mode slight shadow design file possible add make icon color white light mode dark mode increase side maybe dark mode make icon color white light mode dark mode increase side maybe background card transparent try dev mode file think work please let know,issue,positive,positive,neutral,neutral,positive,positive
1942089479,"In the top nav, `Example Gallery` is not title case. Could you capitalize `Gallery`, please?",top example gallery title case could capitalize gallery please,issue,negative,positive,positive,positive,positive,positive
1941935303,Is anyone able to review this PR and merge if ok? Thanks. ,anyone able review merge thanks,issue,negative,positive,positive,positive,positive,positive
1941827458,"@rkooo567
 
@stephenoffer solution solves the problem with one addendum.  In order for the spread strategy to work correctly for multiple concurrent jobs per worker, you need to set the num_cpus = (n_cpus_per_worker / the # of concurrent joblib jobs per worker.) ",solution problem one addendum order spread strategy work correctly multiple concurrent per worker need set concurrent per worker,issue,negative,neutral,neutral,neutral,neutral,neutral
1941635991,"@rkooo567 

I fixed this issue by configuring this like this:

```
with parallel_config(""ray"", n_jobs=n_joblib_jobs, ray_remote_args=dict(num_cpus=n_cpus_per_worker, scheduling_strategy=""SPREAD"")):
    rfecv.fit(xtrain, ytrain)
```
    
  I might suggest that the ray_remote_args=dict(num_cpus=1) be set as the default.",fixed issue like ray spread might suggest set default,issue,negative,positive,neutral,neutral,positive,positive
1941037897,"Any updates on this? Any alternatives?

As of now, the solution is to pull the rllib_contrib folder in our project, and install it as dev. Even so, we had to use a tons of hack to make it work (e.g. using TD3 requires DDPG which is also in rllib_contrib)",solution pull folder project install dev even use hack make work also,issue,negative,neutral,neutral,neutral,neutral,neutral
1941023171,"@aslonnie  I have:
- Created a tmp dir and install `miniconda` in it.
- Add a function to remove this tmp dir. This function is triggered upon exit (when the script finishes or when there's an error) which should delete all relevant files from `miniconda`.
- Put miniconda installation & sanity check into separate smaller functions. 
- Run all python versions in the same script, and each step on Buildkite should only call the script once now.
- https://buildkite.com/ray-project/release-automation/builds/48 passes for all versions on Python sanity check. 
- Bazel installation & C++ sanity check is in this follow up PR #43131 ",install add function remove function triggered upon exit script error delete relevant put installation sanity check separate smaller run python script step call script python sanity check installation sanity check follow,issue,negative,positive,positive,positive,positive,positive
1940562298,Moving this PR to draft for now since I'm trying out another way to install dependencies (`install-dependencies.sh`),moving draft since trying another way install,issue,negative,neutral,neutral,neutral,neutral,neutral
1940531921,- updated with asserts on unknown ray node state. ,unknown ray node state,issue,negative,negative,neutral,neutral,negative,negative
1940497993,"> The PR looks good to me. I'm wondering how is the error being handled?

Errors will be made available through cloud provider's `poll_errors()`. The cloud provider is now async, and would push errors to a queue internally that can be polled by callers.  See this

https://github.com/ray-project/ray/blob/6b6f23a370d8e78c709f36d51014083ead9a2447/python/ray/autoscaler/v2/instance_manager/node_provider.py#L76-L137",good wondering error handled made available cloud provider cloud provider would push queue internally polled see,issue,negative,positive,positive,positive,positive,positive
1940473562,the minimal test failures are happening in the master too (it is a dependency issue). Please ignore and merge the PR! ,minimal test happening master dependency issue please ignore merge,issue,negative,negative,neutral,neutral,negative,negative
1940439045,I feel good about the PR. But not too involved in this area. I'll let others approve.,feel good involved area let approve,issue,negative,positive,positive,positive,positive,positive
1940378845,"> maybe as a follow up, we should merge the two docker files into one.

Ya I'm thinking of that and actually doing it now for the macos ones. The only difference is which conda & bazel version to use, which we can probably pass an arg in for instead of a whole new file. ",maybe follow merge two docker one ya thinking actually difference version use probably pas instead whole new file,issue,negative,positive,positive,positive,positive,positive
1940362821,"I am force merging this for you.

the min install tests are bad due to something else.

not sure about the failing windows tests though.",force min install bad due something else sure failing though,issue,negative,negative,negative,negative,negative,negative
1940204119,The PR looks good to me. I'm wondering how is the error being handled?,good wondering error handled,issue,negative,positive,positive,positive,positive,positive
1940105595,"> I want to understand the semantics here, it sounds like this is best effort, not guaranteed that the rank zero worker will be on the trainer node. If that's the case it doesn't actually provide correctness for use cases that require more memory on rank 0.

Yes, it's true. It will try to colocate trainer and rank 0 worker if it's feasible to accommodate the combined resource bundle onto a single node. 

> We could disable the sorting if rank 0 resources are specified right?

Unfortunately we can't. Libraries like deepspeed and huggingface accelerate assume that the ranks are aligned with the gpu ids. More details in https://github.com/ray-project/ray/issues/40803.

> Tune schedulers assume all workers have the same amount of resources.

It's an implementation detail. Tune scheduler can dynamically adjust the workers resources and assumes every worker have the same amount of base resource. Not so important since we can bypass this if necessary.
",want understand semantics like best effort rank zero worker trainer node case actually provide correctness use require memory rank yes true try trainer rank worker feasible accommodate combined resource bundle onto single node could disable rank right unfortunately ca like accelerate assume tune assume amount implementation detail tune dynamically adjust every worker amount base resource important since bypass necessary,issue,positive,negative,negative,negative,negative,negative
1939917584,"> In this case, the rank 0 worker and the trainer will be scheduled on the same A100 node, which has at least 200GB memory.

I want to understand the semantics here, it sounds like this is best effort, not guaranteed that the rank zero worker will be on the trainer node. If that's the case it doesn't actually provide correctness for use cases that require more memory on rank 0.

> Ray Train always sorts the worker by node id, gpu id, and colocate with trainer. So, under the current design, there's no guarantee that the global rank equals to the corresponding bundle index. (major reason)

We could disable the sorting if rank 0 resources are specified right?

> Tune schedulers assume all workers have the same amount of resources.

Hmm not sure I get this, what's the concrete issue?

",case rank worker trainer node least memory want understand semantics like best effort rank zero worker trainer node case actually provide correctness use require memory rank ray train always worker node id id trainer current design guarantee global rank corresponding bundle index major reason could disable rank right tune assume amount sure get concrete issue,issue,positive,negative,negative,negative,negative,negative
1939855881,Seems to be resolved now. We can close this one and reopen if we see issues coming back.,resolved close one reopen see coming back,issue,negative,neutral,neutral,neutral,neutral,neutral
1939815630,Does it happen if you don't use spot instance? ,happen use spot instance,issue,negative,neutral,neutral,neutral,neutral,neutral
1939806882,"> trainer_resources as a concept is not necessary, and I would like to get rid of it in the long term.
> So, this decision to double down on ""colocating rank 0 with the trainer"" makes it a bit harder to remove the trainer resources concept.

@justinvyu  +1. I don't think `trainer_resources` should be a concept for Ray Train at all. However, since Ray Train is built on top of Tune Trainable, it unavoidable to mention this concept, and this is the only way we can think of to support extra rank 0 resources without fully refractoring `BackendExecutor` and `WorkerGroup`.

> If we considered the problem without Tune Trainable creation in mind, would it make more sense to pass in a list of resources_per_worker?

That's my initial idea. But there are a bunch of limitations that blocked me from doing it:
1. Ray Train always sorts the worker by node id, gpu id, and colocate with trainer. So, under the current design, there's no guarantee that the global rank equals to the corresponding bundle index. (major reason)
2. Tune schedulers assume all workers have the same amount of resources. (implementation details)

The conclusion is, if Ray Core cannot schedule workers in the order of GPU id, we cannot build a static mapping from `rank` to `bundle`, thus won't be able to use a list of `resources_per_worker`.",concept necessary would like get rid long term decision double rank trainer bit harder remove trainer concept think concept ray train however since ray train built top tune trainable unavoidable mention concept way think support extra rank without fully considered problem without tune trainable creation mind would make sense pas list initial idea bunch blocked ray train always worker node id id trainer current design guarantee global rank corresponding bundle index major reason tune assume amount implementation conclusion ray core schedule order id build static rank bundle thus wo able use list,issue,positive,negative,negative,negative,negative,negative
1939805228,In the future such things can probably get lumped into some field in the proxy `grpc_options`. But looks good for now,future probably get field proxy good,issue,negative,positive,positive,positive,positive,positive
1939801853,"Conducted some manual testing with the following snippet (and adding a line to log the time from within `StreamingExecutor.run`):
```python
def sleep(x):
    time.sleep(0.5)
    return x

num_rows = sys.argv[1] if len(sys.argv) > 1 else 10

ds = ray.data.range(num_rows).map(sleep)

for _ in ds.iter_batches(batch_size=1):
    continue
```
For `num_rows = 100` the total scheduling time was `0.23742400000000297` and for `num_rows = 1000` the total scheduling time was `1.063609000000004`. More comprehensive testing will be added in followup pr which will update the metrics being reported out through `DatasetStatsSummary`.",manual testing following snippet line log time within python sleep return else sleep continue total time total time comprehensive testing added update metric,issue,negative,neutral,neutral,neutral,neutral,neutral
1939762801,"This is a WIP as I have not yet implemented the second approach involving in-memory shuffling, nor performed benchmarking showing the performance difference between these two approaches and the original baseline. I'm just waiting for feedback on #42146.",yet second approach shuffling showing performance difference two original waiting feedback,issue,negative,positive,positive,positive,positive,positive
1939723881,"This PR #43117 appears to fix the issue. If the CI passes, I'll delve deeper to get more details about the root cause.",fix issue delve get root cause,issue,negative,neutral,neutral,neutral,neutral,neutral
1939584395,"> * please rebase or merge master head
> * after merging, could you post a link of a passing test run on the release-automation pipeline?

Rebase done. The new build also passes sanity check on both Python & C++ https://buildkite.com/ray-project/release-automation/builds/24",please rebase merge master head could post link passing test run pipeline rebase done new build also sanity check python,issue,negative,positive,positive,positive,positive,positive
1939557266,@aslonnie @GeneDer this PR is force-pushed updated. Please check. Please also help figure out if `doc/source/ray-overview/pip_freeze_ray-py39-cpu.txt` and `doc/source/ray-overview/pip_freeze_ray-ml-py39-cpu.txt` changes make sense. Thanks.,please check please also help figure make sense thanks,issue,positive,positive,positive,positive,positive,positive
1939543083,"When it comes to evaluating the performance of this PR, is there a preferred approach?

In the docs there a few image and CSV formatted datasets. For example we have the following:

```python
ds = ray.data.read_csv(""s3://anonymous@air-example-data/breast_cancer.csv"")
```

It contains 569 records.

```
Dataset(
   num_blocks=20,
   num_rows=569,
   schema={
   ...
   }
)
```
I'm guessing we can do comparisons between a few different datasets like the following:

```python
import timeit
num_trials = 1000
elapsed_time = timeit.timeit(ds.random_shuffle, number=num_trials)
print(f'Average time: {elapsed_time / num_trials:.6f} seconds')
```

Is there a requirement behind the speedup? How much should the benchmarks vary in size? I was only able to find two other datasets that have <4K images. Is that not large enough? Should I just fill up large datasets with random values? What if my machine can't fit that in-memory, can I cap it to a certain amount? 

",come performance preferred approach image example following python guessing different like following python import print time requirement behind much vary size able find two large enough fill large random machine ca fit cap certain amount,issue,positive,positive,neutral,neutral,positive,positive
1939516139,"I also have a proposed solution for the copying in-place, I wanted to run it by you first though:

I'm going to assume that by setting the same seed within the same function scope will guarantee the same shuffle result.

```python
import numpy as np
import pyarrow as pa

def my_shuffle(arr: pa.array) -> pa.array:
    np.random.seed(1234)
    arr = np.array(arr.to_numpy())
    np.random.shuffle(arr)
    return pa.array(arr)
```

```python
>>> print(f'{[my_shuffle(pa.array([1,2,3,4])) for i in range(2)]=}')
[my_shuffle(pa.array([1,2,3,4])) for i in range(5)]=[<pyarrow.lib.Int64Array object at 0x10060ac80>
[
  4,
  3,
  1,
  2
], <pyarrow.lib.Int64Array object at 0x12f49c220>
[
  4,
  3,
  1,
  2
]]
```


#### Proposed Solution

I'm also assuming PyArrow tables are immutable and I cannot directly change the data of a column. Also my other assumption is that the PyArrow tables contain references to PyArrow arrays. So, I will create a new table with the modified column after each shuffle.


```python
def table_shuffle(table: pa.table) -> pa.table:
     num_columns = table.num_columns
     for i in range(num_columns):
             table = table.set_column(i, table.column_names[i], my_shuffle(table.column(i)))
     return table
```


Here is a sample execution of the function:

```python
>>> n_legs = pa.array([2, 4, 5, 100])
>>> animals = pa.array([""Flamingo"", ""Horse"", ""Brittle stars"", ""Centipede""])
>>> names = [""n_legs"", ""animals""]
>>> table = pa.Table.from_arrays([n_legs, animals], names=names)
>>> table
pyarrow.Table
n_legs: int64
animals: string
----
n_legs: [[2,4,5,100]]
animals: [[""Flamingo"",""Horse"",""Brittle stars"",""Centipede""]]

>>> table_shuffle(table)
pyarrow.Table
n_legs: int64
animals: string
----
n_legs: [[100,5,2,4]]
animals: [[""Centipede"",""Brittle stars"",""Flamingo"",""Horse""]]
```


",also solution run first though going assume setting seed within function scope guarantee shuffle result python import import pa return python print range range object object solution also assuming table immutable directly change data column also assumption table contain create new table column shuffle python table range table return table sample execution function python flamingo horse brittle centipede table table string flamingo horse brittle centipede table string centipede brittle flamingo horse,issue,positive,positive,positive,positive,positive,positive
1939510614,"Looks like starlette is no longer a dependency, so this is no longer needed.",like longer dependency longer,issue,negative,neutral,neutral,neutral,neutral,neutral
1939508028,"Thanks so much for the follow-up @stephanie-wang. I'm happy to split the work with @chrislevn or side it solo. Let me know what you all prefer ☺️

For now, I wanted to ask two more clarifying questions:

Just so that it is clear to me, what I'm hearing is that it is fine to shuffle in-place using a second data structure only if it means that we do a single copy? In other words, copying the entire table into data structures and doing the shuffle in-memory in those data structures and copying back? 

I've never worked with managing memory between PyArrow and Python, so how should someone in this PR track memory usage during the shuffling operation?

I've tried figuring this out myself, but its still not clear to me. I have the following PyArrow table:

```python
>>> import pyarrow as pa
>>> pa.total_allocated_bytes()
0
>>> n_legs = pa.array([2, 4, 5, 100])
>>> animals = pa.array([""Flamingo"", ""Horse"", ""Brittle stars"", ""Centipede""])
>>> names = [""n_legs"", ""animals""]
>>> table.nbytes
83
>>> pa.total_allocated_bytes()
192
>>> table = pa.Table.from_arrays([n_legs, animals], names=names)
>>> pa.total_allocated_bytes()
192
>>> table.take([0, 3, 2, 1])
pyarrow.Table
n_legs: int64
animals: string
----
n_legs: [[2,100,5,4]]
animals: [[""Flamingo"",""Centipede"",""Brittle stars"",""Horse""]]
>>> pa.total_allocated_bytes()
512
>>> pa.total_allocated_bytes()
192
```

I noticed that the `total_allocated_bytes()` value fluctuates after a few moments. 

Also, when I delete the reference toward python objects, it seems to not have an effect on the PyArrow memory pool.

```python
>>> del n_legs
>>> del animals
>>> import gc
>>> gc.collect()
28
>>> pa.total_allocated_bytes()
192
```

Should I assume that `total_allocated_bytes` provides a ceiling of a memory pool and then PyArrow manages its memory pool because, after the `take` operation, the size is reduced? Are there better ways of doing this?",thanks much happy split work side solo let know prefer ask two clear hearing fine shuffle second data structure single copy entire table data shuffle data back never worked memory python someone track memory usage shuffling operation tried still clear following table python import pa flamingo horse brittle centipede table string flamingo centipede brittle horse value also delete reference toward python effect memory pool python import assume ceiling memory pool memory pool take operation size reduced better way,issue,positive,positive,positive,positive,positive,positive
1939488749,"looks like all tests passed; should be good to merge.

I will continue investigating the min install thing",like good merge continue investigating min install thing,issue,positive,positive,positive,positive,positive,positive
1939478069,could you merge or rebase on latest master head?,could merge rebase latest master head,issue,negative,positive,positive,positive,positive,positive
1939477911,"Note: 

1. we may not want a hard sigkill at first. We can do something like ""sigterm, after 5s then sigkill"".
2. I am wondering how waterproof is this approach. If we spawn a one time command (`bash start_something.sh`) which spawns a tool, will it be killed? Maybe we can bring back the ""decouple"" arg and only care about the ""coupled"" processes, i.e. workers.",note may want hard first something like wondering waterproof approach spawn one time command bash tool maybe bring back care coupled,issue,positive,negative,neutral,neutral,negative,negative
1939449152,"Updated, now the subreaper test works. i.e. if the worker is dead, its subprocesses are sigkilled.",test work worker dead,issue,negative,negative,negative,negative,negative,negative
1939325558,"> I am updating your branch to rerun the premerge tests. the test failure of `test_update_task` seems unrelated to the ""min install"" failures.

Thank you!!!",branch rerun test failure unrelated min install thank,issue,negative,negative,negative,negative,negative,negative
1939321499,"oh, I see. The command should be running to change 2.9.2 to 2.9.3.",oh see command running change,issue,negative,neutral,neutral,neutral,neutral,neutral
1939258159,"I am updating your branch to rerun the premerge tests. the test failure of `test_update_task` seems unrelated to the ""min install"" failures.",branch rerun test failure unrelated min install,issue,negative,negative,negative,negative,negative,negative
1939226031,"@peytondmurray - Check out this design - https://www.figma.com/proto/DjtgroLeO6BQSD6Si5dedw/Ray-docs-design-system?page-id=1%3A10&type=design&node-id=397-1375&viewport=-10292%2C-7628%2C0.63&t=8I7J9HBD2Pdwj4RK-1&scaling=min-zoom&starting-point-node-id=1%3A176&show-proto-sidebar=1&mode=design
For filters, as you suggested, we can add another dropdown picker with the following options - ""All examples"",""Contributed by Ray community"", ""Examples by the Ray team"". 
@angelinalg Do you have any suggestions on the text for these filters? ",check design add another picker following ray community ray team text,issue,negative,neutral,neutral,neutral,neutral,neutral
1939178122,"I can reproduce this issue. I will try to fix it.

> I tried for python 3.9 and got the same error, but for python 3.8 I dont get it

Based on https://docs.python.org/3/library/types.html#types.GenericAlias, `types.GenericAlias` is first introduced in Python 3.9.",reproduce issue try fix tried python got error python dont get based first python,issue,negative,positive,positive,positive,positive,positive
1938524181,"I tried build C++ code only, but looks like go_sdk download is hanging
```
dcg@oq1:/mnt/nvme1n1/pfray/ray/cpp$ bazel build //cpp:libray_api.so
Starting local Bazel server and connecting to it...
DEBUG: /mnt/nvme1n1/pfray/ray/bazel/ray_deps_setup.bzl:67:14: No implicit mirrors used because urls were explicitly provided
DEBUG: /mnt/nvme1n1/pfray/ray/bazel/ray_deps_setup.bzl:67:14: No implicit mirrors used because urls were explicitly provided
Analyzing: target //cpp:libray_api.so (42 packages loaded, 9 targets configured)
    Fetching @go_sdk; Downloading and extracting Go toolchain 12s
^C
Analyzing: target //cpp:libray_api.so (42 packages loaded, 9 targets configured)
    Fetching @go_sdk; Downloading and extracting Go toolchain 16s
^C
INFO: Repository go_sdk instantiated at:
  /mnt/nvme1n1/pfray/ray/WORKSPACE:17:16: in <toplevel>
  /home/dcg/.cache/bazel/_bazel_dcg/86c0b2da2f7232f44660b8e2d43c00fd/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:56:27: in grpc_extra_deps
  /home/dcg/.cache/bazel/_bazel_dcg/86c0b2da2f7232f44660b8e2d43c00fd/external/io_bazel_rules_go/go/private/sdk.bzl:453:28: in go_register_toolchains
  /home/dcg/.cache/bazel/_bazel_dcg/86c0b2da2f7232f44660b8e2d43c00fd/external/io_bazel_rules_go/go/private/sdk.bzl:129:21: in go_download_sdk
Repository rule _go_download_sdk defined at:
  /home/dcg/.cache/bazel/_bazel_dcg/86c0b2da2f7232f44660b8e2d43c00fd/external/io_bazel_rules_go/go/private/sdk.bzl:116:35: in <toplevel>
ERROR: An error occurred during the fetch of repository 'go_sdk':
   Traceback (most recent call last):
        File ""/home/dcg/.cache/bazel/_bazel_dcg/86c0b2da2f7232f44660b8e2d43c00fd/external/io_bazel_rules_go/go/private/sdk.bzl"", line 100, column 16, in _go_download_sdk_impl
                _remote_sdk(ctx, [url.format(filename) for url in ctx.attr.urls], ctx.attr.strip_prefix, sha256)
        File ""/home/dcg/.cache/bazel/_bazel_dcg/86c0b2da2f7232f44660b8e2d43c00fd/external/io_bazel_rules_go/go/private/sdk.bzl"", line 189, column 21, in _remote_sdk
                ctx.download(
Error in download: java.io.IOException: thread interrupted
ERROR: /mnt/nvme1n1/pfray/ray/WORKSPACE:17:16: fetching _go_download_sdk rule //external:go_sdk: Traceback (most recent call last):
        File ""/home/dcg/.cache/bazel/_bazel_dcg/86c0b2da2f7232f44660b8e2d43c00fd/external/io_bazel_rules_go/go/private/sdk.bzl"", line 100, column 16, in _go_download_sdk_impl
                _remote_sdk(ctx, [url.format(filename) for url in ctx.attr.urls], ctx.attr.strip_prefix, sha256)
        File ""/home/dcg/.cache/bazel/_bazel_dcg/86c0b2da2f7232f44660b8e2d43c00fd/external/io_bazel_rules_go/go/private/sdk.bzl"", line 189, column 21, in _remote_sdk
                ctx.download(
Error in download: java.io.IOException: thread interrupted
ERROR: build interrupted
INFO: Elapsed time: 20.982s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (42 packages loaded, 9 targets configured)
dcg@oq1:/mnt/nvme1n1/pfray/ray/cpp$ 
```",tried build code like hanging build starting local server implicit used explicitly provided implicit used explicitly provided target loaded fetching go target loaded fetching go repository repository rule defined error error fetch repository recent call last file line column sha file line column error thread interrupted error fetching rule recent call last file line column sha file line column error thread interrupted error build interrupted time build complete successfully loaded,issue,negative,positive,positive,positive,positive,positive
1938229787,"> can we split the c++ sanity check and arm64 into 2 separate PR's?

I reverted this PR into just adding Arm64 and its Python sanity check. Bazel, Clang installation and C++ sanity check are moved to #43094 ",split sanity check arm separate arm python sanity check clang installation sanity check,issue,negative,neutral,neutral,neutral,neutral,neutral
1937841201,"I presently use FLAML with Ray Tune to run automl jobs. With FLAML's blendsearch and cfo algos no longer available in Ray Tune, is their a recommended alternative for using Tune to perform automl?",presently use ray tune run longer available ray tune alternative tune perform,issue,negative,positive,positive,positive,positive,positive
1937778665,I believe the test failures are due to a transient issue with hugging face,believe test due transient issue hugging face,issue,negative,negative,negative,negative,negative,negative
1937774844,"@apple1113, I had a similar problem, your code is working for me in Ubuntu with Python 3.9, these are my library versions:

```
gymnasium 0.28.1
nvidia-cuda-runtime-cu12 12.1.105
ray 2.9.2
tensorboard 2.15.2
tensorboardX 2.6.2.2
tensorflow 2.15.0.post1
tensorflow-probability 0.23.0
torch 2.2.0
```",apple similar problem code working python library gymnasium ray post torch,issue,negative,neutral,neutral,neutral,neutral,neutral
1937773157,@GeneDer thanks a lot for your review. All checks are passing now.,thanks lot review passing,issue,negative,positive,positive,positive,positive,positive
1937451811,"this problem still exist, I just update Ray to nightly build then run this benchmark, it will failed.
```
root@oq1:/mnt/nvme1n1/pfray/ray/release/air_tests/air_benchmarks# python workloads/torch_benchmark.py run --num-runs 3 --num-epochs 20 --num-workers 4 --cpus-per-worker 8
Traceback (most recent call last):
  File ""/mnt/nvme1n1/pfray/ray/release/air_tests/air_benchmarks/workloads/torch_benchmark.py"", line 581, in <module>
    main()
  File ""/mnt/nvme1n1/pfray/ray/release/air_tests/air_benchmarks/workloads/torch_benchmark.py"", line 577, in main
    return cli()
  File ""/usr/lib/python3/dist-packages/click/core.py"", line 1128, in __call__
    return self.main(*args, **kwargs)
  File ""/usr/lib/python3/dist-packages/click/core.py"", line 1053, in main
    rv = self.invoke(ctx)
  File ""/usr/lib/python3/dist-packages/click/core.py"", line 1659, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/usr/lib/python3/dist-packages/click/core.py"", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/usr/lib/python3/dist-packages/click/core.py"", line 754, in invoke
    return __callback(*args, **kwargs)
  File ""/mnt/nvme1n1/pfray/ray/release/air_tests/air_benchmarks/workloads/torch_benchmark.py"", line 415, in run
    ray.init(""auto"")
  File ""/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py"", line 1572, in init
    bootstrap_address = services.canonicalize_bootstrap_address(address, _temp_dir)
  File ""/usr/local/lib/python3.10/dist-packages/ray/_private/services.py"", line 530, in canonicalize_bootstrap_address
    addr = get_ray_address_from_environment(addr, temp_dir)
  File ""/usr/local/lib/python3.10/dist-packages/ray/_private/services.py"", line 423, in get_ray_address_from_environment
    raise ConnectionError(
ConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting `--address` flag or `RAY_ADDRESS` environment variable.
root@oq1:/mnt/nvme1n1/pfray/ray/release/air_tests/air_benchmarks# 
```
but I can run python workloads/pytorch_training_e2e.py --data-size-gb=1 with great result. why?",problem still exist update ray nightly build run root python run recent call last file line module main file line main return file line return file line main file line invoke return file line invoke return file line invoke return file line run auto file line wrapper return file line address file line file line raise could find running ray instance please specify one connect setting address flag environment variable root run python great result,issue,positive,positive,positive,positive,positive,positive
1937405985,"Okay so I had some time today to really sit down and pick apart this issue and happy to report back I was able to get both the Kuberay (Kubernetes) and VM (local) examples running on my Macbook Pro M3.

Below I will break down what I did and provide some suggestions to ensure others don't run into this problem moving forward.

Hopefully this information helps some people save some time.  

## Example Kind (Kubernetes)
https://docs.ray.io/en/latest/cluster/kubernetes/getting-started/raycluster-quick-start.html#kuberay-raycluster-quickstart

### Stale Version and AMD64 Image
So as it turns out by default the version of the HELM chart you provide in your example is using `--version 1.0.0`. This version is pinned to docker image `2.7.0` which is only compatible with AMD64 platforms. All M based macs want to run ARM64 based images. The AMD64 images run I suspect due to rosetta magic but this can be problematic for a number of reasons hence the error I was getting.

**Work around:**
Export the `ClusterRay` object from the cluster.
Update the image references in the YAML manifest to the latest version but a tag that is compatible with ARM64. In my case I used `rayproject/ray:2.9.1.98da04-py311-aarch64` and this fixed the issue for me. 

Now as per the doco it works:
```bash
kubectl exec -it $HEAD_POD -- python -c ""import ray; ray.init(); print(ray.cluster_resources())""
2024-02-10 18:42:43,313	INFO worker.py:1405 -- Using address 127.0.0.1:6379 set in the environment variable RAY_ADDRESS
2024-02-10 18:42:43,313	INFO worker.py:1540 -- Connecting to existing Ray cluster at address: 10.244.0.8:6379...
2024-02-10 18:42:43,317	INFO worker.py:1715 -- Connected to Ray cluster. View the dashboard at http://10.244.0.8:8265
{'node:__internal_head__': 1.0, 'CPU': 2.0, 'node:10.244.0.8': 1.0, 'object_store_memory': 751790898.0, 'memory': 3000000000.0, 'node:10.244.0.9': 1.0}
```

**Suggestions:**

1. For images that are semantic and pure in nature (example `2.7.0`, `2.9.1` etc), create these basic core images as multi-arch so that both AMD64 and ARM64 are supported in the one image reference. You should use these in your examples/doco by default as it means a better experience for developers. ARM64 is not going away so best to make support for it a first class citizen. Supporting multi-arch under a single tag has long been the industry standard so a good idea to follow suite.

> Example multi-arch REDIS image

<img width=""501"" alt=""multi-arch-example"" src=""https://github.com/ray-project/ray/assets/4919068/d4199107-4103-4863-943b-6c3edc7ca651"">

2. Update doco to use latest helm chart. Currently `--version 1.0.0` is pinned to `2.7.0` which is old. Best for doco to always point to latest stable version. 

**NOTE:** until you figure out publishing multi-arch image building a quick workaround would be to update the doco to tell customers running on M based Mac's and/or ARM based processors to override XYZ HELM chart variable with an image tag that is compatible. 

## Example VM
https://docs.ray.io/en/latest/cluster/vms/getting-started.html

For this example I cannot be 100% certain but it appeared to be failing (sometimes) when calling `socket.gethostname()`.

**Work around:**

By replacing `socket.gethostname()` with `'localhost'` in the example I was able to get it working consistently. 

This stack overflow article alludes to some problems experienced during that function call. 
https://stackoverflow.com/questions/39970606/gaierror-errno-8-nodename-nor-servname-provided-or-not-known-with-macos-sie

It was likely hitting that hostname call with no entry in the hosts file and doing so in parallel was DDoS'ing the service that provides that information by doing a lookup. When code run without RAY decorators worked because this was all happening slowly and in series. Using ""localhost"" because it is in the hosts file by default prevented this overhead and made the example stable.

Being a NEW mac I don't have my hostname in the `/etc/hosts` file. 
```bash
cat /etc/hosts
##
# Host Database
#
# localhost is used to configure the loopback interface
# when the system is booting.  Do not change this entry.
##
127.0.0.1	localhost
255.255.255.255	broadcasthost
::1             localhost
```

**Suggestions:**

1. Change the example from `return socket.gethostbyname(socket.gethostname())` to `return socket.gethostbyname('localhost')`. ",time today really sit pick apart issue happy report back able get local running pro break provide ensure run problem moving forward hopefully information people save time example kind stale version image turn default version helm chart provide example version version pinned docker image compatible based want run arm based run suspect due magic problematic number hence error getting work around export object cluster update image manifest latest version tag compatible arm case used fixed issue per work bash python import ray print address set environment variable ray cluster address connected ray cluster view dashboard semantic pure nature example create basic core arm one image reference use default better experience arm going away best make support first class citizen supporting single tag long industry standard good idea follow suite example image update use latest helm chart currently version pinned old best always point latest stable version note figure image building quick would update tell running based mac arm based override helm chart variable image tag compatible example example certain failing sometimes calling work around example able get working consistently stack overflow article experienced function call likely call entry file parallel service information code run without ray worked happening slowly series file default overhead made example stable new mac file bash cat host used configure interface system booting change entry change example return return,issue,positive,positive,positive,positive,positive,positive
1937349684,"Generating logs in trainables is currently breaking FLAML AutoML's Ray Tune backend. I have described the specifics on the Ray community slack in

https://ray-distributed.slack.com/archives/CNECXMW22/p1706587052182639

",generating currently breaking ray tune ray community slack,issue,negative,neutral,neutral,neutral,neutral,neutral
1937208379,"Those links are confusing lol Got this merged https://github.com/ray-project/images/pull/18 But there are more issue with your PR tho, make sure to fix the rest",link got issue tho make sure fix rest,issue,negative,positive,positive,positive,positive,positive
1937091510,"Do I understand correctly that this issue was not actually solved, but moved to the #40754? And are there any workarounds for this issue?
From what I understand it makes Ray impossible to use right now for many ML training tasks, where bottleneck on consumer is a desired behavior.",understand correctly issue actually issue understand ray impossible use right many training bottleneck consumer desired behavior,issue,negative,positive,neutral,neutral,positive,positive
1937078072,@shrekris-anyscale @alexeykudinkin any update on if the above changes have addressed the issue or we're closer to a root cause?,update issue closer root cause,issue,negative,neutral,neutral,neutral,neutral,neutral
1936997366,"I confirm I don't get the error for python 3.8 on Windows 11 with this minimal example
```
python3.8 -m venv 3.8_torchrl
3.8_torchrl\Scripts\Activate.ps1
pip install torchrl 
pip install ray
```
Python 3.8.10 on Windows 11
ray 2.9.2
torch 2.2.0
torchrl 0.3.0",confirm get error python minimal example python pip install pip install ray python ray torch,issue,negative,negative,neutral,neutral,negative,negative
1936992958,"> In RLlib, I would like to have access to the policy model in the environment step function, to print some debugging information at each timestep. I would also need to make the step function depend on the state of a layer in my policy model. Is this possible?
> 
> I couldn't find a way to get any information about the model, except for the output action of course, in the environment. Currently, I am passing tensors I would like to print concatenated with the action, but this is quite cumbersome. Thanks!

@adriendoerig  Did you find any solution for your question?",would like access policy model environment step function print information would also need make step function depend state layer policy model possible could find way get information model except output action course environment currently passing would like print action quite cumbersome thanks find solution question,issue,positive,positive,neutral,neutral,positive,positive
1936906428,"> Looked reasonable when I last looked at it. Unfortunately, I don't have bandwidth for a careful review.
> 
> The key thing is to test thoroughly before rolling out to KubeRay users.

Yes - will at least port all the existing kuberay/ray tests available in Ray for test. ",reasonable last unfortunately careful review key thing test thoroughly rolling yes least port available ray test,issue,negative,positive,neutral,neutral,positive,positive
1936905769,"Looked reasonable when I last looked at it.
Unfortunately, I don't have bandwidth for a careful review.

The key thing is to test thoroughly before rolling out to KubeRay users.",reasonable last unfortunately careful review key thing test thoroughly rolling,issue,negative,positive,neutral,neutral,positive,positive
1936773311,"I think @chrislevn may be taking on this issue if you want to work together on it.

I believe you have captured the problem well. There are two approaches here that would be good to evaluate:
- reducing size of the index by using a smaller dtype (as long as the number of rows to shuffle is less than the MAX for that dtype)
- shuffling in place, by copying to another data structure - this is okay as long as we can keep the number of memory copies to 1, which is what you need to do using `.take()` anyway",think may taking issue want work together believe problem well two would good evaluate reducing size index smaller long number shuffle le shuffling place another data structure long keep number memory need anyway,issue,negative,positive,positive,positive,positive,positive
1936599940,@angelinalg can you help review this PR please? It is similar to https://github.com/ray-project/ray/pull/42903 ,help review please similar,issue,positive,neutral,neutral,neutral,neutral,neutral
1936575236,"But I have configured A100 single, kuberay namespace single tenant cluster. But it’s working with single card gpu (TeslaP4) its showing all node details, only for MIG it’s causing issue, and runtime generated workergroup node details are not showing properly. ",single single tenant cluster working single card showing node mig causing issue node showing properly,issue,negative,negative,neutral,neutral,negative,negative
1936502345,reviewing - this can be foundationally fixed on train side only @matthewdeng ,foundationally fixed train side,issue,negative,positive,neutral,neutral,positive,positive
1936493779,"I've somehow made the `metrics_util` test flaky, will fix",somehow made test flaky fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1936462080,"This can be closed, I'll open a new issue with new logs if it comes back again",closed open new issue new come back,issue,negative,positive,neutral,neutral,positive,positive
1936458582,@Sayam753 I'm closing this one and feel free to reopen if @justinvyu's solution doesn't work.,one feel free reopen solution work,issue,positive,positive,positive,positive,positive,positive
1936429770,"@simran-2797 Any idea what controls will we need for filtering community examples? I think it would be useful to have something about this in the example gallery mockups.

Or if the change is not large enough to warrant a design, I can add a dropdown picker for `Community`/`Non-community` or something like that if that sounds okay by you?",idea need filtering community think would useful something example gallery change large enough warrant design add picker community something like,issue,positive,positive,positive,positive,positive,positive
1936389289,the fact that the application crashes without a flag set is worrisome to me; we should at least discuss whether this is a usability versus stability issue and prioritize accordingly,fact application without flag set worrisome least discus whether usability versus stability issue accordingly,issue,negative,negative,negative,negative,negative,negative
1936379029,@spolcyn did your issue show up on latest ray (2.8+),issue show latest ray,issue,negative,positive,positive,positive,positive,positive
1935491003,@rkooo567 do we know root cause and how to fix?,know root cause fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1935489274,@DmitriGekhtman have you had a chance to upgrade to ray latest?,chance upgrade ray latest,issue,negative,positive,positive,positive,positive,positive
1935480468,@2019331099-Rabbi did you get a chance to try out @rynewang 's advice?,get chance try advice,issue,negative,neutral,neutral,neutral,neutral,neutral
1935218802,"I found that `TaskBase` works with vanilla pickle by including `__getnerargs__`.

```python
import pickle


class TaskBase:
    _registry: dict[int, type[""TaskBase""]] = {}
    def __init__(self, task_id: int):
        self.task_id = task_id

    def __init_subclass__(cls, task_ids: tuple[int], **kwargs):
        super().__init_subclass__(**kwargs)
        for _id in task_ids:
            cls._registry[_id] = cls

    def __new__(cls, task_id: int, **kwargs):
        subcls = cls._registry.get(task_id, cls)
        obj = super().__new__(subcls)
        return obj

    def __getnewargs__(self):
        return (self.task_id,)


class TaskA(TaskBase, task_ids=(1, 2, 3)):
    pass


def main():
    task = TaskBase(task_id=1)
    print(task)

    with open(""task.pkl"", ""wb"") as f:
        pickle.dump(task, f)
    with open(""task.pkl"", ""rb"") as f:
        task = pickle.load(f)


if __name__ == ""__main__"":
    main()
```",found work vanilla pickle python import pickle class type self super super return self return class pas main task print task open task open task main,issue,positive,positive,positive,positive,positive,positive
1935060050,@zcin FYI had to make a small change to allow starting the pusher w/o tasks registered,make small change allow starting pusher registered,issue,negative,negative,negative,negative,negative,negative
1934934739,"
> I strongly recommend you to set every port manually when you deploy Ray https://docs.ray.io/en/master/ray-core/configure.html#ports-configurations to avoid port conflict.

Thank you @rkooo567 !

Although I am specifying ports in my `ray start` command, I still encounter random ports being used with this example: https://docs.ray.io/en/releases-2.9.0/train/examples/lightning/dolly_lightning_fsdp_finetuning.html

Why is Ray train using random ports between workers...?
Am I missing something in my `ray start` command...?

Python version: 3.10.13
Ray version: 2.9.0

Head node `ray start`:
```
sudo /opt/conda/envs/rayenv/bin/ray start --head --disable-usage-stats \
--node-manager-port=6380 \ 
--object-manager-port=6381 \ 
--runtime-env-agent-port=6382 \ 
--dashboard-agent-grpc-port=6383 \ 
--metrics-export-port=6384 \ 
--min-worker-port=10010 \ 
--max-worker-port=11010 \ 
--redis-shard-ports=8266  \ 
--dashboard-grpc-port=8267
```


Worker nodes `ray start`:
```
sudo /opt/conda/envs/rayenv/bin/ray start --address=instance-11-head:6379 \ 
--node-manager-port=6380 \ 
--object-manager-port=6381 \ 
--runtime-env-agent-port=6382 \ 
--dashboard-agent-grpc-port=6383 \ 
--metrics-export-port=6384 \ 
--min-worker-port=10010 \ 
--max-worker-port=11010
```

",strongly recommend set every port manually deploy ray avoid port conflict thank although ray start command still encounter random used example ray train random missing something ray start command python version ray version head node ray start start head worker ray start start head,issue,negative,negative,negative,negative,negative,negative
1934801185,`test_deployment_graph_build.py` was reaching into implementation details and completely redundant with `test_model_composition.py`,reaching implementation completely redundant,issue,negative,negative,negative,negative,negative,negative
1934730066,"@antoniomdk yep, looks like FastAPI fixed the starlette issue with 0.109.2! We'll unpin this dep for the Ray 2.10 release.",yep like fixed issue unpin ray release,issue,positive,positive,neutral,neutral,positive,positive
1934445268,"Mamba is now the default resolver for conda.

https://github.com/conda/conda/releases/tag/23.10.0
https://www.anaconda.com/blog/a-faster-conda-for-a-growing-community

So, if your Ray cluster is using one of the latest versions of conda, you should expect a similar performance as Mamba.",mamba default resolver ray cluster one latest expect similar performance mamba,issue,negative,positive,positive,positive,positive,positive
1934394606,"I think there is some incompatibility between tesnorflow and pytorch 2.2 cuda 11.8
Did a clean install->tensorflow and tensorflow probability->dreamer working, but as soon as I also installed pytorch 2.2, it broke
",think incompatibility clean dreamer working soon also broke,issue,negative,positive,positive,positive,positive,positive
1934387273,"Yes it has.  I tested Ray 2.9.2 along with the latest fastapi.

Thank you.

On Tue, Feb 6, 2024 at 7:06 PM Gene Der Su ***@***.***> wrote:

> Should be fixed in Ray 2.9.2
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/ray-project/ray/issues/42654#issuecomment-1930988772>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AXIZ6GN6XX2SSOCPIVG6KQDYSLARJAVCNFSM6AAAAABCI6766OVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTSMZQHE4DQNZXGI>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>
",yes tested ray along latest thank tue gene wrote fixed ray reply directly view id,issue,positive,positive,positive,positive,positive,positive
1934345104,"Hi - we would also like to use mamba to install our dependencies.
What's the current status?",hi would also like use mamba install current status,issue,negative,neutral,neutral,neutral,neutral,neutral
1934325411,"@GeneDer, Thanks for looking into this.
I raised https://github.com/ray-project/ray/issues/43018 for storing the output image.
And in https://github.com/ray-project/ray/pull/43046, I've referenced that image URL in the doc. I checked the failures; looks like premerge and doc build are failing due to ""(serve/tutorials/aws-neuron-core-inference-stablediffusion: line  122) broken    https://raw.githubusercontent.com/ray-project/images/master/docs/serve/stable_diffusion_inferentia2_output.png - 404 Client Error: Not Found for url: https://raw.githubusercontent.com/ray-project/images/master/docs/serve/stable_diffusion_inferentia2_output.png"". 
I am thinking we need to merge  https://github.com/ray-project/ray/issues/43018 first to get the jobs passed to have the image available.
Please let me know your thoughts. ",thanks looking raised output image image doc checked like doc build failing due line broken client error found thinking need merge first get image available please let know,issue,negative,positive,neutral,neutral,positive,positive
1934156972,"@antoniomdk  Thanks for the response.

I'm not sure whether its possible or not if its possible you can tag the Ray team so that this future really important when the multiple users using shared EKS cluster because ray service installed single namespace so that app team can use same endpoint URL across the cluster level who ever logged into the cluster.  ",thanks response sure whether possible possible tag ray team future really important multiple cluster ray service single team use across cluster level ever logged cluster,issue,positive,positive,positive,positive,positive,positive
1934043274,"Hi @tppalani sorry for the late response. Ray has no built-in support for user management, so there's no UI component to show the logged user.",hi sorry late response ray support user management component show logged user,issue,negative,negative,negative,negative,negative,negative
1933774614,"> One general question remains to me: `SingleAgentEnvRunner` is using an `RLModule` and at some other points we are also using the `RLModule`. The changes in this PR enable us to use the MARLModule as the general interface. Shall we - as in the design Doc of 'RLModule`s proposed consider everything as MA or shall we reduce complexity at the points where possible and introduce ""exceptions"" to the standard?

Great point! I do think it's very nice that we have these cases separated right now. Probably 90% of our users are not multi-agent users and we should keep things as simple as possible for them. Hence, they should NOT work with a MARLModule, NOT work with MultiAgentEpisodes, and NOT work with MultiAgentEnvRunners (or connectors).

However, the new Connector APIs make it possible to easily develop connectors that handle (within the same code and NOT requiring extensive use of if-else checks) both cases elegantly, e.g. the new `ConenctorV2.single_agent_episode_iterator` that works on both SAEps and MAEps.",one general question remains also enable u use general interface shall design doc consider everything shall reduce complexity possible introduce standard great point think nice right probably keep simple possible hence work work work however new connector make possible easily develop handle within code extensive use elegantly new work,issue,positive,positive,positive,positive,positive,positive
1933566403,"I managed to have a working environment with the following dependencies:

```
FastAPI version: 0.109.2
Starlette version: 0.36.3
Ray version: 2.9.2
```

Tested with this snippet of code:

```
import os

from fastapi import FastAPI
from ray import serve
from ray.serve.handle import RayServeHandle

app = FastAPI(debug=True)

@serve.deployment()
class Counter:
    def __init__(self):
        self.count = 0

    async def __call__(self):
        self.count += 1
        return {""count"": self.count}


@serve.deployment()
@serve.ingress(app)
class Router:
    def __init__(self, counter: RayServeHandle):
        self.counter = counter

    @app.post(""/"")
    async def run(self):
        return await self.counter.remote()


def application_builder():
    return Router.bind(counter=Counter.bind())


application = application_builder()

```
",working environment following version version ray version tested snippet code import o import ray import serve import class counter self self return count class router self counter counter run self return await return application,issue,negative,neutral,neutral,neutral,neutral,neutral
1933228324,"> Do you have Ray java installed: https://docs.ray.io/en/latest/ray-overview/installation.html#install-ray-java-with-maven?

No I don't think so. We have java installed and have our internal lib jar set in the CLASSPATH. We use `ray.cross_language.java_actor_class(..)` to call that function.",ray think internal jar set use call function,issue,negative,neutral,neutral,neutral,neutral,neutral
1933220238,"@ratnopamc I think you are referring to this PR? https://github.com/ray-project/ray/pull/43046 I see that DCO, premerge, and docs build all failed. Can you try to fix them? I think you tagged the right person for reviews. I can also ping them internally once I see those passing :) ",think see build try fix think tagged right person also ping internally see passing,issue,negative,positive,positive,positive,positive,positive
1933156757,"@scottjlee @stephanie-wang I'm new to the open-source development world, so I really want to give this one a go! However, I just wanted to check in and make sure I'm understanding the problem correctly. I wrote down what I think the problem is in the **Problem Statement** section and also some assumptions I have in the section after. Could you please let me know if I have misunderstood something or if there is a gap in my understanding? Thank you in advance!!


### Problem Statement

**Issue Being Solved**: Significant memory overhead during index creation, if the size of each row is very small (i.e. not many columns)

**Scope**: The problem is limited to the sort and random_shuffle functions, which both build a `numpy.array` to reorder the rows in a given PyArrow block.

**PyArrow Sort Relies on Index**:

When the `DataContext` has `context.use_polars = False`, we default to a sort method that relies on an index: https://github.com/ray-project/ray/blob/master/python/ray/data/_internal/arrow_block.py#L60 However, this is not true for when `context.use_polars = True`

More specifically, the `pyarrow.compute` API has a method that returns the indices that would sort an array, record batch or table. This is used in `transform_pyarrow.py`: https://github.com/ray-project/ray/blob/master/python/ray/data/_internal/arrow_ops/transform_pyarrow.py#L15

**Random Shuffle Relies on Index**

The `return self.take(random.permutation(self.num_rows()))` on https://github.com/ray-project/ray/blob/master/python/ray/data/_internal/arrow_block.py#L255 relies on an index argument (which is a random permutation of the integers from 0 to the number of rows in the table). This is input into the `take` : https://github.com/ray-project/ray/blob/master/python/ray/data/_internal/arrow_ops/transform_pyarrow.py#L19

> **Question**: What should be assume to be a small number of rows? 1, 2, 3 or fewer? 


### Assumptions

**ArrowBlockAccessor** is the name of the class that needs to be changed. It inherits from the `BlockAccessor` class and has data referenced by the `self._table` data member of type `pyarrow.Table`. 

**PyArrow**: is a [standardized in-memory columnar data format](https://voltrondata.com/codex/standards-over-silos#1-2-standardizing-on-arrow). It optimizes for:
1. *columnar data processing*: for use cases that involve selecting, filtering, and aggregating large columns
1. Efficient serialization and deserialization of data
2. *high performance*: SIMD, multi-threading for high performance
3. *memory efficiency:* features like zero-copy serialization and compression reduce memory usage

**Shuffling**: For each column in a table, the indices of a shuffle must be identical. 

```python
>>> table
pyarrow.Table
name: string
age: int64
score: double
----
name: [[""Alice"",""Bob"",""Charlie""]]
age: [[25,30,35]]
score: [[95.5,88,92.5]]
```

Let's say we have our columns in index 0 and 1 shuffled, we should expect the output to be something like 

```python
>>> table
pyarrow.Table
name: string
age: int64
score: double
----
name: [[""Bob"",""Alice"",""Charlie""]]
age: [[30,25,35]]
score: [[88,95.5,92.5]]
```


**Sorting**:

If we instead wanted to sort our table and has out `sort_key = ""score""`, then we should expect the output to be something like this:

```python
>>> table
pyarrow.Table
name: string
age: int64
score: double
----
name: [[""Bob"",""Charlie"",""Alice""]]
age: [[30,35,25]]
score: [[88,92.5,95.5]]
```


**Permutation Function**
Provided with a `self.num_rows() = 3`, the permutation function will generate a list of indices in random order. 

```python
>>> np.random.RandomState(69_420).permutation(3)
array([0, 1, 2])
>>> np.random.RandomState(69_420).permutation(3)
array([0, 1, 2])
>>> np.random.RandomState(69_421).permutation(3)
array([1, 0, 2])
>>> np.random.RandomState(69_425).permutation(3)
array([2, 1, 0])
```

**Potential for Large In-Memory Overhead**:

The function `np.random.RandomState(69_420).permutation(1_000_000_000)` uses **7.45 GB** of data in-memory. [This is because it creates a copy of the array `np.arange(1_000_000_000)` and shuffles its elements randomly](https://numpy.org/doc/stable/reference/random/generated/numpy.random.RandomState.permutation.html) The array has a size of **1 billion** and a data type of **int64**, which means each element occupies **8 bytes** of memory. Therefore, the total memory usage is `1_000_000_000 * 8 / (1024**3) = 7.45 GB`.

**PyArrow Extension Types**:  pyarrow extension type is a way to extend the built-in types of the Arrow data format with custom types and serialization mechanisms. This allows us to utilize arrow-compatible data structures, not natively supported by Arrow and keep the same zero-copy data sharing, fast serialization etc. 


**We have to Handle PyArrow Extension Type Columns Differently**:
If we are to implement our own sort or shuffle in-memory function, then we should take special consideration for extension types. For example the `.take` API for a pyarrow table breaks for an extension type here: https://github.com/ray-project/ray/blob/master/python/ray/data/_internal/arrow_ops/transform_pyarrow.py#L37 and a special type of processing is done here: https://github.com/ray-project/ray/blob/master/python/ray/data/_internal/arrow_ops/transform_pyarrow.py#L196

> **Question**: what type of considerations should be made? I'm not familiar with PyArrow Extensions


**PyArrow Compute API does not give Direct Access to Memory**

If we would like to sort/shuffle rows in place to avoid the memory overhead, then it might not be possible? 

I see that https://arrow.apache.org/docs/python/api/compute.html provides functions on-top of a PyArrow data-structure such as `take` https://arrow.apache.org/docs/python/api/compute.html#selections, but nothing direct (as far as I can tell??)

**Copying the Data into Another Data Structure Defeats the Purpose**:

Let's say I have data from pyarrow, but I want to instead convert the table into an intermediate format. This defeats the purpose even if it does shift/sort our values? For example, we might do something like the following for `shuffle`

```python
>>> arrow_array = pa.array([1, 2, 3, 4, 5])
<pyarrow.lib.Int64Array object at 0x128b88220>
[
  1,
  2,
  3,
  4,
  5
]
>>> numpy_array = np.array(arrow_array.to_numpy())
>>> numpy_array
array([1, 2, 3, 4, 5])
>>> np.random.shuffle(numpy_array)
>>> numpy_array
array([2, 1, 5, 4, 3])
>>> pa.array(numpy_array)
<pyarrow.lib.Int64Array object at 0x128b88220>
[
  2,
  1,
  5,
  4,
  3
]

```



",new development world really want give one go however check make sure understanding problem correctly wrote think problem problem statement section also section could please let know misunderstood something gap understanding thank advance problem statement issue significant memory overhead index creation size row small many scope problem limited sort build reorder given block sort index false default sort method index however true true specifically method index would sort array record batch table used random shuffle index return index argument random permutation number table input take question assume small number name class need class data data member type standardized columnar data format columnar data use involve filtering large efficient serialization data high performance high performance memory efficiency like serialization compression reduce memory usage shuffling column table index shuffle must identical python table name string age score double name bob age score let say index expect output something like python table name string age score double name bob age score instead sort table score expect output something like python table name string age score double name bob age score permutation function provided permutation function generate list index random order python array array array array potential large overhead function data copy array randomly array size billion data type element memory therefore total memory usage extension extension type way extend arrow data format custom serialization u utilize data natively arrow keep data fast serialization handle extension type differently implement sort shuffle function take special consideration extension example table extension type special type done question type made familiar compute give direct access memory would like place avoid memory overhead might possible see take nothing direct far tell data another data structure purpose let say data want instead convert table intermediate format purpose even example might something like following shuffle python object array array object,issue,positive,positive,neutral,neutral,positive,positive
1932821966,"@GeneDer , I've raised a PR https://github.com/ray-project/ray/issues/43018 to store the output image. The github url of the image is referenced in the docs. Can you please merge the above PR? Once it's merged, I can push the PR for the doc changes. 
Thanks.",raised store output image image please merge push doc thanks,issue,positive,positive,positive,positive,positive,positive
1932719839,"> Just realized that we also need to update unit tests that use parallelism. but can we can do this in the next PR.

Yes, those are okay and won't be broken. We can change in another PR.
",also need update unit use parallelism next yes wo broken change another,issue,negative,negative,negative,negative,negative,negative
1932702566,Just realized that we also need to update unit tests that use `parallelism`. but can we can do this in the next PR.,also need update unit use parallelism next,issue,negative,neutral,neutral,neutral,neutral,neutral
1932658348,Hi @stephanie-wang is this ticket already assigned to someone? I would like to make my first contribution to Ray and thought this would be a great starting point. Let me know if you'd like me to move forward :D ,hi ticket already assigned someone would like make first contribution ray thought would great starting point let know like move forward,issue,positive,positive,positive,positive,positive,positive
1932533050,"DCO run is failing with
> Commit sha: [b31089f](https://github.com/ray-project/ray/pull/40217/commits/b31089f833345f283801a9ad72335ccf2116bf47), Author: Yi Cheng, Committer: Yi Cheng; The sign-off is missing.",run failing commit sha author cheng committer cheng missing,issue,negative,negative,negative,negative,negative,negative
1932394577,Some learning tests are failing. Could you take a look? cc: @simonsays1980 ,learning failing could take look,issue,negative,neutral,neutral,neutral,neutral,neutral
1932284376,"Just following up on this - the main blocker here is the lack of documentation or guides on how to run Ray in a private subnet in AWS or other cloud providers without public IPs.

Any help on that? 

(This is due to restrictions on the sensitive data in health care that we train LLMs on, due to federal law - Health Insurance Portability and Accountability Act.)",following main blocker lack documentation run ray private cloud without public help due sensitive data health care train due federal law health insurance portability accountability act,issue,negative,positive,neutral,neutral,positive,positive
1932096508,"Getting closer. Now everything compiles, but there is a linking error:
```
hiredis.lib(ssl.obj) : error LNK2019: unresolved external symbol __imp_CertCloseStore referenced in function redisCreateSSLContextWithOptions
hiredis.lib(ssl.obj) : error LNK2019: unresolved external symbol __imp_CertEnumCertificatesInStore referenced in function redisCreateSSLContextWithOptions
hiredis.lib(ssl.obj) : error LNK2019: unresolved external symbol __imp_CertFreeCertificateContext referenced in function redisCreateSSLContextWithOptions
hiredis.lib(ssl.obj) : error LNK2019: unresolved external symbol __imp_CertOpenSystemStoreA referenced in function redisCreateSSLContextWithOptions
```

[stack overflow](https://stackoverflow.com/questions/37522654/linking-with-openssl-lib-statically) thinks the link is missing Windows' crypt32.lib library. This would be the link command for hiredis.",getting closer everything linking error error unresolved external symbol function error unresolved external symbol function error unresolved external symbol function error unresolved external symbol function stack overflow link missing library would link command,issue,negative,negative,neutral,neutral,negative,negative
1931880161,"@aslonnie  I followed other files in `ci/docker` for clang installation. Is it the correct way to do it for this case? I remember you saying that you have `clang 14` on your machine but I think OSS CI still uses `clang 12`. 
Also not sure if `clang-format` is necessary for now...",clang installation correct way case remember saying clang machine think still clang also sure necessary,issue,negative,positive,positive,positive,positive,positive
1931860317,"I tried for python 3.9 and got the same error, but for python 3.8 I dont get it

```
conda create -n test_env python=3.8 -y
conda activate test_env
pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu121
pip3 install ray
```

Python 3.8.18 on linux
Ray 2.9.2
Torch 2.3.0.dev20240207+cu121


",tried python got error python dont get create activate pip install torch pip install ray python ray torch,issue,negative,neutral,neutral,neutral,neutral,neutral
1931824499,"Hello! thanks a lot for the support :)
Here is how you can reproduce the environment I used, including torch==2.3.0.

```
conda create -n test_env python=3.10 -y
conda activate test_env
pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu121
pip3 install ray
```

I also tried with other versions. But to me seems to be related to the python version. Would that make sense?",hello thanks lot support reproduce environment used create activate pip install torch pip install ray also tried related python version would make sense,issue,positive,positive,neutral,neutral,positive,positive
1931784774,@rkooo567 it will run but will not blocking the release when it is flaky (it will block the release if it become consistently failing),run blocking release flaky block release become consistently failing,issue,negative,positive,positive,positive,positive,positive
1931679608,"in the client, there's a server called proxy server. When you call ray.put in your local machine, it sends gRPC request to the proxy server, and proxy server stores the result using ray.put (within a cluster). Crash usually happens because local -> remote gRPC request cannot handle a large object iiuc",client server proxy server call local machine request proxy server proxy server result within cluster crash usually local remote request handle large object,issue,negative,positive,neutral,neutral,positive,positive
1931670864,Core team doesn't maintain this feature now. @brycehuang30 do you happen to know? ,core team maintain feature happen know,issue,negative,neutral,neutral,neutral,neutral,neutral
1931668813,@jjyao it'd be great if you can approve the PR. I will add changes and merge it ,great approve add merge,issue,positive,positive,positive,positive,positive,positive
1931666767,cc @jjyao this may be a blocker for multi tenant ray cluster,may blocker tenant ray cluster,issue,negative,neutral,neutral,neutral,neutral,neutral
1931665308,cc @jjyao can you make sure this issue is not slipped? ,make sure issue slipped,issue,negative,positive,positive,positive,positive,positive
1931664063,"so in codebase metrics_head.py

```
shutil.copytree(GRAFANA_CONFIG_INPUT_PATH, grafana_config_output_path)
```

you have this code. Can you change it to

```
shutil.copytree(GRAFANA_CONFIG_INPUT_PATH, grafana_config_output_path, copy_function=copy)
```
? (the default is copy2 which I believe requires more extensive permission). ",code change default copy believe extensive permission,issue,negative,neutral,neutral,neutral,neutral,neutral
1931661620,"Hmm we don't have a established policy here now. For other similar cases, the CIs are maintained by community in general.

What's needed to integrate tests these backend? How complicated is the setup? ",established policy similar community general integrate complicated setup,issue,negative,negative,negative,negative,negative,negative
1931626279,"I forgot the exact definition of ""jail"", but we should at least run this in the postmerge build",forgot exact definition jail least run build,issue,negative,negative,neutral,neutral,negative,negative
1931177690,"> This diff will fix the failure in `alloc.c`. There is still a failure to build boringssl. I don't really understand a few things:
> 
> * what changed, why does master build boringssl and this version does not?
> * why use [boringssl for hiredis](https://github.com/ray-project/ray/blob/65ed62d512720c037f545d52cfc653dc830a4589/bazel/BUILD.hiredis#L48), and [openssl for redis](https://github.com/ray-project/ray/blob/65ed62d512720c037f545d52cfc653dc830a4589/bazel/BUILD.redis#L35)? It seems openssl is more prevalent, especially in the documentation.
> * If we are talking about security, why is openssl [pinned to the very old openssl v1.1.1f](https://github.com/ray-project/ray/blob/65ed62d512720c037f545d52cfc653dc830a4589/bazel/ray_deps_setup.bzl#L252) when there are security releases, the final 1.1.1 version [is 1.1.1w](https://github.com/openssl/openssl/tags)

Sorry, I don't know the answer
I always think the dependence in bazel is a messy thing. Like grpc depends implicitly on boringssl and openssl is required for some other libs. Also it seems super hard to make things compiled successfully in bazel.",fix failure still failure build really understand master build version use prevalent especially documentation talking security pinned old security final version sorry know answer always think dependence messy thing like implicitly also super hard make successfully,issue,positive,negative,neutral,neutral,negative,negative
1931122233,@mattip I think exclude hiredis building from windows actually needs more work. let me take a look at the building failure first.,think exclude building actually need work let take look building failure first,issue,negative,negative,neutral,neutral,negative,negative
1931110953,"> To clarify, are multi-card runs the same as running with multiple HPUs?

Yes, multi-card refers to multiple HPUs.

> Is running on a single HPU still possible without the change?

Yes, running on a single HPU does not require this change. However, torch_dist.py is handy when initializing torch dist among inference workers. Our open-source inference framework, [llm-on-ray](https://github.com/intel/llm-on-ray) also utilizes this file in its deepspeed inference workers. Are there any alternatives for the same function in Ray?",clarify running multiple yes multiple running single still possible without change yes running single require change however handy torch among inference inference framework also file inference function ray,issue,positive,positive,neutral,neutral,positive,positive
1931085612,@anyscalesam doc infra team will help incorporating the changes and Libraries team will help migrating examples:),doc infra team help team help,issue,positive,neutral,neutral,neutral,neutral,neutral
1931069225,Yes. I will try this week and find a new owner if I can't finish it,yes try week find new owner ca finish,issue,negative,positive,positive,positive,positive,positive
1931038877,@xieus is Sang still the right owner for this?,sang still right owner,issue,negative,positive,positive,positive,positive,positive
1930994221,"Sg, thanks @ratnopamc Feel free to let us know when the PR is ready or if you runs into any issue ",thanks feel free let u know ready issue,issue,positive,positive,positive,positive,positive,positive
1930954387,"Observation: if you do `ray job submit --working-dir $PWD -- python script.py`, it shows all abs paths. This may related to where the module is loaded (so only in driver mode, not in submission mode)",observation ray job submit python may related module loaded driver mode submission mode,issue,negative,neutral,neutral,neutral,neutral,neutral
1930938866,@vedin-eta would it be possible for you to update to a new version of Ray (e.g. 2.9)? This syncing logic has since been revamped significantly and should no longer rely on the previous IP. cc @justinvyu ,would possible update new version ray logic since significantly longer rely previous,issue,negative,positive,neutral,neutral,positive,positive
1930924930,"@justinvyu could you take a look at this one as it's related to the deprecated fields?

```bash
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/_internal/syncer.py"", line 74, in _deprecation_warning
    raise DeprecationWarning(
DeprecationWarning: `SyncConfig(syncer)` is a deprecated configuration Please remove it from your `SyncConfig`. 
Please implement custom syncing logic with a custom `pyarrow.fs.FileSystem` instead, and pass it into `ray.train.RunConfig(storage_filesystem)`. See here: https://docs.ray.io/en/latest/train/user-guides/persistent-storage.html#custom-storage
```",could take look one related bash file line raise configuration please remove please implement custom logic custom instead pas see,issue,positive,neutral,neutral,neutral,neutral,neutral
1930754479,"> There could be transient state where negative resources can exist.

When a worker is not started yet! ",could transient state negative exist worker yet,issue,negative,negative,negative,negative,negative,negative
1930735625,"I think it would be nice to have support for WSGI, ASGI and unique interfaces like socketify do:
https://docs.socketify.dev/cli.html

There's reasons to not use the event loop, so not sure I'd push ASGI alone.",think would nice support unique like use event loop sure push alone,issue,positive,positive,positive,positive,positive,positive
1930719162,"Yes, because Anyscale Ray Team is part of the Ray community. I was thinking that the special term ""Community Example"" can distinguish this. But based on our discussion, I think ""Contributed by community"" also looks clear to me.",yes ray team part ray community thinking special term community example distinguish based discussion think community also clear,issue,positive,positive,positive,positive,positive,positive
1930636304,"@woshiyyya, may I ask, what is not clear about `Contributed by Ray Community`? The reader is in the Example Gallery, so the word `example` in `Community Example` is redundant. I think @simran-2797 is trying to be more descriptive. Are you concerned that the `Ray Community` may also be interpreted as Anyscale?",may ask clear ray community reader example gallery word example community example redundant think trying descriptive concerned ray community may also,issue,negative,negative,neutral,neutral,negative,negative
1930628905,"I thought ""community example"" was a confusing term. @angelinalg do you have any suggestions here?",thought community example term,issue,negative,neutral,neutral,neutral,neutral,neutral
1930627887,"Thank you @simran-2797 The new design looks pretty cool! 

One suggestion is, could we label this as ""Community Example""? `Contributed by Ray Community` may be a bit confusing, users don't know whether it was built by us(Ray Team) or oss users.",thank new design pretty cool one suggestion could label community example ray community may bit know whether built u ray team,issue,positive,positive,positive,positive,positive,positive
1930469152,"> There could be transient state where negative resources can exist.

When can it happen?",could transient state negative exist happen,issue,negative,negative,negative,negative,negative,negative
1930408610,"> this change is needed for multi-card runs.

To clarify, are multi-card runs the same as running with multiple HPUs? Is running on a single HPU still possible without the change?",change clarify running multiple running single still possible without change,issue,negative,negative,neutral,neutral,negative,negative
1929969391,"> Verify the behaviors when pod is oom killed and when oom killer inside pod kills worker process. Then change the error message to match the behavior.

maybe it's better to remove this hint ""The worker may have exceeded K8s pod memory limits."". 

btw, it's hard enough to debug ray job",verify pod killer inside pod worker process change error message match behavior maybe better remove hint worker may pod memory hard enough ray job,issue,negative,positive,neutral,neutral,positive,positive
1929870068,"Q: Right now ,we retry for 10 minutes with 1 second interval. Should we

1. retry with exponential backoff?
2. retry for longer time? Like infinite or an hour?",right retry second interval retry exponential retry longer time like infinite hour,issue,negative,positive,positive,positive,positive,positive
1929447595,"I think this is related to a ticket I filled recently https://github.com/ray-project/ray/issues/42392

To my mind, if Ray Serve supported any ASGI-compliant web server, that would open the door for many optimizations at the request-handling level.",think related ticket filled recently mind ray serve web server would open door many level,issue,negative,positive,positive,positive,positive,positive
1928854532,"Yea, we have an issue for it: https://github.com/ray-project/ray/issues/34991. Close it as duplicate.",yea issue close duplicate,issue,negative,neutral,neutral,neutral,neutral,neutral
1928710243,It'll use whatever the ray installed on the node. Could you make sure the image you are using contains 2.9 ray?,use whatever ray node could make sure image ray,issue,negative,positive,positive,positive,positive,positive
1928661332,@can-anyscale should we jail if not weekly green blocking?,jail weekly green blocking,issue,negative,negative,negative,negative,negative,negative
1928635592,@edoakes Adding this warning in `serve.deployment` so that it is printed in the most visible location.,warning printed visible location,issue,negative,neutral,neutral,neutral,neutral,neutral
1928616062,@architkulkarni any update on this; should we rerun this to see if it is fixed on anyscale side? thankkks,update rerun see fixed side,issue,negative,positive,neutral,neutral,positive,positive
1928590811,"> @kira-lin thanks again for submitting this contribution! It looks nearly ready to merge.
> 
> One pending issue is the code changes to the private APIs in the `python/ray/air/util/torch_dist.py` file. I synced with the team, and they’re actually planning to remove this file shortly. We don’t currently have the bandwidth to maintain the file and provide support for the DeepSpeed example.
> 
> Could we do the following:
> 
> 1. Update this PR to remove the `Running a sharded model on multiple HPUs` section.
> 2. Remove the code changes to `python/ray/air/util/torch_dist.py`.
> 3. If feasible, host the documentation for `Running a sharded model on multiple HPUs` in the Habana docs?
> 
> We can merge this PR once the first two changes are made. In the future, we can also add a link in the Ray docs to the Habana docs for `Running a sharded model on multiple HPUs`.

@shrekris-anyscale  this change is needed for multi-card runs. ",thanks contribution nearly ready merge one pending issue code private file team actually remove file shortly currently maintain file provide support example could following update remove running sharded model multiple section remove code feasible host documentation running sharded model multiple merge first two made future also add link ray running sharded model multiple change,issue,positive,positive,neutral,neutral,positive,positive
1928521966,"@kira-lin thanks again for submitting this contribution! It looks nearly ready to merge.

One pending issue is the code changes to the private APIs in the `python/ray/air/util/torch_dist.py` file. I synced with the team, and they’re actually planning to remove this file shortly. We don’t currently have the bandwidth to maintain the file and provide support for the DeepSpeed example.

Could we do the following:

1. Update this PR to remove the `Running a sharded model on multiple HPUs` section.
2. Remove the code changes to `python/ray/air/util/torch_dist.py`.
3. If feasible, host the documentation for `Running a sharded model on multiple HPUs` in the Habana docs?

We can merge this PR once the first two changes are made. In the future, we can also add a link in the Ray docs to the Habana docs for `Running a sharded model on multiple HPUs`.",thanks contribution nearly ready merge one pending issue code private file team actually remove file shortly currently maintain file provide support example could following update remove running sharded model multiple section remove code feasible host documentation running sharded model multiple merge first two made future also add link ray running sharded model multiple,issue,positive,positive,neutral,neutral,positive,positive
1928493718,"This may because the worker was never dead. Can you try this?

1. in the function start_subprocess print `os.getpid()`
2. in `psutil.process_iter` also print ppid

",may worker never dead try function print also print,issue,positive,positive,neutral,neutral,positive,positive
1928485541,"The latest PyTorch release seems to be 2.2.0. See [the PyTorch release page](https://github.com/pytorch/pytorch/releases) for more details. In addition, I tried `pip install torch==2.3.0`, and got the following error message:

```
ERROR: Could not find a version that satisfies the requirement torch==2.3.0 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0)
ERROR: No matching distribution found for torch==2.3.0
```",latest release see release page addition tried pip install got following error message error could find version requirement error matching distribution found,issue,negative,positive,positive,positive,positive,positive
1928398379,"> Which did not happen in previous versions of Torch and Ray.

What about previous version of Torch and current version of Ray?",happen previous torch ray previous version torch current version ray,issue,negative,negative,negative,negative,negative,negative
1928345754,"Could you provide more information:
1. what's the os, python version
2. a repro script",could provide information o python version script,issue,negative,neutral,neutral,neutral,neutral,neutral
1928337233,We tried vanilla pickle and it doesn't work with your `TaskBase` class so I think it's not an issue with Ray. Could you make sure `TaskBase` works with normal pickle?,tried vanilla pickle work class think issue ray could make sure work normal pickle,issue,negative,positive,positive,positive,positive,positive
1928295006,Verify the behaviors when pod is oom killed and when oom killer inside pod kills worker process. Then change the error message to match the behavior.,verify pod killer inside pod worker process change error message match behavior,issue,negative,neutral,neutral,neutral,neutral,neutral
1928275720,I have already reached out to the Ray doc team.,already ray doc team,issue,negative,neutral,neutral,neutral,neutral,neutral
1928271009,@gtarcoder any suggestions on how we can improve the error message to make it clear that it's only a guess?,improve error message make clear guess,issue,negative,positive,positive,positive,positive,positive
1928218235,@iycheng could you try to reproduce this and see if it's a problem with openai exception itself?,could try reproduce see problem exception,issue,negative,neutral,neutral,neutral,neutral,neutral
1928164505,"I will reach out to the Ray documentation team to review this PR. I won't conduct another round of review for this PR; instead, I'll directly open a follow-up PR to accelerate the merge process as I said https://github.com/ray-project/ray/pull/42903#issuecomment-1925205038.",reach ray documentation team review wo conduct another round review instead directly open accelerate merge process said,issue,negative,negative,neutral,neutral,negative,negative
1928137160,"I've had the same issue. I've had to manipulate the package versions. I'm still having some issues w.r.t gymnasium compatibility, but hope, maybe this piece of information will help you out. Here are my package versions (poetry).

-> python version is set  to 3.9.18

[tool.poetry.dependencies]
python = "">=3.9,<3.12""
ray = {extras = [""rllib""], version = ""2.9.0""}
mypy = ""^1.4.1""
pyyaml = ""^6.0.1""
torch = ""^2.0.1""
gputil = ""^1.4.0""
gymnasium = ""0.28.1""
glfw = ""^2.6.2""
click = ""^8.1.6""
black = ""^23.7.0""
hydra-core = ""^1.3.2""
tensorflow-estimator = ""2.15.0""
tensorflow-io-gcs-filesystem = ""0.34.0""
tensorflow-macos = ""2.15.0""
tensorflow-probability = ""0.23.0""",issue manipulate package still gymnasium compatibility hope maybe piece information help package poetry python version set python ray version torch gymnasium click black,issue,positive,negative,negative,negative,negative,negative
1928103166,"removeOwnedChild needs a redo. Now we call it in ProcessFD dtor. However there are times we deallocate ProcessFD and does not track the process anymore, yet we don't want to kill it immediately either, e.g. when you spawn a one time util script. So we really need to track exit of those processes, i.e. in sigchld handler.

```
while (waitpid(&pid)) {
if pid in children {
    children.remove(pid)
  }
}
```",need redo call however time track process yet want kill immediately either spawn one time script really need track exit handler,issue,negative,positive,positive,positive,positive,positive
1927936883,"Code links for index creation:
- random_shuffle: https://github.com/ray-project/ray/blob/master/python/ray/data/_internal/arrow_block.py#L255
- sort (forwards to pyarrow sort_indices or polars if available): https://github.com/ray-project/ray/blob/master/python/ray/data/_internal/arrow_block.py#L458",code link index creation sort forward available,issue,negative,positive,positive,positive,positive,positive
1927819513,Noting here that the Ray Data overhead could be from generating block from batches.,ray data overhead could generating block,issue,negative,neutral,neutral,neutral,neutral,neutral
1927737463,"

![Image](https://github.com/ray-project/ray/assets/116198444/ae9ab16d-d0b3-4493-8e1f-9f4402050de9)

looks ot be passing now; but also skipping... closing",image passing also skipping,issue,negative,neutral,neutral,neutral,neutral,neutral
1927676367,"> > the node-pod-task hierarchy is pretty mind-bending for new users.
> 
> 
> 
> I agree. Perhaps we should consider making the Ray scheduler aware of K8s Pods and Nodes.

This is tricky. There has been extensive discussion in this direction but no officially endorsed solutions -- you can ask e.g. Eric L or Philipp M for details.





",hierarchy pretty new agree perhaps consider making ray aware tricky extensive discussion direction officially endorsed ask eric,issue,positive,positive,positive,positive,positive,positive
1927658463,"Slight modification to wording / formatting from @stephanie-wang's feedback.
```
Dataset iterator time breakdown:
* Total time overall: 49.52s
    * Total time in Ray Data iterator initialization code: 20.75s
    * Total time user thread is blocked by Ray Data iter_batches: 28.76s
    * Total execution time for user thread: 4.15ms
* Num blocks local: 0
* Num blocks remote: 0
* Num blocks unknown location: 0
* Batch iteration time breakdown (summed across prefetch threads):
    * In ray.get(): 65.12us min, 544.04us max, 217.22us avg, 5.43ms total
    * In batch creation: 10.67us min, 1.27ms max, 26.82us avg, 26.82ms total
    * In batch formatting: 8.58us min, 311.5us max, 26.0us avg, 26.0ms total
```",slight modification wording feedback time breakdown total time overall total time ray data code total time user thread blocked ray data total execution time user thread local remote unknown location batch iteration time breakdown summed across min total batch creation min total batch min total,issue,negative,negative,neutral,neutral,negative,negative
1927641223,"Update on Doc Infra:

Currently our team are refactoring the examples design([draft PR](http://train_transformers_glue_example/)). The per-library examples will have metadata files that include their framework, difficulty level, ....  Community examples will fill all of the existing metadata entries, and add another attribute `{""community_example"": bool}` to support result filtering.",update doc infra currently team design draft include framework difficulty level community fill add another attribute bool support result filtering,issue,negative,neutral,neutral,neutral,neutral,neutral
1927585848,"> the node-pod-task hierarchy is pretty mind-bending for new users.

I agree. Perhaps we should consider making the Ray scheduler aware of K8s Pods and Nodes.",hierarchy pretty new agree perhaps consider making ray aware,issue,positive,positive,positive,positive,positive,positive
1927101239,Can let the user cancel task in the dashboard?,let user cancel task dashboard,issue,negative,neutral,neutral,neutral,neutral,neutral
1925749922,"> the ground truth is that it received a SIGTERM. ""k8s oom"" is a speculation that may not, and in this case, is not true. Ray does not use SIGTERM to kill workers, so the signal comes from somewhere else. Do you have some guesses on who may send SIGTERM to the workers?

went through all the logs of ray actor/tasks, can not find any message useful to this SIGTERM..",ground truth received speculation may case true ray use kill signal come somewhere else may send went ray find message useful,issue,negative,positive,positive,positive,positive,positive
1925634148,"Same issue here. Tried the following versions and all of them has this behavior: 2.9.1, 2.9.0, 2.8.1, 2.7.1, 2.6.0.",issue tried following behavior,issue,negative,neutral,neutral,neutral,neutral,neutral
1925559825,Looks good! Thanks for the cleanup. Let's merge it.,good thanks cleanup let merge,issue,positive,positive,positive,positive,positive,positive
1925507974,"Oh, I guess I labelled the issue as a doc problem a while ago.
Anyways, thanks for addressing!!",oh guess issue doc problem ago anyways thanks,issue,negative,positive,positive,positive,positive,positive
1925470786,"@ArturNiederfahrenhorst : After some time focusing on my custom environment logic, I am coming back to this topic. 

I want to highlight the ""hidden"" hard requirement to have `tensorflow` installed, even though `torch` would be available. 
This is today - version ray 2.9.1 - still the case for the Trainer API, e.g. in `from ray.rllib.agents.ppo import PPOTrainer`.

Back at the initial point of discussion, I thought the root cause of the error message would be related to the other tensorflow version.",time custom environment logic coming back topic want highlight hidden hard requirement even though torch would available today version ray still case trainer import back initial point discussion thought root cause error message would related version,issue,negative,negative,neutral,neutral,negative,negative
1925205038,"To accelerate the process of merging this PR, the following steps will be taken:

* Address my comments.
* I will ping Ray doc team to review this PR.
* Address comments from Ray doc team.
* Manual test to ensure it works to avoid issues like https://github.com/ray-project/ray/pull/42903#discussion_r1476931218 and https://github.com/ray-project/ray/pull/42903#discussion_r1477002737.
* Merge this PR.
* I will open a follow-up PR to add more details, making the documentation more user-friendly for Ray users who are not familiar with Kubernetes.",accelerate process following taken address ping ray doc team review address ray doc team manual test ensure work avoid like merge open add making documentation ray familiar,issue,negative,positive,positive,positive,positive,positive
1924911361,"In addition, we typically [install Vale](https://docs.ray.io/en/master/ray-contribute/docs.html#how-to-use-vale) to check whether the writing adheres to Google developer documentation style guide. With Vale, our doc team can review the PR faster. Thanks!

",addition typically install vale check whether writing developer documentation style guide vale doc team review faster thanks,issue,negative,positive,neutral,neutral,positive,positive
1924893555,"> I feel like we should not have opinions for the batch size, and this should be the top level config? (similar to backpressure config)

Agreed, batch-size actually won't be assumed, we just need to pick first batch size then subsequently based on how much that batch takes relative to `TARGET_MAX_BATCH_DURATION_US`, batch-size will be scaled up/down, so essentially batching is gonna be time-budget based rather than size-based.

So DEFAULT_BATCH_SIZE is actually not relevant from that perspective and we can just encode some low enough value to work out of the box.

",feel like batch size top level similar agreed actually wo assumed need pick first batch size subsequently based much batch relative scaled essentially gon na based rather actually relevant perspective encode low enough value work box,issue,positive,positive,positive,positive,positive,positive
1924844951,"Closed #42787 in favor of this PR, those changes have been incorporated here:
* `Total time user code is blocked` -> `Total time user thread is blocked by iter_batches`
* `Total time in user code` -> `Total execution time for user thread`

Example of new output:
```
Operator 1 ReadRange->Map(sleep): 25 tasks executed, 25 blocks produced in 49.48s
* Remote wall time: 8.06s min, 20.66s max, 20.14s mean, 503.58s total
* Remote cpu time: 31.72ms min, 90.28ms max, 77.99ms mean, 1.95s total
* Peak heap memory usage (MiB): 143968.75 min, 145968.75 max, 144828 mean
* Output num rows per block: 16 min, 41 max, 40 mean, 1000 total
* Output size bytes per block: 128 min, 328 max, 320 mean, 8000 total
* Output rows per task: 16 min, 41 max, 40 mean, 25 tasks used
* Tasks per node: 25 min, 25 max, 25 mean; 1 nodes used
* Extra metrics: {'num_inputs_received': 13, 'bytes_inputs_received': 33468, 'num_inputs_processed': 1, 'bytes_inputs_processed': 2574, 'bytes_inputs_of_submitted_tasks': 33468, 'num_outputs_generated': 1, 'bytes_outputs_generated': 328, 'rows_outputs_generated': 41, 'num_outputs_taken': 1, 'bytes_outputs_taken': 328, 'num_outputs_of_finished_tasks': 1, 'bytes_outputs_of_finished_tasks': 328, 'num_tasks_submitted': 13, 'num_tasks_running': 12, 'num_tasks_have_outputs': 1, 'num_tasks_finished': 1, 'num_tasks_failed': 0, 'obj_store_mem_alloc': 328, 'obj_store_mem_freed': 2574, 'obj_store_mem_cur': 30894, 'obj_store_mem_peak': 31221, 'obj_store_mem_spilled': 0, 'block_generation_time': 20.64818425, 'cpu_usage': 12, 'gpu_usage': 0, 'ray_remote_args': {'num_cpus': 1, 'scheduling_strategy': 'SPREAD'}}

Dataset iterator time breakdown:
* Total time user thread is blocked by iter_batches: 28.8s
* Total execution time for user thread: 4.24ms
* Total time in iterator initialization code: 20.72s
* Total time overall: 49.53s
* Num blocks local: 0
* Num blocks remote: 0
* Num blocks unknown location: 0
* Batch iteration time breakdown (summed across prefetch threads):
    * In ray.get(): 84.92us min, 493.0us max, 189.09us avg, 4.73ms total
    * In batch creation: 13.54us min, 2.4ms max, 26.47us avg, 26.47ms total
    * In batch formatting: 10.88us min, 385.46us max, 24.39us avg, 24.39ms total
```",closed favor incorporated total time user code blocked total time user thread blocked total time user code total execution time user thread example new output operator map sleep executed produced remote wall time min mean total remote time min mean total peak heap memory usage mib min mean output per block min mean total output size per block min mean total output per task min mean used per node min mean used extra metric time breakdown total time user thread blocked total execution time user thread total time code total time overall local remote unknown location batch iteration time breakdown summed across min total batch creation min total batch min total,issue,negative,negative,neutral,neutral,negative,negative
1924830025,"@rynewang I am figuring that out. This requires very subtle timing (indeed, I could repro when I created so many pgs only), so writing an unit test is not trivial. Writing cpp unit test is even harder because of how our code is structured. ",subtle timing indeed could many writing unit test trivial writing unit test even harder code structured,issue,negative,positive,neutral,neutral,positive,positive
1924798467,"> could you also disable [this line](https://github.com/ray-project/ray/blob/2b73e05e56d78ca2873b09a7ed4ca029def800d5/python/ray/data/_internal/execution/interfaces/physical_operator.py#L230), when the flag is off?

Discussed with @raulchen and decided to not make this change in this PR. In the future we should add some info from OpRuntimeMetrics to DatasetStats.",could also disable line flag decided make change future add,issue,negative,neutral,neutral,neutral,neutral,neutral
1924722600,"Here is the root cause: https://github.com/huggingface/transformers/pull/28364

Previously before this PR, it renames the `tmp-checkpoint-*` dir to `checkpoint-*` on all workers. This PR force it to only rename on global-rank-0 worker (if `save_on_all_nodes=False`). Therefore, except for node 0, the remaining nodes still keep the `tmp-checkpoint-*` dir around and returns `None` in `transformers.trainer.get_last_checkpoint`.",root cause previously force rename worker therefore except node still keep around none,issue,negative,negative,negative,negative,negative,negative
1924718003,"> Good to merge once we confirm that the block locations are correctly populated on multi node scenario with the corresponding `DataContext` attribute enabled.

Tested on anyscale platform and is working as anticipated, The block size needs to be greater than 100KB (see [here](https://github.com/ray-project/ray/issues/31144))  so added a comment in the `get_object_locations` functions to clarify that behavior.",good merge confirm block correctly node scenario corresponding attribute tested platform working block size need greater see added comment clarify behavior,issue,negative,positive,positive,positive,positive,positive
1924681992,"> @edoakes Hi! I would like to know what factors might affect the performance level of ProxyActor, such as the configuration of the server or the complexity of the application. If there are multiple nodes, is the improvement linear? We are hoping to use RayServe in a production environment. Given our current observations, this performance issue is very important to us. Do you know when there might be significant progress on this?

The overall throughput scaling is ~linear as you add more nodes to the cluster as each node will have its own proxy (and they scale independently).

We are making incremental progress to this all the time ([example](https://github.com/ray-project/ray/pull/42943)), but I wouldn't expect orders-of-magnitude improvement in the near future (i.e., the next few releases). Most of our users' workloads involve fairly heavyweight ML inference requests, so supporting very high QPS per node is not often the primary goal. So we are instead focusing our efforts on other issues like improved autoscaling, stability, and observability.",hi would like know might affect performance level configuration server complexity application multiple improvement linear use production environment given current performance issue important u know might significant progress overall throughput scaling add cluster node proxy scale independently making incremental progress time example would expect improvement near future next involve fairly heavyweight inference supporting high per node often primary goal instead like stability observability,issue,positive,positive,positive,positive,positive,positive
1924677485,"> > I meet this issue in my task too, and I re-run the script you provide, the throughput is approximately 220 QPS，can you tell me the misconfiguration adjust to improve the performance, thanks
> 
> @xjhust, a throughput of 220 QPS appears to be reasonable and does not necessarily indicate a misconfiguration, especially when compared with my subsequent experiments. The performance is closely tied to the single-core performance of the CPU. The 800 QPS achieved by @edoakes is likely due to the superior single-core performance of the M1 Max. In fact, our server's CPU was previously set to 'powersave' mode, which resulted in a reduced performance of approximately 70 QPS.

Yes this makes sense given that the proxy is currently a Python process which is largely single-threaded.",meet issue task script provide throughput approximately tell misconfiguration adjust improve performance thanks throughput reasonable necessarily indicate misconfiguration especially subsequent performance closely tied performance likely due superior performance fact server previously set mode reduced performance approximately yes sense given proxy currently python process largely,issue,positive,positive,neutral,neutral,positive,positive
1924648793,"@ray-project/ray-serve the changes in this PR are ready for an initial review, but please note the TODOs in the description (most notably, I still have a lot of tests to write).",ready initial review please note description notably still lot write,issue,positive,positive,positive,positive,positive,positive
1924528313,"Update: let's instead change this so that raylet reserves some system memory. For now, we can just reserve object store memory, although in the future we could dynamically update based on current memory usage, in case raylet heap memory usage is high.",update let instead change raylet system memory reserve object store memory although future could dynamically update based current memory usage case raylet heap memory usage high,issue,negative,positive,neutral,neutral,positive,positive
1924519056,"# Benchmark results

## HTTP no-op latency

### Baseline
```
(ray) eoakes@Edwards-MacBook-Pro-2 serve % RAY_SERVE_ENABLE_QUEUE_LENGTH_CACHE=0 python ...
Latency (ms) for noop HTTP requests (num_replicas=1,num_requests=1000):
count    1000.000000
mean        3.838556
std         1.031378
min         3.371375
50%         3.737083
90%         4.060138
95%         4.188520
99%         5.597760
max        31.916375
dtype: float64
```

### With caching
```
(ray) eoakes@Edwards-MacBook-Pro-2 serve % RAY_SERVE_ENABLE_QUEUE_LENGTH_CACHE=1 python _private/benchmarks/http_noop_latency.py --num-requests 1000
...
Latency (ms) for noop HTTP requests (num_replicas=1,num_requests=1000):
count    1000.000000
mean        3.265671
std         0.964305
min         2.864250
50%         3.183500
90%         3.451258
95%         3.539712
99%         3.917807
max        30.811750
dtype: float64
```

## HTTP throughput (using `ab`)

```python
from ray import serve

@serve.deployment(num_replicas=8, max_concurrent_queries=10)
class A:
    def __call__(self, *args):
        return b""hi""

app = A.bind()
```

### Baseline
```
RAY_SERVE_ENABLE_QUEUE_LENGTH_CACHE=0 serve run noop:app
...
Requests per second:    757.95 [#/sec] (mean)
Time per request:       131.934 [ms] (mean)
Time per request:       1.319 [ms] (mean, across all concurrent requests)
Transfer rate:          141.38 [Kbytes/sec] received

Connection Times (ms)
              min  mean[+/-sd] median   max
Connect:        0    1   0.8      0       4
Processing:    27  121  20.4    117     188
Waiting:       27  118  19.9    114     186
Total:         30  121  20.4    118     188
WARNING: The median and mean for the initial connection time are not within a normal deviation
        These results are probably not that reliable.

Percentage of the requests served within a certain time (ms)
  50%    118
  66%    123
  75%    128
  80%    131
  90%    152
  95%    159
  98%    175
  99%    180
 100%    188 (longest request)
```

### With caching
```
RAY_SERVE_ENABLE_QUEUE_LENGTH_CACHE=1 serve run noop:app
...
Requests per second:    990.00 [#/sec] (mean)
Time per request:       101.010 [ms] (mean)
Time per request:       1.010 [ms] (mean, across all concurrent requests)
Transfer rate:          184.66 [Kbytes/sec] received

Connection Times (ms)
              min  mean[+/-sd] median   max
Connect:        0    1   0.7      1       3
Processing:    13   97  24.5     95     165
Waiting:       11   92  24.4     90     161
Total:         14   98  24.6     96     166

Percentage of the requests served within a certain time (ms)
  50%     96
  66%    102
  75%    106
  80%    114
  90%    140
  95%    143
  98%    155
  99%    159
 100%    166 (longest request)
```

## HTTP streaming throughput

### Baseline
```
(ray) eoakes@Edwards-MacBook-Pro-2 serve % RAY_SERVE_ENABLE_QUEUE_LENGTH_CACHE=0 python _private/benchmarks/streaming/streaming_http_throughput.py
...
HTTP streaming throughput (num_replicas=1, tokens_per_request=1000, batch_size=10, use_intermediate_deployment=False): 228498.48 +- 6799.28 tokens/s
```

### With caching
```
(ray) eoakes@Edwards-MacBook-Pro-2 serve % RAY_SERVE_ENABLE_QUEUE_LENGTH_CACHE=1 python _private/benchmarks/streaming/streaming_http_throughput.py
...
HTTP streaming throughput (num_replicas=1, tokens_per_request=1000, batch_size=10, use_intermediate_deployment=False): 210692.81 +- 5081.87 tokens/s
```

## Handle throughput

### Baseline
```
(ray) eoakes@Edwards-MacBook-Pro-2 serve % RAY_SERVE_ENABLE_QUEUE_LENGTH_CACHE=0 python _private/benchmarks/handle_throughput.py
...
DeploymentHandle throughput (num_replicas=1, batch_size=100): 1830.25 +- 5.44 requests/s
```

### With caching
```
(ray) eoakes@Edwards-MacBook-Pro-2 serve % RAY_SERVE_ENABLE_QUEUE_LENGTH_CACHE=1 python _private/benchmarks/handle_throughput.py
...
DeploymentHandle throughput (num_replicas=1, batch_size=100): 1840.15 +- 30.62 requests/s
```

## Handle streaming throughput

### Baseline
```
(ray) eoakes@Edwards-MacBook-Pro-2 serve % RAY_SERVE_ENABLE_QUEUE_LENGTH_CACHE=0 python _private/benchmarks/streaming/streaming_handle_throughput.py
...
DeploymentHandle streaming throughput (ASYNC) (num_replicas=1, tokens_per_request=1000, batch_size=10): 11267.96 +- 172.1 tokens/s
(ServeReplica:default:CallerDeployment pid=48752) Individual request quantiles:
(ServeReplica:default:CallerDeployment pid=48752)       P50=676.8099579999998
(ServeReplica:default:CallerDeployment pid=48752)       P75=843.8243122500003
(ServeReplica:default:CallerDeployment pid=48752)       P99=915.2339992500001
```

### With caching
```
(ray) eoakes@Edwards-MacBook-Pro-2 serve % RAY_SERVE_ENABLE_QUEUE_LENGTH_CACHE=1 python _private/benchmarks/streaming/streaming_handle_throughput.py
...
DeploymentHandle streaming throughput (ASYNC) (num_replicas=1, tokens_per_request=1000, batch_size=10): 11671.64 +- 209.44 tokens/s
(ServeReplica:default:CallerDeployment pid=48848) Individual request quantiles:
(ServeReplica:default:CallerDeployment pid=48848)       P50=661.6651875000007
(ServeReplica:default:CallerDeployment pid=48848)       P75=806.0847917499996
(ServeReplica:default:CallerDeployment pid=48848)       P99=911.5834779999999
```",latency ray serve python latency noop count mean min float ray serve python latency noop count mean min float throughput python ray import serve class self return hi serve run noop per second mean time per request mean time per request mean across concurrent transfer rate received connection time min mean median connect waiting total warning median mean initial connection time within normal deviation probably reliable percentage within certain time request serve run noop per second mean time per request mean time per request mean across concurrent transfer rate received connection time min mean median connect waiting total percentage within certain time request streaming throughput ray serve python streaming throughput ray serve python streaming throughput handle throughput ray serve python throughput ray serve python throughput handle streaming throughput ray serve python streaming throughput default individual request default default default ray serve python streaming throughput default individual request default default default,issue,negative,negative,negative,negative,negative,negative
1924459716,"> all actors I means actors for first batch and second batch are all start again!

I'm not sure if I understand this. Would you mind elaborating a bit more?",first batch second batch start sure understand would mind bit,issue,negative,positive,positive,positive,positive,positive
1924410962,"can we have a unit test, e.g. start a pg and kill it right away, and see if there's any leak?",unit test start kill right away see leak,issue,negative,positive,positive,positive,positive,positive
1924329595,"This will be in our backlog for now. If anyone is interested in contribution, we will help review and land the PR.",backlog anyone interested contribution help review land,issue,positive,positive,positive,positive,positive,positive
1924294053,"Triggered another job https://buildkite.com/ray-project/release/builds/7612 This is gonna take ~1 day to run, will update the PR once it's completed 
",triggered another job gon na take day run update,issue,negative,neutral,neutral,neutral,neutral,neutral
1924116810,"The method is added above 2.12, if you use dataset>=2.13,  you will have this issue. 2.12, 2.11 and 2.10 works",method added use issue work,issue,negative,neutral,neutral,neutral,neutral,neutral
1923306816,"Sorry for delay, actually I make two `map_batches` one after another:
```
result_first = first_map_batch(xxx)
print(result_first.count())
result_sec = sec_map_batch(result_first)
print(result_sec.count())
```
I found after `print(result_first.count())`, actors for first batch are dead, that's as expected
but when second `map_batchs` started, all actors I means actors for first batch and second batch are all start again!
that's really confuse me, I guess I must miss something @bveeramani ",sorry delay actually make two one another print print found print first batch dead second first batch second batch start really confuse guess must miss something,issue,negative,neutral,neutral,neutral,neutral,neutral
1923293441,"https://github.com/pytorch/pytorch/blob/855d5f144efc1db50316b9fcad1e62bf37caed10/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp#L3622

> Here we have to use the best guesses and will use a single GPU to call allreduce to achieve barrier. In case the multiple processes fall into the same node, we use rank to ensure that each process is on a different GPU.
",use best use single call achieve barrier case multiple fall node use rank ensure process different,issue,positive,positive,neutral,neutral,positive,positive
1923238636,"I tried but could not make progress in figuring out what is wrong with that mix of package versions. I would be more motivated if we had a use case that requires grpcio-1.58, and cannot resolve without it.",tried could make progress wrong mix package would use case resolve without,issue,negative,negative,negative,negative,negative,negative
1923041251,"Hi @simonsays1980, has there been any progress in fixing this or should I use the example you shared?
If the issue is not yet fixed, I personally favor using `policy.compute_single_action(...)`

> ```python
>     action = policy.compute_single_action(obs, explore=False, prev_action=xx, prev_reward==yy)
> # ...
> ```
 because the method has a `prev_action` param, which is useful in my case where `ViewRequirements` requires this information. Will connectors be applied in this setup?
",hi progress fixing use example issue yet fixed personally favor python action method param useful case information applied setup,issue,positive,positive,positive,positive,positive,positive
1922888421,"> REGRESSION 11.32%: 1_n_actor_calls_async (THROUGHPUT) regresses from 8667.010093466013 to 7686.2005077877675 (11.32%) in 2.9.2/microbenchmark.json

I'd rerun this to see if it's real regression: seems it's never below 8k.",regression throughput rerun see real regression never,issue,negative,positive,positive,positive,positive,positive
1922837607,"I'm not sure that this is such a good idea. Seems likely to be confusing to users that the amount of memory available on each node is much lower than expected, and chance of regression for those already using `memory` resource is high.

Also, #41191 mentions looking at actual heap memory usage in dashboard, but I believe this number already includes shared memory usage. And, even if shared memory usage is not included, there are ways around this problem that are less disruptive to other Ray users, e.g., Ray Data can add some % to each memory requirement and/or submit a long-running task to reserve some memory.",sure good idea likely amount memory available node much lower chance regression already memory resource high also looking actual heap memory usage dashboard believe number already memory usage even memory usage included way around problem le disruptive ray ray data add memory requirement submit task reserve memory,issue,negative,positive,positive,positive,positive,positive
1922690718,"> resource version part is a bit subtle
I'm just leaving this comment as a mark to remind myself to come back and explain a bit more in detail

https://github.com/ray-project/ray/pull/42720#discussion_r1475447095",resource version part bit subtle leaving comment mark remind come back explain bit detail,issue,negative,negative,negative,negative,negative,negative
1922624336,"> To my understanding, a resumed RayJob should start from scratch if we don't do checkpointing. This issue is the most common complaint I receive regarding priority scheduling.

Ah gotcha -- this makes sense now. I think the challenge here is that checkpointing is still an implementation detail of the RayJob / RayCluster, it is separate from Kueue's ability to preempt workloads for higher priorty ones. We can update the guide to implement checkpointing for the RayJob, but it doesn't change any behavior with Kueue itself and how it priorities workloads.  ",understanding start scratch issue common complaint receive regarding priority ah sense think challenge still implementation detail separate ability higher update guide implement change behavior,issue,negative,negative,negative,negative,negative,negative
1922620298,let's do it in another PR. I would like to see how it works after merge first.,let another would like see work merge first,issue,negative,positive,positive,positive,positive,positive
1922608392,@aslonnie  This is a draft script for you to review my logic on,draft script review logic,issue,negative,neutral,neutral,neutral,neutral,neutral
1922540230,"> Just small nits. I assume you've manually tested the various combinations of this as well?

The e2e tests added in this PR has pretty good coverage of when `num_replicas=""auto""` and `autoscaling_config` are specified together. Additionally, I've manually tested setting just `num_replicas=""auto""` since the default delays are too long to put in CI tests.",small assume manually tested various well added pretty good coverage auto together additionally manually tested setting auto since default long put,issue,positive,positive,positive,positive,positive,positive
1922533726,I will review the PR and manually try it tomorrow.,review manually try tomorrow,issue,negative,neutral,neutral,neutral,neutral,neutral
1922533157,"> I'm missing context on this part. Why is it difficult to demonstrate priority scheduling without checkpointing?

To my understanding, a resumed RayJob should start from scratch if we don't do checkpointing. This issue is the most common complaint I receive regarding priority scheduling.",missing context part difficult demonstrate priority without understanding start scratch issue common complaint receive regarding priority,issue,negative,negative,negative,negative,negative,negative
1922456076,"Tricky to upgrade the cluster I repro'd with onto nightly, but if this fixed a problem with the same symptoms (can't load the page when there's a dead actor on it) then was likely the same issue.",tricky upgrade cluster onto nightly fixed problem ca load page dead actor likely issue,issue,negative,negative,neutral,neutral,negative,negative
1922452032,"This was probably fixed by https://github.com/ray-project/ray/pull/42788 and confirmed the node detail page works with dead actors
<img width=""1528"" alt=""Screen Shot 2024-02-01 at 3 08 56 PM"" src=""https://github.com/ray-project/ray/assets/6900234/9b6ced04-9ca6-47c1-ae3e-0cbeaa2b923e"">

@ckw017 could you check if this is fixed in your repro also? 
",probably fixed confirmed node detail page work dead screen shot could check fixed also,issue,negative,positive,neutral,neutral,positive,positive
1922349986,"> @zcin I added such a test case. Note that this and deploying a config are going through the same path and we already validate that config is constructed as expected from import path & options, so I don't think we need more than a basic e2e test case.

Agreed, that seems like enough coverage to me. Thanks for adding the test!",added test case note going path already validate import path think need basic test case agreed like enough coverage thanks test,issue,positive,positive,neutral,neutral,positive,positive
1922346893,"I don't have extensive conda knowledge but based on the issue I guess grpcio compiled by conda is broken, while the one compiled by pip is good, so Ray don't have a great way to auto-fix this. @mattip do you have any insights on this?",extensive knowledge based issue guess broken one pip good ray great way,issue,positive,positive,positive,positive,positive,positive
1922300902,"@zcin I added such a test case. Note that this and deploying a config are going through the same path and we already validate that config is constructed as expected from import path & options, so I don't think we need more than a basic e2e test case.",added test case note going path already validate import path think need basic test case,issue,negative,neutral,neutral,neutral,neutral,neutral
1922284411,"> Now that `serve deploy` (with the local provider) takes an import path, could we add a test for that?

Roger that, let me add one",serve deploy local provider import path could add test roger let add one,issue,negative,neutral,neutral,neutral,neutral,neutral
1922279461,TODO: Figure out the behavior of the `barrier()` call. What happens under the hood without specifying a device id.,figure behavior barrier call hood without device id,issue,negative,neutral,neutral,neutral,neutral,neutral
1922261533,Thank you @kevin85421 ! We ended up needing to use terraform to configure virtual private clouds (not sure if this is something KubeRay or RayCluster supports?). We're hoping to give KubeRay a shot soon as it sounds like that will be supported going forward for batch jobs 🙏 ,thank ended needing use configure virtual private sure something give shot soon like going forward batch,issue,positive,positive,positive,positive,positive,positive
1922247279,Good to merge once we confirm that the block locations are correctly populated on multi node scenario with the corresponding `DataContext` attribute enabled.,good merge confirm block correctly node scenario corresponding attribute,issue,negative,positive,positive,positive,positive,positive
1922245875,@rynewang can we think of a systematic fix for this; is there nothing we can do so that dependency crawls between pip and conda are aligned so that if it works in pip it'll also work in condaforge?,think systematic fix nothing dependency pip work pip also work,issue,negative,neutral,neutral,neutral,neutral,neutral
1922126034,I just ran into this as well by following the Learning Ray book.,ran well following learning ray book,issue,negative,neutral,neutral,neutral,neutral,neutral
1922067228,@rynewang Those are typically from the release tests. https://buildkite.com/ray-project/release/builds?branch=releases/2.9.2 We typically collect it each time we run and the script will capture the latest run. ,typically release typically collect time run script capture latest run,issue,negative,positive,neutral,neutral,positive,positive
1922059859,"@GeneDer where are these numbers come from? This release does not have a lot of Core patches so we expect little change. Could be variance? Is there a way we rerun those regressions?

And btw, @cadedaniel mentioned` dashboard_p95_latency_ms` in particularly is very noisy, should ignore",come release lot core expect little change could variance way rerun particularly noisy ignore,issue,negative,negative,neutral,neutral,negative,negative
1922049556,"the ground truth is that it received a SIGTERM. ""k8s oom"" is a speculation that may not, and in this case, is not true. Ray does not use SIGTERM to kill workers, so the signal comes from somewhere else. Do you have some guesses on who may send SIGTERM to the workers?",ground truth received speculation may case true ray use kill signal come somewhere else may send,issue,negative,positive,positive,positive,positive,positive
1922046683,"@kevin85421 overall makes sense! Some comments:

> Demonstrating priority scheduling effectively is challenging without including checkpointing.

I'm missing context on this part. Why is it difficult to demonstrate priority scheduling without checkpointing?

> We can merge this PR, and then update the doc after we figure out the best practice of checkpointing.

Checkpointing would require a shared filesystem (like GCSFuse). I can update the example to use a shared filesystem but I felt the main concepts can be delivered without it. Let me know what you think.

> Add an example document in the examples/ directory to demonstrate gang scheduling using real-world workloads with Kueue in a separate PR.

Separate example for gang scheduling makes sense to me. 

> Add a document in a separate PR to the [k8s-ecosystem/](https://docs.ray.io/en/latest/cluster/kubernetes/k8s-ecosystem.html) directory demonstrating how to use Kueue/RayJob with toy workloads, ensuring that users can try it out on their local laptops. We can refer to https://docs.ray.io/en/latest/cluster/kubernetes/k8s-ecosystem/volcano.html#kuberay-volcano as an example.

Makes sense! I can add a more simplified guide in k8s-ecosystem that uses Kind. 

 




 

",overall sense priority effectively without missing context part difficult demonstrate priority without merge update doc figure best practice would require like update example use felt main without let know think add example document directory demonstrate gang separate separate example gang sense add document separate directory use toy try local refer example sense add simplified guide kind,issue,positive,positive,positive,positive,positive,positive
1922008748,@aloysius-lim thanks for the update. I'll try bump down some requirements and test,thanks update try bump test,issue,negative,positive,positive,positive,positive,positive
1921675531,Closed this PR due to a revert conflict. I will make a new PR. ,closed due revert conflict make new,issue,negative,negative,neutral,neutral,negative,negative
1921643042,@zcin please document the exact behavior changes and roll out plan in the PR description (this should be standard practice for any user-facing change).,please document exact behavior roll plan description standard practice change,issue,negative,positive,positive,positive,positive,positive
1921052526,"> I meet this issue in my task too, and I re-run the script you provide, the throughput is approximately 220 QPS，can you tell me the misconfiguration adjust to improve the performance, thanks

@xjhust, a throughput of 220 QPS appears to be reasonable and does not necessarily indicate a misconfiguration, especially when compared with my subsequent experiments. The performance is closely tied to the single-core performance of the CPU. The 800 QPS achieved by @edoakes is likely due to the superior single-core performance of the M1 Max. In fact, our server's CPU was previously set to 'powersave' mode, which resulted in a reduced performance of approximately 70 QPS.",meet issue task script provide throughput approximately tell misconfiguration adjust improve performance thanks throughput reasonable necessarily indicate misconfiguration especially subsequent performance closely tied performance likely due superior performance fact server previously set mode reduced performance approximately,issue,positive,positive,neutral,neutral,positive,positive
1921027120,"@edoakes Hi! I would like to know what factors might affect the performance level of ProxyActor, such as the configuration of the server or the complexity of the application. If there are multiple nodes, is the improvement linear? We are hoping to use RayServe in a production environment. Given our current observations, this performance issue is very important to us. Do you know when there might be significant progress on this?",hi would like know might affect performance level configuration server complexity application multiple improvement linear use production environment given current performance issue important u know might significant progress,issue,positive,positive,positive,positive,positive,positive
1920919639,"> @edoakes, thank you for the quick and detailed response!
> 
> Upon reevaluating our setup, I discovered a misconfiguration on our end that constrained the performance. 🥲 After addressing this, I am now observing a throughput of approximately 280 QPS on our corrected setup.
> 
> Furthermore, I conducted a test on an AWS t3.2xlarge instance (8-core Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz), which yielded around 175 QPS. While this is a marked improvement over the initial 70 QPS, it seems that we could still encounter scalability limitations under high-load scenarios.

I meet this issue in my task too, and I re-run the script you provide, the throughput is approximately 220 QPS，can you tell me the misconfiguration adjust to improve the performance, thanks",thank quick detailed response upon setup discovered misconfiguration end constrained performance observing throughput approximately corrected setup furthermore test instance platinum around marked improvement initial could still encounter meet issue task script provide throughput approximately tell misconfiguration adjust improve performance thanks,issue,positive,positive,neutral,neutral,positive,positive
1920605812,"Hi @shrekris-anyscale ,
I tried to split them to serve part and client part. I also simplified things like `pad_token_id` a bit. Please have a review again, thanks",hi tried split serve part client part also simplified like bit please review thanks,issue,positive,positive,positive,positive,positive,positive
1920568934,"To test, I put nvidia/cuda image:11.8.0-cudnn8-devel-ubuntu22.04 and podman==4.6.2 (it didn't take as long as trying to update podman on ubuntu20.04). Containers via jobs worked again. But necessarily have to add `--privileged` myself.",test put image take long trying update via worked necessarily add privileged,issue,negative,negative,neutral,neutral,negative,negative
1920413959,"Bad merge, sorry about that! I moved the code over to #42911.",bad merge sorry code,issue,negative,negative,negative,negative,negative,negative
1920400887,I am also experiencing the same issue. One of the GPUs is hanging at the end of training.,also issue one hanging end training,issue,negative,neutral,neutral,neutral,neutral,neutral
1920298139,"> I think non-container case already subtracts cached/buffer because that's what psutil does. I will create a follow up issue

@rkooo567 

non-container case still has issue, `psutil` also substract ""shmem"" but it shoudn't.

The correct formula for non-container case is: used_memory = `MemTotal` - (`MemAvailable` - `Shmem`)",think case already create follow issue case still issue also substract correct formula case,issue,negative,neutral,neutral,neutral,neutral,neutral
1920274374,"> @bveeramani can you help do a double check?
> 
> 
> 
> btw @scottjlee is it possible we add a unit test?

yeah i was attempting this, but wasn't sure how to test different versions of pandas other than adding an entirely new test suite (like we do for various arrow versions). Do you suggest this approach?

I did test the PR locally with pandas 1.5 and pandas 2.2.0, and the fix worked for both cases.

Moreover, we may want to generalize this pattern to also test floor/ceiling versions of pandas and other critical dependencies.",help double check possible add unit test yeah sure test different entirely new test suite like various arrow suggest approach test locally fix worked moreover may want generalize pattern also test critical,issue,positive,positive,neutral,neutral,positive,positive
1920177277,"> > @zcin any reason not to turn on by default generally? I don't remember what our major concern was there.
> 
> @edoakes I think for handles held by deployments, it's a bit unclear whether this will have side effects, because (for example) if deployment A holds a handle to deployment B, and A only has 1 replica whereas B has replicas distributed across N nodes, then A will only forward requests to the replicas on its node.

I see... well, let's do it incrementally then.",reason turn default generally remember major concern think bit unclear whether side effect example deployment handle deployment replica whereas distributed across forward node see well let,issue,negative,positive,neutral,neutral,positive,positive
1920143485,I think non-container case already subtracts cached/buffer because that's what psutil does. I will create a follow up issue,think case already create follow issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1920100040,Thanks for this contribution @n30111! 🚀  Let me know if you get a chance to try it out on nightly.,thanks contribution rocket let know get chance try nightly,issue,positive,positive,positive,positive,positive,positive
1920090164,"The above data raised some questions about why the iterator initialization time was as long as it was. The costly operation comes from the `next(gen)` call from [this line](https://github.com/ray-project/ray/blob/d7a4f25920f78f4e99b78bc95a45922160594733/python/ray/data/_internal/plan.py#L506) in `execute_to_iterator`. For smaller datasets, the initialization time can be a dramatic portion of the total time. I suspect if all the data can fit into one block, then this call to `next` will result in all of the data being read, then it will be read again in `iter_batches`. For large datasets, this initialization will be a smaller fraction of the total time spent, but still will represent non-trivial un-timed work.",data raised time long costly operation come next gen call line smaller time dramatic portion total time suspect data fit one block call next result data read read large smaller fraction total time spent still represent work,issue,negative,positive,neutral,neutral,positive,positive
1919943832,I'm not sure if I'm the best person to help here. @rynewang would you mind taking a look?,sure best person help would mind taking look,issue,positive,positive,positive,positive,positive,positive
1919937970,"> @zcin any reason not to turn on by default generally? I don't remember what our major concern was there.

@edoakes I think for handles held by deployments, it's a bit unclear whether this will have side effects, because (for example) if deployment A holds a handle to deployment B, and A only has 1 replica whereas B has replicas distributed across N nodes, then A will only forward requests to the replicas on its node.",reason turn default generally remember major concern think bit unclear whether side effect example deployment handle deployment replica whereas distributed across forward node,issue,negative,positive,neutral,neutral,positive,positive
1919730220,"@zcin , I'm not sure where best to post this. I want to make sure the fix will definitely work.

I'm currently testing a standard podman installation (per Ray's documentation) with the image `rayproject/ray:2.8.1-py311-gpu`. I noticed a few problems:
1. I can't put a podman version higher than 3.4.2
2. I think from the first problem follows the second one - podman works strangely. When I tried to run an image application through the job - all jobs hang on PENDING. When I tried to run the application through the subman with my image manually - the container started, but flooded with errors (`FileNotFoundError: [Errno 2] No such file or directory: '/home/ray/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py'`). 
But this is not the directory where the image dependencies are installed. I guess this problem is related to an old version of podman (4.9.0 is out now, but Ubuntu 20.04 supports only 3.4.2).

The manual check was run similar to what the job is trying to run:
```bash
podman run --cgroup-manager=cgroupfs --network=host --pid=host --ipc=host --env-host --privileged --rm -log-level=debug --security-opt=label=disable my_image:latest python service.py
```

I think it's a problem because I had previously manually customized the ray cluster image with `python:3.11.5-slim` and installed `podman==4.3.1`. Deploying applications via jobs worked with it. But I didn't quite get the nvidia drivers right, so I decided to try updating the cluster image from the ray image.",sure best post want make sure fix definitely work currently testing standard installation per ray documentation image ca put version higher think first problem second one work strangely tried run image application job pending tried run application subman image manually container flooded file directory directory image guess problem related old version manual check run similar job trying run bash run privileged latest python think problem previously manually ray cluster image python via worked quite get right decided try cluster image ray image,issue,positive,positive,positive,positive,positive,positive
1919723116,Marking this ticket as closed because the `AccelerateTrainer` has been deprecated. @Rubertos if you're still running into the issue with quantization could you create a new issue for it? Thanks!,marking ticket closed still running issue quantization could create new issue thanks,issue,positive,positive,neutral,neutral,positive,positive
1919718639,"@can-anyscale thanks! 

However, looks like the artifacts don't contain the test print out

Do you think it would be possible to add artifacts like these as well?   `C:/raytmp/6w7zvoxv/execroot/com_github_ray_project_ray/bazel-out/x64_windows-opt/testlogs/python/ray/tests/test_autoscaler/test_attempts/attempt_2.log`

",thanks however like contain test print think would possible add like well,issue,positive,positive,neutral,neutral,positive,positive
1919694780,thanks. I still need to figure out how to collect the results from the DB. But let's merge this PR first.,thanks still need figure collect let merge first,issue,negative,positive,positive,positive,positive,positive
1919632612,"https://github.com/ray-project/ray/pull/41998 called for the sidebar to be fixed width, so this is at least in part a consequence of that change.

But interestingly, `pydata-sphinx-theme` _does_ have a mobile design:

![image](https://github.com/ray-project/ray/assets/14017872/b0a0cecc-c563-4a0e-bc1f-d817fc78c17d)

Maybe this should be labeled as a bug if you're seeing the above on mobile? It might be triggered by certain screen sizes. At least, I know that there are CSS media queries being used which might account for this...",fixed width least part consequence change interestingly mobile design image maybe bug seeing mobile might triggered certain screen size least know medium used might account,issue,positive,positive,neutral,neutral,positive,positive
1919615425,Closing since #42097 was merged. Please open a new issue if this causes performance problems,since please open new issue performance,issue,negative,positive,neutral,neutral,positive,positive
1919210663,"> @gabriel-andersson Currently Ray Train doesn't support signal handling since the `train_func` is launched as a subthread. We will consider adding support for this feature in the future release.

Okay, I see. Do you have any kind of time estimate of when it could be happening? 

",currently ray train support signal handling since consider support feature future release see kind time estimate could happening,issue,positive,positive,positive,positive,positive,positive
1919156119,"Follow-ups:

(1) Update logic for ""cgroup2""
(2) Update logic for non-container Linux OS.
(3) See https://github.com/ray-project/ray/pull/42508#discussion_r1460294042

@rkooo567 
For databricks usage, these follow-ups are not urgent, so I might not allocate time-slots for these follow-ups. Would you help assign them to other folks.
",update logic update logic o see usage urgent might allocate would help assign,issue,positive,neutral,neutral,neutral,neutral,neutral
1919118653,"> yes. Can you make sure the premerge tests pass and ping me? I think we can merge it. We should follow up with cgroup2

@rkooo567 Thanks! All tests passed. ",yes make sure pas ping think merge follow thanks,issue,positive,positive,positive,positive,positive,positive
1918838449,"> Could we also have a new learning test for the added pendulum tuned_example file in the rllib/BUILD file? We probably shouldn't merge this before it's confirmed learning.

Yes we definitely should. But we have to first merge the different branches of SAC, i.e. `SACLearner`, `SACModule`, `SACAlgorithm` - and at best also `PrioritizedEpisodeReplayBuffer` (`PERB`), together. Then we can run the new stack on the tuned example test. ",could also new learning test added pendulum file file probably merge confirmed learning yes definitely first merge different sac best also together run new stack tuned example test,issue,positive,positive,positive,positive,positive,positive
1918509050,@mattip I pinged the person who has expertise on this. Can you ping me again if he doesn't respond you in 2 days? ,person ping respond day,issue,negative,neutral,neutral,neutral,neutral,neutral
1918508067,"~~Oh - i actually ran the repro on the current ray master and wasn't able to repro it.~~

Ok - seeing it in master, and verify the PR fixes it. ",actually ran current ray master able seeing master verify,issue,negative,positive,positive,positive,positive,positive
1918506917,I think it won't be very difficult to make a repro actually if you use fractional resources like 0.1 GPU,think wo difficult make actually use fractional like,issue,negative,negative,negative,negative,negative,negative
1918505933,@rickyyx is it possible your newest changes also fix this issue? ,possible also fix issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1918505132,"yeah it was intentional that worker node cannot set temp_dir (since there's no use case, and it complicates deployment). But given this use case, this approach makes sense ",yeah intentional worker node set since use case deployment given use case approach sense,issue,negative,neutral,neutral,neutral,neutral,neutral
1918503860,"We made some progress in the master though the throughput is not as good as gRPC streaming. Btw, @edoakes do we automatically batch requests in the server layer? Maybe we should introduce as the performance tips? ",made progress master though throughput good streaming automatically batch server layer maybe introduce performance,issue,positive,positive,positive,positive,positive,positive
1918500294,"I feel like we should not have opinions for the batch size, and this should be the top level config? (similar to backpressure config)",feel like batch size top level similar,issue,positive,positive,positive,positive,positive,positive
1918498759,"> Q: How should we track to remove this artificial sleep after releasing v2? Or is this test not relevant anymore after releasing v2?

Many of the tests would be refactored. I will be porting existing v1 tests to v2. ",track remove artificial sleep test relevant many would,issue,negative,positive,positive,positive,positive,positive
1918498469,"Actually @suquark do you have the context?

I thought we vendored it to support pickle 5 when Python version is < 3.8. But I am not 100% sure. Or is there any optimization we add here? ",actually context thought support pickle python version sure optimization add,issue,positive,positive,positive,positive,positive,positive
1918495938,"@rynewang I think that's the exactly the idea Cade brought up iirc https://anyscaleteam.slack.com/archives/G015EEPTEMN/p1699466934904329?thread_ts=1699445284.501769&cid=G015EEPTEMN

I think we can only handle this in Linux as an advanced feature if other options are complicated. Btw, this is duplicate of https://github.com/ray-project/ray/issues/26118",think exactly idea cade brought think handle advanced feature complicated duplicate,issue,negative,positive,neutral,neutral,positive,positive
1918495770,@can-anyscale - i thought there was a PR to add logs to windows CI run? Any update on that? ,thought add run update,issue,negative,neutral,neutral,neutral,neutral,neutral
1918493972,"> @rickyyx so there's no additional code we need to check in? Is it possible we have some sort of README to this page how to do add upstream changes? (Like file X needs to add Y sort of things. If nothing is needed the doc can say it can just bring upstream changes)

sgtm. Although I still don't know why we vendored it in the first place. 
",additional code need check possible sort page add upstream like file need add sort nothing doc say bring upstream although still know first place,issue,negative,positive,positive,positive,positive,positive
1918493212,yes. Can you make sure the premerge tests pass and ping me? I think we can merge it. We should follow up with cgroup2,yes make sure pas ping think merge follow,issue,positive,positive,positive,positive,positive,positive
1918380642,"One solution in Linux I can think of, is to mark raylet as PR_SET_CHILD_SUBREAPER (see https://man7.org/linux/man-pages/man2/prctl.2.html). This way, if a recursive subprocess (e.g. core_worker) dies, all the orphaned child subprocesses are now reparented to raylet. And we need to handle SIGCHLD:

1. do something for sigchld
    1. use signalfd, and poll it via the raylet's asio loop (or if this is too complex, make a dedicated thread)
    2. or, use sigaction
4. if there's sigchld:
    1. waitpid
    3. list all raylet's child processes. if there are unrecognized ones, kill them

Problem: this is not portable. macos does not have PR_SET_CHILD_SUBREAPER, we may investigate kqueue, but maybe we don't care either (lol). windows has a JOB_OBJECT_LIMIT_KILL_ON_JOB_CLOSE, but we may care even less (lol).

@rkooo567 wdyt",one solution think mark raylet see way recursive child raylet need handle something use poll via raylet loop complex make thread use list raylet child unrecognized kill problem portable may investigate maybe care either may care even le,issue,negative,negative,negative,negative,negative,negative
1918355605,Can ray plan a upgrade.  This issue troubles anyone using newer version of FastAPI.,ray plan upgrade issue anyone version,issue,negative,neutral,neutral,neutral,neutral,neutral
1918296211,"> Are your dependencies the same as mentioned above? [#40484 (comment)](https://github.com/ray-project/ray/issues/40484#issue-1951251282)

@grizzlybearg they were the same, except I updated Ray to the latest version.",comment except ray latest version,issue,negative,positive,positive,positive,positive,positive
1918229795,">I guess this is a draft that is not intended to be merged?

@aslonnie - yes, that's right.",guess draft intended yes right,issue,negative,positive,positive,positive,positive,positive
1918082019,"`local_dir` is deprecated, the proper way to specify where final results should be saved is to set `storage_path`. For intermediate storage, you can specify `os.environ[""RAY_AIR_LOCAL_CACHE_DIR""] `.",proper way specify final saved set intermediate storage specify,issue,negative,neutral,neutral,neutral,neutral,neutral
1918038492,"@rkooo567 this is batching implementation i was talking about and have been experimenting with.

Doing dynamic batching improves throughput on our benchmarks by about 3x (13k -> 36k / s)",implementation talking dynamic throughput,issue,negative,neutral,neutral,neutral,neutral,neutral
1918018369,"yeah looks like Ray did not start up. Can you try these:

1. add `import ray` then `ray.init()` at the beginning,
2. find logs `/tmp/ray/session_latest/logs/raylet.err` and post it here?",yeah like ray start try add import ray beginning find post,issue,positive,neutral,neutral,neutral,neutral,neutral
1918005898,Closing because the originally issue has been fixed. Will create a new Issue for `locality_with_output` thing,originally issue fixed create new issue thing,issue,negative,positive,positive,positive,positive,positive
1918004328,"Hey @danielezhu, the original issue you described has been fixed by https://github.com/ray-project/ray/pull/42308.

> However, [this comment](https://github.com/ray-project/ray/blob/master/python/ray/data/read_api.py#L334) makes it seem like the only thing that matters is the scheduling strategy that gets set in the remote options, when creating get_datasource_or_legacy_reader.

The comment is unrelated to `local:///` schemes. It has to do with Ray Client, which is a legacy thing.

> what is the difference between these two uses of the NodeAffinitySchedulingStrategy, and which case is actually causing the worker nodes to read the head node's local file?

The first determines where the files are read from. The second determines where the datasource object is constructed.

[This snippet](https://github.com/ray-project/ray/blob/master/python/ray/data/read_api.py#L307-L311) makes the read tasks run on the head node. That said, I think you need to also construct the datasource on the head node, since that's when we do file resolution.

> ctx.scheduling_strategy is still SPREAD, which seems contradictory.

`ctx.scheduling_strategy` just describes the default strategy. Since we're using a non-default value here, `ctx.scheduling_strategy` isn't relevant.


",hey original issue fixed however comment seem like thing strategy set remote comment unrelated local ray client legacy thing difference two case actually causing worker read head node local file first read second object snippet read run head node said think need also construct head node since file resolution still spread contradictory default strategy since value relevant,issue,positive,positive,positive,positive,positive,positive
1918002190,"> > Have you tested it out?
> 
> Not yet (gRPC handles cancellations t/h its own Context in other langs, so i'd expect similar design in here)

Yeah I would expect the same. Most likely we need to look for the `cancelled()` bit in the context and handle it in the same way that we are the `disconnect` message for HTTP.",tested yet context expect similar design yeah would expect likely need look bit context handle way disconnect message,issue,negative,neutral,neutral,neutral,neutral,neutral
1917993086,"> Have you tested it out?

Not yet (gRPC handles cancellations t/h its own Context in other langs, so i'd expect similar design in here)",tested yet context expect similar design,issue,negative,neutral,neutral,neutral,neutral,neutral
1917978000,"Hey @Warhorze, could you confirm the Ray version by executing `print(ray.__version__)`? The `__value__` column name was removed some releases ago, and I wasn't able to reproduce this locally.

```python
import numpy as np

import ray

ds = ray.data.read_numpy(""test.npy"", include_paths=True)
print(ds)
```

```
Dataset(
   num_blocks=20,
   num_rows=1000,
   schema={data: numpy.ndarray(shape=(1000,), dtype=double), path: string}
)
```",hey could confirm ray version print column name removed ago able reproduce locally python import import ray print data path string,issue,negative,positive,positive,positive,positive,positive
1917947862,"All the GCP tests are failing with the same error since some time after last Thursday:

""Job terminated before cluster was launched. No logs available.""

For some reason the ""outer"" cluster is failing to start within 30m (the cluster for the Anyscale job, as opposed to the ""inner"" cluster which is launched by the script in this test).

Not all GCE env tests are failing, just the GCP cluster launcher tests.  Still trying to figure out why, maybe it can be fixed by changing the compute config.

",failing error since time last job cluster available reason outer cluster failing start within cluster job opposed inner cluster script test failing cluster launcher still trying figure maybe fixed compute,issue,negative,positive,positive,positive,positive,positive
1917896815,Ray client does not work with Ray Data now. Closing the issue.,ray client work ray data issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1917858653,"> LGTM. I don't see any harm here.

see no evil, hear no evil, speak no evil",see harm see evil hear evil speak evil,issue,negative,negative,negative,negative,negative,negative
1917794213,@ericl tests passed. please take a look! Thanks.,please take look thanks,issue,positive,positive,positive,positive,positive,positive
1917777334,"under multiplexing, for a single multiplex model id, if it's loaded in 2 replicas, do they share any resources in anyway? or it simply takes twice the memory needed to load the model once",single multiplex model id loaded share anyway simply twice memory load model,issue,negative,negative,neutral,neutral,negative,negative
1917734386,"@gabriel-andersson Currently Ray Train doesn't support signal handling since the `train_func` is launched as a subthread. We will consider adding support for this feature in the future release.

ref: https://github.com/ray-project/ray/pull/42814#issuecomment-1917652038",currently ray train support signal handling since consider support feature future release ref,issue,positive,neutral,neutral,neutral,neutral,neutral
1917729837,"Got it. Signal handling is a missing feature that we need to support it in the future. User can save checkpoint on exception before the actors get killed.

- Lightning's `on_exception` [callback](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.OnExceptionCheckpoint.html)
- [An oss issue](https://github.com/ray-project/ray/issues/39055)",got signal handling missing feature need support future user save exception get lightning issue,issue,positive,negative,neutral,neutral,negative,negative
1917713794,"@sven1977 @simonsays1980 @ahmedammar Just a heads up, I switched to an alternate solution that would involve fewer changes in Ray Tune. See the PR description for the new recommended usage (converting to dict before passing into Ray Tune APIs). Ray Tune expects a dict in many other places, and we should not do this typecheck in every case.",switched alternate solution would involve ray tune see description new usage converting passing ray tune ray tune many every case,issue,negative,positive,positive,positive,positive,positive
1917652038,"@woshiyyya It refers to the main thread of a single worker actor that is processing the result queue from the training thread:
* https://github.com/ray-project/ray/blob/master/python/ray/train/_internal/session.py#L203
* https://github.com/ray-project/ray/blob/master/python/ray/train/_internal/session.py#L274

> Seems that the signal handling issue is unresolvable in current Ray Train design.

Yep this is unresolvable unless we get rid of this threading logic (which is an implementation detail for implementing ""yielding"" behavior, that can be done with Ray Generators in the future).",main thread single worker actor result queue training thread signal handling issue unresolvable current ray train design yep unresolvable unless get rid logic implementation detail yielding behavior done ray future,issue,negative,positive,neutral,neutral,positive,positive
1917643374,"Does the main thread refer to the driver and all Ray actors methods are child threads? Seems that the signal handling issue is unresolvable in current Ray Train design. 
",main thread refer driver ray child signal handling issue unresolvable current ray train design,issue,negative,positive,neutral,neutral,positive,positive
1917562287,"Plan is:
- `target_num_ongoing_requests_per_replica` will default to `0.4 * max_concurrent_queries` when not set.
- For 2.10, when `num_replicas=""auto""` is set AND `max_concurrent_queries` is not set AND `target_num_ongoing_requests_per_replica` is not set, we’ll set max_concurrent_queries=5 (value is up for discussion).
- We’ll also add a warning and in 2.10 that the default will change and change the default entirely in 2.11.",plan default set auto set set set set value discussion also add warning default change change default entirely,issue,negative,neutral,neutral,neutral,neutral,neutral
1917460202,I tried using the Ray implementation of DreamerV3 on a custom environment and multiple built-in environments with no luck. I switched to using the [author's codebase](https://github.com/danijar/dreamerv3) and am getting greatly improved (amazing) performance both in terms of agent behavior and training speed.,tried ray implementation custom environment multiple luck switched author getting greatly amazing performance agent behavior training speed,issue,positive,positive,positive,positive,positive,positive
1917424427,I'm closing this issue as I was able to resolve it. I completely missed the part where the decorated class itself shouldn't have a `__call__` method.,issue able resolve completely part decorated class method,issue,negative,positive,positive,positive,positive,positive
1917305387,@angelinalg could I get a quick stamp on this one? Need Ray docs codeowner approval.,could get quick stamp one need ray approval,issue,negative,positive,positive,positive,positive,positive
1916677926,"@jjyao Yes, I will make a pull request. Thank you.",yes make pull request thank,issue,positive,neutral,neutral,neutral,neutral,neutral
1916475832,"```python
import tempfile
import torch
from torchvision.models import resnet18, alexnet
from torchvision.datasets import FashionMNIST
from torchvision.transforms import ToTensor, Normalize, Compose
from torch.utils.data import DataLoader
from torch.optim import Adam
from torch.nn import CrossEntropyLoss
import datetime
import ray
    # use ray framework
from ray.train.torch import TorchTrainer
from ray.train import ScalingConfig, Checkpoint, RunConfig
import torch
from torch import nn
import torch


class Model(nn.Module):

    def __init__(self):
        super(Model, self).__init__()
        self.model1 = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=1, padding=1),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.BatchNorm2d(64),
            nn.ReLU(),

            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(128 * 8 * 8, 256),
            nn.Dropout(p=0.5),
            nn.ReLU(),
            nn.Linear(256, 10)
        )

    def forward(self, x):
        return self.model1(x)

def train_func(config):

    model = Model()
    # use ray framework prepare model
    model = ray.train.torch.prepare_model(model)
    criterion = CrossEntropyLoss()
    optimizer = Adam(model.parameters(), lr=0.001)

    # Data
    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])
    # local data directory path
    train_data = FashionMNIST(root=""C:/projects/python_project/code_scan/data"", train=True, download=False, transform=transform)
    train_loader = DataLoader(train_data, batch_size=128, shuffle=True)

    # ray prepare dataloader
    train_loader = ray.train.torch.prepare_data_loader(train_loader)

    # Training
    start_time = datetime.datetime.now()
    for epoch in range(2):
        print(epoch)
        i = 0
        print(len(train_loader))
        for images, labels in train_loader:
            i += 1
            outputs = model(images)
            loss = criterion(outputs, labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            print(""epoch: "" + str(epoch) + "" "" + str(len(train_loader)) + '/' + str(i))

        checkpoint_dir = ""C:/projects/python_project/code_scan/model""  # local checkpoint_dir path

        checkpoint_path = checkpoint_dir + ""/model.checkpoint""
        torch.save(model.state_dict(), checkpoint_path)

        ray.train.report({""loss"": loss.item()}, checkpoint=Checkpoint.from_directory(checkpoint_dir))


    end_time = datetime.datetime.now()
    print(""total duration: "" + str((end_time - start_time).total_seconds()))


    # [4] Configure scaling and resource requirements.
scaling_config = ScalingConfig(num_workers=8, use_gpu=False)
    # scaling_config = ScalingConfig(num_workers=2, use_gpu=False, resources_per_worker={""CPU"": 6})


run_config = RunConfig(storage_path=""C:/projects/python_project/code_scan/ray_results"")  # local ray results path

    # [5] Launch distributed training job.
trainer = TorchTrainer(train_func, scaling_config=scaling_config, run_config=run_config)
result = trainer.fit()
```
",python import import torch import import import normalize compose import import import import import ray use ray framework import import import torch torch import import torch class model self super model self forward self return model model use ray framework prepare model model model criterion data transform compose normalize local data directory path ray prepare training epoch range print epoch print model loss criterion print epoch epoch local path loss print total duration configure scaling resource local ray path launch distributed training job trainer result,issue,negative,positive,neutral,neutral,positive,positive
1916440486,"Any update on this open issue? Seems like an fundemental feature (allowing to render and visually check RL policy performance by setting render_env=True) gone missing more than a year ago (or is there any workaround I missed?) ...
@sven1977 ",update open issue like feature render visually check policy performance setting gone missing year ago,issue,negative,negative,neutral,neutral,negative,negative
1916240560,"Single Node which puts many/large objects no regresssion 


Metric | Master Run Value | PR Run Value | Difference (PR - Master)
-- | -- | -- | --
args_time | 17.543 | 16.973 | -0.570
returns_time | 5.544 | 5.622 | +0.078
get_time | 27.719 | 26.194 | -1.525
queued_time | 194.880 | 193.345 | -1.535
large_object_time | 31.502 | 31.156 | -0.346


",single node metric master run value run value difference master,issue,positive,negative,neutral,neutral,negative,negative
1916143409,"Did manual test locally to throw one arbitrary error in `try` code block, made sure the job succeeded.",manual test locally throw one arbitrary error try code block made sure job,issue,negative,positive,positive,positive,positive,positive
1916135372,"Microbecnhmark not showing perf diff:


| Metric                                        | PR Run Value         | Master Run Value    |
|-----------------------------------------------|----------------------|---------------------|
| single_client_get_calls_Plasma_Store          | 10288.52             | 10297.06            |
| single_client_put_calls_Plasma_Store          | 5359.70              | 5466.18             |
| multi_client_put_calls_Plasma_Store           | 12390.50             | 12069.89            |
| single_client_put_gigabytes                   | 20.25                | 19.54               |
| single_client_tasks_and_get_batch             | 7.73                 | 7.86                |
| multi_client_put_gigabytes                    | 33.88                | 36.48               |
| single_client_get_object_containing_10k_refs  | 13.35                | 13.88               |
| single_client_wait_1k_refs                    | 5.10                 | 5.45                |
| single_client_tasks_sync                      | 1034.55              | 1043.13             |
| single_client_tasks_async                     | 7549.56              | 8332.38             |
| multi_client_tasks_async                      | 23556.68             | 23350.87            |
| 1_1_actor_calls_sync                          | 2026.97              | 2071.05             |
| 1_1_actor_calls_async                         | 9144.84              | 8882.80             |
| 1_1_actor_calls_concurrent                    | 5497.23              | 5475.92             |
| 1_n_actor_calls_async                         | 8471.20              | 8540.28             |
| n_n_actor_calls_async                         | 27324.09             | 26895.85            |
| n_n_actor_calls_with_arg_async                | 2826.13              | 2884.08             |
| 1_1_async_actor_calls_sync                    | 1323.36              | 1363.17             |
| 1_1_async_actor_calls_async                   | 3285.37              | 3492.84             |
| 1_1_async_actor_calls_with_args_async         | 2346.97              | 2405.61             |
| 1_n_async_actor_calls_async                   | 7621.07              | 7628.48             |
| n_n_async_actor_calls_async                   | 23419.49             | 23469.56            |
| placement_group_create/removal                | 837.74               | 835.24              |
| client__get_calls                             | 1078.76              | 1048.75             |
| client__put_calls                             | 857.19               | 818.04              |
| client__put_gigabytes                         | 0.13                 | 0.13                |
| client__tasks_and_put_batch                   | 11482.82             | 11581.19            |
| client__1_1_actor_calls_sync                  | 542.96               | 558.50              |
| client__1_1_actor_calls_async                 | 1029.07              | 1064.06             |
| client__1_1_actor_calls_concurrent            | 1030.57              | 1061.52             |
| client__tasks_and_get_batch                   | 0.97                 | 0.99                |
",showing metric run value master run value,issue,positive,neutral,neutral,neutral,neutral,neutral
1916084312,should work now. @architkulkarni please merge when you see this.,work please merge see,issue,negative,neutral,neutral,neutral,neutral,neutral
1916072586,"Looks like the node is cgroup'd into 7.2 CPUs which makes Ray thinks it has 7 physical CPUs, while having  8 logical CPUs for scheduling.",like node ray physical logical,issue,negative,positive,positive,positive,positive,positive
1916065183,"```
>>> import psutil
>>> psutil.cpu_count(logical=False)
4
>>> psutil.cpu_count()
8
>>> import ray
>>> ray._private.utils.get_num_cpus(
...                 override_docker_cpu_warning=True
...             )
2024-01-29 20:42:33,747 WARNING utils.py:587 -- Ray currently does not support initializing Ray with fractional cpus. Your num_cpus will be truncated from 7.2 to 7.
7
>>> 
```",import import ray warning ray currently support ray fractional truncated,issue,negative,neutral,neutral,neutral,neutral,neutral
1915936270,"Ok, I can get further along by changing my RLModule spec to this:
```python
    @override(TorchRLModule)
    def output_specs_exploration(self) -> SpecType:
        return {""action_dist_inputs"": torch.Tensor}
```

Now it no longer complains when I return ` return {""action_dist_inputs"" ...}` and so long as I also override this method:

```python
def get_exploration_action_dist_cls(self) -> Type[Distribution]:
        # the validator uses this to turn the 'action_dist_inputs' into
        # an actual distribution object by using the from_logits method.
        return TorchCategorical
```

Now my `_forward_train` is getting called multiple times.  Interestingly though the `_initialize_loss_from_dummy_batch` calls with a batch size of 32, but after that `compute_bootstrap_value` calls with a batch size of 1.  Gru cells can't change the batch size on the fly like this so I had to insert another setup() call whenever the batch size changes.

But this then hits another weird issue in `compute_bootstrap_value` it is expecting my `forward_exploration` to return dict containing the key `SampleBatch.VF_PREDS` ?  Looks like ultimately it exects that to be a `np.array` ??? But the comment says this is a:
```python
# Value function predictions emitted by the behaviour policy.
```
but `class RLModule` makes no mention of this, but rather this class sets the default `output_specs_exploration` to `return {""action_dist"": Distribution}` which says nothing about VF_PREDS?
",get along spec python override self return longer return return long also override method python self type distribution turn actual distribution object method return getting multiple time interestingly though batch size batch size ca change batch size fly like insert another setup call whenever batch size another weird issue return key like ultimately comment python value function behaviour policy class mention rather class default return distribution nothing,issue,positive,positive,neutral,neutral,positive,positive
1915895088,"@sven1977, you changed some relevant code back in June 2023 in [this commit](https://github.com/ray-project/ray/commit/31ab8a753773eee0cbcbe4ba869bbb947179af30) in torch_policy_v2.py as follows:

```python
                # ACTION_DIST_INPUTS field returned by `forward_exploration()` ->
                # Create a distribution object.
                action_dist = None
                if SampleBatch.ACTION_DIST_INPUTS in fwd_out:
                    dist_inputs = fwd_out[SampleBatch.ACTION_DIST_INPUTS]
                    action_dist_class = self.model.get_exploration_action_dist_cls()
                    action_dist = action_dist_class.from_logits(dist_inputs)

                # If `forward_exploration()` returned actions, use them here as-is.
                if SampleBatch.ACTIONS in fwd_out:
                    actions = fwd_out[SampleBatch.ACTIONS]
                # Otherwise, sample actions from the distribution.
                else:
                    assert action_dist
                    actions = action_dist.sample()
```

and I'm curious why this isn't also looking for `SampleBatch.ACTION_DIST` as follows:

```python
                action_dist = None
                if SampleBatch.ACTION_DIST in fwd_out:
                    action_dist = fwd_out[SampleBatch.ACTION_DIST]
                elif SampleBatch.ACTION_DIST_INPUTS in fwd_out:
                    dist_inputs = fwd_out[SampleBatch.ACTION_DIST_INPUTS]
                    action_dist_class = self.model.get_exploration_action_dist_cls()
                    action_dist = action_dist_class.from_logits(dist_inputs)

                # If `forward_exploration()` returned actions, use them here as-is.
                if SampleBatch.ACTIONS in fwd_out:
                    actions = fwd_out[SampleBatch.ACTIONS]
                # Otherwise, sample actions from the distribution.
                else:
                    if action_dist is None:
                        raise KeyError(
                            ""Your RLModule's `forward_exploration()` method must return""
                            f"" a dict with either the {SampleBatch.ACTIONS} key or the ""
                            f""{SampleBatch.ACTION_DIST_INPUTS} key in it (or both)!""
                        )
                    actions = action_dist.sample()
```
especially given the default in ~\rllib\core\rl_module\rl_module.py for output_specs_exploration and output_specs_inference is `return {""action_dist"": Distribution}`...",relevant code back june commit python field returned create distribution object none returned use otherwise sample distribution else assert curious also looking python none returned use otherwise sample distribution else none raise method must return either key key especially given default return distribution,issue,positive,positive,neutral,neutral,positive,positive
1915871809,"Note: if I try and trick the above code by returning the np.array in `action_dist` then I get yet another error:

```python
ray.rllib.core.models.specs.checker.SpecCheckingError: output spec validation failed on GruModule.forward_exploration, 
Mismatch found in data element ('action_dist',), which is a TensorSpec: Expected data type <class 
 ray.rllib.models.distributions.Distribution'> but found <class 'torch.Tensor'>.
```
from   File ""F:\miniconda3\envs\sr\lib\site-packages\ray\rllib\core\models\specs\checker.py"", line 174, in _validate
    raise SpecCheckingError(",note try trick code get yet another error python output spec validation mismatch found data element data type class found class file line raise,issue,negative,neutral,neutral,neutral,neutral,neutral
1915869617,"Interesting, I think I figured out a path forwards, if my forward_train method returns multiple things:
```python
    @override(TorchRLModule)
    def _forward_train(self, batch: NestedDict) -> Mapping[str, Any]:
        x = nn.functional.relu(self.fc1(batch[""obs""].T.float()))
        self.hidden1 = self.gru1(x, self.hidden1)
        self.hidden2 = self.gru2(self.hidden1, self.hidden2)
        action_logits = self.fc2(self.hidden2)
        return {""action_dist_inputs"": action_logits,
                ""action_dist"": TorchCategorical(logits=action_logits),
                ""action"": action_logits }
```
And if I override this method:
```python
    @override(TorchRLModule)
    def get_exploration_action_dist_cls(self) -> Type[Distribution]:
        return TorchCategorical
```
Then the action_dist satisfies the check in `F:\miniconda3\envs\sr\Lib\site-packages\ray\rllib\core\models\specs\specs_dict.py` 
where it is checking to make sure the datacontains an `action_dist`:

```python
        for spec_key in self:
            if spec_key not in data:
                raise ValueError(
                    _MISSING_KEYS_FROM_DATA.format(spec_key, data_keys_set)
                )
```

And later when it returns up to torch_policy_v2.py the def _compute_action_helper method is looking for `SampleBatch.ACTION_DIST_INPUTS` or `SampleBatch.ACTIONS` but curiously ignores the `SampleBatch.ACTION_DIST` result.  So by adding the ACTION_DIST_INPUTS I enter that code path and it successfully wraps my dist_inputs with a `TorchCategorical` such that `action_dist` is not None when it gets to line 1224.

But then I get a new downstream error from policy.py saying `compute_actions_from_input_dict should not contain the key named `actions` !!  So I really can't seem to win this game of whackamole.  Is there a bug here somewhere?

```python
        for key, value in extra_outs.items():
            self._dummy_batch[key] = value
            if key not in self.view_requirements:
                if isinstance(value, (dict, np.ndarray)):
                    # the assumption is that value is a nested_dict of np.arrays leaves
                    space = get_gym_space_from_struct_of_tensors(value)
                    self.view_requirements[key] = ViewRequirement(
                        space=space, used_for_compute_actions=False
                    )
                else:
                    raise ValueError(
                        ""policy.compute_actions_from_input_dict() returns an ""
                        ""extra action output that is neither a numpy array nor a dict.""
                    )
```",interesting think figured path forward method multiple python override self batch batch return action override method python override self type distribution return check make sure python self data raise later method looking curiously result enter code path successfully none line get new downstream error saying contain key really ca seem win game bug somewhere python key value key value key value assumption value leaf space value key else raise extra action output neither array,issue,positive,positive,positive,positive,positive,positive
1915769920,"Taking a look at this it looks like an issue setting up the Ray Cluster.

Can you try just running a script that calls `ray.init()`?",taking look like issue setting ray cluster try running script,issue,negative,neutral,neutral,neutral,neutral,neutral
1915765354,"Here's a summary of the enhancements achieved by this proposal, once we **fully** migrate to the `v2.XGBoostTrainer`.

| Feature          | Status | Notes |
|------------------|--------|-------|
| Elastic training | ↔️   |  Elastic training implementation is no longer attached to `XGBoostTrainer`, but this is not technically a regression, since it didn't work before anyways.   |
| Checkpointing    | ↗️     |  Still the same as before, except checkpoint loading / post-processing logic is even more flexible now. Before, everything had to be through the xgboost callback.     |
| Ray Data Integration    | ↗️     |  Previously, the integration was mostly done on the `RayDMatrix` level instead of the existing `DataParallelTrainer` logic. Now, the integration is unified across more trainers.  This enables the streaming data implementation to be used with xgboost's experimental [iterator-based data loading feature](https://xgboost.readthedocs.io/en/latest/tutorials/external_memory.html#data-iterator).   |
| Future xgboost features  | ↗️     |   There are many features that are easily accessible since the user has control of the training loop, including a [federated distributed learning backend](https://github.com/dmlc/xgboost/blob/65d7bf2dfeb4ab737e4097ab39f887150be04a92/python-package/xgboost/collective.py#L55-L62), [iterator-based DMatrix loading](https://xgboost.readthedocs.io/en/latest/tutorials/external_memory.html#data-iterator), [multi-output classification](https://xgboost.readthedocs.io/en/latest/tutorials/multioutput.html), and whatever else xgboost adds in the future. |
| Usability  | ↗️     |  The current `XGBoostTrainer` is 2 unnecessary layers on top of the native `xgboost.train` API that people are familiar with. You have to pass configs through `XGBoostTrainer(params, label_column, **train_kwargs)`, which are then passed to `xgboost_ray.train`, which also has a bunch of arguments passed through with `**kwargs` to `xgboost.train`. This is really hard to use and can't utilize editor auto-complete. It's easier to let the user call `xgboost.train` directly. |

Let me know if there are any ""regressions"" that I'm missing with this change.",summary proposal fully migrate feature status elastic training elastic training implementation longer attached technically regression since work anyways still except loading logic even flexible everything ray data integration previously integration mostly done level instead logic integration unified across streaming data implementation used experimental data loading feature future many easily accessible since user control training loop distributed learning loading classification whatever else future usability current unnecessary top native people familiar pas also bunch really hard use ca utilize editor easier let user call directly let know missing change,issue,positive,positive,neutral,neutral,positive,positive
1915749131,"@woshiyyya

> Would we support or remove support for elastic training? (not sure how many users are using it, but my intuition is we are removing this feature)
* This PR would remove the elastic training part. I'm not actually sure if the old `XGBoostTrainer` even allows usage of the elastic training feature. `xgboost_ray` re-implements the execution loop, which waits for distributed worker futures and launching workers when one fails.
* However, this would get caught by Ray Train, the entire placement group would get removed and restarted. OR, this would hang if the user includes a `ray.train.report` in our provided callback.
* We can revisit xgb elastic training in the future along with other data parallel trainers.

> Shall we provide a simple API for users to get a xgboost.DMatrix. I noticed that in xgboost_ray, there will be no further processing logics in the train_func after passing the RayDMatrix into the train().

* I think it's not needed to add another API on top of DMatrix. `RayDMatrix` is an abstraction used to shard the dataset across multiple Ray workers, but we don't really need that since Ray Data can do that already, so I don't want to keep RayDMatrix around. Plus, Ray Data is the only method that we recommend for `XGBoostTrainer` in the first place.
* [Here's what regular `xgboost.DMatrix` usage looks like.](https://github.com/ray-project/xgboost_ray/blob/7cf815bb1a948f8fccc60cc0ec91da85452d3a68/xgboost_ray/examples/simple.py#L12-L17)
* Here's what it looks like with Ray Train + Ray Data. It matches up with the native way of using xgboost.
```python
           train_ds = ray.train.get_dataset_shard(""train"")
            train_df = train_ds.to_pandas()
            X, y = train_df.drop(""y"", axis=1), train_df[""y""]
            dtrain = xgboost.DMatrix(X, label=y)
```

> What would be the checkpointing code looks like? Will that be a Xgboost's native callback, which calls ray.train.report on some hooks?

Two options:
1. `TuneReportCheckpointCallback` which is already our recommendation, and users can implement their own if needed similar to lightning/transformers.
2. Call `ray.train.report` manually by iteratively training more models:
```python
bst_model = None
num_boost_rounds_per_iter =
for i in range(num_iters):
    bst_model = xgboost.train(
        ..., xgb_model=bst_model,  # start from bst_model
        num_boost_rounds=num_boost_rounds_per_iter
    )
    ray.train.report(..., checkpoint=...)
```",would support remove support elastic training sure many intuition removing feature would remove elastic training part actually sure old even usage elastic training feature execution loop distributed worker one however would get caught ray train entire placement group would get removed would user provided revisit elastic training future along data parallel shall provide simple get passing train think add another top abstraction used shard across multiple ray really need since ray data already want keep around plus ray data method recommend first place regular usage like like ray train ray data native way python train would code like native two already recommendation implement similar call manually iteratively training python none range start,issue,positive,positive,positive,positive,positive,positive
1915725564,"Hey @surak, thanks for posting this issue. I think what you're saying makes sense. Let me take a look to see if it's possible to remove this default and use the system default instead.",hey thanks posting issue think saying sense let take look see possible remove default use system default instead,issue,negative,positive,neutral,neutral,positive,positive
1915700295,"Based on the discussions, we would do this for Client:

- error on 2 digit python mismatch
- warning on 3 digit python mismatch
- error on ray version mismatch",based would client error digit python mismatch warning digit python mismatch error ray version mismatch,issue,negative,neutral,neutral,neutral,neutral,neutral
1915687867,"> does this issue require any feature? Or is it just a question? Do we need a doc update? Can you follow up with action items here?

I think the user plans to use KubeRay instead, and we can close this issue. cc @jaanphare to confirm",issue require feature question need doc update follow action think user use instead close issue confirm,issue,negative,positive,neutral,neutral,positive,positive
1915681768,@kevin85421 does this issue require any feature? Or is it just a question? Do we need a doc update? Can you follow up with action items here? ,issue require feature question need doc update follow action,issue,negative,positive,neutral,neutral,positive,positive
1915660495,@y-abe are you willing to contribute the fix? We can help review the PR.,willing contribute fix help review,issue,negative,positive,positive,positive,positive,positive
1915645514,"One general comment: I find the terms `RayJob` and `Ray job` to be confusing. In addition, in the observability docs, we have the convention of capitalize `Ray Job` to distinguish it from the more common term of `job`. In this case, capitalizing `Ray Job` would add to the confusion. At some point, we should think about names that are less similar to reduce confusion for our readers.",one general comment find ray job addition observability convention capitalize ray job distinguish common term job case ray job would add confusion point think le similar reduce confusion,issue,negative,negative,neutral,neutral,negative,negative
1915618706,"Okay, sounds good. If you want to track something to it's genesis, you can use `git blame`. In this case it looks like it was added here, but there's not really an explanation on the PR: https://github.com/ray-project/ray/pull/13846

> Shouldn't this constant be the same on a given ray version anyway?

^I'm still wondering about this. I'm also not sure if this fixes anything with https://github.com/ray-project/ray/issues/42356, isn't this removing a layer of version checking?",good want track something genesis use git blame case like added really explanation constant given ray version anyway still wondering also sure anything removing layer version,issue,positive,positive,positive,positive,positive,positive
1915607862,"In our environment, we are using dns, using host name works fine.",environment host name work fine,issue,negative,positive,positive,positive,positive,positive
1915604009,Eric explained it was safe to remove it. Did not track to its genesis though.,eric safe remove track genesis though,issue,negative,positive,positive,positive,positive,positive
1915581255,"One issue here is the default for `max_concurrent_queries` is too high right now (100). We should change this in an upcoming release.

As a temporary fix, when `num_replicas=""auto""` is set and `max_concurrent_queries` is unset, we'll default it.

Open question: what should the defaults be? To start, let's go with `max_concurrent_queries=5` and `target_num_ongoing_requests_per_replica=0.4*max_concurrent_queries=2`.",one issue default high right change upcoming release temporary fix auto set unset default open question start let go,issue,negative,positive,positive,positive,positive,positive
1915560411,"Very neat solution! I have some general questions:
- Would we support or remove support for elastic training? (not sure how many users are using it, but my intuition is we are removing this feature)
- Shall we provide a simple API for users to get a `xgboost.DMatrix`. I noticed that in xgboost_ray, there will be no further processing logics in the `train_func` after passing the `RayDMatrix` into the `train()`.
- What would be the checkpointing code looks like? Will that be a Xgboost's native callback, which calls `ray.train.report` on some hooks?",neat solution general would support remove support elastic training sure many intuition removing feature shall provide simple get passing train would code like native,issue,positive,positive,positive,positive,positive,positive
1915489000,"Could be. I couldn't actually find any docs on how the Python grpc lib handles cancellation aside from a bit set in the servicer context: https://grpc.github.io/grpc/python/grpc_asyncio.html#grpc.aio.ServicerContext.cancelled

Have you tested it out?",could could actually find python cancellation aside bit set context tested,issue,negative,neutral,neutral,neutral,neutral,neutral
1915468914,"@rkooo567  I think this update should be ok. Besides dataclasses support, it is mostly about:

1. removing python3.7 support
2. linter and formatter change
3. fix typos
4. deprecating the ""slow"" cloudpickler, since we have pickle-5 protocol support by default

They are not broken changes for Ray (Ray supports python >= 3.8), so I think it should be ok.

Anyway we have to make sure all unittests pass, since a lot of functionality in Ray relies on cloudpickle in a complex way, it's hard to see the effect under the change of so much code.",think update besides support mostly removing python support linter change fix slow since protocol support default broken ray ray python think anyway make sure pas since lot functionality ray complex way hard see effect change much code,issue,positive,negative,neutral,neutral,negative,negative
1915426889,"I took a deeper look at the existing logs. We already log a message when the code version of an app has changed ([code link](https://github.com/ray-project/ray/blob/d2311655cf43df629c2e7123414b9e1fa77707a9/python/ray/serve/_private/application_state.py#L414)). This should be sufficient to detect when an entirely new config has been submitted.

Given this, there isn't a pressing reason to log the entire config. I'll close this PR for now.",took look already log message code version code link sufficient detect entirely new given pressing reason log entire close,issue,negative,positive,neutral,neutral,positive,positive
1915403785,"This does look pretty confusing with JSON mode enabled though. There are lots of newlines and spaces:

```
{""levelname"": ""INFO"", ""asctime"": ""2024-01-29 11:19:58,221"", ""component_name"": ""controller"", ""component_id"": ""98998"", ""message"": ""controller.py:791 - Deploying a new config:\n{\n    \""proxy_location\"": \""EveryNode\"",\n    \""http_options\"": {\n        \""host\"": \""0.0.0.0\"",\n        \""port\"": 8000\n    },\n    \""grpc_options\"": {\n        \""port\"": 9000,\n        \""grpc_servicer_functions\"": []\n    },\n    \""logging_config\"": {\n        \""encoding\"": \""JSON\"",\n        \""log_level\"": \""INFO\"",\n        \""logs_dir\"": null,\n        \""enable_access_log\"": true\n    },\n    \""applications\"": [\n        {\n            \""name\"": \""app1\"",\n            \""route_prefix\"": \""/\"",\n            \""import_path\"": \""test_app:app\"",\n            \""runtime_env\"": {},\n            \""deployments\"": [\n                {\n                    \""name\"": \""hello\""\n                }\n            ]\n        }\n    ]\n}""}
```",look pretty mode though lot controller message new null,issue,negative,positive,positive,positive,positive,positive
1915392043,"> This is going to be extremely noisy

I updated the PR to only write a log when a new config is received.",going extremely noisy write log new received,issue,negative,positive,neutral,neutral,positive,positive
1915358745,Were you able to find out why this is there in the first place? Shouldn't this constant be the same on a given ray version anyway?,able find first place constant given ray version anyway,issue,negative,positive,positive,positive,positive,positive
1915346474,"> There is a task that receives the `ASGI` messages. Its exit signals the client disconnecting.
> 
> https://github.com/ray-project/ray/blob/e154c92eb42f532afa02235058cde7837813f268/python/ray/serve/_private/proxy.py#L871
> 
> That causes the `ProxyGenerator` to raise the cancellation error:

I see, makes sense. This is still a problem for gRPC though.",task exit client raise cancellation error see sense still problem though,issue,negative,neutral,neutral,neutral,neutral,neutral
1915335198,@matthewdeng can you take a look and see if this still an issue?,take look see still issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1915309034,Also ran postmerge and seeing all serve tests passing https://buildkite.com/ray-project/postmerge/builds/2735#_,also ran seeing serve passing,issue,negative,neutral,neutral,neutral,neutral,neutral
1915306919,Need to rebase to latest master.,need rebase latest master,issue,negative,positive,positive,positive,positive,positive
1915280364,"I still don't understand what the final solution is for this. The documentation in different is all over the place and its super confusing and not even sure its possible to do something like:

```sh
export RAY_ADDRESS=""...""
ray list nodes
ray job submit

python -c ""import ray; ray.init()
```

My suggestion is to make a different environment variable for each possible API/port that gets exposed, e.g.:

- `RAY_API_ADDRESS` for `8265`
- `RAY_JOBS_ADDRESS` for ??
- `RAY_CLIENT_ADDRESS` for 10001
- etc.

And clearly document for each the URL format that is accepted.
If it accepts the `ray://` protocol please explain what this is actually doing.
Also please explain for cases with RayCluster with custom ingresses how to write URLs for specific clusters, if that is even needed.

I think once you have these individual final overrides you can work on/implement a ""catch-all"" `RAY_ADDRESS` which magically does what you want based on good defaults on port numbers (or some config file that overrides port designations).

This allows for maximum flexibility for end users. Perhaps I want to firewall certain ports and allow  the `/list/nodes` API but not the Jobs, Client API. And in another allow autoscaling, etc.",still understand final solution documentation different place super even sure possible something like sh export ray list ray job submit python import ray suggestion make different environment variable possible exposed clearly document format accepted ray protocol please explain actually also please explain custom write specific even think individual final work magically want based good port file port maximum flexibility end perhaps want certain allow client another allow,issue,positive,positive,positive,positive,positive,positive
1915215981,"> Will the ""_report_task_events"" set for the actor task override the ""_report_task_events"" set during the creation of the actor? For example, in the following code:
> 
> ```
> @ray.remote
> class Actor():
>     def g(self):
>        pass
> 
> 
> a = Actor.options(_report_task_events=False).remote()
> 
> a.g.options(_report_task_events=True).remote()
> ```
> 
> Does the actor task g report task events or not? @rickyyx

Yes, an actor task level config would override the actor level config. 

",set actor task override set creation actor example following code class actor self pas actor task report task yes actor task level would override actor level,issue,positive,neutral,neutral,neutral,neutral,neutral
1914751099,"@matthewdeng I was running the job in a cluster with several nodes, which is not running it in a local machine.",running job cluster several running local machine,issue,negative,neutral,neutral,neutral,neutral,neutral
1914507068,"FWIW I think this is a bug in pedalboard, but I think it would be good to get some log warnings in these situations
",think bug think would good get log,issue,negative,positive,positive,positive,positive,positive
1914467865,"I guess the user used the option `--temp-dir` in `ray start` to change the temp dir path of the worker node. If so, the reason why only the head node could change but the worker node could not is that, the option `--temp-dir` will be ignored when starting worker node. See the [source code](https://github.com/ray-project/ray/blob/fd56d10ef697df411c54e92169eb55a9d5ae3de1/python/ray/scripts/scripts.py#L625).

And in this case, the worker node get the `ray_params` with the `temp_dir = None`, then when [initializing the worker node](https://github.com/ray-project/ray/blob/fd56d10ef697df411c54e92169eb55a9d5ae3de1/python/ray/scripts/scripts.py#L928), it will call a function `self._init_temp()` to get the temp dir. For the worker node, as the `ray_params.temp_dir` is set to be None before, it will always get the temp dir from the head node gsc k-v store, see the [source code](https://github.com/ray-project/ray/blob/3627e946dca7dd90b9f99dd6b3641910b10f932e/python/ray/_private/node.py#L418). Finally, the temp dir path of worker nodes will be same as the head node.

To change the temp dir of the worker node directly, you can export an environment var `RAY_TMPDIR`. For example:
```bash
# start head node
ray start --head --temp-dir /tmp/ray-head ...
or 
export RAY_TMPDIR=/tmp/ray-head && ray satrt --head ...
# start worker node
export RAY_TMPDIR=/tmp/ray-worker
ray start --address $head_ip
```",guess user used option ray start change temp path worker node reason head node could change worker node could option starting worker node see source code case worker node get none worker node call function get temp worker node set none always get temp head node store see source code finally temp path worker head node change temp worker node directly export environment example bash start head node ray start head export ray head start worker node export ray start address,issue,negative,positive,neutral,neutral,positive,positive
1914233809,"Will the ""_report_task_events"" set for the actor task override the ""_report_task_events"" set during the creation of the actor?
For example, in the following code:
```
@ray.remote
class Actor():
    def g(self):
       pass


a = Actor.options(_report_task_events=False).remote()

a.g.options(_report_task_events=True).remote()
```

Does the actor task g report task events or not?
@rickyyx ",set actor task override set creation actor example following code class actor self pas actor task report task,issue,negative,neutral,neutral,neutral,neutral,neutral
1913999989,"After discussion with @aslonnie, I'll be breaking this up into a number of small PRs to avoid impacting a lot of code owners at once.",discussion breaking number small avoid lot code,issue,negative,negative,negative,negative,negative,negative
1913564713,"CC @jjyao

btw, shall I name the new param ""heap_memory_worker_node"" or ""memory_worker_node"" ? ",shall name new param,issue,negative,positive,positive,positive,positive,positive
1913460606,"It's still reproducing. The bundled version of cloudpickle didn't catch up with the upstream version the problem was fixed in.

* Here: https://github.com/ray-project/ray/blob/master/python/ray/cloudpickle/cloudpickle_fast.py#L130-L131
* Upstream: https://github.com/cloudpipe/cloudpickle/pull/495/files#diff-3a0a886a228997d065d615f2f5860bb94e616656b634c433a416f234470ad9e3L114
",still version catch upstream version problem fixed upstream,issue,negative,positive,neutral,neutral,positive,positive
1913437697,"Can you add a TODO to upgrade, merge with the common dependencies as a follow up. Thankkks",add upgrade merge common follow,issue,negative,negative,negative,negative,negative,negative
1913411221,"Does Ray support dataset using client mode? If it doesn't, let's specify it in the document, and this ticket can be closed. ",ray support client mode let specify document ticket closed,issue,negative,negative,neutral,neutral,negative,negative
1913172301,@can-anyscale it's expected that serve min build will fail until https://github.com/ray-project/ray/pull/42740 is merged,serve min build fail,issue,negative,negative,negative,negative,negative,negative
1913102809,"@rkooo567 - not easy to reproduce, and probably related to a specific version. @rickyyx  - you have mentioned this to be related to master, would you be able to confirm the ray version?",easy reproduce probably related specific version related master would able confirm ray version,issue,negative,positive,positive,positive,positive,positive
1913042178,"something went wrong with singing off, i will build doctests locally and resumit when that works",something went wrong singing build locally work,issue,negative,negative,negative,negative,negative,negative
1912973725,"I don't live in California, so I think you mean virtual meetings right ? Also any other information you would need that you feel would be useful? Feel free to PM if you want to do a pair debug. ",live think mean virtual right also information would need feel would useful feel free want pair,issue,positive,positive,positive,positive,positive,positive
1912970370,maybe also update the test tags conditioning?,maybe also update test,issue,negative,neutral,neutral,neutral,neutral,neutral
1912967498,"Btw, the `psutil` library isn't python builtin library, I don't think its implementation is authoritative and trustable,

But the /proc/meminfo and `free` command output should be trustable, they are OS builtin metrics / tools.",library python library think implementation authoritative trustable free command output trustable o metric,issue,positive,positive,positive,positive,positive,positive
1912921915,serveminbuild is broken i'm not sure if that's expected or this needs to come together with your fix @edoakes ,broken sure need come together fix,issue,negative,positive,neutral,neutral,positive,positive
1912903318,@aslonnie  Next step is to put all these flaky tests into a separate test suite. I'll put up a PR for that. ,next step put flaky separate test suite put,issue,negative,neutral,neutral,neutral,neutral,neutral
1912901466,"> unit test not passing?

@aslonnie  Oh ya this is my bad. I forgot to revert the array format to before pushing the reformatting of the test generation. Updated it with the right one.",unit test passing oh ya bad forgot revert array format pushing test generation right one,issue,negative,negative,negative,negative,negative,negative
1912901133,"The min builds passed now, I think we're good. Will wait for all CI to finish, but I'll put up the cherry pick in parallel.",min think good wait finish put cherry pick parallel,issue,negative,positive,positive,positive,positive,positive
1912889963,"anyways, feel free to merge if this unblocks the release short term, as it does not make things worse than status quo.

I will push for discussion on the long-term solution in other threads/forums. ",anyways feel free merge release short term make worse status quo push discussion solution,issue,negative,neutral,neutral,neutral,neutral,neutral
1912848521,"> Turns out it's not the whole story, we need to fix: #42757
> 
> But this should still be merged (though tbh I'm not sure what it does)

Oh I see, no wonder my PR also keeps failing XD that `python -m pip install -U ""ray[serve]""` always overrides the pinned fastapi version. Good find! ",turn whole story need fix still though sure oh see wonder also failing python pip install ray serve always pinned version good find,issue,negative,positive,positive,positive,positive,positive
1912844988,"Turns out it's not the whole story, we need to fix: https://github.com/ray-project/ray/issues/42757

But this should still be merged (though tbh I'm not sure what it does)",turn whole story need fix still though sure,issue,negative,positive,positive,positive,positive,positive
1912834433,"One idea for how to solve this:

Instead of duplicating the dependencies in `setup.py` and `requirements.txt`, only have them in `requirements.txt` and `requirements_{extra}.txt` for each extra target we have.

Then in the testing we can just directly install from those `requirements_{extra}.txt` files.",one idea solve instead extra extra target testing directly install extra,issue,negative,positive,neutral,neutral,positive,positive
1912827935,Bumping this again - has it seen any major progress yet?,bumping seen major progress yet,issue,negative,positive,neutral,neutral,positive,positive
1912818757,"That was not the correct fix (but it's also needed).

It turns out that the minimal [build docker image](https://github.com/ray-project/ray/blob/df3dd96c1e54df5428b6a6dc3f085103421cfc89/ci/docker/min.build.Dockerfile#L31) is actually pulling dependency requirements from the latest release version of Ray rather than the one built in CI:

Logs from [build](https://buildkite.com/ray-project/premerge/builds/17622#018d47df-cf01-4555-8766-88652ac4672a):
```
#9 30.37 Collecting ray[serve]
--
  | #9 30.40   Downloading ray-2.9.1-cp38-cp38-manylinux2014_x86_64.whl.metadata (13 kB)
  | #9 30.53 Collecting click>=7.0 (from ray[serve])
  | #9 30.54   Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)
  | #9 30.57 Collecting filelock (from ray[serve])
  | #9 30.57   Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)
  | #9 30.61 Collecting jsonschema (from ray[serve])
  | #9 30.61   Using cached jsonschema-4.21.1-py3-none-any.whl.metadata (7.8 kB)
  | #9 30.66 Collecting msgpack<2.0.0,>=1.0.0 (from ray[serve])
  | #9 30.67   Using cached msgpack-1.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)
  | #9 30.67 Requirement already satisfied: packaging in /opt/miniconda/lib/python3.8/site-packages (from ray[serve]) (23.2)
  | #9 30.85 Collecting protobuf!=3.19.5,>=3.15.3 (from ray[serve])
  | #9 30.85   Using cached protobuf-4.25.2-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)
  | #9 30.89 Collecting pyyaml (from ray[serve])
  | #9 30.89   Using cached PyYAML-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)
  | #9 30.91 Collecting aiosignal (from ray[serve])
  | #9 30.91   Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)
  | #9 30.96 Collecting frozenlist (from ray[serve])
  | #9 30.96   Using cached frozenlist-1.4.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)
  | #9 30.96 Requirement already satisfied: requests in /opt/miniconda/lib/python3.8/site-packages (from ray[serve]) (2.28.1)
  | #9 31.03 Collecting fastapi (from ray[serve])
  | #9 31.04   Downloading fastapi-0.109.0-py3-none-any.whl.metadata (24 kB)
```",correct fix also turn minimal build docker image actually dependency latest release version ray rather one built build ray serve click ray serve ray serve ray serve ray serve requirement already satisfied ray serve ray serve ray serve ray serve ray serve requirement already satisfied ray serve ray serve,issue,positive,positive,positive,positive,positive,positive
1912806050,"Update: 
The issue happens when there is a gradient sync per batch.  E.g., 
```python
for batch in it.iter_batches():
    all_reduce()
```

We suspect it's because `streaming_split` returns different number of rows for each train worker and makes the all_reduce misaligned. 
We'll merge this PR first, as it doesn't make sense to have this dependency in the first place. ",update issue gradient sync per batch python batch suspect different number train worker merge first make sense dependency first place,issue,negative,positive,positive,positive,positive,positive
1912801831,This is going to be extremely noisy and will not format well if you use multi-line formatting. Is there a way we can distill the information we care about?,going extremely noisy format well use way distill information care,issue,positive,negative,negative,negative,negative,negative
1912800374,"> Oh was this it😯 I was also testing out where else this needs to be pinned as well. Thanks for fixing it Ed!

Not 100% confirmed yet but I believe so :/",oh also testing else need pinned well thanks fixing confirmed yet believe,issue,positive,positive,positive,positive,positive,positive
1912798054,@edoakes @sihanwang41 Just in case the notification is missed. Would be nice to get this in 🙏,case notification would nice get,issue,negative,positive,positive,positive,positive,positive
1912797531,"> > Clients disconnecting is detected via the `http.client.disconnect` message. `uvicorn` will not cancel the handling task.
> 
> But we handle CancelledError in both HTTP and gRPC, what am i missing?

There is a task that receives the `ASGI` messages. Its exit signals the client disconnecting.
https://github.com/ray-project/ray/blob/e154c92eb42f532afa02235058cde7837813f268/python/ray/serve/_private/proxy.py#L871

That causes the `ProxyGenerator` to raise the cancellation error:
https://github.com/ray-project/ray/blob/e154c92eb42f532afa02235058cde7837813f268/python/ray/serve/_private/proxy_response_generator.py#L29",via message cancel handling task handle missing task exit client raise cancellation error,issue,negative,negative,negative,negative,negative,negative
1912796797,"One thing that I don't have full clarity on is the difference between setting the scheduling strategy of `ray_remote_args` as is done [here](https://github.com/ray-project/ray/blob/master/python/ray/data/read_api.py#L307-L311), and setting the scheduling strategy in the remote function options as is done [here](https://github.com/ray-project/ray/blob/master/python/ray/data/read_api.py#L341C9-L343).

In the description of this issue, I surmised that the worker nodes are trying to read the file local to the head node because `ray_remote_args[""scheduling_strategy""]` is not being set to the `NodeAffinitySchedulingStrategy`. However, [this comment](https://github.com/ray-project/ray/blob/master/python/ray/data/read_api.py#L334) makes it seem like the only thing that matters is the scheduling strategy that gets set in the remote options, when creating `get_datasource_or_legacy_reader`.

**Question 1**: what is the difference between these two uses of the `NodeAffinitySchedulingStrategy`, and which case is actually causing the worker nodes to read the head node's local file?  

Another observation I've made is that even when the remote options for `get_datasource_or_legacy_reader` include `scheduling_strategy == NodeAffinitySchedulingStrategy`, when the [line](https://github.com/ray-project/ray/blob/master/python/ray/data/read_api.py#L364) `read_tasks = datasource_or_legacy_reader.get_read_tasks(requested_parallelism)` gets run and I look inside `file_based_datasource.get_read_tasks`, `ctx.scheduling_strategy` is still `SPREAD`, which seems contradictory.

**Question 2**: is the behavior I observed expected?

Perhaps this behavior is fine, since when `read_datasource` creates the block list after calling `get_read_tasks`, it includes the `ray_remote_args`:

```
block_list = LazyBlockList(
        read_tasks,
        read_stage_name=read_stage_name,
        ray_remote_args=ray_remote_args,
        owned_by_consumer=False,
    )
```

Of course, in this hypothetical case, if we rely on `ray_remote_args[""scheduling_strategy""]` to define whether the read tasks get run by worker nodes or not, then we'd need  `ray_remote_args[""scheduling_strategy""]` to get set correctly to `NodeAffinitySchedulingStrategy`.",one thing full clarity difference setting strategy done setting strategy remote function done description issue surmised worker trying read file local head node set however comment seem like thing strategy set remote question difference two case actually causing worker read head node local file another observation made even remote include line run look inside still spread contradictory question behavior perhaps behavior fine since block list calling course hypothetical case rely define whether read get run worker need get set correctly,issue,negative,positive,neutral,neutral,positive,positive
1912794171,"Also, do you have any specific code samples in mind?",also specific code mind,issue,negative,neutral,neutral,neutral,neutral,neutral
1912794016,@anyscalesam could you elaborate? What do you mean by comment coverage?,could elaborate mean comment coverage,issue,negative,positive,neutral,neutral,positive,positive
1912778586,"Ok, I ran the test manually after installing in a fresh conda env from the built wheel and it passed, so I think it's actually a testing setup issue. Looks like the `python/requirements.txt` file was not updated in tandem with `setup.py`. This might be the issue.

Pushing a commit to verify, if it works I'll make a PR to master and then cherry-pick here.",ran test manually fresh built wheel think actually testing setup issue like file tandem might issue pushing commit verify work make master,issue,positive,positive,positive,positive,positive,positive
1912765921,"Seeing an issue where the follow up answer is giving me an endless loop of ""sources"" https://www.loom.com/share/bae40dc9bbd740e49f5a76e27fb79a67",seeing issue follow answer giving endless loop,issue,negative,negative,negative,negative,negative,negative
1912734046,"Ok, looks like there was some kind of behavior change in redirect handling either in `fastapi` or `starlette`.

Working version:
```
INFO 2024-01-26 15:40:49,444 proxy 127.0.0.1 831aae9b-614b-4a41-b7cd-5922a3dc762a /hello proxy.py:480 - GET 307 16.8ms
INFO 2024-01-26 15:40:49,453 proxy 127.0.0.1 275fdd40-36c3-4873-ab86-fd91fbfe2b84 /hello/ proxy.py:480 - GET 200 7.8ms
```

Broken version:
```
INFO 2024-01-26 20:12:28,852 proxy 172.16.0.4 7fefe3cd-fa67-46ce-84fc-ff08ea8b5763 /hello proxy.py:480 - GET 307 20.3ms
INFO 2024-01-26 20:12:28,858 proxy 172.16.0.4 b233ec00-9a66-4978-9015-7d4b8610f2b5 / proxy.py:480 - GET 200 5.5ms
```

Seems this has uncovered some additional bug. Let me try dropping the versions down.",like kind behavior change redirect handling either working version proxy get proxy get broken version proxy get proxy get uncovered additional bug let try dropping,issue,negative,positive,neutral,neutral,positive,positive
1912712821,This one has me a bit stumped. Not able to repro the failure with the same pinned dependencies locally. Might take some time to resolve.,one bit able failure pinned locally might take time resolve,issue,negative,positive,neutral,neutral,positive,positive
1912706516,Adding a `devprod` label in case they have ideas on how to fix this when they look at other API improvements.,label case fix look,issue,negative,neutral,neutral,neutral,neutral,neutral
1912685604,"I added `extra={""log_to_stderr"": False}` to the log statement, so it doesn't show up in `serve run`. I also modified the formatting, so the config looks like this:

```
INFO 2024-01-26 12:55:26,239 controller 97705 controller.py:786 - Received config:
{
    ""proxy_location"": ""EveryNode"",
    ""http_options"": {
        ""host"": ""0.0.0.0"",
        ""port"": 8000
    },
    ""grpc_options"": {
        ""port"": 9000,
        ""grpc_servicer_functions"": []
    },
    ""applications"": [
        {
            ""name"": ""app1"",
            ""route_prefix"": ""/"",
            ""import_path"": ""test_app:app"",
            ""runtime_env"": {},
            ""deployments"": [
                {
                    ""name"": ""hello""
                }
            ]
        }
    ]
}
```",added false log statement show serve run also like controller received host port port name name hello,issue,negative,negative,negative,negative,negative,negative
1912674729,"There is no issue when I submit the job to the cluster using command line. According to this post, ray.put() can put the data to the node when the code runs. However, in my scenario, I am running the code on a node which is not part of the cluster. Is my assumption correct? How do I bypass the issue?",issue submit job cluster command line according post put data node code however scenario running code node part cluster assumption correct bypass issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1912665442,"> Could you include an example of what this would look like? Would this show up in the outputs streamed in `serve run`?

Good question– yes it show up in `serve run` if you pass in a config file. The output looks like this:

```
INFO 2024-01-26 12:31:39,553 controller 91367 controller.py:785 - Received config: proxy_location=<ProxyLocation.EveryNode: 'EveryNode'> http_options=HTTPOptionsSchema(host='0.0.0.0', port=8000, root_path='', request_timeout_s=None, keep_alive_timeout_s=5) grpc_options=gRPCOptionsSchema(port=9000, grpc_servicer_functions=[]) logging_config=None applications=[ServeApplicationSchema(name='app1', route_prefix='/', import_path='test_app:app', runtime_env={}, host='0.0.0.0', port=8000, deployments=[DeploymentSchema(name='hello', num_replicas=<DEFAULT.VALUE: 1>, route_prefix=<DEFAULT.VALUE: 1>, max_concurrent_queries=<DEFAULT.VALUE: 1>, user_config=<DEFAULT.VALUE: 1>, autoscaling_config=<DEFAULT.VALUE: 1>, graceful_shutdown_wait_loop_s=<DEFAULT.VALUE: 1>, graceful_shutdown_timeout_s=<DEFAULT.VALUE: 1>, health_check_period_s=<DEFAULT.VALUE: 1>, health_check_timeout_s=<DEFAULT.VALUE: 1>, ray_actor_options=<DEFAULT.VALUE: 1>, placement_group_bundles=<DEFAULT.VALUE: 1>, placement_group_strategy=<DEFAULT.VALUE: 1>, max_replicas_per_node=<DEFAULT.VALUE: 1>, logging_config=<DEFAULT.VALUE: 1>)], args={}, logging_config=None)] target_capacity=None
```

I'll look into ways to clean this up.",could include example would look like would show serve run good yes show serve run pas file output like controller received look way clean,issue,positive,positive,positive,positive,positive,positive
1912587953,"@iambsk 

> I am seeing that by using ds.iter_rows it doesnt stream but rather loads everything into mem, is this correct?

No, `Dataset.iter_rows` is streamed, and it shouldn't load all of the data in memory.

> however at the end they call .materialize(), which loads everything into the object store memory. Why is that?

In this example, it's not materializing the actual data. It's materializing the number of rows inserted:

```
{""num_success"": np.array([result.upserted_count])}
```

> Is the correct route to follow the anyscale example and not call .materialize()? 

If you don't call `materialize`, the dataset won't execute.

> It slowly builds up to using 1TB of object_store_memory, at which point it then fails

This makes me think there's an issue with the scheduler where Ray Data adds too much the data to the pipeline. This should be fixed in Ray 2.10.

Could you show me what the progress bars look like when you run the program with verbose progress? Depending on the exact problem, I can suggest some workarounds",seeing doesnt stream rather everything mem correct load data memory however end call everything object store memory example actual data number inserted correct route follow example call call materialize wo execute slowly point think issue ray data much data pipeline fixed ray could show progress look like run program verbose progress depending exact problem suggest,issue,positive,positive,neutral,neutral,positive,positive
1912555096,Hmm looks like it's consistently failing. Let me take a closer look.,like consistently failing let take closer look,issue,negative,positive,positive,positive,positive,positive
1912503893,"> Clients disconnecting is detected via the `http.client.disconnect` message. `uvicorn` will not cancel the handling task.

But we handle CancelledError in both HTTP and gRPC, what am i missing?",via message cancel handling task handle missing,issue,negative,negative,negative,negative,negative,negative
1912497960,"We merged a couple PRs that help improve observability and add a bugfix:

* #42631
* #42705",couple help improve observability add,issue,positive,neutral,neutral,neutral,neutral,neutral
1912451819,"@edoakes is this failure relevant? 
```

# Test FastAPI
--
  | serve.run(MyFastAPIDeployment.bind(), name=""FastAPI"")
  | >       assert requests.get(""http://127.0.0.1:8000/hello"").text == '""Hello, world!""'
  | E       assert 'got f' == '""Hello, world!""'
  | E         - ""Hello, world!""
  | E         + got f
  |  
  | python/ray/serve/tests/test_api.py:432: AssertionError


```
https://buildkite.com/ray-project/premerge/builds/17576#018d469c-5fba-4657-b086-80e4b927a6c6/6-4438",failure relevant test assert hello world assert hello world hello world got,issue,negative,positive,neutral,neutral,positive,positive
1912425812,Typo on downgrade; keeping to p0 after discuss with @rkooo567 ,typo downgrade keeping discus,issue,negative,neutral,neutral,neutral,neutral,neutral
1912361024,"> @GeneDer so with these changes, what would be the difference in the example screenshot you posted? Would there just be empty `message` fields instead?

Yes, the screenshot shows the differences. The first highlighted part is without this change. You see those ""Finished recovering deployments..."", ""Starting proxy with name..."" missing. The second run on the bottom shows the fix revealed the missing logs :) ",would difference example posted would empty message instead yes first part without change see finished starting proxy name missing second run bottom fix revealed missing,issue,negative,negative,neutral,neutral,negative,negative
1912353840,"@GeneDer so with these changes, what would be the difference in the example screenshot you posted? Would there just be empty `message` fields instead?",would difference example posted would empty message instead,issue,negative,negative,neutral,neutral,negative,negative
1912344150,Other cherry pick that'll fix the test is now open: https://github.com/ray-project/ray/pull/42740,cherry pick fix test open,issue,negative,neutral,neutral,neutral,neutral,neutral
1912124847,"@rkooo567 

> What's the reason why cgroup2 doesn't have the issue?

cgroup2 should have similar issue, I haven't checked cgroup2 part code, it might need another PR to fix. Our databricks runtime uses cgroup1.

> From the psutil implementation, it doesn't really subtract the shmem

For the reason I substract ""shmem"", you can test 

```
  //  - [/dev/shm test]
  //    If we use dd command to write a large file to /dev/shm,
  //    and no swapping occurs (you can use `free -h` to check whether swap size increases),
  //    after dd completes, the calculated used-memory value should be nearly
  //    previous_in_used_memory_bytes + bytes_of_written_file
```

""shmem"" means the in-memory size used by ""/dev/shm"", i.e., once **free memory** is insufficient, ""OS_managed_cache_and_buffer"" part will be throw away, but ""shmem"" part will be kept in memory or swapped it to swap space, this is the major difference. So ""shmem"" part should be counted as ""in-used"".

You can copy a large file into /dev/shm, and use `free -h` command to check before vs. after changes of the each part of memory, `shmem` is the part of ""shared"" in `free` command output

You can also test:

```
  //  - [Host OS SIGKILL signal test]:
  //    1. get current ""used_memory"" by running this `GetCGroupV1MemoryUsedBytes` function.
  //    2. get ""swap_space_size"" by running `free` command
  //    3. read ""used_swap_size"" value by reading ""total_swap"" item from /sys/fs/cgroup/memory/memory.stat
  //    4. Create a program that gradually requests to allocate memory,
  //       record that after the number ""oom_size"" memory it get allocated,
  //       the process is killed by OS SIGKILL signal.
  //    The ""oom_size"" recorded in step-(4) should approximately satisfy the following formula:
  //    oom_size ~== (total_memory + swap_space_size) - used_memory - used_swap_size
```

Assuming you put large file in /dev/shm, then you will find the ""oom_size"" becomes much smaller, i.e. the difference part is occupied by file data in /dev/shm,
but no matter how large ""OS_managed_cache_and_buffer"" is, it does not affect the ""oom_size"" in this test, because ""OS_managed_cache_and_buffer"" part can be directly thrown away when memory is insufficient.

**also note that Ray uses /dev/shm as the object-store-memory, so this part is important to Ray workloads.**
",reason issue similar issue checked part code might need another fix implementation really subtract reason substract test test use command write large file swapping use free check whether swap size calculated value nearly size used free memory insufficient part throw away part kept memory swap space major difference part copy large file use free command check part memory part free command output also test host o signal test get current running function get running free command read value reading item create program gradually allocate memory record number memory get process o signal approximately satisfy following formula assuming put large file find becomes much smaller difference part file data matter large affect test part directly thrown away memory insufficient also note ray part important ray,issue,positive,positive,positive,positive,positive,positive
1912111944,"same error, see it below
```
RAY_ADDRESS=""http://10.68.112.234:8265"" ray job submit --working-dir deepspeed_distributed_inference/ -- python script.py
Traceback (most recent call last):
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 715, in urlopen
    httplib_response = self._make_request(
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 467, in _make_request
    six.raise_from(e, None)
  File ""<string>"", line 3, in raise_from
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 462, in _make_request
    httplib_response = conn.getresponse()
  File ""/opt/conda/envs/ray/lib/python3.10/http/client.py"", line 1368, in getresponse
    response.begin()
  File ""/opt/conda/envs/ray/lib/python3.10/http/client.py"", line 317, in begin
    version, status, reason = self._read_status()
  File ""/opt/conda/envs/ray/lib/python3.10/http/client.py"", line 286, in _read_status
    raise RemoteDisconnected(""Remote end closed connection without""
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/requests/adapters.py"", line 486, in send
    resp = conn.urlopen(
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 799, in urlopen
    retries = retries.increment(
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/urllib3/util/retry.py"", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/urllib3/packages/six.py"", line 769, in reraise
    raise value.with_traceback(tb)
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 715, in urlopen
    httplib_response = self._make_request(
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 467, in _make_request
    six.raise_from(e, None)
  File ""<string>"", line 3, in raise_from
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 462, in _make_request
    httplib_response = conn.getresponse()
  File ""/opt/conda/envs/ray/lib/python3.10/http/client.py"", line 1368, in getresponse
    response.begin()
  File ""/opt/conda/envs/ray/lib/python3.10/http/client.py"", line 317, in begin
    version, status, reason = self._read_status()
  File ""/opt/conda/envs/ray/lib/python3.10/http/client.py"", line 286, in _read_status
    raise RemoteDisconnected(""Remote end closed connection without""
urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/ray/dashboard/modules/dashboard_sdk.py"", line 262, in _check_connection_and_version_with_url
    r = self._do_request(""GET"", url)
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/ray/dashboard/modules/dashboard_sdk.py"", line 303, in _do_request
    return requests.request(
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/requests/api.py"", line 59, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/requests/sessions.py"", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/requests/sessions.py"", line 703, in send
    r = adapter.send(request, **kwargs)
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/requests/adapters.py"", line 501, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/envs/ray/bin/ray"", line 8, in <module>
    sys.exit(main())
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/ray/scripts/scripts.py"", line 2474, in main
    return cli()
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/click/core.py"", line 1157, in __call__
    return self.main(*args, **kwargs)
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/click/core.py"", line 1078, in main
    rv = self.invoke(ctx)
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/click/core.py"", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/click/core.py"", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/click/core.py"", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/click/core.py"", line 783, in invoke
    return __callback(*args, **kwargs)
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/ray/dashboard/modules/job/cli_utils.py"", line 44, in wrapper
    return func(*args, **kwargs)
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/ray/autoscaler/_private/cli_logger.py"", line 856, in wrapper
    return f(*args, **kwargs)
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/ray/dashboard/modules/job/cli.py"", line 221, in submit
    client = _get_sdk_client(address, create_cluster_if_needed=True, verify=verify)
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/ray/dashboard/modules/job/cli.py"", line 25, in _get_sdk_client
    client = JobSubmissionClient(address, create_cluster_if_needed, verify=verify)
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/ray/dashboard/modules/job/sdk.py"", line 110, in __init__
    self._check_connection_and_version(
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/ray/dashboard/modules/dashboard_sdk.py"", line 248, in _check_connection_and_version
    self._check_connection_and_version_with_url(min_version, version_error_message)
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/ray/dashboard/modules/dashboard_sdk.py"", line 278, in _check_connection_and_version_with_url
    raise ConnectionError(
ConnectionError: Failed to connect to Ray at address: http://10.68.112.234:8265.
```",error see ray job submit python recent call last file line file line none file string line file line file line file line begin version status reason file line raise remote end closed connection without remote end closed connection without response handling exception another exception recent call last file line send resp file line file line increment raise type error error file line reraise raise file line file line none file string line file line file line file line begin version status reason file line raise remote end closed connection without aborted end closed connection without response handling exception another exception recent call last file line get file line return file line request return file line request resp prep file line send request file line send raise err aborted end closed connection without response handling exception another exception recent call last file line module main file line main return file line return file line main file line invoke return file line invoke return file line invoke return file line invoke return file line wrapper return file line wrapper return file line submit client address file line client address file line file line file line raise connect ray address,issue,negative,negative,neutral,neutral,negative,negative
1911820722,"This issue seems to be quite old but still very relevant. I found a workaround and I thought it might help others. 
```python
array_np = np.zeros((23,40))
obj_ref = ray.put(array_np)
array_np = ray.get(obj_ref)
array_torch = torch.from_numpy(array_np)
```
**Explanation**
Ray already supports zero-copy reads for numpy arrays. Also according to [pytorch documentation](https://pytorch.org/docs/stable/generated/torch.from_numpy.html), when using `torch.from_numpy`, the newly created tensor shares the same memory with the numpy tensor. This means that it will use the same buffer already allocated by the object store instead of creating a new one.
**Notes**
- When running this code, Pytorch will complain about the fact that the numpy array is read only but it seems to still work fine. We can make the numpy array writable using this flag `array_np.flags.writeable = True`. However, this will break the immutability assumption about the objects in the Ray object store
- One can test that the pytorch tensors are using the zero copy memory by using the following code
```python
array_np, array_np_2= ray.get([obj_ref]*2)
array_torch, array_torch_2 = torch.from_numpy(array_np), torch.from_numpy(array_np_2)
array_torch.data_ptr() == array_torch_2.data_ptr() 
```
- There is an [old merged PR](https://github.com/ray-project/ray/pull/12344/files) that seems to be solving the same problem but it seems that the code introduced there was overwritten. I manually ran the unit test added in this PR and it failed",issue quite old still relevant found thought might help python explanation ray already also according documentation newly tensor memory tensor use buffer already object store instead new one running code complain fact array read still work fine make array writable flag true however break immutability assumption ray object store one test zero copy memory following code python old problem code manually ran unit test added,issue,negative,positive,positive,positive,positive,positive
1911793353,"@aslonnie  I have added unit tests for both the functions in `ray_release.test` and also the function that queries existing flaky tests in `filter_tests.py` script, which I think is the most essential in that script. 
Should I test that script's ability to read and write file as well? It's more complicated to test the whole flow including these steps since it's on main and the test script has to use `subprocess` to execute the whole flow, which is not easy to mock I think (one function ends up using boto3 which we need mock because there's no credential). I also don't think it's worth to break those steps down to smaller helper functions (which is easier to write unit tests for) since the logic is on the simpler side.",added unit also function flaky script think essential script test script ability read write file well complicated test whole flow since main test script use execute whole flow easy mock think one function need mock credential also think worth break smaller helper easier write unit since logic simpler side,issue,positive,positive,neutral,neutral,positive,positive
1911713769,"The only thing I noticed that is weird is e.g. in the head start command (where it says YOU DO NOT NEED TO CHANGE THIS), an option:

`--autoscaling-config=~/ray_bootstrap_config.yaml`

It looks like it is assuming there is a file named ""ray_bootstrap_config.yaml"" in my home directory? This is super weird, why would that file exist and what would it contain? Is this supposed to be changed to point to the autostart yaml like example-full.yaml?",thing weird head start command need change option like assuming file home directory super weird would file exist would contain supposed point like,issue,positive,negative,negative,negative,negative,negative
1911703120,"I have this issue as well. I get the same issue where it hangs while initializing the workers. Even the very first setup commands never get run on the workers. I have tried between ray 2.6.0 (didn't try earlier) and the nightly, including the current PIP version. 

I have not changed a single thing, I simply downloaded the example-full.yaml file from https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/local/example-full.yaml

I simply added the head and worker IPs. There is no firewall running. Ubuntu 22.04.

The same thing happens if I delete the docker section.

Starting the cluster manually works (i.e. on head running ""ray start and port""  and on workers running ""ray start"" with head IP).",issue well get issue even first setup never get run tried ray try nightly current pip version single thing simply file simply added head worker running thing delete docker section starting cluster manually work head running ray start port running ray start head,issue,negative,positive,neutral,neutral,positive,positive
1911589945,"> disable task event report per task

Yeah，if we support disable task event report per task, both the two issues can be fixed.",disable task event report per task support disable task event report per task two fixed,issue,negative,positive,neutral,neutral,positive,positive
1911581697,I am having difficulty untangling redis and gcs table storage. See #40217.,difficulty untangling table storage see,issue,negative,neutral,neutral,neutral,neutral,neutral
1911578859,"I am trying to untangle this. I have excluded redis_client, but that is not enough. I need to exclude the actual redis sources [from `gcs`](https://github.com/ray-project/ray/blob/d9bd752766a55b4f72ad6a7fab53a948d3cef639/BUILD.bazel#L2264):
```
diff --git a/BUILD.bazel b/BUILD.bazel
index c1453b0037..f575eac8d0 100644
--- a/BUILD.bazel
+++ b/BUILD.bazel
@@ -2268,13 +2285,24 @@ ray_cc_library(
         ],
         exclude = [
             ""src/ray/gcs/*_test.cc"",
+            # These are conditionally included in ray_client
+            ""src/ray/gcs/asio.cc"",
+            ""src/ray/gcs/redis_async_context.cc"",
+            ""src/ray/gcs/redis_client.cc"",
+            ""src/ray/gcs/redis_context.cc"",
         ],
     ),
     hdrs = glob([
-        ""src/ray/gcs/*.h"",
-    ]),
+            ""src/ray/gcs/*.h"",
+        ],
+        exclude = [
+            ""src/ray/gcs/asio.h"",
+            ""src/ray/gcs/redis_async_context.h"",
+            ""src/ray/gcs/redis_client.h"",
+            ""src/ray/gcs/redis_context.h"",
+        ],
+    ),
     deps = [
-        "":hiredis"",
         "":node_manager_fbs"",
         "":node_manager_rpc"",
         "":ray_common"",
@@ -2284,7 +2312,10 @@ ray_cc_library(
         ""//src/ray/protobuf:gcs_service_cc_proto"",
         ""//src/ray/util"",
         ""@boost//:asio"",
-    ],
+    ] + select({
+        ""@bazel_tools//src/conditions:windows"": [],
+        ""//conditions:default"": ["":hiredis"", "":redis_client""],
+    }),
 )

 ray_cc_test(
```

but then various parts of the gcs table storage do not work. Is there someone who understands the interactions between redis and the gcs table storage who can give me some guidance?",trying untangle enough need exclude actual git index exclude conditionally included exclude select default various table storage work someone table storage give guidance,issue,negative,neutral,neutral,neutral,neutral,neutral
1911560735,"Reproduction script
```python
import ray
import datasets
import ray.data
from ray.data import Dataset
from transformers import AutoTokenizer

ray.init()

tokenizer = AutoTokenizer.from_pretrained(""THUDM/chatglm3-6b"", trust_remote_code=True)

data = ray.data.read_csv(""test.csv"")

def test_ray(data: Dataset) -> Dataset:
    t = tokenizer.encode(""test"")
    print(tokenizer.decode(t))
    return data

data = data.map_batches(test_ray)
print(data.take(1))
```",reproduction script python import ray import import import import data data test print return data data print,issue,negative,neutral,neutral,neutral,neutral,neutral
1911551890,"@raulchen , @scottjlee let me know if this will fit your need or not, thankkks",let know fit need,issue,negative,positive,positive,positive,positive,positive
1911535325,@MissiontoMars so it is that issue + we also allow to disable task event report per task right? ,issue also allow disable task event report per task right,issue,negative,positive,positive,positive,positive,positive
1911533492,"@rickyyx can you run the microbenchmark to see if there's any perf change? Also, is it possible you make a small benchmark on your own for the serialization performance? (between older impl vs new upstream changes)

",run see change also possible make small serialization performance older new upstream,issue,negative,positive,neutral,neutral,positive,positive
1911532631,"Hmm normally, for a single node case, the IP should not be printed like `ip=192.168.0.222`. So, I feel like there's something sketchy with the setup here.

Would you like to have a short chat in person for this? I'd love to pair debug this case. ",normally single node case printed like feel like something sketchy setup would like short chat person love pair case,issue,positive,positive,positive,positive,positive,positive
1911517960,I was just running the script on my computer so there wasn't any deployment with multiple machines. Yes all packages are installed.,running script computer deployment multiple yes,issue,negative,neutral,neutral,neutral,neutral,neutral
1911501226,"Hi, thanks for the clarification, if this is the way how this work I still see one major issue there - the metric propagates into the overview of the whole tuning run. Therefore I can see the best run, but I don't have a clue how good it was (if it is outperforming my previous results). The other issue, that I have with this is that I really need to dig deeper into the `Result` object to see how my model has performed. 

The attached image depicts the same training run I mentioned previously. The best value of the metric I optimise for is ~0.78, but if I check the progress so far I can see that the best performing trial has `coco/bbox_mAP_50 = 0.68`. So the question is: Is there an option how to set the UI to show me the metric with `scope=""all""`. Even though I was looking for this option, I was not able to find it anywhere ...
![Uploading Screenshot 2024-01-26 at 6.13.05.png…]()
",hi thanks clarification way work still see one major issue metric overview whole tuning run therefore see best run clue good previous issue really need dig result object see model attached image training run previously best value metric check progress far see best trial question option set show metric even though looking option able find anywhere,issue,positive,positive,positive,positive,positive,positive
1911463186,"
@bveeramani  I cannot share the data I apologize. I believe the issue is that the chunking overlap caused the data to be multiplied in size, then it would try to load everything into memory before inserting it into the vector DB; I believe this because I was using ds.iter... for which the queue would become increasingly large. 

My question is, how do I go about correctly embedding everything without loading it all into memory. I am seeing that by using ds.iter_rows it doesnt stream but rather loads everything into mem, is this correct? 

With that thinking I pivoted towards using map_batches to send it into Milvus, however I am unsure if that is the correct way to go. Anyscale provides an example of something [similar to what I did](https://www.anyscale.com/blog/rag-at-scale-10x-cheaper-embedding-computations-with-anyscale-and-pinecone), however at the end they call .materialize(), which loads everything into the object store memory. Why is that?

Is the correct route to follow the anyscale example and not call .materialize()? I would like to be able to process 100s of GBs of data, but loading that into memory would be near impossible.

",share data apologize believe issue overlap data size would try load everything memory vector believe queue would become increasingly large question go correctly everything without loading memory seeing doesnt stream rather everything mem correct thinking towards send however unsure correct way go example something similar however end call everything object store memory correct route follow example call would like able process data loading memory would near impossible,issue,negative,positive,neutral,neutral,positive,positive
1911447060,@suquark could you verify this way of syncing from upstream works? or if there's anything shouldn't be overwritten?,could verify way upstream work anything,issue,negative,neutral,neutral,neutral,neutral,neutral
1911418305,Nice job deleting the cluster environment file \o/,nice job cluster environment file,issue,negative,positive,positive,positive,positive,positive
1911331218,"> > @rickyyx My understanding of the role of RAY_task_events_report_interval_ms is to turn off the current worker's task event reporter. This may not be accurate, for example:
> > The Driver calls MyActor.ping.remote(), and sets RAY_task_events_report_interval_ms=0 for MyActor, but the reporting of MyActor.ping task events is on the Driver side, so it can't take effect? https://github.com/ray-project/ray/blob/master/src/ray/core_worker/task_manager.cc#L1440
> > If we set RAY_task_events_report_interval_ms=0 for the Driver, then the task events of other actors will also be ignored.So I think we need more granular control options, such as specifying no event reporting for specific tasks, including task status events and task profile events.
> 
> I see - yeah, good catch. You are right here. The workaround will not disable all task events from the driver side.

@rkooo567 Oh, it's not duplicate with https://github.com/ray-project/ray/issues/42076. Can you please take a look at the previous conversation with @rickyy?",understanding role turn current worker task event reporter may accurate example driver task driver side ca take effect set driver task also think need granular control event specific task status task profile see yeah good catch right disable task driver side oh duplicate please take look previous conversation,issue,positive,positive,positive,positive,positive,positive
1911308215,"Ah I think I understand.

When you are calling `ResultGrid.get_best_result`, that particular `Result` is chosen because the value of `0.778` is the highest.

When you are printing the `Result`, the [metrics](https://docs.ray.io/en/latest/train/api/doc/ray.train.Result.html#ray.train.Result.metrics) attribute is the _latest_ set of reported metrics, which is`0.66`.

Here's a quick repro which you can play around with to see the same behavior:

```python
from ray import train, tune
from ray.tune import Tuner

def f(config):
    a = config[""a""]
    if a == 1:
        metrics = [0, 5, 1]
    else:
        metrics = [2, 3, 4]

    for m in metrics:
        train.report({""metric"": m})

tuner = Tuner(f, param_space={""a"": tune.grid_search([1, 2])})
result_grid = tuner.fit()

best_result = result_grid.get_best_result(metric=""metric"", mode=""max"", scope=""all"")
print(best_result)
```

This will choose the trial with input `{""a"": 1}` because 5 is the largest value reported, though the most recent reported value is 1.",ah think understand calling particular result chosen value highest printing result metric attribute set metric quick play around see behavior python ray import train tune import tuner metric else metric metric metric tuner tuner metric print choose trial input value though recent value,issue,positive,positive,positive,positive,positive,positive
1911304571,"> looks pretty reasonable to me :)

Thanks for the prompt feedback!",pretty reasonable thanks prompt feedback,issue,positive,positive,positive,positive,positive,positive
1911300259,"i beleive this is fixed since we also clean up after worker died, and no longer using tasks hierarchy as we were doing it. THere's also tests covered this: 

https://github.com/ray-project/ray/blob/583e6fb1facaaae742d083c14e3375968c641403/python/ray/tests/test_task_events_2.py#L222-L293",fixed since also clean worker longer hierarchy also covered,issue,negative,positive,positive,positive,positive,positive
1911274732,"Oh, can you try port `8265`? Port `10001` is used for Ray Client, while `8265` is used for the Ray Jobs API",oh try port port used ray client used ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1911270492,Hmm sorry but I don't quite understand the deadlock situation in the PR description and the proposed fix. Doesn't SplitCoordinator explicitly require all the consumers to read at the same time? Is the deadlock situation in the PR description somehow different?,sorry quite understand deadlock situation description fix explicitly require read time deadlock situation description somehow different,issue,negative,negative,negative,negative,negative,negative
1911262392,"the code is so different from what it was already, I basically rewrite the required changes.",code different already basically rewrite,issue,negative,neutral,neutral,neutral,neutral,neutral
1911261753,i am not sure of a repro- i saw this on a long running endpoint cluster. ,sure saw long running cluster,issue,negative,positive,positive,positive,positive,positive
1911254731,"different error after I put http

```
`
RAY_ADDRESS=http://$RAY_HEAD_SERVICE_HOST"":""$RAY_HEAD_SERVICE_PORT ray job submit --working-dir deepspeed_distributed_inference/ -- python script.py

Traceback (most recent call last):
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 715, in urlopen
    httplib_response = self._make_request(
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 467, in _make_request
    six.raise_from(e, None)
  File ""<string>"", line 3, in raise_from
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 462, in _make_request
    httplib_response = conn.getresponse()
  File ""/opt/conda/envs/ray/lib/python3.10/http/client.py"", line 1368, in getresponse
    response.begin()
  File ""/opt/conda/envs/ray/lib/python3.10/http/client.py"", line 317, in begin
    version, status, reason = self._read_status()
  File ""/opt/conda/envs/ray/lib/python3.10/http/client.py"", line 299, in _read_status
    raise BadStatusLine(line)
http.client.BadStatusLine: ÿÿÿ@@@?

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/requests/adapters.py"", line 486, in send
    resp = conn.urlopen(
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 799, in urlopen
    retries = retries.increment(
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/urllib3/util/retry.py"", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/urllib3/packages/six.py"", line 769, in reraise
    raise value.with_traceback(tb)
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 715, in urlopen
    httplib_response = self._make_request(
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 467, in _make_request
    six.raise_from(e, None)
  File ""<string>"", line 3, in raise_from
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 462, in _make_request
    httplib_response = conn.getresponse()
  File ""/opt/conda/envs/ray/lib/python3.10/http/client.py"", line 1368, in getresponse
    response.begin()
  File ""/opt/conda/envs/ray/lib/python3.10/http/client.py"", line 317, in begin
    version, status, reason = self._read_status()
  File ""/opt/conda/envs/ray/lib/python3.10/http/client.py"", line 299, in _read_status
    raise BadStatusLine(line)
urllib3.exceptions.ProtocolError: ('Connection aborted.', BadStatusLine('\x00\x00\x1e\x04\x00\x00\x00\x00\x00\x00\x03\x7fÿÿÿ\x00\x04\x00@\x00\x00\x00\x05\x00@\x00\x00\x00\x06\x00\x00@\x00þ\x03\x00\x00\x00\x01\x00\x00\x04\x08\x00\x00\x00\x00\x00\x00?\x00\x01'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/ray/dashboard/modules/dashboard_sdk.py"", line 262, in _check_connection_and_version_with_url
    r = self._do_request(""GET"", url)
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/ray/dashboard/modules/dashboard_sdk.py"", line 303, in _do_request
    return requests.request(
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/requests/api.py"", line 59, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/requests/sessions.py"", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/requests/sessions.py"", line 703, in send
    r = adapter.send(request, **kwargs)
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/requests/adapters.py"", line 501, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', BadStatusLine('\x00\x00\x1e\x04\x00\x00\x00\x00\x00\x00\x03\x7fÿÿÿ\x00\x04\x00@\x00\x00\x00\x05\x00@\x00\x00\x00\x06\x00\x00@\x00þ\x03\x00\x00\x00\x01\x00\x00\x04\x08\x00\x00\x00\x00\x00\x00?\x00\x01'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/envs/ray/bin/ray"", line 8, in <module>
    sys.exit(main())
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/ray/scripts/scripts.py"", line 2474, in main
    return cli()
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/click/core.py"", line 1157, in __call__
    return self.main(*args, **kwargs)
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/click/core.py"", line 1078, in main
    rv = self.invoke(ctx)
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/click/core.py"", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/click/core.py"", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/click/core.py"", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/click/core.py"", line 783, in invoke
    return __callback(*args, **kwargs)
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/ray/dashboard/modules/job/cli_utils.py"", line 44, in wrapper
    return func(*args, **kwargs)
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/ray/autoscaler/_private/cli_logger.py"", line 856, in wrapper
    return f(*args, **kwargs)
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/ray/dashboard/modules/job/cli.py"", line 221, in submit
    client = _get_sdk_client(address, create_cluster_if_needed=True, verify=verify)
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/ray/dashboard/modules/job/cli.py"", line 25, in _get_sdk_client
    client = JobSubmissionClient(address, create_cluster_if_needed, verify=verify)
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/ray/dashboard/modules/job/sdk.py"", line 110, in __init__
    self._check_connection_and_version(
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/ray/dashboard/modules/dashboard_sdk.py"", line 248, in _check_connection_and_version
    self._check_connection_and_version_with_url(min_version, version_error_message)
  File ""/opt/conda/envs/ray/lib/python3.10/site-packages/ray/dashboard/modules/dashboard_sdk.py"", line 278, in _check_connection_and_version_with_url
    raise ConnectionError(
ConnectionError: Failed to connect to Ray at address: http://ray-65b2f4c4cc4ca35014390915-ray-client.compute.svc.cluster.local:10001
`
```",different error put ray job submit python recent call last file line file line none file string line file line file line file line begin version status reason file line raise line handling exception another exception recent call last file line send resp file line file line increment raise type error error file line reraise raise file line file line none file string line file line file line file line begin version status reason file line raise line aborted handling exception another exception recent call last file line get file line return file line request return file line request resp prep file line send request file line send raise err aborted handling exception another exception recent call last file line module main file line main return file line return file line main file line invoke return file line invoke return file line invoke return file line invoke return file line wrapper return file line wrapper return file line submit client address file line client address file line file line file line raise connect ray address,issue,negative,positive,neutral,neutral,positive,positive
1911230482,Please help me merge as well @architkulkarni ,please help merge well,issue,positive,neutral,neutral,neutral,neutral,neutral
1911212332,"@can-anyscale when do you think you'll be able to attempt an image build of both ray and ray-ml and provide a list of breaking dependencies that we can then assign to the relevant Ray team to resolve?
",think able attempt image build ray provide list breaking assign relevant ray team resolve,issue,negative,positive,positive,positive,positive,positive
1911206486,cc @c21 this may be related to issue you observe. We will take a look at it shortly,may related issue observe take look shortly,issue,negative,neutral,neutral,neutral,neutral,neutral
1911199362,I am running this to see if I can reproduce the issue. ,running see reproduce issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1911194407,"We are planning to fix it by ray 2.10! Potentially, this issue should be handled together if possible https://github.com/ray-project/ray/issues/34124",fix ray potentially issue handled together possible,issue,negative,neutral,neutral,neutral,neutral,neutral
1911190693,"@raulchen last time, you mentioned this caused the memory leak from data. Does it block ray data GA? ",last time memory leak data block ray data ga,issue,negative,neutral,neutral,neutral,neutral,neutral
1911184835,@mattip do you know what's the ETA for the PR for this one? (The redis upgrade break windows)? ,know eta one upgrade break,issue,negative,neutral,neutral,neutral,neutral,neutral
1911184432,Maybe @jjyao already fixed the issue. He's going to verify it. ,maybe already fixed issue going verify,issue,negative,positive,neutral,neutral,positive,positive
1911173651,lint is passing at least. force merging to unbreak master.,lint passing least force master,issue,negative,negative,negative,negative,negative,negative
1911171057,"Finding root cause for this issue is P0 for 2.10. Depending on the scope, we can get everything we need in by 2.10 or 2.11",finding root cause issue depending scope get everything need,issue,negative,neutral,neutral,neutral,neutral,neutral
1911164739,Current failures are caused by a merge conflict on @zcin's PRs. Will be addressed shortly: https://github.com/ray-project/ray/pull/42718,current merge conflict shortly,issue,negative,neutral,neutral,neutral,neutral,neutral
1911102105,"merge the latest master doesn't help, still having irrelevant failure.

merge again.",merge latest master help still irrelevant failure merge,issue,negative,negative,negative,negative,negative,negative
1911058313,"I took a look to the code.

- in a Ray worker it checks Python version and Ray version. 
  - python/ray/_private/utils.py
  - `check_version_info`, called in `connect()`
- in a Ray client, it checks Python version and `CURRENT_PROTOCOL_VERSION`, which is set to ""2023-06-27"" rn.
  - python/ray/util/client/__init__.py
  - `_ClientContext._check_versions`

I am not sure where this discrepancy comes from.",took look code ray worker python version ray version connect ray client python version set sure discrepancy come,issue,negative,positive,positive,positive,positive,positive
1911027322,Hi @geoffreylgv. I tested out the fix and it 404's for me. Does it work for you?,hi tested fix work,issue,negative,neutral,neutral,neutral,neutral,neutral
1911023409,"Specifically, this means that setting: `num_replicas=""auto""` should be sufficient for basic POCs.",specifically setting auto sufficient basic,issue,negative,neutral,neutral,neutral,neutral,neutral
1911015964,Hopefully we can unpin by 2.10 (if new fastapi version is out by then),hopefully unpin new version,issue,negative,positive,positive,positive,positive,positive
1911011892,"> Ok, let's wait for Sihan's cherry pick fix then. Better to avoid force-merging when possible.

Agreed!",let wait cherry pick fix better avoid possible agreed,issue,negative,positive,positive,positive,positive,positive
1911001451,"Team, this is solved. Even if you don't have `await` on the batch side, you should have `async` there.",team even await batch side,issue,negative,neutral,neutral,neutral,neutral,neutral
1910977795,"Ok, let's wait for Sihan's cherry pick fix then.  Better to avoid force-merging when possible.",let wait cherry pick fix better avoid possible,issue,negative,positive,positive,positive,positive,positive
1910972968,"@architkulkarni That's actually related to starlette issue. We can cherry pick this PR to unblock from merging https://github.com/ray-project/ray/pull/42417 or wait for Sihan's cherry-pick fixes
",actually related issue cherry pick unblock wait,issue,negative,neutral,neutral,neutral,neutral,neutral
1910969726,"@edoakes serve-minimal failed again, can you see if it's related or if it's safe to ignore?

",see related safe ignore,issue,negative,positive,positive,positive,positive,positive
1910955846,"Hi @matthewdeng , 
first of all thank you for reaching back to me! Sorry for not as clear description. The issue is that my optimization criterion that I wanted to **maximize** was `coco/bbox_mAP_50`. If I select the best result you can se there is the value for this metric of 0.66, but if you check the history the value was as high as 0.778. 

This issue affects the best trial selection, since the value based on which we select the best trial is different from the best value achieved in the trial. 

I hope this explanation helps, if not please feel free to reach back to me and I will provide more in-depth explanation ",hi first thank reaching back sorry clear description issue optimization criterion maximize select best result se value metric check history value high issue best trial selection since value based select best trial different best value trial hope explanation please feel free reach back provide explanation,issue,positive,positive,positive,positive,positive,positive
1910947621,it needs ray docs owner approvals; adding @angelinalg (and she also has merge right).,need ray owner also merge right,issue,negative,positive,positive,positive,positive,positive
1910945998,"Other PR is merged, closing this one. Thanks a lot for the fix & making my life easy @jrosti :) sorry the tests were pretty confusing here...",one thanks lot fix making life easy sorry pretty,issue,positive,positive,neutral,neutral,positive,positive
1910943716,"The issue reported earlier does not occur anymore in ray 2.9.1. Tested with Torch 2.0.1 and the examples/action_masking file in the following version: https://github.com/ray-project/ray/blob/84d17cad835665631c2f68f6fb332e2973028fab/rllib/examples/action_masking.py

",issue occur ray tested torch file following version,issue,negative,neutral,neutral,neutral,neutral,neutral
1910935980,"> Please update the PR description to have the full context.
> 
> What is the testing plan?

manually tested with 0.108.0 fastapi version. and Update the test-requirements.txt",please update description full context testing plan manually tested version update,issue,negative,positive,positive,positive,positive,positive
1910898331,"Contents of PR LGTM - I triggered a postmerge test to see how things look on windows tests.
https://buildkite.com/ray-project/postmerge/builds/2689",content triggered test see look,issue,negative,neutral,neutral,neutral,neutral,neutral
1910873647,"serve minimal test failed in premerge: https://buildkite.com/ray-project/premerge/builds/17432#018d4208-b02c-4fa4-80ea-a8e50efdac0c

But I think I've seen a similar error before and it was unrelated.  Restarting to see what happens",serve minimal test think seen similar error unrelated see,issue,negative,negative,neutral,neutral,negative,negative
1910831436,"@kuntiik can you elaborate what you are expecting the behavior to be? `get_best_result` should depend on the metrics from the other `Result`s in the `ResultGrid, so it's not clear to me from the screenshot what the issue is.",elaborate behavior depend metric result clear issue,issue,negative,positive,positive,positive,positive,positive
1910817841,"Adding a trailing slash to the version address does fail on my laptop, but I'm not sure if it's related:

```
% curl http://localhost:52365/api/ray/version/
404: Not Found
```",trailing slash version address fail sure related curl found,issue,negative,neutral,neutral,neutral,neutral,neutral
1910811402,"Thanks for posting @millermuttu. I tried to reproduce the issue by installing Ray 2.7.1 with Python 3.9 on a fresh conda environment on my laptop. All the following commands worked successfully for me:

```
ray start --head
serve deploy config.yaml
serve deploy config.yaml -a http://localhost:52365
serve deploy config.yaml -a http://localhost:52365/
serve status
serve status -a http://localhost:52365
serve status -a http://localhost:52365/
```

What do you get when you run `curl http://localhost:52365/api/ray/version` inside your Kubernetes cluster? For reference, on my laptop I get:

```
% curl http://localhost:52365/api/ray/version
{""version"": ""4"", ""ray_version"": ""2.7.1"", ""ray_commit"": ""9f07c12615958c3af3760604f6dcacc4b3758a47""}
```

Could you also run `echo $RAY_AGENT_ADDRESS` and post the value? For reference, when I set the `RAY_AGENT_ADDRESS` on my laptop to either `http://localhost:52365/` or `http://localhost:52365`, `serve status` still succeeds. As a sanity check, I tried setting it to the dashboard head address (`http://localhost:8265`), and as expected, `serve status` fails since the env var is expecting the agent address.",thanks posting tried reproduce issue ray python fresh environment following worked successfully ray start head serve deploy serve deploy serve deploy serve status serve status serve status get run curl inside cluster reference get curl version could also run echo post value reference set either serve status still sanity check tried setting dashboard head address serve status since agent address,issue,positive,positive,positive,positive,positive,positive
1910810271,"I'm not familiar with terraform, and I don't think we currently have any Ray docs about terraform.  But you can use the Ray cluster launcher on AWS without terraform, let me know if you run into any issues in the docs!",familiar think currently ray use ray cluster launcher without let know run,issue,negative,positive,positive,positive,positive,positive
1910803127,@harrycoder28 how did you discover this; were you trying to run something or were you just browsing the source?,discover trying run something browsing source,issue,negative,neutral,neutral,neutral,neutral,neutral
1910695151,"> @zcin I assume you manually tested this on a cluster, right?

Yup, I performed manual tests.",assume manually tested cluster right manual,issue,negative,positive,positive,positive,positive,positive
1910592927,I removed the race condition fix from this PR and moved it to #42705.,removed race condition fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1910584635,@rkooo567 it's not. The serve release tests are not in good shape in general. @zcin is working on improving them in the coming months. Adding these should be part of it.,serve release good shape general working improving coming part,issue,positive,positive,positive,positive,positive,positive
1910568228,"@jrosti we want to get this fix into a patch release so I did some local testing to try to fix the test. It ended up being a bit of a rabbit hole. Opened a new PR with my changes to get the test to pass: https://github.com/ray-project/ray/pull/42704

Not 100% sure if it works yet, will let CI run.",want get fix patch release local testing try fix test ended bit rabbit hole new get test pas sure work yet let run,issue,negative,positive,positive,positive,positive,positive
1910442149,"i am using ray 2.7.1 and this issue still persist.

i am trying to execute serve deploy xyz.yaml inside the kubernetes cluster which uses ray 2.7.1 ray image only 

and still get RuntimeError: Serve CLI is not supported on the Ray cluster. Please ensure the cluster is running Ray 1.12 or higher.
this error. 

",ray issue still persist trying execute serve deploy inside cluster ray ray image still get serve ray cluster please ensure cluster running ray higher error,issue,negative,positive,positive,positive,positive,positive
1910419298,@alexeykudinkin I remember you contributed the benchmark. Is it running daily right now? ,remember running daily right,issue,negative,positive,positive,positive,positive,positive
1910414418,"Thanks @rkooo567! I made a first proposal [here](https://github.com/ray-project/ray/pull/42620/files) and tested it manually with a VictoriaMetrics backend. It worked out of the box. 

Do you have a suggestion for how to further test this change? I could image it would be useful to have an integration test that actually runs an instance of Prometheus, VictoriaMetrics, Thanos etc. and confirms they can be used as a backend and pass the response check. I'm new to the Ray codebase, is there a place where you do tests like this?",thanks made first proposal tested manually worked box suggestion test change could image would useful integration test actually instance used pas response check new ray place like,issue,positive,positive,positive,positive,positive,positive
1910398981,CI failure is likely unrelated. Updating branch to re-run,failure likely unrelated branch,issue,negative,negative,negative,negative,negative,negative
1910397676,"@zcin I assume you manually tested this on a cluster, right?",assume manually tested cluster right,issue,negative,positive,positive,positive,positive,positive
1910351299,I don't think the CI failure is related. It would be nice to get some eyes on this.,think failure related would nice get,issue,negative,positive,neutral,neutral,positive,positive
1910258150,"> ### 先搜索后询问
> * [x] 我搜索了[这些问题](https://github.com/ray-project/ray/issues)，没有发现类似的问题。
> 
> ### 射线组件
> RLlib 的
> 
> ### 发生了什么 + 你期望会发生什么
> 我用心训练了一个PPO模型，完整的配置是这样的：
> 
> ```
> {
>             ""env"": ""SimpleCryptoEnv"",  # ""CartPole-v0"", #
>             # ""env_config"": config_train,  # The dictionary we built before
>             ""log_level"": ""WARNING"",
>             ""framework"": ""torch"",
>             ""_fake_gpus"": False,
>             ""callbacks"": MyCallback,
>             ""ignore_worker_failures"": True,
>             ""num_workers"": 12,  # One worker per agent. You can increase this but it will run fewer parallel trainings.
>             ""num_envs_per_worker"": 1,
>             ""num_gpus"": 1,  # I yet have to understand if using a GPU is worth it, for our purposes, but I think it's not. This way you can train on a non-gpu enabled system.
>             ""clip_rewards"": True,
>             # ""lr"": 1e-4,  # Hyperparameter grid search defined above
>             # ""gamma"": 0.99,  # This can have a big impact on the result and needs to be properly tuned (range is 0 to 1)
>             # ""lambda"": 1.0,
>             ""observation_filter"": ""MeanStdFilter"",
>             ""model"": {
>                 ""fcnet_hiddens"": [256, 256],  # Hyperparameter grid search defined above
>                 ""use_attention"": True,
>                 ""attention_use_n_prev_actions"": 64,
>                 ""attention_use_n_prev_rewards"": 64,
>                 ""vf_share_layers"": True,
>             },
>             #""num_sgd_iter"": 10,  # tune.choice([10, 20, 30]),
>             ""sgd_minibatch_size"": 1024, # 128  # tune.choice([128, 512, 2048]),
>             ""train_batch_size"": 32768, # , # 1024 # tune.choice([10000, 20000, 40000]),
>             ""evaluation_interval"": 1,  # Run evaluation on every iteration
>             ""vf_clip_param"": 300000, 
>             ""evaluation_config"": {
>                 ""env_config"": config_eval,  # The dictionary we built before (only the overriding keys to use in evaluation)
>                 ""explore"": False,  # We don't want to explore during evaluation. All actions have to be repeatable.
>             },
>         }
> ```
> 
> 它训练正常，但是当我尝试使用它进行评估时，一切都崩溃了：
> 
> ```
> agent.compute_single_action(input_dict={""obs"":obs, ""state"":[0]*64, ""prev_action"": 0, ""prev_reward"": 0})
> ```
> 
> 我只尝试过观察（我认为这是正确的方法，开发人员必须发现代理如何想要以前的 - 不存在 - 状态是没有意义的;如果这在训练中自动处理，那么在评估中处理这个问题应该没有问题），但它也不是这样工作的。
> 
> 这是错误的完整跟踪。
> 
> ```
> ---------------------------------------------------------------------------
> AssertionError                            Traceback (most recent call last)
> ~\AppData\Local\Temp/ipykernel_348/535668899.py in <module>
> ----> 1 agent.compute_single_action(input_dict={""obs"":obs, ""state"":[0]*64, ""prev_action"": 0, ""prev_reward"": 0})
> 
> ~\anaconda3\envs\cryptorl\lib\site-packages\ray\rllib\agents\trainer.py in compute_single_action(self, observation, state, prev_action, prev_reward, info, input_dict, policy_id, full_fetch, explore, timestep, episode, unsquash_action, clip_action, unsquash_actions, clip_actions, **kwargs)
>    1483         if input_dict is not None:
>    1484             input_dict[SampleBatch.OBS] = observation
> -> 1485             action, state, extra = policy.compute_single_action(
>    1486                 input_dict=input_dict,
>    1487                 explore=explore,
> 
> ~\anaconda3\envs\cryptorl\lib\site-packages\ray\rllib\policy\policy.py in compute_single_action(self, obs, state, prev_action, prev_reward, info, input_dict, episode, explore, timestep, **kwargs)
>     216             episodes = [episode]
>     217 
> --> 218         out = self.compute_actions_from_input_dict(
>     219             input_dict=SampleBatch(input_dict),
>     220             episodes=episodes,
> 
> ~\anaconda3\envs\cryptorl\lib\site-packages\ray\rllib\policy\torch_policy.py in compute_actions_from_input_dict(self, input_dict, explore, timestep, **kwargs)
>     292                 if state_batches else None
>     293 
> --> 294             return self._compute_action_helper(input_dict, state_batches,
>     295                                                seq_lens, explore, timestep)
>     296 
> 
> ~\anaconda3\envs\cryptorl\lib\site-packages\ray\rllib\utils\threading.py in wrapper(self, *a, **k)
>      19         try:
>      20             with self._lock:
> ---> 21                 return func(self, *a, **k)
>      22         except AttributeError as e:
>      23             if ""has no attribute '_lock'"" in e.args[0]:
> 
> ~\anaconda3\envs\cryptorl\lib\site-packages\ray\rllib\policy\torch_policy.py in _compute_action_helper(self, input_dict, state_batches, seq_lens, explore, timestep)
>     932             else:
>     933                 dist_class = self.dist_class
> --> 934                 dist_inputs, state_out = self.model(input_dict, state_batches,
>     935                                                     seq_lens)
>     936 
> 
> ~\anaconda3\envs\cryptorl\lib\site-packages\ray\rllib\models\modelv2.py in __call__(self, input_dict, state, seq_lens)
>     241 
>     242         with self.context():
> --> 243             res = self.forward(restored, state or [], seq_lens)
>     244 
>     245         if isinstance(input_dict, SampleBatch):
> 
> ~\anaconda3\envs\cryptorl\lib\site-packages\ray\rllib\models\torch\attention_net.py in forward(self, input_dict, state, seq_lens)
>     345                 state: List[TensorType],
>     346                 seq_lens: TensorType) -> (TensorType, List[TensorType]):
> --> 347         assert seq_lens is not None
>     348         # Push obs through ""unwrapped"" net's `forward()` first.
>     349         wrapped_out, _ = self._wrapped_forward(input_dict, [], None)
> 
> AssertionError: 
> ```
> 
> ### 版本/依赖关系
> ray '2.0.0.dev0' python 3.8 窗户 10
> 
> ### 复制脚本
> ```
> import ray
> from ray.rllib.agents import ppo
> from ray.tune.registry import register_env
> from ray.rllib.agents.ppo import DEFAULT_CONFIG
> import gym
> config = DEFAULT_CONFIG.copy()
> config.update(
>     {
>             ""env"":  ""CartPole-v0"", #
>             # ""env_config"": config_train,  # The dictionary we built before
>             ""log_level"": ""WARNING"",
>             ""framework"": ""torch"",
>             ""_fake_gpus"": False,
>             ""callbacks"": MyCallback,
>             ""ignore_worker_failures"": True,
>             ""num_workers"": 12,  # One worker per agent. You can increase this but it will run fewer parallel trainings.
>             ""num_envs_per_worker"": 1,
>             ""num_gpus"": 1,  # I yet have to understand if using a GPU is worth it, for our purposes, but I think it's not. This way you can train on a non-gpu enabled system.
>             ""clip_rewards"": True,
>             # ""lr"": 1e-4,  # Hyperparameter grid search defined above
>             # ""gamma"": 0.99,  # This can have a big impact on the result and needs to be properly tuned (range is 0 to 1)
>             # ""lambda"": 1.0,
>             ""observation_filter"": ""MeanStdFilter"",
>             ""model"": {
>                 ""fcnet_hiddens"": [256, 256],  # Hyperparameter grid search defined above
>                 ""use_attention"": True,
>                 ""attention_use_n_prev_actions"": 64,
>                 ""attention_use_n_prev_rewards"": 64,
>                 ""vf_share_layers"": True,
>             },
>             #""num_sgd_iter"": 10,  # tune.choice([10, 20, 30]),
>             ""sgd_minibatch_size"": 1024, # 128  # tune.choice([128, 512, 2048]),
>             ""train_batch_size"": 32768, # , # 1024 # tune.choice([10000, 20000, 40000]),
>             ""evaluation_interval"": 1,  # Run evaluation on every iteration
>             ""vf_clip_param"": 300000, 
>         }
> )
> 
> ray.init(num_gpus=1)
> agent = ppo.PPOTrainer(config=config, env=""CartPole-v0"")
> env = gym.make(""CartPole-v0"")
> 
> episode_reward = 0
> done = False
> obs = env.reset()
> agent.compute_single_action(input_dict={""obs"":obs, ""state"":[0]*64, ""prev_action"": 0, ""prev_reward"": 0})
> # or
> agent.compute_single_action(obs)
> ```
> 
> ### 别的东西
> @deanwampler @ericl @richardliaw
> 
> ### 你愿意提交PR吗？
> * [ ] 是的，我愿意提交PR！

May I ask if you have resolved it? Thank you!",dictionary built warning framework torch false true one worker per agent increase run parallel yet understand worth think way train system true grid search defined gamma big impact result need properly tuned range lambda model grid search defined true true run evaluation every iteration dictionary built use evaluation explore false want explore evaluation repeatable state recent call last module state self observation state explore episode none observation action state extra self state episode explore episode self explore else none return explore wrapper self try return self except attribute self explore else self state state forward self state state list list assert none push unwrapped net forward first none ray dev python import ray import import import import gym dictionary built warning framework torch false true one worker per agent increase run parallel yet understand worth think way train system true grid search defined gamma big impact result need properly tuned range lambda model grid search defined true true run evaluation every iteration agent done false state may ask resolved thank,issue,positive,positive,neutral,neutral,positive,positive
1910235075,"> 嗯，对不起，SAC 目前不支持自动 LSTM 包装，但我们绝对应该在 SAC 的验证配置方法中放置一个错误。`use_lstm=True`

But now there is also this problem in PPO, how should we solve it?Thank you!",sac also problem solve thank,issue,negative,neutral,neutral,neutral,neutral,neutral
1910204991,"Could we also have a new learning test for the added pendulum tuned_example file in the rllib/BUILD file?
We probably shouldn't merge this before it's confirmed learning.",could also new learning test added pendulum file file probably merge confirmed learning,issue,negative,positive,positive,positive,positive,positive
1910042062,"> some one from @ray-project/ray-docs will review this. they are the code owners.
> 
> please do not ping me or @can-anyscale next time.

Got it, thanks",one review code please ping next time got thanks,issue,positive,positive,neutral,neutral,positive,positive
1910015676,"Hi @antoniomdk 

Good News i can able to login using oatuh2-proxy mechanism using localhost method. 
after login into ray dashboard its not showing who logged into dashboard using which account, usually all the webpage has information who logged in to UI page right? Please open the attachment for more information.

![image](https://github.com/ray-project/ray/assets/6921037/8fc4efbd-bd53-4a88-8d0e-d26512fca92d)
![image](https://github.com/ray-project/ray/assets/6921037/846685f6-6fab-47f2-bc21-83d666864093)
 ",hi good news able login mechanism method login ray dashboard showing logged dashboard account usually information logged page right please open attachment information image image,issue,positive,positive,positive,positive,positive,positive
1910011860,"> Hi @angelinalg, @can-anyscale, @ericl, @robertnishihara, @rkooo567, @krfricke Please, can you review this PR Thanks

@angelinalg, @rickyyx @architkulkarni  @aslonnie 

Any updates ?",hi please review thanks,issue,positive,positive,positive,positive,positive,positive
1909678486,@aslonnie  I just reformatted the code and moved the test query function over to `ray_release.test` as part of `Test` class. The new test suite is taking really long and failing to run on `premerge` (though it works locally) so I'll look into that. ,code test query function part test class new test suite taking really long failing run though work locally look,issue,negative,positive,neutral,neutral,positive,positive
1909652633,"> 对于 LSTM，您必须在 rllib 2.0 上传递 state 参数。`full_fetch=True`
> 
> ```
> state=[np.zeros(params['model_config']['lstm_cell_size'], np.float32),
>            np.zeros(params['model_config']['lstm_cell_size'], np.float32)] 
> action = algo_agent.compute_single_action(observation=obs[agt], policy_id=policy_id, explore=False, full_fetch=True) 
> ```
> 
> 对于LSTM模块来说，这对我来说很好用，但令人不安的是注意力模块的工作。如果我能找到解决方案，我会更新。

Hello, I also encountered an LSTM error during predict. May I ask if the solution you provided is to modify it in the predict file? I am a beginner and the questions I ask may be a bit basic. Here is my prediction file, and I don't know where to modify it. Thank you! Looking forward to your reply!
```

user_path = os.path.expanduser(""~"")                                                
checkpoint_subpath = os.path.join(user_path, r""ray_result\PPO"", checkpoint_str)     
entries = os.scandir(checkpoint_subpath)                                             
latest_dir = max((e for e in entries if e.is_dir()), key=lambda e: e.stat().st_mtime)     
checkpoint_subidx = re.sub(r""_0+"", ""-"", latest_dir.name)                           
# checkpoint_path = os.path.join(user_path, r""ray_results\PPO"", checkpoint_str)         
checkpoint_path = os.path.join(user_path, r""ray_result\PPO"")        
config_path = os.path.join(checkpoint_path, ""params.pkl"")


args = vars(parser.parse_args(args=[]))
args['checkpoint'] = checkpoint_path

analysis = ExperimentAnalysis(checkpoint_path)
trial = analysis.trials[0]
best_checkpoint = analysis.get_best_checkpoint(trial, metric=""episode_reward_mean"", mode=""max"")
best_checkpoint_path_str = best_checkpoint.to_directory()
best_checkpoint_dir = os.path.dirname(best_checkpoint_path_str)
print(best_checkpoint_path_str)


config_from_file = trial.config
config = config_creator(**args)
config['framework'] = config_from_file['framework']
config['lr'] = config_from_file['lr']
config['train_batch_size'] = config_from_file['train_batch_size']
env_name = args[""env_name""]
register_env(env_name, env_creator)
env_config = config[""env_config""]
env = env_creator(env_config)
obs_space = env.observation_space
act_space = env.action_space
print(config)

predictor = Algorithm.from_checkpoint(best_checkpoint)

obs, _ = env.reset()
obs_0 = obs.copy()

num_steps = 96         
num_samples = 50       

calc_time_tot = np.zeros((num_steps, num_samples))    
total_reward_tot = np.zeros(num_steps)                 
raw_action_tot = {}                                   
action_power_tot = {}                                   
best_iter_idx_tot = {}
total_reward_steps = {}
bus_voltage_tot = {}                 
line_trans_power_tot = {}            
step_cost_tot = {}                  
shapley_value_tot = {}               
Excess_return_tot = {}               
Prosumer_power_tot = {}            
HVAC_zone_temp_tot = {}              
HVAC_key_word = 'HVAC'
for i_step in range(num_steps-1):
    
    print(""\033[36;1mcurrent step: {}, current scheduling time: {}\033[0m"".format(env.episode_step, env.time))
   
    total_reward_step = np.zeros(num_samples)
    action_step = {}
    raw_action_step = {}
    real_power_step = {}
    bus_voltage_tot[i_step] = {}
    bus_voltage_step = {}
    line_trans_power_tot[i_step] = {}
    line_trans_power_step = {}
    step_cost_tot[i_step] = {}
    step_cost_step = {}
    shapley_value_tot[i_step] = {}
    shapley_value_step = {}
    Excess_return_tot[i_step] = {}
    Excess_return_step = {}
    Prosumer_power_tot[i_step] = {}
    Prosumer_power_step = {}
    HVAC_zone_temp_step = {}
    
    for iter_pred in tqdm(range(num_samples), desc='num_samples:'):
        obs = obs_0.copy()
        # obs = copy.deepcopy(obs_0)
        action_step[iter_pred] = {}
        raw_action_step[iter_pred] = {}
        real_power_step[iter_pred] = {}
        bus_voltage_step[iter_pred] = {}
        line_trans_power_step[iter_pred] = {}
        step_cost_step[iter_pred] = {}
        shapley_value_step[iter_pred] = {}
        Excess_return_step[iter_pred] = {}
        Prosumer_power_step[iter_pred] = {}
        HVAC_zone_temp_step[iter_pred] = {}

        
        for agent_id, agent_obs in obs.items():
            policy_id = config['multiagent']['policy_mapping_fn'](agent_id, episode=None, worker=None)
            start = time.time()
           
            action_step[iter_pred][agent_id] = predictor.compute_single_action(agent_obs, policy_id=policy_id)

            end = time.time()                                    
            calc_time_tot[i_step, iter_pred] += end - start

            raw_action_step[iter_pred][agent_id] = {}
            
            real_power_step[iter_pred][agent_id] = {}
         
            HVAC_zone_temp_step[iter_pred][agent_id] = {}
            for e in env.agent_dict[agent_id].env_dict:    # e: str -> PVone
                _env = env.agent_dict[agent_id].env_dict[e]
                raw_action_step[iter_pred][agent_id][e] = _env.calc_raw_action(action_step[iter_pred][agent_id][e])     
                real_power_step[iter_pred][agent_id][e] = _env.calc_real_power(action_step[iter_pred][agent_id][e])
                if HVAC_key_word in e:
                    HVAC_zone_temp_step[iter_pred][agent_id][e] = _env.calc_zone_temp(action_step[iter_pred][agent_id][e])

            end = time.time()
            calc_time_tot[i_step, iter_pred] += end - start

        _, reward, ter, tru, _, bus_voltage_step[iter_pred], line_trans_power_step[iter_pred], step_cost_step[iter_pred], shapley_value_step[iter_pred], Excess_return_step[iter_pred], Prosumer_power_step[iter_pred] = env.step(action_step[iter_pred], calc_rew=True)
        # done = done['__all__']
        ter = ter['__all__']
        tru = tru['__all__']

        total_reward_step[iter_pred] = sum(reward.values())
```",state action hello also error predict may ask solution provided modify predict file beginner ask may bit basic prediction file know modify thank looking forward reply analysis trial trial print print predictor range print step current time range start end end start end end start reward done done sum,issue,positive,positive,neutral,neutral,positive,positive
1909489367,"Hello, when customizing LSTM in RAY, everything was fine during the training phase, but the same seq occurred during the predict phase_ Len problem, I don't know how to define it. Here is my error message. Have you resolved it? Thank you! Looking forward to your reply!
```
Traceback (most recent call last):
  File ""C:\Users\zyn\Desktop\agent_three\examples\marl\rllib\heterogeneous\load_and_predict.py"", line 178, in <module>
    action_step[iter_pred][agent_id] = predictor.compute_single_action(agent_obs, policy_id=policy_id,full_fetch=True)
  File ""C:\Users\zyn\anaconda3\envs\PGW_RAY2\lib\site-packages\ray\util\tracing\tracing_helper.py"", line 460, in _resume_span
    return method(self, *_args, **_kwargs)
  File ""C:\Users\zyn\anaconda3\envs\PGW_RAY2\lib\site-packages\ray\rllib\algorithms\algorithm.py"", line 1595, in compute_single_action
    action, state, extra = policy.compute_single_action(
  File ""C:\Users\zyn\anaconda3\envs\PGW_RAY2\lib\site-packages\ray\rllib\policy\policy.py"", line 545, in compute_single_action
    out = self.compute_actions_from_input_dict(
  File ""C:\Users\zyn\anaconda3\envs\PGW_RAY2\lib\site-packages\ray\rllib\policy\torch_policy_v2.py"", line 522, in compute_actions_from_input_dict
    return self._compute_action_helper(
  File ""C:\Users\zyn\anaconda3\envs\PGW_RAY2\lib\site-packages\ray\rllib\utils\threading.py"", line 24, in wrapper
    return func(self, *a, **k)
  File ""C:\Users\zyn\anaconda3\envs\PGW_RAY2\lib\site-packages\ray\rllib\policy\torch_policy_v2.py"", line 1141, in _compute_action_helper
    dist_inputs, state_out = self.model(input_dict, state_batches, seq_lens)
  File ""C:\Users\zyn\anaconda3\envs\PGW_RAY2\lib\site-packages\ray\rllib\models\modelv2.py"", line 259, in __call__
    res = self.forward(restored, state or [], seq_lens)
  File ""C:\Users\zyn\anaconda3\envs\PGW_RAY2\lib\site-packages\ray\rllib\models\torch\recurrent_net.py"", line 207, in forward
    assert seq_lens is not None
```



",hello ray everything fine training phase predict problem know define error message resolved thank looking forward reply recent call last file line module file line return method self file line action state extra file line file line return file line wrapper return self file line file line state file line forward assert none,issue,negative,positive,positive,positive,positive,positive
1909488347,"Hello, I am using RAY to customize a centralized critical network. As a beginner, I have encountered many doubts and problems. Do I still need to override ModelV2 when customizing? Thank you! Looking forward to your reply!

```
class CentralizedCriticModel(TFModelV2):
    @override(ModelV2)
    def forward(self, input_dict, state, seq_lens):
        return self.model.forward(input_dict, state, seq_lens)
    @override(ModelV2)
    def value_function(self):
        return self.model.value_function()  # not used
```",hello ray critical network beginner many still need override thank looking forward reply class override forward self state return state override self return used,issue,negative,positive,positive,positive,positive,positive
1909331888,"please fix code format lint error.

If needs running `scripts/format.sh` ",please fix code format lint error need running,issue,negative,neutral,neutral,neutral,neutral,neutral
1909265570,"Sure! Depends a bit on the fix given the constraints of the setup, but I'll try.",sure bit fix given setup try,issue,negative,positive,positive,positive,positive,positive
1909233436,hi @justinvyu :wave: friendly ping on this one,hi wave friendly ping one,issue,negative,positive,positive,positive,positive,positive
1909217966,"@edoakes, thank you for the quick and detailed response!

Upon reevaluating our setup, I discovered a misconfiguration on our end that constrained the performance. 🥲 After addressing this, I am now observing a throughput of approximately 280 QPS on our corrected setup.

Furthermore, I conducted a test on an AWS t3.2xlarge instance (8-core Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz), which yielded around 175 QPS. While this is a marked improvement over the initial 70 QPS, it seems that we could still encounter scalability limitations under high-load scenarios.",thank quick detailed response upon setup discovered misconfiguration end constrained performance observing throughput approximately corrected setup furthermore test instance platinum around marked improvement initial could still encounter,issue,positive,positive,neutral,neutral,positive,positive
1909209168,"Adding onto this, I tried a few things (e.g. replacing GPU with CPU) and I think it has to do with fractional GPUs? 

Here's a repro I put together with some more debugging verbosity:
```python
import os
os.environ[""RAY_DEDUP_LOGS""] = ""0""

import ray
from ray.util.scheduling_strategies import PlacementGroupSchedulingStrategy
from ray.util import placement_group


class Worker:
    def __init__(self, index, num_gpus):
        self.index = index
        self.num_gpus = num_gpus

    def run(self):
        import os
        print(f""CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}"")
    
    def __repr__(self):
        return str(f""Worker[index={self.index}, num_gpus={self.num_gpus}]"")


def create_worker(placement_group, index, num_gpus):
    return (
        ray.remote(Worker)
        .options(
            num_cpus=0,
            num_gpus=num_gpus,
            scheduling_strategy=PlacementGroupSchedulingStrategy(
                placement_group=placement_group, placement_group_bundle_index=index
            ),
        )
        .remote(index, num_gpus)
    )

ray.init()

gpus = [0.2, 0.8, 0.8]  # Hangs
# gpus = [0.8, 0.2, 0.8] # Succeeds
# gpus = [0.8, 0.8, 0.2] # Succeeds

pg = placement_group([{""GPU"": gpu} for gpu in gpus])
ray.get(pg.ready())

workers = [create_worker(pg, i, gpu) for (i, gpu) in enumerate(gpus)]
ray.get([worker.run.remote() for worker in workers])

```

Hangs:
```
(Worker[index=0, num_gpus=0.2] pid=76147) CUDA_VISIBLE_DEVICES: 1
(Worker[index=1, num_gpus=0.8] pid=76148) CUDA_VISIBLE_DEVICES: 0
```

Successful:
```
(Worker[index=0, num_gpus=0.8] pid=77638) CUDA_VISIBLE_DEVICES: 1
(Worker[index=1, num_gpus=0.2] pid=77639) CUDA_VISIBLE_DEVICES: 0
(Worker[index=2, num_gpus=0.8] pid=77640) CUDA_VISIBLE_DEVICES: 0
```
```
(Worker[index=0, num_gpus=0.8] pid=72735) CUDA_VISIBLE_DEVICES: 1
(Worker[index=1, num_gpus=0.8] pid=72736) CUDA_VISIBLE_DEVICES: 0
(Worker[index=2, num_gpus=0.2] pid=72737) CUDA_VISIBLE_DEVICES: 0
```

Not sure if it's pure coincidence, but in the hanging case the ""smaller"" Actor (`num_gpus=0.2`) is scheduled with `CUDA_VISIBLE_DEVICE=1`, but in the successful case it gets scheduled with `CUDA_VISIBLE_DEVICE=0` along with the ""larger"" Actor(`num_gpus=0.8`).",onto tried think fractional put together verbosity python import o import ray import import class worker self index index run self import o print self return worker index return worker index enumerate worker worker worker successful worker worker worker worker worker worker sure pure coincidence hanging case smaller actor successful case along actor,issue,positive,positive,positive,positive,positive,positive
1909180979,"Yeah I think you can try to use the Ray cluster launcher for AWS. I am not an expert on it, maybe @kevin85421 @rkooo567 @scv119 could give some help here?
",yeah think try use ray cluster launcher expert maybe could give help,issue,positive,neutral,neutral,neutral,neutral,neutral
1909094320,"If you think this test is ok to be flaky or not important enough to fix, you can also remove the `weekly-release-blocker` tag. Thankkks",think test flaky important enough fix also remove tag,issue,negative,positive,positive,positive,positive,positive
1909083809,We can merge it. It should show up within an hour,merge show within hour,issue,negative,neutral,neutral,neutral,neutral,neutral
1909079553,"Thanks! LGTM. 

Currently, it currently still looks a bit jarring: lot of icons floating around. Down the road or if it's easy enough, we should probably do something like what workspace is doing so that it looks a bit cleaner?  
<img width=""1150"" alt=""Screenshot 2024-01-24 at 3 10 09 PM"" src=""https://github.com/ray-project/ray/assets/9677264/a5b5730b-3fb9-410c-b8c8-dd0844bcf4de"">
- add a divider between files
- move the download button to the right end





",thanks currently currently still bit jarring lot floating around road easy enough probably something like bit cleaner add divider move button right end,issue,positive,positive,positive,positive,positive,positive
1909043158,"> @sihanwang41 not following the description -- why can't we fix the issue instead of pinning?

Hi @edoakes, right now, there is another issue (paste link in the pr) with latest fastapi version, it is breaking the serve fastapi test. (Our ci is using pinned version, which is not able to catch it).
To repro:
pip install fastapi==0.109.0
```
import ray
import requests
from fastapi import FastAPI
from ray import serve

app = FastAPI()


@serve.deployment
@serve.ingress(app)
class MyFastAPIDeployment:
    @app.get(""/"")
    def root(self):
        return ""Hello, world!""


serve.run(MyFastAPIDeployment.bind(), route_prefix=""/hello"")
resp = requests.get(""http://localhost:8000/hello"")
# Got 404 response
assert resp.json() == ""Hello, world!""
```

The previous version of fastapi works fine. So that is why I pin the fastapi version. I probably don't need to pin starlette version, since the previous fastapi version is using the ""good"" starlette version, so there is no `AttributeError: 'Middleware' object has no attribute 'options'` error.
",following description ca fix issue instead pinning hi right another issue paste link latest version breaking serve test pinned version able catch pip install import ray import import ray import serve class root self return hello world resp got response assert hello world previous version work fine pin version probably need pin version since previous version good version object attribute error,issue,negative,positive,positive,positive,positive,positive
1909026685,@architkulkarni and @zhe-thoughts ^^ as Akashay mentioned we want to have the doc change into the release version for blog. is it okay for now to merge? Or we have to wait till the next release,want doc change release version merge wait till next release,issue,negative,neutral,neutral,neutral,neutral,neutral
1909021881,@sihanwang41 not following the description -- why can't we fix the issue instead of pinning?,following description ca fix issue instead pinning,issue,negative,neutral,neutral,neutral,neutral,neutral
1908968916,"right, so the bug is actually within deployment/bootstrap of the cluster(i.e. the overwriting of node ip address). But, it manifests when you submit a job after deployment of the cluster succeeds (the deployment of the cluster just puts the node ip address file with the wrong address on disk). When you do ray.init(...) + ray.remote(...) and then subsequently ray.get(object_ref) is when it fails with the `OwnerDiedError` with the stack trace I pasted above. ",right bug actually within cluster node address submit job deployment cluster deployment cluster node address file wrong address disk subsequently stack trace pasted,issue,negative,negative,neutral,neutral,negative,negative
1908944729,"Thanks for the help on those variables. I also noticed probably having to change `RAY_DASHBOARD_MAX_EVENTS_TO_CACHE` and `RAY_DASHBOARD_MAX_ACTORS_TO_CACHE` as well.

Having a way to change everything dynamically would be great. I got around the issue by changing the source file but that's not a great long term solution for us.",thanks help also probably change well way change everything dynamically would great got around issue source file great long term solution u,issue,positive,positive,positive,positive,positive,positive
1908921466,"I took another look and I feel the issue is that, there are some extra overhead which slow down the e2e performance.
It has nothing to do with the way objects are created.

The observation is that, the network usage actually is high

<img width=""254"" alt=""image"" src=""https://github.com/ray-project/ray/assets/74173148/674af4fa-5924-473a-85f5-db7a18912037"">



But the e2e performance is poor, only 30Gb/s


```
2024-01-24 13:08:15,079 INFO worker.py:1540 -- Connecting to existing Ray cluster at address: 10.0.59.98:6379...
2024-01-24 13:08:15,087 INFO worker.py:1715 -- Connected to Ray cluster. View the dashboard at https://session-lxgjneble3jd8gi5yfufuhbdkm.i.anyscaleuserdata-staging.com 
2024-01-24 13:08:15,096 INFO packaging.py:358 -- Pushing file package 'gcs://_ray_pkg_31b30bf8fe3979bdda35cc1f90879ab7.zip' (1.72MiB) to Ray cluster...
2024-01-24 13:08:15,104 INFO packaging.py:371 -- Successfully pushed file package 'gcs://_ray_pkg_31b30bf8fe3979bdda35cc1f90879ab7.zip'.
[2024-01-24 13:08:15,104 I 26655 26655] logging.cc:230: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1
(raylet, ip=10.0.36.90) [2024-01-24 13:08:15,979 I 27464 27464] logging.cc:230: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1
Finished benchmark for total_size_MiB: 53687091200, block_size_MiB: 536870912, parallel_block: None
        ray.wait(fetch_local=True) Gbps: 34.550865106563556
        ray.wait(fetch_local=True) total time s: 12.430853128433228
        ray.get() Gbps: 38556.56544985657
        ray.get() total time s: 0.011139392852783203
(raylet, ip=10.0.39.234) [2024-01-24 13:08:16,330 I 26952 26952] logging.cc:230: Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1 [repeated 99x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)
```


",took another look feel issue extra overhead slow performance nothing way observation network usage actually high image performance poor ray cluster address connected ray cluster view dashboard pushing file package ray cluster successfully file package set ray log level environment variable raylet set ray log level environment variable finished none total time total time raylet set ray log level environment variable repeated across cluster ray default set disable log deduplication see,issue,negative,positive,neutral,neutral,positive,positive
1908903095,"Yes, this is to cherry-pick the docs onto the release branch since we want to put out a blog",yes onto release branch since want put,issue,negative,neutral,neutral,neutral,neutral,neutral
1908881197,"It's a bit hard to distinguish the the download button from the link (whether it's the same link or one link + one button). Can we just use an icon with a different color (secondary color)?

<img width=""745"" alt=""Screenshot 2024-01-24 at 12 42 21 PM"" src=""https://github.com/ray-project/ray/assets/9677264/495f5511-a7f0-48ca-a1ac-1f7f6e1d51d7"">
",bit hard distinguish button link whether link one link one button use icon different color secondary color,issue,negative,negative,negative,negative,negative,negative
1908881190,could you fix the reviewer list?,could fix reviewer list,issue,negative,neutral,neutral,neutral,neutral,neutral
1908845884,"> Have you used [Ray Client](https://docs.ray.io/en/latest/cluster/running-applications/job-submission/ray-client.html#ray-client) to connect to a Ray cluster? If so, this will be constrained by protobuf serialization.
> 
> also see #34772

Thanks @zen-xu for pointing it out. I don't use ray client in this case.",used ray client connect ray cluster constrained serialization also see thanks pointing use ray client case,issue,negative,positive,positive,positive,positive,positive
1908845324,"> @iycheng should we track this issue for ray 2.10?
> 
> Also, what's the max bandwidth in this case?


10GB/s

",track issue ray also case,issue,negative,neutral,neutral,neutral,neutral,neutral
1908816716,"> > @alexeykudinkin is this fixing an identified bug or speculative? AFAIK the only cancellations that should be happening in the proxy are when the `ResponseGenerator` times out.
> 
> Speculative. What about clients disconnecting?

Clients disconnecting is detected via the `http.client.disconnect` message. `uvicorn` will not cancel the handling task.",fixing bug speculative happening proxy time speculative via message cancel handling task,issue,negative,neutral,neutral,neutral,neutral,neutral
1908805175,"> @alexeykudinkin is this fixing an identified bug or speculative? AFAIK the only cancellations that should be happening in the proxy are when the `ResponseGenerator` times out.

Speculative. What about clients disconnecting? ",fixing bug speculative happening proxy time speculative,issue,negative,neutral,neutral,neutral,neutral,neutral
1908794436,"> We can sort by state first and then node id.
> 
> one concern I can think of that is the moment the node dies, it will disappear from view to the bottom. Seems like that should be okay?

yeah. I think it's expected to see live nodes first.",sort state first node id one concern think moment node disappear view bottom like yeah think see live first,issue,negative,positive,positive,positive,positive,positive
1908783259,tests not done yet; ping me when they're green in case I miss it,done yet ping green case miss,issue,negative,negative,negative,negative,negative,negative
1908781377,"We can sort by state first and then node id. 

one concern I can think of that is the moment the node dies, it will disappear from view to the bottom. Seems like that should be okay?",sort state first node id one concern think moment node disappear view bottom like,issue,negative,positive,positive,positive,positive,positive
1908779487,@alexeykudinkin is this fixing an identified bug or speculative? AFAIK the only cancellations that should be happening in the proxy are when the `ResponseGenerator` times out.,fixing bug speculative happening proxy time,issue,negative,neutral,neutral,neutral,neutral,neutral
1908774756,@GeneDer @edoakes gonna add the tests in a bit,gon na add bit,issue,negative,neutral,neutral,neutral,neutral,neutral
1908764065,"I have configured the ray cluster with
minReplica : 0 for small group and worker group node okay.
as per demand and based on the job submission, the ray autoscaler generates the worker nodes. ",ray cluster small group worker group node per demand based job submission ray worker,issue,negative,negative,negative,negative,negative,negative
1908727855,"@MaoZiming no workaround that I'm aware of, but I'll try to add a fix for the next release.",aware try add fix next release,issue,negative,positive,positive,positive,positive,positive
1908725110,"Here's the new coverage report after [the last change](https://github.com/ray-project/ray/pull/42631/commits/f0b8f34628c45d7e4e66e5cde08f91cb5c4b7821):

* [htmlcov.zip](https://github.com/ray-project/ray/files/14042483/htmlcov.zip)

We're now missing coverage on [this `except` block](https://github.com/ray-project/ray/pull/42631/files#diff-70b893c408c0451921403ee6c98d85f5107f9f3791c6fdc4cdc651568ace2dddR260-R267). However, this block should only be hit if there's an unexpected bug in our batching implementation, so it's not easily testable. Let me know if you have any concerns or ideas on how to test that logic.",new coverage report last change missing coverage except block however block hit unexpected bug implementation easily testable let know test logic,issue,negative,positive,neutral,neutral,positive,positive
1908701001,"> let's also make sure we have decent test coverage exercising all paths (including failures in both non-gen and generator paths)

Good callout @alexeykudinkin. Here's the code coverage report for the existing `test_batching.py` unit tests. We have 94% coverage from the unit tests. The only parts we're missing:

* `pass` statements in the type hints
* the new debugging methods that we added in this PR. These are tested through integration tests added in the PR.
* a type error when you try to call `serve.batch` on a non-callable

Overall, we have full coverage for all the actual batching logic.

**Coverage report:** [htmlcov.zip](https://github.com/ray-project/ray/files/14042344/htmlcov.zip)

Note: I ran this with `pytest --cov batching --cov-report html -vs test_batching.py`. To get this to work, I had to copy all the `batching.py` logic out of the Ray repo and tweak the `test_batching.py` file a bit, so it imports that `batch` decorator directly form `batching.py` instead of through `serve.batch`.

",let also make sure decent test coverage generator good code coverage report unit coverage unit missing pas type new added tested integration added type error try call overall full coverage actual logic coverage report note ran get work copy logic ray tweak file bit batch decorator directly form instead,issue,negative,positive,positive,positive,positive,positive
1908511227,Thanks for the info! Sounds like it's indeed worthwhile for us to update at this point. Updating the Ray version in our monorepo sounds less frightening than fiddling with gRPC versions :),thanks like indeed u update point ray version le frightening fiddling,issue,negative,negative,negative,negative,negative,negative
1908491006,"> > What to do about passing runtime_env options with a config file? Currently they are ignored.
> 
> Say more about this?

If you pass a multi-app config file to `serve publish`, the runtime-env args are currently ignored and we go by whatever is in the config file. Probably the ideal behavior is to do an override across all the apps instead.

> > Should we default to --working-dir=“.”? We would then need to support something like --no-working-dir.
> 
> Hmm, I imagine we should be able to detect this based on the module import path right? For example, app:name should imply working_dir=""."", unless overridden. What would the use case for --no-working-dir be?

The case for `--no-working-dir` would be if the app is built directly into your image and you don't need to additionally upload local files at all. This is the minority of cases, so I think defaulting to `--working-dir="".""` makes sense.

> > Should --name specify the app name and service name? Or service name only? Does it even matter?
> 
> I'm guessing probably service name if we want to support publishing of multi-app services.

Yep that's what it's currently doing.",passing file currently say pas file serve publish currently go whatever file probably ideal behavior override across instead default would need support something like imagine able detect based module import path right example name imply unless would use case case would built directly image need additionally local minority think sense name specify name service name service name even matter guessing probably service name want support yep currently,issue,positive,positive,positive,positive,positive,positive
1908485814,Great -- we are targeting the upcoming Ray 2.10 release for this feature.,great upcoming ray release feature,issue,positive,positive,positive,positive,positive,positive
1908383435,"@rkooo567 why do we want to run it before fixing it? If you close this issue the test will be unjailed automatically, but the issue will just pop up again if it runs and fails again.",want run fixing close issue test unjailed automatically issue pop,issue,negative,neutral,neutral,neutral,neutral,neutral
1908334433,Thanks so much @woshiyyya !! I couldn't find anything about terraform there. Would you recommend avoiding it in lieu of the Ray Cluster Launcher tool? ,thanks much could find anything would recommend lieu ray cluster launcher tool,issue,positive,positive,positive,positive,positive,positive
1908175109,"Have you used [Ray Client](https://docs.ray.io/en/latest/cluster/running-applications/job-submission/ray-client.html#ray-client) to connect to a Ray cluster? If so, this will be constrained by protobuf serialization.

also see #34772
",used ray client connect ray cluster constrained serialization also see,issue,negative,neutral,neutral,neutral,neutral,neutral
1908019137,"@jakecyr Thanks for posing the issue. Can you check, if following the instructions [here](https://github.com/ray-project/ray/tree/master/rllib_contrib/a3c) can help you? ",thanks posing issue check following help,issue,positive,positive,neutral,neutral,positive,positive
1907980640,"Hi @tppalani I'm not very familiar with the nginx ingress controller, but I think you need to have the same redirect URI in OAuth2Proxy and the GitHub app config. Some providers don't allow for localhost redirect URIs are they're considered unsafe, but you can double check.",hi familiar ingres controller think need redirect allow redirect considered unsafe double check,issue,negative,positive,positive,positive,positive,positive
1907708206,"Hi @angelinalg, can I work on it ?
If so, kindly assign it to me",hi work kindly assign,issue,negative,positive,positive,positive,positive,positive
1907665816,"Btw, there are some core test failures. Can you take a look? ",core test take look,issue,negative,neutral,neutral,neutral,neutral,neutral
1907655808,"Hey @shahsmit1 to be clear, it is not job submission right? It is just deployment of the cluster? ",hey clear job submission right deployment cluster,issue,negative,positive,positive,positive,positive,positive
1907649229,"Hi, @marijncv since none of us has familiarity with VicotriaMetrics, can you make the contribution to support it and test it? Thank you!",hi since none u familiarity make contribution support test thank,issue,positive,neutral,neutral,neutral,neutral,neutral
1907647809,@can-anyscale does it mean we don't run this test now? It is pretty critical test to run and we cannot jail it. Can we make exception for this one? ,mean run test pretty critical test run jail make exception one,issue,negative,negative,neutral,neutral,negative,negative
1907645044,"> Have there been major architectural changes to the dashboard agent and/or Raylet?

There has been major update in discovery of the agent process due to one bug. But I don't know if the bug exists in your environment.

This typically means the dashboard agent was not ready to be started within the timeout. I think a couple things you can try.

One thing you can also try is to use different gRPC versions. Search grpc & agent in the repo and see the working versions suggested there.",major architectural dashboard agent raylet major update discovery agent process due one bug know bug environment typically dashboard agent ready within think couple try one thing also try use different search agent see working,issue,negative,positive,neutral,neutral,positive,positive
1907640075,P1.5 until author takes actions and we can figure out the root cause in more details. The provided repro doesn't work. ,author figure root cause provided work,issue,negative,neutral,neutral,neutral,neutral,neutral
1907638897,"@wjd5480 

Can you also check the dashboard and see what's running / pending when you are in this state? ",also check dashboard see running pending state,issue,negative,neutral,neutral,neutral,neutral,neutral
1907636397,Are you sure your remote node with IP 192.168.0.222 has the packages needed downloaded? How did you deploy Ray? ,sure remote node deploy ray,issue,negative,positive,positive,positive,positive,positive
1907634759,P1.5 until it is resolved & there are additional user feedback,resolved additional user feedback,issue,negative,neutral,neutral,neutral,neutral,neutral
1907620829,"if I have app2 with name model2 which is running, and trying to deploy app1 with name model1:
here is the cli output:
serve run     --address=ray://raycluster-kuberay-head-svc.kube-ray.svc.cluster.local:10001     --working-dir="".""     --runtime-env-json='{""pip"": [""transformers"", ""torch"", ""transformers[torch]"", ""bitsandbytes""]}'     config.yaml
2024-01-24 08:18:45,650	INFO scripts.py:411 -- Running config file: 'config.yaml'.
SIGTERM handler is not set because current thread is not the main thread.
2024-01-24 08:18:50,806	SUCC scripts.py:480 -- Submitted deploy config successfully.


but if I check the dashboard:
app2 will be deleted, and app1 is under deploying
but the deploy failed. the reason is:
Unexpected error occured while applying config for application 'app1': 
Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/site-packages/ray/serve/_private/application_state.py"", line 565, in _reconcile_build_app_task
    overrided_infos = override_deployment_info(
  File ""/usr/local/lib/python3.8/site-packages/ray/serve/_private/application_state.py"", line 1052, in override_deployment_info
    info = deployment_infos[deployment_name]
KeyError: 'model1'

but if I redo the deploy for app1. it will be successful.
",name model running trying deploy name model output serve run pip torch torch running file handler set current thread main thread deploy successfully check dashboard deploy reason unexpected error application recent call last file line file line redo deploy successful,issue,positive,positive,positive,positive,positive,positive
1907469826,"One problem in the benchmark script is the way objects created. Objects are created by using the ray worker ~~and this in the end could lead to too many owners. Each object has a different owner.~~

Because num_cpu=0, so I've observed that too many ray processes have been created.

Though it shouldn't impact the performance so big since the fetch happens after objects are created, change it to something else does making things better.

[new scripts](https://gist.github.com/iycheng/573b868e21b0f9c955ef9c7986d47e3e)

<img width=""278"" alt=""image"" src=""https://github.com/ray-project/ray/assets/74173148/7f6ee35d-82d2-4e99-a1a2-15d822dd7c6f"">


```
Finished benchmark for total_size_MiB: 10737418240, block_size_MiB: 134217728, parallel_block: None
        ray.wait(fetch_local=True) Gbps: 44.925188296770976
        ray.wait(fetch_local=True) total time s: 1.912053108215332
        ray.get() Gbps: 16843.757372119668
        ray.get() total time s: 0.005099773406982422
Finished benchmark for total_size_MiB: 10737418240, block_size_MiB: 268435456, parallel_block: None
        ray.wait(fetch_local=True) Gbps: 48.67517767973902
        ray.wait(fetch_local=True) total time s: 1.7647464275360107
        ray.get() Gbps: 33906.26483998115
        ray.get() total time s: 0.002533435821533203
Finished benchmark for total_size_MiB: 10737418240, block_size_MiB: 536870912, parallel_block: None
        ray.wait(fetch_local=True) Gbps: 48.848062429669554
        ray.wait(fetch_local=True) total time s: 1.758500576019287
        ray.get() Gbps: 55823.980506606706
        ray.get() total time s: 0.0015387535095214844
```

Next step:

- Need to understand the true production workload
- Figure out why the original script is so slow compared with the new one.

",one problem script way ray worker end could lead many object different many ray though impact performance big since fetch change something else making better new image finished none total time total time finished none total time total time finished none total time total time next step need understand true production figure original script slow new one,issue,positive,positive,positive,positive,positive,positive
1907361009,"Given the number of columns, let's move  it before the ""Restarted"" column. I think that orders it relatively more in order of importance.",given number let move column think relatively order importance,issue,negative,neutral,neutral,neutral,neutral,neutral
1907221928,"> @v4if we are considering adding a parameter `max_queued_queries`. This would cause new requests to be dropped if there are already `max_queued_queries` requests that are trying to be scheduled by a proxy or `DeploymentHandle`.
> 
> Would that work for your use case?

It works. Looking forward to this feature.",considering parameter would cause new already trying proxy would work use case work looking forward feature,issue,negative,positive,positive,positive,positive,positive
1907189432,"> What to do about passing runtime_env options with a config file? Currently they are ignored.

Say more about this?

> Should we default to --working-dir=“.”? We would then need to support something like --no-working-dir.

Hmm, I imagine we should be able to detect this based on the module import path right? For example, app:name should imply working_dir=""."", unless overridden. What would the use case for --no-working-dir be?

> Should --name specify the app name and service name? Or service name only? Does it even matter?

I'm guessing probably service name if we want to support publishing of multi-app services.",passing file currently say default would need support something like imagine able detect based module import path right example name imply unless would use case name specify name service name service name even matter guessing probably service name want support,issue,positive,positive,positive,positive,positive,positive
1907160433,Do you think you could merge the message with the previous tip? Seems there's some overlap in the content about local shuffling vs file shuffling.,think could merge message previous tip overlap content local shuffling file shuffling,issue,negative,negative,neutral,neutral,negative,negative
1907160175,"> could you update the PR title?
> 
> also we normally start the title with `[ci]` (lower case) rather than `[CI]`.

Just updated it!",could update title also normally start title lower case rather,issue,negative,positive,positive,positive,positive,positive
1907159767,"Ran 2 test builds on Buildkite, one with flag (the env variable for continuous build) on & one with flag off. Can confirm that the one with flag off doesn't have the changed test listed. ",ran test one flag variable continuous build one flag confirm one flag test listed,issue,negative,neutral,neutral,neutral,neutral,neutral
1907154111,"FYI @matthewdeng, we are experiencing with moving some of the CI tests to a continuous, periodically basis (to save CI cost but still provide useful signals for engineers). Since the ml window job has only one test and window is less impact, we are moving it first. Let us know if you have strong opinion in not doing this. Thankkks.",moving continuous periodically basis save cost still provide useful since window job one test window le impact moving first let u know strong opinion,issue,positive,positive,positive,positive,positive,positive
1907147741,"> Were you able to reproduce the issue? Or is this just an intermediate step to help debugging?

This is just an intermediate step. I wasn't able to reproduce the issue.",able reproduce issue intermediate step help intermediate step able reproduce issue,issue,negative,positive,positive,positive,positive,positive
1907146068,"My bad, let's not move the windows wheel job to continuous run; rather let's move the windows ml job as an exercise",bad let move wheel job continuous run rather let move job exercise,issue,negative,negative,negative,negative,negative,negative
1907144512,"We're going to try to start coordinating a version upgrade in the not-distant future, but it'll take a while.",going try start version upgrade future take,issue,negative,neutral,neutral,neutral,neutral,neutral
1907143232,WIll provider impl soon. But wanna sync on the high-level designs. ,provider soon wan na sync,issue,negative,negative,negative,negative,negative,negative
1907139735,"Yes, the concern is primarily performance driven (both latency and memory overheads)

You could set the below env vars to store a larger number of tasks before starting ray (do this before ray start):

```
export RAY_task_events_max_num_task_in_gcs=1000000 # 1M of tasks allowed in GCS
export RAY_task_events_max_num_status_events_buffer_on_worker=1000000 # 1M of task events allowed to be buffered on worker
export RAY_MAX_LIMIT_FROM_API_SERVER=100000 # 100K to be sent from dashboard server
export RAY_MAX_LIMIT_FROM_DATA_SOURCE=100000 # 100 K to be sent from GCS
```

If you want the dashboard to load more than 10K, @alanwguo is there a config we could change dynamically here? ",yes concern primarily performance driven latency memory could set store number starting ray ray start export export task worker export sent dashboard server export sent want dashboard load could change dynamically,issue,positive,positive,positive,positive,positive,positive
1907133537,"Can only repro when looking at nodes with dead actors on them. I have a repro on an internal cluster, feel free to dm for details",looking dead internal cluster feel free,issue,negative,positive,neutral,neutral,positive,positive
1907132302,"> ActorTable.tsx:384 Uncaught TypeError: Cannot read properties of undefined (reading 'ipAddress')

@alanwguo Can you take a look?",uncaught read undefined reading take look,issue,negative,neutral,neutral,neutral,neutral,neutral
1907125528,"I noticed that when I remove `local_dir=perf_dir` from the RunConfig, I do not have the issue -- except it saves all of the data and results in `~/ray_results`. Isn't this parameter supposed to enable logging/saving of files to an alternative location to the default?",remove issue except data parameter supposed enable alternative location default,issue,negative,neutral,neutral,neutral,neutral,neutral
1907123792,@shrekris-anyscale let's also make sure we have decent test coverage exercising all paths (including failures in both non-gen and generator paths),let also make sure decent test coverage generator,issue,negative,positive,positive,positive,positive,positive
1907110486,"![Image](https://github.com/ray-project/ray/assets/9677264/23a59b50-8f13-400b-8592-b29f596aa9b5)

![Image](https://github.com/ray-project/ray/assets/9677264/560b1d6d-a09c-47cf-a77e-04028510578c)

@ckw017 Is there a way to reproduce this. Cannot reproduce on my side. ",image image way reproduce reproduce side,issue,negative,neutral,neutral,neutral,neutral,neutral
1907107642,@alanwguo @rkooo567 This seems a relatively easy fix. I've heard other internal complaints about it. WDYT?,relatively easy fix internal,issue,negative,positive,positive,positive,positive,positive
1907105624,@rkooo567 not sure if you know this. Is this something easy to fix?,sure know something easy fix,issue,positive,positive,positive,positive,positive,positive
1907100695,"I will reopen this because I keep randomly getting into this issue with each other restart. For now, I don't know how to reproduce, so I can say it's constantly present ```(ProxyActor pid=1979) TypeError: object list can't be used in 'await' expression```",reopen keep randomly getting issue restart know reproduce say constantly present object list ca used expression,issue,negative,negative,negative,negative,negative,negative
1907066175,"@ShenJiahuan thanks for filing the issue! We're aware of the proxy as the bottleneck for each node. The reason for this design decision is because the proxy includes quite a bit of intelligence to handle dynamic service discovery/routing, features like model multiplexing, and handling edge cases like properly draining requests upon node removal (e.g., spot instance interruption).

We've done some work to ensure performance is within a reasonable bound, but we're aware that this may be a dealbreaker for some use cases. Likely our chosen path to improve this bottleneck would be to optimize the proxy performance (reduce overhead from Ray actor calls, consider implementing it in a faster language like C++) rather than rearchitect the system to remove it.

However, the 70 QPS number you've cited here is dramatically lower than we see in our microbenchmarks. I re-ran this benchmark on my laptop (2021 MacBook Pro with M1 max) on the master branch and saw ~800qps:
```
(ray) ➜  ray git:(master) ✗ ab -n 10000 -c 100 http://127.0.0.1:8000/

This is ApacheBench, Version 2.3 <$Revision: 1903618 $>
Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/
Licensed to The Apache Software Foundation, http://www.apache.org/

Benchmarking 127.0.0.1 (be patient)
Completed 1000 requests
Completed 2000 requests
Completed 3000 requests
Completed 4000 requests
Completed 5000 requests
Completed 6000 requests
Completed 7000 requests
Completed 8000 requests
Completed 9000 requests
Completed 10000 requests
Finished 10000 requests


Server Software:        uvicorn
Server Hostname:        127.0.0.1
Server Port:            8000

Document Path:          /
Document Length:        12 bytes

Concurrency Level:      100
Time taken for tests:   12.200 seconds
Complete requests:      10000
Failed requests:        0
Total transferred:      2060000 bytes
HTML transferred:       120000 bytes
Requests per second:    819.64 [#/sec] (mean)
Time per request:       122.005 [ms] (mean)
Time per request:       1.220 [ms] (mean, across all concurrent requests)
Transfer rate:          164.89 [Kbytes/sec] received

Connection Times (ms)
              min  mean[+/-sd] median   max
Connect:        0    1   0.6      0       4
Processing:    17  121  24.2    115     214
Waiting:       15  117  24.3    112     210
Total:         18  121  24.2    116     215
WARNING: The median and mean for the initial connection time are not within a normal deviation
        These results are probably not that reliable.

Percentage of the requests served within a certain time (ms)
  50%    116
  66%    123
  75%    130
  80%    137
  90%    157
  95%    174
  98%    184
  99%    192
 100%    215 (longest request)
```

What instance type did you run the benchmark on? Are you able to re-run it on one that is publicly available such as an AWS instance type?",thanks filing issue aware proxy bottleneck node reason design decision proxy quite bit intelligence handle dynamic service like model handling edge like properly upon node removal spot instance interruption done work ensure performance within reasonable bound aware may use likely chosen path improve bottleneck would optimize proxy performance reduce overhead ray actor consider faster language like rather system remove however number dramatically lower see pro master branch saw ray ray git master version revision copyright technology licensed apache foundation patient finished server server server port document path document length concurrency level time taken complete total transferred transferred per second mean time per request mean time per request mean across concurrent transfer rate received connection time min mean median connect waiting total warning median mean initial connection time within normal deviation probably reliable percentage within certain time request instance type run able one publicly available instance type,issue,positive,positive,neutral,neutral,positive,positive
1907056198,"Hi @jaanphare , this user guide: https://docs.ray.io/en/master/cluster/vms/index.html might help:)",hi user guide might help,issue,negative,neutral,neutral,neutral,neutral,neutral
1907038168,"Hi @zhangch-ss , can you try add `--address=` argument with `serve run`? https://docs.ray.io/en/latest/serve/api/index.html#serve-run",hi try add argument serve run,issue,negative,neutral,neutral,neutral,neutral,neutral
1907034396,"@sihanwang41  
here is the first config yaml:
applications:

- name: app2

  route_prefix: /app2

  import_path: main:app

  runtime_env: {}

  deployments:

  - name: model2
    num_replicas: 2
    ray_actor_options:
      num_cpus: 1.0
      num_gpus: 1.0
then the second config:
applications:

- name: app1

  route_prefix: /app1

  import_path: main:app

  runtime_env: {}

  deployments:

  - name: model1
    num_replicas: 2
    ray_actor_options:
      num_cpus: 1.0
      num_gpus: 1.0

the model name are different. but when I use command:
serve run \
    --address=ray://raycluster-kuberay-head-svc.kube-ray.svc.cluster.local:10001 \
    --working-dir=""."" \
    --runtime-env-json='{""pip"": [""transformers"", ""torch"", ""transformers[torch]"", ""bitsandbytes""]}' \
    config.yaml

it will always remove the previous model. and replace it.
the content of main.py are same. I just want to see if the same model can be served with different prefix. but it seems ray consider this is just an update, instead of a new deployment. and that's why removed the previous model. but is it the expected behaviour?
",first name main name model second name main name model model name different use command serve run pip torch torch always remove previous model replace content want see model different prefix ray consider update instead new deployment removed previous model behaviour,issue,negative,positive,neutral,neutral,positive,positive
1907021834,"Nice catch, @aalmah. This looks like a good candidate for using the GH UI to fix. Basically, there is a link on the page, called `Edit on GitHub`. Click on that and it sets up a PR for you. When you submit it, it will kick off a CI build and you can verify the fix before you submit the PR for approval. Let me know if you would like me to show you. ",nice catch like good candidate fix basically link page edit click submit kick build verify fix submit approval let know would like show,issue,positive,positive,positive,positive,positive,positive
1907018800,"Hi @antoniomdk 

I have configured Ray cluster using Kind in my local system. using port forwarding i can able to access ray dashboard https://docs.ray.io/en/latest/cluster/kubernetes/getting-started/raycluster-quick-start.html#kuberay-operator-deploy


Here the steps which i followed auth2-proxy

Using github oauth client id and secret has been created.

```
 ./helm.exe upgrade --install --wait --create-namespace --namespace tools oauth2-proxy oauth2-proxy/oauth2-proxy --version 6.19.1 --values - <<EOF
config:
  clientID: $CLIENT_ID
  clientSecret: $CLIENT_SECRET

extraArgs:
  provider: github
  whitelist-domain: .127.0.0.1.nip.io
  cookie-domain: .127.0.0.1.nip.io
  redirect-url: http://auth.127.0.0.1.nip.io/oauth2/callback
  cookie-secure: 'false'

ingress:
  enabled: true
  path: /
  hosts:
    - auth.127.0.0.1.nip.io
EOF
```
Then i have created ingress
```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ray-dashboard
  annotations:
    nginx.ingress.kubernetes.io/auth-url: http://oauth2-proxy.tools.svc.cluster.local/oauth2/auth
    nginx.ingress.kubernetes.io/auth-signin: http://auth.127.0.0.1.nip.io/oauth2/sign_in?rd=http://\$host\$request_uri
spec:
  rules:
    - host: ray-dashboard.127.0.0.1.nip.io
      http:
        paths:
          - pathType: ImplementationSpecific
            backend:
              service:
                name: raycluster-kuberay-head-svc
                port:
                  number: 8265
```

Post that when i'm trying to access host name i'm getting initial response with oauth-2proxy, when i'm clicking sign in with github then i'm getting error 404 page not found like

http://localhost/oauth2?error=redirect_uri_mismatch&error_description=The+redirect_uri+MUST+match+the+registered+callback+URL+for+this+application.&error_uri=https%3A%2F%2Fdocs.github.com%2Fapps%2Fmanaging-oauth-apps%2Ftroubleshooting-authorization-request-errors%2F%23redirect-uri-mismatch&state=omjkXEKHspvtYDNnU9WLzKFUobObftoc5f-e_Ne-VDk%3A%2F

http://ray-dashboard.127.0.0.1.nip.io

![image](https://github.com/ray-project/ray/assets/6921037/a60e27b6-95bb-473e-86cf-c00bb8093f55)



",hi ray cluster kind local system port forwarding able access ray dashboard client id secret upgrade install wait version provider ingres true path ingres kind ingres name spec host service name port number post trying access host name getting initial response sign getting error page found like image,issue,positive,positive,positive,positive,positive,positive
1907015804,"This is self-fixed with the reboot of the whole process (I was in reload mode). Apologies. 

`serve run model:entrypoint -r`",whole process reload mode serve run model,issue,negative,positive,positive,positive,positive,positive
1907005343,"@ericl TODOs on this side of the world are:

1. Replace the CLI subprocessing with an SDK call.

2. Answer these open questions:
- What to do about passing runtime_env options with a config file? Currently they are ignored.
- Should we default to --working-dir=“.”? We would then need to support something like `--no-working-dir`.
- Should `--name` specify the app name and service name? Or service name only? Does it even matter?
",side world replace call answer open passing file currently default would need support something like name specify name service name service name even matter,issue,positive,neutral,neutral,neutral,neutral,neutral
1907005170,"Hi @echzhai , you can define the `--name` with serve run to make the application not being terminated. If no name provided, we will use ""default"" as the application name, that is why you are seeing the application is destroyed. 



",hi define name serve run make application name provided use default application name seeing application,issue,negative,neutral,neutral,neutral,neutral,neutral
1906967013,"Thanks for the review! I tested it on a local cluster doing writes with no `overwrite_table` kwarg specified, `overwrite_table=True`, and `overwrite_table=False`. I verified that it correctly appended/overwrote from the BigQuery console.",thanks review tested local cluster correctly console,issue,negative,positive,neutral,neutral,positive,positive
1906950938,"@alexeykudinkin The repro showed failures because the batching deployment was misconfigured. The `batch` method must yield a list the same size as the input on each iteration. I updated the `RayDeploymentWithBatching` deployment to make the repro work:

```python
@serve.deployment()
class RayDeploymentWithBatching:
    @serve.batch(max_batch_size=20)
    async def batch(self, input: List[List[str]]):
        await asyncio.sleep(0.05)
        yield [1] * len(input)

    async def __call__(self, *args):
        resp = [r async for r in self.batch(""request"")]
        return {""response"": resp}
```

<details>
  <summary>Full script</summary>

  Run this with `python -m unittest repro.TestRayServeBatchTimeoutsRepercussions`

  ```python
  # File name: repro.py
  
  import asyncio
  import time
  from concurrent.futures import ThreadPoolExecutor
  from typing import Optional, List, Dict
  from unittest import TestCase
  
  import ray
  import ray.serve as serve
  import requests
  from starlette.requests import Request
  
  
  @serve.deployment()
  class RayDeploymentWithBatching:
      @serve.batch(max_batch_size=20)
      async def batch(self, input: List[List[str]]):
          await asyncio.sleep(0.05)
          yield [1] * len(input)
  
      async def __call__(self, *args):
          resp = [r async for r in self.batch(""request"")]
          return {""response"": resp}
  
  
  class RayClient:
      def __init__(self, url: str, client_timeout: Optional[int] = None):
          self.url = url
          self.client_timeout = client_timeout
  
      def post_request(self) -> Optional[List[Dict]]:
          text_chunk = [str(i) for i in range(5)]
          try:
              resp = requests.post(self.url, json={""items"": text_chunk}, timeout=self.client_timeout)
          except:
              return None
          return resp.json()[""response""]
  
  
  def get_success_rate(results: List[Optional[List[Dict]]]) -> float:
      return len([s for s in results if s is not None]) / len(results)
  
  
  class TestRayServeBatchTimeoutsRepercussions(TestCase):
      @classmethod
      def setUpClass(cls) -> None:
          ray.init(include_dashboard=False)
          serve.start(detached=True, http_options={""host"": ""0.0.0.0""})
          serve.run(RayDeploymentWithBatching.bind(), name=""batching"", route_prefix=""/batching"")
  
      @classmethod
      def tearDownClass(cls) -> None:
          ray.shutdown()
  
      def test_ray_serve_batch_timeouts_repercussions(self):
          ray_client = RayClient(url=""http://0.0.0.0:8000/batching"")
          s1, s2, s3 = run_queries_experiment_return_success_rate(ray_client)
          self.assertEqual(s1, 1.0)
          self.assertLess(s2, 0.2)
          self.assertLess(s3, 1.0)  # repercussions on success of step 3
  
  
  def run_queries_experiment_return_success_rate(
      ray_client: RayClient, n_requests: int = 50
  ):
  
      ray_client.client_timeout = 1.0
      s1 = run_queries_return_success_rate(ray_client, n_requests=n_requests)
      print(""Done with step 1. Waiting 5 seconds."")
      time.sleep(1)
  
      print(""Starting step 2."")
      ray_client.client_timeout = 0.1
      s2 = run_queries_return_success_rate(ray_client, n_requests=n_requests)
  
      time.sleep(1)
      print(""Done with step 2. Waiting for 5 seconds."")
      time.sleep(1)
      print(""Starting step 3."")
  
      ray_client.client_timeout = 1.0
      s3 = run_queries_return_success_rate(ray_client, n_requests=n_requests)
      print(""Done with step 3. Shutting down ray server."")
  
      print(f""Success rate for 1: {s1}"")
      print(f""Success rate for 2: {s2}"")
      print(f""Success rate for 3: {s3}"")
      return s1, s2, s3
  
  
  def run_queries_return_success_rate(ray_client: RayClient, n_requests: int) -> float:
      with ThreadPoolExecutor(max_workers=10) as executor:
          futures = [executor.submit(ray_client.post_request) for _ in range(n_requests)]
          results = [future.result() for future in futures]
  
      return get_success_rate(results)
  ```
</details>

I was able to run this without seeing any errors.",deployment batch method must yield list size input iteration deployment make work python class batch self input list list await yield input self resp request return response resp summary full script run python python file name import import time import import optional list import import ray import serve import import request class batch self input list list await yield input self resp request return response resp class self optional none self optional list range try resp except return none return response list optional list float return none class none none self success step print done step waiting print starting step print done step waiting print starting step print done step shutting ray server print success rate print success rate print success rate return float executor range future return able run without seeing,issue,positive,positive,positive,positive,positive,positive
1906871709,"`pandas.io.formats.format.ExtensionArrayFormatter` was changed to `pandas.io.formats.format._ExtensionArrayFormatter` in the commit for pandas issue [55398](https://github.com/pandas-dev/pandas/commit/6c58a217f5d9a2520d9d4bee665edff5397bbfc6) on October 4, 2023.

Downgrading to pandas 2.1.2 fixes the problem in Ray.",commit issue problem ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1906819560,"> Looks good, but please add testing for the case where you update a task(s) in flight

@edoakes Forgot about adding a unit test after writing an e2e test in the previous version of this PR, thanks for calling out. I've added a unit test for re-registering a task in the metrics pusher.",good please add testing case update task flight forgot unit test writing test previous version thanks calling added unit test task metric pusher,issue,positive,positive,positive,positive,positive,positive
1906758462,"@edoakes can't repro the hanging, but there are failures if you're trying to batch async generator so that might be one of the  contributing factors",ca hanging trying batch generator might one,issue,negative,neutral,neutral,neutral,neutral,neutral
1906751204,"> I have run some of the analyses and it seems to me that the advantage of multiple workers is very quickly lost due to the update cost. Here is the time that it takes to achieve 200 reward:
> 
> ![image](https://user-images.githubusercontent.com/5074413/135729080-78caaed3-1bb9-4d4e-b76e-a9ebce110479.png)
>  
> At least in these two environments, on my system, the bottleneck even with GPU (3090 RTX) is driven by learning update time:
> 
> ![image](https://user-images.githubusercontent.com/5074413/135729117-825ab8a2-014a-4344-a675-e630a4282dc6.png)
> 
> I have tried `Impala` and `APPO` as well which have a much higher throughput in terms of samples due to their asynchronous nature of updates but are not very effective at learning as quickly as `PPO` is even with the bottleneck. What surprises me is that in the other examples more workers were typically used even with PPO so I would be curious as to the setup that enabled more efficient scaling.
> 

Hey! I'm running into similar behavior on my project, and I'd like to run a similar profile to your second set of graphs. How did you get that time breakdown data out of Ray?",run analysis advantage multiple quickly lost due update cost time achieve reward image least two system bottleneck even driven learning update time image tried impala well much higher throughput due asynchronous nature effective learning quickly even bottleneck typically used even would curious setup efficient scaling hey running similar behavior project like run similar profile second set get time breakdown data ray,issue,positive,positive,neutral,neutral,positive,positive
1906692510,"@jjyao This is what I see:
```
shomil@g-c16a09854058c0001:~$ sudo nvidia-smi
Tue Jan 23 18:36:12 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA L4                      On  | 00000000:00:03.0 Off |                    0 |
| N/A   32C    P8              11W /  72W |      4MiB / 23034MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
shomil@g-c16a09854058c0001:~$ sudo lshw -C display
  *-display                 
       description: 3D controller
       product: NVIDIA Corporation
       vendor: NVIDIA Corporation
       physical id: 3
       bus info: pci@0000:00:03.0
       version: a1
       width: 64 bits
       clock: 33MHz
       capabilities: msix pm bus_master cap_list
       configuration: driver=nvidia latency=0
       resources: iomemory:80-7f iomemory:100-ff irq:11 memory:c0000000-c0ffffff memory:800000000-fffffffff memory:1000000000-1001ffffff
```

```
    NODE_ID                                                   NODE_IP    IS_HEAD_NODE    STATE    NODE_NAME    RESOURCES_TOTAL                 LABELS
 0  0c1af79e59ccbca53e5256fd57dbd844967b423d68baf17e33400e3a  10.0.0.27  True            ALIVE    10.0.0.27    CPU: 4.0                        ray.io/node_id: 0c1af79e59ccbca53e5256fd57dbd844967b423d68baf17e33400e3a
                                                                                                               GPU: 1.0
                                                                                                               accelerator_type:L4: 1.0
                                                                                                               memory: 16.000 GiB
                                                                                                               node:10.0.0.27: 1.0
                                                                                                               node:__internal_head__: 1.0
                                                                                                               object_store_memory: 4.173 GiB
```",see tue driver version version name volatile fan temp compute mig mib mib default type process name memory id id usage running found display description controller product corporation vendor corporation physical id bus version width clock configuration memory memory memory state true alive memory gib node node gib,issue,positive,positive,positive,positive,positive,positive
1906667583,"@anyscalesam I took a look at this and it seems to be failing for some obscure reason. Will take a bit more time to look into it, but may leave it jailed for now and rewrite it since we are rewriting a good number of our release tests for 2.10.",took look failing obscure reason take bit time look may leave rewrite since good number release,issue,negative,positive,positive,positive,positive,positive
1906649012,"@tppalani I don't have the full context here, but in general terms, the ArgoCD login is different from this. ArgoCD login will just apply to the Argo UI, not the applications managed by Argo.

Ray doesn't provide an auth layer for the dashboard, but the OAuth2Proxy lets you add SSO pages to applications that don't have that feature by default. So, in this case, your dashboard URL would point to the OAuth2Proxy, the proxy will check for a cookie with the right permissions and if the user is not authenticated, it will be redirected to the provider (Okta or similar). If you are already using Nginx ingress, you can follow this guideline:

https://kubernetes.github.io/ingress-nginx/examples/auth/oauth-external-auth/",full context general login different login apply argo argo ray provide layer dashboard add feature default case dashboard would point proxy check right user provider similar already ingres follow guideline,issue,negative,positive,positive,positive,positive,positive
1906630907,"@v4if we are considering adding a parameter `max_queued_queries`. This would cause new requests to be dropped if there are already `max_queued_queries` requests that are trying to be scheduled by a proxy or `DeploymentHandle`.

Would that work for your use case?",considering parameter would cause new already trying proxy would work use case,issue,negative,positive,positive,positive,positive,positive
1906557958,"Hi @antoniomdk 

Actually we have configured Tekton, Argocd as okta login method. we want to manage the users using AD group.
But when we have seen ray board i don't see any login button by default, do you have any document for setup ? 
![image](https://github.com/ray-project/ray/assets/6921037/155919ff-5444-478b-beb8-9aa871f188e4)
",hi actually login method want manage ad group seen ray board see login button default document setup image,issue,negative,neutral,neutral,neutral,neutral,neutral
1906548751,"You can use [DEX](https://dexidp.io/) to set up an OpenID Connect provider that interacts with Okta, Google, etc. From DEX you can set up different user groups, etc. Then, you can add [OAuth2Proxy](https://github.com/oauth2-proxy/oauth2-proxy) or similar on top of the Ray Dashboard to block unauthorized access.

The main problem with Oauth2Proxy is that you will have to implement a mechanism to authenticate users from the CLI in case you use the Ray Jobs CLI or SDK to submit jobs. That can be achieved using PCKE protocol and adding a public client in DEX. You will also need to configure oauth2proxy to read auth data from headers.

",use set connect provider set different user add similar top ray dashboard block unauthorized access main problem implement mechanism authenticate case use ray submit protocol public client also need configure read data,issue,negative,positive,positive,positive,positive,positive
1906097224,"@rkooo567 Just tried, same result. Full commands were:

```bash
# Starting from clean repo clone
# Just confirm ray-2.8.1 is working
git checkout ray-2.8.1;
cd python;
# Make sure all bazel caches are cleared
bazel clean --expunge;
sudo rm -rf ~/.cache/bazel;
rm -rf ~/bazel_cache/*;
sudo rm -rf ~/tmp/*;
bazel build --verbose_failures -- //:ray_pkg //cpp:ray_cpp_pkg; # succeeds

# Now try ray-2.9.0
git checkout ray-2.9.0;
bazel clean --expunge;
sudo rm -rf ~/.cache/bazel;
rm -rf ~/bazel_cache/*;
sudo rm -rf ~/tmp/*;
bazel build --verbose_failures -- //:ray_pkg //cpp:ray_cpp_pkg; # fails

# Now revert merge commit from https://github.com/ray-project/ray/pull/40852 and rebuild
git revert f811e3e02164faa346e2615c90e9c0397c5ce773
bazel clean --expunge;
sudo rm -rf ~/.cache/bazel;
rm -rf ~/bazel_cache/*;
sudo rm -rf ~/tmp/*;
bazel build --verbose_failures -- //:ray_pkg //cpp:ray_cpp_pkg; # succeeds

```",tried result full bash starting clean clone confirm working git python make sure clean expunge build try git clean expunge build revert merge commit rebuild git revert clean expunge build,issue,positive,positive,positive,positive,positive,positive
1905964519,"@bveeramani after i add the batch_format param it works fine now, thank you so much",add param work fine thank much,issue,positive,positive,positive,positive,positive,positive
1905899698,"Could you tell me if it would work to specify `--env-file .env` in `run_options`? 
If `.env` is passed and then drop the remote node and wait for application to run on another node, will we lose the `.env` information (if `""config"": {""eager_install"": false}` was enabled)?",could tell would work specify drop remote node wait application run another node lose information false,issue,negative,negative,negative,negative,negative,negative
1905177806,"@achordia20 @yvmilir Would you help test against my fix PR https://github.com/ray-project/ray/pull/42508 ? 

You can install my custom built package:
```
pip install ""ray[default] @ https://github.com/WeichenXu123/packages/raw/c5d6cedacec0ec2446a8c0803b14f35937b5fe0e/ray/spark-df-loader/ray-3.0.0.dev0-cp310-cp310-linux_x86_64.whl""
```",would help test fix install custom built package pip install ray default,issue,negative,neutral,neutral,neutral,neutral,neutral
1905153710,"> Can we remove underlines when I hover over the nav links?

Sure.

> There is around 40px of space between the ray logo and the nav links. Can we reduce it and make it same as the spacing between individual nav links?

Sure. FYI the padding is actually 1em on either side of the nav links, so it scales with font size.",remove hover link sure around space ray link reduce make spacing individual link sure padding actually em either side link scale font size,issue,positive,positive,positive,positive,positive,positive
1905140603,"While trying to repro i've slightly modified repro from the previous issue just trying to wrap around the async-generator and it fails:

```
import asyncio
import time
from concurrent.futures import ThreadPoolExecutor
from typing import Optional
from unittest import TestCase

import ray
import ray.serve as serve
import requests
from starlette.requests import Request


@serve.deployment()
class RayDeploymentWithBatching:
    @serve.batch(max_batch_size=20)
    async def batch(self, batch: list[list[str]]):
        await asyncio.sleep(0.05)
        for request in batch:
            yield [f""T({text})"" for text in request]

    async def __call__(self, request: Request):
        request_json = await request.json()
        resp = [r async for r in self.batch(request_json[""items""])]
        return {""response"": resp}


@serve.deployment()
class RayDeploymentWithoutBatching:
    @staticmethod
    async def infer_with_no_ray_batching(batch: list[list[str]]):
        await asyncio.sleep(0.05)
        return [[f""T({text})"" for text in request] for request in batch]

    async def __call__(self, request):
        request_json = await request.json()
        resp = self.infer_with_no_ray_batching(request_json[""items""])
        return {""response"": resp}


class RayClient:
    def __init__(self, url: str, client_timeout: Optional[int] = None):
        self.url = url
        self.client_timeout = client_timeout

    def post_request(self) -> Optional[list[dict]]:
        text_chunk = [str(i) for i in range(5)]
        try:
            resp = requests.post(self.url, json={""items"": text_chunk}, timeout=self.client_timeout)
        except:
            return None
        return resp.json()[""response""]


def get_success_rate(results: list[Optional[list[dict]]]) -> float:
    return len([s for s in results if s is not None]) / len(results)


class TestRayServeBatchTimeoutsRepercussions(TestCase):
    @classmethod
    def setUpClass(cls) -> None:
        ray.init(include_dashboard=False)
        serve.start(detached=True, http_options={""host"": ""0.0.0.0""})
        serve.run(RayDeploymentWithBatching.bind(), name=""batching"", route_prefix=""/batching"")
        serve.run(RayDeploymentWithoutBatching.bind(), name=""no-batching"", route_prefix=""/no-batching"")

    @classmethod
    def tearDownClass(cls) -> None:
        ray.shutdown()

    def test_ray_serve_batch_timeouts_repercussions(self):
        ray_client = RayClient(url=""http://0.0.0.0:8000/batching"")
        s1, s2, s3 = run_queries_experiment_return_success_rate(ray_client)
        self.assertEqual(s1, 1.0)
        self.assertLess(s2, 0.2)
        self.assertLess(s3, 1.0)  # repercussions on success of step 3

    def test_no_repercussions_with_no_batching(self):
        ray_client = RayClient(url=""http://0.0.0.0:8000/no-batching"")
        s1, s2, s3 = run_queries_experiment_return_success_rate(ray_client)
        self.assertEqual(s1, 1.0)
        self.assertLess(s2, 0.2)
        self.assertEqual(s3, 1.0)  # no repercussions on success of step 3


def run_queries_experiment_return_success_rate(
    ray_client: RayClient, n_requests: int = 50
) -> tuple[float, float, float]:

    ray_client.client_timeout = 1.0
    s1 = run_queries_return_success_rate(ray_client, n_requests=n_requests)
    print(""Done with step 1. Waiting 5 seconds."")
    time.sleep(1)

    print(""Starting step 2."")
    ray_client.client_timeout = 0.1
    s2 = run_queries_return_success_rate(ray_client, n_requests=n_requests)

    time.sleep(1)
    print(""Done with step 2. Waiting for 5 seconds."")
    time.sleep(1)
    print(""Starting step 3."")

    ray_client.client_timeout = 1.0
    s3 = run_queries_return_success_rate(ray_client, n_requests=n_requests)
    print(""Done with step 3. Shutting down ray server."")

    print(f""Success rate for 1: {s1}"")
    print(f""Success rate for 2: {s2}"")
    print(f""Success rate for 3: {s3}"")
    return s1, s2, s3


def run_queries_return_success_rate(ray_client: RayClient, n_requests: int) -> float:
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = [executor.submit(ray_client.post_request) for _ in range(n_requests)]
        results = [future.result() for future in futures]

    return get_success_rate(results)
```",trying slightly previous issue trying wrap around import import time import import optional import import ray import serve import import request class batch self batch list list await request batch yield text text request self request request await resp return response resp class batch list list await return text text request request batch self request await resp return response resp class self optional none self optional list range try resp except return none return response list optional list float return none class none none self success step self success step float float float print done step waiting print starting step print done step waiting print starting step print done step shutting ray server print success rate print success rate print success rate return float executor range future return,issue,positive,positive,positive,positive,positive,positive
1905139860,"I haven't seen this issue in specific, but I have some ideas.

@wjd5480 are you running anything else on the Ray cluster? 

Also, one workaround you might want to try is to add `num_cpus=0` to your `map_batches`. Not confident it'll fix your problem, but it's worth a shot.",seen issue specific running anything else ray cluster also one might want try add confident fix problem worth shot,issue,negative,positive,positive,positive,positive,positive
1905117638,"Seems like we've recently fixed very similar issue:
https://github.com/ray-project/ray/issues/41057",like recently fixed similar issue,issue,negative,positive,neutral,neutral,positive,positive
1905099906,"After a discussion, I'm going to break this PR into small bits to minimize code owner impact. Closing.",discussion going break small minimize code owner impact,issue,negative,negative,negative,negative,negative,negative
1905086777,"> I believe it is the same issue as #35989, is this correct?

Correct!

And note that besides the issue of ""Memory resource monitor on cluster takes into account cache memory"", the other parts of memory usage calculation is also not correct 
",believe issue correct correct note besides issue memory resource monitor cluster account cache memory memory usage calculation also correct,issue,negative,neutral,neutral,neutral,neutral,neutral
1905060518,"@WeichenXu123 thanks for the PR. This issue has been for a while, and I really appreicate you try fixing the issue! I will review it shortly. Fixing this is our P0 now. ",thanks issue really try fixing issue review shortly fixing,issue,negative,positive,positive,positive,positive,positive
1905052617,"Can we remove underlines when I hover over the nav links? 
There is around 40px of space between the ray logo and the nav links. Can we reduce it and make it same as the spacing between individual nav links?",remove hover link around space ray link reduce make spacing individual link,issue,negative,neutral,neutral,neutral,neutral,neutral
1905051424,"Bunch of copy edits ready for your review, @shrekris-anyscale!",bunch copy ready review,issue,negative,positive,positive,positive,positive,positive
1905050516,"@edoakes Sounds good! I've separated out the autoscaling changes into a separate PR. This PR now only deals with the changes to metrics pusher, and moving it + its tests into appropriate files. PTAL!",good separate metric pusher moving appropriate,issue,negative,positive,positive,positive,positive,positive
1905047904,"<img width=""212"" alt=""Screenshot 2024-01-22 at 3 59 29 PM"" src=""https://github.com/ray-project/ray/assets/23429473/201318d7-36db-4b62-911a-a9c696d06c66"">
Search icon in dark mode is not visible. ",search icon dark mode visible,issue,negative,negative,negative,negative,negative,negative
1905006724,"> > worker nodes are stopped daily and new worker nodes are spun up.
> 
> When you say worker nodes are stopped. How is it stopped? Did you stop it from your end? I think the flag means it is stopped instead of terminated when ray autoscaler downscales nodes. 

I _believe_ that GCP shuts them down every 24hrs because they are preemptible instances. They are stopped in the GCP compute engine, not deleted.",worker stopped daily new worker spun say worker stopped stopped stop end think flag stopped instead ray every stopped compute engine,issue,negative,positive,neutral,neutral,positive,positive
1904983432,@brombaut can you try using bazel clean --expunge and try again? ,try clean expunge try,issue,negative,positive,positive,positive,positive,positive
1904974389,"@bveeramani have you heard of the similar issues before?

@wjd5480 when you see this warning, what's the output of `ray status`? Can you show us? ",similar see warning output ray status show u,issue,negative,neutral,neutral,neutral,neutral,neutral
1904966586,"What's your workaround now? I think Ray data worked around this by separately yield metadata and block (and the driver coordinates it). I think this request makes sense, but we currently don't have immediate bandwidth to improve it. ",think ray data worked around separately yield block driver think request sense currently immediate improve,issue,negative,neutral,neutral,neutral,neutral,neutral
1904955226,There's one theory in our mind. Is it possible for you to experiment our fix with your environment if I suggest it? ,one theory mind possible experiment fix environment suggest,issue,negative,neutral,neutral,neutral,neutral,neutral
1904946290,"1. Is 10.224.226.71 the address of worker or head?
2. When this happens, if you use ray list nodes in a head node, can you find the IP 10.224.226.71? ",address worker head use ray list head node find,issue,negative,neutral,neutral,neutral,neutral,neutral
1904937544,"> worker nodes are stopped daily and new worker nodes are spun up.

When you say worker nodes are stopped. How is it stopped? Did you stop it from your end? I think the flag means it is stopped instead of terminated when ray autoscaler downscales nodes. ",worker stopped daily new worker spun say worker stopped stopped stop end think flag stopped instead ray,issue,negative,positive,neutral,neutral,positive,positive
1904931105,Why do we test it with 2 different AMIs? Have we decided to keep using this AMIs for all release tests? ,test different decided keep release,issue,negative,neutral,neutral,neutral,neutral,neutral
1904922755,"I think there are 2 possibilities.

1. The strategy itself has an issue
2. The joblib backend doesn't propagate the strategy to actors.

I will verify if 2 is the case.",think strategy issue propagate strategy verify case,issue,negative,neutral,neutral,neutral,neutral,neutral
1904855571,"Dashboard agent logs are shown as one of the screen-shots in the issue description under the head ""Dashboard agent logs have no useful info."" The last log line is ""Get all modules by type: DashboardAgentModule""

We're not easily able to upgrade the Ray version (it requires a thorough cross-team effort to test all functionality that depends on Ray.)
I do see that the log message ""Agent process expected id xxxxx timed out before registering"" no longer exists in upstream master, which suggests that the error would at least manifest differently in a recent Ray version.
Based on your understanding of Ray core development history, do you think the issue would be resolved with a version upgrade? Have there been major architectural changes to the dashboard agent and/or Raylet?",dashboard agent shown one issue description head dashboard agent useful last log line get type easily able upgrade ray version thorough effort test functionality ray see log message agent process id timed longer upstream master error would least manifest differently recent ray version based understanding ray core development history think issue would resolved version upgrade major architectural dashboard agent raylet,issue,positive,positive,neutral,neutral,positive,positive
1904754166,We're on version 2.9.0 and now getting a deprecation error. It doesn't seem that rllib_contrib comes withs the package. How can it be installed?,version getting deprecation error seem come package,issue,negative,neutral,neutral,neutral,neutral,neutral
1904711380,"To test this out, I recreated the failure in https://github.com/ray-project/ray/issues/42098 (by not adding `startup --output_user_root=d:/tmp` to `~/.bazelrc`), and then could fix the build by setting `IS_AUTOMATED_BUILD=1`. Before this PR, setting `IS_AUTOMATED_BUILD` would not be sufficient to fix the build.",test failure could fix build setting setting would sufficient fix build,issue,negative,negative,negative,negative,negative,negative
1904652000,"You should use the device set by Ray Train rather than the default CUDA device:

```diff
-device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
+device = ray.train.torch.get_device() if torch.cuda.is_available() else torch.device(""cpu"")
```",use device set ray train rather default device else else,issue,negative,neutral,neutral,neutral,neutral,neutral
1904139797,"Can you share the log file contents dashboard_agent.log?

Also, you are using an really old version of Ray. Is it possible to try the latest version? ",share log file content also really old version ray possible try latest version,issue,negative,positive,positive,positive,positive,positive
1903373862,I think releasing the pin is enough to solve the issue.,think pin enough solve issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1903275326,">  Hmm still feel a little hacky though. I feel like it is probably more ideal just creating less actors than 30? (like 10 should still trigger the warning?)

I think it's fragile to figure about a number that's enough to trigger the warning without OOM since it depends on the test machine and the environment. With the current way, we don't need a figure out an exact number and it should just work as long as all assertions pass.

",still feel little hacky though feel like probably ideal le like still trigger warning think fragile figure number enough trigger warning without since test machine environment current way need figure exact number work long pas,issue,positive,positive,positive,positive,positive,positive
1902792189,Loom video to confirm expected behavior: https://www.loom.com/share/7983065f1fbf4cbcbce5697d3770b6d5,loom video confirm behavior,issue,negative,neutral,neutral,neutral,neutral,neutral
1902635666,"I solve the issue, a value of type str in info dict returned by env.stape() can't convert to np.ndarray",solve issue value type returned ca convert,issue,negative,neutral,neutral,neutral,neutral,neutral
1902486335,"Hi @architkulkarni . I'm new to open-source contributions. I really like Ray and this is my first PR. Can you help me review this PR? Thank you so much. 

Note: I don't know why buildkit/premerge check was failing even though I already tested with linter mentioned [here](https://docs.ray.io/en/latest/ray-contribute/getting-involved.html#lint-and-formatting)",hi new really like ray first help review thank much note know check failing even though already tested linter,issue,positive,positive,positive,positive,positive,positive
1902478240,"I meet the similar problems when using the ray java 2.9.0 .


```
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/Users/zuston/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.17.1/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/Users/zuston/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/Users/zuston/.m2/repository/org/slf4j/slf4j-reload4j/1.7.36/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

java.lang.UnsatisfiedLinkError: /Users/zuston/Library/Caches/JNA/temp/jna4420077346987399926.tmp: dlopen(/Users/zuston/Library/Caches/JNA/temp/jna4420077346987399926.tmp, 0x0001): tried: '/Users/zuston/Library/Caches/JNA/temp/jna4420077346987399926.tmp' (fat file, but missing compatible architecture (have 'i386,x86_64', need 'arm64')), '/System/Volumes/Preboot/Cryptexes/OS/Users/zuston/Library/Caches/JNA/temp/jna4420077346987399926.tmp' (no such file), '/Users/zuston/Library/Caches/JNA/temp/jna4420077346987399926.tmp' (fat file, but missing compatible architecture (have 'i386,x86_64', need 'arm64'))

	at java.lang.ClassLoader$NativeLibrary.load(Native Method)
	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1950)
	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1832)
	at java.lang.Runtime.load0(Runtime.java:783)
	at java.lang.System.load(System.java:1100)
	at com.sun.jna.Native.loadNativeDispatchLibraryFromClasspath(Native.java:1018)
	at com.sun.jna.Native.loadNativeDispatchLibrary(Native.java:988)
	at com.sun.jna.Native.<clinit>(Native.java:195)
	at com.sun.jna.NativeLibrary.<clinit>(NativeLibrary.java:87)
	at io.ray.runtime.util.JniUtils.loadLibrary(JniUtils.java:47)
	at io.ray.runtime.RayNativeRuntime.<clinit>(RayNativeRuntime.java:53)
	at io.ray.runtime.DefaultRayRuntimeFactory.createRayRuntime(DefaultRayRuntimeFactory.java:32)
	at io.ray.api.Ray.init(Ray.java:32)
	at io.ray.api.Ray.init(Ray.java:19)
	at org.apache.uniffle.client.RayTest.test(RayTest.java:12)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.util.ArrayList.forEach(ArrayList.java:1259)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.util.ArrayList.forEach(ArrayList.java:1259)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
	at com.intellij.junit5.JUnit5IdeaTestRunner.startRunnerWithArgs(JUnit5IdeaTestRunner.java:57)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
	at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:232)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:55)


Process finished with exit code 255
```
",meet similar ray class path multiple found binding jar file jar found binding jar file jar found binding jar file jar see explanation actual binding type tried fat file missing compatible architecture need file fat file missing compatible architecture need native method native method invoke execute repeater process finished exit code,issue,negative,negative,neutral,neutral,negative,negative
1902437277,"@shrekris-anyscale, would you mind reviewing this PR, especially the frameworks column. I guessed at them and didn't know what to put for the Java tutorial. Thanks!",would mind especially column know put tutorial thanks,issue,negative,positive,neutral,neutral,positive,positive
1901790361,"Sorry, i missed that @edoakes had already approved it as well. 

Thanks for reviewing it!",sorry already well thanks,issue,positive,negative,negative,negative,negative,negative
1901766932,"I think it would go a long way to solving this by just printing this information once, rather than on every batch. I'm using this with `torch_iter_batches`  if that makes a difference

I've tried these to no avail

```
logging.getLogger(""ray"").setLevel(logging.ERROR)
logging.getLogger(""ray.data"").setLevel(logging.ERROR)
logging.getLogger(""ray.data._internal.execution.streaming_executor"").setLevel(logging.ERROR)
```

and my current workaround is to delete the offending lines from the modules aha


<details>

<summary>here is couple seconds worth of output</summary>

```
->MapBatches(f)]
2024-01-20 15:30:26,796 INFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)
2024-01-20 15:30:26,796 INFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
1it [00:05,  5.89s2024-01-20 15:30:32,687       INFO set_read_parallelism.py:115 -- Using autodetected parallelism=32 for stage ReadRange to satisfy parallelism at least twice the available number of CPUs (16).
2024-01-20 15:30:32,687 INFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadRange->MapBatches(f)]
2024-01-20 15:30:32,687 INFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)
2024-01-20 15:30:32,687 INFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
2it [00:06,  2.61s2024-01-20 15:30:33,006       INFO set_read_parallelism.py:115 -- Using autodetected parallelism=32 for stage ReadRange to satisfy parallelism at least twice the available number of CPUs (16).
2024-01-20 15:30:33,007 INFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadRange->MapBatches(f)]
2024-01-20 15:30:33,007 INFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)
2024-01-20 15:30:33,007 INFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
3it [00:06,  1.57s2024-01-20 15:30:33,342       INFO set_read_parallelism.py:115 -- Using autodetected parallelism=32 for stage ReadRange to satisfy parallelism at least twice the available number of CPUs (16).
2024-01-20 15:30:33,343 INFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadRange->MapBatches(f)]
2024-01-20 15:30:33,343 INFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)
2024-01-20 15:30:33,343 INFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
4it [00:06,  1.09s2024-01-20 15:30:33,694       INFO set_read_parallelism.py:115 -- Using autodetected parallelism=32 for stage ReadRange to satisfy parallelism at least twice the available number of CPUs (16).
2024-01-20 15:30:33,694 INFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadRange->MapBatches(f)]
2024-01-20 15:30:33,694 INFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)
2024-01-20 15:30:33,694 INFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
5it [00:07,  1.21i2024-01-20 15:30:34,041       INFO set_read_parallelism.py:115 -- Using autodetected parallelism=32 for stage ReadRange to satisfy parallelism at least twice the available number of CPUs (16).
2024-01-20 15:30:34,041 INFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadRange->MapBatches(f)]
2024-01-20 15:30:34,042 INFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)
2024-01-20 15:30:34,042 INFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
6it [00:07,  1.52i2024-01-20 15:30:34,384       INFO set_read_parallelism.py:115 -- Using autodetected parallelism=32 for stage ReadRange to satisfy parallelism at least twice the available number of CPUs (16).
2024-01-20 15:30:34,384 INFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadRange->MapBatches(f)]
2024-01-20 15:30:34,384 INFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)
2024-01-20 15:30:34,385 INFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
7it [00:07,  1.79i2024-01-20 15:30:34,737       INFO set_read_parallelism.py:115 -- Using autodetected parallelism=32 for stage ReadRange to satisfy parallelism at least twice the available number of CPUs (16).
2024-01-20 15:30:34,737 INFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadRange->MapBatches(f)]
2024-01-20 15:30:34,737 INFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)
2024-01-20 15:30:34,737 INFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
8it [00:08,  2.04i2024-01-20 15:30:35,080       INFO set_read_parallelism.py:115 -- Using autodetected parallelism=32 for stage ReadRange to satisfy parallelism at least twice the available number of CPUs (16).
2024-01-20 15:30:35,081 INFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadRange->MapBatches(f)]
2024-01-20 15:30:35,081 INFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)
2024-01-20 15:30:35,081 INFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
9it [00:08,  2.27i2024-01-20 15:30:35,408       INFO set_read_parallelism.py:115 -- Using autodetected parallelism=32 for stage ReadRange to satisfy parallelism at least twice the available number of CPUs (16).
2024-01-20 15:30:35,408 INFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadRange->MapBatches(f)]
2024-01-20 15:30:35,408 INFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)
2024-01-20 15:30:35,408 INFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
10it [00:08,  2.442024-01-20 15:30:35,750       INFO set_read_parallelism.py:115 -- Using autodetected parallelism=32 for stage ReadRange to satisfy parallelism at least twice the available number of CPUs (16).
2024-01-20 15:30:35,750 INFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadRange->MapBatches(f)]
2024-01-20 15:30:35,750 INFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)
2024-01-20 15:30:35,750 INFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
11it [00:09,  2.512024-01-20 15:30:36,124       INFO set_read_parallelism.py:115 -- Using autodetected parallelism=32 for stage ReadRange to satisfy parallelism at least twice the available number of CPUs (16).
2024-01-20 15:30:36,124 INFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadRange->MapBatches(f)]
2024-01-20 15:30:36,124 INFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)
2024-01-20 15:30:36,124 INFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
12it [00:09,  2.632024-01-20 15:30:36,459       INFO set_read_parallelism.py:115 -- Using autodetected parallelism=32 for stage ReadRange to satisfy parallelism at least twice the available number of CPUs (16).
2024-01-20 15:30:36,460 INFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadRange->MapBatches(f)]
2024-01-20 15:30:36,460 INFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)
2024-01-20 15:30:36,460 INFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
13it [00:10,  2.712024-01-20 15:30:36,802       INFO set_read_parallelism.py:115 -- Using autodetected parallelism=32 for stage ReadRange to satisfy parallelism at least twice the available number of CPUs (16).
2024-01-20 15:30:36,803 INFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadRange->MapBatches(f)]
2024-01-20 15:30:36,803 INFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)
2024-01-20 15:30:36,803 INFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
14it [00:10,  2.812024-01-20 15:30:37,129       INFO set_read_parallelism.py:115 -- Using autodetected parallelism=32 for stage ReadRange to satisfy parallelism at least twice the available number of CPUs (16).
2024-01-20 15:30:37,129 INFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadRange->MapBatches(f)]
2024-01-20 15:30:37,130 INFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)
2024-01-20 15:30:37,131 INFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
15it [00:10,  2.812024-01-20 15:30:37,484       INFO set_read_parallelism.py:115 -- Using autodetected parallelism=32 for stage ReadRange to satisfy parallelism at least twice the available number of CPUs (16).
2024-01-20 15:30:37,485 INFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadRange->MapBatches(f)]
2024-01-20 15:30:37,485 INFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)
2024-01-20 15:30:37,485 INFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
16it [00:11,  2.802024-01-20 15:30:37,845       INFO set_read_parallelism.py:115 -- Using autodetected parallelism=32 for stage ReadRange to satisfy parallelism at least twice the available number of CPUs (16).
2024-01-20 15:30:37,846 INFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadRange->MapBatches(f)]
2024-01-20 15:30:37,846 INFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)
2024-01-20 15:30:37,846 INFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
17it [00:11,  2.842024-01-20 15:30:38,183       INFO set_read_parallelism.py:115 -- Using autodetected parallelism=32 for stage ReadRange to satisfy parallelism at least twice the available number of CPUs (16).
2024-01-20 15:30:38,183 INFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadRange->MapBatches(f)]
2024-01-20 15:30:38,183 INFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)
2024-01-20 15:30:38,183 INFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
18it [00:11,  2.822024-01-20 15:30:38,545       INFO set_read_parallelism.py:115 -- Using autodetected parallelism=32 for stage ReadRange to satisfy parallelism at least twice the available number of CPUs (16).
2024-01-20 15:30:38,546 INFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadRange->MapBatches(f)]
2024-01-20 15:30:38,546 INFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)
2024-01-20 15:30:38,546 INFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
19it [00:12,  2.882024-01-20 15:30:38,875       INFO set_read_parallelism.py:115 -- Using autodetected parallelism=32 for stage ReadRange to satisfy parallelism at least twice the available number of CPUs (16).
2024-01-20 15:30:38,876 INFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadRange->MapBatches(f)]
2024-01-20 15:30:38,876 INFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)
```

</details>
",think would go long way printing information rather every batch difference tried avail ray current delete aha summary couple worth output execution tip detailed progress run true stage satisfy parallelism least twice available number dag input execution tip detailed progress run true stage satisfy parallelism least twice available number dag input execution tip detailed progress run true stage satisfy parallelism least twice available number dag input execution tip detailed progress run true stage satisfy parallelism least twice available number dag input execution tip detailed progress run true stage satisfy parallelism least twice available number dag input execution tip detailed progress run true stage satisfy parallelism least twice available number dag input execution tip detailed progress run true stage satisfy parallelism least twice available number dag input execution tip detailed progress run true stage satisfy parallelism least twice available number dag input execution tip detailed progress run true stage satisfy parallelism least twice available number dag input execution tip detailed progress run true stage satisfy parallelism least twice available number dag input execution tip detailed progress run true stage satisfy parallelism least twice available number dag input execution tip detailed progress run true stage satisfy parallelism least twice available number dag input execution tip detailed progress run true stage satisfy parallelism least twice available number dag input execution tip detailed progress run true stage satisfy parallelism least twice available number dag input execution tip detailed progress run true stage satisfy parallelism least twice available number dag input execution tip detailed progress run true stage satisfy parallelism least twice available number dag input execution tip detailed progress run true stage satisfy parallelism least twice available number dag input execution tip detailed progress run true stage satisfy parallelism least twice available number dag input execution tip detailed progress run true stage satisfy parallelism least twice available number dag input execution,issue,positive,positive,positive,positive,positive,positive
1901738386,"I ALSO Meet this problem when I compile ray 2.9.0 on macOS Sonoma 14.2.1

Any one knows why ?",also meet problem compile ray one,issue,negative,neutral,neutral,neutral,neutral,neutral
1901558005,"fyi, I help you retried the failed

````
Error response from daemon: Get ""https://029272617770.dkr.ecr.us-west-2.amazonaws.com/v2/"": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
````

we are seeing these recently (not only on this repo, but all buildkite envs), we are still trying to investigate why it is happening..",help error response daemon get request waiting connection seeing recently still trying investigate happening,issue,negative,neutral,neutral,neutral,neutral,neutral
1901502975,automation detected the test as flaky and open an issue for it (https://github.com/ray-project/ray/issues/42487); but then close it too quickly (after 10 consecutive pass); with my recent fix this test will be go back to be flaky soon ,test flaky open issue close quickly consecutive pas recent fix test go back flaky soon,issue,negative,positive,neutral,neutral,positive,positive
1901495943,"the ml train gpu test is flaky and took me 2 retries:
https://buildkite.com/ray-project/premerge/builds/16928#018d23d2-95ac-4e84-85ad-4db10527607b/179-1191

cc @can-anyscale @anyscalesam @matthewdeng ",train test flaky took,issue,negative,neutral,neutral,neutral,neutral,neutral
1901492826,for tests we only run on intel; but generally right now tests are interpreted as failed if they pass and fail on them same commit but different environment (e.g. data pyarrow 6/14/nightly all point to the same test).,run generally right pas fail commit different environment data point test,issue,negative,negative,neutral,neutral,negative,negative
1901392649,@alexeykudinkin I think those are already approved by us right? Is there anything new that you want us to look in particular? DCO is still failing bc the email you used in that [commit](https://github.com/ray-project/ray/pull/41722/commits/d6b2fb256b2c4948b0f24bb18f1bac2ec9f9798c) is [alexey.kudinkin@gmail.com](alexey.kudinkin@gmail.com) instead of [ak@anyscale.com](ak@anyscale.com). I think you can amend that commit with the anyscale email and force push again then it should be resolved :) ,think already u right anything new want u look particular still failing used commit instead ak ak think amend commit force push resolved,issue,negative,positive,positive,positive,positive,positive
1901283856,"Ah yeah I didn't merge this in time, there was a premerge failure. But we just ignored the test for the release. Closing the PR",ah yeah merge time failure test release,issue,negative,negative,negative,negative,negative,negative
1901279034,"@rkooo567 , @rickyyx : after this change, we will see all osx tests on go/flaky name change to `darwin:...` instead of `osx:...`. Other than that, all related features (issues, test results, etc.) are unchanged and un-interrupted.",change see name change instead related test unchanged,issue,negative,neutral,neutral,neutral,neutral,neutral
1901253833,"it can take 22s on a normal run:
https://buildkite.com/ray-project/premerge/builds/16928#018d23d2-9563-47fe-a513-1a86c4bfed2a
so exceeding 30s timeout on a long tail run seems quite possible.",take normal run exceeding long tail run quite possible,issue,negative,positive,neutral,neutral,positive,positive
1901172309,"have we thought about showing the log file name (or path) somewhere on the page?

if user wants to access the file directly on file system",thought showing log file name path somewhere page user access file directly file system,issue,negative,positive,neutral,neutral,positive,positive
1900970598,"Ran @zcin's autoscaling workload against this PR and manually verified that autoscaling & metrics work as expected

<img width=""352"" alt=""Screenshot 2024-01-19 at 1 15 00 PM"" src=""https://github.com/ray-project/ray/assets/9871461/17de2af9-ead2-485b-8b28-c2d60995073d"">
<img width=""362"" alt=""Screenshot 2024-01-19 at 1 14 35 PM"" src=""https://github.com/ray-project/ray/assets/9871461/0341f0dc-df1f-404d-b1e8-016371edf878"">
<img width=""480"" alt=""Screenshot 2024-01-19 at 1 14 50 PM"" src=""https://github.com/ray-project/ray/assets/9871461/2427ec7c-2849-4cca-b638-f492553a6dea"">
",ran manually metric work,issue,negative,neutral,neutral,neutral,neutral,neutral
1900819900,"This example has been removed from the documentation.

@LarkinDeity To fix, I think you'd just want to add `batch_format=""pandas""` to the `map_batches` call.",example removed documentation fix think want add call,issue,negative,neutral,neutral,neutral,neutral,neutral
1900802394,"@alexeykudinkin just FYI that test failure has to do with the latest Starlette version. If you just rebase to the latest master, it should automatically skipping that test and allow merge. We have a separate PR to fix the starlette issue https://github.com/ray-project/ray/pull/42378
",test failure latest version rebase latest master automatically skipping test allow merge separate fix issue,issue,negative,positive,positive,positive,positive,positive
1900480378,still this issue persisted ? I'm guessing this issue perhaps resolved ? can you confirm me once ?,still issue guessing issue perhaps resolved confirm,issue,negative,neutral,neutral,neutral,neutral,neutral
1900104669,"Yes, this issue does not happen when using Ray 2.6. It happens on any version after that, on 2.7, 2.8, and 2.9.
What I've found so far is that this happens when the node provider attempts to launch two or more nodes at once. This issue does not reproduce when it launchs only one node at once.
I've checked /session_latest/logs/monitor.out for debugging.

```txt
2024-01-19 09:42:14,235	INFO updater.py:329 -- New status: waiting-for-ssh
2024-01-19 09:42:14,235	INFO updater.py:266 -- [1/7] Waiting for SSH to become available
2024-01-19 09:42:14,235	INFO updater.py:271 -- Running `uptime` as a test.
2024-01-19 09:42:14,235	INFO command_runner.py:204 -- Fetched IP: 10.178.0.88
2024-01-19 09:42:14,235	INFO log_timer.py:25 -- NodeUpdater: ray-diffusion-beta0-worker-8fb039f1-compute: Got IP  [LogTimer=0ms]
2024-01-19 09:42:14,236	VINFO command_runner.py:371 -- Running `uptime`
2024-01-19 09:42:14,236	VVINFO command_runner.py:373 -- Full command is `ssh -tt -i ~/ray_bootstrap_key.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_1d41c853af/501043dfa6/%C -o ControlPersist=10s -o ConnectTimeout=10s ubuntu@10.178.0.88 bash --login -c -i 'source ~/.bashrc; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (uptime)'`
2024-01-19 09:42:17,361	INFO updater.py:317 -- SSH still not available (SSH command failed.), retrying in 5 seconds.
2024-01-19 09:42:20,152	INFO updater.py:329 -- New status: waiting-for-ssh
2024-01-19 09:42:20,152	INFO updater.py:266 -- [1/7] Waiting for SSH to become available
2024-01-19 09:42:20,152	INFO updater.py:271 -- Running `uptime` as a test.
2024-01-19 09:42:20,152	INFO command_runner.py:204 -- Fetched IP: 10.178.0.61
2024-01-19 09:42:20,152	INFO log_timer.py:25 -- NodeUpdater: ray-diffusion-beta0-worker-c3457c69-compute: Got IP  [LogTimer=0ms]
2024-01-19 09:42:20,153	VINFO command_runner.py:371 -- Running `uptime`
2024-01-19 09:42:20,153	VVINFO command_runner.py:373 -- Full command is `ssh -tt -i ~/ray_bootstrap_key.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_1d41c853af/501043dfa6/%C -o ControlPersist=10s -o ConnectTimeout=10s ubuntu@10.178.0.61 bash --login -c -i 'source ~/.bashrc; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (uptime)'`
2024-01-19 09:42:20,165	INFO updater.py:317 -- SSH still not available (SSH command failed.), retrying in 5 seconds.
2024-01-19 09:42:22,366	VINFO command_runner.py:371 -- Running `uptime`
2024-01-19 09:42:22,366	VVINFO command_runner.py:373 -- Full command is `ssh -tt -i ~/ray_bootstrap_key.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_1d41c853af/501043dfa6/%C -o ControlPersist=10s -o ConnectTimeout=10s ubuntu@10.178.0.88 bash --login -c -i 'source ~/.bashrc; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (uptime)'`
2024-01-19 09:42:22,377	INFO updater.py:317 -- SSH still not available (SSH command failed.), retrying in 5 seconds.
2024-01-19 09:42:25,172	VINFO command_runner.py:371 -- Running `uptime`
2024-01-19 09:42:25,172	VVINFO command_runner.py:373 -- Full command is `ssh -tt -i ~/ray_bootstrap_key.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_1d41c853af/501043dfa6/%C -o ControlPersist=10s -o ConnectTimeout=10s ubuntu@10.178.0.61 bash --login -c -i 'source ~/.bashrc; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (uptime)'`
2024-01-19 09:42:25,191	INFO updater.py:317 -- SSH still not available (SSH command failed.), retrying in 5 seconds.
2024-01-19 09:42:27,379	VINFO command_runner.py:371 -- Running `uptime`
2024-01-19 09:42:27,380	VVINFO command_runner.py:373 -- Full command is `ssh -tt -i ~/ray_bootstrap_key.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_1d41c853af/501043dfa6/%C -o ControlPersist=10s -o ConnectTimeout=10s ubuntu@10.178.0.88 bash --login -c -i 'source ~/.bashrc; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (uptime)'`
2024-01-19 09:42:27,391	INFO updater.py:317 -- SSH still not available (SSH command failed.), retrying in 5 seconds.
2024-01-19 09:42:30,195	VINFO command_runner.py:371 -- Running `uptime`
2024-01-19 09:42:30,195	VVINFO command_runner.py:373 -- Full command is `ssh -tt -i ~/ray_bootstrap_key.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_1d41c853af/501043dfa6/%C -o ControlPersist=10s -o ConnectTimeout=10s ubuntu@10.178.0.61 bash --login -c -i 'source ~/.bashrc; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (uptime)'`
 09:42:32 up 0 min,  1 user,  load average: 0.52, 0.13, 0.04
2024-01-19 09:42:32,395	VINFO command_runner.py:371 -- Running `uptime`
2024-01-19 09:42:32,395	VVINFO command_runner.py:373 -- Full command is `ssh -tt -i ~/ray_bootstrap_key.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_1d41c853af/501043dfa6/%C -o ControlPersist=10s -o ConnectTimeout=10s ubuntu@10.178.0.88 bash --login -c -i 'source ~/.bashrc; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (uptime)'`
2024-01-19 09:42:32,397	SUCC updater.py:285 -- Success.
2024-01-19 09:42:32,397	INFO log_timer.py:25 -- NodeUpdater: ray-diffusion-beta0-worker-c3457c69-compute: Got remote shell  [LogTimer=12245ms]
2024-01-19 09:42:32,397	INFO updater.py:379 -- Updating cluster configuration. [hash=6403cebb59737a1dcf0d5502a51c03f20eccf17f]
 09:42:34 up 0 min,  1 user,  load average: 0.27, 0.06, 0.02
2024-01-19 09:42:34,502	SUCC updater.py:285 -- Success.
2024-01-19 09:42:34,502	INFO log_timer.py:25 -- NodeUpdater: ray-diffusion-beta0-worker-8fb039f1-compute: Got remote shell  [LogTimer=20267ms]
2024-01-19 09:42:38,116	INFO updater.py:386 -- New status: syncing-files
2024-01-19 09:42:38,116	INFO updater.py:243 -- [2/7] Processing file mounts
2024-01-19 09:42:38,116	INFO updater.py:260 -- [3/7] No worker file mounts to sync
2024-01-19 09:42:43,848	INFO updater.py:397 -- New status: setting-up
2024-01-19 09:42:43,848	INFO updater.py:438 -- [4/7] No initialization commands to run.
2024-01-19 09:42:43,848	INFO updater.py:442 -- [5/7] Initializing command runner
2024-01-19 09:42:43,848	INFO updater.py:489 -- [6/7] No setup commands to run.
2024-01-19 09:42:43,848	INFO updater.py:494 -- [7/7] Starting the Ray runtime
2024-01-19 09:42:43,848	VINFO command_runner.py:371 -- Running `export RAY_OVERRIDE_RESOURCES='{""CPU"":4,""GPU"":1}';export RAY_HEAD_IP=10.178.0.9; ray stop`
2024-01-19 09:42:43,848	VVINFO command_runner.py:373 -- Full command is `ssh -tt -i ~/ray_bootstrap_key.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_1d41c853af/501043dfa6/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@10.178.0.61 bash --login -c -i 'source ~/.bashrc; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (export RAY_OVERRIDE_RESOURCES='""'""'{""CPU"":4,""GPU"":1}'""'""';export RAY_HEAD_IP=10.178.0.9; ray stop)'`
2024-01-19 09:42:44,029	INFO updater.py:379 -- Updating cluster configuration. [hash=6403cebb59737a1dcf0d5502a51c03f20eccf17f]
Did not find any active Ray processes.
2024-01-19 09:42:46,856	VINFO command_runner.py:371 -- Running `export RAY_OVERRIDE_RESOURCES='{""CPU"":4,""GPU"":1}';export RAY_HEAD_IP=10.178.0.9; RAY_ROTATION_MAX_BYTES=256000 RAY_ROTATION_BACKUP_COUNT=0 ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076`
2024-01-19 09:42:46,856	VVINFO command_runner.py:373 -- Full command is `ssh -tt -i ~/ray_bootstrap_key.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_1d41c853af/501043dfa6/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@10.178.0.61 bash --login -c -i 'source ~/.bashrc; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (export RAY_OVERRIDE_RESOURCES='""'""'{""CPU"":4,""GPU"":1}'""'""';export RAY_HEAD_IP=10.178.0.9; RAY_ROTATION_MAX_BYTES=256000 RAY_ROTATION_BACKUP_COUNT=0 ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076)'`
Local node IP: 10.178.0.61
[2024-01-19 09:42:48,690 I 1248 1248] global_state_accessor.cc:374: This node has an IP address of 10.178.0.61, but we cannot find a local Raylet with the same address. This can happen when you connect to the Ray cluster with a different IP address or when connecting to a container.

--------------------
Ray runtime started.
--------------------

To terminate the Ray runtime, run
  ray stop
2024-01-19 09:42:48,883	VINFO command_runner.py:371 -- Running `export RAY_OVERRIDE_RESOURCES='{""CPU"":4,""GPU"":1}';export RAY_HEAD_IP=10.178.0.9; gcloud compute instance-groups unmanaged add-instances vrew-diffusion-beta --instances=$INSTANCE_NAME --zone=asia-northeast3-a || true`
2024-01-19 09:42:48,884	VVINFO command_runner.py:373 -- Full command is `ssh -tt -i ~/ray_bootstrap_key.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_1d41c853af/501043dfa6/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@10.178.0.61 bash --login -c -i 'source ~/.bashrc; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (export RAY_OVERRIDE_RESOURCES='""'""'{""CPU"":4,""GPU"":1}'""'""';export RAY_HEAD_IP=10.178.0.9; gcloud compute instance-groups unmanaged add-instances vrew-diffusion-beta --instances=$INSTANCE_NAME --zone=asia-northeast3-a || true)'`
2024-01-19 09:42:49,753	INFO updater.py:386 -- New status: syncing-files
2024-01-19 09:42:49,754	INFO updater.py:243 -- [2/7] Processing file mounts
2024-01-19 09:42:49,754	INFO updater.py:260 -- [3/7] No worker file mounts to sync
ERROR: (gcloud.compute.instance-groups.unmanaged.add-instances) argument --instances: not enough args
Usage: gcloud compute instance-groups unmanaged add-instances NAME --instances=INSTANCE,[INSTANCE,...] [optional flags]
  optional flags may be  --help | --zone

For detailed information on this command and its flags, run:
  gcloud compute instance-groups unmanaged add-instances --help
2024-01-19 09:42:52,903	INFO log_timer.py:25 -- NodeUpdater: ray-diffusion-beta0-worker-c3457c69-compute: Ray start commands succeeded [LogTimer=9055ms]
2024-01-19 09:42:52,904	INFO log_timer.py:25 -- NodeUpdater: ray-diffusion-beta0-worker-c3457c69-compute: Applied config 6403cebb59737a1dcf0d5502a51c03f20eccf17f  [LogTimer=38457ms]
2024-01-19 09:42:55,707	INFO updater.py:397 -- New status: setting-up
2024-01-19 09:42:55,707	INFO updater.py:438 -- [4/7] No initialization commands to run.
2024-01-19 09:42:55,707	INFO updater.py:442 -- [5/7] Initializing command runner
2024-01-19 09:42:55,708	INFO updater.py:489 -- [6/7] No setup commands to run.
2024-01-19 09:42:55,708	INFO updater.py:494 -- [7/7] Starting the Ray runtime
2024-01-19 09:42:55,708	VINFO command_runner.py:371 -- Running `export RAY_OVERRIDE_RESOURCES='{""CPU"":4,""GPU"":1}';export RAY_HEAD_IP=10.178.0.9; ray stop`
2024-01-19 09:42:55,708	VVINFO command_runner.py:373 -- Full command is `ssh -tt -i ~/ray_bootstrap_key.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_1d41c853af/501043dfa6/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@10.178.0.88 bash --login -c -i 'source ~/.bashrc; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (export RAY_OVERRIDE_RESOURCES='""'""'{""CPU"":4,""GPU"":1}'""'""';export RAY_HEAD_IP=10.178.0.9; ray stop)'`
Did not find any active Ray processes.
2024-01-19 09:42:58,841	VINFO command_runner.py:371 -- Running `export RAY_OVERRIDE_RESOURCES='{""CPU"":4,""GPU"":1}';export RAY_HEAD_IP=10.178.0.9; RAY_ROTATION_MAX_BYTES=256000 RAY_ROTATION_BACKUP_COUNT=0 ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076`
2024-01-19 09:42:58,841	VVINFO command_runner.py:373 -- Full command is `ssh -tt -i ~/ray_bootstrap_key.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_1d41c853af/501043dfa6/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@10.178.0.88 bash --login -c -i 'source ~/.bashrc; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (export RAY_OVERRIDE_RESOURCES='""'""'{""CPU"":4,""GPU"":1}'""'""';export RAY_HEAD_IP=10.178.0.9; RAY_ROTATION_MAX_BYTES=256000 RAY_ROTATION_BACKUP_COUNT=0 ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076)'`
Local node IP: 10.178.0.88
[2024-01-19 09:43:00,715 I 1266 1266] global_state_accessor.cc:374: This node has an IP address of 10.178.0.88, but we cannot find a local Raylet with the same address. This can happen when you connect to the Ray cluster with a different IP address or when connecting to a container.
```",yes issue happen ray version found far node provider launch two issue reproduce one node checked new status waiting become available running test fetched got running full command bash login export still available command new status waiting become available running test fetched got running full command bash login export still available command running full command bash login export still available command running full command bash login export still available command running full command bash login export still available command running full command bash login export min user load average running full command bash login export success got remote shell cluster configuration min user load average success got remote shell new status file worker file sync new status run command runner setup run starting ray running export export ray stop full command bash login export export export ray stop cluster configuration find active ray running export export ray start full command bash login export export export ray start local node node address find local raylet address happen connect ray cluster different address container ray terminate ray run ray stop running export export compute unmanaged true full command bash login export export export compute unmanaged true new status file worker file sync error argument enough usage compute unmanaged name instance optional optional may help zone detailed information command run compute unmanaged help ray start applied new status run command runner setup run starting ray running export export ray stop full command bash login export export export ray stop find active ray running export export ray start full command bash login export export export ray start local node node address find local raylet address happen connect ray cluster different address container,issue,positive,positive,positive,positive,positive,positive
1900091449,"I get this error when i'am running the NYC_TAXI_DATA example from ray data example, I wander how can I fix it?",get error running example ray data example wander fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1900044030,"> 量恰好是正确的形状。我将其切换为复制奖励张量，该张量应该是兼容

I happen to also want to use centralized critical. I have encountered the same doubts as you. Besides, I am using heterogeneous agents with different numbers of state actions. What should I do?Do you need to set some options to True when using centralized critical? Is there anything similar to centralized_ A setting like cric=True? Or do I just need to write the custom network from the following website into the program? I have been troubled by this recently and look forward to your reply. Thank you!I have just started learning this part. If there are any low-level questions, please forgive me！
https://github.com/ray-project/ray/blob/master/rllib/examples/centralized_critic.py#L242",happen also want use critical besides heterogeneous different state need set true critical anything similar setting like need write custom network following program recently look forward reply thank learning part please forgive,issue,positive,positive,neutral,neutral,positive,positive
1899993255,"> 我一直在遇到同样的问题。我使用了集中式 critic A3C，通过使用集中式 crititc 更改了 critic 网络架构，我得到了比参数共享更好的结果。这将是对 ray 2.3.0 的一个很好的补充

I happen to also want to use centralized critical. Do you need to set some options to True when using centralized critical? Is there anything similar to centralized_ A setting like cric=True? Or do I just need to write the custom network from the following website into the program? I have been troubled by this recently and look forward to your reply. Thank you!
https://github.com/ray-project/ray/blob/master/rllib/examples/centralized_critic.py#L242",critic critic ray happen also want use critical need set true critical anything similar setting like need write custom network following program recently look forward reply thank,issue,negative,positive,neutral,neutral,positive,positive
1899938987,"Created a [discussion](https://discuss.ray.io/t/serve-high-stream-to-stream-grpc-methods-do-not-work/13459/4) resulting in an [issue](https://github.com/ray-project/ray/issues/42488) to add a feature.
",discussion resulting issue add feature,issue,negative,neutral,neutral,neutral,neutral,neutral
1899922259,"> I re-ran a few times and it appears the test failures seem to be caused by this change. It appears to be related to the code in the Ray dashboard that handles when pydantic is/isn't installed in the environment: https://buildkite.com/ray-project/premerge/builds/16713#018d18ce-bb5e-4ce7-8e2f-2f7f09ab25f8/197-1121

Thanks. I need to set up local compilation environment to see why it fails. I'll come back with my findings.",time test seem change related code ray dashboard environment thanks need set local compilation environment see come back,issue,negative,positive,neutral,neutral,positive,positive
1899845508,"@rkooo567 i took a look at the doc, and am not sure how we want to cover this change: we're not stating that extra tags are not allowed anywhere, so i think calling it out would look out of line.

It was actually unexpected that the extra tags were not allowed before, so i with this PR we're actually getting to where most people would expect us to be in the first place.",took look doc sure want cover change extra anywhere think calling would look line actually unexpected extra actually getting people would expect u first place,issue,negative,positive,positive,positive,positive,positive
1899552219,Got it. Think https://github.com/ray-project/ray/pull/42504 should fix the issue,got think fix issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1899529151,"Ah sorry issue description is outdated.

What actually happens is:
- some tasks get scheduled right away due to prefetch_batches (expected)
- later, Ray Data always schedules 2 tasks to run at a time, which exceeds the object store limit. We should only be running 1 at a time.",ah sorry issue description outdated actually get right away due later ray data always run time object store limit running time,issue,negative,negative,negative,negative,negative,negative
1899514095,CAn't merge without passing premerge. Retrying...,ca merge without passing,issue,negative,neutral,neutral,neutral,neutral,neutral
1899508975,"No more lint failure after merge master. Still a ""ml: train gpu tests"" failure which looks to be fail to build and irrelavent. ",lint failure merge master still train failure fail build,issue,negative,negative,negative,negative,negative,negative
1899497729,"@stephanie-wang I ran the script with verbose logging, but I only saw at most 2 active tasks. How did you determine that all tasks got scheduled immediately?",ran script verbose logging saw active determine got immediately,issue,negative,negative,negative,negative,negative,negative
1899488020,Hi @architkulkarni just having one doubt after submitting xgboost job ray head node will be use some kind of kubernetes service account right? Do you which service account will be using it?,hi one doubt job ray head node use kind service account right service account,issue,negative,positive,positive,positive,positive,positive
1899406576,"> IUC, the behavior is

> When creating a metric, you define required tags
> you can set default tags using set_default_tags
> If you add additional tags within record API, it ""adds tags if tags don't exist or overwrites if the default tag exists"" tags.

Correct.

> Let's also update the doc!

What doc are you referring to @rkooo567?",behavior metric define set default add additional within record exist default tag correct let also update doc doc,issue,negative,neutral,neutral,neutral,neutral,neutral
1899343301,Hi @mattip after some discussions we decide to not update on our end because in pip it works fine. Would you mind continuing the path in https://github.com/conda-forge/conda-forge-repodata-patches-feedstock/pull/638 and we can call it fixed?,hi decide update end pip work fine would mind path call fixed,issue,negative,positive,positive,positive,positive,positive
1899326417,"Discussed with @can-anyscale offline -- `data: flaky tests` doesn't use GPUs, so this test is indefinitely failing. Once the GPU issue is fixed, we'll see if the test continues to be flaky.",data flaky use test indefinitely failing issue fixed see test flaky,issue,negative,positive,neutral,neutral,positive,positive
1899264735,can we add the `jailed-test` if we don't plan to fix and don't want this to be release blocking? thankkks,add plan fix want release blocking,issue,negative,neutral,neutral,neutral,neutral,neutral
1899231300,The actors should scale down if they're not being used. How are you determining that the first `map_batches` is complete?,scale used first complete,issue,negative,positive,positive,positive,positive,positive
1899214304,"Also, would you mind sharing the data? It'd make it easier for us to investigate ",also would mind data make easier u investigate,issue,negative,neutral,neutral,neutral,neutral,neutral
1899213375,"Hey @iambsk, what do you see when you run the program with verbose progress? Do you observe a queue building up for any operator in particular?

```python
ray.data.DataContext.get_current().execution_options.verbose_progress = True
```

You might see something like this:

```
Running: 1.0/10.0 CPU, 1.0/16.0 GPU, 300.0 MiB/25.0 MiB object_store_memory:   0%|                        | 0/1 [00:04<?, ?it/s]
- MapBatches(f): 64 active, 2642 queued, 0.0 MiB objects: 100%|███████████████████████████████████████| 2/2 [00:04<00:00,  2.25s/it]
- MapBatches(g): 8 active, 163 queued, 300.0 MiB objects:   0%|                                             | 0/1 [00:04<?, ?it/s]
- Write 3:   0%|                           
```",hey see run program verbose progress observe queue building operator particular python true might see something like running mib active mib active mib write,issue,positive,positive,neutral,neutral,positive,positive
1899179708,"This issue is still creating a problem for our team as we try to automate more of our training flow. I would love if this name is exposed via some configuration. For the time being, we're using Weights and Biases artifacts to get around ray's built-in artifact syncing.",issue still problem team try training flow would love name exposed via configuration time get around ray artifact,issue,negative,positive,positive,positive,positive,positive
1899144660,"One simple way to approach this:

```
import ray
from ray.job_submission import JobSubmissionClient


def get_job_submission_id() -> str:
    c = JobSubmissionClient()
    current_job_id = ray.get_runtime_context().get_job_id()
    jobs = c.list_jobs()
    return [job.submission_id for job in jobs if job.job_id == current_job_id][0]

```",one simple way approach import ray import return job,issue,negative,neutral,neutral,neutral,neutral,neutral
1899022988,I think this is passing on master now for nightly: https://buildkite.com/ray-project/postmerge/builds/2574#018d1d4f-5acb-4752-8215-70a3dffea219/186-912,think passing master nightly,issue,negative,neutral,neutral,neutral,neutral,neutral
1899000923,"> Please update the related issue! Also why is this draft? (is this ready to be reviewd? )

Yeah, i would love to get a quick round of review for the high-level approach first. ",please update related issue also draft ready yeah would love get quick round review approach first,issue,positive,positive,positive,positive,positive,positive
1898857573,"@alanwguo picking this back up with some renewed interest from my team...

Could we do something like this in `optional_utils.py`:

```py
class PrefixedRouteTableDef(aiohttp.web.RouteTableDef):
    def __init__(self, prefix):
        super().__init__()
        self._prefix = prefix

    def route(self, method, path, **kwargs)
        path = f""{self._prefix}{path}""
        return super().route(method, path, **kwargs)

def method_route_table_factory(prefix=""""):
    class MethodRouteTable:
        """"""A helper class to bind http route to class method.""""""

        _bind_map = collections.defaultdict(dict)
        _routes = PrefixedRouteTableDef(prefix)
```",back interest team could something like class self prefix super prefix route self method path path path return super method path class helper class bind route class method prefix,issue,positive,positive,positive,positive,positive,positive
1898842661,"how does this work with dynamic batching? with multiplexing, the requests in a batch could belong to different model ids, then they couldn't effectively be batched together",work dynamic batch could belong different model could effectively together,issue,positive,positive,positive,positive,positive,positive
1898826902,I re-ran a few times and it appears the test failures seem to be caused by this change. It appears to be related to the code in the Ray dashboard that handles when pydantic is/isn't installed in the environment: https://buildkite.com/ray-project/premerge/builds/16713#018d18ce-bb5e-4ce7-8e2f-2f7f09ab25f8/197-1121,time test seem change related code ray dashboard environment,issue,negative,neutral,neutral,neutral,neutral,neutral
1898766129,"Hey, I am facing a problem with the above mentioned guide.

If i provide the following configuration for my worker nodes, once the `ray up config.yaml --yes` command is done, then I attached to the cluster I get the error: `ray.worker.default: UnauthorizedOperation ` However the role I used to authenticate with AWS has `AdministratorAcess`.

`node_config:
            InstanceType: t3.micro
            IamInstanceProfile:
                Arn: arn:aws:iam::OURAWSID:instance-profile/ray-worker-v1`

However for the head node, everything works fine.
`IamInstanceProfile:
                Arn: arn:aws:iam::OURAWSID:instance-profile/ray-head-v1`
                
If i do not provide any Arn configuration to the worker node, the cluster also starts without any problems. However all created worker nodes have no IAM role attached to it. If I look at the Worker-Node EC2 instance, under ""IAM Role"" I get no attached role therefore my worker nodes are not able to access my S3 storage for example. 

Does anyone have an Idea why the config is not working?

PS: The only workaround is to manually set the IAM role on the worker using the EC2 portal i.e. Actions -> Security -> Modify IAM role -> select ray-worker-v1 but that is not really a viable solution. ",hey facing problem guide provide following configuration worker ray yes command done attached cluster get error however role used authenticate arn arn however head node everything work fine arn arn provide arn configuration worker node cluster also without however worker role attached look instance role get attached role therefore worker able access storage example anyone idea working manually set role worker portal security modify role select really viable solution,issue,negative,positive,positive,positive,positive,positive
1898587324,Please update the related issue! Also why is this draft? (is this ready to be reviewd? ),please update related issue also draft ready,issue,positive,positive,neutral,neutral,positive,positive
1898561689,"Has this been seen recently? The grpc handling has been refactored, perhaps this should be closed.",seen recently handling perhaps closed,issue,negative,negative,neutral,neutral,negative,negative
1898557930,Closing due to inaction by reporters. Please reopen or open a new issue if this is still relevant.,due inaction please reopen open new issue still relevant,issue,negative,positive,positive,positive,positive,positive
1898312080,"Hello, after reading your replies, I have also encountered this issue. Regarding multi-agent centralized critic, my understanding is to insert the example script you provided above into the code, similar to a custom model. Do you know if I understand correctly? Is there an option to set centralized critic to True or False after customization? Looking forward to your reply!

> 实际上，在 examples 文件夹中有一个示例：https://github.com/ray-project/ray/blob/master/rllib/examples/centralized_critic.py

",hello reading also issue regarding critic understanding insert example script provided code similar custom model know understand correctly option set critic true false looking forward reply,issue,negative,negative,neutral,neutral,negative,negative
1898118907,"Update: I think the issue can be fixed (for PPO at least) by changing line 175 of `rllib/models/torch/complex_input_net.py` into this
```Python3
post_fcnet_hiddens = model_config.get(""post_fcnet_hiddens"", [])

if post_fcnet_hiddens:
    self.num_outputs = post_fcnet_hiddens[-1]
else:
    self.num_outputs = concat_size
```
since `concat_size` is the size of the model ouput before the final FC hidden layers. For RNNSAC, what seems to have worked in addition to the change above was adding this to line 96 of `rllib/algorithms/sac/rnnsac_torch_model.py`
```Python3
if actions is None:
    actions = model_out['prev_actions']
``` 
and changing line 370 of `rllib/algorithms/sac/rnnsac_torch_policy.py` to
```Python3
q_tp1, _ = target_model.get_q_values(
```
That said, even with small replay buffer sizes, RNNSAC seems to gobble up RAM so much so that it causes workers to quit due to memory pressure. I would appreciate it if someone could verify these changes.",update think issue fixed least line python else since size model final hidden worked addition change line python none line python said even small replay buffer size gobble ram much quit due memory pressure would appreciate someone could verify,issue,negative,negative,neutral,neutral,negative,negative
1898074592,"> we also need #ifndef _WIN32 within code where redis client is imported

Yes, makes perfect sense. I will try to push that into this PR.",also need within code client yes perfect sense try push,issue,positive,positive,positive,positive,positive,positive
1898044188,@mattip ^ Does it make sense? We can also discuss in person to quickly find a solution. ,make sense also discus person quickly find solution,issue,negative,positive,positive,positive,positive,positive
1898028217,"Code generally LGTM. Can you update the doc and precisely update the new behavior? 

- RAY_ env vars are inherited.
- specified envs are set (and higher priority than ENVs set within docker images)
- Docker images' env vars are set
 ",code generally update doc precisely update new behavior set higher priority set within docker docker set,issue,negative,positive,positive,positive,positive,positive
1898020537,Hmm still feel a little hacky though. I feel like it is probably more ideal just creating less actors than 30? (like 10 should still trigger the warning?),still feel little hacky though feel like probably ideal le like still trigger warning,issue,positive,positive,positive,positive,positive,positive
1897927270,"@rynewang My problem has been solved. I have circumvented IDLE_Spill by setting object memory to a larger value，but seemingly the ray::IDLE_Spill take up a lot of memory is likely issue 27499， after largen object memory size,  the ray::IDLE take up a lot of memory occasionally appearance like issue 27499",problem setting object memory seemingly ray take lot memory likely issue largen object memory size ray take lot memory occasionally appearance like issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1897792036,"woohoo~ going in~

let's see if this makes things build and run a bit better..",going let see build run bit better,issue,negative,positive,positive,positive,positive,positive
1897698762,the overlayfs issue might be associated with the buildkite stack upgrade.,issue might associated stack upgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
1897697743,"> @n3011 Is it possible to add a small test to confirm that this PR fixes the failure scenario you're running into? Thanks!
> 
> https://github.com/ray-project/ray/blob/master/python/ray/train/tests/test_windows.py

This test covers the scenario https://github.com/ray-project/ray/blob/master/python/ray/train/tests/test_trainer_restore.py#L189, is this run on windows system also?",possible add small test confirm failure scenario running thanks test scenario run system also,issue,negative,negative,neutral,neutral,negative,negative
1897696792,"this PR might fix the issues related to here:
https://buildkite.com/ray-project/postmerge/builds/2564#018d1a25-9e78-44f2-95c5-6095d3b7ab00

I feel that it is the mass move (of an installed ray in the container) caused the overlay fs of docker build to panic somehow (maybe run out of temp disk or something)",might fix related feel mass move ray container overlay docker build panic somehow maybe run temp disk something,issue,negative,neutral,neutral,neutral,neutral,neutral
1897677900,"For what it's worth, I was able to reproduce the original poster's problem, but _not_ the behavior described by @scottjlee .

I have target max block size set to the default of 128MB and read in 1 40MB parquet file of ~1.5M rows using `parallelism=1`. Printing the dataset shows that `ray` knows what's up: `Dataset(num_blocks=1, num_rows=1563418, schema=SCHEMA)` . I then call `Dataset.map_batches(preproc_func, batch_size=None, batch_format=""pandas"")`. Instead of one large batch, I get many batches of 10k rows apiece, just 0.4MB big according to `pandas.DataFrame.memory_usage()`. Increasing the max target block size does not change this behavior.

Strangely, if I call `.limit()` on the dataset before `.map_batches()` using a value larger than the total number of rows, I do indeed get one large batch, 56MB big -- well under the max target size. I also get the expected batch size if I use `.iter_batches()`, i.e. `next(iter(ds.iter_batches(batch_size=None, batch_format=""pandas"")))`. 

This behavior has surprised me, it significantly harms performance, and I don't know how to resolve it in a way that doesn't require a major rewrite. Please help.",worth able reproduce original poster problem behavior target block size set default read parquet file printing ray call instead one large batch get many apiece big according increasing target block size change behavior strangely call value total number indeed get one large batch big well target size also get batch size use next iter behavior significantly performance know resolve way require major rewrite please help,issue,positive,positive,positive,positive,positive,positive
1897656028,We don't do it on the base otherwise we'll miss wanda cache,base otherwise miss cache,issue,negative,negative,negative,negative,negative,negative
1897634716,"> Hmm if it always oom fail, it won't test anything?

It still test things since all the assertions are still there.",always fail wo test anything still test since still,issue,negative,negative,negative,negative,negative,negative
1897624505,"The check of the result code is sufficient see promethus api doc:
https://prometheus.io/docs/prometheus/latest/management_api/ 
which make it then also compatible with other.
Make the string compare optional would be the best.",check result code sufficient see doc make also compatible make string compare optional would best,issue,positive,positive,positive,positive,positive,positive
1897607670,"We only run this once a week so let's wait until this weekend and see what happens
",run week let wait weekend see,issue,negative,neutral,neutral,neutral,neutral,neutral
1897553440,"@ericl it seems like after the FT PR, the segfault happens in normal ray workloads too, not just from compiled DAG",like normal ray dag,issue,negative,positive,positive,positive,positive,positive
1897544298,"@bveeramani 
Not exactly, sorry for misunderstanding.
My case is that: I do two `map_batches` in one python function, and when the first `map_batches` complete, I expect the resource(actors required) should be free up, so the next `map_batches` could reuse them.
I mean for example, I have 10 cpus, and the first `map_batches` required 6 and the second also require 6, in my thought, they could work with 10 cpus, but actually not, the first `map_batches` will not release resources(cpus) until the whole function exited, is the behavior by design?",exactly sorry misunderstanding case two one python function first complete expect resource free next could reuse mean example first second also require thought could work actually first release whole function behavior design,issue,negative,positive,neutral,neutral,positive,positive
1897535879,"if this works, will cut the size of the build context in half

we can probably remove much more if we have an explicit declaration of the closure of source files required by ray.",work cut size build context half probably remove much explicit declaration closure source ray,issue,negative,positive,neutral,neutral,positive,positive
1897428111,Easy to repro with ray job submit. I don't have a more detailed repro.,easy ray job submit detailed,issue,negative,positive,positive,positive,positive,positive
1897257185,"Seems `__all__` may be added here in `Learner.compile_results`:
https://github.com/ray-project/ray/blob/40223ff75a31c4c3fc490923f9578964102cbc70/rllib/core/learner/learner.py#L752

I'm not really sure what __all__ is suppsoed to be for so I'm not sure where the right place to filter it out is.
CC @sven1977 ",may added really sure sure right place filter,issue,positive,positive,positive,positive,positive,positive
1897230473,This is ready for review. Almost all of the LoC changed are added tests.,ready review almost added,issue,negative,positive,positive,positive,positive,positive
1896674573,"@n3011 Is it possible to add a small test to confirm that this PR fixes the failure scenario you're running into? Thanks!

https://github.com/ray-project/ray/blob/master/python/ray/train/tests/test_windows.py
",possible add small test confirm failure scenario running thanks,issue,negative,negative,neutral,neutral,negative,negative
1896465936,"Not honestly sure. I know `__del__` is fragile, and it can lead to some janky interactions.",honestly sure know fragile lead,issue,positive,positive,positive,positive,positive,positive
1896454845,"> Thanks for the quick fix! Anywhere we could add a code example? Either in docstring itself, or in our documentation?

There's a code example in the `FilenameProvider` reference: https://docs.ray.io/en/latest/data/api/doc/ray.data.datasource.FilenameProvider.html#ray.data.datasource.FilenameProvider",thanks quick fix anywhere could add code example either documentation code example reference,issue,negative,positive,positive,positive,positive,positive
1896259034,"> Do we currently have a doc about this feature?

There is a doc in Serve: https://docs.ray.io/en/latest/serve/advanced-guides/multi-app-container.html",currently doc feature doc serve,issue,negative,neutral,neutral,neutral,neutral,neutral
1896158485,"yess, serve has been running python 3.11 tests in CI for a while; this PR just adds 3.8 and migrate the existing 3.11 jobs to the new wanda config",serve running python migrate new,issue,negative,positive,positive,positive,positive,positive
1896069768,This should be put somewhere in docs. I find the default permissions too open for a production setting. ,put somewhere find default open production setting,issue,negative,neutral,neutral,neutral,neutral,neutral
1895641836,@rkooo567 hey，would you be able to take some time to review this pr?,able take time review,issue,negative,positive,positive,positive,positive,positive
1895613156,"missing async method, I will implement it later",missing method implement later,issue,negative,negative,neutral,neutral,negative,negative
1895585160,"Fixed the PR description, which still had a wrong link. The correct one is:
https://docs.ray.io/en/latest/rllib/rllib-rlmodule.html
",fixed description still wrong link correct one,issue,negative,negative,negative,negative,negative,negative
1895584262,"Hey @angelinalg , could you approve as well?",hey could approve well,issue,negative,neutral,neutral,neutral,neutral,neutral
1895525969,Using that exact docker-compose file on another identical (nvidia dgx) host works,exact file another identical host work,issue,negative,positive,positive,positive,positive,positive
1895494398,"## Demo

We must decorate all methods with @remote_method

And we must construct a new actor class by the classmethod `new_actor`

![2c56743f-7378-4e43-a599-887798cb70a1](https://github.com/ray-project/ray/assets/38552291/3cdbbf89-5ee2-472d-87be-e5b73f55b610)

## Type hints
### 1. Constructor type annotation
<img width=""1269"" alt=""image"" src=""https://github.com/ray-project/ray/assets/38552291/2fc0c480-3de6-43d4-997c-69c5b4246952"">

### 2. methods
<img width=""943"" alt=""image"" src=""https://github.com/ray-project/ray/assets/38552291/91c63e75-1ce3-413a-a6ec-06e3f890f0c8"">

### 3. remote method
<img width=""1166"" alt=""image"" src=""https://github.com/ray-project/ray/assets/38552291/0275f876-db19-4b50-b53f-44a515024f6d"">

### 3. remote method with kwarg
<img width=""876"" alt=""image"" src=""https://github.com/ray-project/ray/assets/38552291/5a41e93a-1ebb-4821-bb14-a806bd29bd6c"">

## Declare default ray.remote options
<img width=""1053"" alt=""image"" src=""https://github.com/ray-project/ray/assets/38552291/9765320f-7bd6-4b34-bbf5-18370a29856c"">

If set the wrong option type, we will get the error type hint
<img width=""1707"" alt=""image"" src=""https://github.com/ray-project/ray/assets/38552291/508c74ad-02a5-4aaa-90ae-235d863e7745"">
",must decorate must construct new actor class type constructor type annotation image image remote method image remote method image declare default image set wrong option type get error type hint image,issue,negative,negative,negative,negative,negative,negative
1895374379,"> Thanks for the fix.
> 
> A few CI tests failed but it looks likely unrelated. Let me try running them again.

Yes, likely those are unrelated, but tests are however somehow connected to Pydantic. Let me know if you find something about the root cause.",thanks fix likely unrelated let try running yes likely unrelated however somehow connected let know find something root cause,issue,positive,positive,neutral,neutral,positive,positive
1895361170,"@justinvyu  Hi Justin, that particular option does not work for forks hosted by organizations (see: https://github.com/orgs/community/discussions/5634 ), but we just added you to the fork/repository to enable you to push the changes you made.",hi particular option work see added enable push made,issue,negative,positive,positive,positive,positive,positive
1895121750,"Just curious, how does python quit when ray is still not None?",curious python quit ray still none,issue,negative,negative,neutral,neutral,negative,negative
1894843171,"> It'd be great if you create a new issue with a repro script. We can investigate it shortly if you create an issue and tag me!

Cool, I will try to share the manifest that creates the devcontainer and ray. Thanks!",great create new issue script investigate shortly create issue tag cool try share manifest ray thanks,issue,positive,positive,positive,positive,positive,positive
1894790467,"@Sayam753 Could you pass a ""model factory"" into the Ray task instead of the initialized model?

Example:
```python
import ray

@ray.remote
def task(model_factory):
    model = model_factory()

model_factory = lambda: ...
ray.get(task.remote(model_factory))
```
",could pas model factory ray task instead model example python import ray task model lambda,issue,negative,neutral,neutral,neutral,neutral,neutral
1894784079,"@n3011 @jbedorf Is it possible to ""give maintainers permission to edit"" the PR (should be some setting in the PR)? I've replaced `os.path.join` in many other place sin the code.",possible give permission edit setting many place sin code,issue,negative,positive,positive,positive,positive,positive
1894739359,You can degrade your fastapi to a low version such as `pip install fastapi==0.104.1` which uses a version of starlette  <= `0.34.0`,degrade low version pip install version,issue,negative,neutral,neutral,neutral,neutral,neutral
1894706146,"Rename the `get_running` to `get_non_terminated` and add `is_running` flag in the `CloudInstance` data structure, since we should also be retrieving instances not running yet. 

It's unfortunate the current node provider interface doesn't define explicitly what states the cloud node might be, so we could only include the boolean flag. ",rename add flag data structure since also running yet unfortunate current node provider interface define explicitly cloud node might could include flag,issue,negative,negative,negative,negative,negative,negative
1894697823,"Adding on to this, here's the information I'd like to see:
* Input queue size (in bytes as a stacked line graph, and in blocks as a regular line graph)
* Output queue size (in bytes as a stacked line graph, and in blocks as a regular line graph)
* Cycle time (in seconds as a histogram)
* Output size (in bytes as a histogram)
* Number of outputs (as a histogram)",information like see input queue size line graph regular line graph output queue size line graph regular line graph cycle time histogram output size histogram number histogram,issue,negative,neutral,neutral,neutral,neutral,neutral
1894694154,"Hey @vincent-pli, if I'm understanding correctly, you're saying that actors persist on the cluster after your program exits?",hey understanding correctly saying persist cluster program,issue,negative,neutral,neutral,neutral,neutral,neutral
1894693836,we'll need this matrix system for all tests yes; it would be nice to have a more native way to declare these matrixes ,need matrix system yes would nice native way declare,issue,positive,positive,positive,positive,positive,positive
1894624808,"If the issue is important to you, it would be great if you could open an github issue with it as a feature request.  If it turns out to be an important feature for many users, we may be able to prioritize it.  So far we haven't heard from users requesting this feature.  

Unfortunately there isn't a one-to-one mapping between Ray Client and Ray Jobs.  In your example, if you were using Ray Client you would likely wrap `fibonacci` in a Ray task (otherwise `fibonacci` would run on the client machine).  Putting the computation in a Ray task (with `@ray.remote`) is the recommended approach for Ray jobs as well.",issue important would great could open issue feature request turn important feature many may able far feature unfortunately ray client ray example ray client would likely wrap ray task otherwise would run client machine computation ray task approach ray well,issue,positive,positive,positive,positive,positive,positive
1894610004,It'd be great if you create a new issue with a repro script. We can investigate it shortly if you create an issue and tag me! ,great create new issue script investigate shortly create issue tag,issue,positive,positive,positive,positive,positive,positive
1894606003,"I think that could work, but we also need `#ifndef _WIN32` within code where redis client is imported and used (I believe gcs_server.cc). 

",think could work also need within code client used believe,issue,negative,neutral,neutral,neutral,neutral,neutral
1894595230,"Hi, @ericl @anyscalesam  could you elaborate a bit on this? I don't quite get the problem described by `One limitation of our actor fault tolerance right now is it requires the identical original actor to complete reconstruction. This may be a problem.` Could you kindly point me to the doc/code where it showcases the limitation on why the ray core actor fault tolerance doesn't work for actorpool? ",hi could elaborate bit quite get problem one limitation actor fault tolerance right identical original actor complete reconstruction may could kindly point limitation ray core actor fault tolerance work,issue,negative,positive,positive,positive,positive,positive
1894587445,let me remove the linux prefix when i load the test name from the DB then,let remove prefix load test name,issue,negative,neutral,neutral,neutral,neutral,neutral
1894579264,"> Not sure if there's a good way to just not include redis in this case

There should be a way to tell bazel ""do not add redis_client to the dependencies"" in `BUILD.bazel`, would something like this work?
```
deps = select({
        ""@bazel_tools//src/conditions:windows"": [
            <everything but redis_client>
        ],
        ""//conditions:default"": [
            <everything including redis_client>
        ],
    }),
```",sure good way include case way tell add would something like work select everything default everything,issue,positive,positive,positive,positive,positive,positive
1894571976,"@architkulkarni, for directing me to the scheduling strategies document; I appreciate the insight. But, before coming to the scheduling strategies, I would like to have better understanding regarding different behaviors of 10001 and 8265.

We have been using Ray-Client (10001) for job execution, and we would like to replace it with Ray-Submit (8265). When we set `num-cpus: 0` in `rayStartParams`, Ray-Client handles job executions as expected (automatically) without requiring end-users to explicitly specify resources. We expected a similar behavior with Ray-Submit.

Do you anticipate implementing this feature in Ray-Submit in the future, ensuring that all jobs are executed by worker nodes without relying on external actions?",document appreciate insight coming would like better understanding regarding different job execution would like replace set job automatically without explicitly specify similar behavior anticipate feature future executed worker without external,issue,positive,positive,neutral,neutral,positive,positive
1894568862,`ray::IDLE_Spill` means the system memory is not enough and Ray decides to spill some memory usage to disk. This will make the whole system slower (data being in disk rather than in memory). Did you check your cluster's memory usage?,ray system memory enough ray spill memory usage disk make whole system data disk rather memory check cluster memory usage,issue,negative,positive,neutral,neutral,positive,positive
1894568131,Not sure if there's a good way to just not include redis in this case. I wonder if we don't import any redis related dependency if the platform is windows within code? ,sure good way include case wonder import related dependency platform within code,issue,positive,positive,positive,positive,positive,positive
1894565744,"https://github.com/ray-project/ray/blob/eb19da2add8342a4d34fe3970eb14debbd8f829d/src/ray/gcs/gcs_server/gcs_server.cc#L76

I don't think there's a way to express this in bazel build now, but redis is used only when this condition is met (and in Windows, you always go to IN_MEMORY). ",think way express build used condition met always go,issue,negative,neutral,neutral,neutral,neutral,neutral
1894562033,"Seems to be a transient failure, it passed when I reran it here: https://buildkite.com/ray-project/release/builds/5994 (note `release_images` tests are expected to fail).",transient failure note fail,issue,negative,negative,negative,negative,negative,negative
1894557032,"Unfortunately starlette 0.34.0 is incompatible with the latest fastapi.

```
pip install ""ray[serve]""
pip install ""starlette<=0.34.0""
...
Installing collected packages: starlette
  Attempting uninstall: starlette
    Found existing installation: starlette 0.35.1
    Uninstalling starlette-0.35.1:
      Successfully uninstalled starlette-0.35.1
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
fastapi 0.109.0 requires starlette<0.36.0,>=0.35.0, but you have starlette 0.34.0 which is incompatible.
Successfully installed starlette-0.34.0
```
I'm not sure if this affects Ray's functionality. I am able to successfully run the sample client/server application from [Running a Ray Serve Application](https://docs.ray.io/en/latest/serve/getting_started.html#running-a-ray-serve-application) in this configuration.",unfortunately incompatible latest pip install ray serve pip install collected found installation successfully uninstalled error pip dependency resolver currently take account behaviour source following dependency incompatible successfully sure ray functionality able successfully run sample application running ray serve application configuration,issue,positive,positive,positive,positive,positive,positive
1894549147,"Currently there's no way to configure the behavior during deployment. Even ignoring this feature, if the end user has access to Ray job submission, they can always schedule Ray tasks and actors to run on the head node, for example by using https://docs.ray.io/en/latest/ray-core/api/doc/ray.util.scheduling_strategies.NodeAffinitySchedulingStrategy.html in their Ray script to make it run on the head node. ",currently way configure behavior deployment even feature end user access ray job submission always schedule ray run head node example ray script make run head node,issue,negative,neutral,neutral,neutral,neutral,neutral
1894528915,"Thank you, @architkulkarni, for the response. Is there a way to configure this behaviour during deployment to ensure it works for all job submissions? Currently, with this approach, relying on every individual end-user to include specific configuration for job execution creates complexity and exposes the head node to potential risks.",thank response way configure behaviour deployment ensure work job currently approach every individual include specific configuration job execution complexity head node potential,issue,positive,neutral,neutral,neutral,neutral,neutral
1894524388,"@architkulkarni can you please advise me, and let me know the information that helps us.",please advise let know information u,issue,negative,neutral,neutral,neutral,neutral,neutral
1894510909,"Thanks for the question! By default, the job entrypoint script runs on the head node. 0 cpus are allocated to the entrypoint script by default, so it runs on the head node even if the head node has 0 cpus.

To force the entrypoint script to run on a worker node, you would indeed need to set `entrypoint_num_cpus` as you have done.  You can see https://docs.ray.io/en/latest/cluster/running-applications/job-submission/sdk.html#specifying-cpu-and-gpu-resources for more details.

Does that help?",thanks question default job script head node script default head node even head node force script run worker node would indeed need set done see help,issue,positive,positive,positive,positive,positive,positive
1894500815,"@jjyao  and @kevin85421 

As you mentioned I have validated with jobsubmittionclient approach and it works as expected, but, earlier with ray client even for a less compute expected or small jobs execution purpose worker pods are created by the system, but now the worker pods are not creating, instead the job is very small one so it's executing on top of head node. 

can you please let me know reason ? 

",approach work ray client even le compute small execution purpose worker system worker instead job small one top head node please let know reason,issue,negative,neutral,neutral,neutral,neutral,neutral
1894498706,"because the test names in S3/DB have the prefix, but the yaml doesn't have, so I need to support both in order to use the data from S3/DB",test prefix need support order use data,issue,negative,neutral,neutral,neutral,neutral,neutral
1894495894,"@sihanwang41 yess, make sure you are on linux + have python 3.9 in your environment first, thankks",make sure python environment first,issue,negative,positive,positive,positive,positive,positive
1894491469,"Hi @can-anyscale @aslonnie, PTAL.
One question: should i manually run `bazel run //release:requirements_byod_3.9.update` to update release/ray_release/byod/requirements_byod_3.9.txt by myself?",hi one question manually run run update update,issue,negative,neutral,neutral,neutral,neutral,neutral
1894382722,"Great, thanks! Look forward to the fix :) ",great thanks look forward fix,issue,positive,positive,positive,positive,positive,positive
1894381204,"@edoakes yeah, sorry, i haven't added the test with prometheus in the loop, but i did validate that it's working.

I'm gonna update one more test with prometheus in the loop so that we have it running continuously",yeah sorry added test loop validate working gon na update one test loop running continuously,issue,positive,negative,negative,negative,negative,negative
1894224246,"> premerge fails due to some broken tests on master

why there are broken tests on master?",due broken master broken master,issue,negative,negative,negative,negative,negative,negative
1894179458,"Hi @cclauss and folks, it's exciting to see the community enthusiasm for Python 3.12. We are still behind on this. At this point, we are focusing on supporting ray-ml for python 3.11 (as you might know, ray supports the release of 3 artifacts ray, ray docker and ray-ml docker, and we are way too overdue for the ray-ml community), which is planned for 2.11 the latest (https://github.com/ray-project/ray/issues/42343). The plan is to look into Python 3.12 after that. CC: @richardliaw ",hi exciting see community enthusiasm python still behind point supporting python might know ray release ray ray docker docker way overdue community latest plan look python,issue,positive,positive,positive,positive,positive,positive
1894173552,I don't work on Ray Core. I assume that you have to recreate. Will defer to @rickyyx  @rkooo567 to confirm.,work ray core assume recreate defer confirm,issue,negative,neutral,neutral,neutral,neutral,neutral
1894139598,Does this actually work? I believe the reason it currently only accepts tags declared in the constructor is because that's how Prometheus metrics work (you can't dynamically add keys). The tests are mocking out the actual metric component so they aren't verifying that this behavior actually gets propagated all the way to the prometheus metrics scraping endpoint.,actually work believe reason currently declared constructor metric work ca dynamically add actual metric component behavior actually way metric scraping,issue,negative,neutral,neutral,neutral,neutral,neutral
1894036095,This is completely failing on premerge/postmerge so I move it to flaky to unblock.,completely failing move flaky unblock,issue,negative,positive,neutral,neutral,positive,positive
1893864427,"> is it possible to just make it build mocked version if it is used for Windows?

If `redis_client` is never needed for windows, it would be better to redo `BUILD.bazel` to include it only when building on non-windows. Is there a way to express this in bazel?",possible make build version used never would better redo include building way express,issue,negative,positive,positive,positive,positive,positive
1893848208,I'm assuming its currently not possible to do so. Can you suggest if the only way is to tear down and recreate? @scottsun94 Thank you!,assuming currently possible suggest way tear recreate thank,issue,negative,neutral,neutral,neutral,neutral,neutral
1893832780,"> It is only used when GCS HA

What is the name of GCS HA in bazel terms? In other words: where is this non-windows-only dependency expressed in the bazel build?",used ha name ha dependency expressed build,issue,positive,neutral,neutral,neutral,neutral,neutral
1893542627,"I am getting the same issue with `ray==2.7.1, grpcio==1.59.2, python==3.11.5` on Ubuntu 20.04. Even if I specify the number of cpu as and gpus in `ray.init`, the function call still hangs.",getting issue even specify number function call still,issue,negative,neutral,neutral,neutral,neutral,neutral
1893424720,"HI @architkulkarni 

Just adding note when i'm doing curl i can able to pull the bucket data from my git bash CLI local widows system.
can you please look into this. see the ouput file lists

```
$ curl -O https://air-example-data-2.s3.us-west-2.amazonaws.com/10G-xgboost-data.parquet/8034b2644a1d426d9be3bbfa78673dfa_000000.parquet --insecure
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 39.2M  100 39.2M    0     0  2184k      0  0:00:18  0:00:18 --:--:-- 3080k

$ ls -lrt
total 40208
-rw-r--r-- 1 user1049089      600 Jan 16 16:49 xgboost_submit.py
-rw-r--r-- 1 user 1049089 41166379 Jan 16 17:03 8034b2644a1d426d9be3bbfa78673dfa_000000.parquet

$ curl -O https://air-example-data-2.s3.us-west-2.amazonaws.com/100G-xgboost-data.parquet/8034b2644a1d426d9be3bbfa78673dfa_000000.parquet --insecure
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   344    0   344    0     0    124      0 --:--:--  0:00:02 --:--:--   124


$ ls -lrt
total 8
-rw-r--r-- 1 user 1049089 600 Jan 16 16:49 xgboost_submit.py
-rw-r--r-- 1 user 1049089 344 Jan 16 17:04 8034b2644a1d426d9be3bbfa78673dfa_000000.parquet
```

Path1 10G 
```
curl -O https://air-example-data-2.s3.us-west-2.amazonaws.com/10G-xgboost-data.parquet/8034b2644a1d426d9be3bbfa78673dfa_000000.parquet --insecure
```

Path 2 100G
```
curl -O https://air-example-data-2.s3.us-west-2.amazonaws.com/100G-xgboost-data.parquet/8034b2644a1d426d9be3bbfa78673dfa_000000.parquet --insecure
```",hi note curl able pull bucket data git bash local system please look see file curl insecure total received average speed time time time current total spent left speed total user user curl insecure total received average speed time time time current total spent left speed total user user path curl insecure path curl insecure,issue,negative,negative,neutral,neutral,negative,negative
1893418677,"Fixed some tests. Just waiting for everything to pass, then merge.",fixed waiting everything pas merge,issue,negative,positive,neutral,neutral,positive,positive
1893194695,"Figured out the issue:
https://github.com/ray-project/ray/blob/3f7f1b52c840d6780fd89ef9bc125ff96eb68e52/python/ray/data/dataset.py#L5182-L5184

`ray` becomes `None` between the `ray is not None` and `ray.is_initialized()` evaluations. So, `ray.is_initialized()` raises a `TypeError`. You can confirm this behavior by making the following change:

```python
        if ray is not None:
            time.sleep(1)
            print(ray)
            if ray.is_initialized():
                self._current_executor.shutdown()
```

```
<module None from None>
```

You can't catch the error because the error happens as Python is quitting.",figured issue ray becomes none ray none confirm behavior making following change python ray none print ray module none none ca catch error error python,issue,positive,neutral,neutral,neutral,neutral,neutral
1893174067,"Thanks! Was able to narrow it down to this reproduction:

```python
import deepchem as dc  # For some reason I need this to reproduce the error.

import ray

ds = ray.data.read_csv(""smiles.csv"")

for i, batch in enumerate(ds.iter_batches()):
    print(i)
```",thanks able narrow reproduction python import reason need reproduce error import ray batch enumerate print,issue,negative,positive,positive,positive,positive,positive
1893091747,"> @Superskyyy does it happen in the latest master?

Hi @rkooo567, yes, I use a nightly build for python3.10. 

more context: I'm developing on ray, so I enabled the symbolic links on latest master branch from the latest nightly build. (shouldn't matter right?) 

Environment is bookworm python3.10, a pretty standard devcontainer. Everytime I do ray stop it leaves a bunch of zombie processes.",happen latest master hi yes use nightly build python context ray symbolic link latest master branch latest nightly build matter right environment bookworm pretty standard ray stop leaf bunch zombie,issue,positive,positive,positive,positive,positive,positive
1893085100,is it possible to just make it build mocked version if it is used for Windows? ,possible make build version used,issue,negative,neutral,neutral,neutral,neutral,neutral
1893084594,@mattip everything will work without redis actually. It is only used when GCS HA is enabled (which is not supported in windows anyway). ,everything work without actually used ha anyway,issue,negative,neutral,neutral,neutral,neutral,neutral
1893080112,"Is that output from lsof? Also, do you use the latest stable version (2.9)?",output also use latest stable version,issue,negative,positive,positive,positive,positive,positive
1893079263,"@ilml I think that's a known issue, and it must've been fixed in the latest version Ray! ",think known issue must fixed latest version ray,issue,negative,positive,positive,positive,positive,positive
1893077928,I strongly recommend you to set every port manually when you deploy Ray https://docs.ray.io/en/master/ray-core/configure.html#ports-configurations to avoid port conflict. ,strongly recommend set every port manually deploy ray avoid port conflict,issue,negative,positive,positive,positive,positive,positive
1892942426,"This unfortunate conflict problem is often encountered when running multiple ray programs at the same time. Is there any way to avoid it? How do I assign different ports to different programs? Thank you!

[2024-01-16, 03:29:11 CST] {logging_mixin.py:137} INFO - ValueError: Ray component dashboard_agent_http is trying to use a port number 52365 that is used by other components.
[2024-01-16, 03:29:11 CST] {logging_mixin.py:137} INFO - Port information: {'gcs': 'random', 'object_manager': 'random', 'node_manager': 'random', 'gcs_server': 60025, 'client_server': 'random', 'dashboard': 'random', 'dashboard_agent_grpc': 52365, 'dashboard_agent_http': 52365, 'dashboard_grpc': 'random', 'metrics_export': 51216, 'redis_shards': 'random', 'worker_ports': 'random'}
[2024-01-16, 03:29:11 CST] {logging_mixin.py:137} INFO - If you allocate ports, please make sure the same port is not used by multiple components.",unfortunate conflict problem often running multiple ray time way avoid assign different different thank ray component trying use port number used port information allocate please make sure port used multiple,issue,negative,neutral,neutral,neutral,neutral,neutral
1892379477,"yup you're basically just avoiding the race condition by doing that, it can still happen just with a low % chance",basically race condition still happen low chance,issue,negative,neutral,neutral,neutral,neutral,neutral
1892338611,"@Phirefly9 Thanks for the response,

I have been reading about it as well. For us it is because we run multiple tune runs at the same time on the same machine. Setting the ""TUNE_GLOBAL_CHECKPOINT_S"" variable in our bash file to a high number seems to have fix it because it crashed on an error produced by the TuneController (tune_controller.py) saving the experiment state. Setting this variable to a high number (we just do one year of seconds) results in the experiment state not being saved after one save. Which means they wont intefere with eachother anymore. This of course is only possible if you do not need the expirement state later on.

For us it is fine because we have custom checkpointing and for now we don't need to pause or resume a tune run.

Hope it helps.",thanks response reading well u run multiple tune time machine setting variable bash file high number fix error produced saving experiment state setting variable high number one year experiment state saved one save wont course possible need state later u fine custom need pause resume tune run hope,issue,positive,positive,positive,positive,positive,positive
1892273758,"I persistently see zombie processes (defunct) within my devcontainer when starting ray on an alternative port (idk if its relevant info). it says:

```
Stopped only 0 out of 6 Ray processes within the grace period 16 seconds. Set `-v` to see more details. Remaining processes [psutil.Process(pid=16373, name='raylet', status='zombie', started='09:20:36'), psutil.Process(pid=16255, name='python', status='zombie', started='09:20:35'), psutil.Process(pid=16374, name='python', status='zombie', started='09:20:36'), psutil.Process(pid=16256, name='python', status='zombie', started='09:20:35'), psutil.Process(pid=16254, name='python', status='zombie', started='09:20:35'), psutil.Process(pid=16175, name='gcs_server', status='zombie', started='09:20:34')] will be forcefully terminated.
You can also use `--force` to forcefully terminate processes or set higher `--grace-period` to wait longer time for proper termination.
```",persistently see zombie defunct within starting ray alternative port relevant stopped ray within grace period set see forcefully also use force forcefully terminate set higher wait longer time proper termination,issue,negative,positive,positive,positive,positive,positive
1892266332,"my current hypothesis is that recently ray/rllib made some changes that made it a lot more difficult to use a shared filesystem

1. _temp_dir will now collide on a shared file system, it is now 100% required to change the _temp_dir `per training run` or else any instances of a ray cluster will collide.  if you are in a situation like me using an HPC the ray init on a node will look into the _temp_dir, see the `current_session` file, and then attempt to connect to an existing ray cluster, hanging and eventually crashing.  this is very not ideal

edit to add more to this: the `current_session` file is also very annoying because if ray is not cleanly shut down, the file will continue to exist on the filesystem, breaking any attempt to start a ray cluster until it is deleted

2. this issue, I think what's happening is that rllib is making  `<storage_path>/.tmp_generator` instead of saving .tmp_genenerator in a location specific to a job, as `<storage_path>` is shared between any running jobs, this can collide between multiple runs, causing incorrect data for the first job, and a race condition for the second",current hypothesis recently made made lot difficult use collide file system change per training run else ray cluster collide situation like ray node look see file attempt connect ray cluster hanging eventually ideal edit add file also annoying ray cleanly shut file continue exist breaking attempt start ray cluster issue think happening making instead saving location specific job running collide multiple causing incorrect data first job race condition second,issue,negative,positive,neutral,neutral,positive,positive
1892191108,@Phirefly9 I'm running into the same issue. And yes it happens when we run multiple tune runs at the same time. Still this shouldn't be a problem right? I want that all my runs go to the same path. How can we fix this issue? :),running issue yes run multiple tune time still problem right want go path fix issue,issue,negative,positive,positive,positive,positive,positive
1892141120,How to cancel a task if I lose the ref object?,cancel task lose ref object,issue,negative,neutral,neutral,neutral,neutral,neutral
1892068333,"This does not work properly IMHO, even if you specify all the correct ip addresses, such as:
`ray start --head --node-ip-address=10.100.100.2 --num-cpus=1 --num-gpus=0 --dashboard-host=10.100.100.2 `
You still get bindings on `0.0.0.0`, which should not be there:
```
tcp        0      0 10.100.100.2:8265       0.0.0.0:*               LISTEN      3438774/python3
tcp        0      0 0.0.0.0:58526           0.0.0.0:*               LISTEN      3438949/python3
tcp        0      0 0.0.0.0:52365           0.0.0.0:*               LISTEN      3438949/python3
tcp        0      0 0.0.0.0:51586           0.0.0.0:*               LISTEN      3438951/python3
tcp        0      0 0.0.0.0:44217           0.0.0.0:*               LISTEN      3438772/python3
tcp        0      0 0.0.0.0:44227           0.0.0.0:*               LISTEN      3438774/python3
tcp6       0      0 :::60172                :::*                    LISTEN      3438949/python3
tcp6       0      0 :::41095                :::*                    LISTEN      3438774/python3
tcp6       0      0 :::40373                :::*                    LISTEN      3438891/raylet
tcp6       0      0 :::37531                :::*                    LISTEN      3438891/raylet
tcp6       0      0 :::10001                :::*                    LISTEN      3438773/python3
tcp6       0      0 :::6379                 :::*                    LISTEN      3438669/gcs_server
```
",work properly even specify correct ray start head still get listen listen listen listen listen listen listen listen listen listen listen listen,issue,negative,neutral,neutral,neutral,neutral,neutral
1892057768,"> > This wasn't fixed in the master. It happens because once in 100 times, the port randomly selected for agent & metrics conflict. We need to avoid choosing a random port when it is already assigned to sth else
> 
> In our application, this seems to occur with more than a 1% probability. It has happened multiple times in the past year and a simple restart cannot solve it. We use Docker to deploy nodes, strangely there is always the same port conflict every time restarts.

> a simple restart cannot solve it
I'm encountering the same thing, it happens when a worker group starts to scale up, the master group works fine. What do you mean by simple re-start? Restart of what? 
",fixed master time port randomly selected agent metric conflict need avoid choosing random port already assigned else application occur probability multiple time past year simple restart solve use docker deploy strangely always port conflict every time simple restart solve thing worker group scale master group work fine mean simple restart,issue,negative,negative,neutral,neutral,negative,negative
1891738302,"I also get this error when I try to update the app (the cluster shows that the application itself is running, but the controller and proxy on the head node are not running):
```bash
WARNING worker.py:2052 -- The autoscaler failed with the following error:
5Terminated with signal 15
6  File ""/usr/local/lib/python3.11/site-packages/ray/autoscaler/_private/monitor.py"", line 711, in <module>
7    monitor.run()
8  File ""/usr/local/lib/python3.11/site-packages/ray/autoscaler/_private/monitor.py"", line 586, in run
9    self._run()
10  File ""/usr/local/lib/python3.11/site-packages/ray/autoscaler/_private/monitor.py"", line 440, in _run
11    time.sleep(AUTOSCALER_UPDATE_INTERVAL_S)
```",also get error try update cluster application running controller proxy head node running bash warning following error signal file line module file line run file line,issue,negative,neutral,neutral,neutral,neutral,neutral
1891172271,"Here is a script to generate `smiles.csv` (also added in the reproduction script)

```py
import csv

with open('smiles.csv', 'w', newline='') as csvfile:
    fieldnames = ['smiles', 'logp']
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

    writer.writeheader()
    for i in range(1000):
        writer.writerow({'smiles': 'Cc1[nH]ccc1C(=O)N[C@@H](C)CNC(=O)c1cccc2[nH]ccc21', 'logp': 2})
```",script generate also added reproduction script import open writer range,issue,negative,neutral,neutral,neutral,neutral,neutral
1891089611,"Discussed with @jjyao offline. We agreed that it might be better to instead of passing a `min_nodes_shape` to the the node provider, callers should just inform the node providers what to launch. e.g.:

```
def launch(shape) -> launch some nodes
def terminate(ids) -> terminate nodes
def get_running() -> all running cloud instances info
def poll_errors() -> retrieve the errors from the last poll. 
```

The main reason is:
- This would make the node provider a simpler layer since it would not need to compute what to launch from the current shape it tracks, and the desired min shape it is given.  This doesn't add additional overheads to the caller since the caller is expected to do some computation to derive a desired min shape (which it also should have the delta, i.e. what to launch). This would simplify the node provider, and centralize the cluster shape calculation at the instance manager. 
- The v1 node provider already works with this semantics, so it would be easier to integrate. 

cc @scv119 ",agreed might better instead passing node provider inform node launch launch shape launch terminate terminate running cloud retrieve last poll main reason would make node provider simpler layer since would need compute launch current shape desired min shape given add additional caller since caller computation derive desired min shape also delta launch would simplify node provider centralize cluster shape calculation instance manager node provider already work semantics would easier integrate,issue,positive,positive,positive,positive,positive,positive
1890981413,"Thanks much for sharing, saved a ton of time.",thanks much saved ton time,issue,positive,positive,positive,positive,positive,positive
1890933695,"Hi @architkulkarni 

As you mentioned i have update ray image version to latest now i,m getting issue with s3 bucket access, https://github.com/ray-project/ray/blob/releases/2.0.0/release/air_tests/air_benchmarks/workloads/xgboost_benchmark.py 

```

$ kubectl get rayclusters -n xgboost xgboost-kuberay

NAME              AGE
xgboost-kuberay   6h20m


$ kubectl describe rayclusters -n xgboost xgboost-kuberay

request
Name:         xgboost-kuberay
Namespace:    xgboost
Labels:       app.kubernetes.io/instance=xgboost
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/name=kuberay
              helm.sh/chart=ray-cluster-0.4.0
Annotations:  meta.helm.sh/release-name: xgboost
              meta.helm.sh/release-namespace: xgboost


$ kubectl get pods --selector=ray.io/cluster=xgboost-kuberay -n xgboost
NAME                         READY   STATUS    RESTARTS   AGE
xgboost-kuberay-head-7ldrl   2/2     Running   0          6h25m

$ 

Name:             xgboost-kuberay-head-7ldrl
Namespace:        xgboost
Priority:         0
Service Account:  xgboost-kuberay
Node:             ip-10.1.2.3.us-east-2.compute.internal/10.1.2.3
Start Time:       Sun, 14 Jan 2024 09:04:06 +0530
Labels:           app.kubernetes.io/created-by=kuberay-operator
                  app.kubernetes.io/instance=xgboost
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=kuberay
                  helm.sh/chart=ray-cluster-0.4.0
                  ray.io/cluster=xgboost-kuberay
                  ray.io/cluster-dashboard=xgboost-kuberay-dashboard
                  ray.io/group=headgroup
                  ray.io/identifier=xgboost-kuberay-head
                  ray.io/is-ray-node=yes
                  ray.io/node-type=head
Annotations:      ray.io/ft-enabled: false
                  ray.io/health-state:
Status:           Running
API Version:  ray.io/v1alpha1

 Limits:
      cpu:                14
      ephemeral-storage:  700Gi
      memory:             54Gi
    Requests:
      cpu:                14
      ephemeral-storage:  700Gi
      memory:             54Gi
Kind:         RayCluster
```

```
Job entrypoint command failed with exit code 1, last available logs (truncated to 20,000 chars):

Traceback (most recent call last):
  File ""ray/release/air_tests/air_benchmarks/workloads/xgboost_benchmark.py"", line 191, in <module>
    main(args)
  File ""ray/release/air_tests/air_benchmarks/workloads/xgboost_benchmark.py"", line 150, in main
    training_time = run_xgboost_training(data_path, num_workers, cpus_per_worker)
  File ""ray/release/air_tests/air_benchmarks/workloads/xgboost_benchmark.py"", line 74, in wrapper
    raise p.exception
OSError: Failing to read AWS S3 file(s): ""air-example-data-2/100G-xgboost-data.parquet"". Please check that file exists and has properly configured access. You can also run AWS CLI command to get more detailed error message (e.g., aws s3 ls <file-name>). See https://awscli.amazonaws.com/v2/documentation/api/latest/reference/s3/index.html and https://docs.ray.io/en/latest/data/creating-datasets.html#reading-from-remote-storage for more information.
``` ",hi update ray image version latest getting issue bucket access get name age describe request name get name ready status age running name priority service account node start time sun false status running version memory memory kind job command exit code last available truncated recent call last file line module main file line main file line wrapper raise failing read file please check file properly access also run command get detailed error message see information,issue,negative,positive,positive,positive,positive,positive
1890918789,"Hello @jjyao , Good day and hope you are well. Could you please update on the above? Thank you very much for your reply!!",hello good day hope well could please update thank much reply,issue,positive,positive,positive,positive,positive,positive
1890780369,"Hey @arunppsg, thanks for opening this! Would you mind sharing `smiles.csv` so that we can reproduce this? Or, could you reproduce this with publicly available data?",hey thanks opening would mind reproduce could reproduce publicly available data,issue,negative,positive,positive,positive,positive,positive
1890600361,"> Why are you cloning the Ray repo at runtime? That's not recommended, Ray should already be installed on all nodes.

Ray already installed as part of helm chat inside my eks cluster you can refer the above pod status.
coming to the clone - clone option is defined inside the xgboost.py file its pre-defined i didn't changed anything.",ray ray already ray already part helm chat inside cluster refer pod status coming clone clone option defined inside file anything,issue,negative,neutral,neutral,neutral,neutral,neutral
1890464933,"As mentioned above, the assertion makes it impossible to use websockets with Ray, despite the FastAPI back-end being capable of supporting it. Should we raise a new issue for this? I believe this issue should be reopened because it is not possible to use Ray Serve for websocket endpoints right now.",assertion impossible use ray despite capable supporting raise new issue believe issue possible use ray serve right,issue,negative,positive,neutral,neutral,positive,positive
1890386579,"@can-anyscale Given Python 3.12's substantial performance improvements in asyncio and several other modules, can we please understand what are the blockers to migrating to Py3.12?

Is Python 3.12 compatibility related to #41373 or are there unrelated todos?",given python substantial performance several please understand python compatibility related unrelated,issue,negative,neutral,neutral,neutral,neutral,neutral
1890221489,"I believe the carriage return already hits the send button but it also inserts a newline in the text box -- probably we want it to only hit the send button, right?",believe carriage return already send button also text box probably want hit send button right,issue,negative,positive,positive,positive,positive,positive
1890189471,"I just ran into the same error, which burned me an hour. Thanks so much for sharing it!

I resolved the issue by installing Starelette first. Below is my `requirements.txt` file:

```
starlette<=0.34.0
ray[serve,data]==2.9.0
```",ran error burned hour thanks much resolved issue first file ray serve data,issue,negative,positive,positive,positive,positive,positive
1890172405,"i plan to run as a job in the postmerge pipeline, in the continuous run",plan run job pipeline continuous run,issue,negative,neutral,neutral,neutral,neutral,neutral
1890069978,Note this could be a core issue too.,note could core issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1890068356,Thanks for the contribution! I assigned some Ray Data folks for codeowner approval.,thanks contribution assigned ray data approval,issue,positive,positive,positive,positive,positive,positive
1890051485,"Seems like a regression and stability issue, so marking p0.

cc @raulchen @franklsf95 ",like regression stability issue marking,issue,negative,neutral,neutral,neutral,neutral,neutral
1889960554,"> REGRESSION 10.41%: single_client_tasks_and_get_batch (THROUGHPUT) regresses from 8.429852592930626 to 7.552591739788697 (10.41%) in 2.9.1/microbenchmark.json

Noise

> REGRESSION 7.08%: placement_group_create/removal (THROUGHPUT) regresses from 845.7511547073977 to 785.8508797030252 (7.08%) in 2.9.1/microbenchmark.json

Noise

> REGRESSION 7.03%: single_client_put_gigabytes (THROUGHPUT) regresses from 20.6372354079233 to 19.186501513119794 (7.03%) in 2.9.1/microbenchmark.json

Noise

> REGRESSION 19.26%: stage_0_time (LATENCY) regresses from 13.148497581481934 to 15.681537866592407 (19.26%) in 2.9.1/stress_tests/stress_test_many_tasks.json

Noise.
",regression throughput noise regression throughput noise regression throughput noise regression latency noise,issue,negative,neutral,neutral,neutral,neutral,neutral
1889888450,"It's hard to know without more details, but it looks like you're trying to run `xgboost_benchmark.py`, and one hypothesis is the script is taken from the master branch of Ray, but the installed Ray version is some stable Ray version which is older than master.  If that's the case, there's a version incompatibility issue which can be resolved by using the `xgboost_benchmark.py` from the release branch branch of the Ray version you're running.",hard know without like trying run one hypothesis script taken master branch ray ray version stable ray version older master case version incompatibility issue resolved release branch branch ray version running,issue,negative,negative,neutral,neutral,negative,negative
1889851603,"Ah, yeah. I was able to reproduce this.

Shouldn't be too hard to fix. ",ah yeah able reproduce hard fix,issue,negative,positive,positive,positive,positive,positive
1889827873,"`write_sql` already supports appends because the user specifies the SQL. 

Not sure what the default behavior is for `write_mongo`, but I haven't heard any user requests related to Mongo, so we can probably defer for that API.",already user sure default behavior user related probably defer,issue,negative,positive,positive,positive,positive,positive
1889677114,"Any progress on this issue? Is the implication that if `num_gpus` is defined, that the associated task is constrained to 1 CPU?",progress issue implication defined associated task constrained,issue,negative,neutral,neutral,neutral,neutral,neutral
1889640222,"# Microbenchmark results (run locally)

## HTTP noop latency

Master:
```
Latency (ms) for noop HTTP requests (num_replicas=1,num_requests=100):
count    100.000000
mean       3.569598
std        0.229940
min        3.265416
50%        3.505709
90%        3.846450
95%        4.068335
99%        4.227305
max        4.389583
```

This PR:
```
Latency (ms) for noop HTTP requests (num_replicas=1,num_requests=100):
count    100.000000
mean       3.626885
std        0.200631
min        3.311250
50%        3.569500
90%        3.900396
95%        3.993731
99%        4.145391
max        4.344958
dtype: float64
```

## Streaming handle throughput

Master:
```
DeploymentHandle streaming throughput (ASYNC) (num_replicas=1, tokens_per_request=1000, batch_size=10): 12632.2 +- 316.83 tokens/s
(ServeReplica:default:CallerDeployment pid=63933) Individual request quantiles:
(ServeReplica:default:CallerDeployment pid=63933)       P50=785.4314794999997
(ServeReplica:default:CallerDeployment pid=63933)       P75=835.9155417499999
(ServeReplica:default:CallerDeployment pid=63933)       P99=916.2086944999998
```

This PR:
```
DeploymentHandle streaming throughput (ASYNC) (num_replicas=1, tokens_per_request=1000, batch_size=10): 12119.74 +- 479.87 tokens/s
(ServeReplica:default:CallerDeployment pid=64031) Individual request quantiles:
(ServeReplica:default:CallerDeployment pid=64031)       P50=795.5596665
(ServeReplica:default:CallerDeployment pid=64031)       P75=907.079750249999
(ServeReplica:default:CallerDeployment pid=64031)       P99=946.3582387200001
```

## Streaming HTTP throughput

Master:
```
HTTP streaming throughput (num_replicas=1, tokens_per_request=1000, batch_size=10, use_intermediate_deployment=False): 247110.96 +- 2480.37 tokens/s
```

This PR:
```
HTTP streaming throughput (num_replicas=1, tokens_per_request=1000, batch_size=10, use_intermediate_deployment=False): 238696.0 +- 5205.73 tokens/s
```",run locally noop latency master latency noop count mean min latency noop count mean min float streaming handle throughput master streaming throughput default individual request default default default streaming throughput default individual request default default default streaming throughput master streaming throughput streaming throughput,issue,negative,negative,negative,negative,negative,negative
1889587832,"Still blocked by #42058 as well.

@salotz just so you're aware, there is PR #42121 awaiting review which will solve this issue.",still blocked well aware review solve issue,issue,negative,positive,positive,positive,positive,positive
1889570903,"I would like to add that I am seeing the same issue as well.
I formed the submission using the python sdk from the ray package, to submit the ray job submission type job using the class `JobSubmissionClient`. I confirmed that the dictionary relating to the `runtime_env` variable only has the entry for `container`.

```
2024-01-12 15:48:20.175 | INFO     | ray.dashboard.modules.job.sdk:submit_job:249 - Job Submission Request: JobSubmitRequest(entrypoint='/home/ray/anaconda3/bin/python /home/ray/entry.py nta-sim', submission_id='9ba5e6dcb2144c29bc8702f0d78ba1e3', job_id=None, runtime_env={'container': {'image': 'rayclusternodes.azurecr.io/mpdsim-1.2.0-ray-2.9.0:latest', 'worker_path': '/root/python/ray/_private/workers/default_worker.py', 'run_options': ['--cap-drop SYS_ADMIN', '--log-level=debug']}}, metadata={'entrypoint_num_cpus': '0', 'entrypoint_num_gpus': '0', 'resource': ""{'headnode-num-cpus': 1}""}, entrypoint_num_cpus=0, entrypoint_num_gpus=0, entrypoint_memory=None, entrypoint_resources={'headnode-num-cpus': 1})
```

However, on submission, the job fails and reports the following:

```
Failed to start supervisor actor 9ba5e6dcb2144c29bc8702f0d78ba1e3: 'The 'container' field currently cannot be used together with other fields of runtime_env. Specified fields: dict_keys(['container', 'env_vars'])'
```



I am running `python 3.10.13` and `ray 2.9.0`.
I followed the recommendation to install `podman` into the standard ray docker image, and then deployed this to a Kubernetes Cluster. The base image used was `rayproject/ray:2.9.0-py310-cpu`.

",would like add seeing issue well formed submission python ray package submit ray job submission type job class confirmed dictionary variable entry container job submission request latest however submission job following start supervisor actor field currently used together running python ray recommendation install standard ray docker image cluster base image used,issue,positive,positive,neutral,neutral,positive,positive
1889421682,"HI @architkulkarni 

I have resolved above issue, followed by i'm getting another error message while submitting job

```
$ ray job logs 'raysubmit_K6fX2B2DGxp6jWzu' --follow --address http://127.0.0.1:8265
Job submission server address: http://localhost:8265
Cloning into 'ray'...
Checking out files:  32% (2418/7358)
Checking out files:  33% (2429/7358)
Checking out files:  34% (2502/7358)
Checking out files:  35% (2576/7358)
Checking out files:  36% (2649/7358)
Checking out files:  37% (2723/7358)
Checking out files:  38% (2797/7358)
Checking out files:  39% (2870/7358)
Checking out files:  40% (2944/7358)
Checking out files:  41% (3017/7358)
Checking out files:  42% (3091/7358)
Checking out files:  43% (3164/7358)
Checking out files:  44% (3238/7358)
Checking out files:  45% (3312/7358)
Checking out files:  46% (3385/7358)
Checking out files:  47% (3459/7358)
Checking out files:  48% (3532/7358)
Checking out files:  49% (3606/7358)
Checking out files:  50% (3679/7358)
Checking out files:  51% (3753/7358)
Checking out files:  52% (3827/7358)
Checking out files:  53% (3900/7358)
Checking out files:  54% (3974/7358)
Checking out files:  55% (4047/7358)
Checking out files:  56% (4121/7358)
Checking out files:  57% (4195/7358)
Checking out files:  58% (4268/7358)
Checking out files:  59% (4342/7358)
Checking out files:  60% (4415/7358)
Checking out files:  61% (4489/7358)
Checking out files:  62% (4562/7358)
Checking out files:  63% (4636/7358)
Checking out files:  64% (4710/7358)
Checking out files:  65% (4783/7358)
Checking out files:  66% (4857/7358)
Checking out files:  67% (4930/7358)
Checking out files:  68% (5004/7358)
Checking out files:  69% (5078/7358)
Checking out files:  70% (5151/7358)
Checking out files:  71% (5225/7358)
Checking out files:  72% (5298/7358)
Checking out files:  73% (5372/7358)
Checking out files:  74% (5445/7358)
Checking out files:  75% (5519/7358)
Checking out files:  76% (5593/7358)
Checking out files:  77% (5666/7358)
Checking out files:  78% (5740/7358)
Checking out files:  79% (5813/7358)
Checking out files:  80% (5887/7358)
Checking out files:  81% (5960/7358)
Checking out files:  82% (6034/7358)
Checking out files:  83% (6108/7358)
Checking out files:  84% (6181/7358)
Checking out files:  85% (6255/7358)
Checking out files:  86% (6328/7358)
Checking out files:  87% (6402/7358)
Checking out files:  88% (6476/7358)
Checking out files:  89% (6549/7358)
Checking out files:  90% (6623/7358)
Checking out files:  91% (6696/7358)
Checking out files:  92% (6770/7358)
Checking out files:  93% (6843/7358)
Checking out files:  94% (6917/7358)
Checking out files:  95% (6991/7358)
Checking out files:  96% (7064/7358)
Checking out files:  97% (7138/7358)
Checking out files:  98% (7211/7358)
Checking out files:  99% (7285/7358)
Checking out files: 100% (7358/7358)
Checking out files: 100% (7358/7358), done.
Traceback (most recent call last):
  File ""ray/release/air_tests/air_benchmarks/workloads/xgboost_benchmark.py"", line 16, in <module>
    from ray.train import RunConfig, ScalingConfig
ImportError: cannot import name 'RunConfig' from 'ray.train' (/home/ray/anaconda3/lib/python3.7/site-packages/ray/train/__init__.py)

---------------------------------------
Job 'raysubmit_K6fX2B2DGxp6jWzu' failed
---------------------------------------

```",hi resolved issue getting another error message job ray job follow address job submission server address done recent call last file line module import import name job,issue,negative,neutral,neutral,neutral,neutral,neutral
1889147685,"I also faced the same issue. To address this problem, I fixed MLflowLoggerCallback. Please check the pull request I created.",also faced issue address problem fixed please check pull request,issue,negative,positive,neutral,neutral,positive,positive
1888881470,"Fixed the issue for macos and linux with #42332.

Keep this issue open to track the windows fix. @mattip could you help with the Windows fix. I tried but don't know how to correctly escape the command. After the windows fix, we should be able to unskip the test for windows.",fixed issue keep issue open track fix could help fix tried know correctly escape command fix able test,issue,positive,positive,positive,positive,positive,positive
1888659208,Test is flaky - kept until fixed.,test flaky kept fixed,issue,negative,positive,neutral,neutral,positive,positive
1888258957,"> TESTS_TO_RUN='pattern'
> 
> is there a way to just utilize this? If this pattern is regex, we can just use it?

it's not a regex pattern yet, but maybe making that a regex pattern is a better idea. ",way utilize pattern use pattern yet maybe making pattern better idea,issue,negative,positive,positive,positive,positive,positive
1888240752,@alanwguo This PR is ready for some early feedback.  The remaining work is listed in the PR description.,ready early feedback work listed description,issue,negative,positive,positive,positive,positive,positive
1888185515,"I updated the PR to soft import `debugpy`

Belowing are the manual tests after adding the soft import for debugpy
breakpoint without `debugpy` installed
<img width=""1641"" alt=""image"" src=""https://github.com/ray-project/ray/assets/18074733/03cc3b62-f474-4ebf-a084-27278410a169"">

breakpoint with `debugpy` installed
<img width=""1646"" alt=""image"" src=""https://github.com/ray-project/ray/assets/18074733/41853e6c-ab44-4bd3-ad4c-c7a728f6150c"">


Post mortem without `debugpy` installed
<img width=""1638"" alt=""image"" src=""https://github.com/ray-project/ray/assets/18074733/97162d2d-d3c0-46aa-9a8d-d8ddfaa10f5b"">

post mortem with `debugpy` installed
<img width=""1643"" alt=""image"" src=""https://github.com/ray-project/ray/assets/18074733/12d541fb-d6d4-4a8c-91a7-cb0540ecabee"">
",soft import manual soft import without image image post without image post image,issue,negative,positive,neutral,neutral,positive,positive
1888164786,"> But it may not be optimal to evenly distribute the memory for stages. Because output sizes of different stages are usually different.

Instead of assigning the available object store memory evenly for each op in the beginning, an alternative approach can probably better deal with this issue. that is, in each scheduling loop iteration, assign the current available object store memory evenly for each op. 

I tried these 2 approaches with `ray-data-resnet50-ingest-file-size-benchmark`, the latter indeed works better. The former has a 30+% perf regression compared with no backpressure, and the latter is only <5%. 

",may optimal evenly distribute memory output size different usually different instead available object store memory evenly beginning alternative approach probably better deal issue loop iteration assign current available object store memory evenly tried latter indeed work better former regression latter,issue,positive,positive,positive,positive,positive,positive
1888096823,"Thanks for isolating the issue! Looks like we override the scheduling strategy if you set `locality_with_output` (even if you've specified a local path):

https://github.com/ray-project/ray/blob/3f7f1b52c840d6780fd89ef9bc125ff96eb68e52/python/ray/data/_internal/execution/operators/map_operator.py#L189-L206

This isn't expected behavior, so we'll need to write a fix",thanks isolating issue like override strategy set even local path behavior need write fix,issue,positive,positive,neutral,neutral,positive,positive
1888096317,"@ray-project/ray-serve I'm planning to add unit tests for `UserCallableWrapper` in this PR before merging (because we can actually do that now 🎉), but PTAL at the general approach.",add unit actually general approach,issue,negative,positive,neutral,neutral,positive,positive
1888020929,"Looks like this (https://github.com/ray-project/ray/commit/f295e9465524d2b68d07324b4e79afc0893e20b2) is where the errors happen (it's flaky, my bisect ran 10 runs on each commit, and this is the commit where things started failing consistently in 10 runs) 

cc @ericl @rkooo567 @stephanie-wang ",like happen flaky bisect ran commit commit failing consistently,issue,negative,positive,positive,positive,positive,positive
1887995099,"This issue I am investigated intermittently appearing, sometimes YES, sometimes NO. 
if we enable entrypoint_num_cpus=1, then the ray cluster spanning the worker-node and the job executing successfully.

 ",issue intermittently sometimes yes sometimes enable ray cluster job successfully,issue,positive,positive,positive,positive,positive,positive
1887980570,"@mantaco1  
I have tried with following approach its working for me

import ray
from ray.job_submission import JobSubmissionClient
import time

# Ray cluster information
ray_head_ip = ""kuberay-head-svc.kuberay.svc.cluster.local""
ray_head_port = 8265
ray_address = f""http://{ray_head_ip}:{ray_head_port}""

# Submit Ray job using JobSubmissionClient
client = JobSubmissionClient(ray_address)
job_id = client.submit_job(
    entrypoint=""python test.py"",
    runtime_env={
        ""working_dir"": ""./"", 
    },
    entrypoint_num_cpus=1
)

but if you didn't mentions the ""entrypoint_num_cpus=1"" then the system again generating the above register issue, intermittently. 

Reference: https://docs.ray.io/en/latest/_modules/ray/dashboard/modules/job/sdk.html#JobSubmissionClient
",tried following approach working import ray import import time ray cluster information submit ray job client python system generating register issue intermittently reference,issue,negative,neutral,neutral,neutral,neutral,neutral
1887893941,"From @edoakes:

This is a big pain point for me as well. This along with not being able to copy-paste effectively from the log viewer cause a lot of pain.",big pain point well along able effectively log viewer cause lot pain,issue,negative,positive,positive,positive,positive,positive
1887893166,"From @rynewang:

+1 on this. My use case is that I am debugging memory issues in Ray, so I followed the Ray document https://docs.ray.io/en/latest/ray-observability/user-guides/debug-apps/debug-memory.html to use `memray` which writes to the worker node's log. I can see the log in the Dash, but it's a ~6GB binary so I can't open it in the log details page. I ended up manually edited the URL to download it but if we have a ""download"" button in the log viewer page that'd be great.

![image](https://github.com/anyscale/product/assets/56065503/f99af145-0949-459f-acf0-3b7eea3f2fef)",use case memory ray ray document use worker node log see log dash binary ca open log page ended manually button log viewer page great image,issue,positive,positive,positive,positive,positive,positive
1887762671,"@rkooo567 ignore this one, i'm using it for experimentation. I'm putting out worthy optimizations as separate PRs and tagged you on the first batch:

 - https://github.com/ray-project/ray/pull/42259
 - https://github.com/ray-project/ray/pull/42260",ignore one experimentation worthy separate tagged first batch,issue,negative,positive,positive,positive,positive,positive
1887737739,"> Nice! Have you tested it with real AMD GPU and make sure the auto-detection works?

Yes.
```
>>> ray.cluster_resources()
{'accelerator_type:AMD-Instinct-MI210': 1.0, 'memory': 243229553664.0, 'node:172.17.0.2': 1.0, 'CPU': 128.0, 'node:__internal_head__': 1.0, 'GPU': 8.0, 'object_store_memory': 15300782080.0}
```
",nice tested real make sure work yes,issue,positive,positive,positive,positive,positive,positive
1887703090,"I manually tested that the task can be paused and it awaits connection from debugger client. Tested tasks running on worker nodes and head node. I also tested post-mortem debugging is working

Manually verified it's e2e working",manually tested task connection client tested running worker head node also tested working manually working,issue,negative,neutral,neutral,neutral,neutral,neutral
1887186803,"We're observing the same problem with Ray 2.9.0, we're running a single trial and the checkpoints end up in /tmp where they're not being deleted despite setting `num_to_keep` to a low number.",observing problem ray running single trial end despite setting low number,issue,negative,negative,neutral,neutral,negative,negative
1886422608,@tvildo I did more investigation and found the root cause and have a fix here: https://github.com/ray-project/ray/pull/42332. Could you try it out and see if it fixes your issue?,investigation found root cause fix could try see issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1886359579,This may be worth adding to the documentation about building on windows.,may worth documentation building,issue,negative,positive,positive,positive,positive,positive
1886359010,"Thanks, adding `startup --output_user_root=d:/tmp` to `~/.bazelrc` did allow the build to complete. Closing.",thanks allow build complete,issue,positive,positive,positive,positive,positive,positive
1886244147,@zcin Thank you for your response and work! I'm looking forward to version 2.10!,thank response work looking forward version,issue,negative,neutral,neutral,neutral,neutral,neutral
1886147015,I'll come to that part of the implementation soon ;),come part implementation soon,issue,negative,neutral,neutral,neutral,neutral,neutral
1886030867,"@rickyyx yes, after I merge this bisect should behave as you expected",yes merge bisect behave,issue,negative,neutral,neutral,neutral,neutral,neutral
1885989242,"Thanks for looking into this, Balaji! I isolated the node affinity scheduling issue in my code and found that it's due to the combined effect of `DataContext.execution_options.locality_with_output = True` and `ds.show()` (or other iter methods i guess). here's the reproduduction code
```python
import ray
ctx = ray.data.DataContext.get_current()
ctx.execution_options.locality_with_output = True

datasource = ray.data.datasource.BinaryDatasource(
    [f""local:///tmp/{i}.dat"" for i in range(10_000)],
)
datasource._supports_distributed_reads = False

ds = ray.data.read_datasource(datasource)
ds.show(3)
```

it would schedule reading task on other nodes and raise `FileNotFound ` error, is it expected behaviour?  ",thanks looking isolated node affinity issue code found due combined effect true iter guess code python import ray true local range false would schedule reading task raise error behaviour,issue,positive,positive,neutral,neutral,positive,positive
1885978286,"It's flaky so I think the passing bound (low end of the bisect) might be even further back. 

We had one failure on f295e9465524d2b68d07324b4e79afc0893e20b2 as well with the similar traceback. 

So I am extending the bisection range further (not sure how much furher) but this 28fdcb6f6b5141a953b5d3a02167d12996f66bfb for now ",flaky think passing bound low end bisect might even back one failure well similar extending bisection range sure much,issue,negative,positive,neutral,neutral,positive,positive
1885973446,"awesome - i will take a look, and thanks for the repro. ",awesome take look thanks,issue,positive,positive,positive,positive,positive,positive
1885952178,"@defrag-bambino let me know if cartpole-v1 is converging with dreamerv3 for you.
Here I'm not getting it working (I'm running the same command like you).

<img width=""1091"" alt=""image"" src=""https://github.com/ray-project/ray/assets/79345/069b83d6-6f09-44e1-a53f-dcc738f83f95"">
",let know converging getting working running command like image,issue,negative,neutral,neutral,neutral,neutral,neutral
1885950087,Removed the test result history length. I'll add it in subsequent PR when it becomes clearer why it needs a bigger history.,removed test result history length add subsequent becomes clearer need bigger history,issue,negative,neutral,neutral,neutral,neutral,neutral
1885943947,yeah let's separate the tech-debt clean up from this,yeah let separate clean,issue,positive,positive,positive,positive,positive,positive
1885931547,"The files do matches the diff based on my review, please verify end to end working before merging.",based review please verify end end working,issue,negative,neutral,neutral,neutral,neutral,neutral
1885920276,(1) is fixed. Now users can set `DataConfig.execution_option.locality_with_output` to False to disable locality.,fixed set false disable locality,issue,negative,negative,negative,negative,negative,negative
1885912748,"A few notes on scope of work, as the designs show some changes which needed clarification:

1. The logo shouldn't be changed to the gradient in the design; we will keep it as-is
2. The new navbar background colors in dark and light mode _also_ apply globally to `--pst-color-on-background`
3. The new navbar shouldn't drop the chevrons as per the design - we will keep them as-is
4. Although the styles of the search button have been modified, no change in behavior is implied by these changes. We will be keeping the current behavior (i.e. the ""search bar"" is just a button that opens up a modal dialog where users can search)
5. The currently active page is bolded in the top nav bar, not just `Docs`",scope work show clarification gradient design keep new background color dark light mode apply globally new drop per design keep although search button change behavior keeping current behavior search bar button modal search currently active page top bar,issue,negative,positive,positive,positive,positive,positive
1885890057,I think it's still failing on the nightly version of this test https://buildkite.com/ray-project/postmerge/builds/2470#018cf566-0efd-46a3-83d6-8acab80c045a,think still failing nightly version test,issue,negative,neutral,neutral,neutral,neutral,neutral
1885876203,"Thanks for the feedback, should be ready for merge pending approval from @simran-2797.",thanks feedback ready merge pending approval,issue,positive,positive,positive,positive,positive,positive
1885868655,"> > It's https://github.com/ray-project/ray/blob/master/python/requirements.txt#L22-L24
> 
> It requires `grpcio >= 1.54.2; sys_platform != ""darwin""` without upper version bound, is that right?
> 
> I couldn't make the dashboard start with grpcio=1.56.x, downgrading to 1.54.3 fixed the issue.

For reproduction, the following is the command which installs ray with an incompatible version of grpcio:

```
conda create -n ray-test-env python=3.10 pytorch torchvision torchaudio pytorch-cuda=11.8 cuda-version=11.8 matplotlib numpy pandas pandas-stubs seaborn pytest pytest-asyncio ipython py-spy mypy jinja2 ujson scikit-learn odfpy websockets toml python-lsp-server orjson cython types-{toml,ujson,requests,orjson,python-dateutil} tensorboard reactivex pytorch-lightning xgboost hyperopt ray-tune termcolor rich attrs -c pytorch -c nvidia -c conda-forge
```

Adding `grpcio=1.54` into the list of libraries fixed the issue.",without upper version bound right could make dashboard start fixed issue reproduction following command ray incompatible version create jinja rich list fixed issue,issue,positive,positive,positive,positive,positive,positive
1885851747,"```
chain DAG calls, n=32 actors per second 36.91 +- 0.47
compiled chain DAG calls, n=32 actors per second 381.28 +- 27.07
2024-01-09 00:50:13,440 INFO worker.py:1442 -- Using address local set in the environment variable RAY_ADDRESS
2024-01-09 00:50:15,474 INFO worker.py:1752 -- Started a local Ray instance. View the dashboard at https://session-39wy2k6j6lw4fzlffu92e2gruh.i.anyscaleuserdata-staging.com 
*** SIGSEGV received at time=1704790217 on cpu 10 ***
[failure_signal_handler.cc : 332] RAW: Signal 11 raised at PC=0x556fb40 while already in AbslFailureSignalHandler()
[failure_signal_handler.cc : 332] RAW: Signal 11 raised at PC=0x556fb40 while already in AbslFailureSignalHandler()
PC: @          0x556fb40  (unknown)  (unknown)
    @     0x7ff896e7b420       3712  (unknown)
    @     0x7ff8957b84ec         64  ray::core::CoreWorkerMemoryStore::Get()
    @     0x7ff8957b86fd        208  ray::core::CoreWorkerMemoryStore::Get()
    @     0x7ff895741f4e       1088  ray::core::CoreWorker::Get()
    @     0x7ff8955fe91b        256  __pyx_pw_3ray_7_raylet_10CoreWorker_41get_objects()
    @           0x4f9ba6  1512829648  method_vectorcall_VARARGS_KEYWORDS
    @           0x73a180  (unknown)  (unknown)
[2024-01-09 00:50:17,351 E 4143 93643] logging.cc:361: *** SIGSEGV received at time=1704790217 on cpu 10 ***
[2024-01-09 00:50:17,352 E 4143 93643] logging.cc:361: PC: @          0x556fb40  (unknown)  (unknown)
[2024-01-09 00:50:17,353 E 4143 93643] logging.cc:361:     @     0x7ff896e7b420       3712  (unknown)
[2024-01-09 00:50:17,353 E 4143 93643] logging.cc:361:     @     0x7ff8957b84ec         64  ray::core::CoreWorkerMemoryStore::Get()
[2024-01-09 00:50:17,353 E 4143 93643] logging.cc:361:     @     0x7ff8957b86fd        208  ray::core::CoreWorkerMemoryStore::Get()
[2024-01-09 00:50:17,353 E 4143 93643] logging.cc:361:     @     0x7ff895741f4e       1088  ray::core::CoreWorker::Get()
[2024-01-09 00:50:17,353 E 4143 93643] logging.cc:361:     @     0x7ff8955fe91b        256  __pyx_pw_3ray_7_raylet_10CoreWorker_41get_objects()
[2024-01-09 00:50:17,353 E 4143 93643] logging.cc:361:     @           0x4f9ba6  1512829648  method_vectorcall_VARARGS_KEYWORDS
[2024-01-09 00:50:17,354 E 4143 93643] logging.cc:361:     @           0x73a180  (unknown)  (unknown)

```",chain dag per second chain dag per second address local set environment variable local ray instance view dashboard received raw signal raised already raw signal raised already unknown unknown unknown ray ray ray unknown unknown received unknown unknown unknown ray ray ray unknown unknown,issue,negative,negative,neutral,neutral,negative,negative
1885838481,"> should we call out at the bazel install section as well?

I (or @can-anyscale ) can take a look at this page, and give an update.

`ci/env/install-bazel.sh` today basically installs bazelisk and symlink bazel to bazelisk. it feels very CI specific, and should not really be recommended to devs on local machines today.",call install section well take look page give update today basically specific really local today,issue,negative,neutral,neutral,neutral,neutral,neutral
1885830920,should we call out at the bazel install section as well? ,call install section well,issue,negative,neutral,neutral,neutral,neutral,neutral
1885829087,"I see- I think this is definitely a gap in the error message. 

> so it was even more surprising to me that Ray would even try to start the dashboard at all, despite my explicit request to not do that.

I think ray does run the dashboard in a ""minimal"" mode for some of the functionalities (e.g. health checks). So this flag only turns off the ""default"" dashboard behaviour. I think this should also be changed.  ",think definitely gap error message even surprising ray would even try start dashboard despite explicit request think ray run dashboard minimal mode health flag turn default dashboard behaviour think also,issue,negative,positive,positive,positive,positive,positive
1885823388,"0f2a327783 [ci] replace xcommit with mem_pressure (#42249)
a8d0a4936e [ci] use ray_release for ci tests (#42229)
246677aef1 [ci] merge rayci and ray_release requirements (#42237)
121353b87a [data] fix error in _StatsManager background thread (#42189)
ed82a446c5 [Serve][Doc] Fix pydantic config documentation (#42216)
6241d5c58d [Doc] Add improved 'ray start --resources' help (#41009)
2244e89a4d [spark] Fix DefaultDatabricksRayOnSparkStartHook.on_spark_job_created (#42178)
f3c90421f9 [ci] migrate awscli to python (#42225)
01ad8c1a5a [Core] Improve the error message for failed task/actor imports on workers (#42087)
615b0ab5bf [Serve] Add `policy` config to `AutoscalingConfig` (#42072)
76193e2d3c [Doc] [Release] Update dask-on-ray table for 2.9.1 (#42127)
2c1f6f2341 [ci] remove boost as a dependency (#42226)
4d779ba843 [spark] Fix Gloo detecting incorrect Interfaces on DBR (#42202)
715f20b1d5 [Core] Remove dead code (#42132)
f0f0a23e6c [serve] Centralize status (#41429)
6724eb88b0 [Serve] Improve handling the websocket server disconnect scenario (#42130)
18b2cab6ca [Doc UI] Disable wrapping for example gallery tags pills (#42212)
363ebac4c1 [ci] remove duplicate mount in linux container (#42219)
68c949f779 Remove numpy for Ray wheel build (#40603)
f66f1cc0b8 [ci][7up/2.1] unify default python across rayci (#42213)
3d0bbabc9d [data][train] Fix locality config not being respected in DataConfig (#42204)
0780a3c12d [Data] Fix data test which unconditionally passes (#41814)
7ac13c3475 [ci][mac/3] move ray cpp+java tests to civ2 (#42209)
152ef7fd4b [ci] support artifact mount for all containers (#42205)
bfc1f78e7a [Doc] Add back in missing search button to top nav bar (#42206)
9fb2415dd3 [ci][mac/2] move python mac tests to civ2 (#42166)
7b67dfa6eb [ci] fix bazel log uploading conditions (#42200)
11a54d58c1 [data] Stabilize 100TB shuffle tests (#41985)",fa replace use merge ba data fix error background thread serve doc fix documentation doc add start help spark fix migrate python core improve error message serve add policy doc release update table remove boost dependency spark fix incorrect core remove dead code serve centralize status ebb serve improve handling server disconnect scenario doc disable wrapping example gallery remove duplicate mount container remove ray wheel build unify default python across data train fix locality data fix data test unconditionally move ray support artifact mount doc add back missing search button top bar move python mac fix log data stabilize shuffle,issue,positive,positive,neutral,neutral,positive,positive
1885812921,"I think we might as well pick this because we're waiting for other cherry-picks anyway.

It's a fix for an example cluster YAML file which is actually broken out of the box, which is why the release test was failing.",think might well pick waiting anyway fix example cluster file actually broken box release test failing,issue,negative,negative,negative,negative,negative,negative
1885804307,"> Actually even if I override self._supports_distributed_reads to False, the reading task will still be scheduled to a different node, seems like scheduling strategy option to force local read is ignored somewhere?

That's weird. Here's the code for scheduling tasks on the driver's node:

https://github.com/ray-project/ray/blob/cd8fef3331b30c044b7d49f2e3561c81f8dbee37/python/ray/data/read_api.py#L307-L311

I tried reproducing this with 10,000 files on a 32 node cluster and it seems to work fine? If I don't override `_supports_distributed_reads`, I get `FileNotFound` errors, but if I override there aren't any errors.

```python
import ray

datasource = ray.data.datasource.BinaryDatasource(
    [f""local:///tmp/{i}.dat"" for i in range(10_000)],
)
# If you uncomment this, the program won't error.
# datasource._supports_distributed_reads = False
ray.data.read_datasource(datasource).materialize()
```",actually even override false reading task still different node like strategy option force local read somewhere weird code driver node tried node cluster work fine override get override python import ray local range program wo error false,issue,negative,negative,negative,negative,negative,negative
1885800909,Good catch. Here's a PR to fix the issue https://github.com/ray-project/ray/pull/42308,good catch fix issue,issue,negative,positive,positive,positive,positive,positive
1885793436,Closed as it turns out this won't be the final commit for 2.9.1.  But feel free to check this PR for regressions anyway in case it gives you a head start.,closed turn wo final commit feel free check anyway case head start,issue,positive,positive,positive,positive,positive,positive
1885784828,"@jjyao @rickyyx Please help to reassign if you're not the right assignees, thanks!",please help reassign right thanks,issue,positive,positive,positive,positive,positive,positive
1885624008,"> > > It's https://github.com/ray-project/ray/blob/master/python/requirements.txt#L22-L24
> > 
> > 
> > It requires `grpcio >= 1.54.2; sys_platform != ""darwin""` without upper version bound, is that right?
> > I couldn't make the dashboard start with grpcio=1.56.x, downgrading to 1.54.3 fixed the issue.
> 
> Can you share the dashboard.log or dashboard.err?

Sure, but these don't have any errors in them. I attached other log files which contain an error message. They don't provide any details about required version of grpcio. That's what prompted me to ask the question.

[logs.zip](https://github.com/ray-project/ray/files/13892971/logs.zip)

The dashboard is explicitly disabled with `ray.init(..., include_dashboard=False)`, so it was even more surprising to me that Ray would even try to start the dashboard at all, despite my explicit request to not do that.
",without upper version bound right could make dashboard start fixed issue share sure attached log contain error message provide version ask question dashboard explicitly disabled even surprising ray would even try start dashboard despite explicit request,issue,negative,positive,positive,positive,positive,positive
1885500470,"So great! Thank you, @chris-ray-zhang! Really, really appreciate the quick turnaround and the infinite polish improvement. Two comments:

1. For the input box, could we equate a carriage return as hitting the send button? 
2. For the disclaimer, could we change the ampersand to the spelled out word, ""and""?",great thank really really appreciate quick turnaround infinite polish improvement two input box could equate carriage return send button disclaimer could change ampersand word,issue,positive,positive,positive,positive,positive,positive
1885415447,@edoakes @alexeykudinkin I included those naming changing suggestions from pervious PR here. Let me know if you prefer to split those out for easier review. But the main focus of this PR should just be adding the `AutoscalingContext` class ,included naming pervious let know prefer split easier review main focus class,issue,negative,positive,positive,positive,positive,positive
1885401024,"> @edoakes thanks for clarifying!
> 
> > More generally, I'd like to avoid having user code able to fully block our code in the replica (can help improve observability quite a lot) and make the concurrency model more explicit. Currently some of the methods implicitly run on separate threads/event loops (using concurrency_group support) which makes it very easy to have subtle race conditions. For example user health check methods are not currently thread safe because they're actually called on a separate event loop.
> 
> Am i reading this right that we want to get away from using concurrency-groups instead maintaining just a dichotomy of control-plane/user event-loops and running all the code in either of them?

Yes that's exactly right",thanks generally like avoid user code able fully block code replica help improve observability quite lot make concurrency model explicit currently implicitly run separate support easy subtle race example user health check currently thread safe actually separate event loop reading right want get away instead dichotomy running code either yes exactly right,issue,positive,positive,positive,positive,positive,positive
1885399265,"@edoakes thanks for clarifying!

> More generally, I'd like to avoid having user code able to fully block our code in the replica (can help improve observability quite a lot) and make the concurrency model more explicit. Currently some of the methods implicitly run on separate threads/event loops (using concurrency_group support) which makes it very easy to have subtle race conditions. For example user health check methods are not currently thread safe because they're actually called on a separate event loop.

Am i reading this right that we want to get away from using concurrency-groups instead maintaining just a dichotomy of control-plane/user event-loops and running all the code in either of them?",thanks generally like avoid user code able fully block code replica help improve observability quite lot make concurrency model explicit currently implicitly run separate support easy subtle race example user health check currently thread safe actually separate event loop reading right want get away instead dichotomy running code either,issue,positive,positive,positive,positive,positive,positive
1885286963,@can-anyscale is working on stopping python 3.8 support. we might look at ubuntu upgrade after that is finished.,working stopping python support might look upgrade finished,issue,negative,neutral,neutral,neutral,neutral,neutral
1885229260,"Hi @defrag-bambino the package ```tensorflow_probability``` is missing.
Install it with ```pip install tensorflow_probability``` and try again.
",hi package missing install pip install try,issue,negative,negative,negative,negative,negative,negative
1885123936,"@alexeykudinkin to your question about the motivation:

I am splitting up the PRs so this one will have no behavioral change and should be a pure refactor, that's why we're still passing in the existing event loop rather than creating a standalone one. The following PR should be only a few additional lines of code.

There are multiple intertwined motivations for running the user code on its own loop:

- The primary reason is to productionize my prototype for late binding/new routing. In order for this to work, we need to ensure the call to get the queue length/start executing a request (1) cannot hang due to user code blocking the event loop and (2) run on the same thread as each other.
- More generally, I'd like to avoid having user code able to fully block our code in the replica (can help improve observability quite a lot) and make the concurrency model more explicit. Currently some of the methods implicitly run on separate threads/event loops (using `concurrency_group` support) which makes it very easy to have subtle race conditions. For example user health check methods are not currently thread safe because they're actually called on a separate event loop.
- We also currently rely on some very experimental/sketchy features of Ray to support our use case (actor method call stats dictionary & `concurrency_group` support in conjunction with `asyncio`). These aren't really used heavily elsewhere or well-tested, so I'd prefer to avoid them.",question motivation splitting one behavioral change pure still passing event loop rather one following additional code multiple running user code loop primary reason prototype late routing order work need ensure call get queue request due user code blocking event loop run thread generally like avoid user code able fully block code replica help improve observability quite lot make concurrency model explicit currently implicitly run separate support easy subtle race example user health check currently thread safe actually separate event loop also currently rely ray support use case actor method call dictionary support conjunction really used heavily elsewhere prefer avoid,issue,positive,positive,neutral,neutral,positive,positive
1884609765,"> Hey @simonsays1980 , could you check the tests? I think 2 are failing due to the new changes.

@sven1977 There were a couple of minor bugs yesterday - the ones you saw after my last push. Hopefully with my commit today tests should all pass.",hey could check think failing due new couple minor yesterday saw last push hopefully commit today pas,issue,negative,negative,neutral,neutral,negative,negative
1884285218,"I face the same issue. 
python 3.7.12
ray 2.4.0
",face issue python ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1884262914,":point_up: Good question. All of the recent docs design changes originate with @simran-2797, who would be better positioned to answer these questions, but I've also been trying to push us toward collecting all custom colors into CSS variables in `custom.css` as a way of organizing these customizations, and to ensure we don't run into a situation where tons of different colors get scattered throughout our code base. We saw this in the previous iteration of the docs, where we had instances where several different ""ray blue"" colors were used in various places.

For now, if there's a custom color somewhere, it's put in a CSS variable in `custom.css`.",good question recent design originate would better positioned answer also trying push u toward custom color way ensure run situation different color get scattered throughout code base saw previous iteration several different ray blue color used various custom color somewhere put variable,issue,positive,positive,neutral,neutral,positive,positive
1884229627,"> Hover color for light mode: #09326C
Hover color for dark mode: #CCE0FF

what are the reasoning behind these colors? is there a color scheme guiding?",hover color light mode hover color dark mode reasoning behind color color scheme,issue,negative,negative,neutral,neutral,negative,negative
1884125769,"> @rickyyx can you confirm the way we validate this is we are going to configure the new autoscaler v2 path for this test and confirm it passes? if passes than we can close this one as well?

yeah - it should be eventually replaced by the v2 autoscaler tests. ",confirm way validate going configure new path test confirm close one well yeah eventually,issue,positive,positive,positive,positive,positive,positive
1884066320,And also xgboost model is coming after submitting the job as mentioned above comments.,also model coming job,issue,negative,neutral,neutral,neutral,neutral,neutral
1884062442,Hi @architkulkarni  thanks for helping i didn’t changed any python xgboost sample code i used as its.,hi thanks helping python sample code used,issue,positive,positive,positive,positive,positive,positive
1884051663,"Hi, thanks. I will reply to you soon when I'm back.",hi thanks reply soon back,issue,negative,positive,neutral,neutral,positive,positive
1884047794,@rickyyx can you confirm the way we validate this is we are going to configure the new autoscaler v2 path for this test and confirm it passes? if passes than we can close this one as well?,confirm way validate going configure new path test confirm close one well,issue,positive,positive,positive,positive,positive,positive
1884039350,"We ran it on a Mac with python 3.11 and ray 2.6.3 but there's still some issue with the ray worker. It was generated with `reuse_actors=False`

```
2024-01-09 17:49:32,362	ERROR tune_controller.py:911 -- Trial task failed for trial objective_fn_57913_00005
Traceback (most recent call last):
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py"", line 110, in resolve_future
    result = ray.get(future)
             ^^^^^^^^^^^^^^^
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/_private/auto_init_hook.py"", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/_private/worker.py"", line 2526, in get
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ImplicitFunc
	actor_id: 81387472d4b38e5cdee6a0d601000000
	namespace: 9c932ba2-96c7-4f5b-9875-384893a5a2a9
The actor is dead because its owner has died. Owner Id: 01000000ffffffffffffffffffffffffffffffffffffffffffffffff Owner Ip address: 127.0.0.1 Owner worker exit type: SYSTEM_ERROR Worker exit detail: Owner's node has crashed.
The actor never ran - it was cancelled before it started running.
2024-01-09 17:49:32,368	ERROR tune_controller.py:911 -- Trial task failed for trial objective_fn_57913_00006
Traceback (most recent call last):
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py"", line 110, in resolve_future
    result = ray.get(future)
             ^^^^^^^^^^^^^^^
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/_private/auto_init_hook.py"", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/_private/worker.py"", line 2526, in get
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ImplicitFunc
	actor_id: f3d74f7bcce8e9f60db1465e01000000
	namespace: 9c932ba2-96c7-4f5b-9875-384893a5a2a9
The actor is dead because its node has died. Node Id: 5d11b677f340fc9012ce947e1d4bf5e1c79aa76095c93a28c4b5ae53
The actor never ran - it was cancelled before it started running.
2024-01-09 17:49:32,371	ERROR tune_controller.py:911 -- Trial task failed for trial objective_fn_57913_00001
Traceback (most recent call last):
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py"", line 110, in resolve_future
    result = ray.get(future)
             ^^^^^^^^^^^^^^^
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/_private/auto_init_hook.py"", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/_private/worker.py"", line 2526, in get
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ImplicitFunc
	actor_id: 776fa51d17f2438142a34dfc01000000
	namespace: 9c932ba2-96c7-4f5b-9875-384893a5a2a9
The actor is dead because its node has died. Node Id: 5d11b677f340fc9012ce947e1d4bf5e1c79aa76095c93a28c4b5ae53
The actor never ran - it was cancelled before it started running.
2024-01-09 17:49:32,375	ERROR tune_controller.py:911 -- Trial task failed for trial objective_fn_57913_00003
Traceback (most recent call last):
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py"", line 110, in resolve_future
    result = ray.get(future)
             ^^^^^^^^^^^^^^^
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/_private/auto_init_hook.py"", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/_private/worker.py"", line 2526, in get
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ImplicitFunc
	actor_id: a784953ed942f99f31fd311e01000000
	namespace: 9c932ba2-96c7-4f5b-9875-384893a5a2a9
The actor is dead because its node has died. Node Id: 5d11b677f340fc9012ce947e1d4bf5e1c79aa76095c93a28c4b5ae53
The actor never ran - it was cancelled before it started running.
2024-01-09 17:49:32,379	ERROR tune_controller.py:911 -- Trial task failed for trial objective_fn_57913_00002
Traceback (most recent call last):
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py"", line 110, in resolve_future
    result = ray.get(future)
             ^^^^^^^^^^^^^^^
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/_private/auto_init_hook.py"", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/_private/worker.py"", line 2526, in get
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ImplicitFunc
	actor_id: dabfadb2b8f83dcafcd5757301000000
	namespace: 9c932ba2-96c7-4f5b-9875-384893a5a2a9
The actor is dead because its node has died. Node Id: 5d11b677f340fc9012ce947e1d4bf5e1c79aa76095c93a28c4b5ae53
The actor never ran - it was cancelled before it started running.
2024-01-09 17:49:32,383	ERROR tune_controller.py:911 -- Trial task failed for trial objective_fn_57913_00007
Traceback (most recent call last):
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py"", line 110, in resolve_future
    result = ray.get(future)
             ^^^^^^^^^^^^^^^
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/_private/auto_init_hook.py"", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/_private/worker.py"", line 2526, in get
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ImplicitFunc
	actor_id: c22dfa77030b7162e206fb6901000000
	namespace: 9c932ba2-96c7-4f5b-9875-384893a5a2a9
The actor is dead because its node has died. Node Id: 5d11b677f340fc9012ce947e1d4bf5e1c79aa76095c93a28c4b5ae53
The actor never ran - it was cancelled before it started running.
(raylet) [2024-01-09 17:49:32,358 E 64916 10149657] (raylet) agent_manager.cc:135: The raylet exited immediately because the Ray agent failed. The raylet fate shares with the agent. This can happen because the Ray agent was unexpectedly killed or failed. Agent can fail when
(raylet) - The version of `grpcio` doesn't follow Ray's requirement. Agent can segfault with the incorrect `grpcio` version. Check the grpcio version `pip freeze | grep grpcio`.
(raylet) - The agent failed to start because of unexpected error or port conflict. Read the log `cat /tmp/ray/session_latest/logs/dashboard_agent.log`. You can find the log file structure here https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure.
(raylet) - The agent is killed by the OS (e.g., out of memory).
2024-01-09 17:49:32,387	ERROR tune_controller.py:911 -- Trial task failed for trial objective_fn_57913_00004
Traceback (most recent call last):
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py"", line 110, in resolve_future
    result = ray.get(future)
             ^^^^^^^^^^^^^^^
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/_private/auto_init_hook.py"", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/_private/worker.py"", line 2526, in get
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ImplicitFunc
	actor_id: 65ed5c48e50903aba4a2a8c201000000
	namespace: 9c932ba2-96c7-4f5b-9875-384893a5a2a9
The actor is dead because its node has died. Node Id: 5d11b677f340fc9012ce947e1d4bf5e1c79aa76095c93a28c4b5ae53
The actor never ran - it was cancelled before it started running.
2024-01-09 17:49:32,391	ERROR tune_controller.py:911 -- Trial task failed for trial objective_fn_57913_00000
Traceback (most recent call last):
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py"", line 110, in resolve_future
    result = ray.get(future)
             ^^^^^^^^^^^^^^^
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/_private/auto_init_hook.py"", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/james.park/anaconda3/envs/raytune-test/lib/python3.11/site-packages/ray/_private/worker.py"", line 2526, in get
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ImplicitFunc
	actor_id: 504963e871c28bfa14fd0c6201000000
	namespace: 9c932ba2-96c7-4f5b-9875-384893a5a2a9
The actor is dead because its node has died. Node Id: 5d11b677f340fc9012ce947e1d4bf5e1c79aa76095c93a28c4b5ae53
The actor never ran - it was cancelled before it started running.
2024-01-09 17:49:37,190	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #2...
2024-01-09 17:49:37,694	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #3...
2024-01-09 17:49:38,200	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #4...
2024-01-09 17:49:38,708	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #5...
2024-01-09 17:49:39,214	WARNING resource_updater.py:275 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.
2024-01-09 17:49:42,239	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #2...
2024-01-09 17:49:42,744	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #3...
2024-01-09 17:49:43,249	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #4...
2024-01-09 17:49:43,754	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #5...
2024-01-09 17:49:44,260	WARNING resource_updater.py:275 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.
2024-01-09 17:49:47,280	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #2...
2024-01-09 17:49:47,784	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #3...
2024-01-09 17:49:48,289	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #4...
2024-01-09 17:49:48,793	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #5...
2024-01-09 17:49:49,300	WARNING resource_updater.py:275 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.
2024-01-09 17:49:52,317	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #2...
2024-01-09 17:49:52,824	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #3...
2024-01-09 17:49:53,331	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #4...
2024-01-09 17:49:53,834	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #5...
2024-01-09 17:49:54,340	WARNING resource_updater.py:275 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.
2024-01-09 17:49:57,342	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #2...
2024-01-09 17:49:57,846	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #3...
2024-01-09 17:49:58,349	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #4...
2024-01-09 17:49:58,853	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #5...
2024-01-09 17:49:59,357	WARNING resource_updater.py:275 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.
2024-01-09 17:50:02,386	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #2...
2024-01-09 17:50:02,890	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #3...
2024-01-09 17:50:03,395	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #4...
2024-01-09 17:50:03,897	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #5...
2024-01-09 17:50:04,401	WARNING resource_updater.py:275 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.
2024-01-09 17:50:07,417	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #2...
2024-01-09 17:50:07,921	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #3...
2024-01-09 17:50:08,424	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #4...
2024-01-09 17:50:08,930	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #5...
2024-01-09 17:50:09,434	WARNING resource_updater.py:275 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.
2024-01-09 17:50:12,452	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #2...
2024-01-09 17:50:12,958	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #3...
2024-01-09 17:50:13,464	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #4...
2024-01-09 17:50:13,969	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #5...
2024-01-09 17:50:14,473	WARNING resource_updater.py:275 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.
2024-01-09 17:50:17,480	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #2...
2024-01-09 17:50:17,985	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #3...
2024-01-09 17:50:18,492	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #4...
2024-01-09 17:50:18,997	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #5...
2024-01-09 17:50:19,502	WARNING resource_updater.py:275 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.
2024-01-09 17:50:22,514	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #2...
2024-01-09 17:50:23,019	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #3...
2024-01-09 17:50:23,523	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #4...
2024-01-09 17:50:24,028	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #5...
2024-01-09 17:50:24,532	WARNING resource_updater.py:275 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.
2024-01-09 17:50:27,527	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #2...
2024-01-09 17:50:28,029	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #3...
2024-01-09 17:50:28,537	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #4...
2024-01-09 17:50:29,043	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #5...
2024-01-09 17:50:29,549	WARNING resource_updater.py:275 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.
2024-01-09 17:50:32,573	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #2...
2024-01-09 17:50:33,077	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #3...
2024-01-09 17:50:33,582	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #4...
2024-01-09 17:50:34,089	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #5...
2024-01-09 17:50:34,590	WARNING resource_updater.py:275 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.
2024-01-09 17:50:37,599	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #2...
2024-01-09 17:50:38,104	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #3...
2024-01-09 17:50:38,607	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #4...
2024-01-09 17:50:39,113	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #5...
2024-01-09 17:50:39,618	WARNING resource_updater.py:275 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.
2024-01-09 17:50:42,659	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #2...
2024-01-09 17:50:43,166	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #3...
2024-01-09 17:50:43,677	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #4...
2024-01-09 17:50:44,194	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #5...
2024-01-09 17:50:44,699	WARNING resource_updater.py:275 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.
2024-01-09 17:50:47,706	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #2...
2024-01-09 17:50:48,209	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #3...
2024-01-09 17:50:48,720	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #4...
2024-01-09 17:50:49,224	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #5...
2024-01-09 17:50:49,732	WARNING resource_updater.py:275 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.
2024-01-09 17:50:52,795	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #2...
2024-01-09 17:50:53,300	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #3...
2024-01-09 17:50:53,807	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #4...
2024-01-09 17:50:54,314	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #5...
2024-01-09 17:50:54,818	WARNING resource_updater.py:275 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.
2024-01-09 17:50:57,839	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #2...
2024-01-09 17:50:58,344	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #3...
2024-01-09 17:50:58,850	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #4...
2024-01-09 17:50:59,355	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #5...
2024-01-09 17:50:59,859	WARNING resource_updater.py:275 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.
2024-01-09 17:51:02,886	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #2...
2024-01-09 17:51:03,392	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #3...
2024-01-09 17:51:03,897	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #4...
2024-01-09 17:51:04,400	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #5...
2024-01-09 17:51:04,907	WARNING resource_updater.py:275 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.
2024-01-09 17:51:07,957	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #2...
2024-01-09 17:51:08,465	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #3...
2024-01-09 17:51:08,969	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #4...
2024-01-09 17:51:09,474	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #5...
2024-01-09 17:51:09,982	WARNING resource_updater.py:275 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.
2024-01-09 17:51:13,010	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #2...
2024-01-09 17:51:13,518	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #3...
2024-01-09 17:51:14,024	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #4...
2024-01-09 17:51:14,528	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #5...
2024-01-09 17:51:15,033	WARNING resource_updater.py:275 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.
2024-01-09 17:51:18,059	WARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #2...

``` 
![Screenshot from 2024-01-10 10-28-50](https://github.com/ray-project/ray/assets/36796950/a805f239-a470-48f4-88bf-fa024205a5e8)
![Screenshot from 2024-01-10 10-29-05](https://github.com/ray-project/ray/assets/36796950/e2763239-9b9c-4f85-ac68-c7ff60ead29e)

",ran mac python ray still issue ray worker error trial task trial recent call last file line result future file line return file line wrapper return file line get raise value actor unexpectedly finishing task actor dead owner owner id owner address owner worker exit type worker exit detail owner node actor never ran running error trial task trial recent call last file line result future file line return file line wrapper return file line get raise value actor unexpectedly finishing task actor dead node node id actor never ran running error trial task trial recent call last file line result future file line return file line wrapper return file line get raise value actor unexpectedly finishing task actor dead node node id actor never ran running error trial task trial recent call last file line result future file line return file line wrapper return file line get raise value actor unexpectedly finishing task actor dead node node id actor never ran running error trial task trial recent call last file line result future file line return file line wrapper return file line get raise value actor unexpectedly finishing task actor dead node node id actor never ran running error trial task trial recent call last file line result future file line return file line wrapper return file line get raise value actor unexpectedly finishing task actor dead node node id actor never ran running raylet raylet raylet immediately ray agent raylet fate agent happen ray agent unexpectedly agent fail raylet version follow ray requirement agent incorrect version check version pip freeze raylet agent start unexpected error port conflict read log cat find log file structure raylet agent o memory error trial task trial recent call last file line result future file line return file line wrapper return file line get raise value actor unexpectedly finishing task actor dead node node id actor never ran running error trial task trial recent call last file line result future file line return file line wrapper return file line get raise value actor unexpectedly finishing task actor dead node node id actor never ran running warning cluster attempt warning cluster attempt warning cluster attempt warning cluster attempt warning cluster resume experiment passing run warning cluster attempt warning cluster attempt warning cluster attempt warning cluster attempt warning cluster resume experiment passing run warning cluster attempt warning cluster attempt warning cluster attempt warning cluster attempt warning cluster resume experiment passing run warning cluster attempt warning cluster attempt warning cluster attempt warning cluster attempt warning cluster resume experiment passing run warning cluster attempt warning cluster attempt warning cluster attempt warning cluster attempt warning cluster resume experiment passing run warning cluster attempt warning cluster attempt warning cluster attempt warning cluster attempt warning cluster resume experiment passing run warning cluster attempt warning cluster attempt warning cluster attempt warning cluster attempt warning cluster resume experiment passing run warning cluster attempt warning cluster attempt warning cluster attempt warning cluster attempt warning cluster resume experiment passing run warning cluster attempt warning cluster attempt warning cluster attempt warning cluster attempt warning cluster resume experiment passing run warning cluster attempt warning cluster attempt warning cluster attempt warning cluster attempt warning cluster resume experiment passing run warning cluster attempt warning cluster attempt warning cluster attempt warning cluster attempt warning cluster resume experiment passing run warning cluster attempt warning cluster attempt warning cluster attempt warning cluster attempt warning cluster resume experiment passing run warning cluster attempt warning cluster attempt warning cluster attempt warning cluster attempt warning cluster resume experiment passing run warning cluster attempt warning cluster attempt warning cluster attempt warning cluster attempt warning cluster resume experiment passing run warning cluster attempt warning cluster attempt warning cluster attempt warning cluster attempt warning cluster resume experiment passing run warning cluster attempt warning cluster attempt warning cluster attempt warning cluster attempt warning cluster resume experiment passing run warning cluster attempt warning cluster attempt warning cluster attempt warning cluster attempt warning cluster resume experiment passing run warning cluster attempt warning cluster attempt warning cluster attempt warning cluster attempt warning cluster resume experiment passing run warning cluster attempt warning cluster attempt warning cluster attempt warning cluster attempt warning cluster resume experiment passing run warning cluster attempt warning cluster attempt warning cluster attempt warning cluster attempt warning cluster resume experiment passing run warning cluster attempt,issue,positive,negative,neutral,neutral,negative,negative
1884032356,"Actually even if I override `self._supports_distributed_reads` to `False`, the reading task will still be scheduled to a different node, seems like scheduling strategy option to force local read is ignored somewhere?",actually even override false reading task still different node like strategy option force local read somewhere,issue,negative,negative,negative,negative,negative,negative
1884032195,"> > It's https://github.com/ray-project/ray/blob/master/python/requirements.txt#L22-L24
> 
> It requires `grpcio >= 1.54.2; sys_platform != ""darwin""` without upper version bound, is that right?
> 
> I couldn't make the dashboard start with grpcio=1.56.x, downgrading to 1.54.3 fixed the issue.

Can you share the dashboard.log or dashboard.err? ",without upper version bound right could make dashboard start fixed issue share,issue,negative,positive,positive,positive,positive,positive
1884014276,"> 
> 
> 
> 
> Actually, for the GPU column, the tooltip currently appears, but only if you hover over the ""[0]"" part, not over the utilization bar.
> 
> 
> 
> <img width=""188"" alt=""Screenshot 2024-01-09 at 3 23 46 PM"" src=""https://github.com/ray-project/ray/assets/5459654/9ae26b55-0b63-405f-9760-131f438529ee"">
> 
> 
> 
> For the GRAM column, the tooltip appears if you hover over the ""[0]"" part or the utilization bar.  <img width=""197"" alt=""Screenshot 2024-01-09 at 3 24 26 PM"" src=""https://github.com/ray-project/ray/assets/5459654/0c21a27f-aabc-4355-a5a1-3b31d36051ef"">
> 
> 
> 
> So I think we just need to make sure that the tooltip appears when hovering over the utilization bar in the GPU column.

+1. Let's start with only tooltip. We can add the text into [0] if needed down the road.",actually column currently hover part utilization bar gram column hover part utilization bar think need make sure hovering utilization bar column let start add text road,issue,negative,positive,positive,positive,positive,positive
1884014220,"Since there's no further action items for this issue, I'll close it. @tomchify Please feel free to reopen it if we haven't addressed your concerns fully.",since action issue close please feel free reopen fully,issue,positive,positive,positive,positive,positive,positive
1884009200,"I just hit this again trying to run something using rllib on a worker running an official ray-ml container. It's very annoying. Shall I re-open, or can someone open a new issue that is more accurate?",hit trying run something worker running official container annoying shall someone open new issue accurate,issue,negative,negative,neutral,neutral,negative,negative
1884006257,"It's worth a try, let's go back and change the default if it's a problem.",worth try let go back change default problem,issue,negative,positive,positive,positive,positive,positive
1883985206,"Hi @psydok, indeed the `worker_path` parameter was not being picked up in 2.8.1, and we fixed this issue in 2.9. However 2.9 introduced an issue with using the `container` runtime env with Ray Jobs, which I see you identified in https://github.com/ray-project/ray/issues/42106. The fix for this should be merged soon, and you'll be able to try it out on the nightly Ray image (and it will be included in 2.10).",hi indeed parameter picked fixed issue however issue container ray see fix soon able try nightly ray image included,issue,negative,positive,positive,positive,positive,positive
1883969953,"> @alanwguo how would you recommend testing this part (manually or otherwise)?

One way is to let this PR build a ray wheel and install it as a cluster env to use on Anyscale and launch a machine with a GPU.

Alternatively, you can test with a kube-ray cluster with a GPU node?",would recommend testing part manually otherwise one way let build ray wheel install cluster use launch machine alternatively test cluster node,issue,negative,neutral,neutral,neutral,neutral,neutral
1883965723,@alanwguo how would you recommend testing this part (manually or otherwise)?,would recommend testing part manually otherwise,issue,negative,neutral,neutral,neutral,neutral,neutral
1883955366,"

Actually, for the GPU column, the tooltip currently appears, but only if you hover over the ""[0]"" part, not over the utilization bar.

<img width=""188"" alt=""Screenshot 2024-01-09 at 3 23 46 PM"" src=""https://github.com/ray-project/ray/assets/5459654/9ae26b55-0b63-405f-9760-131f438529ee"">

For the GRAM column, the tooltip appears if you hover over the ""[0]"" part or the utilization bar.  <img width=""197"" alt=""Screenshot 2024-01-09 at 3 24 26 PM"" src=""https://github.com/ray-project/ray/assets/5459654/0c21a27f-aabc-4355-a5a1-3b31d36051ef"">

So I think we just need to make sure that the tooltip appears when hovering over the utilization bar in the GPU column.",actually column currently hover part utilization bar gram column hover part utilization bar think need make sure hovering utilization bar column,issue,negative,positive,positive,positive,positive,positive
1883944251,"Thanks @robin-anyscale , just to clarify, is the desired behavior that ""NVIDIA L4"" appear as a tooltip when you mouse over the GPU entry, just like how it currently does when you mouse over the GRAM entry?

We could also maybe always display it in the GPU entry where it currently says [0], but I'm not sure if that will take too much space.",thanks clarify desired behavior appear mouse entry like currently mouse gram entry could also maybe always display entry currently sure take much space,issue,positive,positive,positive,positive,positive,positive
1883916420,Can you provide a bit more context on the problem and fix? The PR description doesn't say anything about how it relates to the case in the unit test added.,provide bit context problem fix description say anything case unit test added,issue,negative,neutral,neutral,neutral,neutral,neutral
1883902110,"> It's https://github.com/ray-project/ray/blob/master/python/requirements.txt#L22-L24

It requires `grpcio >= 1.54.2; sys_platform != ""darwin""` without upper version bound, is that right?

I couldn't make the dashboard start with grpcio=1.56.x, downgrading to 1.54.3 fixed the issue.",without upper version bound right could make dashboard start fixed issue,issue,negative,positive,positive,positive,positive,positive
1883870671,"@zcin container related issue in jobs, can you help taking a look? Thanks!",container related issue help taking look thanks,issue,positive,positive,neutral,neutral,positive,positive
1883867202,@zhangyilun Thanks for reporting the workaround. You are correct that '_thread.lock' is not picklable so Ray is unable to serialize it and send to to the worker node which runs this code. It is expected that the best you can do here is to embed it into the constructor method of your deployment. Feel free to let me know if you have other questions!  ,thanks correct ray unable serialize send worker node code best embed constructor method deployment feel free let know,issue,positive,positive,positive,positive,positive,positive
1883862703,"Ah thanks for the investigation! I'd say it's better to just change `python` to `python3` in the release test script, rather than installing a package for all users of the cluster launcher.  Want to give that a shot? I can restart the release tests afterwards to make sure the other cluster launcher release tests still pass.

Or do you expect that this change might cause some of the non-AWS release tests to start failing? If that's the case, maybe we need to modify the test to somehow check if python or python3 is available on the image and just use the correct one.  Not sure what the standard solution is here.",ah thanks investigation say better change python python release test script rather package cluster launcher want give shot restart release afterwards make sure cluster launcher release still pas expect change might cause release start failing case maybe need modify test somehow check python python available image use correct one sure standard solution,issue,positive,positive,positive,positive,positive,positive
1883848938,I'm a bit confused by `fatal: destination path 'ray' already exists and is not an empty directory.`.  Are you git cloning the Ray repository in your job? That shouldn't be necessary.,bit confused fatal destination path already empty git ray repository job necessary,issue,negative,negative,negative,negative,negative,negative
1883847005,"It's probably because Python 3 is only available through the `python3` command, and the `python` alias [doesn't exist](https://askubuntu.com/a/1296887/387382).

Maybe we can install the `python-is-python3` package or create a symbolic link as part of the `run_init` code:

https://github.com/ray-project/ray/blob/cd8fef3331b30c044b7d49f2e3561c81f8dbee37/python/ray/autoscaler/_private/command_runner.py#L708

... or use `python3` on all the tests. 🤔 ",probably python available python command python alias exist maybe install package create symbolic link part code use python,issue,negative,positive,positive,positive,positive,positive
1883828777,"~~~To debug the `xgboost` import error, you can print `sys.path` inside a Ray job to get a list of directories where Python is trying to import from, and compare it to the directory where`xgboost` is installed on the node. Alternatively, you can specify `xgboost` in the Job's `runtime_env` field to install it at runtime: https://docs.ray.io/en/latest/ray-core/handling-dependencies.html~~~ Actually I'm guessing the `xgboost_benchmark.py` isn't intended to be part of your job. I'm not sure why this file is being run.  What's your job `entrypoint` script?

To delete a job, you can use the `ray job delete` CLI.  You can't delete it from the dashboard.

> HI @architkulkarni THanks for the checking failed job we don't get any JOB_ID right?
<img width=""400"" alt=""Screenshot 2024-01-09 at 1 30 39 PM"" src=""https://github.com/ray-project/ray/assets/5459654/bc40c59f-d89b-437a-94c3-755b0a309c6a"">

You can use the submission ID above.

",import error print inside ray job get list python trying import compare directory node alternatively specify job field install actually guessing intended part job sure file run job script delete job use ray job delete ca delete dashboard hi thanks job get right use submission id,issue,negative,positive,positive,positive,positive,positive
1883740726,"changed test size, and decreased the wait. seems that redis tests run slower.",test size wait run,issue,negative,neutral,neutral,neutral,neutral,neutral
1883717386,"Hey @simonsays1980 , could you check the tests? I think 2 are failing due to the new changes.",hey could check think failing due new,issue,negative,positive,neutral,neutral,positive,positive
1883665427,"I retried the repro with the fix. There's no longer a memory leak:

<img width=""1367"" alt=""Screenshot 2024-01-09 at 11 37 29 AM"" src=""https://github.com/ray-project/ray/assets/92341594/7efe3d3e-6cbe-4146-ad3c-d3b5b475b44e"">
",fix longer memory leak,issue,negative,neutral,neutral,neutral,neutral,neutral
1883664468,"Streaming microbenchmark results show no significant difference.

Before (on master):
```
(ray) eoakes@Edwards-MacBook-Pro-2 serve % python _private/benchmarks/streaming_http_throughput.py
...
HTTP streaming throughput (num_replicas=1, tokens_per_request=1000, batch_size=10, use_intermediate_deployment=False): 242411.47 +- 4603.52 tokens/s
(ray) eoakes@Edwards-MacBook-Pro-2 serve % python _private/benchmarks/streaming_handle_throughput.py
...
DeploymentHandle streaming throughput (num_replicas=1, tokens_per_request=1000, batch_size=10): 16975.09 +- 1303.82 tokens/s
```

After (on this branch):
```
(ray) eoakes@Edwards-MacBook-Pro-2 serve % python _private/benchmarks/streaming_http_throughput.py
...
HTTP streaming throughput (num_replicas=1, tokens_per_request=1000, batch_size=10, use_intermediate_deployment=False): 243444.54 +- 5633.78 tokens/s
(ray) eoakes@Edwards-MacBook-Pro-2 serve % python _private/benchmarks/streaming_handle_throughput.py
...
DeploymentHandle streaming throughput (num_replicas=1, tokens_per_request=1000, batch_size=10): 16921.91 +- 599.67 tokens/s
```

",streaming show significant difference master ray serve python streaming throughput ray serve python streaming throughput branch ray serve python streaming throughput ray serve python streaming throughput,issue,negative,positive,positive,positive,positive,positive
1883653998,"@bveeramani I thought it's because this error message isn't included. But it turned out that the exception was raised outside of our current retry function (see below).
We should add a try-catch for the entire write tasks instead.

<img width=""2120"" alt=""image"" src=""https://github.com/ray-project/ray/assets/2883335/c540798d-deed-4774-8915-2d95076283f1"">

",thought error message included turned exception raised outside current retry function see add entire write instead image,issue,negative,neutral,neutral,neutral,neutral,neutral
1883578627,Following up: @can-anyscale are you the right person to ask about merging this?,following right person ask,issue,negative,positive,positive,positive,positive,positive
1883564869,"> Are there any downside for using tempfs universally for all linux tests instead of having that as an option?

in theory, no, but it does not work.. many more tests will fail, and I am not really interested on investigating and fixing all of those.",downside universally instead option theory work many fail really interested investigating fixing,issue,negative,positive,neutral,neutral,positive,positive
1883528564,"I think I just did `pip install ray`, I wanted to confirm by testing on the server with multiple cores running Windows but I can get access end of next week and provide more info on what I see during installation.",think pip install ray confirm testing server multiple running get access end next week provide see installation,issue,negative,neutral,neutral,neutral,neutral,neutral
1883526683,"Final state of the benchmarks run in here:

```
----------------------------------------------------------------
Batching responses (IO threads: 1)
----------------------------------------------------------------

# DYNAMIC BATCH SIZE, TARGET DURATION = 1us

Core Actors streaming throughput (ASYNC) (num_replicas=1, tokens_per_request=1000, batch_size=10): 36636.12 +- 51.19 tokens/s
(CallerActor pid=6357) Individual request quantiles:
(CallerActor pid=6357) 	P50=192.93610449999932
(CallerActor pid=6357) 	P75=257.85448975
(CallerActor pid=6357) 	P99=297.78491591

# BATCH SIZE = 50

Core Actors streaming throughput (ASYNC) (num_replicas=1, tokens_per_request=1000, batch_size=10): 25801.6 +- 4004.66 tokens/s
(CallerActor pid=1832) Individual request quantiles:
(CallerActor pid=1832) 	P50=215.52814600000005
(CallerActor pid=1832) 	P75=308.86266650000186
(CallerActor pid=1832) 	P99=498.30295621999915

# BATCH SIZE = 20

Core Actors streaming throughput (ASYNC) (num_replicas=1, tokens_per_request=1000, batch_size=10): 37707.66 +- 364.56 tokens/s
(CallerActor pid=2287) Individual request quantiles:
(CallerActor pid=2287) 	P50=186.39720849999895
(CallerActor pid=2287) 	P75=260.4036565000005
(CallerActor pid=2287) 	P99=287.04247549999985

# BATCH SIZE = 10

Core Actors streaming throughput (ASYNC) (num_replicas=1, tokens_per_request=1000, batch_size=10): 35848.56 +- 159.47 tokens/s
(CallerActor pid=1345) Individual request quantiles:
(CallerActor pid=1345) 	P50=199.287749999999
(CallerActor pid=1345) 	P75=266.65879199999983
(CallerActor pid=1345) 	P99=299.7154335700003

# BATCH SIZE = 5

Core Actors streaming throughput (ASYNC) (num_replicas=1, tokens_per_request=1000, batch_size=10): 31694.32 +- 661.67 tokens/s
(CallerActor pid=99597) Individual request quantiles:
(CallerActor pid=99597) 	P50=193.6036460000006
(CallerActor pid=99597) 	P75=264.2684792500014
(CallerActor pid=99597) 	P99=331.6293704799994

# BATCH SIZE = 2

Core Actors streaming throughput (ASYNC) (num_replicas=1, tokens_per_request=1000, batch_size=10): 22004.42 +- 266.35 tokens/s
(CallerActor pid=736) Individual request quantiles:
(CallerActor pid=736) 	P50=298.7863335000012
(CallerActor pid=736) 	P75=383.73084374999956
(CallerActor pid=736) 	P99=492.2973509999991

----------------------------------------------------------------
Stream responses asynchronously (IO threads: 4)
----------------------------------------------------------------

# AFTER FIXING (#2; REMOVING VECTOR)

Core Actors streaming throughput (ASYNC) (num_replicas=1, tokens_per_request=1000, batch_size=10): 11688.37 +- 379.4 tokens/s
(CallerActor pid=42855) Individual request quantiles:
(CallerActor pid=42855) 	P50=555.9930205000007
(CallerActor pid=42855) 	P75=710.3086662499986
(CallerActor pid=42855) 	P99=1064.9366758199994

----------------------------------------------------------------
Stream responses asynchronously (IO threads: 2)
----------------------------------------------------------------

# AFTER FIXING (#3; FINE-GRAINED LOCKS)

Core Actors streaming throughput (ASYNC) (num_replicas=1, tokens_per_request=1000, batch_size=10): 13647.46 +- 213.32 tokens/s
(CallerActor pid=99052) Individual request quantiles:
(CallerActor pid=99052) 	P50=496.7188125000001
(CallerActor pid=99052) 	P75=630.3058014999996
(CallerActor pid=99052) 	P99=836.4252895600001

# AFTER FIXING (#2; REMOVING VECTOR)

Core Actors streaming throughput (ASYNC) (num_replicas=1, tokens_per_request=1000, batch_size=10): 13336.61 +- 159.58 tokens/s
(CallerActor pid=42403) Individual request quantiles:
(CallerActor pid=42403) 	P50=501.92681299999987
(CallerActor pid=42403) 	P75=635.8994060000009
(CallerActor pid=42403) 	P99=817.4323538700003

# AFTER FIXING (#1; ADDING LOCKS)

Core Actors streaming throughput (ASYNC) (num_replicas=1, tokens_per_request=1000, batch_size=10): 13276.45 +- 468.14 tokens/s
(CallerActor pid=71706) Individual request quantiles:
(CallerActor pid=71706) 	P50=495.943041499999
(CallerActor pid=71706) 	P75=634.3365417499997
(CallerActor pid=71706) 	P99=857.8107148799994

# BEFORE FIXING LOCKING (BROKEN)

Core Actors streaming throughput (ASYNC) (num_replicas=1, tokens_per_request=1000, batch_size=10): 15828.38 +- 57.91 tokens/s
(CallerActor pid=28414) Individual request quantiles:
(CallerActor pid=28414) 	P50=425.29577100000006
(CallerActor pid=28414) 	P75=548.1356040000003
(CallerActor pid=28414) 	P99=659.215728219999

----------------------------------------------------------------
Stream responses asynchronously (IO threads: 1; default)
----------------------------------------------------------------

# AFTER FIXING (#3; FINE-GRAINED LOCKS)

Core Actors streaming throughput (ASYNC) (num_replicas=1, tokens_per_request=1000, batch_size=10): 14199.39 +- 93.58 tokens/s
(CallerActor pid=98043) Individual request quantiles:
(CallerActor pid=98043) 	P50=480.73095849999964
(CallerActor pid=98043) 	P75=603.5131979999991
(CallerActor pid=98043) 	P99=770.7011399399998

# AFTER FIXING (#2; REMOVING VECTOR)

Core Actors streaming throughput (ASYNC) (num_replicas=1, tokens_per_request=1000, batch_size=10): 14081.07 +- 230.85 tokens/s
(CallerActor pid=41554) Individual request quantiles:
(CallerActor pid=41554) 	P50=475.90718750000076
(CallerActor pid=41554) 	P75=607.0160522500023
(CallerActor pid=41554) 	P99=765.3397844800002

# AFTER FIXING (#1; ADDING LOCKS) (5847daef4a90afd76b20400339ec8ed164787f13)

Core Actors streaming throughput (ASYNC) (num_replicas=1, tokens_per_request=1000, batch_size=10): 14019.33 +- 457.36 tokens/s
(CallerActor pid=3818) Individual request quantiles:
(CallerActor pid=3818) 	P50=483.27275000000066
(CallerActor pid=3818) 	P75=605.1864587499999
(CallerActor pid=3818) 	P99=769.4822761800002


# BEFORE FIXING LOCKING (BROKEN)

Core Actors streaming throughput (ASYNC) (num_replicas=1, tokens_per_request=1000, batch_size=10): 14477.66 +- 216.94 tokens/s
(CallerActor pid=8662) Individual request quantiles:
(CallerActor pid=8662) 	P50=466.0777494999999
(CallerActor pid=8662) 	P75=592.6898645000005
(CallerActor pid=8662) 	P99=718.6345975599995

----------------------------------------------------------------
Skip back-pressure handler
----------------------------------------------------------------

Core Actors streaming throughput (ASYNC) (num_replicas=1, tokens_per_request=1000, batch_size=10): 14643.21 +- 352.34 tokens/s
(CallerActor pid=13155) Individual request quantiles:
(CallerActor pid=13155) 	P50=685.5565414999995
(CallerActor pid=13155) 	P75=707.7666252500006
(CallerActor pid=13155) 	P99=777.5089473099982

----------------------------------------------------------------
Baseline
----------------------------------------------------------------

Core Actors streaming throughput (ASYNC) (num_replicas=1, tokens_per_request=1000, batch_size=10): 14264.66 +- 162.12 tokens/s
(CallerActor pid=12256) Individual request quantiles:
(CallerActor pid=12256) 	P50=705.1566460000007
(CallerActor pid=12256) 	P75=723.1873227499985
(CallerActor pid=12256) 	P99=771.7197762799998
```",final state run io dynamic batch size target duration u core streaming throughput individual request batch size core streaming throughput individual request batch size core streaming throughput individual request batch size core streaming throughput individual request batch size core streaming throughput individual request batch size core streaming throughput individual request stream io fixing removing vector core streaming throughput individual request stream io fixing core streaming throughput individual request fixing removing vector core streaming throughput individual request fixing core streaming throughput individual request fixing locking broken core streaming throughput individual request stream io default fixing core streaming throughput individual request fixing removing vector core streaming throughput individual request fixing core streaming throughput individual request fixing locking broken core streaming throughput individual request skip handler core streaming throughput individual request core streaming throughput individual request,issue,negative,negative,neutral,neutral,negative,negative
1883515685,"Apologies all for the disruption, I'll get Nevergrad added back into Ray Tune.",disruption get added back ray tune,issue,negative,neutral,neutral,neutral,neutral,neutral
1883490136,"Yes. I think all tests are blocking unless marked ""unstable"".",yes think blocking unless marked unstable,issue,negative,positive,neutral,neutral,positive,positive
1883484420,"If needed, here's the code comparing optuna with and without ray-tune

```
params = {
    ""x1"": tune.uniform(-10, 10),
    ""x2"": tune.uniform(-10, 10),
}

def parabolic_surface(x1, x2):
    return (x1 - 3)**2 + (x2 - 5)**2

num_samples = 100
```

```
start = time.time()

searcher = OptunaSearch(sampler=op.samplers.TPESampler())
tune_config = tune.TuneConfig(
    metric=""f(x1,x2)"",
    mode=""min"",
    search_alg=searcher,
    num_samples=num_samples,
)

tuner = tune.Tuner(
    lambda config: {""f(x1,x2)"": parabolic_surface(**config)},
    tune_config=tune_config, 
    param_space=params
)
results = tuner.fit()
elapsed = time.time() - start
```

```
start = time.time()

def objective(trial):
    x1 = trial.suggest_float(""x1"", -10, 10)
    x2 = trial.suggest_float(""x2"", -10, 10)
    return parabolic_surface(x1=x1, x2=x2)

sampler = op.samplers.TPESampler()
study = op.create_study(sampler=sampler, direction=""minimize"")
study.optimize(objective, n_trials=num_samples)

elapsed = time.time() - start
```",code without return start searcher min tuner lambda start start objective trial return sampler study minimize objective start,issue,negative,neutral,neutral,neutral,neutral,neutral
1883481053,"Additionally, here are some timing results from comparing optuna with ray-tune vs directly using optuna. I tried it for different number of samples

- For `num_sumples=100`, optuna with ray-tune had an elapsed time that is around `16x` longer than directly using optuna
- For `num_sumples=1000`, optuna with ray-tune took around `8x` longer
- For `num_sumples=5000`, optuna with ray-tune took around `4x` longer

It seems that the gap becomes smaller when more samples are used. 


![Screenshot from 2024-01-09 11-45-20](https://github.com/ray-project/ray/assets/36796950/be07fb81-6e85-4617-b44c-85c8cc28476a)
![Screenshot from 2024-01-09 11-50-32](https://github.com/ray-project/ray/assets/36796950/eb08f021-221c-4d44-a57c-b1da808ba21a)
![Screenshot from 2024-01-09 12-10-41](https://github.com/ray-project/ray/assets/36796950/b933f864-6fa4-46f9-b29e-0ac99759c650)

",additionally timing directly tried different number time around longer directly took around longer took around longer gap becomes smaller used,issue,negative,positive,neutral,neutral,positive,positive
1883468951,"Thank you for your reply!

We don't have a remote cluster setup so we are only using ray-tune in an instance locally. So we're just using 1 node and our instance type is `g4dn.xlarge`.

Indeed, using `reuse_actors=False` removes the errors but the overall speed is slower. Specifically, this is how I timed it:

```
start = time.time()

params = {
    ""x1"": tune.uniform(-5, 5),
    ""x2"": tune.uniform(-5, 5),
}

def objective_fn(config):
    fn = (config[""x1""] - 2)**2 + (config[""x2""] + 3)**2
    return {""fn"": fn}

searcher = BasicVariantGenerator()
tune_config = tune.TuneConfig(
    metric=""fn"",
    mode=""min"",
    search_alg=searcher,
    num_samples=200,
    reuse_actors=False
)

tuner = tune.Tuner(
    objective_fn, 
    tune_config=tune_config, 
    param_space=params
)
results = tuner.fit()
elapsed = time.time() - start
print(f""Elapsed: {elapsed} s"")
```

And `elapsed=162.90476822853088 s` for `reuse_actors=False` while `elapsed=18.27258825302124 s` for `reuse_actors=True`",thank reply remote cluster setup instance locally node instance type indeed overall speed specifically timed start return searcher min tuner start print,issue,negative,negative,neutral,neutral,negative,negative
1883465943,Are there any downside for using tempfs universally for all linux tests instead of having that as an option?,downside universally instead option,issue,negative,neutral,neutral,neutral,neutral,neutral
1883451033,"Hi @architkulkarni  can you please help me on this, i have been stuck past two weeks",hi please help stuck past two,issue,negative,negative,negative,negative,negative,negative
1883381224,"Sorry to reopen this but @luyuhengCN is there any chance you were operating 2 jobs at a time using the same storage path?  I'm running into this now, and looking to recreate, and as far as I can tell from our runs that is the current similarity between people hitting it",sorry reopen chance operating time storage path running looking recreate far tell current similarity people,issue,negative,negative,negative,negative,negative,negative
1883380861,"With some trials and errors, it seems like initializing `apm` within the `init` method works:
```
from ray import serve
from fastapi import FastAPI
from elasticapm.contrib.starlette import ElasticAPM, make_apm_client

app = FastAPI()

@serve.deployment()
@serve.ingress(app)
class MLDeployment:

    def __init__(self):
        apm = make_apm_client()
        app.add_middleware(ElasticAPM, client=apm)
    
    @app.get(""/"")
    def home(self) -> dict:
        return {""message"": ""Hello World!""}


app = MLDeployment.bind()
```

Is this the expected usage? (Confirmed working from Elastic APM side as well)",like within method work ray import serve import import class self home self return message hello world usage confirmed working elastic side well,issue,positive,positive,positive,positive,positive,positive
1883378013,"> Re-opening issue as test is still failing. Latest run: https://buildkite.com/ray-project/release/builds/5289#018cde80-6077-4406-a258-14dd970f341e

Ah sorry, introduced a bug in the test script...",issue test still failing latest run ah sorry bug test script,issue,negative,neutral,neutral,neutral,neutral,neutral
1883064948,"@architkulkarni  it would be really helpful, if you give me right direction, because of application team planning to deploy the AI workload into Ray cluster after my POC.",would really helpful give right direction application team deploy ai ray cluster,issue,negative,positive,positive,positive,positive,positive
1882913278,"HI @architkulkarni  THanks for the checking failed job we don't get any JOB_ID right?

And i can able to check the logs using job submission logs when i'm doing this i can see xgboost module is not found, but i have installed ray cluster in EKS with help of karpenter and we have 4 nodes all the nodes i have installed xgboost python module but still I'm getting error.

And also can you please how to delete the failed job from ray dashboard or cli ?

```
ray job logs 'raysubmit_sFUs2e7y2C3YhDk9' --follow --address http://127.0.0.1:8265
$ ray job logs 'raysubmit_sFUs2e7y2C3YhDk9' --follow --address http://127.0.0.1:8265
Job submission server address: http://127.0.0.1:8265
fatal: destination path 'ray' already exists and is not an empty directory.
Traceback (most recent call last):
  File ""ray/release/air_tests/air_benchmarks/workloads/xgboost_benchmark.py"", line 11, in <module>
    import xgboost as xgb
ModuleNotFoundError: No module named 'xgboost'

---------------------------------------
Job 'raysubmit_sFUs2e7y2C3YhDk9' failed
---------------------------------------

Status message: Job failed due to an application error, last available logs (truncated to 20,000 chars):
fatal: destination path 'ray' already exists and is not an empty directory.
Traceback (most recent call last):
  File ""ray/release/air_tests/air_benchmarks/workloads/xgboost_benchmark.py"", line 11, in <module>
    import xgboost as xgb
ModuleNotFoundError: No module named 'xgboost'
```",hi thanks job get right able check job submission see module found ray cluster help python module still getting error also please delete job ray dashboard ray job follow address ray job follow address job submission server address fatal destination path already empty directory recent call last file line module import module job status message job due application error last available truncated fatal destination path already empty directory recent call last file line module import module,issue,negative,positive,neutral,neutral,positive,positive
1882794815,"I've identified a potential solution for the issue at hand. By adding a space to `disallowed_chars`, I was able to resolve the problem on my machine.

https://github.com/ray-project/ray/blob/0f2a32778307ee02a193357a94c24933e5e191d4/python/ray/_private/runtime_env/packaging.py#L210

If this aligns with your expectations, I'd be happy to make a pull request. Please let me know if this is acceptable to you.",potential solution issue hand space able resolve problem machine happy make pull request please let know acceptable,issue,positive,positive,positive,positive,positive,positive
1882783485,"> Nevergrad's OnePlusOne and the recent LogNormalOnePlusOne have been my favorite. They yielded better performance than Optuna for me. Kind of sad because it got removed for no valid reason.

I have added LogNormalOnePlusOne recently and I love it. As seemingly I am not the only one maybe I should add variants of it in Nevergrad as well.",recent favorite better performance kind sad got removed valid reason added recently love seemingly one maybe add well,issue,positive,positive,positive,positive,positive,positive
1882780845,I am one of the developers of Nevergrad. In Nevergrad's group we see users mentioning that we are not available anymore in Ray; is there anything we should do in Nevergrad so that it is not a problem for people taking care of Ray ? Happy to do anything that can help.,one group see available ray anything problem people taking care ray happy anything help,issue,positive,positive,positive,positive,positive,positive
1882470350,"ok, thx. 
I will have a try the managed ray first, but maybe our infra structure is settled.
if there need some contribution, maybe i can give some commit.
",try ray first maybe infra structure settled need contribution maybe give commit,issue,negative,positive,positive,positive,positive,positive
1882408662,Generate a random string to use as tmpdir. I do this in `run_tests` instead of init because an object can call `run_tests` multiple time and the tmpdir need to be different across test runs.,generate random string use instead object call multiple time need different across test,issue,negative,negative,negative,negative,negative,negative
1882406126,"> interestingly, it seems that it will make `test_ood_events` flaky somehow..

my guess is that there is an event queue or something, and `list_cluster_events` is async, and can need some time to wait to receive all the events to appear. I am adding a 5 second wait as an attempt to deflake. it is not the right fix though.",interestingly make flaky somehow guess event queue something need time wait receive appear second wait attempt right fix though,issue,negative,positive,positive,positive,positive,positive
1882403437,"> Can you check if there is indeed a Ray server running at ""127.0.0.1:8265""? Otherwise would be helpful to get the output of `ray logs` to see why it wasn't able to connect.

I am sure I did not run another ray server. maybe let us fix orginal issue. if I just run this benchmark with latest code download. I see below error.
```
dcg@oq1:/mnt/nvme1n1/ray/release/air_tests/air_benchmarks$ python3 workloads/torch_benchmark.py run --num-runs 3 --num-epochs 20 --num-workers 4 --cpus-per-worker 8
Traceback (most recent call last):
  File ""/mnt/nvme1n1/ray/release/air_tests/air_benchmarks/workloads/torch_benchmark.py"", line 599, in <module>
    main()
  File ""/mnt/nvme1n1/ray/release/air_tests/air_benchmarks/workloads/torch_benchmark.py"", line 595, in main
    return cli()
  File ""/usr/lib/python3/dist-packages/click/core.py"", line 1128, in __call__
    return self.main(*args, **kwargs)
  File ""/usr/lib/python3/dist-packages/click/core.py"", line 1053, in main
    rv = self.invoke(ctx)
  File ""/usr/lib/python3/dist-packages/click/core.py"", line 1659, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/usr/lib/python3/dist-packages/click/core.py"", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/usr/lib/python3/dist-packages/click/core.py"", line 754, in invoke
    return __callback(*args, **kwargs)
  File ""/mnt/nvme1n1/ray/release/air_tests/air_benchmarks/workloads/torch_benchmark.py"", line 433, in run
    ray.init(""auto"")
  File ""/home/dcg/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
  File ""/home/dcg/.local/lib/python3.10/site-packages/ray/_private/worker.py"", line 1537, in init
    bootstrap_address = services.canonicalize_bootstrap_address(address, _temp_dir)
  File ""/home/dcg/.local/lib/python3.10/site-packages/ray/_private/services.py"", line 530, in canonicalize_bootstrap_address
    addr = get_ray_address_from_environment(addr, temp_dir)
  File ""/home/dcg/.local/lib/python3.10/site-packages/ray/_private/services.py"", line 423, in get_ray_address_from_environment
    raise ConnectionError(
ConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting `--address` flag or `RAY_ADDRESS` environment variable.
dcg@oq1:/mnt/nvme1n1/ray/release/air_tests/air_benchmarks$ 
```
if I run ray logs cluster, I see this error:
```
dcg@oq1:~$ ray logs cluster
Usage: ray logs cluster [OPTIONS] [GLOB_FILTER]
Try 'ray logs cluster --help' for help.

Error: Could not find any running Ray instance. Please specify the one to connect to by setting the `--address` flag or `RAY_ADDRESS` environment variable.
dcg@oq1:~$ 
```

this is just example benchmark, I suppose it should run without problem",check indeed ray server running otherwise would helpful get output ray see able connect sure run another ray server maybe let u fix issue run latest code see error python run recent call last file line module main file line main return file line return file line main file line invoke return file line invoke return file line invoke return file line run auto file line wrapper return file line address file line file line raise could find running ray instance please specify one connect setting address flag environment variable run ray cluster see error ray cluster usage ray cluster try cluster help help error could find running ray instance please specify one connect setting address flag environment variable example suppose run without problem,issue,positive,positive,positive,positive,positive,positive
1882384627,"interestingly, it seems that it will make `test_ood_events` flaky somehow..",interestingly make flaky somehow,issue,negative,positive,positive,positive,positive,positive
1882381650,@architkulkarni is this test important (defined as something we would block on if still failing ahead of a ray release)?,test important defined something would block still failing ahead ray release,issue,negative,positive,positive,positive,positive,positive
1882266530,"@matthewdeng , @scottjlee , @raulchen let me know if you have strong opinion about not running tests using python 3.9 in CI, thankkks.",let know strong opinion running python,issue,positive,positive,positive,positive,positive,positive
1882265779,"In raylet.out:
```
[2024-01-09 02:32:18,415 I 351197 351246] (raylet) agent_manager.cc:66: Agent process with name dashboard_agent/424238335 exited, exit code 0.
[2024-01-09 02:32:18,415 E 351197 351246] (raylet) agent_manager.cc:70: The raylet exited immediately because one Ray agent failed, agent_name = dashboard_agent/424238335.
The raylet fate shares with the agent. This can happen because
- The version of `grpcio` doesn't follow Ray's requirement. Agent can segfault with the incorrect `grpcio` version. Check the grpcio version `pip freeze | grep grpcio`.
- The agent failed to start because of unexpected error or port conflict. Read the log `cat /tmp/ray/session_latest/logs/{dashboard_agent|runtime_env_agent}.log`. You can find the log file structure here https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure.
- The agent is killed by the OS (e.g., out of memory).
```

Where are Ray's requirements of `grpcio` documented, please?",raylet agent process name exit code raylet raylet immediately one ray agent raylet fate agent happen version follow ray requirement agent incorrect version check version pip freeze agent start unexpected error port conflict read log cat find log file structure agent o memory ray please,issue,negative,positive,neutral,neutral,positive,positive
1882245831,"Hey @FlorinAndrei thanks for the repro script. This looks like a bug that happens when the controller is being overloaded. 

The workaround is to disable `TuneConfig.reuse_actors`:

```python
tune_config = tune.TuneConfig(num_samples=-1, max_concurrent_trials=os.cpu_count(), time_budget_s=10, reuse_actors=False)
```",hey thanks script like bug controller disable python,issue,positive,positive,positive,positive,positive,positive
1882110009,"It looks like the release tests are failing in this PR with the following:

```
+ python launch_and_verify_cluster.py aws/tests/aws_cluster.yaml --num-expected-nodes 2 --retries 10
Using cluster configuration file: aws/tests/aws_cluster.yaml
Number of retries for 'verify ray is running' step: 10
Using --no-config-cache flag: False
Number of expected nodes for 'verify ray is running': 2
======================================
Overriding ray wheel...: 
======================================
Overriding docker image...: disable
Using docker image: None
======================================
Downloading ssh key...
======================================
======================================
Cleaning up cluster...
======================================
Starting new cluster...
ray up -v -y /tmp/tmpsqsnpvxo.yaml
======================================
Verifying Ray is running...
2023-12-21 09:26:46,817 INFO util.py:375 -- setting max workers for head node type to 0
2023-12-21 09:26:46,929 VWARN commands.py:357 -- Loaded cached provider configuration from /tmp/ray-config-1bf6fd1c4814e0cd90053edb1d6a57e7f0dfd0a0
2023-12-21 09:26:46,929 WARN commands.py:363 -- If you experience issues with the cloud provider, try re-running the command with --no-config-cache.
2023-12-21 09:26:46,930 VINFO utils.py:149 -- Creating AWS resource `ec2` in `us-west-2`
2023-12-21 09:26:47,296 VINFO utils.py:149 -- Creating AWS resource `ec2` in `us-west-2`
2023-12-21 09:26:47,692 INFO command_runner.py:204 -- Fetched IP: 44.234.126.97
2023-12-21 09:26:47,692 VINFO command_runner.py:371 -- Running `python -c 'import ray; ray.init(""localhost:6379""); assert len(ray.nodes()) >= 2'`

Command 'python' not found, did you mean:

  command 'python3' from deb python3
  command 'python' from deb python-is-python3

Shared connection to 44.234.126.97 closed.
```

Any ideas about this? It seems weird that the deep learning AMI wouldn't have the `python` command.",like release failing following python cluster configuration file number ray running step flag false number ray running ray wheel docker image disable docker image none key cleaning cluster starting new cluster ray ray running setting head node type loaded provider configuration warn experience cloud provider try command resource resource fetched running python ray assert command found mean command deb python command deb connection closed weird deep learning ami would python command,issue,negative,negative,negative,negative,negative,negative
1882104933,"For the failed job, can you check the status with `ray job status Lq...`? It might have more information.  

> And also, when I'm submitting the job from CLI, how can we check which Python job is running (i.e., which node is using it)?


By default, the job driver script runs on the head node. But child tasks and actors in the job might run on different nodes. The best way to check is using the Ray Dashboard.  The [Ray State API](https://docs.ray.io/en/latest/ray-observability/reference/api.html) might also be helpful from the command line, though it will need to be run from a Ray node.  ",job check status ray job status might information also job check python job running node default job driver script head node child job might run different best way check ray dashboard ray state might also helpful command line though need run ray node,issue,positive,positive,positive,positive,positive,positive
1882075842,"As a follow-up, I think we should add explicit memory consumption regression metrics to GCS and other components for our long running tests. https://github.com/ray-project/ray/issues/42250",think add explicit memory consumption regression metric long running,issue,negative,negative,neutral,neutral,negative,negative
1882057459,"@rdeaton-freenome Do you have an example script that reproduces the error? I am not able to trace why this attribute is `None` at runtime.

It gets set to None [here](https://github.com/ray-project/ray/blob/246677aef162bba8df430ae9a67f68088d37301f/python/ray/tune/tune.py#L957-L959), but then should be set again [here](https://github.com/ray-project/ray/blob/246677aef162bba8df430ae9a67f68088d37301f/python/ray/tune/tune.py#L975-L979).

Do you pass in a custom progress reporter?",example script error able trace attribute none set none set pas custom progress reporter,issue,negative,positive,positive,positive,positive,positive
1882036452,"Hi @stephanie-wang,

Sounds good.  Do you mind sharing your environment information (python, unix, etc.) with me?  I would like to, if possible, duplicate your environment as much as I could.  Thanks.",hi good mind environment information python would like possible duplicate environment much could thanks,issue,positive,positive,positive,positive,positive,positive
1882011766,"> What happens in this case? Deadlock / slowdown?

The backpressure would be not strict enough for this case, and spilling will still happen. But it's better than no backpressure.

> I think it'd be good to do something adaptive here. Like if there are N stages total, give each stage a buffer of size object store memory limit / N.

I thought of this. Determine the limit based on object store memory makes sense. But it may not be optimal to evenly distribute the memory for stages. Because output sizes of different stages are usually different. ",case deadlock slowdown would strict enough case still happen better think good something adaptive like total give stage buffer size object store memory limit thought determine limit based object store memory sense may optimal evenly distribute memory output size different usually different,issue,positive,positive,positive,positive,positive,positive
1881993038,"Hey @n30111 sorry for the delay, I'll do another review on this PR, so that we can get it in for Ray 2.10. ",hey sorry delay another review get ray,issue,negative,negative,negative,negative,negative,negative
1881991320,Currently we do not have a way to configure file/module specific logging (without modifying the underlying classes/files). We could expose this as a parameter from DataContext specifically for streaming executor logs (e.g. can generalize the `enable_auto_log_stats` parameter already in the class to gate logging to stdout for streaming execution in general). @raulchen thoughts on this? https://github.com/ray-project/ray/blob/master/python/ray/data/context.py#L299,currently way configure specific logging without underlying could expose parameter specifically streaming executor generalize parameter already class gate logging streaming execution general,issue,negative,positive,neutral,neutral,positive,positive
1881986241,"For (2), I believe the root issue is https://github.com/microsoft/DeepSpeed/issues/3228, though it's not clear why the issue is happening again.",believe root issue though clear issue happening,issue,negative,positive,positive,positive,positive,positive
1881973436,"Just so I understand this correctly, is the requested behavior to restore a previous experiment but set a higher `TuneConfig.num_samples`?",understand correctly behavior restore previous experiment set higher,issue,negative,positive,neutral,neutral,positive,positive
1881968431,"> I couldn't repro the failures and the Intel tests were failing for me as well. Please retest with the latest modifications I made.

@jjyao can you help here?",could failing well please retest latest made help,issue,negative,positive,positive,positive,positive,positive
1881951217,"This is because in this line:
```
ds = ds.add_column('meta',lambda df: [{}] * len(df))
```

the same dict object is used in for each element in the resulting list. After updating this to:
```
ds = ds.add_column('meta',lambda df: [{} for _ in range(len(df))])
```

I get the following expected result:

```
test1 ds =  [{'id': 0, 'meta': {'id': 0}}, {'id': 1, 'meta': {'id': 1}}, {'id': 20, 'meta': {'id': 20}}, {'id': 21, 'meta': {'id': 21}}, {'id': 2, 'meta': {'id': 2}}, {'id': 3, 'meta': {'id': 3}}, {'id': 4, 'meta': {'id': 4}}, {'id': 5, 'meta': {'id': 5}}, {'id': 6, 'meta': {'id': 6}}, {'id': 7, 'meta': {'id': 7}}, {'id': 8, 'meta': {'id': 8}}, {'id': 9, 'meta': {'id': 9}}, {'id': 10, 'meta': {'id': 10}}, {'id': 11, 'meta': {'id': 11}}, {'id': 12, 'meta': {'id': 12}}, {'id': 13, 'meta': {'id': 13}}, {'id': 14, 'meta': {'id': 14}}, {'id': 15, 'meta': {'id': 15}}, {'id': 16, 'meta': {'id': 16}}, {'id': 17, 'meta': {'id': 17}}, {'id': 18, 'meta': {'id': 18}}, {'id': 19, 'meta': {'id': 19}}, {'id': 22, 'meta': {'id': 22}}, {'id': 23, 'meta': {'id': 23}}, {'id': 24, 'meta': {'id': 24}}, {'id': 25, 'meta': {'id': 25}}, {'id': 26, 'meta': {'id': 26}}, {'id': 27, 'meta': {'id': 27}}, {'id': 28, 'meta': {'id': 28}}, {'id': 29, 'meta': {'id': 29}}, {'id': 30, 'meta': {'id': 30}}, {'id': 31, 'meta': {'id': 31}}, {'id': 32, 'meta': {'id': 32}}, {'id': 33, 'meta': {'id': 33}}, {'id': 34, 'meta': {'id': 34}}, {'id': 35, 'meta': {'id': 35}}, {'id': 36, 'meta': {'id': 36}}, {'id': 37, 'meta': {'id': 37}}, {'id': 38, 'meta': {'id': 38}}, {'id': 39, 'meta': {'id': 39}}]
2024-01-08 14:53:37,353	INFO set_read_parallelism.py:115 -- Using autodetected parallelism=20 for stage ReadRange to satisfy parallelism at least twice the available number of CPUs (10).
2024-01-08 14:53:37,353	INFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadRange->Map(fn)]
2024-01-08 14:53:37,353	INFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)
2024-01-08 14:53:37,353	INFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
test2 ds =  [{'id': 0, 'meta': {'id': 0}}, {'id': 1, 'meta': {'id': 1}}, {'id': 2, 'meta': {'id': 2}}, {'id': 3, 'meta': {'id': 3}}, {'id': 4, 'meta': {'id': 4}}, {'id': 5, 'meta': {'id': 5}}, {'id': 6, 'meta': {'id': 6}}, {'id': 7, 'meta': {'id': 7}}, {'id': 8, 'meta': {'id': 8}}, {'id': 9, 'meta': {'id': 9}}, {'id': 10, 'meta': {'id': 10}}, {'id': 11, 'meta': {'id': 11}}, {'id': 12, 'meta': {'id': 12}}, {'id': 13, 'meta': {'id': 13}}, {'id': 14, 'meta': {'id': 14}}, {'id': 15, 'meta': {'id': 15}}, {'id': 16, 'meta': {'id': 16}}, {'id': 17, 'meta': {'id': 17}}, {'id': 18, 'meta': {'id': 18}}, {'id': 19, 'meta': {'id': 19}}, {'id': 20, 'meta': {'id': 20}}, {'id': 21, 'meta': {'id': 21}}, {'id': 22, 'meta': {'id': 22}}, {'id': 23, 'meta': {'id': 23}}, {'id': 24, 'meta': {'id': 24}}, {'id': 25, 'meta': {'id': 25}}, {'id': 26, 'meta': {'id': 26}}, {'id': 27, 'meta': {'id': 27}}, {'id': 28, 'meta': {'id': 28}}, {'id': 29, 'meta': {'id': 29}}, {'id': 30, 'meta': {'id': 30}}, {'id': 31, 'meta': {'id': 31}}, {'id': 32, 'meta': {'id': 32}}, {'id': 33, 'meta': {'id': 33}}, {'id': 34, 'meta': {'id': 34}}, {'id': 35, 'meta': {'id': 35}}, {'id': 36, 'meta': {'id': 36}}, {'id': 37, 'meta': {'id': 37}}, {'id': 38, 'meta': {'id': 38}}, {'id': 39, 'meta': {'id': 39}}]
```

Please feel free to re-open the issue if I missed anything.",line lambda object used element resulting list lambda range get following result test stage satisfy parallelism least twice available number dag input map execution tip detailed progress run true test please feel free issue anything,issue,positive,positive,positive,positive,positive,positive
1881949833,"@JosephRRB What's your cluster setup (instance type, number of nodes, etc.)? I am not able to reproduce these errors on a `m5.8xlarge`.

What's the order of magnitude of the slowdown when moving from non-Ray to Ray Tune?",cluster setup instance type number able reproduce order magnitude slowdown moving ray tune,issue,negative,positive,positive,positive,positive,positive
1881944227,"This is not something we can commit to for now due to huge backlog of items we have.. Contribution is welcome.

At the same time, as @anyscalesam mentioned, you can try the managed Ray product so that you don't need to worry about managing the grafana/prometheus by yourself.",something commit due huge backlog contribution welcome time try ray product need worry,issue,positive,positive,positive,positive,positive,positive
1881939005,"@KeyOfSpectator we actually offer a managed Ray Dashboard offering as part of the Anyscale Platform. See here https://www.anyscale.com/platform for more details.

We also offer managed offering flavors specific to LLM serving in [Anyscale Endpoints ](https://www.anyscale.com/endpoints)and [Anyscale Private Endpoints](https://www.anyscale.com/private-endpoints)",actually offer ray dashboard offering part platform see also offer offering specific serving private,issue,negative,neutral,neutral,neutral,neutral,neutral
1881904078,"> Did you try it on a Mac ARM notebook as well? Were you able to test this on an Intel CPUs, perhaps on a cluster?

I tested on Linux with Intel. Most likely it has something to do with the runtime env.

We can keep the ticket open for now if you find out more information, but there is not much we can do without a repro. Feel free to remove the `needs-repro` label once you have something.
",try mac arm notebook well able test perhaps cluster tested likely something keep ticket open find information much without feel free remove label something,issue,negative,positive,positive,positive,positive,positive
1881903938,This issue has been fixed in the latest Ray version 2.9.0.,issue fixed latest ray version,issue,negative,positive,positive,positive,positive,positive
1881899901,"Can you check if there is indeed a Ray server running at ""127.0.0.1:8265""? Otherwise would be helpful to get the output of `ray logs` to see why it wasn't able to connect.",check indeed ray server running otherwise would helpful get output ray see able connect,issue,negative,positive,positive,positive,positive,positive
1881898290,The issue is resolved by using grpcio 1.44.0. You can also try to upgrade Ray to the latest version.,issue resolved also try upgrade ray latest version,issue,negative,positive,positive,positive,positive,positive
1881872662,"Hi @stephanie-wang,

Did you try it on a Mac ARM notebook as well?  Were you able to test this on an Intel CPUs, perhaps on a cluster?

I've tried step (2) you suggested, to no avail.  I think (1) and (3) require a little more time, which I will investigate later.  I've also tried installing/removing different openmp/llvm packages from my environment, which seem to have some effect, but the impedance issue remains.

I don't expect to have anything meaningful to add to this ticket over the next few days.  How should we handle its status?",hi try mac arm notebook well able test perhaps cluster tried step avail think require little time investigate later also tried different environment seem effect impedance issue remains expect anything meaningful add ticket next day handle status,issue,positive,positive,positive,positive,positive,positive
1881861789,i think it would also be very helpful to put actor logs on the replica page -- this is another level of unnecessary clicking to get to the actor logs,think would also helpful put actor replica page another level unnecessary get actor,issue,negative,negative,negative,negative,negative,negative
1881826287,"Hi @SeanHeelan !
Indeed, you can choose smaller instances with LoRA.
For example, you can choose the 70B model on a single p4de instance with 512 tokens context length.
We only supply a starting point for the full-paramter fine-tuning case and treat everything else as use-case specific - there are just too many combinations of [lora/no lora], instance type, model size, context length and even more things to cover. If possible, start with some big instance and explore your problem. Once you have settled on model size etc, you'll need to experiment a little to chip away hardware cost 🙂 ",hi indeed choose smaller lora example choose model single instance context length supply starting point case treat everything else specific many lora instance type model size context length even cover possible start big instance explore problem settled model size need experiment little chip away hardware cost,issue,negative,positive,neutral,neutral,positive,positive
1881817345,"I've applied the suggested workaround, and can confirm things looking better with it:
<img width=""813"" alt=""image"" src=""https://github.com/ray-project/ray/assets/53612764/eb8f9ffe-0d60-466c-945f-a2a2e4b27cfd"">

Looking forward to the final fix. Thanks @shrekris-anyscale @rickyyx for the extra efforts with the investigation
",applied confirm looking better image looking forward final fix thanks extra investigation,issue,positive,positive,positive,positive,positive,positive
1881735861,"For this you need to set `CUDA_VISIBLE_DEVICES` when initializing the Ray Cluster - see [here](https://docs.ray.io/en/master/ray-core/scheduling/accelerators.html#starting-ray-nodes-with-accelerators).

Here's a quick example:

```
CUDA_VISIBLE_DEVICES=1,3 ray start --head --num-gpus=2
```

```python
from ray import tune, train
import os
import time

def train_fn(config):
     time.sleep(1)
     train.report({""GPU_ID"": int(os.environ[""CUDA_VISIBLE_DEVICES""])})

train_fn = tune.with_resources(train_fn, {""cpu"": 1, ""gpu"": 1})

tuner = tune.Tuner(
    train_fn,
    tune_config=tune.TuneConfig(
        num_samples=20,
    )
)

tuner.fit()
```
```
╭──────────────────────────────────────────────────────────────────────────╮
│ Trial name             status         iter     total time (s)     GPU_ID │
├──────────────────────────────────────────────────────────────────────────┤
│ train_fn_39449_00000   TERMINATED        1            1.00536          1 │
│ train_fn_39449_00001   TERMINATED        1            1.00231          3 │
│ train_fn_39449_00002   TERMINATED        1            1.00494          1 │
│ train_fn_39449_00003   TERMINATED        1            1.00401          3 │
│ train_fn_39449_00004   TERMINATED        1            1.00452          1 │
│ train_fn_39449_00005   TERMINATED        1            1.00478          3 │
│ train_fn_39449_00006   TERMINATED        1            1.00245          1 │
│ train_fn_39449_00007   TERMINATED        1            1.00566          3 │
│ train_fn_39449_00008   TERMINATED        1            1.00446          1 │
│ train_fn_39449_00009   TERMINATED        1            1.00447          3 │
│ train_fn_39449_00010   TERMINATED        1            1.00094          1 │
│ train_fn_39449_00011   TERMINATED        1            1.00464          3 │
│ train_fn_39449_00012   TERMINATED        1            1.00449          1 │
│ train_fn_39449_00013   TERMINATED        1            1.00373          3 │
│ train_fn_39449_00014   TERMINATED        1            1.00425          1 │
│ train_fn_39449_00015   TERMINATED        1            1.00335          3 │
│ train_fn_39449_00016   TERMINATED        1            1.00407          1 │
│ train_fn_39449_00017   TERMINATED        1            1.00327          3 │
│ train_fn_39449_00018   TERMINATED        1            1.00285          1 │
│ train_fn_39449_00019   TERMINATED        1            1.00244          3 │
╰──────────────────────────────────────────────────────────────────────────╯
```",need set ray cluster see quick example ray start head python ray import tune train import o import time tuner trial name status iter total time,issue,negative,positive,positive,positive,positive,positive
1881722693,"I have same issue, great solution, please, merge asap",issue great solution please merge,issue,positive,positive,positive,positive,positive,positive
1881700096,"Tests are all green \o/, do you want to go forward @jjyao , @rkooo567 , @rickyyx , @matthewdeng , @edoakes , @scottjlee , thankks",green want go forward,issue,negative,negative,negative,negative,negative,negative
1881647024,not sure if the hermetic rule can work on windows.. seems that on osx and linux they are fine.,sure hermetic rule work fine,issue,negative,positive,positive,positive,positive,positive
1881553693,"The `TransformersTrainer` was removed in Ray 2.7 - for migration to using the `TorchTrainer`, see [this migration guide](https://docs.ray.io/en/releases-2.9.0/train/getting-started-transformers.html#transformerstrainer-migration-guide).",removed ray migration see migration guide,issue,negative,neutral,neutral,neutral,neutral,neutral
1881521977,@WIND-D @angelinalg can you help clarify the issue or the proposed fix? All three of the mentioned links make sense to me as they are right now.,help clarify issue fix three link make sense right,issue,negative,positive,positive,positive,positive,positive
1881502989,"> Thank you folks, we're on KubeRay. Where should this property go?

Set the `RAY_enable_timeline=0` env var directly on the Ray containers in both the [`headGroupSpec`](https://github.com/ray-project/kuberay/blob/5b1a5a11f5df76db2d66ed332ff0802dc3bbff76/ray-operator/config/samples/ray-service.text-ml.yaml#L49-L66) and [`workerGroupSpec`](https://github.com/ray-project/kuberay/blob/5b1a5a11f5df76db2d66ed332ff0802dc3bbff76/ray-operator/config/samples/ray-service.text-ml.yaml#L82-L94). You can follow the syntax from [this example](https://github.com/ray-project/kuberay/blob/83327f222ebe8ff5c1497f874c88a78fb384d8f3/ray-operator/config/samples/ray-service.high-availability.yaml#L135-L143).",thank property go set directly ray follow syntax example,issue,negative,positive,neutral,neutral,positive,positive
1881501313,"CC: @anyscalesam, this is the PR to reuse the existing test & test result schema we already have for release test; the schema is as follows

Test:
- name
- oncall
- state (passed, failed, flaky, jailed, etc.)
- github_issue

TestResult:
- commit
- status (passed, failed, timedout, etc.)
- buildkite_url 

This PR will put test state and github issue into a same schema, which will make the queries we need such as how many linux tests at a certain commit are passing, failing, flaky or jailed, etc. easier.",reuse test test result schema already release test schema test name state flaky commit status put test state issue schema make need many certain commit passing failing flaky easier,issue,positive,positive,positive,positive,positive,positive
1881230884,"it might be because the character limit on windows, can you try `https://github.com/ray-project/ray/blob/master/ci/env/install-bazel.sh#L128`",might character limit try,issue,negative,neutral,neutral,neutral,neutral,neutral
1881215997,"CC: @jjyao , @rickyyx , @rkooo567 I'm merging this soon to unblock pre/postmerge; let me know otherwise, thankkks",soon unblock let know otherwise,issue,negative,neutral,neutral,neutral,neutral,neutral
1881105157,"> for other cases where the total object store memory is limited, the default 3.75GB buffer size is too large.

What happens in this case? Deadlock / slowdown?

I think it'd be good to do something adaptive here. Like if there are N stages total, give each stage a buffer of size `object store memory limit / N`.",total object store memory limited default buffer size large case deadlock slowdown think good something adaptive like total give stage buffer size object store memory limit,issue,negative,positive,positive,positive,positive,positive
1881093389,"@jjyao  and @kevin85421  as you mentioned I am following the RAY REST approach and RAY JobSubmissionClient
could you please let me know if we follow the above approach how can we reuse and defined decorates (@ray.remote(num_cpus=1))

Referring now:
https://docs.ray.io/en/latest/cluster/running-applications/job-submission/doc/ray.job_submission.JobInfo.html#ray.job_submission.JobInfo ",following ray rest approach ray could please let know follow approach reuse defined,issue,negative,neutral,neutral,neutral,neutral,neutral
1880572303,"> I fear the CI builds are succeeding due to a very fragile setup that happens to work, and could break at any tool update.

fwiw, this statement is also kind of true for Linux and OSX. It is just that Windows's toolchain config steps are inherently more complex than Linux and OSX (and foreign to many developers), so when something is broken, it is harder to fix.

Ray is not really using bazel in bazel's idiomatic/recommended way. It is not using bazel to define everything in the build toolchain, but using the native ones from the system all the time. Using an old version of bazel is not really the root cause here.",fear succeeding due fragile setup work could break tool update statement also kind true inherently complex foreign many something broken harder fix ray really way define everything build native system time old version really root cause,issue,negative,positive,neutral,neutral,positive,positive
1880169161,"I am currently working on something similar, but focusing on reading partitioned dataset

Previously I also asked this #41378 : in my case, I would like to read a partitioned dataset, apply map_batch(.., batch_size=None) to each file, and save them (keeping the partition as how they were read). So technically no sort involved.",currently working something similar reading partitioned previously also case would like read partitioned apply file save keeping partition read technically sort involved,issue,positive,negative,neutral,neutral,negative,negative
1880142015,"Hi @sven1977 
when I install ray 
> pip install -U ""ray[all]""==2.9.0

I don't see the ""rllib-contrib"" to be part of the installation",hi install ray pip install ray see part installation,issue,negative,neutral,neutral,neutral,neutral,neutral
1879996036,"It is not only conda-forge support that is broken. Windows support is also broken, see #42098. I fear the CI builds are succeeding due to a very fragile setup that happens to work, and could break at any tool update.",support broken support also broken see fear succeeding due fragile setup work could break tool update,issue,negative,negative,negative,negative,negative,negative
1879994342,"> Is it possible to include redis-client but not redis sever?

I don't think that would solve the current problem. The dependency is [redis_client depends on hiredis](https://github.com/ray-project/ray/blob/18b2cab6ca79220d1d0470a60b91b1eb95c1e4a6/BUILD.bazel#L2113), and hiredis [does not build on windows](https://buildkite.com/ray-project/premerge/builds/15210#018c884b-7576-47ac-bba1-0703a94ae38b/6-11203) due to a failure to build boringssl:
```
ERROR: C:/tmp/wqe5u2zk/external/com_github_redis_hiredis/BUILD.bazel:28:11: Compiling ssl.c failed: (Exit 2): cl.exe failed: error executing command
...
external/boringssl/src/include\openssl/base.h(299): error C2059: syntax error: '<parameter-list>'
external/boringssl/src/include\openssl/pkcs7.h(123): error C2059: syntax error: '<parameter-list>'
external/boringssl/src/include\openssl/x509.h(186): error C2059: syntax error: '<parameter-list>'
external/boringssl/src/include\openssl/x509.h(189): error C2059: syntax error: '<parameter-list>'
external/boringssl/src/include\openssl/x509.h(325): error C2059: syntax error: '('
```",possible include sever think would solve current problem dependency build due failure build error exit error command error syntax error error syntax error error syntax error error syntax error error syntax error,issue,negative,negative,negative,negative,negative,negative
1879918392,"just following up to say that i did find a way to repartition a dataset by a single key but which fails for multiple keys. i got thoroughly lost in ray data internals while trying to implement the latter, more general case, but it seems like it should be possible for someone who understands the framework better. anyway, here's what i came up with:

```python
def repartition_by(
    ds: ray.data.Dataset, *, key: str, descending: bool = False
) -> ray.data.Dataset:
    """"""
    See Also:
        :meth:`ray.data.Dataset.repartition()`
    """"""
    # sort ensures that batches all have same key
    ds_sorted = ds.sort(key, descending)
    df_refs = [
        ray.put(batch)
        # null batch size ensures that all rows with same key included in each batch
        for batch in ds_sorted.iter_batches(batch_size=None, batch_format=""pandas"")
    ]
    return ray.data.from_pandas_refs(df_refs)
```",following say find way repartition single key multiple got thoroughly lost ray data internals trying implement latter general case like possible someone framework better anyway came python key descending bool false see also sort key key descending batch null batch size key included batch batch return,issue,negative,positive,neutral,neutral,positive,positive
1879545039,Just added the additional `min-width: auto` rule. Let's see what it looks like when it's done building.,added additional auto rule let see like done building,issue,negative,neutral,neutral,neutral,neutral,neutral
1879544731,"> > the root cause of the issue seems to be the tag text is too long. it has to have a max length/size at some place.
> 
> Arguably we should make the sidebar wider, or put a newline inside the label. For now, does this seem okay?
> 
> ![image](https://private-user-images.githubusercontent.com/14017872/294645647-097a0cfd-f2ab-47a4-8b23-d38b41d78c9f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDQ1MTU2MDksIm5iZiI6MTcwNDUxNTMwOSwicGF0aCI6Ii8xNDAxNzg3Mi8yOTQ2NDU2NDctMDk3YTBjZmQtZjJhYi00N2E0LThiMjMtZDM4YjQxZDc4YzlmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAxMDYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMTA2VDA0MjgyOVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTMyNGEyMTA4MzdlODM5MDQ1ZjM2MWE1MGI5MjE2N2MwZDViMTY4ZWJiODY2ODY0MjU5NWRmZDBlMzFjMjUzY2ImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.NsKFMnAKh4I5UZmwVYzGo_zD96XzBnV6JgoA7vpHJnY)

looks good.

could you just add `min-width: auto` too? so that it won't render in weird ways when hits `max-width`?",root cause issue tag text long place make put inside label seem image good could add auto wo render weird way,issue,negative,positive,neutral,neutral,positive,positive
1879530281,"failure example on new buildkite stack

https://buildkite.com/ray-project/premerge/builds/15658#018ccf27-04b7-42c3-9a1a-d5e0e440d906",failure example new stack,issue,negative,negative,neutral,neutral,negative,negative
1879522418,"> the root cause of the issue seems to be the tag text is too long. it has to have a max length/size at some place.

Arguably we should make the sidebar wider, or put a newline inside the label. For now, does this seem okay?

![image](https://github.com/ray-project/ray/assets/14017872/097a0cfd-f2ab-47a4-8b23-d38b41d78c9f)
",root cause issue tag text long place make put inside label seem image,issue,negative,negative,neutral,neutral,negative,negative
1879488182,"Yeah, let's merge it if the tests pass :)",yeah let merge pas,issue,negative,neutral,neutral,neutral,neutral,neutral
1879462025,"Sorry I missed this for too long now that it has conflicts, this looks valid, @pcmoritz do you want to rebase and merge this",sorry long valid want rebase merge,issue,negative,negative,negative,negative,negative,negative
1879461816,the root cause of the issue seems to be the tag text is too long. it has to have a max length/size at some place.,root cause issue tag text long place,issue,negative,negative,neutral,neutral,negative,negative
1879458282,"clean up my queue, feel free to add me back if need to",clean queue feel free add back need,issue,positive,positive,positive,positive,positive,positive
1879457400,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an [`ignore` condition](https://docs.github.com/en/code-security/supply-chain-security/configuration-options-for-dependency-updates#ignore) with the desired `update_types` to your config file.

If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",wo notify release get touch new version available rather skip next major minor version let know ignore major version ignore minor version also ignore major minor patch dependency ignore condition desired file change mind resolve,issue,negative,positive,neutral,neutral,positive,positive
1879327562,Unassigned myself. Is this something the Core team can update?,unassigned something core team update,issue,negative,neutral,neutral,neutral,neutral,neutral
1879326369,"Yes, let's close this. Please open a new issue if there are further issues with doc dependencies.",yes let close please open new issue doc,issue,positive,positive,neutral,neutral,positive,positive
1879325497,yup we've kept it intentionally stateless for now but we can add multi-turn capabilities soon.,kept intentionally stateless add soon,issue,negative,neutral,neutral,neutral,neutral,neutral
1879299683,"The landing page video is already indexed by Google Search Console with YouTube, and the new designs for the landing page omit the video, so let's not fix this.",landing page video already indexed search console new landing page omit video let fix,issue,negative,positive,positive,positive,positive,positive
1879280797,"@zhe-thoughts fyi, I'm merging as this affects only the release test pipeline and we need this to run some release tests on the branch, thankkks",release test pipeline need run release branch,issue,negative,neutral,neutral,neutral,neutral,neutral
1879257017," @alanwguo and I chatted a bit about the process of burning down this kind of polish/friction items.

@gvspraveen maybe we can leverage the ux friction slots we have? Or maybe we can have a new way of doing it.",bit process burning kind maybe leverage friction maybe new way,issue,positive,positive,positive,positive,positive,positive
1879231662,"Yess, let me break this down into smaller pieces.

These changes are not enough to support ml-311 yet.",let break smaller enough support yet,issue,negative,neutral,neutral,neutral,neutral,neutral
1879176488,"It looks like the size of this field never got GC https://github.com/ray-project/ray/blob/master/src/ray/protobuf/gcs.proto#L204

Not sure what's the correct logic for this. I'll sync it offline with @rickyyx ",like size field never got sure correct logic sync,issue,negative,positive,positive,positive,positive,positive
1879160640,"> @rickyyx , @rkooo567 , @stephanie-wang with this migration, osx test jobs now also turn red where there are test failures

can we mark them as soft fail on civ2 postmerge until we have it fixed? (or until we have similar flaky/failing test separation)?",migration test also turn red test mark soft fail fixed similar test separation,issue,negative,negative,neutral,neutral,negative,negative
1879144579,The memory is pretty stable. I'll dig deeper,memory pretty stable dig,issue,positive,positive,positive,positive,positive,positive
1879128463,"Update: for 2.10, we plan to (1) enable this policy with unlimited cap by default; (2) connect the new `concurrency` parameter to this policy, so users can manually specify concurrency ranges for each operator; (3) also support `concurrency` parameter for task pool-based operators. ",update plan enable policy unlimited cap default connect new concurrency parameter policy manually specify concurrency operator also support concurrency parameter task,issue,negative,positive,positive,positive,positive,positive
1879121696,How often do you see this behavior? Does it cause any other issues or is it just that the logging is wrong?,often see behavior cause logging wrong,issue,negative,negative,negative,negative,negative,negative
1879121018,"Not sure if I can do much else here since I cannot reproduce, but I'd try checking a few things:
1. Check what CPU affinity is set within Ray worker processes
2. To avoid issue of raylet inheriting some CPU affinity, manually start Ray with `ray start --head` before running any Ray script. The Ray script will connect automatically to the raylet server.
3. Check if the problem is coming from Ray / the driver script's environment by running the ""tasks"" in parallel as separate bash scripts.",sure much else since reproduce try check affinity set within ray worker avoid issue raylet affinity manually start ray ray start head running ray script ray script connect automatically raylet server check problem coming ray driver script environment running parallel separate bash,issue,negative,positive,positive,positive,positive,positive
1879100789,"@rickyyx , @rkooo567 , @stephanie-wang with this migration, osx test jobs now also turn red where there are test failures",migration test also turn red test,issue,negative,neutral,neutral,neutral,neutral,neutral
1879098450,"There are two failing osx test remaining, one is failing on master (osx://python/ray/tests:test_actor_client_mode) and one is flaky on master and also flaky fail with different test functions on retries (osx://python/ray/tests:test_plasma_unlimited)",two failing test one failing master one flaky master also flaky fail different test,issue,negative,negative,negative,negative,negative,negative
1879097844,"> How is the performance if we just fix (1)? Do we need to investigate anything besides turning off locality_with_output?

Per benchmarks done by @scottjlee , if only fixing (1), adding extra CPU nodes will hurt performance.",performance fix need investigate anything besides turning per done fixing extra hurt performance,issue,negative,neutral,neutral,neutral,neutral,neutral
1879097127,"@anyscalesam Yes let's fix it. I think it's caused by some external change (GPU out of memory error without any code change on our side).

```
(ServeReplica:default:PredictDeployment pid=12306)     attn_outputs = self.attn(
--
  | (ServeReplica:default:PredictDeployment pid=12306)   File ""/opt/miniconda/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl
  | (ServeReplica:default:PredictDeployment pid=12306)     return forward_call(*args, **kwargs)
  | (ServeReplica:default:PredictDeployment pid=12306)   File ""/opt/miniconda/lib/python3.8/site-packages/accelerate/hooks.py"", line 165, in new_forward
  | (ServeReplica:default:PredictDeployment pid=12306)     output = old_forward(*args, **kwargs)
  | (ServeReplica:default:PredictDeployment pid=12306)   File ""/tmp/ray/session_2023-11-29_11-26-47_341554_8580/runtime_resources/pip/9fc91d65ff21920e7899b7316ff121f0058d7e36/virtualenv/lib/python3.8/site-packages/transformers/models/gptj/modeling_gptj.py"", line 254, in forward
  | (ServeReplica:default:PredictDeployment pid=12306)     present = (key.to(hidden_states.dtype), value)
  | (ServeReplica:default:PredictDeployment pid=12306) torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 14.56 GiB total capacity; 11.48 GiB already allocated; 1.50 MiB free; 11.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
```
",yes let fix think external change memory error without code change side default default file line default return default file line default output default file line forward default present value default memory tried allocate mib gib total capacity gib already mib free gib reserved total reserved memory memory try setting avoid fragmentation see documentation memory management,issue,positive,positive,neutral,neutral,positive,positive
1878960268,"@architkulkarni @hongchaodeng not sure this is the place to ask.

We already have `boto3` and `botocore` installed and don't want them to be installed again while launching the cluster. Is there a way to skip installation on cluster launch? 

Also, somewhat related, we get a `.bashrc no file or directory` on launch. Not sure what the cluster is attempting to do with it, but do you know of a way to rectify this as well?",sure place ask already want cluster way skip installation cluster launch also somewhat related get file directory launch sure cluster know way rectify well,issue,positive,positive,positive,positive,positive,positive
1878871810,"@architkulkarni, ping[^1] 🛎️ 

[^1]: ... and happy New Year!",ping happy new year,issue,positive,positive,positive,positive,positive,positive
1878869183,"Yes, we need to fix this. :) 
We will (in the near future) go back to requiring the user to always bring along their (original or changed) configs when restoring.

For now as a workaround, the following hack should work:
```
from ray.rllib.utils.checkpoints import get_checkpoint_info

# Instead of calling .from_checkpoint directly, do this procedure:
checkpoint_info = get_checkpoint_info(checkpoint)
state = Algorithm._checkpoint_info_to_algorithm_state(
    checkpoint_info=checkpoint_info,
    policy_ids=None,
    policy_mapping_fn=None,
    policies_to_train=None,
)

state[""config""] = ...  # drop-in your own, altered (num_rollout_workers?) AlgorithmConfig (not old config dict!!) object here.

algo = Algorithm.from_state(state)

# This `algo` should now have/require fewer rollout workers.
```",yes need fix near future go back user always bring along original following hack work import instead calling directly procedure state state old object state,issue,positive,positive,neutral,neutral,positive,positive
1878859608,"Ok, product already onboard and informed customers - it might take a while for us to know if the customers might protest so let's do this. We can add py38 back if necessary, that's way easier than removing it.",product already informed might take u know might protest let add back necessary way easier removing,issue,negative,neutral,neutral,neutral,neutral,neutral
1878635692,"> > Hey @aloysius-lim, were you able to get a `adlfs` custom filesystem to work? Were you encountering any pickling/serialization issues like this issue: #41125
> 
> I'm sorry for the long radio silence. My issue is now resolved, thank you! I did not encounter any pickling / serialization issues.

Hey @aloysius-lim. I'm still experiencing a pickling/serialization issue with the following dependencies:
```
OS: Ubuntu 22.04
python: 3.11
adlfs: 2023.12.0
fsspec: 2023.12.0
pyarrow: 14.0.2
ray: 2.9.0
torch: 2.0.1
``` 

Are your dependencies the same as mentioned above? https://github.com/ray-project/ray/issues/40484#issue-1951251282",hey able get custom work like issue sorry long radio silence issue resolved thank encounter serialization hey still issue following o python ray torch,issue,positive,negative,neutral,neutral,negative,negative
1878328079,"Then it should really NOT go through any preprocessor code anymore and the error should disappear.

Either way, this does not change the fact that we have to clean up the model configuration process.",really go code error disappear either way change fact clean model configuration process,issue,negative,positive,positive,positive,positive,positive
1878324100,"Actually, this should be even easier:
Could you try setting the preprocessor API to False in your `experimental` settings?
```
    config.experimental(
        _enable_new_api_stack=True,
        _disable_preprocessor_api=True,
    )
```",actually even easier could try setting false experimental,issue,negative,negative,negative,negative,negative,negative
1878283447,"I can confirm this is bug on our end.

We'll provide a fix (and clean up the described conundrum thereby :) ).
In the meantime, as a quick workaround, could you simply add the extra key(s) to your MODEL_DEFAULTS.

At least the repro script above runs fine with this hack:

In `rllib/models/catalog.py`:
```
MODEL_DEFAULTS: ModelConfigDict = {
    ""unused_additional_model_key"": 1,

    # Experimental flag.
    # If True, user specified no preprocessor to be created
    # (via config._disable_preprocessor_api=True). If True, observations
    # will arrive in model as they are returned by the env.
    ""_disable_preprocessor_api"": False,
    ....
```",confirm bug end provide fix clean conundrum thereby quick could simply add extra key least script fine hack experimental flag true user via true arrive model returned false,issue,positive,positive,positive,positive,positive,positive
1878214715,"Great, @iycheng and @shrekris-anyscale. Thanks for the fast repro and investigation.",great thanks fast investigation,issue,positive,positive,positive,positive,positive,positive
1878107300,"Thanks @justinvyu for sharing the details on this, looking forward to the 2.10 release.",thanks looking forward release,issue,negative,positive,positive,positive,positive,positive
1878093255,"Solve.

obs type should be identical to observation space def and Box type should be np.float32 not others, e.g. int

```
class SimpleCorridor(gym.Env):
    """"""Example of a custom env in which you have to walk down a corridor.

    You can configure the length of the corridor via the env config.""""""

    def __init__(self, config: EnvContext):
        self.end_pos = config[""corridor_length""]
        self.cur_pos = 0
        self.action_space = Discrete(2)
        self.observation_space = Box(0, self.end_pos, shape=(1,), dtype=np.float32)  
        # Set the seed. This is only used for the final (reach goal) reward.
        self.reset(seed=42)

    def reset(self, *, seed=None, options=None):
        random.seed(seed)
        self.cur_pos = 0
        return np.array([self.cur_pos]).astype(np.float32), {}  # convert return type to  np.float32

    def step(self, action):
        assert action in [0, 1], action
        if action == 0 and self.cur_pos > 0:
            self.cur_pos -= 1
        elif action == 1:
            self.cur_pos += 1
        done = truncated = self.cur_pos >= self.end_pos
        # Produce a random reward when we reach the goal.
        return (
            np.array([self.cur_pos]).astype(np.float32),  # convert return type to  np.float32
            random.random() * 2 if done else -0.1,
            done,
            truncated,
            {},
        )
```
",solve type identical observation space box type class example custom walk corridor configure length corridor via self discrete box set seed used final reach goal reward reset self seed return convert return type step self action assert action action action action done truncated produce random reward reach goal return convert return type done else done truncated,issue,positive,neutral,neutral,neutral,neutral,neutral
1878079925,Please help investigate and fix. It's broken not due to a code change so I jail this to unblock premerge https://github.com/ray-project/ray/pull/42193.,please help investigate fix broken due code change jail unblock,issue,negative,negative,negative,negative,negative,negative
1878073611,The memory looks stable after removing it. I'll sync with @rickyyx  to see how to fix it.,memory stable removing sync see fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1878066647,let's leave it here a bit to confirm from product,let leave bit confirm product,issue,negative,neutral,neutral,neutral,neutral,neutral
1878059499,Removed the logic and run it again. Waiting for result.,removed logic run waiting result,issue,negative,neutral,neutral,neutral,neutral,neutral
1878058686,"> Sorry for the delay here. I went ahead and updated the unit tests to simplify. LGTM now.

Thanks a lot !  😁😁😁",sorry delay went ahead unit simplify thanks lot,issue,negative,negative,negative,negative,negative,negative
1878046097,This logic doesn't exist in 2.8.1. So high probably it's because of this.,logic exist high probably,issue,negative,positive,positive,positive,positive,positive
1878041249,"[prof.pdf](https://github.com/ray-project/ray/files/13837289/prof.pdf)

It seems the events data take some space. Let me increase the interval and reduce the observability events.",data take space let increase interval reduce observability,issue,negative,neutral,neutral,neutral,neutral,neutral
1878031501,"https://github.com/google/space is a new project that can solve joining unstructured data and works with Ray. https://github.com/google/space/blob/main/notebooks/segment_anything_tutorial.ipynb is an example showing how it works, and we are working towards a fully supported version.",new project solve joining data work ray example showing work working towards fully version,issue,negative,positive,positive,positive,positive,positive
1877943791,"- Remove the usage of docker, run well on CI
- The clean up can happen safely either on pre or post-command; I use pre-command since that is where the clean up is currently. post-command is currently used for buildkite annotation.",remove usage docker run well clean happen safely either use since clean currently currently used annotation,issue,positive,positive,positive,positive,positive,positive
1877926241,@cosnicolaou we are working on make multi-node training works. Will merge this PR after the release test is passed. ,working make training work merge release test,issue,negative,neutral,neutral,neutral,neutral,neutral
1877915940,Does this issue still happen in Ray 2.9?,issue still happen ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1877914324,"For (1), the warning message is not the cause, it's saying that some training worker(s) are not consuming data. It's worth checking the healthiness of the training workers. E.g., if they are still alive, and their stack traces. 
For (2), cc @woshiyyya to confirm if this is an issue from Ray Train.
For (3), it should be duplicated with https://github.com/ray-project/ray/issues/40960",warning message cause saying training worker consuming data worth healthiness training still alive stack confirm issue ray train,issue,negative,positive,positive,positive,positive,positive
1877910600,oh yeah hmm this script doesn't work on macs or windows,oh yeah script work,issue,negative,neutral,neutral,neutral,neutral,neutral
1877902279,Ongoing discussion on Ray discourse: https://discuss.ray.io/t/how-to-match-the-inference-result-after-the-dataset-id-and-batch-map/13233/3,ongoing discussion ray discourse,issue,negative,neutral,neutral,neutral,neutral,neutral
1877759864,"Hi @stephanie-wang,

Thank you very much for your reply.  I think you're onto something.  Here's my long-winded followup, please bear with me:

When I test out the script (`n_tasks>=2`) on my work laptop (MacBook Pro 2023, Apple M2 Pro, 16 GB, Sonoma 14.2.1, osx-arm64), I get some nice >12 it/s training rates for each task:
```
Epoch 0:   3%|▎         | 5/161 [00:00<00:12, 12.41it/s, v_num=3, train_loss_step=384.0]
Epoch 0:  27%|██▋       | 43/161 [00:03<00:09, 11.97it/s, v_num=3, train_loss_step=130.0]
```

But when I run it on a cluster node (Intel Xeon Gold 6230, 768 GB, x86_64, GNU/Linux, ubuntu 20.04.6,
slurm 23.02.5), I get ~0.1 it/s again:
```
Epoch 0:   1%|          | 2/161 [00:19<25:44,  0.10it/s, v_num=6.16e+7, train_loss_step=394.0]
Epoch 0:   1%|          | 2/161 [00:20<27:01,  0.10it/s, v_num=6.16e+7, train_loss_step=320.0]
```

In both cases, I use conda 23.11.0, and installed my environments with the following steps:
```
conda create -n tft python=3.10 numpy xarray pandas cython netcdf4 scipy 
conda activate tft
pip install pytorch-forecasting
pip install -U 'ray[default]'
```

I looked at my two environments (MacBook vs. cluster), and I do notice that on typing `conda list openmp`, the MacBook env has
```
# Name                    Version                   Build  Channel
llvm-openmp               14.0.6               hc6e5704_0  
```
whereas the cluster env has
```
# Name                    Version                   Build  Channel                                                               
_openmp_mutex             4.5                  2_kmp_llvm    conda-forge                                                         
llvm-openmp               14.0.6               h9e868ea_0
```

I wonder if the discrepancy comes from the issue mentioned [here](https://github.com/pytorch/pytorch/issues/99625)?  Following some of the suggestions within, I rebuilt my env on the cluster.  Now `conda list openmp` shows
```
# Name                    Version                   Build  Channel
_openmp_mutex             5.1                       1_gnu  
intel-openmp              2023.1.0         hdb19cb5_46306  
``` 
and running the script with `n_tasks=2` yields
```
Epoch 0:   2%|▏         | 3/161 [00:01<01:14,  2.11it/s, v_num=6.16e+7, train_loss_step=368.0]
Epoch 0:   1%|          | 2/161 [00:01<01:34,  1.68it/s, v_num=6.16e+7, train_loss_step=320.0]
```
whereas `n_tasks=4` gives
```
Epoch 0:   2%|▏         | 4/161 [00:04<02:48,  0.93it/s, v_num=6.16e+7, train_loss_step=298.0]
Epoch 0:   2%|▏         | 3/161 [00:04<03:49,  0.69it/s, v_num=6.16e+7, train_loss_step=369.0]
```
While it is improvement, it does show that the total throughput is ~3.7 it/s (the `n_tasks=1` case), and that adding workers just linearly scales down that rate.  This is true whether I set `num_workers=0 or 1`, and it still indicates competition between worker

Long story short, these are my new finds thus far:

1. On my MacBook, Ray parallelization works.
2. On my Intel cluster, the workers seems to compete with each other for resources.  The situation is slightly ameliorated via updates to `openmp` packages, but the impedance remains.

I wonder if you observe the same trends on your end?",hi thank much reply think onto something please bear test script work pro apple pro get nice training task epoch epoch run cluster node gold get epoch epoch use following create activate pip install pip install default two cluster notice list name version build channel whereas cluster name version build channel wonder discrepancy come issue following within rebuilt cluster list name version build channel running script epoch epoch whereas epoch epoch improvement show total throughput case linearly scale rate true whether set still competition worker long story short new thus far ray parallelization work cluster compete situation slightly via impedance remains wonder observe end,issue,positive,positive,positive,positive,positive,positive
1877746393,"Unfortunately this is still an issue.

Running a test job with unittest (`python -m unittest test_ray.py`) produces multiple ResourceWarnings like the original comment by @09wakharet.
```python
# contents of test_ray.py
import ray
import unittest

class RayTest(unittest.TestCase):
    def setUp(self):
        self.ray_ctx = ray.init(num_cpus=1, num_gpus=0)

    def tearDown(self):
        ray.shutdown()

    def test_method(self):
        pass
```

I have tested this with Python 3.10 on macOS Big Sur/Ray==2.8 and macOS Sonoma/Ray==2.9.",unfortunately still issue running test job python multiple like original comment python content import ray import class setup self teardown self self pas tested python big,issue,negative,negative,neutral,neutral,negative,negative
1877681228,Quick update: this issue is on hold until a few questions about the design are resolved.,quick update issue hold design resolved,issue,negative,positive,positive,positive,positive,positive
1877632712,"Here are the keys from the internal kv list in the Ray 2.9.0 cluster:

<details>
  <summary>2.9.0 KV keys</summary>

  ```
  >>> import ray
  >>> import ray.experimental.internal_kv as kv
  >>> ray.init(address=""auto"")
  RayContext(dashboard_url='session-44rt3qcdjplsk1v8pl23eg875t.i.anyscaleuserdata-staging.com', python_version='3.9.18', ray_version='2.9.0', ray_commit='34ab695d5248aff4ddecbf5fb7d6e8035f74437b', protocol_version=None)
  >>> import json
  >>> print(json.dumps([str(key) for key in kv._internal_kv_list("""")], indent=4))
  [
      ""b'CLUSTER_METADATA'"",
      ""b'ActorClass:01000000:](\\xc1\\xe2\\xe9\\x1e\\xb6\\x9b\\x96\\xbc\\x0f\\xdc&\\x90\""\\x95Y@\\xcf\\xf5\\x07\\xbai\\x9f\\x9f9f\\xfe'"",
      ""b'extra_usage_tag_serve_http_proxy_used'"",
      ""b'extra_usage_tag_serve_num_apps'"",
      ""b'extra_usage_tag_serve_num_deployments'"",
      ""b'extra_usage_tag_serve_num_gpu_deployments'"",
      ""b'temp_dir'"",
      ""b'DASHBOARD_AGENT_PORT_PREFIX:8a1c4ff64b9a2ad2ccaddd3b4c5a88972f5ef16bb895cfdef755013e'"",
      ""b'DASHBOARD_AGENT_PORT_PREFIX:51b003aa8458aaed48d101050c3429d5eb6b0cd4360dcf848302dbb8'"",
      ""b'dashboard'"",
      ""b'extra_usage_tag_gcs_storage'"",
      ""b'DASHBOARD_AGENT_PORT_PREFIX:239fec11187df96c8e7608901552ecabfefdd5c3a8c3a8d7e56ee0a1'"",
      ""b'DashboardMetricsAddress'"",
      ""b'DASHBOARD_AGENT_PORT_PREFIX:b0986c77e97fca702edc1ce3599d387de773ebfee7b54f996bb25f9f'"",
      ""b'SERVE_CONTROLLER_ACTOR-_ray_internal_dashboard-serve-application-state-checkpoint'"",
      ""b'SERVE_CONTROLLER_ACTOR-_ray_internal_dashboard-serve-app-config-checkpoint'"",
      ""b'SERVE_CONTROLLER_ACTOR-_ray_internal_dashboard-serve-deployment-state-checkpoint'"",
      ""b'extra_usage_tag_dashboard_metrics_prometheus_enabled'"",
      ""b'SERVE_CONTROLLER_ACTOR-_ray_internal_dashboard-serve-endpoint-state-checkpoint'"",
      ""b'extra_usage_tag_num_drivers'"",
      ""b'extra_usage_tag_serve_get_deployment_handle_api_used'"",
      ""b'RemoteFunction:01000000:\\xe6\\x9bI\\xc8\\x99\\xcd\\x91\\xcb\\xbb\\xf6]\\xecj\\x05\\x0c\\xa8\\tU{Y\\xe1l\\x04\\xa7$3\\x04O'"",
      ""b'RemoteFunction:01000000:\\x1e>\\xdc\\xfa\\xe9&l\\x88?+\\xc8\\x17\\x86\\xb5\\x8cUt\\x7f~TQ\\xc7B%l\\xad\\xae\\x9d'"",
      ""b'library_usage_serve'"",
      ""b'extra_usage_tag_pg_num_created'"",
      ""b'extra_usage_tag_serve_rest_api_version'"",
      ""b'extra_usage_tag_serve_api_version'"",
      ""b'ActorClass:01000000:\\x1d\\x19\\xaa+\\xce\\x87\\xc7\\xd3\\x06\\xc6\\x9et\\x1a\\xb3\\xcb\\xece\\x99\\x02i\\xfe\\xb2UIo\\x07>\\x8f'"",
      ""b'head_node_id'"",
      ""b'DASHBOARD_AGENT_PORT_PREFIX:ca9e904f57a85065adc08c45d3487dfd6cf3632abc544c53a6e43e19'"",
      ""b'webui:url'"",
      ""b'SERVE_CONTROLLER_ACTOR-_ray_internal_dashboard-serve-logging-config-checkpoint'"",
      ""b'ActorClass:01000000:F\\xb49\\xa2\\xb5\\x97\\x7f\\xa76\\xdd\\xc7\\xb0\\n\\xf9\\xc7\\xc9+5\\x9es\\x00\\x96\\xe2\\xa2\\xbc\\x7f|8'"",
      ""b'ray_cluster_id'"",
      ""b'ray_client_server'"",
      ""b'DASHBOARD_AGENT_PORT_PREFIX:171f40ad49706c7557fb3e6d5f99888a1399eb91eec1c8d94a1a9d09'"",
      ""b'DASHBOARD_AGENT_PORT_PREFIX:6b833e51c96ef6b7a1cdb4db86afce4c2ee8e2468bb822bd18bfc38a'"",
      ""b'extra_usage_tag_actor_num_created'"",
      ""b'hardware_usage_Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz'"",
      ""b'DASHBOARD_AGENT_PORT_PREFIX:d5c9da700adc3c0046e412223f7ba63f4eb94cacfdfa304a64d8cc33'"",
      ""b'DASHBOARD_AGENT_PORT_PREFIX:c600bdaa5fde305e6ad5a0861e72a6d50cfaa0ac829eee898bd9874e'"",
      ""b'session_dir'"",
      ""b'DASHBOARD_AGENT_PORT_PREFIX:550c9f52029ca67360b183880513790f8b02334d95d0c7aa1e363b95'"",
      ""b'extra_usage_tag_dashboard_metrics_grafana_enabled'"",
      ""b'extra_usage_tag_serve_fastapi_used'"",
      ""b'__autoscaler_v2_enabled'"",
      ""b'extra_usage_tag_num_actor_tasks'"",
      ""b'extra_usage_tag_num_actor_creation_tasks'"",
      ""b'session_name'"",
      ""b'DASHBOARD_AGENT_PORT_PREFIX:bc01916ca0020fdb0d6c0a2b77177e3caef8ab023bb63ce0c20b971e'"",
      ""b'extra_usage_tag_num_normal_tasks'"",
      ""b'extra_usage_tag_dashboard_used'"",
      ""b'dashboard_rpc'""
  ]
  >>> len(kv._internal_kv_list(""""))
  53
  ```
</details>

And the 2.8.1 cluster:

<details>
  <summary>2.8.1 KV keys</summary>

  ```
  >>> import ray
  >>> import ray.experimental.internal_kv as kv
  >>> import json
  >>> ray.init(address=""auto"")
  RayContext(dashboard_url='session-g9ruxv1ijt26a5h2gh142v7slg.i.anyscaleuserdata-staging.com', python_version='3.9.15', ray_version='2.8.1', ray_commit='523c184201976c46d1be3a60d461c4bd9b5e473a', protocol_version=None)
  >>> print(json.dumps([str(key) for key in kv._internal_kv_list("""")], indent=4))
  [
      ""b'extra_usage_tag_num_normal_tasks'"",
      ""b'extra_usage_tag_num_actor_creation_tasks'"",
      ""b'CLUSTER_METADATA'"",
      ""b'dashboard'"",
      ""b'extra_usage_tag_serve_get_deployment_handle_api_used'"",
      ""b'extra_usage_tag_serve_num_gpu_deployments'"",
      ""b'DASHBOARD_AGENT_PORT_PREFIX:e81beb9ea4d005441b87b21372c6c37bdf6f6d4c06b832120b093675'"",
      ""b'DASHBOARD_AGENT_PORT_PREFIX:114c6a567dcfc43f652a9148854d3b48723cf0870eb4b8bdc2e78500'"",
      ""b'head_node_id'"",
      ""b'dashboard_rpc'"",
      ""b'DASHBOARD_AGENT_PORT_PREFIX:b608ef87ab5c62f617638555cfe0fd3653cd2dda06def1c9f8511089'"",
      ""b'extra_usage_tag_actor_num_created'"",
      ""b'DASHBOARD_AGENT_PORT_PREFIX:479628f252c0a1b04885e8c5c4383ee6b3de28666a910c621de4e532'"",
      ""b'ActorClass:01000000:\\xab\\xd6\\xe3S\\xc9\\x871\\x14\\xb3\\xe7:&\\xa0\\xbe\\x9a\\xb7\\x99\\xb0\\xd2F\\xb4\\xf2\\xc2\\xe6\\xb9\\x87\\xfbP'"",
      ""b'DASHBOARD_AGENT_PORT_PREFIX:c860039280cc6f264ca88e3c2de94c31fe69a250f90547f1a477058d'"",
      ""b'extra_usage_tag_dashboard_metrics_prometheus_enabled'"",
      ""b'SERVE_CONTROLLER_ACTOR-_ray_internal_dashboard-serve-endpoint-state-checkpoint'"",
      ""b'DASHBOARD_AGENT_PORT_PREFIX:2243b1dc05e3a8219394a6b44ff496bbb278620c0a0cac5e7a77dd58'"",
      ""b'DASHBOARD_AGENT_PORT_PREFIX:8fb32b2efaa3f04351bd14d9adc480143d681cfca856038d6e44c7c6'"",
      ""b'DashboardMetricsAddress'"",
      ""b'DASHBOARD_AGENT_PORT_PREFIX:f9f3025727cd75a00780cea6e863a2e99191cbd939378d0bb58ab4be'"",
      ""b'extra_usage_tag_serve_fastapi_used'"",
      ""b'extra_usage_tag_num_drivers'"",
      ""b'extra_usage_tag_core_state_api_get_log'"",
      ""b'extra_usage_tag_serve_api_version'"",
      ""b'extra_usage_tag_serve_rest_api_version'"",
      ""b'extra_usage_tag_dashboard_used'"",
      ""b'extra_usage_tag_serve_http_proxy_used'"",
      ""b'RemoteFunction:01000000:\\xe0\\xae\\xb4\\xf0\\xbb;H\\xc87\\xd7\\xd1\\x9e;\\x92\\x0f\\xa6\\xb6\\x1f)\\xe8ctn\\xb0;\\xb8\\x92O'"",
      ""b'SERVE_CONTROLLER_ACTOR-_ray_internal_dashboard-serve-app-config-checkpoint'"",
      ""b'ray_client_server'"",
      ""b'__autoscaler_v2_enabled'"",
      ""b'library_usage_serve'"",
      ""b'temp_dir'"",
      ""b'ActorClass:01000000:w|\\x81_\\xfa1\\x7fe\\x17\\xc10\\xfa\\xd8C~:\\x06\\xe7\\xe3+\\xe1#\\xe9\\x1dH\\xfc\\xd3-'"",
      ""b'extra_usage_tag_gcs_storage'"",
      ""b'DASHBOARD_AGENT_PORT_PREFIX:ce50e1d00f14860b813ead9425f25c00d4cdc3d8de214935fda0aaf4'"",
      ""b'session_dir'"",
      ""b'DASHBOARD_AGENT_PORT_PREFIX:4c6c065351a39adbbeb67c304acdc6cfafb285bc0625440625e3e191'"",
      ""b'webui:url'"",
      ""b'session_name'"",
      ""b'ActorClass:01000000:R\\xa3JE\\xb7R\\x0fFJ8\\xfe\\xac\\xb6\\x0c\\xf9\\x98\\xe2g\\x04\\xbc\\x00\\xf8p\\x11\\xf2\\xa6\\xbc`'"",
      ""b'SERVE_CONTROLLER_ACTOR-_ray_internal_dashboard-serve-application-state-checkpoint'"",
      ""b'extra_usage_tag_num_actor_tasks'"",
      ""b'DASHBOARD_AGENT_PORT_PREFIX:0f1422470667b3f48f546f6150d094b6ef7c1a1ab815953965873c95'"",
      ""b'RemoteFunction:01000000:H\\xc0\\xeeu\\x12\\xedy\\xbeN\\x919\\xed\\x84\\xd9\\x9fC\\xc0\\xf40\\xd4\\xc0~A\\xcf\\xc0\\x19\\xd1\\xe2'"",
      ""b'extra_usage_tag_pg_num_created'"",
      ""b'extra_usage_tag_dashboard_metrics_grafana_enabled'"",
      ""b'extra_usage_tag_serve_num_deployments'"",
      ""b'SERVE_CONTROLLER_ACTOR-_ray_internal_dashboard-serve-deployment-state-checkpoint'"",
      ""b'ray_cluster_id'"",
      ""b'extra_usage_tag_serve_num_apps'""
  ]
  >>> len(kv._internal_kv_list(""""))
  52
  ```
</details>

",internal list ray cluster summary import ray import auto import print key key platinum cluster summary import ray import import auto print key key,issue,negative,neutral,neutral,neutral,neutral,neutral
1877564256,@justinvyu can we upgrade this to p0 given it's a linux release test and something when it gets to ray210 release we would block correct?,upgrade given release test something ray release would block correct,issue,negative,neutral,neutral,neutral,neutral,neutral
1877554331,"> Thank you @rickyyx . Do you think autoscaler_v2 is something the end user can try right now? Or it's still recommended to wait.

It's still under active dev - so still not yet ready. ",thank think something end user try right still wait still active dev still yet ready,issue,positive,positive,positive,positive,positive,positive
1877535515,"The process growing in memory seems to be the GCS server. Here's some `top` outputs over time (courtesy of @morhidi):

<details>
  <summary>top outputs</summary>

  <img width=""632"" alt=""Screenshot 2024-01-04 at 10 01 11 AM"" src=""https://github.com/ray-project/ray/assets/92341594/910193d4-876a-496e-b9fe-9d76193c5e49"">

  <img width=""631"" alt=""Screenshot 2024-01-04 at 9 59 10 AM"" src=""https://github.com/ray-project/ray/assets/92341594/20bbdbeb-606d-4905-a796-6445ac4263d5"">
  
  <img width=""631"" alt=""Screenshot 2024-01-04 at 9 59 22 AM"" src=""https://github.com/ray-project/ray/assets/92341594/53b25aa6-748b-4bc2-8201-9fcfa6583a3c"">

</details>",process growing memory server top time courtesy summary top,issue,positive,positive,positive,positive,positive,positive
1877528675,"Thanks @morhidi for all your help with narrowing down this issue. With the following repro, we observe a memory leak:

<details>
  <summary>Repro</summary>

  ```python
  import logging
  
  from fastapi import FastAPI, Request
  from fastapi.encoders import jsonable_encoder
  from ray import serve
  from starlette.responses import JSONResponse
  
  logger = logging.getLogger(""ray.serve"")
  
  app = FastAPI()
  
  
  @serve.deployment
  @serve.ingress(app)
  class ModelServer:
  
      def __init__(self):
          logger.info(""Initialized"")
  
  
      @app.post(""/inference"")
      def inference(self, request: Request) -> JSONResponse:
  
          response = {
              ""result"": ""OK""
          }
  
          return JSONResponse(content=jsonable_encoder(response))
  
  esp_model_app = ModelServer.bind()
  ```
</details>

2.8.1 head node memory:

<img width=""1357"" alt=""Screenshot 2024-01-04 at 9 55 10 AM"" src=""https://github.com/ray-project/ray/assets/92341594/e58d8dc4-4ced-4db3-80df-e5d9642e9da5"">

2.9.0 head node memory:

<img width=""1353"" alt=""Screenshot 2024-01-04 at 9 55 51 AM"" src=""https://github.com/ray-project/ray/assets/92341594/9307fb6e-b652-46d8-8159-9fe6386a1de6"">",thanks help issue following observe memory leak summary python import logging import request import ray import serve import logger class self inference self request request response result return response head node memory head node memory,issue,negative,positive,neutral,neutral,positive,positive
1877511979,"Looks great. Waiting for tests to pass, then merge.",great waiting pas merge,issue,positive,positive,positive,positive,positive,positive
1877367308,"Hi @chunweiyuan I tried out your script on my laptop and wasn't able to reproduce with num_tasks=2 and 4. The only changes I made were to the `ray.init` line (my machine did not have enough memory, and setting `num_cpus` manually should not be necessary).

```python
ray.init(include_dashboard=False,
         #num_cpus=n_tasks + 1,  # just some number >= n_tasks
         #object_store_memory=50 * 1e9,  # large enough
         runtime_env=runtime_env)    
```

Can you provide more details on the machine that you're running on? Perhaps there is just not enough physical compute.",hi tried script able reproduce made line machine enough memory setting manually necessary python number large enough provide machine running perhaps enough physical compute,issue,negative,positive,positive,positive,positive,positive
1877120384,Sorry for the delay here. I went ahead and updated the unit tests to simplify. LGTM now.,sorry delay went ahead unit simplify,issue,negative,negative,negative,negative,negative,negative
1877114867,"> Hey @aloysius-lim, were you able to get a `adlfs` custom filesystem to work? Were you encountering any pickling/serialization issues like this issue: #41125

I'm sorry for the long radio silence. My issue is now resolved, thank you! I did not encounter any pickling / serialization issues.",hey able get custom work like issue sorry long radio silence issue resolved thank encounter serialization,issue,positive,negative,neutral,neutral,negative,negative
1876882438,@brenting Great catch! Thanks for filing this! We are reworking the multi-agent setup for our new stack to be released this year and will take a look into it soon.,great catch thanks filing setup new stack year take look soon,issue,positive,positive,positive,positive,positive,positive
1876360914,"> Good find.
> 
> * Have you manually confirmed that it fixes the issue in the test?
> * Any automated testing that can/should be added here?

1. Tested locally to make sure there is no traceback in the log.
2. Add unit test to make sure the last message should always be set. (not None)",good find manually confirmed issue test testing added tested locally make sure log add unit test make sure last message always set none,issue,positive,positive,positive,positive,positive,positive
1876258531,"Working now, merge to unblock master",working merge unblock master,issue,negative,neutral,neutral,neutral,neutral,neutral
1876202096,Thank you @rickyyx . Do you think autoscaler_v2 is something the end user can try right now? Or it's still recommended to wait.  ,thank think something end user try right still wait,issue,negative,positive,positive,positive,positive,positive
1876174987,"> The clean up for mac should also be done in a job clean up step/post-command hook and not in the life-time of the step

I totally agree on this! should we just do that instead? and just remove this `cleanup` thing in the `upload` script? it is actually weird that after ""uploading"", the files will just disappear from the filesystem.. (that should be called ""ascending"")",clean mac also done job clean hook step totally agree instead remove cleanup thing script actually weird disappear ascending,issue,positive,positive,neutral,neutral,positive,positive
1876151098,The clean up for mac should also be done in a job clean up step/post-command hook and not in the life-time of the step,clean mac also done job clean hook step,issue,positive,positive,positive,positive,positive,positive
1876080489,"> > the best way forward seems to just not include redis for windows build
> 
> I see `redis-client` is a dependency of `_raylet`, `gcs_pub_sub_lib`, `gcs_client_lib`, `gcs_table_storage_test_lib`, `gcs_table_storage_lib`, `chaos_redis_store_client_test`. Will any of that work without redis?

Is it possible to include redis-client but not redis server?",best way forward include build see dependency work without possible include server,issue,positive,positive,positive,positive,positive,positive
1876074481,@Crer-lu is this off the latest nightly wheel only? are you able to repro with either latest stable ray29?,latest nightly wheel able either latest stable ray,issue,negative,positive,positive,positive,positive,positive
1876073345,"I didn't run the lora part. But for 7B, 13B, and 70B full parameter fine-tuning, I can run the template code without these 2 dependencies. I didn't compare the performances, etc.

Someone should confirm whether they are needed:
- for lora
- for better performance
- etc.",run lora part full parameter run template code without compare someone confirm whether lora better performance,issue,negative,positive,positive,positive,positive,positive
1876064024,"> @vitsai @anyscalesam @rickyyx Could you please share more about how to use autoscaler v2 in ray29? Didn't find related doc

Hey @llidev we are still working on the fix with autoscaler v1. @vitsai has a PR here https://github.com/ray-project/ray/pull/40488: , while we try doing so, we are also working on v2 autoscaler. This has been delayed due to other priority, so it's not available with ray29 yet. ",could please share use ray find related doc hey still working fix try also working due priority available ray yet,issue,positive,positive,neutral,neutral,positive,positive
1876022041,seems that Jonathan is an intern and has already left the project,intern already left project,issue,negative,neutral,neutral,neutral,neutral,neutral
1876015371,Let's discuss the design in person maybe tomorrow. ,let discus design person maybe tomorrow,issue,negative,neutral,neutral,neutral,neutral,neutral
1876012290,@vitsai @anyscalesam @rickyyx  Could you please share more about how to use autoscaler v2 in ray29? Didn't find related doc,could please share use ray find related doc,issue,positive,neutral,neutral,neutral,neutral,neutral
1875999049,@anyscalesam Please see the gist I've linked at the top of this issue. Let me provide the link again: https://gist.github.com/FlorinAndrei/c4b9fed2e2bf63998e521526bb8b49e2,please see gist linked top issue let provide link,issue,negative,positive,positive,positive,positive,positive
1875891944,@jonathan-anyscale is `attach_profile()` part of ray public API? is it okay to add the `verbose` arg?,part ray public add verbose,issue,negative,neutral,neutral,neutral,neutral,neutral
1875891047,should be fixed. please take another look,fixed please take another look,issue,negative,positive,neutral,neutral,positive,positive
1875869378,"as @cadedaniel said, to build ray, it needs to run `ci/env/install-bazel.sh` to install bazel, or at least needs to respect the bazel version in `.bazeliskrc` (like just use `bazelisk` with `bazel` symlinked to `bazelisk`).

https://docs.ray.io/en/latest/ray-contribute/development.html#preparing-to-build-ray-on-linux

I agree that the build toolchain setup is a bit convoluted right now. we are working on simplifying it.

we do not have plans to upgrade bazel in the near term. to upgrade bazel, a big cleanup needs to be performed first, on the (weird) use of bazel rules in the ray repo today, and discarding old, legacy stuff from the repo that is not worth upgrading. there is no planned timeline for now.

although upgrading bazel is a nice to have, the current version of bazel serves okay for our purpose of building/testing/releasing ray. and upgrading it does not seem to give us any immediate benefit.

fwiw, we will probably upgrade bazel some time in the future when the repo structure is cleaner, and hence upgrading can be easier.

specifically for the grpc related issues, it is unclear about the urgency, or even if upgrading grpc is the right way to go at the moment. in the worst case, the grpc stubs can be generated in other ways. upgrading bazel is not a must.

unfortunately, conda-forge support is not really in our scope today afaik. (sorry, conda-forge friends..) if you are looking for smoother integration in the long run, maybe consider reaching out to @zhe-thoughts or Ray leadership for a more formal collaboration deal or something. :)

----

I am closing this issue as the original issue (cannot build with 6.0.0) is ""intended behavior"".

for ""ray's build tool chain needs to upgrade to bazel 6"", the current answer is ""won't fix"". sorry.",said build ray need run install least need respect version like use agree build setup bit convoluted right working upgrade near term upgrade big cleanup need first weird use ray today old legacy stuff worth although nice current version purpose ray seem give u immediate benefit probably upgrade time future structure cleaner hence easier specifically related unclear urgency even right way go moment worst case way must unfortunately support really scope today sorry looking smoother integration long run maybe consider reaching ray leadership formal collaboration deal something issue original issue build intended behavior ray build tool chain need upgrade current answer wo fix sorry,issue,positive,negative,neutral,neutral,negative,negative
1875818961,"@tchordia , please take a look in this PR: https://github.com/alanwguo/ray/pull/76#issue-2053469039

@csivanich, the top-level page will show all deployments across all applications. Each deployment has a column for the parent application it belongs to. A user can click the application name to go into an application detail page if they only want to see info about a single application.

The idea here is we were trying to address the feedback that it takes too many clicks to get to valuable information and that majority of workflows, people care primarily about deployments and not applications. So this is moving things from being ""applications first"" to ""deployments first""

",please take look page show across deployment column parent application user click application name go application detail page want see single application idea trying address feedback many get valuable information majority people care primarily moving first first,issue,positive,positive,positive,positive,positive,positive
1875778759,"> Look like test failed, is that expected?

nope. trying to fix it now.",look like test nope trying fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1875732286,"@edoakes @shrekris-anyscale I have addressed all the comments and removed the need to serialize policy with base64 (thanks to the idea on separating them into 2 fields). PTAL again when you have a sec!
",removed need serialize policy base thanks idea separating sec,issue,negative,negative,negative,negative,negative,negative
1875702651,@AmirBFar bump - have you had a chance to try latest ray (we're on ray29 now). If this issue still occurs please reopen with error output and repro script.,bump chance try latest ray ray issue still please reopen error output script,issue,negative,positive,positive,positive,positive,positive
1875696532,Closing as no response and been > 2w; @AvisP please reopen when/if you have a response to @mattip ,response please reopen response,issue,negative,neutral,neutral,neutral,neutral,neutral
1875693716,"note that this test will fail when `/tmp` is a later version of overlayfs, and removing the file seems not to free the space from the disk.",note test fail later version removing file free space disk,issue,negative,negative,neutral,neutral,negative,negative
1875688401,@FlorinAndrei can you share the error output when it crashes? Might be an Ubuntu ver issue (311 should work fine though...),share error output might issue work fine though,issue,negative,positive,positive,positive,positive,positive
1875438450,"It's been a while so I don't know if it's still the case, but I think you can take any of your examples, add evaluation workers and enable the learner api",know still case think take add evaluation enable learner,issue,negative,neutral,neutral,neutral,neutral,neutral
1875414727,@RobinKa Thanks for filing this! Can you provide a reproducable example? ,thanks filing provide example,issue,negative,positive,positive,positive,positive,positive
1874778272,"@maxpumperla If the build looks good to you, would you be able to merge this?",build good would able merge,issue,negative,positive,positive,positive,positive,positive
1874750082,"This fixes the problem only partly for victoria metrics.
The path to use for health: http://vmselect-victoria-metrics.monitoring.svc.cluster.local:8481/-/healthy with response: `VictoriaMetrics is Healthy`
path for query: http://vmselect-victoria-metrics.monitoring.svc.cluster.local:8481/select/0/prometheus
As there only one setting for RAY_PROMETHEUS_HOST, there no way to get data_head.py working, as then always the health check fails.",problem partly metric path use health response healthy path query one setting way get working always health check,issue,negative,positive,positive,positive,positive,positive
1874716440,@architkulkarni - would you able to provide fix or insight into the issue? we are still unable to do multi-node training using Ray to conclude our POC,would able provide fix insight issue still unable training ray conclude,issue,negative,neutral,neutral,neutral,neutral,neutral
1874652240,"Yes, the VM launcher belongs to the OSS infra team. Archit is currently OOO and will be back next Monday. cc @architkulkarni 

",yes launcher infra team currently back next,issue,negative,neutral,neutral,neutral,neutral,neutral
1874630284,@kevin85421 is this one go to oss infra team? thankks,one go infra team,issue,negative,neutral,neutral,neutral,neutral,neutral
1874589079,"So strictly speaking this doesn't make any changes to the example gallery. @simran-2797 how do you want to proceed? We're still going to want to redirect people to the example gallery from the separate library example pages, right?",strictly speaking make example gallery want proceed still going want redirect people example gallery separate library example right,issue,negative,positive,positive,positive,positive,positive
1874519462,"@ray-project/ray-serve Hi folks, I've addressed the comments that have been left so far (thanks for those who reviewed!), could folks help take another look?",hi left far thanks could help take another look,issue,positive,positive,positive,positive,positive,positive
1874488233,"Right, I looked at those logs and couldn't figure out where my build differs. The buildkite build uses a docker image, I wonder if something there is very different from my local setup (and Azure Build Pipeline as used by conda-forge) that is causing the build to fail.",right could figure build build docker image wonder something different local setup azure build pipeline used causing build fail,issue,negative,negative,neutral,neutral,negative,negative
1874473129,"Mostly concerned about this base64 encoding and dynamic behavior of the policy field.

If something nonstandard like this is required in a PR I expect it to be called out in the description to let reviewers know why it's necessary and what alternatives there are, if any.",mostly concerned base dynamic behavior policy field something nonstandard like expect description let know necessary,issue,positive,negative,neutral,neutral,negative,negative
1874458331,"Hi @stephanie-wang,

1. Yes, I believe under-the-hood Dataloader uses Python's multiprocessing package.  The high-level instruction for the `num_workers` is [here](https://lightning.ai/docs/pytorch/stable/advanced/speed.html#num-workers).  

2. The default multiprocessing context, in Unix, seems to be `fork` [link](https://discuss.pytorch.org/t/the-default-value-of-dataloader-multiprocessing-context-is-spawn-in-a-spawned-process/107494).  On my end I've experimented with various permutations of `num_workers=0 or 1` and `multiprocessing_context=""fork"" or ""spawn"" or ""forkserver""`, and none of them solves the bottleneck of `n_tasks > 1`.

3. I have played with `os.sched_setaffinity()` like this within my Ray task:
    ```
    cpu_ids = list(os.sched_getaffinity(0))

    print(seed, os.sched_getaffinity(0))
    print(f""setting seed {seed} to use {cpu_ids[seed]}"")

    os.sched_setaffinity(0, [cpu_ids[seed]])
    print(f""now seed {seed} uses cpu_ids: "", os.sched_getaffinity(0))
    ```
    and obtained the following
    ```
    (train_task pid=1380821) 1 {96, 98, 100, 102, 40, 42, 44, 46, 48, 50, 92, 94}
    (train_task pid=1380821) setting seed 1 to use 98
    (train_task pid=1380821) now seed 1 uses cpu_ids:  {98}
    (train_task pid=1380821) [rank: 0] Seed set to 1
    (train_task pid=1380820) [rank: 0] Seed set to 0
    (train_task pid=1380820) 0 {96, 98, 100, 102, 40, 42, 44, 46, 48, 50, 92, 94}
    (train_task pid=1380820) setting seed 0 to use 96
    (train_task pid=1380820) now seed 0 uses cpu_ids:  {96}
    ```
    but then everything moves _even slower_, whether I set `num_workers=0 or 1`:
    ```
    Epoch 0:   1%|          | 1/161 [01:10<3:08:37,  0.01it/s, v_num=5.98e+7, train_loss_step=301.0]
    Epoch 0:   1%|          | 1/161 [01:10<3:08:48,  0.01it/s, v_num=5.98e+7, train_loss_step=331.0]
    Epoch 0:   1%|          | 2/161 [02:21<3:07:35,  0.01it/s, v_num=5.98e+7, train_loss_step=394.0]
    Epoch 0:   1%|          | 2/161 [02:21<3:08:01,  0.01it/s, v_num=5.98e+7, train_loss_step=320.0]
    ```

Not sure if the root cause of the problem is exactly [this](https://github.com/pytorch/pytorch/issues/99625), but I have tried some of their suggestions, to no avail.",hi yes believe python package instruction default context fork link end experimented various fork spawn none bottleneck like within ray task list print seed print setting seed seed use seed seed print seed seed following setting seed use seed rank seed set rank seed set setting seed use seed everything whether set epoch epoch epoch epoch sure root cause problem exactly tried avail,issue,negative,negative,negative,negative,negative,negative
1874412177,"How does this work with multiple applications, seeing that Ray 2.9 no longer has a single-application mode",work multiple seeing ray longer mode,issue,negative,neutral,neutral,neutral,neutral,neutral
1874400738,"Oh I see, I just read [the comment](https://github.com/ray-project/ray/issues/38257#issuecomment-1873931382) and didn't look carefully, sorry for the inconvenience :)",oh see read comment look carefully sorry inconvenience,issue,negative,negative,negative,negative,negative,negative
1874397388,what does the deployment details page look like?,deployment page look like,issue,negative,neutral,neutral,neutral,neutral,neutral
1874396037,Hey @anmyachev - I didn't. The PR was closed rather than reverted. We thought it was root cause for some regression but found out not related!,hey closed rather thought root cause regression found related,issue,negative,negative,neutral,neutral,negative,negative
1874390333,Hi @rickyyx! Looks like you reverted my fix: #41774. Could you give some context? Maybe I can improve it.,hi like fix could give context maybe improve,issue,positive,neutral,neutral,neutral,neutral,neutral
1874354508,@edoakes would be nice if you can give this a look as well,would nice give look well,issue,positive,positive,positive,positive,positive,positive
1874272337,"Yes, it looks like this is probably an issue with the dataloaders competing for resources. Do you know if the data loading is using multithreading under the hood? If so, there can be contention between the two dataloaders, and you will need to either manually limit the dataloaders to different cores, or share the same dataloader between Ray tasks/actors.",yes like probably issue know data loading hood contention two need either manually limit different share ray,issue,positive,neutral,neutral,neutral,neutral,neutral
1874259453,"Hi @mattip, you could take a look at the CI builds from master. Here's a link to the [build](https://buildkite.com/ray-project/oss-ci-build-branch/builds/7393#018ccad9-bcf8-4903-bb04-9bd3f3d92e02) from the latest commit.",hi could take look master link build latest commit,issue,negative,positive,positive,positive,positive,positive
1873574560,"In addition, when I use the following code, I can obtain cuda information normally:
```
import torch
import ray
import time

ray.init()

@ray.remote(
    num_gpus=1,
    runtime_env={
        ""nsight"": {
            ""-t"": ""cuda,cudnn,cublas,nvtx"",
            ""stop-on-exit"": ""true"",
        }
    },
)
def run_indenpent():
    a = torch.randint(0, 2, [128, 2, 2048, 2048]).cuda()
    b = torch.randint(0, 2, [128, 2, 2048, 2048]).cuda()
    for _ in range(5):
        c = a * b
        time.sleep(0.01)
    print(""Result on GPU:"", c)

ray.get(run_indenpent.remote())
```

Why I can't get any information in class use case?",addition use following code obtain information normally import torch import ray import time true range print result ca get information class use case,issue,negative,positive,positive,positive,positive,positive
1873307113,"To produce Ray datasets to Kafka in a efficient
 way, you can use https://github.com/ujjawal-khare-27/ray-kafka-producer",produce ray efficient way use,issue,negative,neutral,neutral,neutral,neutral,neutral
1872558876,"Hey @sven1977, using Ray has become quite challenging. The frequent deprecations and changes in its structure are causing a lot of confusion and frustration. ",hey ray become quite frequent structure causing lot confusion frustration,issue,negative,positive,neutral,neutral,positive,positive
1872383061,"> @rickyyx My understanding of the role of RAY_task_events_report_interval_ms is to turn off the current worker's task event reporter. This may not be accurate, for example:
> 
> The Driver calls MyActor.ping.remote(), and sets RAY_task_events_report_interval_ms=0 for MyActor, but the reporting of MyActor.ping task events is on the Driver side, so it can't take effect? https://github.com/ray-project/ray/blob/master/src/ray/core_worker/task_manager.cc#L1440
> 
> If we set RAY_task_events_report_interval_ms=0 for the Driver, then the task events of other actors will also be ignored.So I think we need more granular control options, such as specifying no event reporting for specific tasks, including task status events and task profile events.

I see - yeah, good catch. You are right here. The workaround will not disable all task events from the driver side. ",understanding role turn current worker task event reporter may accurate example driver task driver side ca take effect set driver task also think need granular control event specific task status task profile see yeah good catch right disable task driver side,issue,positive,positive,positive,positive,positive,positive
1872369432,"> Sending `None` message doesn't make sense to me. The serve [code](https://github.com/ray-project/ray/blob/master/python/ray/serve/_private/proxy.py#L1040) logic issue is that websockets server didn't send any message when it exists, so `proxy_asgi_receive_task` is not done, which makes the `status` is None. (this is a bug to me). But following two scenarios, one is causing the issue, the other one is not. I need to more investigation before i can make a fix confidentially.
> 
> this is working script: (client send message to the server only)
> 
> ```
> from fastapi import FastAPI, WebSocket, WebSocketDisconnect
> 
> from ray import serve
> import time
> from websockets.sync.client import connect
> 
> 
> app = FastAPI()
> 
> @serve.deployment
> @serve.ingress(app)
> class WebSocketServer:
>     @app.websocket(""/"")
>     async def ws_handler(self, ws: WebSocket):
>         await ws.accept()
> 
>         try:
>             msg = await ws.receive_text()
>         except Exception:
>             pass
> 
> 
> 
> serve.run(WebSocketServer.bind())
> 
> msg = ""Hello from client""
> 
> url = ""ws://localhost:8000/""
> with connect(url) as websocket:
> 
>     websocket.send(msg)
> ```
> 
> this is having issue script (Server sends message to the client only).
> 
> ```
> from fastapi import FastAPI, WebSocket, WebSocketDisconnect
> 
> from ray import serve
> import time
> from websockets.sync.client import connect
> 
> 
> app = FastAPI()
> 
> @serve.deployment
> @serve.ingress(app)
> class WebSocketServer:
>     @app.websocket(""/"")
>     async def ws_handler(self, ws: WebSocket):
>         await ws.accept()
> 
>         try:
>             await ws.send_text(""hello from server"")
>         except Exception:
>             pass
> 
> serve.run(WebSocketServer.bind())
> 
> msg = ""Hello from client""
> 
> url = ""ws://localhost:8000/""
> with connect(url) as websocket:
>     msg = websocket.recv()
> 
>     pass
> ```

Ignore the scenario described above, they both can be abstracted that serve code doesn't handle the scenario that websocket server is not sending disconnect message.",sending none message make sense serve code logic issue server send message done status none bug following two one causing issue one need investigation make fix confidentially working script client send message server import ray import serve import time import connect class self await try await except exception pas hello client connect issue script server message client import ray import serve import time import connect class self await try await hello server except exception pas hello client connect pas ignore scenario abstracted serve code handle scenario server sending disconnect message,issue,negative,neutral,neutral,neutral,neutral,neutral
1872315073,I couldn't repro the failures and the Intel tests were failing for me as well. Please retest with the latest modifications I made.,could failing well please retest latest made,issue,negative,positive,positive,positive,positive,positive
1871887671,"I guess I can replicate using this script using `ray==2.7.1`:

```python
from tqdm import trange
from ray.rllib.algorithms import Algorithm
from ray.rllib.algorithms.ppo import PPOConfig

env_name = ""LunarLanderContinuous-v2""
config = (
    PPOConfig()
    .environment(env_name, env_config={})
    .framework(""torch"")
    .debugging(seed=0)
    .rl_module(
        _enable_rl_module_api=True,
    )
    #.experimental(_enable_new_api_stack=True)#
    .training(
        model={
            ""_disable_preprocessor_api"": False,
            ""unused_additional_model_key"": True,            
        },
        _enable_learner_api=True,
    )
)

algo = config.build()

iterator = trange(2)
for epoch in iterator:
    result = algo.train()
    iterator.set_postfix(
        {
            ""reward_max"": result[""episode_reward_max""],
            ""reward_mean"": result[""episode_reward_mean""],
        }
    )
```
Please verify @gresavage ",guess replicate script python import import algorithm import torch false true epoch result result result please verify,issue,positive,negative,neutral,neutral,negative,negative
1871830206,"@stephanie-wang @jiwq  Hello, is there anything else I need to add about this pull-request?",hello anything else need add,issue,negative,neutral,neutral,neutral,neutral,neutral
1871819711,"Sending `None` message doesn't make sense to me.  The serve [code](https://github.com/ray-project/ray/blob/master/python/ray/serve/_private/proxy.py#L1040) logic issue is that websockets server didn't send any message when it exists, so `proxy_asgi_receive_task` is not done, which makes the `status` is None. (this is a bug to me). But following two scenarios, one is causing the issue, the other one is not. I need to more investigation before i can make a fix confidentially. 

this is working script: (client send message to the server only)
```
from fastapi import FastAPI, WebSocket, WebSocketDisconnect

from ray import serve
import time
from websockets.sync.client import connect


app = FastAPI()

@serve.deployment
@serve.ingress(app)
class WebSocketServer:
    @app.websocket(""/"")
    async def ws_handler(self, ws: WebSocket):
        await ws.accept()

        try:
            msg = await ws.receive_text()
        except Exception:
            pass



serve.run(WebSocketServer.bind())

msg = ""Hello from client""

url = ""ws://localhost:8000/""
with connect(url) as websocket:

    websocket.send(msg)
```

this is having issue script (Server sends message to the client only).
```
from fastapi import FastAPI, WebSocket, WebSocketDisconnect

from ray import serve
import time
from websockets.sync.client import connect


app = FastAPI()

@serve.deployment
@serve.ingress(app)
class WebSocketServer:
    @app.websocket(""/"")
    async def ws_handler(self, ws: WebSocket):
        await ws.accept()

        try:
            await ws.send_text(""hello from server"")
        except Exception:
            pass

serve.run(WebSocketServer.bind())

msg = ""Hello from client""

url = ""ws://localhost:8000/""
with connect(url) as websocket:
    msg = websocket.recv()

    pass
```

",sending none message make sense serve code logic issue server send message done status none bug following two one causing issue one need investigation make fix confidentially working script client send message server import ray import serve import time import connect class self await try await except exception pas hello client connect issue script server message client import ray import serve import time import connect class self await try await hello server except exception pas hello client connect pas,issue,negative,neutral,neutral,neutral,neutral,neutral
1871682277,"@rickyyx My understanding of the role of RAY_task_events_report_interval_ms is to turn off the current worker's task event reporter. This may not be accurate, for example:

The Driver calls MyActor.ping.remote(), and sets RAY_task_events_report_interval_ms=0 for MyActor, but the reporting of MyActor.ping task events is on the Driver side, so it can't take effect? https://github.com/ray-project/ray/blob/master/src/ray/core_worker/task_manager.cc#L1440

If we set RAY_task_events_report_interval_ms=0 for the Driver, then the task events of other actors will also be ignored.So I think we need more granular control options, such as specifying no event reporting for specific tasks, including task status events and task profile events.",understanding role turn current worker task event reporter may accurate example driver task driver side ca take effect set driver task also think need granular control event specific task status task profile,issue,negative,positive,positive,positive,positive,positive
1871649904,"Test failures: https://buildkite.com/ray-project/premerge/builds/15490#018cad56-2df4-4e4e-a7e2-ae5d9e70ac83

```
=================================== FAILURES ===================================
--
  | ___________________________ test_visible_amd_gpu_ids ___________________________
  |  
  | monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f3976c5f190>
  | shutdown_only = None
  |  
  | def test_visible_amd_gpu_ids(monkeypatch, shutdown_only):
  | with patch.object(Accelerator, ""get_current_node_num_accelerators"", return_value=4):
  | monkeypatch.setenv(""ROCR_VISIBLE_DEVICES"", ""0,1,2"")
  | ray.init()
  | manager = ray._private.accelerators.get_accelerator_manager_for_resource(""GPU"")
  | >           assert manager.get_current_node_num_accelerators() == 4
  | E           assert 0 == 4
  | E             +0
  | E             -4
  |  
  | python/ray/tests/accelerators/test_amd_gpu.py:15: AssertionError
  | __________________________ test_visible_amd_gpu_type ___________________________
  |  
  | shutdown_only = None
  |  
  | def test_visible_amd_gpu_type(shutdown_only):
  | with patch.object(
  | Accelerator,
  | ""_get_amd_pci_ids"",
  | return_value={
  | ""card0"": {""GPU ID"": ""0x740f""},
  | ""card1"": {""GPU ID"": ""0x740f""},
  | ""card2"": {""GPU ID"": ""0x740f""},
  | ""card3"": {""GPU ID"": ""0x740f""},
  | },
  | ):
  | ray.init()
  | manager = ray._private.accelerators.get_accelerator_manager_for_resource(""GPU"")
  | >           assert manager.get_current_node_accelerator_type() == ""AMD-Instinct-MI210""
  | E           AssertionError: assert None == 'AMD-Instinct-MI210'
  | E             +None
  | E             -'AMD-Instinct-MI210'
```

Check `test_intel_gpu.py`, I think you need 

```
# Delete the cache so it can be re-populated the next time
        # we call get_accelerator_manager_for_resource
        del get_accelerator_manager_for_resource._resource_name_to_accelerator_manager
```",test object none accelerator manager assert assert none accelerator card id card id card id card id manager assert assert none check think need delete cache next time call,issue,negative,neutral,neutral,neutral,neutral,neutral
1871406648,"This is a great idea! I think allowing users to ignore task events report for some tasks could be a good mitigation for running into large number of tasks. 

I think one current workaround (maybe not so clean at the API level) is through setting runtime env on the task with `RAY_task_events_report_interval_ms=0`

This is kind of similar to https://github.com/ray-project/ray/issues/42076

cc @rkooo567 ",great idea think ignore task report could good mitigation running large number think one current maybe clean level setting task kind similar,issue,positive,positive,positive,positive,positive,positive
1871368601,"@gresavage Thanks for filing this. This is indeed tricky. 

Afaics the reparameterization trick is only needed for stochastic nodes in the computation graph - so where action sampling happens. At these nodes the `RLlib` implementation uses [`rsample()`](https://pytorch.org/docs/stable/_modules/torch/distributions/normal.html#Normal.rsample) from the `torch.distributions.normal.Normal` and this function already does the reparameterization trick for us. So we get gradients when we backpropagate through this node. 

The difference might come from the difference in the `logp` as this is not the same for `x` and `z=(x-mu)/sigma`, but to be precise I have to go through all steps in the optimization and see where it pops up.",thanks filing indeed tricky trick stochastic computation graph action sampling implementation function already trick u get node difference might come difference precise go optimization see,issue,positive,positive,positive,positive,positive,positive
1871238907,"@RobinKa Thanks for posting this. I can replicate the error of @jfurches on `ray==2.7.1`. However, I cannot replicate on `ray-nightly`. WIth the nightly install I can run the example for long times without any error, even though the threshold rate has been exceeded for a long time. I guess this error has been already fixed. 

Could you try the last version or the nightly one? ",thanks posting replicate error however replicate nightly install run example long time without error even though threshold rate long time guess error already fixed could try last version nightly one,issue,negative,positive,neutral,neutral,positive,positive
1871228944,"@fulacse @WindyHu001 could you provide a reproducable example in small form? I could take a look into it and see what I can do. As an alternative you could try using the master and see, if the `nan`  rewards are still occurring.",could provide example small form could take look see alternative could try master see nan still,issue,negative,negative,negative,negative,negative,negative
1871223297,"@rk0n Thanks for posting this. Which Ray version did you use? And could you provide a reproducable example. I do not have such problems using other custom gymnasium environments.

WHat kind of error do you experience? Could you post your error output here, please?",thanks posting ray version use could provide example custom gymnasium kind error experience could post error output please,issue,negative,positive,positive,positive,positive,positive
1871170320,"Yes, I think the RAY_SERVE_EVENT_LOOP_IMPL would be a good solution. 
From RAY_SERVE_DEBUG_MODE=1 I would think of stuff like fine granular log level, maybe time measurements and other things which are necessary for debugging, that's why it feels not right to use this.",yes think would good solution would think stuff like fine granular log level maybe time necessary right use,issue,positive,positive,positive,positive,positive,positive
1871057657,"We haven't fixed the issue yet -- for us, it's rare enough to sit in the backlog for a while (, plus most of our jobs are wrapped in global retries, i.e. we'll typically automatically create a distinct set of Ray pods in case of startup failure.)

When we get around to fixing it, we'll aim to specify all of the ports (since we're not totally sure if retrying would have side-effects.)





",fixed issue yet u rare enough sit backlog plus wrapped global typically automatically create distinct set ray case failure get around fixing aim specify since totally sure would,issue,negative,positive,neutral,neutral,positive,positive
1871050394,@ahmedammar thanks for posting this. I have created a PR that should fix this issue. The reason was that our `AlgorithmConfig` is not a dicitonary and therefore its method `to_dict()` has to be called in the `JsonLogger`.,thanks posting fix issue reason therefore method,issue,negative,positive,positive,positive,positive,positive
1870990634,"Running another instance with version `2.6.3`. The bug is a little different from `2.8.1`. Here are related logs:

```shell
UID        PID  PPID  C STIME TTY          TIME CMD
ray          1     0  0 Dec27 ?        00:00:20 /bin/bash /home/ray/startup.sh worker
ray         37     1  1 Dec27 ?        00:20:10 .../ray/raylet/raylet...
ray         38     1  0 Dec27 ?        00:12:00 ...log_monitor.py...
ray         78    37  1 Dec27 ?        00:19:19 ...dashboard/agent.py
ray        118     1  0 Dec27 ?        00:00:05 tail -f /tmp/ray/session_latest/logs/raylet.out
ray        328     0  0 Dec27 pts/0    00:00:00 /bin/sh
ray        336   328  0 Dec27 pts/0    00:00:00 bash
ray      33142     1  0 13:07 ?        00:00:00 sleep 5
ray      33143   336  0 13:07 pts/0    00:00:00 ps -ef
```

```log
2023-12-28 13:08:13,049 ERROR reporter_agent.py:1112 -- Error publishing node physical stats.
Traceback (most recent call last): 
  File ""/home/ray/anaconda3/lib/python3.10/site-packages/ray/dashboard/modules/reporter/reporter_agent.py"", line 1095, in _perform_iteration
    stats = self._get_all_stats()
  File ""/home/ray/anaconda3/lib/python3.10/site-packages/ray/dashboard/modules/reporter/reporter_agent.py"", line 626, in _get_all_stats
    ""workers"": self._get_workers(),
  File ""/home/ray/anaconda3/lib/python3.10/site-packages/ray/dashboard/modules/reporter/reporter_agent.py"", line 490, in _get_workers
    self._generate_worker_key(proc): proc for proc in raylet_proc.children()
  File ""/home/ray/anaconda3/lib/python3.10/site-packages/ray/thirdparty_files/psutil/__init__.py"", line 277, in wrapper
    raise NoSuchProcess(self.pid, self._name, msg=msg)
psutil.NoSuchProcess: process no longer exists and its PID has been reused (pid=37)
```
The `raylet` process is still there (`PID=37`), but `dashboard_agent` detected the `process no longer exists` and `its PID has been reused (pid=37)`. 

Another difference is that the two process does not exit, and keep producing logs like the above.

---

The compose file is slightly modified to read logs from the startup script.

```yaml
services:
  head: # <service name>
    image: ray:${RAY_VERSION:-2.6.3-py310-cpu}
    container_name: ray-head
    hostname: ray-head
    networks:
      - net
    ports:
      - 8265:8265
      - 10001:10001
    command: [""head""]

    environment:
      - RAY_DISABLE_DOCKER_CPU_WARNING=1
    shm_size: 2g
    deploy:
      resources:
        limits:
          cpus: ""2""
          memory: 4096m
        reservations:
          cpus: ""1""
          memory: 1024m

  worker: # <service name>
    image: ray:${RAY_VERSION:-2.6.3-py310-cpu}
    shm_size: 2g
    depends_on:
      - head
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: ""2""
          memory: 2048m
        reservations:
          cpus: ""1""
          memory: 1024m
    networks:
      - net
    command: [""worker""]
    environment:
      - RAY_DISABLE_DOCKER_CPU_WARNING=1

networks:
  net:
    ipam:
      driver: default
      config:
        - subnet: ""172.29.1.0/24""

```

and the docker file:

```dockerfile
ARG RAY_VERSION=2.6.3-py310-cpu
FROM rayproject/ray:${RAY_VERSION}

USER root
ENV TZ='Asia/Shanghai'
RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime && \
    echo ""$TZ"" > /etc/timezone
RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \
    --mount=type=cache,target=/var/lib/apt,sharing=locked \
    apt-get update && apt-get install --yes ca-certificates 

RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \
    --mount=type=cache,target=/var/lib/apt,sharing=locked \
    apt-get update && \
    apt-get install --no-install-recommends --yes \
    gcc libc-dev openjdk-11-jre
ENV JAVA_HOME /usr/lib/jvm/java-11-openjdk-amd64

ADD ./startup.sh /home/ray/
ENTRYPOINT [""/home/ray/startup.sh""]

USER ray
RUN --mount=type=cache,target=/home/ray/.cache/pip,sharing=locked,uid=1000,gid=100 \
    pip install --index-url https://pypi.doubanio.com/simple/ \
        'raydp==1.6.0' \
        'dask>=2022.10.1' \
        'modin[ray]'
```

with customized startup script `startup.sh`

```shell
#!/bin/bash

service=${1:?'Please specify the service to start!'}
shift
monitor_file='raylet.out'

case $service in
head)
    ray start \
        --head \
        --port=6379 \
        --dashboard-host=0.0.0.0 \
        --disable-usage-stats \
        ""${@}""
    ;;
worker)
    ray start \
        --address=ray-head:6379 \
        --disable-usage-stats \
        ""${@}""
    ;;
*)
    echo ""error: unknown service <${service}>to start!""
    exit 1
    ;;
esac
tail -f ""/tmp/ray/session_latest/logs/${monitor_file}"" < /dev/null &

while true; do
    if ps -C raylet -f > /dev/null; then
        sleep 5
    else
        echo ""warning: raylet process has exited, the monitor process now exit.""
        exit 0
    fi
done
```",running another instance version bug little different related shell stime time ray worker ray ray ray ray tail ray ray bash ray sleep ray log error error node physical recent call last file line file line file line file line wrapper raise process longer raylet process still process longer another difference two process exit keep like compose file slightly read script head service name image ray net command head environment deploy memory memory worker service name image ray head deploy memory memory net command worker environment net driver default docker file user root run echo run update install yes run update install yes add user ray run pip install ray script shell specify service start shift case service head ray start head worker ray start echo error unknown service service start exit tail true raylet sleep else echo warning raylet process monitor process exit exit fi done,issue,negative,negative,neutral,neutral,negative,negative
1870981415,"Output from running the script provided in #41290 (the part after reloading) using the modifications in this PR:

```2023-12-27 19:19:54,789 INFO trainable.py:188 -- Trainable.setup took 21.533 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2023-12-27 19:19:54,790 WARNING util.py:68 -- Install gputil for GPU system monitoring.
epoch=0 reward_max=357.000000 reward_mean=252.666667
epoch=1 reward_max=357.000000 reward_mean=357.000000
epoch=2 reward_max=335.000000 reward_mean=335.000000
epoch=3 reward_max=335.000000 reward_mean=335.000000
epoch=4 reward_max=335.000000 reward_mean=335.000000
epoch=5 reward_max=335.000000 reward_mean=335.000000
epoch=6 reward_max=335.000000 reward_mean=335.000000
epoch=7 reward_max=335.000000 reward_mean=335.000000
epoch=8 reward_max=335.000000 reward_mean=335.000000
epoch=9 reward_max=500.000000 reward_mean=500.000000
```",output running script provided part took trainable slow initialize consider setting reduce actor creation warning install system,issue,negative,negative,negative,negative,negative,negative
1870798098,"@justinvyu I have updated the PR, It would be great if we can include these changes in the next release, thanks. ",would great include next release thanks,issue,positive,positive,positive,positive,positive,positive
1870649994,"I've tested the code in a plain `test.py` file, running on Ubuntu 22.04 / Python 3.11.7 and it crashes exactly the same way. This has nothing to do with Jupyter.",tested code plain file running python exactly way nothing,issue,negative,positive,neutral,neutral,positive,positive
1870609335,"I'm running into the same issue, where `dashboard_agent_http` is trying to use the same port as `dashboard_agent_grpc`.

```
ValueError: Ray component dashboard_agent_http is trying to use a port number 52365 that is used by other components.

Port information: {'gcs': 'random', 'object_manager': 'random', 'node_manager': 'random', 'gcs_server': 9339, 'client_server': 'random', 'dashboard': 8265, 'dashboard_agent_grpc': 52365, 'dashboard_agent_http': 52365, 'dashboard_grpc': 'random', 'runtime_env_agent': 48745, 'metrics_export': 45339, 'redis_shards': 'random', 'worker_ports': '9998 ports from 10002 to 19999'}
```

@DmitriGekhtman for your workaround, did you go with the ""retry approach"" (i.e. re-run `ray start --head` if you encounter the above `ValueError`) or were you able to manually configure the ports used by `dashboard_agent_http` and `dashboard_agent_grpc`?",running issue trying use port ray component trying use port number used port information go retry approach ray start head encounter able manually configure used,issue,negative,positive,positive,positive,positive,positive
1870588379,"@matthewdeng @pcmoritz I second @anhnami 's comment. My team uses Nevergrad extensively, and removal of it from ray tune is a major loss for us.",second comment team extensively removal ray tune major loss u,issue,negative,positive,neutral,neutral,positive,positive
1869804361,I added a comment on the REP doc. Overall it looks good! It would be a nice improvement to be able to specify that a task needs multiple GPUs with a certain amount of memory on each.,added comment rep doc overall good would nice improvement able specify task need multiple certain amount memory,issue,positive,positive,positive,positive,positive,positive
1869795731,We implemented async Serve calls using the existing Workflows API. The Workflow provides a reference value that can be shared with the caller and used to look up the state of the request. It seems to be working well. The only missing piece is rate limiting. I'm curious why the previous proposal to build on top of Workflows was abandoned?,serve reference value caller used look state request working well missing piece rate limiting curious previous proposal build top abandoned,issue,negative,positive,neutral,neutral,positive,positive
1869705374,"Thank you for the investigation! The `checkpoint_at_end` and `checkpoint_frequency` do indeed go through different codepaths, and I was able to reproduce with `checkpoint_frequency=1`. I'll put up a fix PR to clean this up!",thank investigation indeed go different able reproduce put fix clean,issue,positive,positive,positive,positive,positive,positive
1869700863,Is there a build log from a CI run I can look at to see how it compares?,build log run look see,issue,negative,neutral,neutral,neutral,neutral,neutral
1869677349,I also ran the test added in #29993 locally and it passed.,also ran test added locally,issue,negative,neutral,neutral,neutral,neutral,neutral
1869644316,"As @peytondmurray said, I'm doing some more design work this week on the example gallery. Got some feedback from @matthewdeng so trying to include that here. ",said design work week example gallery got feedback trying include,issue,negative,neutral,neutral,neutral,neutral,neutral
1869610696,step 1: release the pin. Let's see what tests fail?,step release pin let see fail,issue,negative,negative,negative,negative,negative,negative
1869440629,"> @chris-aeviator @gpucce does 2.7.1 work for you? If so I think we can close this.

hi, sorry for the late reply, it looks like I can use it now",work think close hi sorry late reply like use,issue,negative,negative,negative,negative,negative,negative
1869424959,"Running a mini-cluster with Ray 2.8.1, this problem still occurs.

Here is the recent logs of `dashboard_agent`:

```log
2023-12-24 16:56:38,169 INFO process_watcher.py:44 -- raylet pid is 41
2023-12-24 16:56:38,175 INFO event_agent.py:56 -- Report events to 172.29.1.2:33455
2023-12-24 16:56:38,176 INFO event_utils.py:132 -- Monitor events logs modified after 1703463998.0062358 on /tmp/ray/session_2023-12-24_16-56-33_688310_1/logs/events, the source types are all.
2023-12-25 20:39:55,379 ERROR process_watcher.py:158 -- Failed to check parent PID, exiting.
Traceback (most recent call last):
  File ""/home/ray/anaconda3/lib/python3.10/site-packages/ray/_private/process_watcher.py"", line 73, in _check_parent
    parent = curr_proc.parent()
  File ""/home/ray/anaconda3/lib/python3.10/site-packages/ray/thirdparty_files/psutil/__init__.py"", line 564, in parent
    ppid = self.ppid()
  File ""/home/ray/anaconda3/lib/python3.10/site-packages/ray/thirdparty_files/psutil/_common.py"", line 486, in wrapper
    raise raise_from(err, None)
  File ""<string>"", line 3, in raise_from
  File ""/home/ray/anaconda3/lib/python3.10/site-packages/ray/thirdparty_files/psutil/_common.py"", line 484, in wrapper
    return fun(self)
  File ""/home/ray/anaconda3/lib/python3.10/site-packages/ray/thirdparty_files/psutil/__init__.py"", line 623, in ppid
    self._raise_if_pid_reused()
  File ""/home/ray/anaconda3/lib/python3.10/site-packages/ray/thirdparty_files/psutil/__init__.py"", line 436, in _raise_if_pid_reused
    raise NoSuchProcess(self.pid, self._name, msg=msg)
psutil.NoSuchProcess: process no longer exists and its PID has been reused (pid=83)
```
in which, the `process_watcher`, one of the thread of `dashboard_agent` process complains about its parent (i.e., `raylet`) no longer exists. We can see that the initial PID of `raylet` is 41, but the error message show the PID is 83, and hence the `psutil` may have some bug for recording its parent process.

Almost the same time, the `raylet` also detected that the `dashboard_agent` has gone, showing in the following logs:

```log
[2023-12-25 20:39:55,615 I 41 84] (raylet) agent_manager.cc:66: Agent process with name dashboard_agent/1059961393 exited, exit code 0.
[2023-12-25 20:39:55,615 E 41 84] (raylet) agent_manager.cc:70: The raylet exited immediately because one Ray agent failed, agent_name = dashboard_agent/1059961393.
[2023-12-25 20:39:55,615 I 41 84] (raylet) raylet_util.h:28: Sending SIGTERM to gracefully shutdown raylet
[2023-12-25 20:39:55,615 I 41 41] (raylet) main.cc:368: Raylet received SIGTERM, shutting down...
[2023-12-25 20:39:55,615 I 41 41] (raylet) accessor.cc:451: Unregistering node info, node id = 0cbd570687c28f137a1c5acb08026740602efeb0bc95ed5a9d2abbb3
[2023-12-25 20:39:55,616 I 41 41] (raylet) agent_manager.cc:95: Killing agent dashboard_agent/1059961393, pid 83.
[2023-12-25 20:39:55,616 I 41 41] (raylet) agent_manager.cc:95: Killing agent runtime_env_agent, pid 85.
[2023-12-25 20:39:55,620 I 41 86] (raylet) agent_manager.cc:66: Agent process with name runtime_env_agent exited, exit code 0.
```

**The dashboard_agent.log says that ""PID has been reused"", but my cluster is running in docker, and has no workload, so there should be no new process being created that reuses the PID of the died `raylet`. Otherwise, the `raylet` might not be truly died, but could the detection mechanism of `process_watcher` have some flaws?**

---

Besides, I run the cluster with `docker compose` with the following compose file.

```yml
services:
  head:  # <service name>
    # - official image: rayproject/ray:2.8.1-py310-cpu
    #   tags: 2.8.1, 2.8.1-py310-cpu, 2.8.1-py310-gpu
    # - custom image: ray:2.8.1-py310-cpu
    image: ray:2.8.1-py310-cpu
    container_name: ray-head
    hostname: ray-head
    networks:
      - net
    ports:
      - 8265:8265
      - 10001:10001
    entrypoint: [""ray"", ""start""]
    command: 
      - --head
      - --port=6379
      - --dashboard-host=0.0.0.0
      - --block
      - --disable-usage-stats

    environment:
      # Detecting docker specified CPUs. In previous versions of Ray, CPU detection 
      # in containers was incorrect. Please ensure that Ray has enough CPUs allocated. 
      # As a temporary workaround to revert to the prior behavior, 
      # set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. 
      # Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.
      - RAY_DISABLE_DOCKER_CPU_WARNING=1
      # - RAY_USE_MULTIPROCESSING_CPU_COUNT=1
    shm_size: 2g
    deploy:
      resources:
        limits:
          cpus: ""2""
          memory: 4096m
        reservations:
          cpus: ""1""
          memory: 1024m
    # run_options:   # Extra options to pass into ""docker run""
    #     - --ulimit nofile=65536:65536

  worker:  # <service name>
    image: ray:2.8.1-py310-cpu
    shm_size: 2g
    depends_on:
      - head
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: ""2""
          memory: 2048m
        reservations:
          cpus: ""1""
          memory: 1024m
    networks:
      - net
    entrypoint: [""ray"", ""start""]
    command: 
      - --address=ray-head:6379
      - --block
      - --disable-usage-stats

    environment:
      - RAY_DISABLE_DOCKER_CPU_WARNING=1
  
networks:
  net:
    ipam:
      driver: default
      config:
        - subnet: ""172.29.1.0/24""
```

With customized image build with the following docker file:

```Dockerfile
FROM rayproject/ray:2.8.1-py310-cpu

USER root
ENV TZ='Asia/Shanghai'
RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime && \
    echo ""$TZ"" > /etc/timezone
RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \
    --mount=type=cache,target=/var/lib/apt,sharing=locked \
    apt-get update && apt-get install --yes ca-certificates 

# base image is based on utuntu:focal
RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \
    --mount=type=cache,target=/var/lib/apt,sharing=locked \
    apt-get update && \
    apt-get install --no-install-recommends --yes \
    gcc libc-dev openjdk-11-jre
ENV JAVA_HOME /usr/lib/jvm/java-11-openjdk-amd64

USER ray
RUN --mount=type=cache,target=/home/ray/.cache/pip,sharing=locked \
    pip install --index-url https://pypi.doubanio.com/simple/ \
        'raydp==1.6.0' \
        'dask>=2022.10.1' \
        'modin[ray]'
```",running ray problem still recent log raylet report monitor source error check parent recent call last file line parent file line parent file line wrapper raise err none file string line file line wrapper return fun self file line file line raise process longer one thread process parent raylet longer see initial raylet error message show hence may bug recording parent process almost time raylet also gone showing following log raylet agent process name exit code raylet raylet immediately one ray agent raylet sending gracefully shutdown raylet raylet raylet received shutting raylet node node id raylet killing agent raylet killing agent raylet agent process name exit code cluster running docker new process raylet otherwise raylet might truly could detection mechanism besides run cluster docker compose following compose file head service name official image custom image ray image ray net ray start command head block environment docker previous ray detection incorrect please ensure ray enough temporary revert prior behavior set starting ray set mute warning deploy memory memory extra pas docker run worker service name image ray head deploy memory memory net ray start command block environment net driver default image build following docker file user root run echo run update install yes base image based focal run update install yes user ray run pip install ray,issue,negative,negative,neutral,neutral,negative,negative
1869171868,@jonathan-anyscale Thanks! I was able to get rid of the error by upgrading grpcio to 1.59.3.,thanks able get rid error,issue,negative,positive,positive,positive,positive,positive
1869167725,"I am using some 3D meshing code which also requires the use of a signal similar to @mitar's example. Unfortunately this code is not really trivial to rewrite as `gmsh` is quite an involved application

It would be great to have a way to handle these situations",code also use signal similar example unfortunately code really trivial rewrite quite involved application would great way handle,issue,negative,positive,positive,positive,positive,positive
1869069492,After a discussion with @simran-2797 this effort is on hold until further design work on the example gallery is complete.,discussion effort hold design work example gallery complete,issue,negative,positive,neutral,neutral,positive,positive
1869054338,"@simonsays1980 , next topic, that helps is new functionality in v. 2.9.0: https://docs.ray.io/en/latest/rllib/rllib-learner.html
It provides new config attribute:
config = config.experimental(_enable_new_api_stack=True)

Also, it provides new methods that save and load learner_group.state: 
algo.learner_group.save_state(""-"".join((""checkpoint_lg"", env_name)))
algo2.learner_group.load_state(""-"".join((""checkpoint_lg"", env_name)))

**This script:**
import numpy as np
import torch
from numpy.testing import assert_equal
from torch import use_deterministic_algorithms
from tqdm import trange
import random
from ray.rllib.algorithms import Algorithm
from ray.rllib.algorithms.ppo import PPOConfig

env_name = ""CartPole-v1""

config = PPOConfig()
config = config.experimental(_enable_new_api_stack=True)
config = config.training(gamma=0.9, lr=0.01, kl_coeff=0.3, train_batch_size=128)
config = config.resources(num_gpus=0)
config = config.rollouts(num_rollout_workers=1)

algo = config.build(env=env_name)

iterator = trange(500)
for epoch in iterator:
    result = algo.train()
    iterator.set_postfix({
        ""reward_max"": result[""episode_reward_max""],
        ""reward_mean"": result[""episode_reward_mean""],
    })

checkpoint = algo.save(""-"".join((""checkpoint_old"", env_name)))

learner_state_true = algo.learner_group.get_state()

local_worker_policy_weights_saved = algo.workers.local_worker().get_policy().get_weights()
remote_workers_policy_weights_saved = algo.workers.foreach_worker(lambda w: w.get_policy().get_weights())
local_worker_optimizer_state_saved = algo.workers.local_worker().get_policy().optimizer()[0].state_dict()
local_worker_kl_coeff_saved = algo.workers.local_worker().get_policy().kl_coeff

path_to_checkpoint = checkpoint.checkpoint.path
algo.learner_group.save_state(""-"".join((""checkpoint_lg"", env_name)))

print(""checkpoint"", checkpoint.checkpoint.path)

algo2 = Algorithm.from_checkpoint(""-"".join((""checkpoint_old"", env_name)))
algo2.learner_group.load_state(""-"".join((""checkpoint_lg"", env_name)))

learner_state_loaded = algo2.learner_group.get_state()

local_worker_policy_weights_loaded = algo2.workers.local_worker().get_policy().get_weights()
remote_workers_policy_weights_loaded = algo2.workers.foreach_worker(lambda w: w.get_policy().get_weights())
local_worker_optimizer_state_loaded = algo2.workers.local_worker().get_policy().optimizer()[0].state_dict()
local_worker_kl_coeff_loaded = algo2.workers.local_worker().get_policy().kl_coeff

assert_equal(local_worker_policy_weights_saved, local_worker_policy_weights_loaded)
assert_equal(remote_workers_policy_weights_saved, remote_workers_policy_weights_loaded)
assert_equal(local_worker_optimizer_state_saved, local_worker_optimizer_state_loaded)
assert_equal(local_worker_kl_coeff_saved, local_worker_kl_coeff_loaded)

tolerance = 1e-8

for key in learner_state_true['module_state']['default_policy']:
    if key in learner_state_loaded['module_state']['default_policy']:
        if np.allclose(learner_state_true['module_state']['default_policy'][key], learner_state_loaded['module_state']['default_policy'][key], atol=tolerance):
            print(f""learner_state 'module_state' for key '{key}' are identical."")
        else:
            print(f""learner_state 'module_state' for key '{key}' are not identical."")

for key in learner_state_true['optimizer_state']['default_policy_default_optimizer']['state']:
    for tens in learner_state_true['optimizer_state']['default_policy_default_optimizer']['state'][key]:
        if tens in learner_state_loaded['optimizer_state']['default_policy_default_optimizer']['state'][key]:
            if torch.equal(learner_state_true['optimizer_state']['default_policy_default_optimizer']['state'][key][tens], 
                        learner_state_loaded['optimizer_state']['default_policy_default_optimizer']['state'][key][tens]):
                print(f""learner_state for key optimizer_state default_policy_default_optimizer state'{key} {tens}' are identical."")
            else:
                print(f""learner_state for key optimizer_state default_policy_default_optimizer state'{key} {tens}' are not identical."")

for epoch in range(10):
    result = algo2.train()
    print(""epoch=%(epoch)d reward_max=%(reward_max)f reward_mean=%(reward_mean)f"" % {
        ""epoch"": epoch,
        ""reward_max"": result[""episode_reward_max""],
        ""reward_mean"": result[""episode_reward_mean""],
    })

**provides following results:**
learner_state 'module_state' for key 'encoder.actor_encoder.net.mlp.0.weight' are identical.
learner_state 'module_state' for key 'encoder.actor_encoder.net.mlp.0.bias' are identical.
learner_state 'module_state' for key 'encoder.actor_encoder.net.mlp.2.weight' are identical.
learner_state 'module_state' for key 'encoder.actor_encoder.net.mlp.2.bias' are identical.
learner_state 'module_state' for key 'encoder.critic_encoder.net.mlp.0.weight' are identical.
learner_state 'module_state' for key 'encoder.critic_encoder.net.mlp.0.bias' are identical.
learner_state 'module_state' for key 'encoder.critic_encoder.net.mlp.2.weight' are identical.
learner_state 'module_state' for key 'encoder.critic_encoder.net.mlp.2.bias' are identical.
learner_state 'module_state' for key 'pi.net.mlp.0.weight' are identical.
learner_state 'module_state' for key 'pi.net.mlp.0.bias' are identical.
learner_state 'module_state' for key 'vf.net.mlp.0.weight' are identical.
learner_state 'module_state' for key 'vf.net.mlp.0.bias' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'0 step' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'0 exp_avg' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'0 exp_avg_sq' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'1 step' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'1 exp_avg' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'1 exp_avg_sq' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'2 step' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'2 exp_avg' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'2 exp_avg_sq' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'3 step' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'3 exp_avg' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'3 exp_avg_sq' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'4 step' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'4 exp_avg' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'4 exp_avg_sq' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'5 step' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'5 exp_avg' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'5 exp_avg_sq' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'6 step' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'6 exp_avg' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'6 exp_avg_sq' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'7 step' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'7 exp_avg' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'7 exp_avg_sq' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'8 step' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'8 exp_avg' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'8 exp_avg_sq' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'9 step' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'9 exp_avg' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'9 exp_avg_sq' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'10 step' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'10 exp_avg' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'10 exp_avg_sq' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'11 step' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'11 exp_avg' are identical.
learner_state for key optimizer_state default_policy_default_optimizer state'11 exp_avg_sq' are identical.
epoch=0 reward_max=119.000000 reward_mean=119.000000
epoch=1 reward_max=119.000000 reward_mean=119.000000
epoch=2 reward_max=142.000000 reward_mean=126.000000
epoch=3 reward_max=142.000000 reward_mean=119.250000
epoch=4 reward_max=142.000000 reward_mean=122.200000
epoch=5 reward_max=142.000000 reward_mean=122.200000
epoch=6 reward_max=244.000000 reward_mean=142.500000
epoch=7 reward_max=244.000000 reward_mean=142.500000
epoch=8 reward_max=244.000000 reward_mean=150.571429
epoch=9 reward_max=244.000000 reward_mean=149.000000
",next topic new functionality new attribute also new save load script import import torch import torch import import import random import algorithm import epoch result result result lambda print lambda tolerance key key key key print key key identical else print key key identical key key key key key print key state key identical else print key state key identical epoch range result print epoch epoch epoch result result following key weight identical key bias identical key weight identical key bias identical key weight identical key bias identical key weight identical key bias identical key weight identical key bias identical key weight identical key bias identical key state step identical key state identical key state identical key state step identical key state identical key state identical key state step identical key state identical key state identical key state step identical key state identical key state identical key state step identical key state identical key state identical key state step identical key state identical key state identical key state step identical key state identical key state identical key state step identical key state identical key state identical key state step identical key state identical key state identical key state step identical key state identical key state identical key state step identical key state identical key state identical key state step identical key state identical key state identical,issue,negative,negative,neutral,neutral,negative,negative
1869049460,"@simonsays1980 , here is some feedback in several posts, because I'm not sure, what exactly this script should check. First of all,  as I can see, If you will change the code a bit and put get_state after the set_state the result will be different. 
**From:**
algo = Algorithm.from_checkpoint(""-"".join((""checkpoint"", env_name)))
learner_state_loaded = algo.learner_group.get_state()
algo.learner_group.set_state(learner_state_true)
**To:**
algo = Algorithm.from_checkpoint(""-"".join((""checkpoint"", env_name)))
algo.learner_group.set_state(learner_state_true)
learner_state_loaded = algo.learner_group.get_state()

But if you checked, that Algorithm.from_checkpoint restores learner_state correctly, it wasn't so in rllib 2.7.1. ",feedback several sure exactly script check first see change code bit put result different checked correctly,issue,negative,positive,positive,positive,positive,positive
1869008021,"@can-anyscale The fix appears to work, as can be seen from the successful AWS test. For GCP, I'm seemingly unable to acquire instances. Can we still move forward with this fix?",fix work seen successful test seemingly unable acquire still move forward fix,issue,positive,positive,positive,positive,positive,positive
1868902217,"Hello @simonsays1980 !

Thank you for your feedback. We will try your code soon and will return with the feedback. ",hello thank feedback try code soon return feedback,issue,negative,neutral,neutral,neutral,neutral,neutral
1868583613,"> the best way forward seems to just not include redis for windows build

I see `redis-client` is a dependency of `_raylet`, `gcs_pub_sub_lib`, `gcs_client_lib`, `gcs_table_storage_test_lib`, `gcs_table_storage_lib`, `chaos_redis_store_client_test`. Will any of that work without redis?",best way forward include build see dependency work without,issue,positive,positive,positive,positive,positive,positive
1868374928,I've also encountered the same issue. Have you find a solution?,also issue find solution,issue,negative,neutral,neutral,neutral,neutral,neutral
1868353856,"Just want to add, this does not allow me to load my old tune results. 

`[327](https://vscode-remote+ssh-002dremote-002be13k-002eeu-002eorg-002e47.vscode-resource.vscode-cdn.net/home/anhnht/csaiomodel/~/miniforge3/envs/dev/lib/python3.11/site-packages/ray/tune/impl/tuner_internal.py:327) fs, fs_path = get_fs_and_path(path_or_uri, storage_filesystem)
    [328](https://vscode-remote+ssh-002dremote-002be13k-002eeu-002eorg-002e47.vscode-resource.vscode-cdn.net/home/anhnht/csaiomodel/~/miniforge3/envs/dev/lib/python3.11/site-packages/ray/tune/impl/tuner_internal.py:328) with fs.open_input_file(os.path.join(fs_path, _TUNER_PKL)) as f:
--> [329](https://vscode-remote+ssh-002dremote-002be13k-002eeu-002eorg-002e47.vscode-resource.vscode-cdn.net/home/anhnht/csaiomodel/~/miniforge3/envs/dev/lib/python3.11/site-packages/ray/tune/impl/tuner_internal.py:329)     tuner_state = pickle.loads(f.readall())
    [331](https://vscode-remote+ssh-002dremote-002be13k-002eeu-002eorg-002e47.vscode-resource.vscode-cdn.net/home/anhnht/csaiomodel/~/miniforge3/envs/dev/lib/python3.11/site-packages/ray/tune/impl/tuner_internal.py:331) old_trainable_name, flattened_param_space_keys = self._load_tuner_state(
    [332](https://vscode-remote+ssh-002dremote-002be13k-002eeu-002eorg-002e47.vscode-resource.vscode-cdn.net/home/anhnht/csaiomodel/~/miniforge3/envs/dev/lib/python3.11/site-packages/ray/tune/impl/tuner_internal.py:332)     tuner_state
    [333](https://vscode-remote+ssh-002dremote-002be13k-002eeu-002eorg-002e47.vscode-resource.vscode-cdn.net/home/anhnht/csaiomodel/~/miniforge3/envs/dev/lib/python3.11/site-packages/ray/tune/impl/tuner_internal.py:333) )
    [335](https://vscode-remote+ssh-002dremote-002be13k-002eeu-002eorg-002e47.vscode-resource.vscode-cdn.net/home/anhnht/csaiomodel/~/miniforge3/envs/dev/lib/python3.11/site-packages/ray/tune/impl/tuner_internal.py:335) # Perform validation and set the re-specified `trainable` and `param_space`

ModuleNotFoundError: No module named 'ray.tune.search.nevergrad'`",want add allow load old tune perform validation set trainable module,issue,negative,positive,neutral,neutral,positive,positive
1868352990,Nevergrad's OnePlusOne and the recent LogNormalOnePlusOne  have been my favorite. They yielded better performance than Optuna for me. Kind of sad because it got removed for no valid reason.,recent favorite better performance kind sad got removed valid reason,issue,positive,positive,positive,positive,positive,positive
1868338189,"I think I got the beast. It appears that - at least in `Ray 2.7.1` `Algorithm.from_checkpoint()` does not update the learner worker's weights. If I store the weights and load them after loading the checkpoint explicitly I get the following:

```
2023-12-23 18:11:52,357 WARNING deprecation.py:50 -- DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
2023-12-23 18:11:52,431 WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
2023-12-23 18:11:52,432 WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
/home/simon/git-projects/ray/.venv-ray-2.7.1/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:484: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=""ignore::DeprecationWarning""
`UnifiedLogger` will be removed in Ray 2.7.
  return UnifiedLogger(config, logdir, loggers=None)
/home/simon/git-projects/ray/.venv-ray-2.7.1/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=""ignore::DeprecationWarning""
The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
/home/simon/git-projects/ray/.venv-ray-2.7.1/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=""ignore::DeprecationWarning""
The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
/home/simon/git-projects/ray/.venv-ray-2.7.1/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=""ignore::DeprecationWarning""
The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
2023-12-23 18:11:54,449 INFO worker.py:1633 -- Started a local Ray instance. View the dashboard at 127.0.0.1:8265 
(pid=45555) DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
(RolloutWorker pid=45554) 2023-12-23 18:11:58,611       WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
(RolloutWorker pid=45554) 2023-12-23 18:11:58,619       WARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!
(RolloutWorker pid=45554) 2023-12-23 18:11:58,619       WARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!
(RolloutWorker pid=45554) 2023-12-23 18:11:58,619       WARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!
(RolloutWorker pid=45554) 2023-12-23 18:11:58,619       WARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!
2023-12-23 18:11:58,642 WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
2023-12-23 18:11:58,650 WARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!
2023-12-23 18:11:58,650 WARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!
2023-12-23 18:11:58,650 WARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!
2023-12-23 18:11:58,650 WARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!
2023-12-23 18:11:58,722 WARNING util.py:68 -- Install gputil for GPU system monitoring.
100%|██████████████████████████████████████████████████████████████████████████| 70/70 [12:37<00:00, 10.82s/it, reward_max=174, reward_mean=19.9]
checkpoint checkpoint-LunarLanderContinuous-v2
2023-12-23 18:24:36,326 WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
2023-12-23 18:24:36,328 WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
/home/simon/git-projects/ray/.venv-ray-2.7.1/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:484: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=""ignore::DeprecationWarning""
`UnifiedLogger` will be removed in Ray 2.7.
  return UnifiedLogger(config, logdir, loggers=None)
/home/simon/git-projects/ray/.venv-ray-2.7.1/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=""ignore::DeprecationWarning""
The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
/home/simon/git-projects/ray/.venv-ray-2.7.1/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=""ignore::DeprecationWarning""
The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
/home/simon/git-projects/ray/.venv-ray-2.7.1/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=""ignore::DeprecationWarning""
The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
(pid=47497) DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future! [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)
(RolloutWorker pid=45555) 2023-12-23 18:11:58,556       WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
2023-12-23 18:24:39,199 WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
2023-12-23 18:24:39,229 WARNING util.py:68 -- Install gputil for GPU system monitoring.
(RolloutWorker pid=47497) 2023-12-23 18:24:39,174       WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
(RolloutWorker pid=47497) 2023-12-23 18:24:39,181       WARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!
(RolloutWorker pid=47497) 2023-12-23 18:24:39,181       WARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!
(RolloutWorker pid=47497) 2023-12-23 18:24:39,181       WARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!
(RolloutWorker pid=47497) 2023-12-23 18:24:39,181       WARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!
Learner states are not identical: 

Arrays are not equal
key='encoder.actor_encoder.net.mlp.0.weight'
key='default_policy'
key='module_state'

Mismatched elements: 2048 / 2048 (100%)
Max absolute difference: 0.10584226
Max relative difference: 93.31444
 x: array([[-1.392942e-04,  1.945784e-01, -3.034158e-01, ...,  1.375632e-01,
        -1.800514e-03,  2.797161e-01],
       [-3.417057e-02,  8.789328e-02, -9.027085e-02, ..., -2.808709e-01,...
 y: array([[-0.002647,  0.189661, -0.29099 , ...,  0.094808, -0.007005,
         0.280329],
       [-0.031376,  0.093555, -0.106848, ..., -0.234152, -0.145743,...
epoch=0 reward_max=194.642570 reward_mean=67.889576
epoch=1 reward_max=194.642570 reward_mean=77.619273
epoch=2 reward_max=194.642570 reward_mean=61.786369
epoch=3 reward_max=194.642570 reward_mean=26.306612
epoch=4 reward_max=194.642570 reward_mean=5.745107
epoch=5 reward_max=194.642570 reward_mean=19.390381
epoch=6 reward_max=194.642570 reward_mean=24.923322
epoch=7 reward_max=194.642570 reward_mean=21.782238
epoch=8 reward_max=194.642570 reward_mean=22.877175
epoch=9 reward_max=194.642570 reward_mean=24.819051
(pid=47498) DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
(RolloutWorker pid=47498) 2023-12-23 18:24:39,169       WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
```
@a-zhenya @Alex-Golod Could you try, if this improves further training for you as well (at least not deteriotes)?

Code:

```python
import numpy as np
from numpy.testing import assert_equal
from torch import use_deterministic_algorithms
from tqdm import trange
import random
from ray.rllib.algorithms import Algorithm
from ray.rllib.algorithms.ppo import PPOConfig

#random.seed(0)
#np.random.seed(0)
# use_deterministic_algorithms(True)

env_name = ""LunarLanderContinuous-v2""
config = (
    PPOConfig()
    .environment(env_name, env_config={})
    .framework(""torch"")
    .debugging(seed=0)
)

algo = config.build()

iterator = trange(70)
for epoch in iterator:
    result = algo.train()
    iterator.set_postfix({
        ""reward_max"": result[""episode_reward_max""],
        ""reward_mean"": result[""episode_reward_mean""],
    })

checkpoint = algo.save(""-"".join((""checkpoint"", env_name)))

learner_state_true = algo.learner_group.get_state()

local_worker_policy_weights_saved = algo.workers.local_worker().get_policy().get_weights()
remote_workers_policy_weights_saved = algo.workers.foreach_worker(lambda w: w.get_policy().get_weights())
local_worker_optimizer_state_saved = algo.workers.local_worker().get_policy().optimizer()[0].state_dict()
local_worker_kl_coeff_saved = algo.workers.local_worker().get_policy().kl_coeff

print(""checkpoint"", checkpoint.checkpoint.path)

env_name = ""LunarLanderContinuous-v2""
algo = Algorithm.from_checkpoint(""-"".join((""checkpoint"", env_name)))
learner_state_loaded = algo.learner_group.get_state()
algo.learner_group.set_state(learner_state_true)

local_worker_policy_weights_loaded = algo.workers.local_worker().get_policy().get_weights()
remote_workers_policy_weights_loaded = algo.workers.foreach_worker(lambda w: w.get_policy().get_weights())
local_worker_optimizer_state_loaded = algo.workers.local_worker().get_policy().optimizer()[0].state_dict()
local_worker_kl_coeff_loaded = algo.workers.local_worker().get_policy().kl_coeff

assert_equal(local_worker_policy_weights_saved, local_worker_policy_weights_loaded)
assert_equal(remote_workers_policy_weights_saved, remote_workers_policy_weights_loaded)
assert_equal(local_worker_optimizer_state_saved, local_worker_optimizer_state_loaded)
assert_equal(local_worker_kl_coeff_saved, local_worker_kl_coeff_loaded)
try:
    assert_equal(learner_state_true, learner_state_loaded)
except AssertionError as e:
    print(""Learner states are not identical: "")
    print(e)

for epoch in range(10):
    result = algo.train()
    print(""epoch=%(epoch)d reward_max=%(reward_max)f reward_mean=%(reward_mean)f"" % {
        ""epoch"": epoch,
        ""reward_max"": result[""episode_reward_max""],
        ""reward_mean"": result[""episode_reward_mean""],
    })
``` 
I have to check now, if this is still in the master.",think got beast least ray update learner worker store load loading explicitly get following warning raise error future warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done may removed future ray could suppress warning setting variable ignore removed ray return may removed future ray could suppress warning setting variable ignore interface favor interface removed ray may removed future ray could suppress warning setting variable ignore interface favor interface removed ray may removed future ray could suppress warning setting variable ignore interface favor interface removed ray local ray instance view dashboard raise error future warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done warning raise error future warning raise error future warning raise error future warning raise error future warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done warning raise error future warning raise error future warning raise error future warning raise error future warning install system warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done may removed future ray could suppress warning setting variable ignore removed ray return may removed future ray could suppress warning setting variable ignore interface favor interface removed ray may removed future ray could suppress warning setting variable ignore interface favor interface removed ray may removed future ray could suppress warning setting variable ignore interface favor interface removed ray raise error future repeated across cluster ray default set disable log deduplication see warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done warning install system warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done warning raise error future warning raise error future warning raise error future warning raise error future learner identical equal absolute difference relative difference array array raise error future warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done could try training well least code python import import torch import import import random import algorithm import true torch epoch result result result lambda print lambda try except print learner identical print epoch range result print epoch epoch epoch result result check still master,issue,negative,negative,neutral,neutral,negative,negative
1868328979,"Hi @Alex-Golod , @a-zhenya ,

I can reproduce this with the settings and code given by  @a-zhenya:

```
2023-12-23 16:21:55,767 WARNING deprecation.py:50 -- DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
2023-12-23 16:21:55,845 WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
2023-12-23 16:21:55,846 WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
/home/simon/git-projects/ray/.venv-ray-2.7.1/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:484: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=""ignore::DeprecationWarning""
`UnifiedLogger` will be removed in Ray 2.7.
  return UnifiedLogger(config, logdir, loggers=None)
/home/simon/git-projects/ray/.venv-ray-2.7.1/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=""ignore::DeprecationWarning""
The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
/home/simon/git-projects/ray/.venv-ray-2.7.1/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=""ignore::DeprecationWarning""
The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
/home/simon/git-projects/ray/.venv-ray-2.7.1/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=""ignore::DeprecationWarning""
The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
2023-12-23 16:21:59,018 INFO worker.py:1633 -- Started a local Ray instance. View the dashboard at 127.0.0.1:8265 
(pid=32243) DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
(RolloutWorker pid=32243) 2023-12-23 16:22:02,976       WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
(RolloutWorker pid=32242) 2023-12-23 16:22:03,181       WARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!
(RolloutWorker pid=32242) 2023-12-23 16:22:03,181       WARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!
(RolloutWorker pid=32242) 2023-12-23 16:22:03,181       WARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!
(RolloutWorker pid=32242) 2023-12-23 16:22:03,181       WARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!
2023-12-23 16:22:03,204 WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
2023-12-23 16:22:03,213 WARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!
2023-12-23 16:22:03,213 WARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!
2023-12-23 16:22:03,214 WARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!
2023-12-23 16:22:03,214 WARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!
2023-12-23 16:22:03,382 WARNING util.py:68 -- Install gputil for GPU system monitoring.
100%|████████████████████████████████████████████████████████████████████████████| 70/70 [11:32<00:00,  9.89s/it, reward_max=115, reward_mean=43]
checkpoint checkpoint-LunarLanderContinuous-v2
2023-12-23 16:33:35,778 WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
2023-12-23 16:33:35,780 WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
/home/simon/git-projects/ray/.venv-ray-2.7.1/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:484: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=""ignore::DeprecationWarning""
`UnifiedLogger` will be removed in Ray 2.7.
  return UnifiedLogger(config, logdir, loggers=None)
/home/simon/git-projects/ray/.venv-ray-2.7.1/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=""ignore::DeprecationWarning""
The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
/home/simon/git-projects/ray/.venv-ray-2.7.1/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=""ignore::DeprecationWarning""
The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
/home/simon/git-projects/ray/.venv-ray-2.7.1/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=""ignore::DeprecationWarning""
The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
(pid=33574) DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future! [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)
(RolloutWorker pid=32242) 2023-12-23 16:22:03,173       WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
2023-12-23 16:33:38,660 WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
2023-12-23 16:33:38,691 WARNING util.py:68 -- Install gputil for GPU system monitoring.
(RolloutWorker pid=33574) 2023-12-23 16:33:38,630       WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
(RolloutWorker pid=33573) 2023-12-23 16:33:38,629       WARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!
(RolloutWorker pid=33573) 2023-12-23 16:33:38,629       WARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!
(RolloutWorker pid=33573) 2023-12-23 16:33:38,629       WARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!
(RolloutWorker pid=33573) 2023-12-23 16:33:38,629       WARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!
epoch=0 reward_max=93.344314 reward_mean=62.094129
epoch=1 reward_max=93.344314 reward_mean=-274.113687
epoch=2 reward_max=93.344314 reward_mean=-301.282279
epoch=3 reward_max=93.344314 reward_mean=-275.462022
epoch=4 reward_max=-16.637611 reward_mean=-245.099437
epoch=5 reward_max=21.759212 reward_mean=-183.156077
epoch=6 reward_max=21.759212 reward_mean=-130.001128
epoch=7 reward_max=21.759212 reward_mean=-100.011878
epoch=8 reward_max=-7.240942 reward_mean=-89.582568
epoch=9 reward_max=24.435569 reward_mean=-73.508823
(pid=33573) DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
(RolloutWorker pid=33573) 2023-12-23 16:33:38,621       WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
```
Running with the same model 70 training steps shows that this is not simply a random occurrence. We continuously (not monotonously) improve on the policy:

```
2023-12-23 17:24:08,498 WARNING deprecation.py:50 -- DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
2023-12-23 17:24:08,586 WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
2023-12-23 17:24:08,588 WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
/home/simon/git-projects/ray/.venv-ray-2.7.1/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:484: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=""ignore::DeprecationWarning""
`UnifiedLogger` will be removed in Ray 2.7.
  return UnifiedLogger(config, logdir, loggers=None)
/home/simon/git-projects/ray/.venv-ray-2.7.1/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=""ignore::DeprecationWarning""
The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
/home/simon/git-projects/ray/.venv-ray-2.7.1/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=""ignore::DeprecationWarning""
The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
/home/simon/git-projects/ray/.venv-ray-2.7.1/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=""ignore::DeprecationWarning""
The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
2023-12-23 17:24:10,629 INFO worker.py:1633 -- Started a local Ray instance. View the dashboard at 127.0.0.1:8265 
(pid=39494) DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
2023-12-23 17:24:14,718 WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
(RolloutWorker pid=39494) 2023-12-23 17:24:14,688       WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
(RolloutWorker pid=39494) 2023-12-23 17:24:14,696       WARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!
(RolloutWorker pid=39494) 2023-12-23 17:24:14,696       WARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!
(RolloutWorker pid=39494) 2023-12-23 17:24:14,696       WARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!
(RolloutWorker pid=39494) 2023-12-23 17:24:14,696       WARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!
2023-12-23 17:24:14,728 WARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!
2023-12-23 17:24:14,728 WARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!
2023-12-23 17:24:14,728 WARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!
2023-12-23 17:24:14,728 WARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!
2023-12-23 17:24:14,905 WARNING util.py:68 -- Install gputil for GPU system monitoring.
epoch=0 reward_max=71.942782 reward_mean=29.362950
epoch=1 reward_max=71.942782 reward_mean=-246.399912
epoch=2 reward_max=71.942782 reward_mean=-250.449457
epoch=3 reward_max=-8.959841 reward_mean=-256.991591
epoch=4 reward_max=-8.959841 reward_mean=-204.166962
epoch=5 reward_max=-8.959841 reward_mean=-155.205488
epoch=6 reward_max=0.737502 reward_mean=-129.185057
epoch=7 reward_max=29.699549 reward_mean=-103.336022
epoch=8 reward_max=29.699549 reward_mean=-83.767958
epoch=9 reward_max=29.699549 reward_mean=-61.880901
epoch=10 reward_max=32.528726 reward_mean=-53.517398
epoch=11 reward_max=58.738799 reward_mean=-48.017133
epoch=12 reward_max=58.738799 reward_mean=-47.562251
epoch=13 reward_max=60.825983 reward_mean=-44.327294
epoch=14 reward_max=60.825983 reward_mean=-45.227916
epoch=15 reward_max=60.825983 reward_mean=-52.559765
epoch=16 reward_max=60.825983 reward_mean=-53.005637
epoch=17 reward_max=60.825983 reward_mean=-54.849816
epoch=18 reward_max=60.825983 reward_mean=-67.617736
epoch=19 reward_max=75.886626 reward_mean=-69.306415
epoch=20 reward_max=75.886626 reward_mean=-71.098000
epoch=21 reward_max=75.886626 reward_mean=-74.823604
epoch=22 reward_max=75.886626 reward_mean=-75.405332
epoch=23 reward_max=75.886626 reward_mean=-73.744116
epoch=24 reward_max=75.886626 reward_mean=-69.962604
epoch=25 reward_max=75.886626 reward_mean=-65.896632
epoch=26 reward_max=75.886626 reward_mean=-62.623619
epoch=27 reward_max=75.886626 reward_mean=-58.905915
epoch=28 reward_max=75.886626 reward_mean=-48.597242
epoch=29 reward_max=75.886626 reward_mean=-45.351422
epoch=30 reward_max=75.886626 reward_mean=-43.791728
epoch=31 reward_max=75.886626 reward_mean=-28.644430
epoch=32 reward_max=57.148407 reward_mean=-24.519006
epoch=33 reward_max=64.052325 reward_mean=-18.820716
epoch=34 reward_max=64.052325 reward_mean=-13.764601
epoch=35 reward_max=64.052325 reward_mean=-13.029260
epoch=36 reward_max=64.052325 reward_mean=-11.892111
epoch=37 reward_max=64.052325 reward_mean=-12.454157
epoch=38 reward_max=64.052325 reward_mean=-10.326727
epoch=39 reward_max=64.052325 reward_mean=-11.071436
epoch=40 reward_max=64.052325 reward_mean=-12.370016
epoch=41 reward_max=64.052325 reward_mean=-15.845623
epoch=42 reward_max=64.052325 reward_mean=-16.522023
epoch=43 reward_max=64.052325 reward_mean=-18.664119
epoch=44 reward_max=64.052325 reward_mean=-14.998912
epoch=45 reward_max=64.052325 reward_mean=-14.883150
epoch=46 reward_max=64.052325 reward_mean=-13.537082
epoch=47 reward_max=60.800750 reward_mean=-16.632559
epoch=48 reward_max=60.800750 reward_mean=-19.092039
epoch=49 reward_max=60.800750 reward_mean=-21.617650
epoch=50 reward_max=60.800750 reward_mean=-19.765703
epoch=51 reward_max=60.800750 reward_mean=-18.781244
epoch=52 reward_max=60.800750 reward_mean=-23.683272
epoch=53 reward_max=60.800750 reward_mean=-26.093546
epoch=54 reward_max=60.800750 reward_mean=-29.741240
epoch=55 reward_max=60.800750 reward_mean=-29.149571
epoch=56 reward_max=60.800750 reward_mean=-28.150223
epoch=57 reward_max=60.800750 reward_mean=-29.957675
epoch=58 reward_max=60.800750 reward_mean=-23.518154
epoch=59 reward_max=60.800750 reward_mean=-22.627249
epoch=60 reward_max=60.800750 reward_mean=-20.992431
epoch=61 reward_max=60.800750 reward_mean=-21.115987
epoch=62 reward_max=60.800750 reward_mean=-20.012039
epoch=63 reward_max=60.800750 reward_mean=-18.052865
epoch=64 reward_max=60.684727 reward_mean=-19.173438
epoch=65 reward_max=60.684727 reward_mean=-14.204731
epoch=66 reward_max=60.684727 reward_mean=-14.249633
epoch=67 reward_max=60.684727 reward_mean=-4.956989
epoch=68 reward_max=60.684727 reward_mean=-4.933177
epoch=69 reward_max=60.684727 reward_mean=-1.276906
(pid=39495) DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
(RolloutWorker pid=39495) 2023-12-23 17:24:14,629       WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
```

I have to dig deeper into this. My guess is that there might be somewhere an initialization operation happening after loading the weights.",hi reproduce code given warning raise error future warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done may removed future ray could suppress warning setting variable ignore removed ray return may removed future ray could suppress warning setting variable ignore interface favor interface removed ray may removed future ray could suppress warning setting variable ignore interface favor interface removed ray may removed future ray could suppress warning setting variable ignore interface favor interface removed ray local ray instance view dashboard raise error future warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done warning raise error future warning raise error future warning raise error future warning raise error future warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done warning raise error future warning raise error future warning raise error future warning raise error future warning install system warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done may removed future ray could suppress warning setting variable ignore removed ray return may removed future ray could suppress warning setting variable ignore interface favor interface removed ray may removed future ray could suppress warning setting variable ignore interface favor interface removed ray may removed future ray could suppress warning setting variable ignore interface favor interface removed ray raise error future repeated across cluster ray default set disable log deduplication see warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done warning install system warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done warning raise error future warning raise error future warning raise error future warning raise error future raise error future warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done running model training simply random occurrence continuously monotonously improve policy warning raise error future warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done may removed future ray could suppress warning setting variable ignore removed ray return may removed future ray could suppress warning setting variable ignore interface favor interface removed ray may removed future ray could suppress warning setting variable ignore interface favor interface removed ray may removed future ray could suppress warning setting variable ignore interface favor interface removed ray local ray instance view dashboard raise error future warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done warning raise error future warning raise error future warning raise error future warning raise error future warning raise error future warning raise error future warning raise error future warning raise error future warning install system raise error future warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done dig guess might somewhere operation happening loading,issue,negative,negative,neutral,neutral,negative,negative
1868088953,"`RAY_SERVE_DEBUG_MODE` currently exclusively controls whether we're using `asyncio` or `uvloop` and doesn't have any other side-effects so you should be good with that.

Are you asking for an explicit lever to control what loop is going to be used (like `RAY_SERVE_EVENT_LOOP_IMPL=asyncio`)?",currently exclusively whether good explicit lever control loop going used like,issue,positive,positive,positive,positive,positive,positive
1867996040,"Hi All,

What is the solution to the problem, within Ray i too cannot see GPU utilization. Says RuntimeError : No GPU found.
Request if anyone knows the solution this problem 

-Rani",hi solution problem within ray see utilization found request anyone solution problem,issue,negative,neutral,neutral,neutral,neutral,neutral
1867953881,"Hi,

I need to correct myself. The minimum example does not show the problem, I encounter. The occupied memory is not related to ray, somehow I thought that it would only track processes related to ray.

So, the problem is not with ray.init(), but I have a memory leak later on. I guess that I should open a new issue, but to inform you, I attach two screenshots. 
The memory is increasing, until ray crashes due to oom.

Unfortunately, I am not able to make a minimum working example.
<img width=""700"" alt=""Screenshot 2023-12-22 at 17 06 22"" src=""https://github.com/ray-project/ray/assets/89135626/c6940443-5c38-41b4-9f9b-0927805d8489"">
<img width=""700"" alt=""Screenshot 2023-12-22 at 17 07 45"" src=""https://github.com/ray-project/ray/assets/89135626/8bb64f1b-914b-4853-8a97-fc8afa9c5d17"">

Perhaps, we should we close this issue and I make a new oom one.
",hi need correct minimum example show problem encounter memory related ray somehow thought would track related ray problem memory leak later guess open new issue inform attach two memory increasing ray due unfortunately able make minimum working example perhaps close issue make new one,issue,negative,positive,neutral,neutral,positive,positive
1867949482,"While this is still an going issue, any comment on how I can regularly delete the contents of my _temp_dir? While training, checkpoints are being saved to the node (which has very limited disk space). I want to set _temp_dir to cloud storage but I am blocked by this issue.",still going issue comment regularly delete content training saved node limited disk space want set cloud storage blocked issue,issue,negative,negative,neutral,neutral,negative,negative
1867927100,"The remaining test fails I think only have to do with `num_blocks` in the dataset repr. These are the cases where the number of read tasks produced is less than parallelism, since there is only one file. ",test think number read produced le parallelism since one file,issue,negative,neutral,neutral,neutral,neutral,neutral
1867839473,"Thanks for confirming my guesses and the link in the docs, I didn't notice that part (buried in Serve section and search had no results).

Would try but currently blocked on 2.9.0 by #42058.",thanks confirming link notice part buried serve section search would try currently blocked,issue,negative,positive,neutral,neutral,positive,positive
1867682001,"> @lsc64 can you check to make sure the head node and the worker node can talk to each other. From the error message, seems there are communication issues between the two nodes/dockers.

The nodes show up in the dashboard as part of the cluster. Also, the startup log doesn't show any errors (unlike if e.g. the head adress is wrong). Seems like they can connect or what else should I check? 
The issue doesn't appear if I only have the head, so it might still have something to do with communication.

> Also are you able to try kuberay which is the recommended way to run Ray cluster with dockers and k8s.

I can't use cgroup2, so this is currently not an option.
",check make sure head node worker node talk error message communication two show dashboard part cluster also log show unlike head wrong like connect else check issue appear head might still something communication also able try way run ray cluster ca use currently option,issue,negative,positive,positive,positive,positive,positive
1867155007,maybe we can have optional field to disable task metric collection?,maybe optional field disable task metric collection,issue,negative,neutral,neutral,neutral,neutral,neutral
1867131975,"> Will do! You may need to add a kuberay.tests.yaml for the test_in_docker to be happy.

added.",may need add happy added,issue,positive,positive,positive,positive,positive,positive
1867020307,"This is a likely caused by a combination of 

 - (Addressed in 2.9) Serve producing new time-series for every request: https://github.com/ray-project/ray/issues/41282
 - (Addressed in 2.9) Metrics not being delivered to agent from workers: https://github.com/ray-project/ray/issues/40033

We've validated both of these were addressed in 2.9, so closing this one",likely combination serve new every request metric agent one,issue,negative,positive,neutral,neutral,positive,positive
1866972997,"> Neither `rayproject/ray:nightly-py311` and `rayproject/ray:2.9.0.932eed-py311` have the `podman` executable in them. Am I looking at the right images?

@salotz Correct, podman currently isn't built into the ray images as a dependency because the feature is still experimental. You will have to build a new image using the ray image as a base image and install podman into the new image: see [here](https://docs.ray.io/en/master/serve/advanced-guides/multi-app-container.html#install-podman).",neither executable looking right correct currently built ray dependency feature still experimental build new image ray image base image install new image see,issue,negative,negative,neutral,neutral,negative,negative
1866780507,"> Nice. Can we add a unit test?

Thanks, added. ",nice add unit test thanks added,issue,positive,positive,positive,positive,positive,positive
1866759248,Can probably follow up by a doc change 🙃,probably follow doc change,issue,negative,neutral,neutral,neutral,neutral,neutral
1866702541,"Hi, the workaround above should remove the warning, but I will remove these deprecation warnings fully in 2.10. I'll re-open this issue to track that and will update this thread once it's done.",hi remove warning remove deprecation fully issue track update thread done,issue,negative,neutral,neutral,neutral,neutral,neutral
1866687372,"Great catch @jesuspc ! Thanks for pointing us towards this issue. I have looked into it a bit deeper and found a little twist in the docs which I will change later. 

I can follow your argumentation. Afaics in the source code connectors are considered in `compute_single_action`. The question here is why they do not apply. I try to dig deeper in the next hours. So thanks again for reporting this.

For now, could you try (as shown in the docs, but forgetting the policy id) use this and tell me, if this makes it easier for you? 

```python
import gymnasium as gym

from ray.rllib.algorithms.appo.appo import APPOConfig
from ray.rllib.policy.policy import Policy
from ray.rllib.utils.policy import local_policy_inference
from ray import air, tune

config = (
    APPOConfig()
    .environment(
        env=""CartPole-v1"",
    )
    .rollouts(
        num_envs_per_worker=5,
        num_rollout_workers=1,
        observation_filter=""MeanStdFilter"",
        # Default.
        enable_connectors=True,
    )
    .training(
        num_sgd_iter=6,
        vf_loss_coeff=0.01,
        vtrace=False,
        model={
            ""fcnet_hiddens"": [32],
            ""fcnet_activation"": ""linear"",
            ""vf_share_layers"": False,
        },
    )
)

tuner = tune.Tuner(
    ""APPO"",
    run_config=air.RunConfig(
        name=""test_appo_issue"",
        stop={
            ""sampler_results/episode_reward_mean"": 150,
            ""timesteps_total"": 200000
        }
    ),
    param_space=config,
)
tuner.fit()

chkpt = ""<HOME>/ray_results/test_appo_issue/folder_name/checkpoint_000000""

policy = Policy.from_checkpoint(chkpt)

env = gym.make(""CartPole-v1"")

obs, info = env.reset()
terminated = truncated = False
step = 0
while not terminated and not truncated:
    step += 1

    # Use local_policy_inference() to run inference, so we do not have to
    # provide policy states or extra fetch dictionaries.
    # ""env_1"" and ""agent_1"" are dummy env and agent IDs to run connectors with.
    policy_outputs = local_policy_inference(
        policy[""default_policy""], ""env_1"", ""agent_1"", obs, explore=False
    )
    assert len(policy_outputs) == 1
    action, _, _ = policy_outputs[0]
    print(f""step {step}"", obs, action)

    # Step environment forward one more step.
    obs, _, terminated, truncated, _ = env.step(action)
```

This should at least use the connectors.",great catch thanks pointing u towards issue bit found little twist change later follow argumentation source code considered question apply try dig next thanks could try shown forgetting policy id use tell easier python import gymnasium gym import import policy import ray import air tune default linear false tuner home policy truncated false step truncated step use run inference provide policy extra fetch dummy agent run policy assert action print step step action step environment forward one step truncated action least use,issue,positive,positive,neutral,neutral,positive,positive
1866657942,"Example of artificially injecting a `0.8s` sleep into the response:
```
(ProxyActor pid=33502) WARNING 2023-12-21 11:06:20,159 proxy 127.0.0.1 router.py:740 - Failed to get queue length from replica default#A#u3m0im2u within 0.1s. If this happens repeatedly it's likely caused by high network latency in the cluster. You can configure the deadline using the `RAY_SERVE_QUEUE_LENGTH_RESPONSE_DEADLINE_S` environment variable.
(ProxyActor pid=33502) WARNING 2023-12-21 11:06:20,361 proxy 127.0.0.1 router.py:740 - Failed to get queue length from replica default#A#u3m0im2u within 0.2s. If this happens repeatedly it's likely caused by high network latency in the cluster. You can configure the deadline using the `RAY_SERVE_QUEUE_LENGTH_RESPONSE_DEADLINE_S` environment variable.
(ProxyActor pid=33502) WARNING 2023-12-21 11:06:20,815 proxy 127.0.0.1 router.py:740 - Failed to get queue length from replica default#A#u3m0im2u within 0.4s. If this happens repeatedly it's likely caused by high network latency in the cluster. You can configure the deadline using the `RAY_SERVE_QUEUE_LENGTH_RESPONSE_DEADLINE_S` environment variable.
(ProxyActor pid=33502) WARNING 2023-12-21 11:06:21,718 proxy 127.0.0.1 router.py:740 - Failed to get queue length from replica default#A#u3m0im2u within 0.8s. If this happens repeatedly it's likely caused by high network latency in the cluster. You can configure the deadline using the `RAY_SERVE_QUEUE_LENGTH_RESPONSE_DEADLINE_S` environment variable.
(ServeReplica:default:A pid=33504) INFO 2023-12-21 11:06:22,688 default_A u3m0im2u c39176f8-7c1b-4966-bd5b-c45135dc3e46 / replica.py:745 - __CALL__ OK 0.1ms
```",example artificially sleep response warning proxy get queue length replica default within repeatedly likely high network latency cluster configure deadline environment variable warning proxy get queue length replica default within repeatedly likely high network latency cluster configure deadline environment variable warning proxy get queue length replica default within repeatedly likely high network latency cluster configure deadline environment variable warning proxy get queue length replica default within repeatedly likely high network latency cluster configure deadline environment variable default,issue,negative,positive,neutral,neutral,positive,positive
1866589798,"The check passed:

```
   ...: verify_amis()
AMI Matched in us-east-1
AMI Matched in us-east-2
AMI Matched in us-west-1
AMI Matched in us-west-2
AMI Matched in ca-central-1
AMI Matched in eu-central-1
AMI Matched in eu-west-1
AMI Matched in eu-west-2
AMI Matched in eu-west-3
AMI Matched in sa-east-1
AMI Matched in ap-northeast-1
AMI Matched in ap-northeast-2
AMI Matched in ap-northeast-3
AMI Matched in ap-southeast-1
AMI Matched in ap-southeast-2
```

Running the cluster launcher release tests, then I'll merge it!",check ami ami ami ami ami ami ami ami ami ami ami ami ami ami ami running cluster launcher release merge,issue,negative,neutral,neutral,neutral,neutral,neutral
1866082115,hi 👋 https://github.com/ray-project/ray/pull/41775 was not included in ray 2.9.0 which was just released. could someone cherry-pick it onto 2.9.1?,hi included ray could someone onto,issue,negative,neutral,neutral,neutral,neutral,neutral
1866076817,"I also tried to apply a similar modification to TBXLoggerCallback. 
I discovered the following issue with image handling: 

When the image in episode.media gets processed by the JSONLoggerCallback, significant delays (minutes!) are introduced by the JSON logger. This is because the JSON logger needs to rewrite the log file at each logging call (see #21416) . I worked around this by disabling other Logger Callbacks by setting `TUNE_DISABLE_AUTO_CALLBACK_LOGGERS` to `1`.

Also I think it needs to be decided how to handle the case when images are logged by multiple episodes at one time in `summarize_episodes`. In my opinion, all images should be kept as it is currently implemented. The user can then decide how to log images across multiple episodes in his algorithm class by adding a on_train_result callback:
```python
        def on_train_result(
                self,
                *,
                algorithm: ""Algorithm"",
                result: dict,
                **kwargs,
        ) -> None:
            """"""Called at the end of Algorithm.train().

            Args:
                algorithm: Current Algorithm instance.
                result: Dict of results returned from Algorithm.train() call.
                    You can mutate this object to add additional metrics.
                kwargs: Forward compatibility placeholder.
            """"""
            if 'trajectory' in result['episode_media'].keys():
                result['episode_media']['myimage'] = result['episode_media']['myimage'][0]
```

My modification of the TBXLoggerCallback.log_trial_result() function looks like this. I decided to make it more strict with the numpy array, to not automatically interpret any 3D array as image:
```python
            ...
            elif (isinstance(value, list) and len(value) > 0) or (
                    isinstance(value, np.ndarray) and value.size > 0
            ):
                valid_result[full_attr] = value

                # Check for list of images:
                if all(isinstance(v, np.ndarray) and v.ndim == 3 and v.shape[0] in [1, 3] for v in value):
                    if len(value) == 1:
                        # only one image
                        self._trial_writer[trial].add_image(full_attr, value[0], global_step=step)
                    else:
                        # Multiple images - stack them as tensorboard requires
                        imgs = np.stack(value)
                        self._trial_writer[trial].add_images(full_attr, imgs, global_step=step)
                    continue

                # Check for list of videos:
                if all(isinstance(v, np.ndarray) and v.ndim == 5 and v.shape[2] in [1,3] for v in value):
                    video = np.concatenate(value, axis=1)
                    self._trial_writer[trial].add_video(
                        full_attr, video, global_step=step, fps=20)
                    continue

                # Cover either a single video or a single image
                if isinstance(value, np.ndarray) and value.size > 0:
                    # Video - Must have 5 dimensions in NTCHW format:
                    # C must be either 1 for grayscale of 3 for RGB
                    if value.ndim == 5 and value.shape[2] in [1, 3]:
                        self._trial_writer[trial].add_video(
                            full_attr, value, global_step=step, fps=20
                        )
                        continue

                    # Image - Must have 3 dimensions in CHW format.
                    # C must be either 1 for grayscale of 3 for RGB
                    if value.ndim == 3 and value.shape[0] in [1, 3]:
                        self._trial_writer[trial].add_image(
                            full_attr, value, global_step=step
                        )
                        continue

                try:
                ...
```",also tried apply similar modification discovered following issue image handling image significant logger logger need rewrite log file logging call see worked around logger setting also think need decided handle case logged multiple one time opinion kept currently user decide log across multiple algorithm class python self algorithm algorithm result none end algorithm current algorithm instance result returned call mutate object add additional metric forward compatibility result result result modification function like decided make strict array automatically interpret array image python value list value value value check list value value one image trial value else multiple stack value trial continue check list value video value trial video continue cover either single video single image value video must format must either trial value continue image must format must either trial value continue try,issue,positive,positive,neutral,neutral,positive,positive
1866073821,hi @justinvyu 👋 in which release will the deprecation warning be removed? 2.9 just came out with the warning [still in place](https://github.com/ray-project/ray/blob/ray-2.9.0/python/ray/tune/trainable/util.py#L132),hi release deprecation warning removed came warning still place,issue,negative,neutral,neutral,neutral,neutral,neutral
1866035004,"Hi @alexeykudinkin ,
it is not that simple because uvloop comes as a transitive dependency of Ray serve. So how can I exclude it from being installed? 
Can you therefore please reopen the issue?

Details:
Ray serve installs uvicorn[standard] in the setup.py:
![image](https://github.com/ray-project/ray/assets/112665668/bfa3ce2a-09c3-4f65-b357-4e2acac25525)

uvicorn[standard] adds uvloop:
![image](https://github.com/ray-project/ray/assets/112665668/4c867cb3-3b51-4fc6-b353-c50f1e517d2d)

I also tried to tell uvicorn to use the asyncio-loop with the environment variable UVICORN_LOOP=asyncio but this seems to be ignored.



",hi simple come transitive dependency ray serve exclude therefore please reopen issue ray serve standard image standard image also tried tell use environment variable,issue,negative,neutral,neutral,neutral,neutral,neutral
1865999296,"@rkooo567 any progression for this problem? we have the same problem
But we have no idea about why this would happen",progression problem problem idea would happen,issue,negative,neutral,neutral,neutral,neutral,neutral
1865517162,Will do! You may need to add a kuberay.tests.yaml for the test_in_docker to be happy.,may need add happy,issue,positive,positive,positive,positive,positive,positive
1865508538,"@can-anyscale , I leave it to you to handle this new team in the flaky test tracker.",leave handle new team flaky test tracker,issue,negative,positive,positive,positive,positive,positive
1865451883,Just realized 2.9.0 was released and built. I also don't see it in the container though.,built also see container though,issue,negative,neutral,neutral,neutral,neutral,neutral
1865446142,Neither `rayproject/ray:nightly-py311` and `rayproject/ray:2.9.0.932eed-py311` have the `podman` executable in them. Am I looking at the right images?,neither executable looking right,issue,negative,positive,positive,positive,positive,positive
1865395639,"Hmm, it seems `memray` has a substantially larger dependency tree than `py-spy` does. For this reason, I think it would be best if we started off with this as a soft-dependency (error telling the user to install memray).

```
memray==1.11.0
├── Jinja2 [required: >=2.9, installed: 3.1.2]
│   └── MarkupSafe [required: >=2.0, installed: 2.1.1]
├── rich [required: >=11.2.0, installed: 12.6.0]
│   ├── commonmark [required: >=0.9.0,<0.10.0, installed: 0.9.1]
│   ├── Pygments [required: >=2.6.0,<3.0.0, installed: 2.11.2]
│   └── typing-extensions [required: >=4.0.0,<5.0, installed: 4.2.0]
└── textual [required: >=0.34.0, installed: 0.46.0]
    ├── markdown-it-py [required: >=2.1.0, installed: 3.0.0]
    │   └── mdurl [required: ~=0.1, installed: 0.1.2]
    ├── rich [required: >=13.3.3, installed: 12.6.0]
    │   ├── commonmark [required: >=0.9.0,<0.10.0, installed: 0.9.1]
    │   ├── Pygments [required: >=2.6.0,<3.0.0, installed: 2.11.2]
    │   └── typing-extensions [required: >=4.0.0,<5.0, installed: 4.2.0]
    └── typing-extensions [required: >=4.4.0,<5.0.0, installed: 4.2.0]
```",substantially dependency tree reason think would best error telling user install jinja rich textual rich,issue,positive,positive,positive,positive,positive,positive
1865358137,"Reran the Parquet metadata release test, no regression - https://buildkite.com/ray-project/release/builds/4714#018c89f0-5bde-4011-835d-90d244d2c3f0 .",parquet release test regression,issue,negative,neutral,neutral,neutral,neutral,neutral
1865347543,"Tests look good, retrying flaky one.",look good flaky one,issue,negative,positive,positive,positive,positive,positive
1865283263,"> The image will be replaced with rayproject/ray:nightly-py38.

it will be replaced with `rayci:kuberay-test` , which is built from the commit in the test job.",image built commit test job,issue,negative,neutral,neutral,neutral,neutral,neutral
1865281699,"> how would someone run `test_autoscaling_e2e.py` locally on their laptop, for example to reproduce some failure in CI? I guess `pytest -vs test_autoscaling_e2e.py` wouldn't work because it would be missing some setup steps.

I do not know how one would do it today before this change..

I guess one would need to run the setup steps manually in `ci/k8s/run-operator-tests.sh` first, and then run `bazel test //python/ray/tests:kuberay/test_autoscaling_e2e`

but it is still an approximation.",would someone run locally example reproduce failure guess would work would missing setup know one would today change guess one would need run setup manually first run test still approximation,issue,negative,negative,neutral,neutral,negative,negative
1865272918,"Thanks for your response, I'll try the nightlies. I was about to go building my own image with podman in it. Looks like there was more to it than that.

I agree with some of the other concerns in this thread that this is a super important feature for me as I have lots of software that needs special compilation and isn't available in those package managers.",thanks response try go building image like agree thread super important feature lot need special compilation available package,issue,positive,positive,positive,positive,positive,positive
1865268981,"The `container` field of `runtime_env` is fixed in https://github.com/ray-project/ray/pull/40419 and will be included in Ray 2.9, which should be released today or tomorrow.  Or you can try it today on the Ray nightly image.  Let us know if you run into any issues!",container field fixed included ray today tomorrow try today ray nightly image let u know run,issue,negative,positive,neutral,neutral,positive,positive
1865265601,"@mattip since we don't support ray ha in windows, the best way forward seems to just not include redis for windows build. Do you think it is possible to take this approach instead?",since support ray ha best way forward include build think possible take approach instead,issue,positive,positive,positive,positive,positive,positive
1865260118,"I ran into this issue as well. I can report a little more from the logs though.

In `logs/runtime_env_setup-04000000.log` I get this repeated every minute:

```
2023-12-20 14:58:51,442	INFO plugin.py:257 -- Runtime env working_dir gcs://_ray_pkg_bc8d66d491161e12.zip is already installed and will be reused. Search all runtime_env_setup-*.log to find the corresponding setup log.
2023-12-20 14:58:51,442	INFO uri_cache.py:71 -- Marked URI gcs://_ray_pkg_bc8d66d491161e12.zip used.
```

In `logs/runtime_env_agent.log` every minute this is repeated:

```
2023-12-20 15:04:51,469	INFO runtime_env_agent.py:506 -- Got request from raylet to decrease reference for runtime env: {""container"": {""image"": ""apps-rdkit-container:latest""}, ""env_vars"": {""RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES"": ""1"", ""RAY_worker_niceness"": ""0""}, ""working_dir"": ""gcs://_ray_pkg_bc8d66d491161e12.zip""}.
2023-12-20 15:04:51,469	INFO runtime_env_agent.py:128 -- Unused runtime env {""container"": {""image"": ""apps-rdkit-container:latest""}, ""env_vars"": {""RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES"": ""1"", ""RAY_worker_niceness"": ""0""}, ""working_dir"": ""gcs://_ray_pkg_bc8d66d491161e12.zip""}.
2023-12-20 15:04:51,469	INFO runtime_env_agent.py:260 -- Runtime env {""container"": {""image"": ""apps-rdkit-container:latest""}, ""env_vars"": {""RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES"": ""1"", ""RAY_worker_niceness"": ""0""}, ""working_dir"": ""gcs://_ray_pkg_bc8d66d491161e12.zip""} removed from env-level cache.
2023-12-20 15:04:51,469	INFO runtime_env_agent.py:109 -- Unused uris [('gcs://_ray_pkg_bc8d66d491161e12.zip', 'working_dir')].
2023-12-20 15:04:51,469	INFO runtime_env_agent.py:353 -- Creating runtime env: {""container"": {""image"": ""apps-rdkit-container:latest""}, ""env_vars"": {""RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES"": ""1"", ""RAY_worker_niceness"": ""0""}, ""working_dir"": ""gcs://_ray_pkg_bc8d66d491161e12.zip""} with timeout 600 seconds.
2023-12-20 15:04:51,470	INFO runtime_env_agent.py:399 -- Successfully created runtime env: {""container"": {""image"": ""apps-rdkit-container:latest""}, ""env_vars"": {""RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES"": ""1"", ""RAY_worker_niceness"": ""0""}, ""working_dir"": ""gcs://_ray_pkg_bc8d66d491161e12.zip""}, the context: {""command_prefix"": [""cd"", ""/tmp/ray/session_2023-12-20_14-15-21_259552_8/runtime_resources/working_dir_files/_ray_pkg_bc8d66d491161e12"", ""&&""], ""env_vars"": {""RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES"": ""1"", ""RAY_worker_niceness"": ""0"", ""PYTHONPATH"": ""/tmp/ray/session_2023-12-20_14-15-21_259552_8/runtime_resources/working_dir_files/_ray_pkg_bc8d66d491161e12""}, ""py_executable"": ""podman run -v /tmp/ray:/tmp/ray --cgroup-manager=cgroupfs --network=host --pid=host --ipc=host --env-host --env RAY_RAYLET_PID=115 --entrypoint python apps-rdkit-container:latest"", ""resources_dir"": null, ""container"": {}, ""java_jars"": []}

```
Finally the smoking gun under `logs/raylet.err` which repeats every minute:

```
[2023-12-20 14:59:51,444 E 115 115] (raylet) worker_pool.cc:553: Some workers of the worker process(584) have not registered within the timeout. The process is dead, probably it crashed during start.
bash: line 0: exec: podman: not found
```

I'll try to install `podman` to my cluster/worker containers and see if that helps. For reference I was using: `rayproject/ray:2.8.0-py310` for the head group and my single worker group.

Running on linux and cluster on minikube:

```console
$ uname -src
Linux 6.6.6-1-default #1 SMP PREEMPT_DYNAMIC Mon Dec 11 09:46:39 UTC 2023 (a946a9f)

$ lsb_release -a
LSB Version:    n/a
Distributor ID: openSUSE
Description:    openSUSE Tumbleweed
Release:        20231215
Codename:       n/a

$ minikube version
minikube version: v1.32.0
commit: 69993dc5f6ebf06f02b2ddf105c428f1a0d85030
```

",ran issue well report little though get repeated every minute already search find corresponding setup log marked used every minute repeated got request raylet decrease reference container image latest unused container image latest container image latest removed cache unused container image latest successfully container image latest context run python latest null container finally smoking gun every minute raylet worker process registered within process dead probably start bash line found try install see reference head group single worker group running cluster console default mon version distributor id description tumbleweed release version version commit,issue,negative,positive,positive,positive,positive,positive
1865258667,"@Martin4R uvloop is just being installed by default now with Serve, which by itself bumps up performance of HTTP/gRPC applications by about 10-20%.

However, it's not a requirement to use Serve and you can switch it off by simply removing uvloop from dependencies (in that case we'll fallback to use default Python asyncio impl.
",default serve performance however requirement use serve switch simply removing case fallback use default python,issue,negative,neutral,neutral,neutral,neutral,neutral
1865191349,"Thanks both! The changes look good from CI's perspective, but if you two can help with the test logic I would appreciate.",thanks look good perspective two help test logic would appreciate,issue,positive,positive,positive,positive,positive,positive
1865181127,"

> But on closer inspection, the test just uses Ray 2.7.0

https://github.com/ray-project/ray/blob/b908d57cb3b6ec88a6a8d19ba7c1557b886b88b9/python/ray/tests/kuberay/test_autoscaling_e2e.py#L122

The image will be replaced with `rayproject/ray:nightly-py38`.

> what is this test doing?

This is to test Ray Autoscaler's KubeRay NodeProvider.

> Who should be the right owner of this test

OSS Infra on-call can own this test (i.e., @architkulkarni or I) and help debug.",closer inspection test ray image test test ray right owner test infra test help,issue,negative,positive,positive,positive,positive,positive
1865176196,"> At a glance, it looks like the point of this test is to ensure compatibility between Ray master <> KubeRay, because we have no such test in KubeRay CI. But on closer inspection, the test just uses Ray 2.7.0: https://github.com/architkulkarni/ray/blob/2b88eb5c8063634ec6d2561666b4d5b70908b392/python/ray/tests/kuberay/test_files/ray-cluster.autoscaler-template.yaml#L12 So I'd need to look more carefully to figure out if this test is necessary. @kevin85421 thoughts?

Ah, even though the KubeRay RayCluster uses Ray 2.7.0, the test still uses the current Ray commit for the client side (Ray Client, and Ray Job Submission).  So it is actually testing something that isn't tested in KubeRay CI.",glance like point test ensure compatibility ray master test closer inspection test ray need look carefully figure test necessary ah even though ray test still current ray commit client side ray client ray job submission actually testing something tested,issue,positive,negative,neutral,neutral,negative,negative
1865175972,Sounds good to me - @simran-2797 I'm happy to make additional buttons for these tags. Do we have appropriate icons for the ray library buttons?,good happy make additional button appropriate ray library button,issue,positive,positive,positive,positive,positive,positive
1865172509,"Is this something that you and @simran-2797 can explore, to determine the best UX? When I briefly sync'd with her this morning, we acknowledged that the number of tags/buttons was getting a bit too high. ",something explore determine best briefly sync morning acknowledged number getting bit high,issue,positive,positive,positive,positive,positive,positive
1865170405,"I don't have context on this test, @kevin85421 do you? If you also don't have context, I can read through the test, answer @can-anyscale's questions and own it going forward.

At a glance, it looks like the point of this test is to ensure compatibility between Ray master <> KubeRay, because we have no such test in KubeRay CI.  But on closer inspection, the test just uses Ray 2.7.0: https://github.com/architkulkarni/ray/blob/2b88eb5c8063634ec6d2561666b4d5b70908b392/python/ray/tests/kuberay/test_files/ray-cluster.autoscaler-template.yaml#L12 So I'd need to look more carefully to figure out if this test is necessary. @kevin85421 thoughts?",context test also context read test answer going forward glance like point test ensure compatibility ray master test closer inspection test ray need look carefully figure test necessary,issue,positive,negative,neutral,neutral,negative,negative
1865137699,"I'll merge this as one of the final steps in the release process, after the wheels are uploaded to PyPI and pass the sanity checks.",merge one final release process pas sanity,issue,negative,neutral,neutral,neutral,neutral,neutral
1865126655,"At a high level, what is this test doing? Who should be the right owner of this test @architkulkarni? Who can debug this when it fails? Who will be unhappy if we don't even have this test?",high level test right owner test unhappy even test,issue,negative,negative,neutral,neutral,negative,negative
1865124745,"It's hard to understand a unit test given it depends on failure from prefetching metadata.

I can run the Parquet metadata prefetching release test, to make sure it still works - https://github.com/ray-project/ray/blob/master/release/release_tests.yaml#L5485 .",hard understand unit test given failure run parquet release test make sure still work,issue,negative,negative,neutral,neutral,negative,negative
1865088299,linkcheck is running now \o/; i think it should be safe to merge as long as that passes for now,running think safe merge long,issue,negative,positive,positive,positive,positive,positive
1864999304,"> > Can you make the default flush interval 0? It doesn't seem like a safe change to make by default otherwise, since we don't have a background thread to trigger flushes on timeout.
> 
> I was concerned about this issue as well initially. But after thinking more, this seemed fine. The behavior is the same as logging libraries. new logs are flushed by the next write or when closing.

@ericl does this make sense? alternatively, I think we can at least make the interval default to 1s for Data. Not sure if there will be an issue for other libraries.",make default flush interval seem like safe change make default otherwise since background thread trigger concerned issue well initially thinking fine behavior logging new next write make sense alternatively think least make interval default data sure issue,issue,positive,positive,positive,positive,positive,positive
1864925180,"Ok, I will close this for now, and if needed please reopen or open a new issue",close please reopen open new issue,issue,negative,positive,neutral,neutral,positive,positive
1864863904,"Sorry, would not be able to check in the foreseeable future.

> On 20 Dec 2023, at 16:56, Matti Picus ***@***.***> wrote:
> 
> 
> @ilyakochik <https://github.com/ilyakochik> does this still not work?
> 
> —
> Reply to this email directly, view it on GitHub <https://github.com/ray-project/ray/issues/14771#issuecomment-1864820679>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AK3WEOCMY2MA7CS5BB3TTR3YKMKEVAVCNFSM4ZM2JRH2U5DIOJSWCZC7NNSXTN2JONZXKZKDN5WW2ZLOOQ5TCOBWGQ4DEMBWG44Q>.
> You are receiving this because you were mentioned.
> 

",sorry would able check foreseeable future matti wrote still work reply directly view,issue,negative,positive,neutral,neutral,positive,positive
1864834726,"@bobbyscharmann please open a new issue, I think yours is not connected to this one. There was a problem with logging that would have killed processes, this was fixed in #30643. Since this issue is quite old, I will close it assuming the kill command was causing the error. Please reopen or open a new issue if the problem is not solved.",please open new issue think connected one problem logging would fixed since issue quite old close assuming kill command causing error please reopen open new issue problem,issue,negative,positive,neutral,neutral,positive,positive
1864825951,@userbot2000 does this still occur? Can one of the reporters provide a complete reproducer so I can try to figure out what is going wrong?,still occur one provide complete reproducer try figure going wrong,issue,negative,negative,negative,negative,negative,negative
1864822352,"Closing, the dashboard now works on windows. Please reopen if the problem still occurs.",dashboard work please reopen problem still,issue,negative,neutral,neutral,neutral,neutral,neutral
1864817004,The issue author has not responded for two weeks. Closing. @xiezhipeng-git if this is still a problem please reopen or open a new issue.,issue author two still problem please reopen open new issue,issue,negative,positive,neutral,neutral,positive,positive
1864803611,"The requirement `gymnasium[atari,mujoco]==0.26.3` no longer appears in python/requirements. The additional requirements in `python/requirements/ml/rllib-test-requirements.txt` and `python/requirements_compiled.txt` are identical to the one in `python/setup.py`.

Closing, please reopen or open a new issue if there is still a problem.",requirement gymnasium longer additional identical one please reopen open new issue still problem,issue,negative,positive,neutral,neutral,positive,positive
1864792811,"I am intrigued by the `The version of `grpcio` doesn't follow Ray's requirement` in the log. How exactly are you installing the software stack? What is pulling in a non-compliant version of `grpcio`? Perhaps you are seeing some stall due to some other package, like NumPy, that tries to open worker threads on every core?",version follow ray requirement log exactly stack version perhaps seeing stall due package like open worker every core,issue,negative,positive,neutral,neutral,positive,positive
1864791834,@jjyao Can I do anything else to be able to merge this pull request ?,anything else able merge pull request,issue,negative,positive,positive,positive,positive,positive
1864307345,"True. I think another possibility is to set 

```python
self.lookback = lookback if lookback <= len(self.data) else len(self.data)
```
in `BufferWIthInifniteLookback.init()`. However, then we have to take care of this relation when setting the extra model outputs in the `SingleAgentEpisode` as these get defined via the `defaultdict`.",true think another possibility set python else however take care relation setting extra model get defined via,issue,positive,positive,positive,positive,positive,positive
1864233197,"I have a better understanding of what's happening: actually, the get_status() method returns the correct status, but not the get_metadata() method. This seems to occur only for the 'RESUMABLE' status. I have added a small bug reproduction script to the ticket to facilitate its reproduction and correction.",better understanding happening actually method correct status method occur status added small bug reproduction script ticket facilitate reproduction correction,issue,negative,positive,neutral,neutral,positive,positive
1864163442,"Hey @anyscalesam, I'm excited to see this completed!

Out of curiosity, would you mind linking the commit that implements it? I'm curious to know how you've handled this :eyes: ",hey excited see curiosity would mind linking commit curious know handled,issue,positive,positive,positive,positive,positive,positive
1864139302,"I'd like to undertake this issue please.
",like undertake issue please,issue,positive,neutral,neutral,neutral,neutral,neutral
1863914557,"In testing this, I realized that we don't currently have buttons that can filter for these libraries. With the way that we currently have the example gallery page, we can't actually filter on these tags unless we provide buttons for them. Is that something we want to do here @angelinalg?",testing currently button filter way currently example gallery page ca actually filter unless provide button something want,issue,negative,neutral,neutral,neutral,neutral,neutral
1863798548,"Hi, I'm not sure if this is fixed or was just closed due to inactivity. I'm also running in WSL2. 

My code: 
```python
>>> import ray                                                                                                                                                                                                             
>>> ray.init()                                                                                                                                                                                                             2023-12-20 03:25:04,767 INFO worker.py:1673 -- Started a local Ray instance.                                                                                                                                               
RayContext(dashboard_url='', python_version='3.10.12', ray_version='2.8.1', 
ray_commit='82a8df138fe7fcc5c42536ebf26e8c3665704fee', protocol_version=None)                                                                  
>>> ray.get_gpu_ids()                                                                                                                                                                                                      
[]                  
```

Your test script: 
```python
import ray
@ray.remote(num_gpus=1)
def f():
    import os
    print(os.environ.get(""CUDA_VISIBLE_DEVICES""))
print(ray.get(f.remote()))
```

Result of test script: 
```python
2023-12-20 03:30:24,028 INFO worker.py:1673 -- Started a local Ray instance.                                                                                                                                               
(autoscaler +6s) Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.                                                                                             
(autoscaler +6s) Error: No available node types can fulfill resource request {'CPU': 1.0, 'GPU': 1.0}. Add suitable node types to 
this cluster to resolve this issue.                                                      
(autoscaler +41s) Error: No available node types can fulfill resource request {'CPU': 1.0, 'GPU': 1.0}. Add suitable node types to this cluster to resolve this issue.                                                    
(autoscaler +1m16s) Error: No available node types can fulfill resource request {'CPU': 1.0, 'GPU': 1.0}. Add suitable node types to this cluster to resolve this issue.      
```

Nvidia-smi
```bash
Wed Dec 20 03:37:22 2023                                                                                                                                                                                                   +---------------------------------------------------------------------------------------+                                                                                                                                  
| NVIDIA-SMI 545.23.06              Driver Version: 545.92       CUDA Version: 12.3     |                                                                                                                                  
|-----------------------------------------+----------------------+----------------------+                                                                                                                                  
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |                                                                                                                                  
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |                                                                                                                                  
|                                         |                      |               MIG M. |                                                                                                                                  
|=========================================+======================+================
======|                                                                                                                                  |   0  NVIDIA GeForce RTX 3090        
On  | 00000000:0A:00.0  On |                  N/A |                                                                                                                                  |  
0%   43C    P8              35W / 350W |   1202MiB / 24576MiB |      5%      Default |                                                                                                                                  
|                                         |                      |                  N/A |                                                                                                                                  
+-----------------------------------------+----------------------+----------------------+                                                                                                                                  
|   1  NVIDIA GeForce RTX 3090        On  | 00000000:0B:00.0 Off |                  N/A |                                                                                                                                  
|  0%   30C    P8               8W / 350W |     47MiB / 24576MiB |      0%      Default |                                                                                                                                  
|                                         |                      |                  N/A |                                                                                                                                  
+-----------------------------------------+----------------------+----------------------+                                                                                                                                                                                                                                                                                                                                                             
+---------------------------------------------------------------------------------------+                                                                                                                                  
| Processes:                                                                            |                                                                                                                                  
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |                                                                                                                                  
|        ID   ID                                                             Usage      |                                                                                                                                  
|=================================================================================
======|                                                                                                                                  |    0   N/A  N/A        20      G   
/Xwayland                                 N/A      |                                                                                                                                  |    0   
N/A  N/A        20      G   /Xwayland                                 N/A      |                                                                                                                                  
|    0   N/A  N/A        34      G   /Xwayland                                 N/A      |                                                                                                                                  
|    1   N/A  N/A        20      G   /Xwayland                                 N/A      |                                                                                                                                  
|    1   N/A  N/A        20      G   /Xwayland                                 N/A      |                                                                                                                                  
|    1   N/A  N/A        34      G   /Xwayland                                 N/A      |                                                                                                                                  
+---------------------------------------------------------------------------------------+ 
```",hi sure fixed closed due inactivity also running code python import ray local ray instance test script python import ray import o print print result test script python local ray instance tip use ray status view detailed cluster status disable set error available node fulfill resource request add suitable node cluster resolve issue error available node fulfill resource request add suitable node cluster resolve issue error available node fulfill resource request add suitable node cluster resolve issue bash wed driver version version name volatile fan temp compute mig mib mib default mib mib default type process name memory id id usage,issue,positive,positive,positive,positive,positive,positive
1863734795,"> https://discuss.ray.io/
> 
> Sign up for Slack access here: https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform

thanks",sign slack access thanks,issue,negative,positive,positive,positive,positive,positive
1863680734,"Hmm, the hover link colors are nearly indistinguishable from standard ray blue, so it will be really hard to perceive any kind of hover color change. Let's revisit the color changes in a separate PR.",hover link color nearly indistinguishable standard ray blue really hard perceive kind hover color change let revisit color separate,issue,negative,positive,neutral,neutral,positive,positive
1863675140,"> I got pip_download_test.sh to work on windows with the following commands. the cpp wheels don't support windows so you'll need to comment out the cpp tests.

Seeing the following errors when I try to run it:

```
C:\tools\Anaconda3/etc/profile.d/conda.sh: line 9: /cygdrive/c/tools/Anaconda3/Scripts/conda.exe: No such file or directory
``` 

and 

```
ERROR: To modify pip, please run the following command:
C:\tools\Anaconda3\python.exe -m pip install --upgrade pip
```

It also prints things like 

```
=========================================================
Python version.
Python 3.11.5
This should be equal to 3.9
=========================================================
```

I guess because none of the conda commands are working.

",got work following support need comment seeing following try run line file directory error modify pip please run following command pip install upgrade pip also like python version python equal guess none working,issue,positive,neutral,neutral,neutral,neutral,neutral
1863642661,"Sync'd with @simran. Instead of creating a new category called `Talk`, the solution is to replace the term `Video` with `Talk`, that can apply to both categories. This makes sense to me. Barring any other issues, I'm good with this solution.",sync instead new category talk solution replace term video talk apply sense barring good solution,issue,positive,positive,positive,positive,positive,positive
1863633291,"hey @Lvjinhong glad to see you joining the community; these sort of questions are best discussed in discourse and/or in our Slack. 

Can you ask the same question there and I can route to the right folks at Anyscale who can address your question (you'll get much better response from community members as well there).",hey glad see joining community sort best discourse slack ask question route right address question get much better response community well,issue,positive,positive,positive,positive,positive,positive
1863584818,"To use Ray Data for ML training:

- Create a Dataset from various sources, such as files, databases, or in-memory objects.
- Apply functional transformations to the Dataset, such as map, filter, batch, sort, join, and other operations.
- Use streaming_split() to split the Dataset into n subsets that can be consumed in parallel by different workers or processes.
- Use DataIterator to read the data in batches from each subset, and feed the data to your ML model for training.
- Write the trained model to a file or a database, or deploy it to a serving endpoint.",use ray data training create various apply functional map filter batch sort join use split parallel different use read data subset feed data model training write trained model file deploy serving,issue,positive,neutral,neutral,neutral,neutral,neutral
1863550811,@anyscalesam Hi it seems like you're marking lots of issues as completed. Have they been? Could you share more info?,hi like marking lot could share,issue,positive,neutral,neutral,neutral,neutral,neutral
1863529014,Revewed with @c21  < this is still relevant but not currently opened. Want to continue to keep on our backlog.,still relevant currently want continue keep backlog,issue,negative,positive,positive,positive,positive,positive
1863527548,I am not sure why this is marked as closed when the issue has not been resolved?,sure marked closed issue resolved,issue,negative,positive,positive,positive,positive,positive
1863340987,"> leave it up to the users to know what they are doing

I think the only users are you and maybe me..",leave know think maybe,issue,negative,neutral,neutral,neutral,neutral,neutral
1863330421,it is not an inherent conflict though. docker has no problem with running multiple containers on the same host network. it is just that ray cannot run on the same localhost network as multiple ray clusters.,inherent conflict though docker problem running multiple host network ray run network multiple ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1863321952,"> Will this cause issues when multiple docker runs in parallel?

yes, it will in cases where they all want to start ray clusters. they will run into port conflicts.

this is required for running the kuberay operator test, otherwise the test cannot connect to the kind cluster.",cause multiple docker parallel yes want start ray run port running operator test otherwise test connect kind cluster,issue,positive,positive,positive,positive,positive,positive
1863308647,Will this cause issues when multiple docker runs in parallel?,cause multiple docker parallel,issue,negative,neutral,neutral,neutral,neutral,neutral
1863258691,"After a discussion on slack for clarification:

- [x] Remove link hover underline text decoration for all links sitewide
- [ ] Hovered links should have a different color: light mode `#1D7AFC`, dark mode `#388BFF`
- [x] Set the sidebar to a fixed width of 280px",discussion slack clarification remove link hover underline text decoration link link different color light mode dark mode set fixed width,issue,negative,positive,neutral,neutral,positive,positive
1863239121,"Thanks @peytondmurray . 
1. When I hover over the sidebar, can we remove the underline? 
2. Can we change --pst-color-border to #2C333A?
3. On some places, the width of the primary nav exceeds it's original width. 
<img width=""1722"" alt=""Screenshot 2023-12-19 at 9 53 00 AM"" src=""https://github.com/ray-project/ray/assets/23429473/b7cb2712-39f3-4a54-8e1e-2c4c8285b48a"">
4. Is it possible to increase the spacing between the links in the left nav? Maybe increase the padding to 0.40em (top and bottom)?",thanks hover remove underline change ca width primary original width possible increase spacing link left maybe increase padding top bottom,issue,positive,positive,positive,positive,positive,positive
1863196684,"I think that's a correct fix. Keep in mind that the `len_lookback_buffer` in the constructor should only be a suggestion, meaning if the data (provided also in the constructor) is not even as long as `len_lookback_buffer`, then we should simply create all `InfiniteLookbackBuffer` instance inside SingleAgentEpisode with the smaller number.",think correct fix keep mind constructor suggestion meaning data provided also constructor even long simply create instance inside smaller number,issue,negative,negative,neutral,neutral,negative,negative
1863105544,"We are seeing the same error as @seydar , but we are running in a docker container with the correct grpcio versions. It seems like an issues with launching the dashboard, but we install ray without extras, and pass `include_dashboard=False` too as a stop gap. This is only happening to us after upgrading 2.7.1 -> 2.8.1",seeing error running docker container correct like dashboard install ray without pas stop gap happening u,issue,negative,neutral,neutral,neutral,neutral,neutral
1862984742,So far I cannot reproduce any of these errors on my Linux platform. Tomorrow I can hopefully tell more. ,far reproduce platform tomorrow hopefully tell,issue,negative,positive,neutral,neutral,positive,positive
1862900337,"Hello, are there any updates on this issue? I get the same error with PPO and cannot figure this out. In my case it's an action space gym.spaces.Discrete(8) and always the training stops due to this error:

tensorflow.python.framework.errors_impl.InvalidArgumentError: Received a label value of 8 which is outside the valid range of [0, 8).  Label values: 8
         [[{{node default_policy_wk1/SparseSoftmaxCrossEntropyWithLogits_18/SparseSoftmaxCrossEntropyWithLogits}}]]",hello issue get error figure case action space always training due error received label value outside valid range label node,issue,negative,negative,neutral,neutral,negative,negative
1862631003,Let me set up a test cluster to reproduce and fix. I will submit a PR after fixing it. ,let set test cluster reproduce fix submit fixing,issue,negative,neutral,neutral,neutral,neutral,neutral
1862512733,"@sven1977 In regard to the error that occurred always in the `validate()` method I adjusted the `__len__` of the buffer to use the `max(_, 0)`. Imo this is a good solution as the length should never consider the lookback (e.g. in comparisons between different data) anyway - and especially not, if the data length is smaller than the lookback.  ",regard error always validate method buffer use good solution length never consider different data anyway especially data length smaller,issue,positive,positive,positive,positive,positive,positive
1862283263,"Hello @jjyao, thank you again for getting back. I followed the steps you shared above to cancel a running Ray task and got the below error:

**My latest code:**

<img width=""623"" alt=""Screenshot 2023-12-19 at 11 02 03"" src=""https://github.com/ray-project/ray/assets/36266903/73aef34f-d5f2-4df9-b473-7bfb2facff70"">

<img width=""874"" alt=""Screenshot 2023-12-19 at 11 02 18"" src=""https://github.com/ray-project/ray/assets/36266903/16d0ec15-3509-4cdd-bcbe-4258a60a3fa8"">



The error returned as result of cancelling ray running task:

**Error returned:**

Failed to look up actor with name 'some_unique_key'. This could because 1. You are trying to look up a named actor you didn't create. 2. The named actor died. 3. You did not use a namespace matching the namespace of the actor.
       
I would appreciate to look at the above and I am looking forward for your response. Thank you",hello thank getting back cancel running ray task got error latest code error returned result ray running task error returned look actor name could trying look actor create actor use matching actor would appreciate look looking forward response thank,issue,negative,positive,positive,positive,positive,positive
1862255402,"@irshadcc Hi, Ray on IPV6 network is a new and interesting scenario for us. I can fix this point to adopt ipv6 address, but i'm not sure if there are other block point because we don't have ipv6 network to verify it. Do you have any thought of this? Can you verify it quickly by the Ray source code after we commit?",hi ray network new interesting scenario u fix point adopt address sure block point network verify thought verify quickly ray source code commit,issue,positive,positive,positive,positive,positive,positive
1862125260,"- Updated naming:
  - Map the `cluster.yaml` as AutoscalingConfig for the entire config groups
  - Extract the needed info for scheduling (node type configs and max number of worker nodes)",naming map entire extract node type number worker,issue,negative,neutral,neutral,neutral,neutral,neutral
1861941300,"> This allows error handling to avoid race conditions, while still preserving exception messages in most cases.

What's missing to support ""all errors"" in this case?",error handling avoid race still exception missing support case,issue,negative,negative,negative,negative,negative,negative
1861934786,"Hey @Han-taz, the `HuggingFaceTrainer` is deprecated. Could you take a look at this [Get Started with Hugging Face Transformers](https://docs.ray.io/en/releases-2.8.1/train/getting-started-transformers.html) guide and see if it solves the problem for you?",hey could take look get hugging face guide see problem,issue,negative,neutral,neutral,neutral,neutral,neutral
1861926462,@fostiropoulos did you have chance to check the REP and try the prototype?,chance check rep try prototype,issue,negative,neutral,neutral,neutral,neutral,neutral
1861925916,@thatcort @martystack @achordia20 have you guys have chance to take a look in the REP and try the prototype?,chance take look rep try prototype,issue,negative,neutral,neutral,neutral,neutral,neutral
1861925148,"Thanks -- any hints as to why it works with 1 worker but not more than 1 worker, then?",thanks work worker worker,issue,negative,positive,positive,positive,positive,positive
1861922895,"Looks like the issue is because an error happens on one/some of the training workers. Then the data consumption cannot proceed, thus this hanging issue happens. 
Closing this issue for now. as it should be an issue on the app side, not something that needs to be fixed in Ray Data. ",like issue error training data consumption proceed thus hanging issue issue issue side something need fixed ray data,issue,negative,positive,neutral,neutral,positive,positive
1861916537,"@spinezhang your script works well for me with latest Ray + PyArrow 14.0.1. It may have been fixed. Closing this issue. If you still see it with the latest Ray, feel free to reopen.",script work well latest ray may fixed issue still see latest ray feel free reopen,issue,positive,positive,positive,positive,positive,positive
1861914746,"confirmed it's still an issue on the master with pyarrow 14.0.1
Could be a bug of data concatenation of read tasks.

```
ray.exceptions.RayTaskError(ArrowTypeError): ray::ReadParquet() (pid=69972, ip=127.0.0.1)
    for b_out in map_transformer.apply_transform(iter(blocks), ctx):
  File ""/Users/chenh/code/ray/python/ray/data/_internal/execution/operators/map_transformer.py"", line 377, in __call__
    yield output_buffer.next()
  File ""/Users/chenh/code/ray/python/ray/data/_internal/output_buffer.py"", line 73, in next
    block_to_yield = self._buffer.build()
  File ""/Users/chenh/code/ray/python/ray/data/_internal/delegating_block_builder.py"", line 64, in build
    return self._builder.build()
  File ""/Users/chenh/code/ray/python/ray/data/_internal/table_block.py"", line 123, in build
    return self._concat_tables(tables)
  File ""/Users/chenh/code/ray/python/ray/data/_internal/arrow_block.py"", line 148, in _concat_tables
    return transform_pyarrow.concat(tables)
  File ""/Users/chenh/code/ray/python/ray/data/_internal/arrow_ops/transform_pyarrow.py"", line 249, in concat
    table = pyarrow.Table.from_arrays(cols, schema=schema)
  File ""pyarrow/table.pxi"", line 3969, in pyarrow.lib.Table.from_arrays
  File ""pyarrow/table.pxi"", line 1463, in pyarrow.lib._sanitize_arrays
  File ""pyarrow/array.pxi"", line 371, in pyarrow.lib.asarray
  File ""pyarrow/table.pxi"", line 565, in pyarrow.lib.ChunkedArray.cast
  File ""/Users/chenh/.pyenv/versions/3.10.11/lib/python3.10/site-packages/pyarrow/compute.py"", line 404, in cast
    return call_function(""cast"", [arr], options, memory_pool)
  File ""pyarrow/_compute.pyx"", line 590, in pyarrow._compute.call_function
  File ""pyarrow/_compute.pyx"", line 385, in pyarrow._compute.Function.call
  File ""pyarrow/error.pxi"", line 154, in pyarrow.lib.pyarrow_internal_check_status
  File ""pyarrow/error.pxi"", line 91, in pyarrow.lib.check_status
pyarrow.lib.ArrowTypeError: Casting from 'extension<ray.data.arrow_variable_shaped_tensor<ArrowVariableShapedTensorType>>' to different extension type 'extension<ray.data.arrow_tensor<ArrowTensorType>>' not permitted. One can first cast to the storage type, then to the extension type.
```",confirmed still issue master could bug data concatenation read ray iter file line yield file line next file line build return file line build return table file line return table file line table file line file line file line file line file line cast return cast file line file line file line file line casting different extension type permitted one first cast storage type extension type,issue,negative,positive,positive,positive,positive,positive
1861899717,"Everything is passing except for one test (another one, but also yellow in go/flaky) again. Let's merge this and continue to monitor in postmerge. We'll need to continue to adjust.",everything passing except one test another one also yellow let merge continue monitor need continue adjust,issue,negative,neutral,neutral,neutral,neutral,neutral
1861880753,"Backend changes addressed:
- memory profiler manager unit test testing failure cases and attach multiple times
- add docstring to `profile_manager.py` and `reporter_head.py`
- `/memory_profile` in reporter_head raise Error if ip not exist + test
- include flags native and trace_python_allocator to `Memory Profiling` button
- combine task and worker api into one `/memory_profile` in `reporter_head.py`
- test task when missing params (node_id or attempt_number)
- better error message from `profile_manager.py`
",memory profiler manager unit test testing failure attach multiple time add raise error exist test include native memory button combine task worker one test task missing better error message,issue,negative,negative,neutral,neutral,negative,negative
1861873975,"Result:

`
[{'perf_metric_name': 'time_to_broadcast_1073741824_bytes_to_50_nodes', 'perf_metric_value': 84.94552126900004, 'perf_metric_type': 'LATENCY'}]
`

Previous:

<img width=""1212"" alt=""image"" src=""https://github.com/ray-project/ray/assets/56065503/dee4ea0f-b3ae-44d4-a762-3e2ad198b0be"">


so no visible perf gains.",result previous image visible gain,issue,positive,negative,negative,negative,negative,negative
1861840807,"> is this going to be in 2.9?

Unfortunately I think won't make 2.9 cut :( going to try",going unfortunately think wo make cut going try,issue,negative,negative,negative,negative,negative,negative
1861831139,"Working on updating unit tests, etc, but this is ready for review.",working unit ready review,issue,negative,positive,positive,positive,positive,positive
1861824000,@anyscalesam Still the same as the PR description. I added the size label. ,still description added size label,issue,negative,neutral,neutral,neutral,neutral,neutral
1861814811,"but we should make the repo osx/windows friendly anyways, so nice find.",make friendly anyways nice find,issue,positive,positive,positive,positive,positive,positive
1861813222,"@chrisn-pik 

Seems you already successfully reduced object store memory to `95.37MB` instead of 30% of available memory. Could you use `ps` or `top` to see what processes use `5.17GB` of memory?",already successfully reduced object store memory instead available memory could use top see use memory,issue,positive,positive,positive,positive,positive,positive
1861809481,"> Can you update the PR description to include the issue link for the failing release test?

it's already there",update description include issue link failing release test already,issue,negative,neutral,neutral,neutral,neutral,neutral
1861795349,"hi @anyscalesam, serve team will revamp the release tests as a whole. 

Couple of key things in my mind:
1. metrics collection is not guaranteed to be functioning since ray cluster nodes can be terminated by autoscaling.
2. release tests perf are not trackable.
3. Need to re-categorize the release tests. (e.g. gRPC release tests)
4. Use anyscale service instead of python api in release tests.

cc: @akshay-anyscale for planning.",hi serve team revamp release whole couple key mind metric collection since ray cluster release trackable need release release use service instead python release,issue,negative,positive,neutral,neutral,positive,positive
1861790595,"fwiw, git tag itself is not always case sensitive. it depends on the filesystem's filename implementation.",git tag always case sensitive implementation,issue,negative,positive,neutral,neutral,positive,positive
1861779923,currently this is unplanned but we may consider this in the future! closing this for now,currently unplanned may consider future,issue,negative,neutral,neutral,neutral,neutral,neutral
1861779229,"Hi @MissiontoMars,

Sorry currently there is no way to completely avoid scheduling on head node and some actors are scheduled on head node on purpose (e.g. to fate share with GCS). Normally zero-cpu actor should be very lightweight so running on head node should be fine. 

What's your use case where running on head node is not acceptable?",hi sorry currently way completely avoid head node head node purpose fate share normally actor lightweight running head node fine use case running head node acceptable,issue,negative,positive,neutral,neutral,positive,positive
1861762688,"Ran `actors_per_second` again (https://buildkite.com/ray-project/release/builds/4506#018c7e60-0aed-48e3-9eed-825cbbc5566e): 

```
actors_per_second = 614.2315272090922
```

Still slower than master. There might be a real regression in release branch.",ran still master might real regression release branch,issue,negative,positive,positive,positive,positive,positive
1861753634,"> FYI, we are still awaiting one last cherry-pick PR: #41990
> 
> @raulchen @rickyyx @jjyao do you think we should rerun these performance metrics after that PR is picked?

I submitted another PR to fix the issue instead https://github.com/ray-project/ray/pull/42000. This PR only touches data code. Shouldn't impact the core metrics you listed. ",still one last think rerun performance metric picked another fix issue instead data code impact core metric listed,issue,negative,neutral,neutral,neutral,neutral,neutral
1861726796,"@rickyyx all test passed, except for one `windows://python/ray/tests:test_task_metrics` which has 7% of yellowness on go/flaky. Are you ok with giving this the benefit of the doubt to merge this as it is and see how the test behave in postmerge. Thankkks

<img width=""335"" alt=""Screenshot 2023-12-18 at 1 41 13 PM"" src=""https://github.com/ray-project/ray/assets/128072568/f343047b-30fa-4f8c-b603-8f7d58f752f4"">
",test except one yellowness giving benefit doubt merge see test behave,issue,negative,neutral,neutral,neutral,neutral,neutral
1861704534,"Memory pressure looks much more stable in the release test run, but looks like it's still failing with OutOfDiskError. Updated release test to add more disk.",memory pressure much stable release test run like still failing release test add disk,issue,negative,positive,positive,positive,positive,positive
1861625627,Should we run a couple data release tests before cherry picking?,run couple data release cherry,issue,negative,neutral,neutral,neutral,neutral,neutral
1861507932,"@rickyyx good point, let me clone the yml blob",good point let clone blob,issue,negative,positive,positive,positive,positive,positive
1861502633,"> @rickyyx good point, let me maybe run it 5 times on this PR and let postmerge run takes care of the rest; main reason for 5 vs. 10 is because of my own dev velocity and cost (will take 1 hour each to re-run the suite)

Sure - Could we hack the pipeline yaml a bit so that it automatically runs X times? e.g. random thoughts, like what if we duplicate the steps 10 times with wait in between the steps? ",good point let maybe run time let run care rest main reason dev velocity cost take hour suite sure could hack pipeline bit automatically time random like duplicate time wait,issue,positive,positive,positive,positive,positive,positive
1861497102,"> @rickyyx Gotcha, thanks!
> 
> Thanks for the details about the regressions! Is the conclusion that there's no release-blocking regression? If so you can approve this PR (we need two independent approvals to proceed with the release)

@jjyao and I looked through them together, and we think there are 2 we wanted to run to verify if they are variance merely in the release branch. Will update once that's cleared. ",thanks thanks conclusion regression approve need two independent proceed release together think run verify variance merely release branch update,issue,positive,negative,neutral,neutral,negative,negative
1861480548,@simran-2797 will follow up with @matthewdeng on more details to ensure that we make the right implementation decisions.,follow ensure make right implementation,issue,negative,positive,positive,positive,positive,positive
1861478828,"From offline discussion, this should be refactored so that
- app-level exception -> errors returned via application level writes
- worker crash -> caught / logged by monitor and dag torn down via EOS
- teardown -> dag torn down via EOS

EOS will be implemented as an ""error bit"" that can be set on the channel without locking. This allows error handling to avoid race conditions, while still preserving exception messages in most cases.",discussion exception returned via application level worker crash caught logged monitor dag torn via teardown dag torn via error bit set channel without locking error handling avoid race still exception,issue,negative,neutral,neutral,neutral,neutral,neutral
1861472435,"@rickyyx good point, let me maybe run it 5 times on this PR and let postmerge run takes care of the rest; main reason for 5 vs. 10 is because of my own dev velocity and cost (will take 1 hour each to re-run the suite)",good point let maybe run time let run care rest main reason dev velocity cost take hour suite,issue,positive,positive,positive,positive,positive,positive
1861465742,"@rickyyx Gotcha, thanks!

Thanks for the details about the regressions!  Is the conclusion that there's no release-blocking regression? If so you can approve this PR (we need two independent approvals to proceed with the release)",thanks thanks conclusion regression approve need two independent proceed release,issue,positive,positive,positive,positive,positive,positive
1861450685,"> FYI, we are still awaiting one last cherry-pick PR: #41990
> 
> @raulchen @rickyyx @jjyao do you think we should rerun these performance metrics after that PR is picked?

Shouldn't impact core metrics I think",still one last think rerun performance metric picked impact core metric think,issue,negative,neutral,neutral,neutral,neutral,neutral
1861422127,"FYI, we are still awaiting one last cherry-pick PR: https://github.com/ray-project/ray/pull/41990/

@raulchen @rickyyx @jjyao do you think we should rerun these performance metrics after that PR is picked?",still one last think rerun performance metric picked,issue,negative,neutral,neutral,neutral,neutral,neutral
1861412021,"`single_client_tasks_sync`  also due to grpc upgrade  (the initial drop from 1.2k has been fixed in https://github.com/ray-project/ray/issues/41695) but the grpc regression wasn't fixed yet. 

<img width=""1371"" alt=""image"" src=""https://github.com/ray-project/ray/assets/11676094/1db59ef9-fc35-4e4b-9247-355901301b5e"">


`multi_client_tasks_async` same story (there are 2 drops, one fixed, another due to grpc)
<img width=""1331"" alt=""image"" src=""https://github.com/ray-project/ray/assets/11676094/8a4a00f3-686c-4d65-8203-a27b21f63c56"">

",also due upgrade initial drop fixed regression fixed yet image story one fixed another due image,issue,negative,positive,neutral,neutral,positive,positive
1861401790,"`1_n_async_actor_calls_async ` due to GRPC upgrade at 10.31 -> no fix
<img width=""1402"" alt=""image"" src=""https://github.com/ray-project/ray/assets/11676094/65ba0dc7-5aec-484f-9bd4-9f17c2e7ee5c"">

`n_n_actor_calls_async` also grpc

<img width=""1320"" alt=""image"" src=""https://github.com/ray-project/ray/assets/11676094/08aa0d9d-64ef-4c3e-ba67-733470a642bc"">

`1_n_actor_calls_async` same
<img width=""1347"" alt=""image"" src=""https://github.com/ray-project/ray/assets/11676094/512f7041-e091-4296-8568-c5ac38a46b78"">

Same for  
- `1_1_async_actor_calls_sync` 
- `placement_group_create/removal`
- `1_1_actor_calls_sync`
- `stage_3_time`

",due upgrade fix image also image image,issue,negative,negative,negative,negative,negative,negative
1861394117,"`multi_client_put_gigabytes` is variance. 
![image](https://github.com/ray-project/ray/assets/11676094/9cdeddf2-a83f-4f06-a3ab-9549bec4145f)


`n_n_actor_calls_with_arg_async` variance
<img width=""1317"" alt=""image"" src=""https://github.com/ray-project/ray/assets/11676094/4d81da2c-822e-410b-a82d-b4026c52e82a"">


`dashboard_p99_latency_ms` variance
<img width=""1337"" alt=""image"" src=""https://github.com/ray-project/ray/assets/11676094/f5acc913-480c-40a0-9857-172bd7b2e9fe"">

`time_to_broadcast_1073741824_bytes_to_50_nodes` seems variance - rerunning. 
<img width=""1341"" alt=""image"" src=""https://github.com/ray-project/ray/assets/11676094/060d06cd-c27d-4fce-be5c-693c2e17242f"">

",variance image variance image variance image variance image,issue,negative,neutral,neutral,neutral,neutral,neutral
1861344269,Yes let's remove it as a release blocker for now,yes let remove release blocker,issue,negative,neutral,neutral,neutral,neutral,neutral
1861261603,"No longer a release blocker, it's an issue with the test and not with Ray. Dicsussed offline",longer release blocker issue test ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1861116802,"The test failed on the release branch: https://buildkite.com/ray-project/release-tests-branch/builds/2407#018c7adb-46cf-4def-9a3f-dd5077f2114e

@sihanwang41 @edoakes please remove `release-blocker` or other tags as necessary if it's not a release blocking issue",test release branch please remove necessary release blocking issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1861107556,"@harborn please ask these questions in our discourse at https://discuss.ray.io/

We are aiming for a 2.9 before the end of this year.
",please ask discourse aiming end year,issue,negative,neutral,neutral,neutral,neutral,neutral
1861103951,Closing as this is no longer planned. Please reopen if this becomes needed.,longer please reopen becomes,issue,negative,neutral,neutral,neutral,neutral,neutral
1860394508,"It's good that the issue can be worked around by coding up some port allocation logic, or retrying. 
It's pretty bad that the basic Ray start API has a random chance of failure, for known reasons.",good issue worked around port allocation logic pretty bad basic ray start random chance failure known,issue,negative,negative,neutral,neutral,negative,negative
1859938459,"> This wasn't fixed in the master. It happens because once in 100 times, the port randomly selected for agent & metrics conflict. We need to avoid choosing a random port when it is already assigned to sth else

In our application, this seems to occur with more than a 1% probability. It has happened multiple times in the past year and a simple restart cannot solve it.  We use Docker to deploy nodes, strangely there is always the same port conflict every time restarts.",fixed master time port randomly selected agent metric conflict need avoid choosing random port already assigned else application occur probability multiple time past year simple restart solve use docker deploy strangely always port conflict every time,issue,negative,negative,negative,negative,negative,negative
1859777856,"@pcmoritz I cannot find any conext regarding the performance issue. Could you help to explain that.

Besides, what's the recommend way to consume large huggingface dataset now? I'm not sure the way I do is correct
1. use `to_parquet` to save huggingface datasets as parquet format.
2. use `ray` for parallel read.",find regarding performance issue could help explain besides recommend way consume large sure way correct use save parquet format use ray parallel read,issue,positive,positive,positive,positive,positive,positive
1859735545,"other than this, we don't really have a good set of object transfer benchmark",really good set object transfer,issue,negative,positive,positive,positive,positive,positive
1859735275,"for benchmark you can run 
```
- name: object_store
  group: core-scalability-test
  working_dir: benchmarks

  frequency: nightly
  team: core
  cluster:
    byod:
      type: gpu
      runtime_env:
        - LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libjemalloc.so
    cluster_compute: object_store.yaml

  run:
    timeout: 3600
    script: python object_store/test_object_store.py
    wait_for_nodes:
      num_nodes: 50

  variations:
    - __suffix__: aws
    - __suffix__: gce
      env: gce
      frequency: manual
      cluster:
        cluster_compute: object_store_gce.yaml
```",run name group frequency nightly team core cluster type run script python frequency manual cluster,issue,negative,neutral,neutral,neutral,neutral,neutral
1859702800,"Using self-defined checkpoint callback

```python
from typing import Optional
import ray
from ray.train import SyncConfig, RunConfig, CheckpointConfig, FailureConfig, ScalingConfig, Checkpoint
from ray.train.xgboost import XGBoostTrainer
from ray.tune.integration.xgboost import TuneReportCheckpointCallback
from contextlib import contextmanager
import tempfile
import xgboost as xgb
import os


class MyXGBoostCheckpointCallback(TuneReportCheckpointCallback):
    @contextmanager
    def _get_checkpoint(
        self, model: xgb.Booster, epoch: int, filename: str, frequency: int
    ) -> Optional[Checkpoint]:
        if not frequency or epoch % frequency > 0 or (not epoch and frequency > 1):
            # Skip 0th checkpoint if frequency > 1
            yield None
            return

        with tempfile.TemporaryDirectory() as checkpoint_dir:
            if hasattr(model, 'feature_names'):
                model.set_attr(feature_names='|'.join(model.feature_names))
            model.save_model(os.path.join(checkpoint_dir, filename))
            checkpoint = Checkpoint.from_directory(checkpoint_dir)
            yield checkpoint


class MyXGBoostTrainer(XGBoostTrainer):
    # HERE
    # This is a must-have, even though we have set the callback in the trainer's callback argument.
    # In GBDTTrainer training_loop, it will check the object, if not the same it will create another one,
    # And you will dump double checkpoints unconsciously
    _tune_callback_checkpoint_cls = MyXGBoostCheckpointCallback

    @staticmethod
    def get_model(checkpoint: Checkpoint) -> xgb.Booster:
        """"""Retrieve the XGBoost model stored in this checkpoint.""""""
        with checkpoint.as_directory() as checkpoint_path:
            booster = xgb.Booster()
            booster.load_model(
                os.path.join(checkpoint_path, 'model.json')
            )
            if booster.attr('feature_names') is not None:
                booster.feature_names = booster.attr(
                    'feature_names').split('|')
            return booster

    def _save_model(self, model: xgb.Booster, path: str) -> None:
        if hasattr(model, 'feature_names'):
            model.set_attr(feature_names='|'.join(model.feature_names))
        model.save_model(os.path.join(path, 'model.json'))


dataset = ray.data.read_csv(
    ""s3://anonymous@air-example-data/breast_cancer.csv"")
train_dataset, valid_dataset = dataset.train_test_split(test_size=0.3)
sync_config = SyncConfig(sync_artifacts=True)
run_config = RunConfig(
    name=f""XGBoost_Test_Checkpoint_Save_Load"",
    storage_path=""/NAS/ShareFolder/ray_debug"",
    checkpoint_config=CheckpointConfig(
        checkpoint_frequency=1,
        num_to_keep=10,
        checkpoint_at_end=True,
        checkpoint_score_attribute='train-error',
        checkpoint_score_order='min',
    ),
    failure_config=FailureConfig(max_failures=2),
    sync_config=sync_config,
)
scaling_config = ScalingConfig(
    num_workers=3,
    placement_strategy=""SPREAD"",
    use_gpu=False,
)
trainer = MyXGBoostTrainer(
    scaling_config=scaling_config,
    run_config=run_config,
    label_column=""target"",
    num_boost_round=30,
    params={
        ""objective"": ""binary:logistic"",
        ""eval_metric"": [""logloss"", ""error""],
    },
    datasets={""train"": train_dataset, ""valid"": valid_dataset},
    # HERE
    callbacks=[MyXGBoostCheckpointCallback(
        filename=""model.json"", frequency=1)],
)
result = trainer.fit()
print(checkpoint := result.get_best_checkpoint('valid-logloss', 'min'))
booster = MyXGBoostTrainer.get_model(checkpoint)
print(booster.num_boosted_rounds())
print(booster.feature_names)
```",python import optional import ray import import import import import import import o class self model epoch frequency optional frequency epoch frequency epoch frequency skip th frequency yield none return model yield class even though set trainer argument check object create another one dump double unconsciously retrieve model booster none return booster self model path none model path spread trainer target objective binary logistic error train valid result print booster print print,issue,negative,neutral,neutral,neutral,neutral,neutral
1859658549,"Okay, I found the exact issue! The ""non-end"" checkpoint is preserved by `_tune_callback_checkpoint_cls`

https://github.com/ray-project/ray/blob/ee10ea692a0fc8863589411c39a7175aae3fdb06/python/ray/train/gbdt_trainer.py#L307-L319

https://github.com/ray-project/ray/blob/ee10ea692a0fc8863589411c39a7175aae3fdb06/python/ray/train/xgboost/xgboost_trainer.py#L84

https://github.com/ray-project/xgboost_ray/blob/9081780c5826194b780fdad4dbe6872470527cab/xgboost_ray/tune.py#L69-L76

https://github.com/ray-project/ray/blob/ee10ea692a0fc8863589411c39a7175aae3fdb06/python/ray/tune/integration/xgboost.py#L156-L175

And the filename ""model"" is been set here.

https://github.com/ray-project/ray/blob/ee10ea692a0fc8863589411c39a7175aae3fdb06/python/ray/air/constants.py#L5

---

Now I am building a workaround for this. If it is successful, I will post the solution here.

But this is kind of tricky and non-intuitive that not all models were saved by the trainer's `_save_model()` method.

@justinvyu I think this issue can be reproduced by setting CheckpointConfig for non-zero `checkpoint_frequency`.

```python
checkpoint_config=CheckpointConfig(
        checkpoint_frequency=1,
        num_to_keep=10,
        checkpoint_at_end=True,
    ),
```",found exact issue model set building successful post solution kind tricky saved trainer method think issue setting python,issue,positive,positive,positive,positive,positive,positive
1859650361,"I found only the latest iteration checkpoint is correctly called `_save_model()`.
![image](https://github.com/ray-project/ray/assets/1515662/d1ac20cb-cebc-4680-889f-e72d2fa89cc3)

Other iteration dumps are not calling the `_save_model()`.

https://github.com/ray-project/ray/blob/ee10ea692a0fc8863589411c39a7175aae3fdb06/python/ray/train/gbdt_trainer.py#L272-L288

Seems if `GBDTTrainer._checkpoint_at_end()` means the very end, then it works as expected.",found latest iteration correctly image iteration calling end work,issue,negative,positive,positive,positive,positive,positive
1859441328,"I tried to print the package version by doing this

```python
import ray
import logging

ray.init()

@ray.remote(scheduling_strategy='SPREAD')
class Actor:
    def __init__(self):
        logging.basicConfig(level=logging.INFO)

    def log(self):
        logger = logging.getLogger(__name__)
        import xgboost
        import xgboost_ray
        logger.info({
            'xgboost': xgboost.__version__,
            'xgboost_ray': xgboost_ray.__version__,
            'ray': ray.__version__,
        })


for _ in range(3):
    actor = Actor.remote()
    ray.get(actor.log.remote())
```

And get the following logs

```bash
/mnt/NAS/ShareFolder/MyRepo/MyVenv/bin/python /mnt/NAS/ShareFolder/MyRepo/ray_environment_check.py 
2023-12-18 10:19:37,829 INFO worker.py:1489 -- Connecting to existing Ray cluster at address: 192.168.222.235:6379...
2023-12-18 10:19:37,858 INFO worker.py:1664 -- Connected to Ray cluster. View the dashboard at http://192.168.222.235:8265 
(Actor pid=38713, ip=192.168.222.236) INFO:__main__:{'xgboost': '2.0.2', 'xgboost_ray': '0.1.19', 'ray': '2.8.1'}
(Actor pid=35015, ip=192.168.222.237) INFO:__main__:{'xgboost': '2.0.2', 'xgboost_ray': '0.1.19', 'ray': '2.8.1'}
(Actor pid=38897) INFO:__main__:{'xgboost': '2.0.2', 'xgboost_ray': '0.1.19', 'ray': '2.8.1'}
```

Not sure if this confirms they are using the same packages.",tried print package version python import ray import logging class actor self log self logger import import range actor get following bash ray cluster address connected ray cluster view dashboard actor actor actor sure,issue,negative,positive,positive,positive,positive,positive
1859383411,"> Q: What's your cluster setup? Are you running on multiple nodes, and is the `xgboost`/`xgboost_ray`/`ray` version the same on every node?

I have three machines. I set up the workspace (`/mnt/NAS/ShareFolder/MyRepo`) in a NAS directory which are accessible for these three machine and have mounted under the same directory structure.
In the workspace, I created a Python 3.8.13 virtual environment (`/mnt/NAS/ShareFolder/MyRepo/MyVenv`), which installed `ray==2.8.1`, `xgboost-ray==0.1.19`, `xgboost==2.0.2`. 

And I start the cluster like this

1. Start the head node on a machine

```bash
# launch_ray_head_node.sh
RAY_record_ref_creation_sites=1 RAY_PROMETHEUS_HOST=http://192.168.222.235:9000 RAY_GRAFANA_HOST=http://192.168.222.235:3000 RAY_scheduler_spread_threshold=0.0 /mnt/NAS/ShareFolder/MyRepo/MyVenv/bin/ray start --head --node-ip-address 192.168.222.235 --port 6379 --dashboard-host 0.0.0.0 --dashboard-port 8265 --object-store-memory 450000000000
```

2. Start the other two machines

```bash
# launch_ray_worker_node.sh
RAY_record_ref_creation_sites=1 RAY_scheduler_spread_threshold=0.0 /mnt/NAS/ShareFolder/MyRepo/MyVenv/bin/ray --address 192.168.222.235:6379 --object-store-memory 450000000000
```

3. Start the training 

```bash
/mnt/NAS/ShareFolder/MyRepo/MyVenv/bin/python -m trainer.ray_training
```

In this script, I have a RunConfig like this, which directs the checkpoint to the NAS share folder

```python
run_config = RunConfig(
    name=""ExperimentName"",
    storage_path=""/mnt/NAS/ShareFolder/MyRepo/Results"",
    ...
)
```

If the Ray version is inconsistent, it will raise an error at the cluster starting phase, but I am not sure if it will warn for other packages.",cluster setup running multiple ray version every node three set directory accessible three machine mounted directory structure python virtual environment start cluster like start head node machine bash start head port start two bash address start training bash script like share folder python ray version inconsistent raise error cluster starting phase sure warn,issue,positive,positive,positive,positive,positive,positive
1859253051,"- Added state machine transition comments 
- Added InvalidInstanceStatusError for invalid instance status instead of generic ValueError
- Rename get_status_times_ns to better match the behaviour ",added state machine transition added invalid instance status instead generic rename better match behaviour,issue,negative,positive,positive,positive,positive,positive
1859219524,@jjyao  I tried with single quote and it still needs **strip** https://buildkite.com/ray-project/premerge/builds/15005 I am reverting back my changes,tried single quote still need strip back,issue,negative,negative,neutral,neutral,negative,negative
1859208410,"Hello,
Sorry i have not been working on this project for a couple weeks
here are the commands i am using so that you guys can repo this

on headnode: 
ray start --head --port=6379 --node-ip-address=192.168.1.210 --temp-dir=""C:\Users\Administrator\AppData\Local\Temp\2""

on worker node:
ray start --address=192.168.1.210:6379

then on the server node i run our ray tune script with this configuration for storage

`run_config=train.RunConfig(
            name=""REMOTETRY"",
            storage_path=""F:\\NFS\\"",
        ),`

that storage path is the NFS shared path from the head node where we have 58 TB of storage 
![image](https://github.com/ray-project/ray/assets/17682552/7fbd218f-5c97-47d5-a208-18403c04f3e4)

currently there are 65 terminated trials and the worker node as well as the headnode has over 160 tmp folders still with models filling up the space as the worker node only has 256GB of storage






 ",hello sorry working project couple ray start head worker node ray start server node run ray tune script configuration storage storage path path head node storage image currently worker node well still filling space worker node storage,issue,negative,negative,negative,negative,negative,negative
1859149340,"As you mentioned I have ran the above script on top of head node, getting the same error.",ran script top head node getting error,issue,negative,positive,positive,positive,positive,positive
1858975658,"There may be a good enough workaround here by calling Dataset.filter() twice, possibly P2 instead.",may good enough calling twice possibly instead,issue,negative,positive,positive,positive,positive,positive
1858749764,"Thank you for your reply! I am glad to hear that you have solved the problem successfully. If possible, I'd love to take a look at your branch code to better understand your solution. I would appreciate it if you could share the branch code of your work.",thank reply glad hear problem successfully possible love take look branch code better understand solution would appreciate could share branch code work,issue,positive,positive,positive,positive,positive,positive
1858739054,"I was able to fix it. Unfortunately, this fundamentally altered the Samplebatch class, as data is no longer homogeneous with there being arrays of differently sized arrays. If it helps, I can give the branch that works for me, although its no longer updated.",able fix unfortunately fundamentally class data longer homogeneous differently sized give branch work although longer,issue,negative,neutral,neutral,neutral,neutral,neutral
1858705508,Looks related to #41036 Are you able to try nightly? ,related able try nightly,issue,negative,positive,positive,positive,positive,positive
1858700104,"> The following CI failure looks relevant, @JingChen23 would you mind taking a look?
> 
> https://buildkite.com/ray-project/oss-ci-build-pr/builds/44524#018c67b9-2228-47d2-a0d6-901c18aca407/7568-7591
> 
> ```
> 
> (02:50:22) ERROR: /ray/python/ray/tests/BUILD:546:8: in py_test rule //python/ray/tests:vsphere/test_vsphere_sdk_provider.py: cycle in dependency graph:
> --
>   | .-> //python/ray/tests:vsphere/test_vsphere_sdk_provider.py (55ad732f6975b235948cd7d463da0b2ea456542d7b13aab0a1b46252497a25f2) [self-edge]
>   | `--
>   | (02:50:22) WARNING: errors encountered while analyzing target '//python/ray/tests:vsphere/test_vsphere_sdk_provider.py': it will not be built
> ```

Sure, I will handle this.",following failure relevant would mind taking look error rule cycle dependency graph warning target built sure handle,issue,negative,positive,positive,positive,positive,positive
1858689276,Reopening the issue as we are actively discussing the regression for this release test. ,issue actively regression release test,issue,negative,negative,negative,negative,negative,negative
1858666809,"@aljeshishe Thanks for raising this issue, I have repro'd the error on my end, and I'm still investigating. A quick workaround for now would be to set `tune.run(..., reuse_actors=False)`. Let me know if that also works for you!",thanks raising issue error end still investigating quick would set let know also work,issue,negative,positive,positive,positive,positive,positive
1858658087,@zhe-thoughts yes. I sent a group chat related to this change. I think there may be a way to avoid picking this. Currently discussing with shomil and tanmay,yes sent group chat related change think may way avoid currently,issue,negative,neutral,neutral,neutral,neutral,neutral
1858644889,I think some of tests that pass 100% of time don't come up,think pas time come,issue,negative,neutral,neutral,neutral,neutral,neutral
1858640661,Need to give you more free time ;)),need give free time,issue,positive,positive,positive,positive,positive,positive
1858639690,It's passing because I fixed it in my free time lol,passing fixed free time,issue,positive,positive,positive,positive,positive,positive
1858636704,For e2e testing: repro unsuccessful when manually triggering failover. Leaving this service up to catch the bug in the wild: https://console.anyscale.com/services/service2_2rmwzj42bh4142kv8trb7fvide,testing unsuccessful manually leaving service catch bug wild,issue,negative,positive,neutral,neutral,positive,positive
1858633318,They should show up on go/flaky once I fix it in Jan. Release test results are broken on go/flaky.,show fix release test broken,issue,negative,negative,negative,negative,negative,negative
1858621416,@anyscalesam look like the test still run in buildkite and stable https://buildkite.com/ray-project/postmerge/builds/2197#018c6f79-6329-46e9-b430-7cbe6a1a28d2/6-267; to find it on buildkite you just need to know its job; i just happen to know this is a core cpp tests.,look like test still run stable find need know job happen know core,issue,positive,neutral,neutral,neutral,neutral,neutral
1858618961,@can-anyscale I think you're already tracking this but another example where a quick search on go/flaky brings up no recent runs of this test. How does one verify that the test is still passing?,think already another example quick search recent test one verify test still passing,issue,negative,positive,positive,positive,positive,positive
1858616481,"@can-anyscale im doing a search for ""gcs_placement_group_manager_test"" but nothing is coming up in go/flaky. is this test not in rotation anymore; how can i systematically check which tests are in rotation in buildkite?",search nothing coming test rotation systematically check rotation,issue,negative,neutral,neutral,neutral,neutral,neutral
1858613528,anddd it is passing again. definitely one test that i will dig deeper into in the new year to verify reasoning of flakiness.,passing definitely one test dig new year verify reasoning flakiness,issue,negative,positive,neutral,neutral,positive,positive
1858605362,"The following CI failure looks relevant, @JingChen23 would you mind taking a look?

https://buildkite.com/ray-project/oss-ci-build-pr/builds/44524#018c67b9-2228-47d2-a0d6-901c18aca407/7568-7591
```

(02:50:22) ERROR: /ray/python/ray/tests/BUILD:546:8: in py_test rule //python/ray/tests:vsphere/test_vsphere_sdk_provider.py: cycle in dependency graph:
--
  | .-> //python/ray/tests:vsphere/test_vsphere_sdk_provider.py (55ad732f6975b235948cd7d463da0b2ea456542d7b13aab0a1b46252497a25f2) [self-edge]
  | `--
  | (02:50:22) WARNING: errors encountered while analyzing target '//python/ray/tests:vsphere/test_vsphere_sdk_provider.py': it will not be built


```",following failure relevant would mind taking look error rule cycle dependency graph warning target built,issue,negative,positive,neutral,neutral,positive,positive
1858597545,"> Can you make the default flush interval 0? It doesn't seem like a safe change to make by default otherwise, since we don't have a background thread to trigger flushes on timeout.

I was concerned about this issue as well initially. But after thinking more, this seemed fine. The behavior is the same as logging libraries. new logs are flushed by the next write or when closing. ",make default flush interval seem like safe change make default otherwise since background thread trigger concerned issue well initially thinking fine behavior logging new next write,issue,positive,positive,positive,positive,positive,positive
1858582430,"> But actually yeah it is a bit tricky if the process fails while holding the pthread_mutex in WriteAcquire or WriteRelease. I think we need to rethink that concurrency mechanism...

So how about this, we can switch the error writing path of WriteAcquire to `sem_timedwait` and `pthread_mutex_timedlock` with a 10 second timeout for now?

> By the way, seems like we need something like this to support multi-node too, so that we have a way to signal that we should stop waiting for values to send to the other node. I think it'd be best if we can send a special value like ""EOF"" instead of storing an exception, so that way it works for both python and C++ readers.

Can you explain more?",actually yeah bit tricky process holding think need rethink concurrency mechanism switch error writing path second way like need something like support way signal stop waiting send node think best send special value like instead exception way work python explain,issue,positive,positive,positive,positive,positive,positive
1858550620,"> Another question: why isn't there a time for 70B. We don't support it yet?

Resolved.",another question time support yet resolved,issue,positive,neutral,neutral,neutral,neutral,neutral
1858546310,@alanwguo Do we plan to fix the texts and merge soon or just leave it here for now?,plan fix merge soon leave,issue,negative,neutral,neutral,neutral,neutral,neutral
1858519464,"@daviddwlee84 

> I haven't found how ray.train.report save the Checkpoint to the destination.

This is where the checkpoint gets persisted -- it does get copied from the temp dir to the location on persistent storage (NFS/S3). It happens during the `ray.train.report` call:

https://github.com/ray-project/ray/blob/ee10ea692a0fc8863589411c39a7175aae3fdb06/python/ray/train/_internal/session.py#L416-L417

> Did your checkpoint content change when you modify XGBoostTrainer._save_model()?

I tried this:

```python
class MyXGBoostTrainer(XGBoostTrainer):
    def _save_model(self, model, path: str):
        model.save_model(os.path.join(path, ""model.ubj""))
```

This works fine for me:

```
$ ls /home/ray/ray_results/XGBoost_ResumeExperiment/MyXGBoostTrainer_da263_00000_0_2023-12-15_13-35-25/checkpoint_000020
model.ubj
```

-----

Q: What's your cluster setup? Are you running on multiple nodes, and is the `xgboost`/`xgboost_ray`/`ray` version the same on every node?",found save destination get copied temp location persistent storage call content change modify tried python class self model path path work fine cluster setup running multiple ray version every node,issue,positive,positive,positive,positive,positive,positive
1858374513,"@gaborgsomogyi totally, and what you explained makes sense - here is the current instructions to build the wheel on linux and macos in case you find it useful https://github.com/ray-project/ray/blob/master/python/README-building-wheels.md. We're around for any questions that you might have in speeding up your workflow as well. Thankks!",totally sense current build wheel case find useful around might speeding well,issue,positive,positive,neutral,neutral,positive,positive
1858368101,"By the way, seems like we need something like this to support multi-node too, so that we have a way to signal that we should stop waiting for values to send to the other node. I think it'd be best if we can send a special value like ""EOF"" instead of storing an exception, so that way it works for both python and C++ readers.",way like need something like support way signal stop waiting send node think best send special value like instead exception way work python,issue,positive,positive,positive,positive,positive,positive
1858319354,Closing as this is confirmed fixed in Ray 2.9+ (releasing coming in the coming weeks),confirmed fixed ray coming coming,issue,negative,positive,positive,positive,positive,positive
1858158124,It’s awesome if we can use rust ray together with huggingface/candle,awesome use rust ray together,issue,positive,positive,positive,positive,positive,positive
1858157934,A long enough timeout on pthread_mutex_lock seems okay for now; we can probably improve it later.,long enough probably improve later,issue,negative,negative,neutral,neutral,negative,negative
1858156959,"> > I think there is an edge case here where an actor dies after WriteAcquire but before WriteRelease
> 
> I see, would we need a timeout here to force release of the lock?

If we know that the original writer has definitely died, it should be okay to directly write the plasma buffer with the exception object and WriteRelease.

But actually yeah it is a bit tricky if the process fails while holding the pthread_mutex in WriteAcquire or WriteRelease. I think we need to rethink that concurrency mechanism...",think edge case actor see would need force release lock know original writer definitely directly write plasma buffer exception object actually yeah bit tricky process holding think need rethink concurrency mechanism,issue,positive,positive,positive,positive,positive,positive
1858155328,@kyle-v6x this is a very interesting scenario. I wonder if you have 30 mins to have a Zoom call to discuss more.. If so please email zhz at anyscale.com. Thanks!,interesting scenario wonder zoom call discus please thanks,issue,positive,positive,positive,positive,positive,positive
1858066308,"Just a gentle ping to make this unstale. 
@ArturNiederfahrenhorst @sven1977 @avnishn Anything we could do to help facilitate things? ",gentle ping make anything could help facilitate,issue,positive,positive,positive,positive,positive,positive
1858042528,"cc: @zhe-thoughts please approve, this fix has already been merged into master. Thx!
",please approve fix already master,issue,negative,neutral,neutral,neutral,neutral,neutral
1857735772,"version 1.2.0 is too low for me, I update to 2.0.0, and no error.
python=3.7  tensorflow=2.11.0",version low update error,issue,negative,neutral,neutral,neutral,neutral,neutral
1857599958,Thanks for taking care! I'll take the advise and changing the approach.,thanks taking care take advise approach,issue,positive,positive,positive,positive,positive,positive
1857552121,"pledge to change the many jobs test to only submit 50 jobs rather than 100. I feel that it does not really need to run for 50mins to detect possible errors. (it will be run on every single commit postmerge).

probably 30 will also be enough..",pledge change many test submit rather feel really need run detect possible run every single commit probably also enough,issue,negative,positive,positive,positive,positive,positive
1857528643,"Thanks for having a look! To answer the questions. We're in a POC phase where we wanted to build everything from scratch and due to time pressure we've made a shortcut not to use cache but it's crystal clear that at the end we need caching not to wait hours for the results. Having such a cache server makes sense but to create such requires some time because of the approvals. What I can imagine as a cheap improvement is when no cache URL is provided then local file caching can be enabled (almost zero XP with bazel so correct me if wrong).

Regarding building we've tried many ways to make it work on custom build environments. I'm not saying I've rock solid condensed opinion. As a forming opinion I can say that there are some given constrains where the common working solution to build is to use `manylinux2014` docker and commands like this:
```
# Mount source code under /ray dir
cd /ray
export TRAVIS_COMMIT=$(git rev-parse HEAD)
export RAY_INSTALL_JAVA=1
export BUILD_ONE_PYTHON_ONLY=pyXX
/ray/python/build-wheel-manylinux2014.sh
```

I've some cloudy memories that we've tried to build the docker from MacOS which was failing for god knows why🤷🏻‍♂️",thanks look answer phase build everything scratch due time pressure made use cache crystal clear end need wait cache server sense create time imagine cheap improvement cache provided local file almost zero correct wrong regarding building tried many way make work custom build saying rock solid condensed opinion forming opinion say given common working solution build use docker like mount source code export git head export export cloudy tried build docker failing god,issue,positive,positive,neutral,neutral,positive,positive
1857511139,"Hi, thanks for the contribution.

While rebasing is nice, you do not need to worry too much about the rebasing. We always squash merge.",hi thanks contribution nice need worry much always squash merge,issue,negative,positive,positive,positive,positive,positive
1857489584,I've done the same rebase + adjust code combo here.,done rebase adjust code,issue,negative,neutral,neutral,neutral,neutral,neutral
1857482624,"Thank you guys to have a look! Since this is my first contribution here (though I've OSP XP in other communities) not sure what are the customs so I'm intended to learn them, suggestions are welcome 🙂 I've rebased, adapted the code based on the suggestion and force pushed to have only a single commit in the tree.",thank look since first contribution though sure custom intended learn welcome code based suggestion force single commit tree,issue,positive,positive,positive,positive,positive,positive
1857364594,"> I think there is an edge case here where an actor dies after WriteAcquire but before WriteRelease

I see, would we need a timeout here to force release of the lock?",think edge case actor see would need force release lock,issue,negative,neutral,neutral,neutral,neutral,neutral
1857328388,decision: we will go with proper solution with ordering instead of quick fix since it is not urgent,decision go proper solution instead quick fix since urgent,issue,negative,positive,positive,positive,positive,positive
1857327117,This causes confusion to one of critical users. The usage of this APi is growing among critical users and we should make sure we are raising proper error when the wrong pattern is used,confusion one critical usage growing among critical make sure raising proper error wrong pattern used,issue,negative,neutral,neutral,neutral,neutral,neutral
1857326197,This causes the check failure which is critical for stability ,check failure critical stability,issue,negative,negative,negative,negative,negative,negative
1857305712,@silverbulletmdc thanks for clarifying. Could you try your Gradio application with the Ray nightly to make sure that it works?,thanks could try application ray nightly make sure work,issue,positive,positive,positive,positive,positive,positive
1857224397,"But gradio 4 relies on pydantic2, so ray2.8 is not compitable with gradio currently. Hope the 2.9 be released as soon as possible.",ray currently hope soon possible,issue,negative,neutral,neutral,neutral,neutral,neutral
1857163646,"Maybe another clue is where the warning message was generated
```text
WARNING: /workspace/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
```

I can do the experiment in `get_model()`

```python
class MyXGBoostTrainer(XGBoostTrainer):
    @staticmethod
    def get_model(checkpoint: Checkpoint) -> xgb.Booster:
        with checkpoint.as_directory() as checkpoint_path:
            booster = xgb.Booster()
            booster.load_model(
                os.path.join(checkpoint_path, 'model.json')
            )

            # This will dump the user warning
            # WARNING: /workspace/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
            booster.save_model('model')

            # This works fine
            booster.save_model('model.json')
```

It shows that as long as I didn't call `booster.save_model()` without giving `*.json` postfix in `_save_model()`, this warning shouldn't exist unless there is somewhere override my code.

Somehow Ray didn't use what I set in the `_save_model()`, and I think `xgboost_ray` only involves the worker training part.
Because the TensorBoard-like checkpoint directory structure is generated by Ray Tuner API, `xgboost_ray` only returns booster and we will have to save it manually.",maybe another clue warning message text warning saving binary model format please consider model format default experiment python class booster dump user warning warning saving binary model format please consider model format default work fine long call without giving postfix warning exist unless somewhere override code somehow ray use set think worker training part directory structure ray tuner booster save manually,issue,negative,positive,positive,positive,positive,positive
1857142649,"- Added explicit graphs tests
- Added tests for iterating through all possible states to catch future regression
- Remove constrain on the ordering of status history 
- Modify get_status_time_ms API. ",added explicit added possible catch future regression remove constrain status history modify,issue,negative,neutral,neutral,neutral,neutral,neutral
1857137035,"I think it is worth trying though... But we may need some time to actually fix it properly (and I have no time for it at this point...)

Why don't we start from it to fix the flaky test & fix it properly when we fix the ordering issue? ",think worth trying though may need time actually fix properly time point start fix flaky test fix properly fix issue,issue,negative,positive,neutral,neutral,positive,positive
1857135112,"Sure, I will take a look today to see if it is `xgboost_ray` related issue. (Version is the latest already)

```bash
# https://github.com/ray-project/xgboost_ray/releases/tag/v0.1.19
$ pip list | grep xgboost-ray
xgboost-ray                       0.1.19
```

But the weird thing is that, no matter how I modify XGBoostTrainer (`get_model()` and especially `_save_model()` method), the content in the destination stays unchanged. (So, modifying `get_model()` to load what it gives me is the only workaround now)

I tried:

1. No change, which is expected to have [`model.json`](https://github.com/ray-project/ray/blob/8362c507a18bd557eff94d2881f186d5257c36d7/python/ray/train/xgboost/xgboost_checkpoint.py#L18)
2. Change the model name from `model.json` to `model`
3. Save the entire booster as a pickle
4. Save additional information (`feature_name`) in the [booster attributes](https://github.com/dmlc/xgboost/issues/3089#issuecomment-370065246)

Conclusion

1. It will successfully store in the temp folder
2. The destination that was set in `RunConfig` will always and can only find the legacy `model` file without all operations I did in the `_save_model()`

I haven't found how `ray.train.report` save the Checkpoint to the destination. I expected there should be a copy operation (should have the same content as what's in the temp dir) or reinvoke of the `_save_model()` (which forces the booster checkpoint to be the legacy `model` output name).
Maybe it is the [`_TrainSession.persist_artifacts`'s](https://github.com/ray-project/ray/blob/ee10ea692a0fc8863589411c39a7175aae3fdb06/python/ray/train/_internal/session.py#L244) work, but haven't found out how it works.

@justinvyu Did your checkpoint content change when you modify `XGBoostTrainer._save_model()`?",sure take look today see related issue version latest already bash pip list weird thing matter modify especially method content destination stay unchanged load tried change change model name model save entire booster pickle save additional information booster conclusion successfully store temp folder destination set always find legacy model file without found save destination copy operation content temp booster legacy model output name maybe work found work content change modify,issue,positive,positive,positive,positive,positive,positive
1857126543,"@jjyao last time when we tried this, this failed lots of tests. I think that approach has some implication (that raylet becomes slower to be ready). ",last time tried lot think approach implication raylet becomes ready,issue,negative,positive,neutral,neutral,positive,positive
1857092896,@jjyao ETA on start and whose going to pick this up; we updated the priority here so...,eta start whose going pick priority,issue,negative,neutral,neutral,neutral,neutral,neutral
1857085679,"@daviddwlee84 I am not able to reproduce this with all the same package versions as you -- the biggest thing I can think of is `xgboost_ray` being out of date, and saving to the `model` file instead of the updated `model.json`. Could you double-check your `xgboost_ray` version and upgrade it to latest?",able reproduce package biggest thing think date saving model file instead could version upgrade latest,issue,negative,positive,positive,positive,positive,positive
1856972578,How is the performance if we just fix (1)? Do we need to investigate anything besides turning off locality_with_output?,performance fix need investigate anything besides turning,issue,negative,neutral,neutral,neutral,neutral,neutral
1856909309,"actually let's try testing it first
",actually let try testing first,issue,negative,positive,positive,positive,positive,positive
1856859579,"@AnirudhDagar Thanks for raising this issue! The error is getting raised unintentionally, because the `**trainable_kwargs` can technically take in any parameter name, including `checkpoint_dir`. So, when we do `_detect_checkpoint_function` and try to bind `checkpoint_dir` to the signature, it works out and raises the deprecation warning.

To fix this, avoid using `**kwargs` in your function arguments:

```python
from ray import tune

trainable_kwargs = {""kwarg1"": 1, ""kwarg2"": 2}


def train_fn(config, kwarg1=None, kwarg2=None):
    pass


tuner = tune.Tuner(
    tune.with_parameters(train_fn, **trainable_kwargs),
)
```

One other thing: this check/deprecation warning will be removed by the next release -- we wanted to leave it for some time for migration purposes.

I'll close this issue for now, but feel free to re-open and ask any followups!
",thanks raising issue error getting raised unintentionally technically take parameter name try bind signature work deprecation warning fix avoid function python ray import tune pas tuner one thing warning removed next release leave time migration close issue feel free ask,issue,negative,positive,positive,positive,positive,positive
1856833759,We have since added new user guides for using checkpoints.,since added new user,issue,negative,positive,positive,positive,positive,positive
1856795418,"Setting the deepspeed fp16 configs `hysteresis=4` and `consecutive_hysteresis=True` basically gave me the same behavior as `fp16-scale-tolerance=0.25`. 🟢 
*  You must have 4 batches in a row give NaN/inf gradients before reducing the loss scale factor by 2. Any non-NaN/inf gradient batch will reset the counter back to 4.",setting basically gave behavior must row give reducing loss scale factor gradient batch reset counter back,issue,negative,neutral,neutral,neutral,neutral,neutral
1856748612,"The user errors for `map / map_batches` have already been solved by hao's pr, that makes user error more clear. I will address the read api error message",user map already hao user error clear address read error message,issue,negative,positive,positive,positive,positive,positive
1856715986,Closing as I cannot reproduce this currently anymore. It should have been resolved as described in the previous comment.,reproduce currently resolved previous comment,issue,negative,negative,neutral,neutral,negative,negative
1856642146,"After a discussion yesterday, it sounds like we should not add icons here because the ray libraries each have their own icons, so this branding would conflict with that.",discussion yesterday like add ray would conflict,issue,negative,neutral,neutral,neutral,neutral,neutral
1856509525,"@aslonnie yes, that error will go away with the migration, the ami already has the correct bazel built in",yes error go away migration ami already correct built,issue,negative,neutral,neutral,neutral,neutral,neutral
1856457357,[p2] In the future we may want to remove PythonGcsClient and double down on GcsClient.,future may want remove double,issue,negative,neutral,neutral,neutral,neutral,neutral
1856454607,"> Ping me when ready for review

Sounds good, let me know if you think anything needed to be added. The major thing waiting from nvidia is Dockerfile part.",ping ready review good let know think anything added major thing waiting part,issue,positive,positive,positive,positive,positive,positive
1856452166,"Pydantic 2 is not supported in Ray 2.8. Please downgrade to `pydantic<2`.

Starting in Ray 2.9 (which should be released around the end of the month), `pydantic>=2.5.0` will be compatible with Ray.

If you want to start using Pydantic 2 ASAP, then the [Ray nightly wheel](https://docs.ray.io/en/releases-2.8.1/ray-overview/installation.html#daily-releases-nightlies) is already compatible with it. I would recommend installing the nightly and using `pydantic>=2.5.0`.",ray please downgrade starting ray around end month compatible ray want start ray nightly wheel already compatible would recommend nightly,issue,positive,neutral,neutral,neutral,neutral,neutral
1856421747,are we still pinned < 3.12?  feel this should be higher priority as you're asking devs to choose between the language or your library.,still pinned feel higher priority choose language library,issue,negative,positive,positive,positive,positive,positive
1856362294,"Some additional context:

- RTD actors run on m5.large AWS instances, meaning there are only 2 vCPUs available. Speedup will not be as much as we want it to be.
- RTD cannot upgrade us to a larger instance.

For now, let's close this while we think another other paths forward.",additional context run meaning available much want upgrade u instance let close think another forward,issue,negative,positive,positive,positive,positive,positive
1856342396,"Seem to be flaky; I ran them this morning and they passed https://buildkite.com/ray-project/release-tests-bisect/builds/792. 
",seem flaky ran morning,issue,negative,neutral,neutral,neutral,neutral,neutral
1856325811,"@daviddwlee84 Thanks for posting all of your investigation, I'll take a closer look today.",thanks posting investigation take closer look today,issue,negative,positive,positive,positive,positive,positive
1856312875,"@can-anyscale That one has not been picked in yet. Here's the pick PR, we should merge these one right after the other: https://github.com/ray-project/ray/pull/41844",one picked yet pick merge one right,issue,negative,positive,positive,positive,positive,positive
1856309995,Was https://github.com/ray-project/ray/pull/41807 already picked? Somehow the serve release tests are not failing on release branch I think,already picked somehow serve release failing release branch think,issue,negative,neutral,neutral,neutral,neutral,neutral
1856294509,"Oh, it looks like the doc build actually succeeded despite showing a red X on github.  https://readthedocs.com/projects/anyscale-ray/builds/1900331/

Merging",oh like doc build actually despite showing red,issue,negative,neutral,neutral,neutral,neutral,neutral
1856292779,The docs build failed.  Restarting now. I'll merge once the doc build passes,build merge doc build,issue,negative,neutral,neutral,neutral,neutral,neutral
1856274474,"LGTM! Just curious, how are you currently building the wheel locally? Thankks

Asking to see if we should add the bazel cache link to the documentation. The build is significantly faster with the cache.",curious currently building wheel locally see add cache link documentation build significantly faster cache,issue,negative,positive,neutral,neutral,positive,positive
1856266888,discussed internal to core - we'll possibly plan for this in ray210,internal core possibly plan ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1856153739,"> because we start it asynchronously

Can/should we make it synchronously? Raylet can make sure the agent process is ready before marking itself as ready.",start make synchronously raylet make sure agent process ready marking ready,issue,positive,positive,positive,positive,positive,positive
1856119661,"@ericl Could you assign this to someone to review, please? Thanks for considering this PR.",could assign someone review please thanks considering,issue,positive,positive,positive,positive,positive,positive
1856101016,"The core failure is fixed. Removing a release blocker. Seems like there's still discussion going on, so I will keep the issue open (@stephanie-wang let us know the priority of the other issue!)",core failure fixed removing release blocker like still discussion going keep issue open let u know priority issue,issue,negative,negative,neutral,neutral,negative,negative
1856028819,"@rkooo567 let's close, the test is passing now on runtime ;)",let close test passing,issue,negative,neutral,neutral,neutral,neutral,neutral
1855948534,"**reproducible script**: 

When running the following module: https://github.com/MCFpy/mcf/blob/main/examples/min_parameters_optpolicy.py  (which unfortunately relies on external data given in https://github.com/MCFpy/mcf/blob/main/data/data_x_ps_1_1000.csv )  on the following **dependencies**: 

```
dependencies:
- python=3.11.5
- spyder
- pip
- pip:
  - matplotlib
  - numba
  - pandas
  - ray[default]
  - scikit-learn
  - scipy
  - sympy
  - mcf
```

I get the following **traceback error:**

```
Traceback (most recent call last):

  File ~\AppData\Local\anaconda3\envs\test-env_raydefault\Lib\site-packages\spyder_kernels\py3compat.py:356 in compat_exec
    exec(code, globals, locals)

  File c:\users\aarmendarizpacheco\onedrive - universitaet st.gallen\dokumente\phdhsgapap\testing_065\testing_065\min_parameters_optpolicy.py:66
    alloc_train_df = myoptp.solve(train_df, data_title=TRAINDATA)

  File ~\AppData\Local\anaconda3\envs\test-env_raydefault\Lib\site-packages\mcf\optpolicy_functions.py:324 in solve
    allocation_df = op_pt.policy_tree_allocation(self, data_new_df)

  File ~\AppData\Local\anaconda3\envs\test-env_raydefault\Lib\site-packages\mcf\optpolicy_pt_functions.py:46 in policy_tree_allocation
    best_tree, _, _ = optimal_tree_proc(optp_, data_df, seed=12345)

  File ~\AppData\Local\anaconda3\envs\test-env_raydefault\Lib\site-packages\mcf\optpolicy_pt_functions.py:266 in optimal_tree_proc
    data_x_ref = ray.put(data_x)

  File ~\AppData\Local\anaconda3\envs\test-env_raydefault\Lib\site-packages\ray\_private\auto_init_hook.py:24 in auto_init_wrapper
    return fn(*args, **kwargs)

  File ~\AppData\Local\anaconda3\envs\test-env_raydefault\Lib\site-packages\ray\_private\client_mode_hook.py:103 in wrapper
    return func(*args, **kwargs)

  File ~\AppData\Local\anaconda3\envs\test-env_raydefault\Lib\site-packages\ray\_private\worker.py:2636 in put
    object_ref = worker.put_object(value, owner_address=serialize_owner_address)

  File ~\AppData\Local\anaconda3\envs\test-env_raydefault\Lib\site-packages\ray\_private\worker.py:720 in put_object
    self.core_worker.put_serialized_object_and_increment_local_ref(

  File python\ray\_raylet.pyx:3361 in ray._raylet.CoreWorker.put_serialized_object_and_increment_local_ref

  File python\ray\_raylet.pyx:3253 in ray._raylet.CoreWorker._create_put_buffer

  File python\ray\_raylet.pyx:468 in ray._raylet.check_status

RaySystemError: System error: Unknown error
```
",reproducible script running following module unfortunately external data given following pip pip ray default get following error recent call last file code file file solve self file file file return file wrapper return file put value file file file file system error unknown error,issue,negative,negative,neutral,neutral,negative,negative
1855924106,@can-anyscale can I close this? Or do you want to keep it open for investigation? ,close want keep open investigation,issue,negative,neutral,neutral,neutral,neutral,neutral
1855142836,Some of the pinned torch version doesn't compile on windows. The constraint files are currently ignored on windows (https://github.com/ray-project/ray/blob/master/ci/env/install-dependencies.sh#L470). Let see if I can fix that while I'm here.,pinned torch version compile constraint currently let see fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1855142111,"We're working on a similar system to fine-tune some small models for users. Our current solution:

1. Run a dispatcher deployment which accepts requests to begin training.
2. Dispatcher verifies the request and then calls a new actor with the required training code. 
3. This Actor's task is called async
4. We return the TaskID and make calls to ray_state.get_task for scheduling status.

In actuallity, the Task is tied to an S3 object associted with the fine-tuning run which we poll for persistent status.

Roughly:

```
@ray.remote(
    num_gpus=1.0,
    num_cpus=4.0
)
class Trainer:
    def train(self):
        *long-running task here*
        *save to s3 etc.*

@serve.deployment()
@serve.ingress(app)
class Dispatcher:
    @app.post(""/train"")
    async def call_trainer(self, http_request: Request):
        body = await http_request.json()

        trainer = Trainer.remote()
        ref = trainer.train.remote()

        return str(ref.task_id())
```

The major issue here is the persistence is reliant on something like S3, and importantly, Serve will downscale Dispatcher replicas with active training runs (as they have already returned), resulting in Ray ending the Train call before completion. So the Dispatcher cannot be autoscaled safely.


Both suggestions above work, but I prefer the second option as se have a deployment graph for pre-processing and option 2 seems easier to work around. It's also more similar to the workflow of Ray core.

",working similar system small current solution run dispatcher deployment begin training dispatcher request new actor training code actor task return make status task tied object run poll persistent status roughly class trainer train self task save class dispatcher self request body await trainer ref return major issue persistence reliant something like importantly serve dispatcher active training already returned resulting ray ending train call completion dispatcher safely work prefer second option se deployment graph option easier work around also similar ray core,issue,positive,positive,neutral,neutral,positive,positive
1855123480,"Thanks for all your input, 
I tried `[RAY_TASK_MAX_RETRIES=0]` -- did not help, 
@stephanie-wang how to use `ray stack`? I have a hung ray process with the message 
```
[2023-12-14 08:56:35,320 E 3620529 3620803] core_worker.cc:593: :info_message: Attempting to recover 6 lost objects by resubmitting their tasks. To disable object reconstruction, set @ray.remote(max_retries=0).
```

but `ray stack` shows:

```
Stack dump for ubuntu   3756186  1.8  1.0 22599752 709792 pts/51 SNl+ 08:52   3:09 ray::IDLE
Process 3756186: ray::IDLE
Python v3.11.6 (/usr/bin/python3.11)

Error: Failed to merge native and python frames (Have 1 native and 2 python)

Stack dump for ubuntu   3779883  1.4  0.1 21825680 118900 pts/51 SNl+ 08:54   2:24 ray::IDLE
Process 3779883: ray::IDLE
Python v3.11.6 (/usr/bin/python3.11)

Error: Failed to merge native and python frames (Have 1 native and 2 python)

```
",thanks input tried help use ray stack hung ray process message recover lost disable object reconstruction set ray stack stack dump ray process ray python error merge native python native python stack dump ray process ray python error merge native python native python,issue,negative,positive,positive,positive,positive,positive
1855107102,"Updated. Changes:

- Pinned all versions according to requirements_compiled.txt
- Discovered that Serve actually uses aiohttp. Added back dependency to `ray[serve]` (Removed from `ray[default]`).
- Restricted access to the internal deps to Dashboard, Runtime Env Agent, Dashboard tests, and release script (`release/ray_release/command_runner/_prometheus_metrics.py`).",pinned according discovered serve actually added back dependency ray serve removed ray default restricted access internal dashboard agent dashboard release script,issue,negative,neutral,neutral,neutral,neutral,neutral
1855028042,"Nice, this actually fix the linkcheck on post-merge. Let's merge this!",nice actually fix let merge,issue,negative,positive,positive,positive,positive,positive
1854938304,I haven't made any progress on the root cause. It looks like the recent red is coming from a new pr #41637 ,made progress root cause like recent red coming new,issue,positive,positive,neutral,neutral,positive,positive
1854903016,"Here is my findings: I'm working on the Ray cluster over 2 months in on-perm with BM servers. 

First i had issue with version 2.7.0, after lot trail & error, foud upgraded to 2.8.0 to resolve the uninitialized issue for Ray cluster with 1 head and 1 worker node, we tested autoscaler, hyperparameter and ray server every thing fine.

But now we wanted to test mult-node (more than 2 or more woker node). i start cluster with 2 or more node as worker node, hardly spin up cluster in first try with all nodes, once i able to star the cluster with 3 worker node, but raylet was died in one of the worker node, so my point is ray local provider cluster with 2 or more node is not reliable and most of the time worker nodes are ""uninitialized"".

Env:
Cluster: local provider
Nodes: 1 head, 3 worker nodes
Docker version: rayproject/ray-ml:2.8.1
Network: all 4 servers are in same switch and same subnet.

Cluster Launcher env:
Conda ray env:
Ray: 2.8.1
Python version: 3.9.18

I also tested the step suggested by @ajachemmanam, doesn't work for me 
I tweak cluster state file in /tmp/ray with worker node state changed from terminated to ""up-to-down"", tear down the cluster, then relaunch works some time this way, but with autoscaler don't work this way. 

Please let me know if you need any additional information. 

",working ray cluster first issue version lot trail error foud resolve issue ray cluster head worker node tested ray server every thing fine test node start cluster node worker node hardly spin cluster first try able star cluster worker node raylet one worker node point ray local provider cluster node reliable time worker cluster local provider head worker docker version network switch cluster launcher ray ray python version also tested step work tweak cluster state file worker node state tear cluster relaunch work time way work way please let know need additional information,issue,negative,positive,positive,positive,positive,positive
1854899765,@justinvyu did we address @zhe-thoughts 's last comment above (it was made post merge and I didn't see any additional links in this ticket).,address last comment made post merge see additional link ticket,issue,negative,neutral,neutral,neutral,neutral,neutral
1854877891,I also tried deploying the app as a config using `serve deploy`. The logs do show up. I'll close this issue.,also tried serve deploy show close issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1854875995,"The problem goes away if I use `serve run --address=""auto""`. Serve does write logs to `temp-dir` in that case. The root cause for `serve run` seems to be that it doesn't connect to the long-lived Ray cluster automatically.",problem go away use serve run auto serve write case root cause serve run connect ray cluster automatically,issue,negative,neutral,neutral,neutral,neutral,neutral
1854863690,"Any progress here @Zandew?

@can-anyscale  we've failed this correct? Since it's looking bright red in our CI right now.",progress correct since looking bright red right,issue,positive,positive,positive,positive,positive,positive
1854837806,"@rickyyx I'm going to close this one but feel free to take over. Moving to CI v1 didn't seem to do anything, and neither did increasing the timeout to 10s. But maybe a much longer timeout would help.",going close one feel free take moving seem anything neither increasing maybe much longer would help,issue,positive,positive,positive,positive,positive,positive
1854786803,"Here's what I tried:
- Setting a lower learning rate 1e-5, still saw the same error 🔴 
- Shuffling the dataset with a different seed (made the error happen quicker) 🔴 
  - Global shuffle will materialize & consume the full dataset, which gives a *single* ordering of the data every single epoch. This may need to be combined with a local shuffle to get a different batch ordering per epoch of training. (see next point).
  - Switched from global shuffle that happens once, to a local shuffle that will give different orderings per epoch. (This doesn't really affect anything due to the script only running for 1 epoch.) 🟡 
- Reducing the batch size to just use fp32. This works, but significantly slows down the training. (~35 minutes -> 3+ hours projected) 🟡 
  - Using gradient accumulation to still get the same effective batch size.
- Next idea to validate: try to set the `fp16-scale-tolerance=0.25` parameter so that we skip more batches before decreasing loss scale (less training happens, but it's ok just for this example to run successfully).",tried setting lower learning rate still saw error shuffling different seed made error happen global shuffle materialize consume full single data every single epoch may need combined local shuffle get different batch per epoch training see next point switched global shuffle local shuffle give different per epoch really affect anything due script running epoch reducing batch size use work significantly slows training gradient accumulation still get effective batch size next idea validate try set parameter skip decreasing loss scale le training example run successfully,issue,negative,positive,positive,positive,positive,positive
1854686438,Let's cherry pick tomorrow after confirming that it has no perf impact.,let cherry pick tomorrow confirming impact,issue,negative,neutral,neutral,neutral,neutral,neutral
1854638334,"Thanks Ed! yea, we should get this cherry picked and fix the release tests. Can rework the release tests and docs later :) ",thanks yea get cherry picked fix release rework release later,issue,negative,positive,neutral,neutral,positive,positive
1854626707,"@edoakes , are you going to cherry pick into the release branch? hopefully this can fix the release test failures in the release branch 🙏 ",going cherry pick release branch hopefully fix release test release branch,issue,negative,neutral,neutral,neutral,neutral,neutral
1854607346,"To add some additional context, the proposal here is to introduce pre-commit in stages to increase confidence in our dev tooling:

1. Add a `.pre-commit-config.yaml` that does basically what `format.sh` does - maybe with a little of the extra functionality we've been lacking (think stuff like adding `prettier` for the html and js in `docs/`). At this stage it would be completely optional to use; people who don't enable it won't even know or care about it.
2. Ensure that the tool is doing what we want - that it effectively supersedes `format.sh`. This could be done in a number of ways (CI job, thorough manual testing, etc)
3. Remove `format.sh`, and instead change `install-hooks.sh` to do something like
```bash
#!/bin/bash

pre-commit install
```
At this point we would also want to change whatever CI job that runs `format.sh` to instead call `pre-commit run --all`.",add additional context proposal introduce increase confidence dev tooling add basically maybe little extra functionality think stuff like stage would completely optional use people enable wo even know care ensure tool want effectively could done number way job thorough manual testing remove instead change something like bash install point would also want change whatever job instead call run,issue,positive,positive,positive,positive,positive,positive
1854600290,"> Kinda wondering what error does this throw before the change? And how bytes help to fix the issue? Neither is json serializable so probably not related to that.
> 
> Also, is it tested? And should we also add a doc somewhere (maybe in [http guide](https://docs.ray.io/en/master/serve/http-guide.html) noting request object can't be recursive passed?

It throws a bunch of error messages in the release test but we aren't actually checking that the requests succeed 😞 I'd like to rework the test to make it more useful.

Bytes fixes it because bytes are trivially serializable for `DeploymentHandle` calls. Let me make sure it's explicitly documented.",wondering error throw change help fix issue neither probably related also tested also add doc somewhere maybe guide request object ca recursive bunch error release test actually succeed like rework test make useful trivially let make sure explicitly,issue,positive,positive,positive,positive,positive,positive
1854588843,"So there's a few things going on here which result in the checks failing:

1. The `trim-trailing-whitespace` and `end-of-file-fixer` are definitely things we want but are not currently part of `format.sh`. These result in trivial changes in many files. I can remove these if people want initially, or I can make a PR to fix these things beforehand, or whatever. I just wish IDEs did this stuff automatically when you save a file...
2. The `check-json` check fails because we have a bunch of files which are invalid JSON:
```
rllib/tests/data/pendulum/large.json: Failed to json decode (Extra data: line 2 column 1 (char 102630))
python/ray/data/examples/data/iris.json: Failed to json decode (Extra data: line 2 column 1 (char 95))
rllib/tests/data/pendulum/small.json: Failed to json decode (Extra data: line 2 column 1 (char 685))
rllib/tests/data/cartpole/large.json: Failed to json decode (Extra data: line 2 column 1 (char 63050))
python/ray/data/examples/data/different-extensions/data.json: Failed to json decode (Expecting value: line 1 column 1 (char 0))
python/ray/data/examples/data/image-datasets/different-extensions/data.json: Failed to json decode (Expecting value: line 1 column 1 (char 0))
rllib/tests/data/cartpole/small.json: Failed to json decode (Extra data: line 2 column 1 (char 16336))
release/release_logs/1.12.0/benchmarks/many_tasks.json: Failed to json decode (Extra data: line 8 column 2 (char 1221))
```
I asked on slack about what these files were and why they weren't valid JSON, but haven't heard back. I looked in a bunch of these and they don't even look like JSON5. IMO if these aren't valid JSON we should change the file extension; otherwise if there is some good reason these have a `.json` extension but are not JSON we could add exclude rules for them for this check.

3. Black fails due to reformatting `ci/lint/git-clang-format`, I suspect because people don't touch it very frequently - maybe whoever touched it last wasn't using `format.sh`? I couldn't find any exclude rule in `format.sh` that would ignore this file. Should we just format this?
4. There were a few flake8 issues:
```
ci/lint/git-clang-format:593:9: F841 local variable 'index_tree' is assigned to but never used
release/release_logs/compare_perf_metrics:134:5: F841 local variable 'regressions' is assigned to but never used
release/release_logs/compare_perf_metrics:144:89: E501 line too long (162 > 88 characters)
```
Again, I'm guessing whoever touched `git-clang-format` and `compare_perf_metrics` last didn't lint the files; I couldn't find any rule that excluded these files from `format.sh`. Anyway, the fixes would be minor.

5. `prettier` fails because the docs have a bunch of HTML and JS that need formatting. I can do that in a different PR, or here - up to you.
6. I added a few RST checks which fail due to actual problems in our docs - these are being fixed in another PR.
7. The `python-no-log-warn` indicates parts of the code base we need to update to use `logger.warning` instead of `logger.warn`. I can turn this off if you want, but we should really fix these. Again, I can do this in another PR or however you'd like.
8. The `python-check-mock-methods` caught two instances where we have tests unconditionally passing. This is being fixed in another PR.
9. The `google-java-format` check finds 3 files that need to be formatted. If I run `format.sh --all`, the google java formatter finds exactly the same 3 files, so I think this is fine. I think we just need to run the formatter on the code and submit a PR for this.",going result failing definitely want currently part result trivial many remove people want initially make fix beforehand whatever wish ides stuff automatically save file check bunch invalid decode extra data line column char decode extra data line column char decode extra data line column char decode extra data line column char decode value line column char decode value line column char decode extra data line column char decode extra data line column char slack valid back bunch even look like valid change file extension otherwise good reason extension could add exclude check black due suspect people touch frequently maybe whoever touched last could find exclude rule would ignore file format flake local variable assigned never used local variable assigned never used line long guessing whoever touched last lint could find rule anyway would minor bunch need different added fail due actual fixed another code base need update use instead turn want really fix another however like caught two unconditionally passing fixed another check need run exactly think fine think need run code submit,issue,positive,positive,neutral,neutral,positive,positive
1854498305,"Still need to:
- fix xplat (windows) build
- e2e test
- only change parent async_context if it is not equal to current one (to reduce churn if multiple contexts get redirected and try to change the same parent)",still need fix build test change parent equal current one reduce churn multiple get try change parent,issue,negative,neutral,neutral,neutral,neutral,neutral
1854492858,awesome. thanks for the quick fix!,awesome thanks quick fix,issue,positive,positive,positive,positive,positive,positive
1854458513,@architkulkarni @zhe-thoughts small change to serve release test code to (hopefully) fix the failures,small change serve release test code hopefully fix,issue,negative,negative,negative,negative,negative,negative
1854443361,"> Now that we only install if it doesn't already exist, is there any risk that the already existing version is the wrong version?

on CI we run every test in a new vm today.",install already exist risk already version wrong version run every test new today,issue,negative,negative,negative,negative,negative,negative
1854432478,"Failing test is unrelated.

The doc continue to hide internal methods https://anyscale-ray--41868.com.readthedocs.build/en/41868/serve/api/doc/ray.serve.grpc_util.RayServegRPCContext.html",failing test unrelated doc continue hide internal,issue,negative,neutral,neutral,neutral,neutral,neutral
1854389007,"<img width=""1357"" alt=""Screenshot 2023-12-13 at 18 11 12"" src=""https://github.com/ray-project/ray/assets/9356806/c0fa55c0-e36a-4ac1-a8c5-b490e66c489f"">

The failing test on premerge is a broken link that - unrelated to this PR.",failing test broken link unrelated,issue,negative,negative,negative,negative,negative,negative
1854177420,"Hah, I actually get the wanda solution working that runs 2x faster now. Moving this to draft and will put the wanda stack for review.",hah actually get solution working faster moving draft put stack review,issue,negative,neutral,neutral,neutral,neutral,neutral
1854080482,"I forgot to switch the CUDA version back to 121 so match the pytorch version in the Dockerfile.
I've produced another Docker image and tested it in a workspace.

Let's wait for tests to run through so we can merge.",forgot switch version back match version produced another docker image tested let wait run merge,issue,negative,neutral,neutral,neutral,neutral,neutral
1854062002,"@architkulkarni when you retried, did it pass? It seems like the failure is from the release branch",pas like failure release branch,issue,negative,negative,negative,negative,negative,negative
1854050386,@rkooo567  I guess end of year will be hard? Is january 24 the current target for 2.10 ?,guess end year hard current target,issue,negative,negative,negative,negative,negative,negative
1854043347,"I think pymalloc not releasing memory is the expected behavior (see https://bloomberg.github.io/memray/python_allocators.html#). It is due to small objects are allocated to arena which is not easy to be deallocated. 

I think malloced memory is all deallocated from linux. Maybe this can help macos memory usage, but I don't think it is critical. ",think memory behavior see due small arena easy think memory maybe help memory usage think critical,issue,positive,positive,neutral,neutral,positive,positive
1854038311,"https://github.com/ray-project/ray/pull/41864 This PR fixes the dashboard tester error messages, but that error message should be unrelated to the failure. From the logs, the test itself seems passing. @can-anyscale I am going to remove a release blocker in this case?",dashboard tester error error message unrelated failure test passing going remove release blocker case,issue,negative,negative,negative,negative,negative,negative
1854009074,"also verified OSS error has the same message as dashboard tester failure. So runtime failure is not related to test itself (but should be related to the ray_release issue). If you look at job logs, it successfully finished the test (I can find logs https://github.com/ray-project/ray/blob/fe8e1c55dd81bbdd49b7e9f1e82d34b91b4591a9/release/benchmarks/distributed/many_nodes_tests/actor_test.py#L134 from the end of the failed build)",also error message dashboard tester failure failure related test related issue look job successfully finished test find end build,issue,negative,positive,neutral,neutral,positive,positive
1853997785,"@can-anyscale btw, while we should fix the dashboard tester, it shouldn't affect the test result (test still should pass). dashboard tester runs in a separate actor and just prints logs when query fails, but it doesn't fail the test.

```
[ERROR 2023-12-12 02:39:39,710] util.py: 137  Retry function call failed due to An error occurred (404) when calling the HeadObject operation: Not Found in 10 seconds...

<br class=""Apple-interchange-newline"">
```

I suspect this is the actual error? ",fix dashboard tester affect test result test still pas dashboard tester separate actor query fail test error retry function call due error calling operation found suspect actual error,issue,negative,negative,negative,negative,negative,negative
1853505078,"Hi, 
Thank you for your reply!
I've implemented the above which seemed to do what I was hoping for, but it caused a memory error. After further inspection it appears that its launching all 4 GPUS but then only training over 1.
![image](https://github.com/ray-project/ray/assets/58394834/86f8751c-c490-47c1-971e-af874327a122)
![image](https://github.com/ray-project/ray/assets/58394834/5c133770-68b6-4d5d-ab68-8a921d6e3efa)

```
def train_func(config):
    # Define Parameters
    super_gradients.init_trainer()
    early_stop_map = EarlyStop(Phase.VALIDATION_EPOCH_END, monitor=""mAP@0.50:0.95"", mode=""max"", patience=3, verbose=True)

    # list of the possible classes
    classes = [
        ""drone"",
        ""Herc"",
        ""Bird"",
        ""FixedWing"",
        ""Person"",
        ""Aircraft"",
        ""Helicopter"",
        ""FighterJet"",
    ]
    torch.cuda.empty_cache()
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    #train_data, val_data = load_data()
    classes = [
        ""drone"",
        ""Herc"",
        ""Bird"",
        ""FixedWing"",
        ""Person"",
        ""Aircraft"",
        ""Helicopter"",
        ""FighterJet"",
    ]

    dataset_params = {
        ""data_dir"": ""../../../SageMaker/data/sample_data_small"",  # root directory of data
        ""train_images_dir"": ""images/train/"",  # train images
        ""train_labels_dir"": ""labels/train/"",  # train labels
        ""val_images_dir"": ""images/valid/"",  # validation images
        ""val_labels_dir"": ""labels/valid/"",  # validation labels
        ""test_images_dir"": ""images/test/"",  # test images
        ""test_labels_dir"": ""labels/test/"",  # test labels
        ""classes"": classes,
    }

    # create dataloaders for yolonas model
    train_data = coco_detection_yolo_format_train(
        dataset_params={
            ""data_dir"": dataset_params[""data_dir""],
            ""images_dir"": dataset_params[""train_images_dir""],
            ""labels_dir"": dataset_params[""train_labels_dir""],
            ""classes"": dataset_params[""classes""],
        },
        dataloader_params={
            ""shuffle"": True,
            ""pin_memory"": True,
            ""batch_size"": config[""batch_size""],
            ""num_workers"": 4,
        },
    )

    val_data = coco_detection_yolo_format_val(
        dataset_params={
            ""data_dir"": dataset_params[""data_dir""],
            ""images_dir"": dataset_params[""val_images_dir""],
            ""labels_dir"": dataset_params[""val_labels_dir""],
            ""classes"": dataset_params[""classes""],
        },
        dataloader_params={
            ""batch_size"": config[""batch_size""],
            ""num_workers"": 4,
            ""shuffle"": True,
            ""pin_memory"": True,
        },
    )

    # set training parameters
    train_params = {
        ""silent_mode"": False,
        ""average_best_models"": True,
        ""warmup_mode"": ""linear_epoch_step"",
        ""warmup_initial_lr"": 1e-6,
        ""lr_warmup_epochs"": 3,
        ""initial_lr"": config[""initial_lr""],
        ""lr_mode"": ""cosine"",
        ""cosine_final_lr_ratio"": 0.1,
        ""optimizer"": config[""optimizer""],
        ""optimizer_params"": {""weight_decay"": config[""weight_decay""]},
        ""zero_weight_decay_on_bias_and_bn"": True,
        ""ema"": True,
        ""ema_params"": {""decay"": 0.9, ""decay_type"": ""threshold""},
        ""max_epochs"": config[""epochs""],
        ""mixed_precision"": True,
        ""loss"": PPYoloELoss(
            use_static_assigner=False,
            num_classes=len(dataset_params[""classes""]),
            reg_max=16,
        ),
        ""valid_metrics_list"": [
            DetectionMetrics_050(
                score_thres=config[""confidence_threshold""],
                top_k_predictions=30,
                num_cls=len(dataset_params[""classes""]),
                normalize_targets=True,
                post_prediction_callback=PPYoloEPostPredictionCallback(
                    score_threshold=0.01,
                    nms_top_k=100,
                    max_predictions=30,
                    nms_threshold=0.7,
                ),
            ),
            DetectionMetrics_050_095(
                score_thres=config[""confidence_threshold""],
                top_k_predictions=30,
                num_cls=len(dataset_params[""classes""]),
                normalize_targets=True,
                post_prediction_callback=PPYoloEPostPredictionCallback(
                    score_threshold=0.01,
                    nms_top_k=100,
                    max_predictions=30,
                    nms_threshold=0.7,
                ),
            ),
        ],
        ""metric_to_watch"": ""mAP@0.50:0.95"",
        ""phase_callbacks"": [early_stop_map]
    }
    model = models.get(
        config[""model_to_train""],
        num_classes=8,
        pretrained_weights=""coco"",
    )
    CHECKPOINT_DIR = ""../../../SageMaker/outputs/logs""
    # create a unique identifier for model versioning
    UI = f'{config[""epochs""]}_{config[""confidence_threshold""]}_{config[""initial_lr""]}_{config[""optimizer""]}'
    # for each of the model types specified, train the model
    experiment_name = config[""model_to_train""] + ""-"" + UI
    model = model.to(device)
    trainer = Trainer(experiment_name=experiment_name, ckpt_root_dir=CHECKPOINT_DIR)
    trainer.train(
            model=model,
            training_params=train_params,
            train_loader=train_data,
            valid_loader=val_data,
        )

    metrics = trainer.test(
        model=model,
        test_loader=val_data,
        test_metrics_list=DetectionMetrics_050_095(
                score_thres=config[""confidence_threshold""],
                top_k_predictions=30,
                num_cls=len(classes),
                normalize_targets=True,
                post_prediction_callback=PPYoloEPostPredictionCallback(
                score_threshold=0.01,
                nms_top_k=100,
                max_predictions=30,
                nms_threshold=0.7
            )
        )
    )

    # Return the metric you want to optimize (e.g., mAP@0.50:0.95)
    best_metric = metrics['mAP@0.50:0.95']
    train.report({'mAP@0.50:0.95':best_metric})
    print(best_metric)


config = {
        ""initial_lr"": tune.loguniform(1e-6, 1e-2),
        ""optimizer"": tune.choice([""adam"", ""sgd""]),
        ""epochs"": tune.randint(1, 2),
        ""model_to_train"": tune.choice([""yolo_nas_s"", ""yolo_nas_m"", ""yolo_nas_l""]),
        ""weight_decay"": tune.loguniform(1e-6, 1e-3),
        ""confidence_threshold"": tune.loguniform(0.1, 1),
        ""batch_size"": tune.randint(1, 16),
    }

trainer = TorchTrainer(
    train_func,
    scaling_config=ScalingConfig(num_workers=4, use_gpu=True),
)
tuner = ray.tune.Tuner(
    trainer,
    param_space={""train_loop_config"": config},
    tune_config=ray.tune.TuneConfig(
        search_alg=OptunaSearch(),
        num_samples=10,
        metric=""mAP@0.50:0.95"",
        mode=""max"",
        reuse_actors = True,
    ),
)
results = tuner.fit()
best_result = results.get_best_result(""mAP@0.50:0.95"", ""max"")
print(""Best trial config: {}"".format(best_result.config))
print(best_result)
```

Could you please explain why this is happening and how i might be able to fix it?

Thank you,

Hannah",hi thank reply memory error inspection training image image define map list possible class class drone bird person aircraft helicopter device else class drone bird person aircraft helicopter root directory data train train validation validation test test class class create model class class shuffle true true class class shuffle true true set training false true cosine true true decay threshold true loss class class class map model coco create unique identifier model model train model model device trainer trainer metric class return metric want optimize map metric print trainer tuner trainer map true map print best trial print could please explain happening might able fix thank,issue,positive,positive,positive,positive,positive,positive
1853482714,"@jjyao, can you elaborate how ray.put works in a client mode? Does it put data into the object store of the local node or sends data to a node of the connected cluster? ",elaborate work client mode put data object store local node data node connected cluster,issue,negative,positive,positive,positive,positive,positive
1853443869,"Confirmed it more like the [`ray.train.report`](https://docs.ray.io/en/latest/train/api/doc/ray.train.report.html#ray.train.report) issue. Not XGBoostTrainer.

```python
class MyXGBoostTrainer(XGBoostTrainer):
    @staticmethod
    def get_model(checkpoint: Checkpoint) -> xgb.Booster:
        with checkpoint.as_directory() as checkpoint_path:
            booster = xgb.Booster()
            booster.load_model(
                os.path.join(checkpoint_path, 'model.json')
            )
            if booster.attr('feature_names') is not None:
                booster.feature_names = booster.attr(
                    'feature_names').split('|')
            return booster

    def _save_model(self, model: xgb.Booster, path: str) -> None:
        if hasattr(model, 'feature_names'):
            model.set_attr(feature_names='|'.join(model.feature_names))
        model.save_model(os.path.join(path, 'model.json'))

        # Can found ['model.json'] in the temp dir
        print(os.listdir(path))
        print(ckpt := Checkpoint.from_directory(path))
        # Successfully load XGBoost booster and print feature names
        print(MyXGBoostTrainer.get_model(ckpt).feature_names)
```",confirmed like issue python class booster none return booster self model path none model path found temp print path print path successfully load booster print feature print,issue,positive,positive,positive,positive,positive,positive
1853411034,"- [How to Configure Persistent Storage in Ray Tune — Ray 2.8.1](https://docs.ray.io/en/latest/tune/tutorials/tune-storage.html#configuring-tune-with-a-network-filesystem-nfs)
- [Saving and Loading Checkpoints — Ray 2.8.1](https://docs.ray.io/en/latest/train/user-guides/checkpoints.html#saving-checkpoints-during-training)
  - In Ray 2.8+ it will use `ray.train.report(..., checkpoint=checkpoint)` to dump checkpoint to ""persistent storage"".
  - Haven't found how the ""copying"" failed to copy `model.json` (The `Trainer._save_model()` not properly working) but got `model` instead
- [Configuring Persistent Storage — Ray 2.8.1](https://docs.ray.io/en/latest/train/user-guides/persistent-storage.html#persisting-training-artifacts)
  - Seems adding `SyncConfig(sync_artifacts=True)` to `RunConfig` still not working

```python
import ray
from ray.train import SyncConfig, RunConfig, CheckpointConfig, FailureConfig, ScalingConfig, Checkpoint
from ray.train.xgboost import XGBoostTrainer
import xgboost as xgb
import os

class MyXGBoostTrainer(XGBoostTrainer):
    """"""
    Workaround
    https://github.com/ray-project/ray/issues/41608
    https://github.com/dmlc/xgboost/issues/3089
    """"""
    @staticmethod
    def get_model(checkpoint: Checkpoint) -> xgb.Booster:
        """"""Retrieve the XGBoost model stored in this checkpoint.""""""
        with checkpoint.as_directory() as checkpoint_path:
            booster = xgb.Booster()
            booster.load_model(
                os.path.join(checkpoint_path, 'model')
            )
            if booster.attr('feature_names') is not None:
                booster.feature_names = booster.attr(
                    'feature_names').split('|')
            return booster

    def _save_model(self, model: xgb.Booster, path: str) -> None:
        """"""
        BUG: somehow we are not saving to the correct place we want
        https://github.com/ray-project/ray/issues/41608
        Path direct to the temp file
        /tmp/tmppbsxfulk
        """"""
        if hasattr(model, 'feature_names'):
            model.set_attr(feature_names='|'.join(model.feature_names))
        model.save_model(os.path.join(path, 'model'))

dataset = ray.data.read_csv(""s3://anonymous@air-example-data/breast_cancer.csv"")
train_dataset, valid_dataset = dataset.train_test_split(test_size=0.3)
sync_config = SyncConfig(sync_artifacts=True)
run_config = RunConfig(
    name=f""XGBoost_Test_Checkpoint_Save_Load"",
    storage_path=""/NAS/ShareFolder/ray_debug"",
    checkpoint_config=CheckpointConfig(
        checkpoint_frequency=1,
        num_to_keep=10,
        checkpoint_at_end=True,
        checkpoint_score_attribute='train-error',
        checkpoint_score_order='min',
    ),
    failure_config=FailureConfig(max_failures=2),
    sync_config=sync_config,
)
scaling_config = ScalingConfig(
    num_workers=3,
    placement_strategy=""SPREAD"",
    use_gpu=False,
)
trainer = XGBoostTrainer(
    scaling_config=scaling_config,
    run_config=run_config,
    label_column=""target"",
    num_boost_round=20,
    params={
        ""objective"": ""binary:logistic"",
        ""eval_metric"": [""logloss"", ""error""],
    },
    datasets={""train"": train_dataset, ""valid"": valid_dataset},
)
result = trainer.fit()
checkpoint = result.get_best_checkpoint('valid-logloss', 'min')
# Failed using XGBoostTrainer
booster = XGBoostTrainer.get_model(checkpoint)
# This will work
booster = MyXGBoostTrainer.get_model(checkpoint)
# But feature_name is None
print(booster.feature_names)
```

If modify Trainer to store the entire booster as a pickle, I will still not be able to find the `model.pickle` in the directory.

```python
class XGBoostTrainerWithPickle(XGBoostTrainer):
    @staticmethod
    def get_model(checkpoint: Checkpoint) -> xgb.Booster:
        with checkpoint.as_directory() as checkpoint_path:
            with open(os.path.join(checkpoint_path, 'model.pickle'), 'rb') as fp:
                booster = pickle.load(fp)
            return booster

    def _save_model(self, model: xgb.Booster, path: str) -> None:
        with open(os.path.join(path, 'model.pickle'), 'wb') as fp:
            pickle.dump(model, fp)
```
",configure persistent storage ray tune ray saving loading ray ray use dump persistent storage found copy properly working got model instead persistent storage ray still working python import ray import import import import o class retrieve model booster none return booster self model path none bug somehow saving correct place want path direct temp file model path spread trainer target objective binary logistic error train valid result booster work booster none print modify trainer store entire booster pickle still able find directory python class open booster return booster self model path none open path model,issue,negative,positive,neutral,neutral,positive,positive
1853267912,"It's always this test case

```
python/ray/tests/test_object_spilling.py::test_spill_objects_automatically[fs_only_object_spilling_config0]
```

It passed on my laptop. It hangs on the `ray.put`.

I grep'd the test logs. One interesting log is like this:

```
% grep -rnI local_object_manager.cc:245
./raylet.out:237:[2023-12-12 21:04:21,191 I 88522 845783] (raylet) local_object_manager.cc:245: :info_message:Spilled 64 MiB, 4 objects, write throughput 6 MiB/s.
./raylet.out:263:[2023-12-12 21:04:27,555 I 88522 845783] (raylet) local_object_manager.cc:245: :info_message:Spilled 104 MiB, 6 objects, write throughput 6 MiB/s.
./raylet.out:291:[2023-12-12 21:04:28,980 I 88522 845783] (raylet) local_object_manager.cc:245: :info_message:Spilled 112 MiB, 7 objects, write throughput 6 MiB/s.
./raylet.out:303:[2023-12-12 21:04:37,995 I 88522 845783] (raylet) local_object_manager.cc:245: :info_message:Spilled 176 MiB, 10 objects, write throughput 6 MiB/s.
./raylet.out:330:[2023-12-12 21:04:46,701 I 88522 845783] (raylet) local_object_manager.cc:245: :info_message:Spilled 232 MiB, 15 objects, write throughput 6 MiB/s.
./raylet.out:368:[2023-12-12 21:04:56,783 I 88522 845783] (raylet) local_object_manager.cc:245: :info_message:Spilled 296 MiB, 18 objects, write throughput 6 MiB/s.
./raylet.out:405:[2023-12-12 21:05:07,561 I 88522 845783] (raylet) local_object_manager.cc:245: :info_message:Spilled 360 MiB, 21 objects, write throughput 6 MiB/s.
./raylet.out:667:[2023-12-12 21:05:18,882 I 88522 845783] (raylet) local_object_manager.cc:245: :info_message:Spilled 432 MiB, 28 objects, write throughput 6 MiB/s.
./raylet.out:704:[2023-12-12 21:05:28,873 I 88522 845783] (raylet) local_object_manager.cc:245: :info_message:Spilled 496 MiB, 32 objects, write throughput 6 MiB/s.
./raylet.out:732:[2023-12-12 21:05:37,626 I 88522 845783] (raylet) local_object_manager.cc:245: :info_message:Spilled 552 MiB, 35 objects, write throughput 6 MiB/s.
./raylet.out:770:[2023-12-12 21:05:48,018 I 88522 845783] (raylet) local_object_manager.cc:245: :info_message:Spilled 616 MiB, 40 objects, write throughput 6 MiB/s.
./raylet.out:797:[2023-12-12 21:05:53,571 I 88522 845783] (raylet) local_object_manager.cc:245: :info_message:Spilled 656 MiB, 44 objects, write throughput 6 MiB/s.
./raylet.out:834:[2023-12-12 21:06:03,460 I 88522 845783] (raylet) local_object_manager.cc:245: :info_message:Spilled 720 MiB, 49 objects, write throughput 6 MiB/s.
./raylet.out:1095:[2023-12-12 21:06:14,677 I 88522 845783] (raylet) local_object_manager.cc:245: :info_message:Spilled 792 MiB, 53 objects, write throughput 6 MiB/s.
./raylet.out:1121:[2023-12-12 21:06:21,101 I 88522 845783] (raylet) local_object_manager.cc:245: :info_message:Spilled 832 MiB, 57 objects, write throughput 6 MiB/s.
./raylet.out:1150:[2023-12-12 21:06:22,528 I 88522 845783] (raylet) local_object_manager.cc:245: :info_message:Spilled 840 MiB, 58 objects, write throughput 6 MiB/s.
./raylet.out:1161:[2023-12-12 21:06:31,353 I 88522 845783] (raylet) local_object_manager.cc:245: :info_message:Spilled 904 MiB, 61 objects, write throughput 6 MiB/s.
./raylet.out:1201:[2023-12-12 21:06:42,402 I 88522 845783] (raylet) local_object_manager.cc:245: :info_message:Spilled 976 MiB, 65 objects, write throughput 6 MiB/s.
./raylet.out:1227:[2023-12-12 21:06:51,226 I 88522 845783] (raylet) local_object_manager.cc:245: :info_message:Spilled 1032 MiB, 68 objects, write throughput 6 MiB/s.
./raylet.out:1254:[2023-12-12 21:06:59,825 I 88522 845783] (raylet) local_object_manager.cc:245: :info_message:Spilled 1088 MiB, 72 objects, write throughput 6 MiB/s.
./raylet.out:1291:[2023-12-12 21:07:09,852 I 88522 845783] (raylet) local_object_manager.cc:245: :info_message:Spilled 1152 MiB, 76 objects, write throughput 6 MiB/s.
```

It's writing at a stable 6 MiB/s which is too low and broke the test.

And I looked at a passing test (also OSX), the write throughput is (MiB/s):

```
109,98,141,153,158,161,163,163,165,165,166,167,66,51,48,68,53,49,58,68,54,51,14,130,70,97,136,140,75,113,120,124,126,127,128,130,136,137,138,139,140,74,121,114,64,116,126,136,134,132,131,74,114,124,136,91,138,153,159,161,161,161,161,161,161,140,134,132,131,77,118,123,135,78,88,133,140,78,80,129,68,65,19,149,67,118,69,54,50,54,75,55,51,72,54,50,94,20,208,63,14,111,77,80,71,105,123,141,147,92,132,141,74,119,32,29,29,137,141,19,167,74,123,49,83,123,138,142,71
```

On my laptop it ranges from 36 MiB/s to 2568 MiB/s.",always test case test one interesting log like raylet mib write throughput raylet mib write throughput raylet mib write throughput raylet mib write throughput raylet mib write throughput raylet mib write throughput raylet mib write throughput raylet mib write throughput raylet mib write throughput raylet mib write throughput raylet mib write throughput raylet mib write throughput raylet mib write throughput raylet mib write throughput raylet mib write throughput raylet mib write throughput raylet mib write throughput raylet mib write throughput raylet mib write throughput raylet mib write throughput raylet mib write throughput writing stable low broke test passing test also write throughput,issue,negative,positive,positive,positive,positive,positive
1853191658,"Yes, this is suitable.
Even though it is not very intuitive for users who haven't used Tuner before, since it is hard to find in the Trainer's document.

Both ways are reasonable to me.

If the concept that ""a single Train == a tuner Trial round"" exists, I think using TuneConfig (with or without wrapped with Tuner) would be more elegant for me.
Otherwise, if users don't need to know Tuner is underneath Trainer, maybe making this a part of Trainer's config would be more friendly.",yes suitable even though intuitive used tuner since hard find trainer document way reasonable concept single train tuner trial round think without wrapped tuner would elegant otherwise need know tuner underneath trainer maybe making part trainer would friendly,issue,positive,positive,positive,positive,positive,positive
1853149086,Somehow I am not seeing the medium A-J job started - cc @can-anyscale is it because of some optional CI flag? ,somehow seeing medium job optional flag,issue,negative,neutral,neutral,neutral,neutral,neutral
1853147319,succeeded 3 times in a row (trying 3 more times),time row trying time,issue,negative,neutral,neutral,neutral,neutral,neutral
1853135981,"`repartition` api is not optimized with streaming execution yet. All data will be loaded memory. If your purpose is to change the number of rows in each block, a workaround would be using `map_batches(lambda x: x, batch_size=N)`",repartition streaming execution yet data loaded memory purpose change number block would lambda,issue,negative,neutral,neutral,neutral,neutral,neutral
1853113360,`ray stack` is also useful for pinpointing where in python/C++ the driver and workers are hanging.,ray stack also useful driver hanging,issue,negative,positive,positive,positive,positive,positive
1853097742,"@movy It's a little hard to pinpoint the issue right now without a reliable repro -- we have this release test that runs 10k short-lived trials and has been pretty stable: https://github.com/ray-project/ray/blob/master/release/tune_tests/scalability_tests/workloads/test_bookkeeping_overhead.py#L5-L6

When the next hang happens, would it be possible for you to try figuring out where in the driver code things are hanging? The Ray Dashboard would be useful to see determine whether the actor is failing to be terminated, or if it's just the driver script getting stuck at a certain place.

Once you encounter the hang, I can also help to debug more via Slack or a call.

Also, have you tried upgrading to the latest version of Ray? In particular, Ray 2.7+ introduced some major internal refactors.",little hard pinpoint issue right without reliable release test pretty stable next would possible try driver code hanging ray dashboard would useful see determine whether actor failing driver script getting stuck certain place encounter also help via slack call also tried latest version ray particular ray major internal,issue,positive,positive,positive,positive,positive,positive
1853083300,"@edoakes Tests are passing, this change is ready to merge.",passing change ready merge,issue,negative,positive,positive,positive,positive,positive
1853071525,"@bveeramani `o2.add_input` is a `MagicMock` instance. If you access any attribute or call any 'method' on a `MagicMock` what you get is another `MagicMock` instance - and since `bool(MagicMock()) == True` this test always passes. You could call _any_ attribute of the object we are testing above and the test would pass:

![image](https://github.com/ray-project/ray/assets/14017872/5c86431a-2562-45ef-bd5c-fbac3929dd19)

So the test isn't actually doing anything at all. On the other hand [`assert_called_once_with`](https://docs.python.org/3/library/unittest.mock.html#unittest.mock.Mock.assert_called_once_with) is a _special_ attribute of `Mock` objects which actually does the test we want.",instance access attribute call get another instance since bool true test always could call attribute object testing test would pas image test actually anything hand attribute mock actually test want,issue,positive,positive,positive,positive,positive,positive
1853066992,"> Nice investigation :)! Guess we don't need to cherry pick?

yeah, maybe not a regression. ",nice investigation guess need cherry pick yeah maybe regression,issue,positive,positive,positive,positive,positive,positive
1853053058,"Kicking off mac tests as well. 

A prototype test run passes on the draft PR: https://buildkite.com/ray-project/oss-ci-build-pr/builds/44311#018c605b-2807-4b21-89e6-1c9936acf511 ",kicking mac well prototype test run draft,issue,negative,neutral,neutral,neutral,neutral,neutral
1852973244,@woshiyyya I added some extra notes talking about caveats in this section: https://anyscale-ray--41807.com.readthedocs.build/en/41807/train/getting-started-pytorch.html#set-up-a-dataset,added extra talking section,issue,negative,neutral,neutral,neutral,neutral,neutral
1852930011,im closing and marking this back to triage if it comes back up because it doesnt look too bad per latest go/flaky,marking back triage come back doesnt look bad per latest,issue,negative,negative,neutral,neutral,negative,negative
1852928577,"@jjyao Another round of release tests done.  Here is the script output from commit 0637878102f7013c9d345048665eb668c73ba38f.  


```
architkulkarni@archit-Q4WXGF2WQY release_logs % python compare_perf_metrics 2.8.0 2.9.0                                                                                            
REGRESSION 23.56%: single_client_tasks_sync (THROUGHPUT) regresses from 1161.670131632561 to 888.0015388864962 (23.56%) in 2.9.0/microbenchmark.json
REGRESSION 15.93%: actors_per_second (THROUGHPUT) regresses from 753.4446893211699 to 633.4057987254154 (15.93%) in 2.9.0/benchmarks/many_actors.json
REGRESSION 12.54%: 1_n_actor_calls_async (THROUGHPUT) regresses from 9581.728569086026 to 8379.808743224377 (12.54%) in 2.9.0/microbenchmark.json
REGRESSION 12.25%: single_client_get_object_containing_10k_refs (THROUGHPUT) regresses from 13.55352518200595 to 11.8936233279613 (12.25%) in 2.9.0/microbenchmark.json
REGRESSION 12.11%: placement_group_create/removal (THROUGHPUT) regresses from 926.0840791839338 to 813.9271849820519 (12.11%) in 2.9.0/microbenchmark.json
REGRESSION 10.53%: 1_n_async_actor_calls_async (THROUGHPUT) regresses from 8601.993472120319 to 7695.844099244959 (10.53%) in 2.9.0/microbenchmark.json
REGRESSION 9.45%: client__1_1_actor_calls_sync (THROUGHPUT) regresses from 535.3383020010909 to 484.76031562100263 (9.45%) in 2.9.0/microbenchmark.json
REGRESSION 9.41%: n_n_actor_calls_async (THROUGHPUT) regresses from 30108.565209428394 to 27274.36790185034 (9.41%) in 2.9.0/microbenchmark.json
REGRESSION 7.67%: multi_client_put_calls_Plasma_Store (THROUGHPUT) regresses from 12224.373147431208 to 11287.073436978544 (7.67%) in 2.9.0/microbenchmark.json
REGRESSION 7.24%: multi_client_tasks_async (THROUGHPUT) regresses from 27211.51041454346 to 25241.947934149597 (7.24%) in 2.9.0/microbenchmark.json
REGRESSION 6.68%: client__tasks_and_put_batch (THROUGHPUT) regresses from 11415.752622212967 to 10653.509318978906 (6.68%) in 2.9.0/microbenchmark.json
REGRESSION 5.24%: n_n_async_actor_calls_async (THROUGHPUT) regresses from 24290.541801601616 to 23017.40167315184 (5.24%) in 2.9.0/microbenchmark.json
REGRESSION 4.05%: 1_1_actor_calls_sync (THROUGHPUT) regresses from 2213.6033025230176 to 2123.910076736858 (4.05%) in 2.9.0/microbenchmark.json
REGRESSION 4.01%: single_client_put_calls_Plasma_Store (THROUGHPUT) regresses from 5697.447666436941 to 5469.112578582617 (4.01%) in 2.9.0/microbenchmark.json
REGRESSION 3.85%: 1_1_async_actor_calls_sync (THROUGHPUT) regresses from 1377.3257452550822 to 1324.3542022881197 (3.85%) in 2.9.0/microbenchmark.json
REGRESSION 3.59%: client__1_1_actor_calls_concurrent (THROUGHPUT) regresses from 1038.8711159440322 to 1001.622808984831 (3.59%) in 2.9.0/microbenchmark.json
REGRESSION 2.98%: n_n_actor_calls_with_arg_async (THROUGHPUT) regresses from 2895.292478069285 to 2808.955905323157 (2.98%) in 2.9.0/microbenchmark.json
REGRESSION 2.93%: single_client_put_gigabytes (THROUGHPUT) regresses from 20.25137053756333 to 19.657949044035732 (2.93%) in 2.9.0/microbenchmark.json
REGRESSION 2.89%: client__get_calls (THROUGHPUT) regresses from 1164.1583807193044 to 1130.514445226808 (2.89%) in 2.9.0/microbenchmark.json
REGRESSION 2.52%: single_client_tasks_and_get_batch (THROUGHPUT) regresses from 8.7124898510668 to 8.4926821108741 (2.52%) in 2.9.0/microbenchmark.json
REGRESSION 2.09%: client__put_calls (THROUGHPUT) regresses from 856.533614603169 to 838.6617053826939 (2.09%) in 2.9.0/microbenchmark.json
REGRESSION 1.09%: client__1_1_actor_calls_async (THROUGHPUT) regresses from 1036.0321459583472 to 1024.720293004619 (1.09%) in 2.9.0/microbenchmark.json
REGRESSION 159.21%: dashboard_p50_latency_ms (LATENCY) regresses from 34.774 to 90.139 (159.21%) in 2.9.0/benchmarks/many_actors.json
REGRESSION 63.67%: stage_0_time (LATENCY) regresses from 7.927043914794922 to 12.974204301834106 (63.67%) in 2.9.0/stress_tests/stress_test_many_tasks.json
REGRESSION 47.73%: dashboard_p95_latency_ms (LATENCY) regresses from 2237.99 to 3306.182 (47.73%) in 2.9.0/benchmarks/many_actors.json
REGRESSION 34.21%: dashboard_p99_latency_ms (LATENCY) regresses from 3088.301 to 4144.723 (34.21%) in 2.9.0/benchmarks/many_actors.json
REGRESSION 17.56%: avg_pg_remove_time_ms (LATENCY) regresses from 0.7885501576572757 to 0.9270061066077775 (17.56%) in 2.9.0/stress_tests/stress_test_placement_group.json
REGRESSION 11.89%: dashboard_p95_latency_ms (LATENCY) regresses from 7335.859 to 8208.055 (11.89%) in 2.9.0/benchmarks/many_tasks.json
REGRESSION 6.34%: avg_iteration_time (LATENCY) regresses from 1.5873150753974914 to 1.6879484224319459 (6.34%) in 2.9.0/stress_tests/stress_test_dead_actors.json
REGRESSION 5.80%: 3000_returns_time (LATENCY) regresses from 5.899374322999989 to 6.241540623999995 (5.80%) in 2.9.0/scalability/single_node.json
REGRESSION 4.63%: stage_3_time (LATENCY) regresses from 2943.001654624939 to 3079.2003531455994 (4.63%) in 2.9.0/stress_tests/stress_test_many_tasks.json
REGRESSION 3.28%: 10000_args_time (LATENCY) regresses from 17.66019733799999 to 18.240226582999995 (3.28%) in 2.9.0/scalability/single_node.json
REGRESSION 2.63%: avg_pg_create_time_ms (LATENCY) regresses from 0.8868904699705661 to 0.9102352252259525 (2.63%) in 2.9.0/stress_tests/stress_test_placement_group.json
```

I will continue posting here about once a day. Check https://buildkite.com/ray-project/release-tests-branch/builds?branch=releases%2F2.9.0 to see all currently running and past release tests.
",another round release done script output commit python regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression latency regression latency regression latency regression latency regression latency regression latency regression latency regression latency regression latency regression latency regression latency continue posting day check see currently running past release,issue,negative,negative,negative,negative,negative,negative
1852922596,"I mean, since in production we have the need to ask pymalloc to release memory to the OS we may need some tricks like malloc_trim.",mean since production need ask release memory o may need like,issue,negative,negative,negative,negative,negative,negative
1852916461,closing as no response - @MissiontoMars please reopen if you are able to repro and provide the relevant ,response please reopen able provide relevant,issue,negative,positive,positive,positive,positive,positive
1852896614,"Actually you can also try PYTHONMALLOC=malloc, and I also verified no leaks with this option",actually also try also option,issue,negative,neutral,neutral,neutral,neutral,neutral
1852888076,"By the way, @movy you can pass the environment variable [`RAY_TASK_MAX_RETRIES=0`](https://docs.ray.io/en/latest/ray-core/fault_tolerance/tasks.html#retrying-failed-tasks) when starting your driver to override the default number of retries. The only problem is that this will not work if libraries override the number of retries (which Tune may do internally).

It might also be helpful to find out what tasks are failing. Ray's state API might help with that. Here's the one for [tasks](https://docs.ray.io/en/latest/ray-observability/reference/doc/ray.util.state.list_tasks.html#ray.util.state.list_tasks) (actors should already be in the dashboard).",way pas environment variable starting driver override default number problem work override number tune may internally might also helpful find failing ray state might help one already dashboard,issue,negative,neutral,neutral,neutral,neutral,neutral
1852884060,"Linkcheck is failing on the release branch. E.g. https://buildkite.com/ray-project/oss-ci-build-branch/builds/7224#018c5fda-9f97-404d-944f-5ea40ecd290b/500-922

```

ERROR: Cannot install Pygments==2.13.0 and Pygments==2.16.1 because these package versions have conflicting dependencies.
--
  |  
  | The conflict is caused by:
  | The user requested Pygments==2.13.0
  | The user requested Pygments==2.16.1
  | The user requested (constraint) pygments==2.13.0
  |  
  | To fix this you could try to:
  | 1. loosen the range of package versions you've specified
  | 2. remove package versions to allow pip attempt to solve the dependency conflict
  |  
  | ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts
```

Based on the above, it looks like the setup fails so the job doesn't run.  

@can-anyscale is this a release blocker, or do we just ignore the linkcheck job for the 2.9.0 release?",failing release branch error install package conflicting conflict user user user constraint fix could try loosen range package remove package allow pip attempt solve dependency conflict error help visit based like setup job run release blocker ignore job release,issue,negative,neutral,neutral,neutral,neutral,neutral
1852880788,"It looks like there are actually two issues here:
1. Ray Core is crashing, possibly on a bad assertion.
2. Ray Tune/Train is hanging even though max_retries=0.

I can look into 1 but I don't know about 2. @matthewdeng can you find someone for 2?",like actually two ray core possibly bad assertion ray hanging even though look know find someone,issue,negative,negative,negative,negative,negative,negative
1852852750,"@peytondmurray yes, there's a tag we need to remove to turn this on premerge",yes tag need remove turn,issue,negative,neutral,neutral,neutral,neutral,neutral
1852843244,@briandamaged Thanks for filing the issue! The best fix would be to update the job submission API to be compatible with `ray.init()`'s behavior.  If would be great if you could open a PR!,thanks filing issue best fix would update job submission compatible behavior would great could open,issue,positive,positive,positive,positive,positive,positive
1852730897,"I wonder if we can try this [1]: when we invoke gc.collect(), also invoke a `malloc_trim`. I am not sure this would work or not. We can do an experiment for it


https://www.softwareatscale.dev/p/run-python-servers-more-efficiently",wonder try invoke also invoke sure would work experiment,issue,negative,positive,positive,positive,positive,positive
1852711515,interesting - i would expect both oss/runtime affected by this change assuming they are essentially the same test/code base/test runner?,interesting would expect affected change assuming essentially runner,issue,negative,positive,positive,positive,positive,positive
1852707634,"@HannahAlexander You'll want to ""tune"" over the ""distributed trainer"".

```python
trainer = TorchTrainer(
    ..., scaling_config=ray.train.ScalingConfig(num_workers=4, use_gpu=True
)
tuner = ray.tune.Tuner(
    trainer,
    param_space={""train_loop_config"": {""lr"": tune.grid_search([0.1, 0.01, 0.001])}}
)
tuner.fit()  # 3 x 4 GPUs needed to run all trials concurrently
```

See this user guide for more explanation: https://docs.ray.io/en/latest/train/user-guides/hyperparameter-optimization.html",want tune distributed trainer python trainer tuner trainer run concurrently see user guide explanation,issue,negative,neutral,neutral,neutral,neutral,neutral
1852703657,"@HannahAlexander It looks like you're trying to pass a `DataLoader` as the argument to `train_datset` in the transformers trainer. The `Trainer` only takes in a `torch.utils.data.Dataset` or `torch.utils.data.IterableDataset`. [See here.](https://huggingface.co/docs/transformers/v4.36.0/en/main_classes/trainer#transformers.Trainer.train_dataset)

If you really need to pass in a torch DataLoader, it looks like transformers lets you override this `get_train_dataloader` method of the `Trainer` class: https://huggingface.co/docs/transformers/v4.36.0/en/main_classes/trainer#transformers.Trainer.get_train_dataloader",like trying pas argument trainer trainer see really need pas torch like override method trainer class,issue,negative,positive,positive,positive,positive,positive
1852698464,@xieus could you please help find an assignee for this issue?,could please help find assignee issue,issue,positive,neutral,neutral,neutral,neutral,neutral
1852678550,@Han-taz Which example are you running into difficulties with? (could you provide a link/code snippet?),example running could provide snippet,issue,negative,neutral,neutral,neutral,neutral,neutral
1852618669,"Hi, I’m not working on this. It’d be great if you can submit a patch!

On Tue, Dec 12, 2023 at 9:41 AM Huaiwei Sun ***@***.***>
wrote:

> I'm fine with changing it for the 1st graph. The other two cards show the
> real-time info and we shouldn't modify them
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/ray-project/ray/issues/41781#issuecomment-1852510892>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAFNZ766QWW3S2U4WZFWIZ3YJCJMRAVCNFSM6AAAAABAOSBXPCVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTQNJSGUYTAOBZGI>
> .
> You are receiving this because you were mentioned.Message ID:
> ***@***.***>
>
",hi working great submit patch tue sun wrote fine st graph two show modify reply directly view id,issue,positive,positive,positive,positive,positive,positive
1852616230,"@daviddwlee84 Is this workaround suitable for now?  https://github.com/ray-project/ray/issues/40531#issuecomment-1828536267

Also open to adding this experimental config if you prefer to stick with working with the `Trainer` only.",suitable also open experimental prefer stick working trainer,issue,negative,positive,positive,positive,positive,positive
1852587587,@can-anyscale I don't see the linkcheck job running here. Did I get the configuration right?,see job running get configuration right,issue,negative,positive,positive,positive,positive,positive
1852514950,"Hi, want to quick update on this. So we have REP and prototype ready for review. Please try out and leave feedback!
Prototype: https://github.com/ray-project/ray/pull/41147
REP: https://github.com/ray-project/enhancements/pull/47",hi want quick update rep prototype ready review please try leave feedback prototype rep,issue,positive,positive,positive,positive,positive,positive
1852510892,I'm fine with changing it for the 1st graph. The other two cards show the real-time info and we shouldn't modify them,fine st graph two show modify,issue,negative,positive,positive,positive,positive,positive
1852485734,"Oh, I see. Is there a framework that includes Particle Swarm Optimization or Genetic Algorithm?",oh see framework particle swarm optimization genetic algorithm,issue,negative,neutral,neutral,neutral,neutral,neutral
1852339632,@angelinalg could you please give codeowner approval to this PR?,could please give approval,issue,positive,neutral,neutral,neutral,neutral,neutral
1852216324,"Ah it is due to changes from log API.

@rickyyx do you know which API we should use in this case? ",ah due log know use case,issue,negative,negative,negative,negative,negative,negative
1852125741,"This issue is a collection of multiple issues. 

1. When there are lots of custom metrics with high cardinality, it can increase the agent memory usage.
2. Dashboard agent may have memory leak when cleaning up metrics.

1 is mitigated from the application side already, and they are planning to test soon.
2. is something I tried reproducing. I basically created a script which creates a lot of custom metrics.

```python3
import ray
from ray.util.metrics import Histogram
import time

ray.init()

DEFAULT_LATENCY_BUCKET_MS = [
    i for i in range(1, 1000)
]

@ray.remote
class Actor:
    def __init__(self):
        self.hists = [
            Histogram(
                    f""{i}"",
                    description=(""my histogram ""),
                    boundaries=DEFAULT_LATENCY_BUCKET_MS,
                    tag_keys=())
            for i in range(100)
        ]

    def record(self):
        for hist in self.hists:
            for i in DEFAULT_LATENCY_BUCKET_MS:
                hist.observe(i)

while True:
    actors = [Actor.remote() for _ in range(40)]
    ray.get([actor.record.remote() for actor in actors])
    time.sleep(5)
    del actors
```

And run ray agent with very small metrics export interval (by default, 10 seconds, and I used 10ms).

Investigation
-------------
From OSX and Linux, I observed memory leak-looking symptoms initially, but it turns out not to be a memory leak.

From htop. it shows the high resident size initially, but it eventually shrinked. For OSX, resident size didn't decrease, but not every allocator free the memory immediately. My guess is OSX allocator doesn't free actively although memory is freed.

I verified heap size eventually shrinks for both OS. See the screenshots below.

After that, I ran memray profiler to verify leaks. I verified heap size eventually shrinks for both OS. See the screenshots below.

OSX
<img width=""1728"" alt=""Screenshot 2023-12-12 at 11 14 18 PM"" src=""https://github.com/ray-project/ray/assets/18510752/1f87c82d-73de-4f1b-9fca-7c3e509248f4"">

Linux 
<img width=""1719"" alt=""Screenshot 2023-12-12 at 11 14 39 PM"" src=""https://github.com/ray-project/ray/assets/18510752/11ed02c4-855f-426b-957d-40c535a82bef"">

Note that heap size is still 200MB (which is bigger than expected). It was due to Python allocator (see details here https://bloomberg.github.io/memray/python_allocators.html#). When I tracked the python allocator / deallocator using --trace-python-allocator, I verified the code doesn't have any leak and deallocation are properly called. 

<img width=""1728"" alt=""Screenshot 2023-12-12 at 11 45 58 PM"" src=""https://github.com/ray-project/ray/assets/18510752/e911eed0-4a00-4705-a675-35b032ea48ec"">

Conclusion
-----------
There's no leak from existing dashboard agent code when metrics are deleted. Thus I will remove the release blocker. We can close the issue once cardinality reduction from Ray serve solves endpoint problem. 

Follow up
----------
We still need a couple of follow up for metrics. This will be more of P1 in Core team's future tasks, but we don't currently have bandwidth to handle them. 

- Need actual memory leak testing
- When cardinality is very high, we should overwrite the tag to reduce cardinality (instead of exploding the dashboard agent).
- We should report volume of metrics generated. 

",issue collection multiple lot custom metric high increase agent memory usage dashboard agent may memory leak cleaning metric application side already test soon something tried basically script lot custom metric python import ray import histogram import time range class actor self histogram histogram range record self hist true range actor run ray agent small metric export interval default used investigation memory initially turn memory leak high resident size initially eventually resident size decrease every allocator free memory immediately guess allocator free actively although memory freed heap size eventually o see ran profiler verify heap size eventually o see note heap size still bigger due python allocator see tracked python allocator code leak properly conclusion leak dashboard agent code metric thus remove release blocker close issue reduction ray serve problem follow still need couple follow metric core team future currently handle need actual memory leak testing high overwrite tag reduce instead dashboard agent report volume metric,issue,negative,positive,neutral,neutral,positive,positive
1852122253,"FYI: I'm happy to implement a PR to address this! However, I just need to know which fix is compatible w/ the project's longer-term plans.",happy implement address however need know fix compatible project,issue,positive,positive,positive,positive,positive,positive
1851964925,"@edoakes @GeneDer Thanks for the reviews, I've addressed your comments - could you take another look?",thanks could take another look,issue,negative,positive,positive,positive,positive,positive
1851800196,"> Are you referring to the first graph in the screenshot? @jiwq
> 
> ![Image](https://private-user-images.githubusercontent.com/9677264/289702771-2ebb22c3-9843-4b49-aacb-6f3cb674b559.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDIzNzg0MDUsIm5iZiI6MTcwMjM3ODEwNSwicGF0aCI6Ii85Njc3MjY0LzI4OTcwMjc3MS0yZWJiMjJjMy05ODQzLTRiNDktYWFjYi02ZjNjYjY3NGI1NTkucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQUlXTkpZQVg0Q1NWRUg1M0ElMkYyMDIzMTIxMiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyMzEyMTJUMTA0ODI1WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ODU3ZGEzYjRiMTQ0OWMzMjkzNDY4ZGY5OTcwNWE2MTViMGJlYjI0ODRmNzJlNzMxZjY5YmFlZjdlZGNkODJmOSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.Q7ghJomyRabrMqiH4xfPYfW6mAim7Fi_xSyyAOBkh-g)

Yes. I think all the cards in this section should be limit in the job running time range. But the other two cards should change the job structure, so maybe we can simply fix it and add new patch for other.",first graph image yes think section limit job running time range two change job structure maybe simply fix add new patch,issue,negative,positive,positive,positive,positive,positive
1851733282,"Found printing the `path` in the `XGBoostTrainer._save_model` shows dynamically generated temporary directory path like `/tmp/tmppbsxfulk`. (Seems this issue belongs to Ray 2.8+)

https://github.com/ray-project/ray/blob/ee10ea692a0fc8863589411c39a7175aae3fdb06/python/ray/train/gbdt_trainer.py#L272-L288

Haven't found where Ray actually dumped the model itself

Maybe controlled by the StorageContext. Not sure.

https://github.com/ray-project/ray/blob/ee10ea692a0fc8863589411c39a7175aae3fdb06/python/ray/train/gbdt_trainer.py#L288

https://github.com/ray-project/ray/blob/ee10ea692a0fc8863589411c39a7175aae3fdb06/python/ray/train/_internal/session.py#L244",found printing path dynamically temporary directory path like issue ray found ray actually model maybe sure,issue,positive,positive,positive,positive,positive,positive
1851570566,base branch was the previous PR instead of master so the rebase made it look really big,base branch previous instead master rebase made look really big,issue,negative,negative,negative,negative,negative,negative
1851399461,"@zhe-thoughts this doesn't need to block the release. That said, given that it's a documentation-only change, it should be low risk to merge.

For context: in Ray 2.8, we removed the example of how to implement a custom datasource (https://github.com/ray-project/ray/pull/40127). And, in Ray 2.9, we made several changes to the datasource interface. So, there's currently a gap in our documentation, and this PR addresses it.",need block release said given change low risk merge context ray removed example implement custom ray made several interface currently gap documentation,issue,negative,neutral,neutral,neutral,neutral,neutral
1851394849,@bveeramani @c21 Please help me understand why this is a release blocker. Thanks,please help understand release blocker thanks,issue,positive,positive,positive,positive,positive,positive
1851341101,"ah got you, let me reopen this then, still failing pretty consistently on release branch",ah got let reopen still failing pretty consistently release branch,issue,negative,positive,positive,positive,positive,positive
1851339080,"Hi @can-anyscale , I don't pick up any change yet. (I pretty sure the bisect result is not correct :) )",hi pick change yet pretty sure bisect result correct,issue,positive,positive,positive,positive,positive,positive
1851328330,This is also failing on release branch. Have we pick the fix yet @sihanwang41 ? Thanks,also failing release branch pick fix yet thanks,issue,negative,positive,positive,positive,positive,positive
1851277963,Sound good. We should also try to get this test use rayci if possible. ,sound good also try get test use possible,issue,negative,positive,positive,positive,positive,positive
1851220984,"> We confirmed the behavior and we think it's a bug.
> 
> The right behavior should be that if `ray stop` gracefully shutdown every Ray process, the exit code of `ray start --head --block` should be 0 and if `ray stop --force`, the exit code should be non-zero. @MissiontoMars do you think this is the reasonable behavior.

That make sence, thanks. Which version is expected to fix this?",confirmed behavior think bug right behavior ray stop gracefully shutdown every ray process exit code ray start head block ray stop force exit code think reasonable behavior make sence thanks version fix,issue,negative,positive,positive,positive,positive,positive
1851182752,"Ah this seems like a duplicate of https://github.com/ray-project/ray/issues/40531

@justinvyu do you think it would make sense to start with a simple experimental config?",ah like duplicate think would make sense start simple experimental,issue,negative,positive,neutral,neutral,positive,positive
1851164062,"Got it. The logic is indeed quite convoluted. There are actually 3 factors that may affects the shuffling behavior.

- If the user specified a `DistributedSampler` in the `DataLoader`
- `IterableDataset` or `Dataset`
- `DataLoader(shuffle = True or False)`

~~Shall we have a table in the docstring to illustrate the priority of these factors?~~

We can mention that if the users provided a `DistributedSampler`, then Ray Train will not add a new sampler, and the shuffling behavior will go with the user's config.",got logic indeed quite convoluted actually may shuffling behavior user shuffle true false table illustrate priority mention provided ray train add new sampler shuffling behavior go user,issue,negative,positive,neutral,neutral,positive,positive
1851154830,"@woshiyyya If the dataloader already has a `DistributedSampler(shuffle=True)` attached, then `prepare_data_loader` is a noop, so it will respect the user's custom config. 

https://github.com/ray-project/ray/blob/bb719efa0a952390f3b77134655e3b31ff52b419/python/ray/train/torch/train_loop_utils.py#L429",already attached noop respect user custom,issue,negative,neutral,neutral,neutral,neutral,neutral
1851152018,"It seems Trainer's `fit()` creates Tuner but has no user-defined TuneConfig passed

Maybe we can have `trial_dirname_creator` in the `RunConfig` or be able to pass `TuneConfig` in the Trainer's `fit()`

Like

```python
    def fit(self, tune_config: Optional[TuneConfig] = None) -> Result:
        ...

        if self._restore_path:
            ...
        else:
            tuner = Tuner(
                tune_config=tune_config,
                ...
            )
```

---

https://github.com/ray-project/ray/blob/171b828177420b2530d9fbe07f6914cdbf6ee0ba/python/ray/train/base_trainer.py#L578-L617

https://github.com/ray-project/ray/blob/171b828177420b2530d9fbe07f6914cdbf6ee0ba/python/ray/tune/impl/tuner_internal.py#L430-L451

https://github.com/ray-project/ray/blob/171b828177420b2530d9fbe07f6914cdbf6ee0ba/python/ray/train/_internal/storage.py#L660-L671
",trainer fit tuner maybe able pas trainer fit like python fit self optional none result else tuner tuner,issue,positive,positive,positive,positive,positive,positive
1851138332,"> However, with Ray Train, since this prepare_data_loader utility injects the DistributedSampler for the user, there's no visibility on the shuffle parameter. Ray Train will detect the shuffle parameter set on the original dataloader, then pass that along to the DistributedSampler. So, it's not possible to have this False+True situation.

What would Ray Train do if a user does have `shuffle=False` but `Sampler(shuffle=True)`?",however ray train since utility user visibility shuffle parameter ray train detect shuffle parameter set original pas along possible situation would ray train user sampler,issue,negative,positive,positive,positive,positive,positive
1851138068,"Seems Ray Trainer creates TensorBoard checkpoint in a similar way ([ray.train.RunConfig — Ray 2.8.1](https://docs.ray.io/en/latest/train/api/doc/ray.train.RunConfig.html#ray.train.RunConfig)) e.g. `storage_path/experiment_name/TrainerClassName_unique_id_00000_0_datetime`

Do we have a similar config like `trial_dirname_creator` in [ray.tune.TuneConfig — Ray 2.8.1](https://docs.ray.io/en/latest/tune/api/doc/ray.tune.TuneConfig.html) for Trainer?

Not found in any of the Train Configs [Ray Train API — Ray 2.8.1](https://docs.ray.io/en/latest/train/api/api.html#ray-train-configuration)",ray trainer similar way ray similar like ray trainer found train ray train ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1851132559,@kaya you mean you run ray start multiple times within the same instance right (for parallel unit test or sth like that)?,mean run ray start multiple time within instance right parallel unit test like,issue,negative,negative,neutral,neutral,negative,negative
1851093552,"The exception was `pyarrow.lib.ArrowInvalid: Referenced field image was extension<arrow.py_extension_type<ArrowTensorType>> but should have been extension<arrow.py_extension_type<ArrowTensorType>>`, which is weird because the two types are the same. I think it could have something to do with https://github.com/apache/arrow/pull/38608 (Nov 6), which was fixed by #41036 (Nov 20) (CUJ was on Nov 15). Although its still weird that the dataset could execute. I can't seem to reproduce this again.",exception field image extension extension weird two think could something fixed although still weird could execute ca seem reproduce,issue,negative,negative,negative,negative,negative,negative
1851069599,"Hi @architkulkarni , I updated the doc and submitted a MR. Could you please review it?",hi doc could please review,issue,negative,neutral,neutral,neutral,neutral,neutral
1851060214,"- Fix doc build
- Skip memory_pressure test on premerge, pipe data to go/flaky

Let's keep the //bk to go/flaky fix separate from this fix",fix doc build skip test pipe data let keep fix separate fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1851056236,Ray 2.8.1 was released. Please try it out. Thanks for reporting @DanMcInerney and please reopen this issue as you see fit,ray please try thanks please reopen issue see fit,issue,positive,positive,positive,positive,positive,positive
1851008394,We will include this work as part of Ray multi-tenancy project.,include work part ray project,issue,negative,neutral,neutral,neutral,neutral,neutral
1851005164,"@lanbochen-anyscale, could you tell us the urgency and importance of this feature request?",could tell u urgency importance feature request,issue,negative,neutral,neutral,neutral,neutral,neutral
1851004076,"discussed with train team, action items:
- @woshiyyya will move it out of doctests into a formal train test, so we can get actual failure errors
- if needed, I will assist with debugging the data failure",train team action move formal train test get actual failure assist data failure,issue,negative,negative,negative,negative,negative,negative
1851003413,Also are you able to try kuberay which is the recommended way to run Ray cluster with dockers and k8s.,also able try way run ray cluster,issue,negative,positive,positive,positive,positive,positive
1851001648,"@lsc64 can you check to make sure the head node and the worker node can talk to each other. From the error message, seems there are communication issues between the two nodes/dockers.",check make sure head node worker node talk error message communication two,issue,negative,positive,positive,positive,positive,positive
1850996332,"If you don't need Ray job submission or runtime env, you don't need to use `ray[default]`.

Also can you provide a repro so that we can debug on our side? Also can you show the full stack trace?",need ray job submission need use ray default also provide side also show full stack trace,issue,negative,positive,positive,positive,positive,positive
1850994240,"@mkolyaei , as a workaround can you install a version of pydantic between >= 1.9 and < 2.5?

In the latest master, we've updated this pydantic compatibility logic so starting in ray 2.9.0, you should not run into this.",install version latest master compatibility logic starting ray run,issue,negative,positive,positive,positive,positive,positive
1850984601,@andrecunha could you try using the nightly version of Ray? This should be fixed by https://github.com/ray-project/ray/pull/41036 in Ray 2.9,could try nightly version ray fixed ray,issue,negative,positive,neutral,neutral,positive,positive
1850981514,"> this assume these operator would only use some small key such as 'id', 'date', and rows num is relatively small
For this, another possible optimization could be to filter the input dataset by selecting only the relevant columns, and excluding large-size columns.

> In my use case, I need to set some 'small index info' for every row/block in block metadata,not sure this is the way ReadTask metadata to go

For this, our BlockMetadata currently doesn't have a great abstraction to store per-row information (I don't think this is planned on our roadmap). If you have changes that you believe would be helpful for the broader community and would like to contribute in a PR, feel free to start the process for that as well.

",assume operator would use small key relatively small another possible optimization could filter input relevant excluding use case need set index every block sure way go currently great abstraction store information think believe would helpful community would like contribute feel free start process well,issue,positive,positive,positive,positive,positive,positive
1850977252,"We confirmed the behavior and we think it's a bug.

The right behavior should be that if `ray stop` gracefully shutdown every Ray process, the exit code of `ray start --head --block` should be 0 and if `ray stop --force`, the exit code should be non-zero. @MissiontoMars do you think this is the reasonable behavior.

",confirmed behavior think bug right behavior ray stop gracefully shutdown every ray process exit code ray start head block ray stop force exit code think reasonable behavior,issue,negative,positive,positive,positive,positive,positive
1850953530,"> Where is the page that's being removed?

The old example was removed in early October https://github.com/ray-project/ray/pull/40127/files#diff-bdb1b1b7bd5ed18cb40f385021de9e23dfdb78061615a5490b3ca20fc82098f5.",page removed old example removed early,issue,negative,positive,neutral,neutral,positive,positive
1850782537,"This is necessary to fully support #41219. Otherwise, if the user sets a number of rows that is too large (larger than target_max_block_size), we are very likely to OOM.",necessary fully support otherwise user number large likely,issue,positive,positive,neutral,neutral,positive,positive
1850764503,"This issue should be resolved from #40986, could you try using the latest dev build from master to verify? Or you could wait for the fix to be included in the next version Ray 2.9, to be released in 1-2 weeks. ",issue resolved could try latest dev build master verify could wait fix included next version ray,issue,negative,positive,positive,positive,positive,positive
1850642303,"Sounds good. Everything looks good, will merge after we have the first ""green commit"" on the release branch",good everything good merge first green commit release branch,issue,positive,positive,positive,positive,positive,positive
1850571537,"The failing test has been failing on trunk.
<img width=""593"" alt=""Screenshot 2023-12-11 at 9 45 54 AM"" src=""https://github.com/ray-project/ray/assets/7005244/f5dbfb44-8793-400b-92df-263213eb8161"">
",failing test failing trunk,issue,negative,neutral,neutral,neutral,neutral,neutral
1850404803,"> Right now Ray Data is focused on offline batch inference and online preprocessing for training. Both of these read from bounded datasets, so we're not really focused on the streaming problem at the moment.
> 
> External contributions or designs on this would be welcome, however. The Ray Data execution model supports reading from unbounded sources in principle, it's just not exposed in the API.

i have not found any unbounded datasource api in the recently source code, can you give me a hint? @ericl ",right ray data batch inference training read bounded really streaming problem moment external would welcome however ray data execution model reading unbounded principle exposed found unbounded recently source code give hint,issue,negative,positive,positive,positive,positive,positive
1850343785,"Code examples: https://anyscale-ray--41783.com.readthedocs.build/en/41783/serve/advanced-guides/grpc-guide.html#use-grpc-context

Docs: https://anyscale-ray--41783.com.readthedocs.build/en/41783/serve/api/doc/ray.serve.grpc_util.RayServegRPCContext.html

Note: those docs are mostly copy from https://grpc.github.io/grpc/python/_modules/grpc.html#ServicerContext",code note mostly copy,issue,negative,positive,positive,positive,positive,positive
1850337668,"I applied a similar fix with the TensorboardX logger. In my case, the media metrics are being compiled into a list within the `summarize_episodes` function, as shown [here](https://github.com/ray-project/ray/blob/master/rllib/evaluation/metrics.py#L195). Consequently, in `TBXLoggerCallback`, I had to retrieve the last element from this list prior to invoking the `add_image` method.
Could you recommend a more efficient approach for this situation, similar to how you directly incorporated the image numpy array in your solution? I would appreciate your advice. Thank you.",applied similar fix logger case medium metric list within function shown consequently retrieve last element list prior method could recommend efficient approach situation similar directly incorporated image array solution would appreciate advice thank,issue,positive,positive,neutral,neutral,positive,positive
1850272428,"> Can this be resolved now @zcin

@akshay-anyscale No, deployments are still shown as `UPDATING` when controller is recovering from a crash. From a UX perspective, this is slightly improved now because we added replica states to `serve status`, so it will also show that all replicas are in the state `RECOVERING`. However deployment status will still be `UPDATING`; if this is confusing to users, I think the solution is to add a new deployment status `RECOVERING`.",resolved still shown controller crash perspective slightly added replica serve status also show state however deployment status still think solution add new deployment status,issue,negative,negative,neutral,neutral,negative,negative
1850189713,Verified the same code doesn't work in Ray 2.6 (meaning it is not regression),code work ray meaning regression,issue,negative,neutral,neutral,neutral,neutral,neutral
1850173042,"> hmmm, i might be misunderstanding the worker setup hook's usecase, but Isn't the worker setup function expected to be run on the remote cluster tho? Like it's just some part of code in the entrypoint script moved before task is run?

@rickyyx let's sync up this offline? I think it is a bit weird you are defining the setup function for your driver inside ""job submission script"". IMO the setup function should reside in the driver code, not in the submission code. 

I.e., 
1. If you submit a job via SDK, you should define the module name
2. If you want to directly insert a function, it should be inside ray.init() ",might misunderstanding worker setup hook worker setup function run remote cluster tho like part code script task run let sync think bit weird setup function driver inside job submission script setup function reside driver code submission code submit job via define module name want directly insert function inside,issue,negative,negative,negative,negative,negative,negative
1850111676,"Also linked to : #40626 #40777 and #37515 

Documentation should clearly explain how to do that",also linked documentation clearly explain,issue,negative,positive,positive,positive,positive,positive
1849076248,"I found an additional requirement when query ready state asynchronously, we also need know whether the object ref is empy? When the call is return, the obj.is_nil() can used to check whether this object contains no value, correspond to 'None' in python. The use case thus updated as:

```python
    for _ in range(1000):
        object = return_big_data.remote()
        try:
            await object.ready()
            if object.is_nil():
                ...
        except RuntimeError as ex:
            if isinstance(ex, ...):
                continue
            else
                break
```

Please update the null state at return. @jjyao , Thanks a lot!
",found additional requirement query ready state also need know whether object ref call return used check whether object value correspond python use case thus python range object try await except ex ex continue else break please update null state return thanks lot,issue,positive,positive,positive,positive,positive,positive
1849069735,"added back the unused asioclients, apparently we need them to stay alive",added back unused apparently need stay alive,issue,negative,positive,neutral,neutral,positive,positive
1848907090,"For now, one workaround is to comment out [this build configuration line](https://github.com/ray-project/ray/blob/5edabc7b2f92712982b0e590f739eb0110d80176/.bazelrc#L47) and then run `pip install -e . --verbose` as usual. The warnings still show up, but the build proceeds. I'm not sure what the consequences are, but I verified that I can run a simple Ray program:

```
(ae) shrekris@Shreyas-MacBook-Pro python % python
Python 3.8.12 | packaged by conda-forge | (default, Sep 29 2021, 19:21:23) 
[Clang 11.1.0 ] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import ray
>>> from ray import serve
>>> ray.init()
2023-12-10 01:24:16,032	INFO worker.py:1746 -- Started a local Ray instance. View the dashboard at http://127.0.0.1:8265 
RayContext(dashboard_url='127.0.0.1:8265', python_version='3.8.12', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}', protocol_version=None)
>>> @ray.remote
... def f():
...     return ""hello""
... 
>>> ray.get(f.remote())
'hello'
```",one comment build configuration line run pip install verbose usual still show build proceeds sure run simple ray program ae python python python default clang type help copyright license information import ray ray import serve local ray instance view dashboard dev return hello,issue,positive,positive,neutral,neutral,positive,positive
1848884124,Seems ok now. The rest failure is because of the trunk failure.,rest failure trunk failure,issue,negative,negative,negative,negative,negative,negative
1848872472,"Hi, I have faced the same issue. let's see if we get any help from the team here.. just tagging @richardliaw for quick response.",hi faced issue let see get help team quick response,issue,negative,positive,positive,positive,positive,positive
1848841103,"I feel like it is just this PR actually. Maybe trying reverting it? 

![Image](https://github.com/ray-project/ray/assets/18510752/2aad2fef-ebed-4536-be0e-e013d3ec0677)

",feel like actually maybe trying image,issue,negative,neutral,neutral,neutral,neutral,neutral
1848840248,"This is a feature requested from Ray data team.

It took a bit more time than we expected due to complex codebase",feature ray data team took bit time due complex,issue,negative,negative,negative,negative,negative,negative
1848818364,Some test might fail. I'll update them. Waiting for test.,test might fail update waiting test,issue,negative,negative,negative,negative,negative,negative
1848608187,@rynewang @rkooo567 @jjyao Could you help me understand why it's important to get this in 2.9.0 (instead of 2.10)?,could help understand important get instead,issue,positive,positive,positive,positive,positive,positive
1848597688,"> @sihanwang41 @edoakes @akshay-anyscale Could you help me understand why this is a release blocker? Thanks

See my comment above",could help understand release blocker thanks see comment,issue,positive,positive,positive,positive,positive,positive
1848380587,seems that this will boil the ocean.. let me split this into smaller PR's.,boil ocean let split smaller,issue,negative,neutral,neutral,neutral,neutral,neutral
1848303952,@jjyao Thank you for reply. I will try your solution and keep you posted with the result.,thank reply try solution keep posted result,issue,positive,neutral,neutral,neutral,neutral,neutral
1848226338,@aslonnie ah i removed the sudo and forget that change; will fix this and update on master,ah removed forget change fix update master,issue,negative,neutral,neutral,neutral,neutral,neutral
1848225636,@sihanwang41 @edoakes @akshay-anyscale Could you help me understand why this is a release blocker? Thanks,could help understand release blocker thanks,issue,positive,positive,positive,positive,positive,positive
1848077761,"RTD looks like it failed here, but if you click on the link it looks like the docs built successfully :/. It also clearly didn't reach the 1h time limit, not sure what's happening here. cc @angelinalg I know you've seen this before as well.",like click link like built successfully also clearly reach time limit sure happening know seen well,issue,positive,positive,positive,positive,positive,positive
1848060586,"<div><img src=""https://media0.giphy.com/media/nJHGSMfWQ1olVfiWZf/200.gif?cid=5a38a5a2hdwjl4nnw3uh8gof7p7yf1vlbku58vjt45hskmg4&amp;ep=v1_gifs_search&amp;rid=200.gif&amp;ct=g"" style=""border:0;height:168px;width:300px""/><br/>via <a href=""https://giphy.com/agt/"">America&#x27;s Got Talent</a> on <a href=""https://giphy.com/gifs/agt-nbc-season-18-americas-got-talent-nJHGSMfWQ1olVfiWZf"">GIPHY</a></div>",div border height width via got talent,issue,negative,neutral,neutral,neutral,neutral,neutral
1848023877,"Yess, but bisect couldn't find anything;  could be some external changes break this",bisect could find anything could external break,issue,negative,neutral,neutral,neutral,neutral,neutral
1848014014,"Thanks for the additional info and the discussion. We'll see if we can reproduce this using your latest steps and try to find the root cause.  We should be able to fix it internally without needing to expose a new API `worker_stop_commands`, but if there are enough use cases for it we can consider adding the new API as well.",thanks additional discussion see reproduce latest try find root cause able fix internally without needing expose new enough use consider new well,issue,positive,positive,positive,positive,positive,positive
1848011855,This actually looks like a real bug,actually like real bug,issue,negative,positive,neutral,neutral,positive,positive
1847993914,I believe the failing exploding-death-star tests were signed off by @sven1977 except for the `exploding-death-star-rllib-contrib-qmix-tests`. I presume this is also a flakey test. Could someone from RLlib confirm?,believe failing except presume also test could someone confirm,issue,negative,neutral,neutral,neutral,neutral,neutral
1847987371,@zhe-thoughts shortened it. Should i just replicate the title from the original one?,replicate title original one,issue,negative,positive,positive,positive,positive,positive
1847981117,"Hmm, would be good to figure this out, since in principle you could have a complex DAG of tasks running for a long time when it should have failed quickly due to an exception raised by a single task.
That could lead to waste of computational and human resources.",would good figure since principle could complex dag running long time quickly due exception raised single task could lead waste computational human,issue,negative,negative,neutral,neutral,negative,negative
1847979560,"Awesome job! It was harder than we originally thought, but I think we got the good working solution! ",awesome job harder originally thought think got good working solution,issue,positive,positive,positive,positive,positive,positive
1847959479,"There's no action item for us. We are waiting for grpc to improve the perf (it is planned on their side). 

We will mark it P2 just for tracking purpose",action item u waiting improve side mark purpose,issue,negative,positive,neutral,neutral,positive,positive
1847958844,I will downgrade all v1 autoscaling issue to p2. please make surer all relevant bugs are fixed in v2!,downgrade issue please make relevant fixed,issue,negative,positive,positive,positive,positive,positive
1847952952,"This is slightly difficult to fix. The problem is submitter should wait until both a and b are ready, and we can check the value (which contains the exception) only at the executor (because getting the value requires to pull the object), which means we have to wait until both a and b are ready. 

If we can check if the ref is an exception with small overhead, we can easily fix it, but this has not been prioritized yet. ",slightly difficult fix problem submitter wait ready check value exception executor getting value pull object wait ready check ref exception small overhead easily fix yet,issue,positive,positive,neutral,neutral,positive,positive
1847947416,"> I think unless you set all the ports manually, there's always a small possibility of conflict (unless we start dashboard_agent before starting other procs, but that's not the case). So the ideal case is to set all ports manually.

It's not possible to set all ports manually when Ray runs in a multi-tenant CI/CD setup.",think unless set manually always small possibility conflict unless start starting case ideal case set manually possible set manually ray setup,issue,negative,positive,positive,positive,positive,positive
1847906343,"> the default is 30s and it can be set for each deployment via the graceful_shutdown_timeout_s parameter

Awesome! As long as it is configurable, this is great! 

You can also check to see if there are any running requests on the replica and if there are none, the replica can be killed. Although I'm not totally sure if the replica will continue receiving requests if it is unhealthy or if there are other reasons for graceful shutdown.",default set deployment via parameter awesome long great also check see running replica none replica although totally sure replica continue unhealthy graceful shutdown,issue,positive,positive,positive,positive,positive,positive
1847904681,Note: there's no evidence of this on https://flaky-tests.ray.io/ so it's easy for this to get hidden. It must be failing before https://flaky-tests.ray.io/ can track it.,note evidence easy get hidden must failing track,issue,negative,positive,positive,positive,positive,positive
1847904003,"Echoing @GeneDer, this is to unblock a key user and the risk is low due to it primarily touching this nascent feature.",unblock key user risk low due primarily touching nascent feature,issue,negative,positive,neutral,neutral,positive,positive
1847902990,@smit-kiri the default is 30s and it can be set for each deployment via the `graceful_shutdown_timeout_s` parameter,default set deployment via parameter,issue,negative,neutral,neutral,neutral,neutral,neutral
1847902673,Thanks @zcin -- I just realized my approval is irrelevant for the process so I've stopped hitting approve for cherry picks. It just needs approval from @zhe-thoughts and a TL.,thanks approval irrelevant process stopped approve cherry need approval,issue,positive,negative,negative,negative,negative,negative
1847896360,I like this idea! Although this could mean a longer downtime in case ALL replicas for a deployment are unhealthy. What's the default for graceful_shutdown_timeout_s and can it be configured by setting an environment variable for example?,like idea although could mean longer case deployment unhealthy default setting environment variable example,issue,negative,negative,negative,negative,negative,negative
1847884020,"If all the changes were in `doc` I think it would be okay to merge after the final wheel build.  But this PR has changes outside of `doc`, and it even looks like an RLlib unit test is modified. So it might be better to merge it before.  (Assuming the unit test passes)",doc think would merge final wheel build outside doc even like unit test might better merge assuming unit test,issue,positive,positive,positive,positive,positive,positive
1847872179,Is it OK to pick this after we build the wheel? This is just me being over cautious about having the wheel on time,pick build wheel cautious wheel time,issue,negative,neutral,neutral,neutral,neutral,neutral
1847860907,"@zhe-thoughts majority of this came from added tests, feature code is only ~150 lines. There are customers asking for this feature so they can utilize our gRPC proxy feature in the production environment. I think it be the best to provide this early rather than later to help unblocking them 🙏",majority came added feature code feature utilize proxy feature production environment think best provide early rather later help,issue,positive,positive,positive,positive,positive,positive
1847855777,@GeneDer @edoakes Big change. Could you help me understand the delta between having it in 2.9 vs. having it in 2.10?,big change could help understand delta,issue,negative,neutral,neutral,neutral,neutral,neutral
1847665464,"> You can use `RAY_RUNTIME_ENV_HOOK` to inject cluster-level common system plugins. Actually, this is already used by workspaces to distribute workspace code artifacts.

That's what we already did, I think the problem is sometime user could run their python script in a different python env which cannot resolve the plugin location.",use inject common system actually already used distribute code already think problem sometime user could run python script different python resolve location,issue,negative,negative,neutral,neutral,negative,negative
1847646789,"I think we were going to revisit this once `LazyBlockList` is deprecated, which is scheduled for 2.10. Although I think this error can potentially happen in other places as well.",think going revisit although think error potentially happen well,issue,negative,neutral,neutral,neutral,neutral,neutral
1847645792,@raulchen can you re-run on latest master to determine repro; if repros we should hold bar at fixing this for ray210,latest master determine hold bar fixing ray,issue,negative,positive,positive,positive,positive,positive
1847631495,"How likely is it that `memray` fails to detach? IIRC, `memray` doesn't make any guarantees about whether or not it can detach.

Should we add a warning somewhere (maybe as a popup in the UI?) that `memray` will impact performance and may not be able to detach? Otherwise, users may try to use `memray` to diagnose a memory leak in production without knowing the risks.",likely detach make whether detach add warning somewhere maybe impact performance may able detach otherwise may try use diagnose memory leak production without knowing,issue,negative,positive,positive,positive,positive,positive
1847628787,discussed with @c21 this can map to the error improvement project @Zandew is looking at. upping to p1 cause this would make it sooo much easier to debug issues with ray data.,map error improvement project looking upping cause would make much easier ray data,issue,negative,positive,positive,positive,positive,positive
1847608319,@c21 @Zandew can you follow up on target ray release priority and size?,follow target ray release priority size,issue,negative,neutral,neutral,neutral,neutral,neutral
1847599547,"Good question, @peytondmurray. Yes, I removed that tag from the examples.rst file in another PR. You should be able to verify that in master.",good question yes removed tag file another able verify master,issue,positive,positive,positive,positive,positive,positive
1847595212,Sorry missed DCO (and there was a build infra issue). Re-running CI.,sorry build infra issue,issue,negative,negative,negative,negative,negative,negative
1847579049,@edoakes @shrekris-anyscale I addressed all the comments PTAL and hopefully we can get this merged and cherry picked today🙏,hopefully get cherry picked today,issue,negative,neutral,neutral,neutral,neutral,neutral
1847530936,"Oh, I see what the issue is. You are not supposed to serialize and deserialize ObjectRefs out of band since it will break features like reference counting and cancellation.

One solution is using a named actor to hold the obj ref for you so you can get it and cancel the task in another job.

```
import ray
import time

ray.init(namespace=""MyNamespace"")

@ray.remote
class CancellationActor:
  def __init__(self):
    self.refs = {}

  def put(self, name, ref):
    self.refs[name] = ref[0]

  def cancel(self, name):
    ray.cancel(self.refs[name], force=True)
    del self.refs[name]

@ray.remote
def f():
  time.sleep(100000)

actor = CancellationActor.options(name=""MyActor"").remote()
obj_ref = f.remote()
actor.put.remote(""MyTask"", [obj_ref])
print(""Waiting..."")
ray.get(obj_ref)
```
Separately you can cancel it:
```
import ray
ray.init(namespace=""MyNamespace"")
actor = ray.get_actor(""MyActor"")
actor.cancel.remote(""MyTask"")
```",oh see issue supposed serialize band since break like reference counting cancellation one solution actor hold ref get cancel task another job import ray import time class self put self name ref name ref cancel self name name name actor print waiting separately cancel import ray actor,issue,negative,neutral,neutral,neutral,neutral,neutral
1847473867,"> Yes tried but not working

Which one you tried?",yes tried working one tried,issue,negative,neutral,neutral,neutral,neutral,neutral
1847365773,"> cc @aslonnie @can-anyscale This PR will be adding `memray` to Ray. Just wanted to loop you in for awareness due to our earlier conversations about `memray`.

1. pin a version.
2. make sure everything builds and do not break. last time, arm64 images are failing to build with memray.",ray loop awareness due pin version make sure everything break last time arm failing build,issue,negative,positive,positive,positive,positive,positive
1847284229,can you merge the latest master? dashboard test failure (premerge) seems to be fixed if you merge the latest master,merge latest master dashboard test failure fixed merge latest master,issue,negative,positive,positive,positive,positive,positive
1847261954,"In my case, I'm using containers. But for that also, ray seems to be copying the required files to worker containers by mounting it from some /tmp/ path. any redundant files here might also cause issue.
There are also chances that the ray process may not exit in time (for which a --force command and a --grace-period flags are also provided by ray in it's stop commands).

If in case the worker ray process failed to exit, the container may not be getting destroyed. 

But as @jmakov said, there should be a proper cleanup mechanism whether its inside containers or not.",case also ray worker mounting path redundant might also cause issue also ray process may exit time force command also provided ray stop case worker ray process exit container may getting said proper cleanup mechanism whether inside,issue,negative,negative,neutral,neutral,negative,negative
1847233315,"I think I had a similar debate with ray devs before - it was about not cleaning up the state after shutdown. The answer was they primarily support containers and in that context once you call ray to shutdown, the containers are simply destroyed (so things don't need to be cleaned up). Which of course causes problems when you're running on prem without containers.",think similar debate ray cleaning state shutdown answer primarily support context call ray shutdown simply need course running without,issue,negative,positive,positive,positive,positive,positive
1847063398,"Hi,

Thank you so much for your reply!

I've tried following the tutorial, but I'm coming into an error when i run trainer.train(): TypeError: 'DataLoader' object is not subscriptable.
It appears that the data type DataLoader that i need to use for my object detection model isnt compatible? Do you know a way around this? Please find my updated code below for reference:

```
def train_func():
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    config = {
        ""initial_lr"": 1e-2,
        ""optimizer"": ""sgd"",
        ""batch_size"": 16,
        ""epochs"": 2,
        ""model_to_train"": ""yolo_nas_s"",
        ""weight_decay"": 1e-6,
        ""confidence_threshold"": 0.9,
    }
    
    CHECKPOINT_DIR = ""../../../SageMaker/outputs/logs""
    # list of the possible classes
    classes = [
        ""drone"",
        ""Herc"",
        ""Bird"",
        ""FixedWing"",
        ""Person"",
        ""Aircraft"",
        ""Helicopter"",
        ""FighterJet"",
    ]

    dataset_params = {
        ""data_dir"": ""../../../SageMaker/sample_data_small"",  # root directory of data
        ""train_images_dir"": ""images/train/"",  # train images
        ""train_labels_dir"": ""labels/train/"",  # train labels
        ""val_images_dir"": ""images/valid/"",  # validation images
        ""val_labels_dir"": ""labels/valid/"",  # validation labels
        ""test_images_dir"": ""images/test/"",  # test images
        ""test_labels_dir"": ""labels/test/"",  # test labels
        ""classes"": classes,
    }

    # create dataloaders for yolonas model
    train_data = coco_detection_yolo_format_train(
        dataset_params={
            ""data_dir"": dataset_params[""data_dir""],
            ""images_dir"": dataset_params[""train_images_dir""],
            ""labels_dir"": dataset_params[""train_labels_dir""],
            ""classes"": dataset_params[""classes""],
        },
        dataloader_params={
            ""shuffle"": True,
            ""pin_memory"": True,
            ""batch_size"": config[""batch_size""],
            ""num_workers"": 4,
        },
    )

    val_data = coco_detection_yolo_format_val(
        dataset_params={
            ""data_dir"": dataset_params[""data_dir""],
            ""images_dir"": dataset_params[""val_images_dir""],
            ""labels_dir"": dataset_params[""val_labels_dir""],
            ""classes"": dataset_params[""classes""],
        },
        dataloader_params={
            ""batch_size"": config[""batch_size""],
            ""num_workers"": 4,
            ""shuffle"": True,
            ""pin_memory"": True,
        },
    )

    # set training parameters
    train_params = {
        ""silent_mode"": False,
        ""average_best_models"": True,
        ""warmup_mode"": ""linear_epoch_step"",
        ""warmup_initial_lr"": 1e-6,
        ""lr_warmup_epochs"": 3,
        ""initial_lr"": config[""initial_lr""],
        ""lr_mode"": ""cosine"",
        ""cosine_final_lr_ratio"": 0.1,
        ""optimizer"": config[""optimizer""],
        ""optimizer_params"": {""weight_decay"": config[""weight_decay""]},
        ""zero_weight_decay_on_bias_and_bn"": True,
        ""ema"": True,
        ""ema_params"": {""decay"": 0.9, ""decay_type"": ""threshold""},
        ""max_epochs"": config[""epochs""],
        ""mixed_precision"": True,
        ""loss"": PPYoloELoss(
            use_static_assigner=False,
            num_classes=len(dataset_params[""classes""]),
            reg_max=16,
        ),
        ""valid_metrics_list"": [
            DetectionMetrics_050(
                score_thres=config[""confidence_threshold""],
                top_k_predictions=300,
                num_cls=len(dataset_params[""classes""]),
                normalize_targets=True,
                post_prediction_callback=PPYoloEPostPredictionCallback(
                    score_threshold=0.01,
                    nms_top_k=1000,
                    max_predictions=300,
                    nms_threshold=0.7,
                ),
            ),
            DetectionMetrics_050_095(
                score_thres=config[""confidence_threshold""],
                top_k_predictions=300,
                num_cls=len(dataset_params[""classes""]),
                normalize_targets=True,
                post_prediction_callback=PPYoloEPostPredictionCallback(
                    score_threshold=0.01,
                    nms_top_k=1000,
                    max_predictions=300,
                    nms_threshold=0.7,
                ),
            ),
        ],
        ""metric_to_watch"": ""mAP@0.50:0.95"",
    }
    
    model = models.get(
        config[""model_to_train""],
        num_classes=len(dataset_params[""classes""]),
        pretrained_weights=""coco"",
    )

    model = model.to(device)
    
    # Hugging Face Trainer
    training_args = TrainingArguments(
        output_dir=""test_trainer"",
        evaluation_strategy=""epoch"",
        save_strategy=""epoch"",
        report_to=""none"",
    )
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_data,
        eval_dataset=val_data,
        #compute_metrics=compute_metrics,
    )

    # [2] Report Metrics and Checkpoints to Ray Train
    # ===============================================
    callback = ray.train.huggingface.transformers.RayTrainReportCallback()
    trainer.add_callback(callback)
    # [3] Prepare Transformers Trainer
    # ================================
    trainer = ray.train.huggingface.transformers.prepare_trainer(trainer)

    trainer.train() # ERROR OCCURS HERE

    metrics = trainer.test(
        model=model,
        test_loader=val_data,
        test_metrics_list=DetectionMetrics_050_095(
                score_thres=config[""confidence_threshold""],
                top_k_predictions=300,
                num_cls=len(classes),
                normalize_targets=True,
                post_prediction_callback=PPYoloEPostPredictionCallback(
                score_threshold=0.01,
                nms_top_k=1000,
                max_predictions=300,
                nms_threshold=0.7
            )
        )
    )

# defines the number of distributed training workers and whether to use GPUs.
scaling_config = ScalingConfig(num_workers=4, use_gpu=True)

# [4] Define a Ray TorchTrainer to launch `train_func` on all workers
# ===================================================================
ray_trainer = TorchTrainer(
    train_func,
    scaling_config=ScalingConfig(num_workers=4, use_gpu=True),
    # [4a] If running in a multi-node cluster, this is where you
    # should configure the run's persistent storage.
    #run_config=ray.train.RunConfig(storage_path=""../../../SageMaker/FS-Air-Classification/outputs""),
)
result = ray_trainer.fit()
```",hi thank much reply tried following tutorial coming error run object data type need use object detection model compatible know way around please find code reference device else list possible class class drone bird person aircraft helicopter root directory data train train validation validation test test class class create model class class shuffle true true class class shuffle true true set training false true cosine true true decay threshold true loss class class class map model class coco model device hugging face trainer epoch epoch none trainer trainer report metric ray train prepare trainer trainer trainer error metric class number distributed training whether use define ray launch running cluster configure run persistent storage result,issue,positive,positive,positive,positive,positive,positive
1846904482,Thanks for your reply! I checked my code and I found  the reason!,thanks reply checked code found reason,issue,negative,positive,positive,positive,positive,positive
1846837315,"Dropping a little reminder regarding this issue, would really appreciate if someone from ray team could take a look. Thanks! :)",dropping little reminder regarding issue would really appreciate someone ray team could take look thanks,issue,positive,positive,neutral,neutral,positive,positive
1846726708,"> Is there a way to add a unit test to prevent the issue in the future?

@c21 I can't think of a good way to directly unit test this. If it's okay with you, I'll merge this PR now since it's a release blocker, and if we come up with an effective test, I'll add it in a follow-up PR?

Also, per @raulchen's suggestion, we could make `ParquetDatasource` non-serializable in the future.",way add unit test prevent issue future ca think good way directly unit test merge since release blocker come effective test add also per suggestion could make future,issue,positive,positive,positive,positive,positive,positive
1846620812,I am going to target fixing it by next Mon,going target fixing next mon,issue,negative,neutral,neutral,neutral,neutral,neutral
1846595876,"@architkulkarni 
I was obsering the behaviour further for the past 1 week. It seems that ray is not properly stopped in worker and exited the container while we do 'ray down config.yaml'

In that case, when we do ray up the second time, the Node updater gets stuck (showing uninitialized state in ray monitor logs and launching state in ray status & dashboard).


1. Do Ray up config.yaml
2. Worker nodes connects to head node.
3. Before doing ray down config.yaml, login to the worker container, do ray stop manually. Then exit the container.
4. Do ray down.config.yaml. Head node shuts down.
5. Do ray up again, worker will get connected to the head node properly.

If we don't do Step 3, next time the workers won't get connected and gets stuck in launching & uninitialized state

I think we need to have a feature like 'worker_stop_commands' similar to 'worker_start_commands' in config.yaml. Which helps to properly shutdown the nodes / cleanup the nodes before it's shutting down.
Let me know your thoughts on this.",behaviour past week ray properly stopped worker container case ray second time node stuck showing state ray monitor state ray status dashboard ray worker head node ray login worker container ray stop manually exit container ray head node ray worker get connected head node properly step next time wo get connected stuck state think need feature like similar properly shutdown cleanup shutting let know,issue,negative,negative,neutral,neutral,negative,negative
1846547883,"Follow up required (after merging the PR):
- Add threaded actor tests
- Make ray.method(max_retries/max_task_retries) public",follow add threaded actor make public,issue,negative,neutral,neutral,neutral,neutral,neutral
1846508359,"Thanks @scottjlee. We had a couple moving parts so it's hard for me to replicate my issue. 

1. We recently reconfigured our Anyscale platform and made AWS IAM changes.
2. We completely changed our S3 permissions.
3. The data I'm querying is different. 

I think the problem was either (1) a permission issue that was resolved or (2) none of the provided S3 paths existed - I think if every path is missing then it results in an error, and that error could be more clear.",thanks couple moving hard replicate issue recently platform made completely data querying different think problem either permission issue resolved none provided think every path missing error error could clear,issue,negative,negative,neutral,neutral,negative,negative
1846416803,"@edoakes , before this change, `ml` tag will also trigger serve ha tests. is that really required?",change tag also trigger serve ha really,issue,negative,positive,positive,positive,positive,positive
1846409095,"@seydar I think you need to update your grpcio version as described in `python/requirements.txt`

Python version-specific requirements
grpcio == 1.54.2; sys_platform == ""darwin""
grpcio >= 1.54.2; sys_platform != ""darwin""",think need update version python,issue,negative,neutral,neutral,neutral,neutral,neutral
1846392058,"Update: Still triaging the root cause.

What we found so far:
- The issue can be reproduced prety often: ~3 times killing the head node gcs server will crash the dashboard agent.
- The main issue is dashboard_agent crashed with `segmentation fault` instead of exiting with exit_code 0 as one described in the `raylet.out`.
- stack trace: gcs client `Connect` is where segfault happened.
<img width=""877"" alt=""Screenshot 2023-12-08 at 1 28 12 AM"" src=""https://github.com/ray-project/ray/assets/144177685/5b495e55-27db-4537-b32c-61edeb6b5bc0"">
",update still root cause found far issue often time killing head node server crash dashboard agent main issue segmentation fault instead one stack trace client connect,issue,negative,positive,positive,positive,positive,positive
1846340031,This sounds to me a release blocker. We should cherry pick to prevent regression.,release blocker cherry pick prevent regression,issue,negative,neutral,neutral,neutral,neutral,neutral
1846337726,Is there a way to add a unit test to prevent the issue in the future?,way add unit test prevent issue future,issue,negative,neutral,neutral,neutral,neutral,neutral
1846304938,"Thanks, I agree it is a release blocker",thanks agree release blocker,issue,positive,positive,positive,positive,positive,positive
1846301847,"> @sihanwang41 @edoakes (I'm not familiar enough in this module)
> 
> Could you help me understand why having this kind of duplication is a P0 issue?

Currently we are having duplicated items inside the serve details response, when payload is large (caused by large duplicated items, we encounter one customer has large number of duplicated items in the payload), we are seeing the controller event loop is very slow and delay the service status report.  ",familiar enough module could help understand kind duplication issue currently inside serve response large large encounter one customer large number seeing controller event loop slow delay service status report,issue,positive,positive,positive,positive,positive,positive
1846301483,"Hey @pmandadkes1207, thanks for raising this. Looks like this is an issue on AWS' end. Could you open an issue in the AWS SDK for pandas repo?

We made a change to a developer API in Ray 2.7, and AWS hasn't updated the SDK accordingly. In particular, https://github.com/ray-project/ray/pull/37986 removed the `block` parameter from the `BlockWritePathProvider` interface.

",hey thanks raising like issue end could open issue made change developer ray accordingly particular removed block parameter interface,issue,negative,positive,positive,positive,positive,positive
1846295590,"> is it possible to link to the example gallery with pre-selected tags?

Yeah, I wrote a custom sphinx directive for this: `query-param-ref`. Then I added some js which applies filters based on the URL query parameters. There are a few cases where we use this in the code. If you can't find any examples, let me know and I'll hunt one down.

> Also, is there a way to do ""or"" based filtering (give me all the examples with workload X or workload Y) instead of ""and"" based?

Well, there were some very specific requests around this kind of querying when we originally built out the example gallery.

![image](https://github.com/ray-project/ray/assets/14017872/9c1ef436-abec-45ec-8da7-7d4bce63cdc6)

Currently, there are 4 groups of tags you can filter by. If you select two tags in the same group, an `or` will be used, so you get the set containing both tags. If you select tags in different groups, you get `and`, so you get the intersection of the examples containing those tags.

> I'm looking to see if in Ray Data we can link to the global examples gallery with pre-selected filters, rather than maintain a separate, independent gallery. But this would require being able to select multiple workloads

Yep, I think we can already do this. The first solution to come to mind is to make a page template that redirects to the example gallery with the appropriate tags specified as URL query parameters. There's probably other ways to do this as well.",possible link example gallery yeah wrote custom sphinx directive added based query use code ca find let know hunt one also way based filtering give instead based well specific around kind querying originally built example gallery image currently filter select two group used get set select different get get intersection looking see ray data link global gallery rather maintain separate independent gallery would require able select multiple yep think already first solution come mind make page template example gallery appropriate query probably way well,issue,positive,positive,positive,positive,positive,positive
1846291875,"@sihanwang41 @edoakes (I'm not familiar enough in this module)

Could you help me understand why having this kind of duplication is a P0 issue?",familiar enough module could help understand kind duplication issue,issue,positive,positive,positive,positive,positive,positive
1846276267,I have a hunch as to why this test is failing. Opening a PR to test a fix now.,hunch test failing opening test fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1846255926,"I'm using ray 2.8.1 on databricks and I'm still seeing the randomly distributed behavior described here: https://github.com/ray-project/ray/issues/23184

For example: if I have 10 jobs, I'll have 4 on a single worker, while another worker has 1.  One worker is always jobless, while the head node that I've set to take 0 jobs is being used. 

Here is how I'm initializing:

```
setup_ray_cluster(
  num_worker_nodes=workers,
  num_cpus_head_node=0,
  num_cpus_worker_node=num_cpus
)
ray.init()
register_ray()

with joblib.parallel_backend(""ray"", n_jobs=10):
    rfecv.fit(train_df)
```",ray still seeing randomly distributed behavior example single worker another worker one worker always jobless head node set take used ray,issue,negative,negative,negative,negative,negative,negative
1846240747,"`single_client_tasks_sync`


1st regression: 10.25, from 1.2k -> 1k

fe6d5f44a5 [Train] Improve loggings for Train worker scheduling information (#40536)
8cfc894d9f [Core] Support Intel GPU (#38553)
89f18fbbe3 [core][serve] Disable flaky tests (#40643)
792e4e6cad [Data] Move `_resolve_paths_and_filesystem` to `util` file (#40304)
ec2eaf72a0 [Data][Docs] Cross-link XGBoost example in Ray Data examples (#40650)
f1a34b9f3e [cluster launcher] [vSphere Provider] Support GPU Ray nodes on vSphere (#40616)
fc98a5f286 [deflakey] Deflakey test_actor_advanced.py (#40636)
a10aaa9b3f [serve] Lazily construct `asyncio.Event` to avoid attaching to the wrong loop (#40629)
887eddd924 [RLlib] Updating codeblocks in RLlib part 1 (#37271)
b22ef7b159 Run test_basic and other minimal test separately (#40615)
be2606e1cd [serve] Remove `numpy` as a required dependency (#40565)
490f7bb869 split workflow into 2 shards (#40624)
99543b26cf [RLlib] De-flake rl_module/mobilenet test (#40621)
9c2990d0fd [release] Release logs for 2.7.1 (#40611)
469f4d296a [Doc] Update miscellaneous.rst to reflect deletion of feature flag 'use_ray_syncer' (#40208)
36b66e673d [ci] add missing gpu tag to serve doc gpu test (#40623)
94beb5eb3d [Core]Fix NullPointerException cause by raylet id is empty when get actor info in java worker (#40560)
cf122d05f24c43835c3df9e3433663a73d3491d2",st regression train improve train worker information core support core serve disable flaky data move file data example ray data cluster launcher provider support ray serve lazily construct avoid wrong loop part run minimal test separately serve remove dependency split test release release doc update reflect deletion feature flag bed add missing tag serve doc test core fix cause raylet id empty get actor worker,issue,negative,negative,negative,negative,negative,negative
1846240498,"I've run into another example with the same env setup, sharing here just for reference:

```python
>>> import pandas as pd
>>> import ray.data
>>> foo_df = pd.DataFrame(data=pd.date_range(""2023-12-01T00:00:00"", ""2023-12-02T00:00:00"", freq=""1H"", tz=""UTC""), columns=[""dttm""])
>>> foo_df.dtypes
dttm    datetime64[ns, UTC]
dtype: object
>>> foo_ds = ray.data.from_pandas(foo_df)
>>> foo_ds
MaterializedDataset(
   num_blocks=1,
   num_rows=25,
   schema={dttm: datetime64[ns, UTC]}
)
>>> foo_ds.schema()
2023-12-07 18:01:52,585	ERROR dataset.py:5285 -- Error converting dtype datetime64[ns, UTC] to Arrow.
Traceback (most recent call last):
  File ""/Users/burtondewilde/.pyenv/versions/3.9.18/envs/ev-detection/lib/python3.9/site-packages/ray/data/dataset.py"", line 5281, in types
    arrow_types.append(pa.from_numpy_dtype(dtype))
  File ""pyarrow/types.pxi"", line 5138, in pyarrow.lib.from_numpy_dtype
TypeError: Cannot interpret 'datetime64[ns, UTC]' as a data type

Column  Type
------  ----
dttm    None
>>> foo_ds.to_pandas().dtypes
dttm    datetime64[ns, UTC]
dtype: object
```",run another example setup reference python import import object error error converting arrow recent call last file line file line interpret data type column type none object,issue,negative,neutral,neutral,neutral,neutral,neutral
1846180887,This one maps to Ammar in go/ray-data-cuj @c21 can you pop in the recording as well so @Zandew  has context?,one pop recording well context,issue,negative,neutral,neutral,neutral,neutral,neutral
1846179845,see go/ray-data-cuj > I believe the CUJ test for both Amelie and Pouyan ran into this. @c21 can you pop in the link to the Zoom call so @Zandew can have the raw data for this?,see believe test ran pop link zoom call raw data,issue,negative,negative,negative,negative,negative,negative
1846169980,"Hey @fy222fy, could you provide more details about the custom training setup you're trying to achieve? Maybe a diagram / pseudocode skeleton of what the system looks like. Also happy to move to Slack messaging or a call, and we can post the findings back on this thread after.",hey could provide custom training setup trying achieve maybe diagram skeleton system like also happy move slack call post back thread,issue,positive,positive,positive,positive,positive,positive
1846153266,"You can use `RAY_RUNTIME_ENV_HOOK` to inject cluster-level common system plugins. Actually, this is already used by workspaces to distribute workspace code artifacts.",use inject common system actually already used distribute code,issue,negative,negative,negative,negative,negative,negative
1846142974,@m-walters Could you provide the code that was used to cause 1/5 runs to error?,could provide code used cause error,issue,negative,neutral,neutral,neutral,neutral,neutral
1846141201,Closing as the above workaround seems to resolve the issue for me. Feel free to reopen with more info if I don't have the same failure scenario.,resolve issue feel free reopen failure scenario,issue,negative,positive,neutral,neutral,positive,positive
1846126822,"Yes tried but not working, the head node and worker node and cluster is up and running, but while trying to connect the ray server it's generating such error, versions and setup details updated above. ",yes tried working head node worker node cluster running trying connect ray server generating error setup,issue,negative,neutral,neutral,neutral,neutral,neutral
1846120768,"https://buildkite.com/ray-project/premerge/builds/13994#018c45e3-f4cc-4509-89ff-268dfae71602 

passed in one shot.

interesting that the test is very sensitive to memory usage. one additional container + `ray_ci` test wrapping will just fail the test..",one shot interesting test sensitive memory usage one additional container test wrapping fail test,issue,negative,positive,neutral,neutral,positive,positive
1845911528,"@HannahAlexander You should use Ray Train with Huggingface Transformers, rather than Ray Tune with multiple GPUs per trial.

Why is this?
* **Ray Tune** launches a single process (ray actor) to run your custom training script. Ray Tune's main functionality is parallelizing many single process runs. (ex: many non-distributed training runs). Because there's just a single process with no torch distributed environment set up, HF transformers will just use a single device (of the 2 that are available from your resource specification).
* **Ray Train** launches multiple worker processes to run your training script across multiple devices and set up the distributed communication backend so that they can synchronize gradients with each other.
* It's also possible to Tune over Train runs if that's also why you were looking into Ray Tune originally!

Take a look here to get started: https://docs.ray.io/en/master/train/getting-started-transformers.html

I'll close this for now, but feel free to follow up with any more questions, or create a new issue if you run into any problems getting onboarded!",use ray train rather ray tune multiple per trial ray tune single process ray actor run custom training script ray tune main functionality many single process ex many training single process torch distributed environment set use single device available resource specification ray train multiple worker run training script across multiple set distributed communication synchronize also possible tune train also looking ray tune originally take look get close feel free follow create new issue run getting,issue,positive,positive,positive,positive,positive,positive
1845906049,"> What's the diff between ""Native"" and ""Leaks""?

native is for tracing C++ stack trace as well (py-spy also able to do this)
leaks is for showing memory that was not deallocated by pymalloc.",native native tracing stack trace well also able showing memory,issue,negative,positive,positive,positive,positive,positive
1845873339,"> Bc that would be a breaking change given there are already users for gRPC proxy. Also I feel since there are already mechanisms for getting request metadata such as multiplex api, following that pattern is a better alternatives (than forcing users to take an gRPC context that they might not case about)

You can make it an optional kwarg (I think that's what gRPC does to begin with). If user-provided functions take that kwarg, we pass it, else we don't.",would breaking change given already proxy also feel since already getting request multiplex following pattern better forcing take context might case make optional think begin take pas else,issue,negative,positive,positive,positive,positive,positive
1845862466,"Bc that would be a breaking change given there are already users for gRPC proxy. Also I feel since there are already mechanisms for getting request metadata such as multiplex api, following that pattern is a better alternatives (than forcing users to take an gRPC context that they might not case about)",would breaking change given already proxy also feel since already getting request multiplex following pattern better forcing take context might case,issue,negative,positive,positive,positive,positive,positive
1845812522,@edoakes would be nice if you have bandwidth to take a look at this one as well🙏,would nice take look one well,issue,positive,positive,positive,positive,positive,positive
1845782112,@shrekris-anyscale could you help review this please given your experience using memray and adding support in Serve?,could help review please given experience support serve,issue,positive,neutral,neutral,neutral,neutral,neutral
1845780331,"I retried the tests 3 times. `test_target_capacity` passed every time on Linux and flaked [once on Windows](https://buildkite.com/ray-project/premerge/builds/13822#018c4124-10f8-4c21-acda-d37d09a68cfa/2998-6443) for a different reason. @edoakes could you merge this PR?

If the test continues to flake on Windows, I'll use the tracebacks to deflake it further.",time every time different reason could merge test flake use,issue,negative,neutral,neutral,neutral,neutral,neutral
1845732273,Docs build completed this time https://readthedocs.com/projects/anyscale-ray/builds/1885865/ not sure why it's showing failed here 😅,build time sure showing,issue,negative,positive,positive,positive,positive,positive
1845645915,"> REGRESSION 22.84%: single_client_tasks_sync (THROUGHPUT) regresses from 1161.670131632561 to 896.3011478159317 (22.84%) in 2.9.0/microbenchmark.json

**Regression before the grpc upgrade.**

> REGRESSION 18.91%: 1_n_actor_calls_async (THROUGHPUT) regresses from 9581.728569086026 to 7769.431743767464 (18.91%) in 2.9.0/microbenchmark.json

Caused by the grpc upgrade.

> REGRESSION 13.46%: multi_client_tasks_async (THROUGHPUT) regresses from 27211.51041454346 to 23547.65328133274 (13.46%) in 2.9.0/microbenchmark.json

**Regression before the grpc upgrade.**

> REGRESSION 13.42%: actors_per_second (THROUGHPUT) regresses from 753.4446893211699 to 652.3025965764776 (13.42%) in 2.9.0/benchmarks/many_actors.json

Noise.

> REGRESSION 11.72%: placement_group_create/removal (THROUGHPUT) regresses from 926.0840791839338 to 817.5855922503765 (11.72%) in 2.9.0/microbenchmark.json

Caused by the grpc upgrade.

> REGRESSION 9.71%: client__tasks_and_put_batch (THROUGHPUT) regresses from 11415.752622212967 to 10306.863268325735 (9.71%) in 2.9.0/microbenchmark.json

**Regression before grpc upgrade.**

> REGRESSION 9.38%: n_n_actor_calls_async (THROUGHPUT) regresses from 30108.565209428394 to 27282.915588001837 (9.38%) in 2.9.0/microbenchmark.json

Caused by the grpc upgrade.

> REGRESSION 8.69%: multi_client_put_calls_Plasma_Store (THROUGHPUT) regresses from 12224.373147431208 to 11162.251480555622 (8.69%) in 2.9.0/microbenchmark.json

**Regression before grpc upgrade.**

> REGRESSION 7.54%: 1_n_async_actor_calls_async (THROUGHPUT) regresses from 8601.993472120319 to 7953.049246488258 (7.54%) in 2.9.0/microbenchmark.json

Caused by the grpc upgrade.

> REGRESSION 69.95%: stage_3_creation_time (LATENCY) regresses from 2.260662794113159 to 3.8420045375823975 (69.95%) in 2.9.0/stress_tests/stress_test_many_tasks.json

Noise.

> REGRESSION 63.82%: stage_0_time (LATENCY) regresses from 7.927043914794922 to 12.985974550247192 (63.82%) in 2.9.0/stress_tests/stress_test_many_tasks.json

Noise.

> REGRESSION 14.96%: avg_pg_remove_time_ms (LATENCY) regresses from 0.7885501576572757 to 0.9064963108115691 (14.96%) in 2.9.0/stress_tests/stress_test_placement_group.json

Caused by the grpc upgrade.

> REGRESSION 6.92%: stage_3_time (LATENCY) regresses from 2943.001654624939 to 3146.527221918106 (6.92%) in 2.9.0/stress_tests/stress_test_many_tasks.json

Caused by the grpc upgrade.

> REGRESSION 6.91%: avg_pg_create_time_ms (LATENCY) regresses from 0.8868904699705661 to 0.9481992612621639 (6.91%) in 2.9.0/stress_tests/stress_test_placement_group.json

Noise.

> REGRESSION 6.58%: 3000_returns_time (LATENCY) regresses from 5.899374322999989 to 6.287682662999998 (6.58%) in 2.9.0/scalability/single_node.json

Noise.",regression throughput regression upgrade regression throughput upgrade regression throughput regression upgrade regression throughput noise regression throughput upgrade regression throughput regression upgrade regression throughput upgrade regression throughput regression upgrade regression throughput upgrade regression latency noise regression latency noise regression latency upgrade regression latency upgrade regression latency noise regression latency noise,issue,negative,neutral,neutral,neutral,neutral,neutral
1845281125,"I'm also having issues with this - from the documentation it wasn't clear to me at all that this isn't possible. 

I want to run separate tune experiments on separate nodes, but instead now every node executed each experiment.

Is it possible to block communication between multiple runs?

I don't need the global scheduling because we use slurm. I think Ray is great overall and it would be really cool if this could be made easier. Or if there were documentation on an alternative, better way of doing it.  
similar libraries (like NNI) have this global scheduling setup but there seems to be little to nothing out there for us folks who want to run independent, smaller experiments instead of just one large one.",also documentation clear possible want run separate tune separate instead every node executed experiment possible block communication multiple need global use think ray great overall would really cool could made easier documentation alternative better way similar like global setup little nothing u want run independent smaller instead one large one,issue,positive,positive,positive,positive,positive,positive
1844770407,"It seems I can just use my own read func then use ray data's from_arrow_refs instead of read_datasource to get precise metadata per row/block
The only downsize is this will materialize the ds at the read time,can't use dataset.split local hints anymore",use read use ray data instead get precise per materialize read time ca use local,issue,negative,positive,positive,positive,positive,positive
1844275554,"Hi @lee1258561, yes and we are adding support to allow retrying user exception on Core - https://github.com/ray-project/ray/pull/41194 . ETA is Ray 2.9 in next few weeks, then you can specify which exceptions you want to retry, and configure number of retries for actor task.",hi lee yes support allow user exception core eta ray next specify want retry configure number actor task,issue,positive,neutral,neutral,neutral,neutral,neutral
1844247118,"Discussed with @rynewang offline, this PR should not need any change on Data side.",need change data side,issue,negative,neutral,neutral,neutral,neutral,neutral
1844188149,"I do a lot of hack,in our use case, we don't have many rows, maybe just thousands but large item that is ndarray about serveral GB, we use just 1 row 1 block
So I use metadata info to archive index,so we can use these index do operator such as lookback or join in map reduce way.
Because iter over metadata is much faster than iter over real data @scottjlee 
I am not sure this could be use broader,this assume these operator would only use some small key such as 'id', 'date', and rows num is relatively small, not like billions.
And I found now (2.7.1) ReadTask class seems hard to set different metadata for every block it yield,also need do some hack,but I found it's a DeveloperAPI,I don't know how ray team's plan
In my use case, I need to set some 'small index info' for every row/block in block metadata,not sure this is the way ReadTask metadata to go
",lot hack use case many maybe large item use row block use archive index use index operator join map reduce way iter much faster iter real data sure could use assume operator would use small key relatively small like found class hard set different every block yield also need hack found know ray team plan use case need set index every block sure way go,issue,negative,positive,positive,positive,positive,positive
1844124121,"> Lmk when all comments are addressed! I am going to approve the PR

@rkooo567 I've addressed your comments, please take another look!",going approve please take another look,issue,negative,neutral,neutral,neutral,neutral,neutral
1844013703,@stephanie-wang thanks. unit test is added and other comments are addressed too. ,thanks unit test added,issue,negative,positive,positive,positive,positive,positive
1843983465,"



> Please check lint: https://buildkite.com/ray-project/premerge/builds/13793#018c3e28-1d52-4436-ac52-7b6aec579539/186-445
> 
> You can run https://github.com/ray-project/ray/blob/master/setup_hooks.sh to install a hook that automatically runs the linter before pushing to github.

OK. Done.",please check lint run install hook automatically linter pushing done,issue,negative,neutral,neutral,neutral,neutral,neutral
1843951270,Might break linux://python/ray/train:distributed_sage_example. I'm validating if reverting fix the test https://buildkite.com/ray-project/postmerge/builds/1966,might break fix test,issue,negative,neutral,neutral,neutral,neutral,neutral
1843925217,"As a temporary solution I wrapped `.fit()` with `try...except` and at least I can terminate such problematic trials:

```py
try:
    results = tune.Tuner(tune.with_resources(backtest_rungs, param_space=hyperparams, tune_config=tune_config, run_config=run_config).fit()
except Exception as e:
    print(""❌ Exception in worker:"", e)
    train.report({})
    ray.shutdown()
    os.kill(os.getpid(), signal.SIGTERM)
```

So far been running for nearly 48 hours on 4 nodes, did not see a single hang, so I assume such termination works as expected.",temporary solution wrapped try except least terminate problematic try except exception print exception worker far running nearly see single assume termination work,issue,negative,negative,neutral,neutral,negative,negative
1843891924,"Synced with @rkooo567 that this is not a release blocker as it's not really a regression, but we should treat it as P0 to fix. ",release blocker really regression treat fix,issue,negative,positive,positive,positive,positive,positive
1843861310,"I will address the Many Model Training issue in another PR.
",address many model training issue another,issue,negative,positive,positive,positive,positive,positive
1843842273,"For an example dataset: `ray.data.range(5e9).map_batches(lambda x: x).materialize()` I noticed that the rss bytes for a single task is 100x the total output bytes. Is this normal? It could also just be an estimation error, since `max_rss_bytes` is an upperbound.

<img width=""861"" alt=""Screenshot 2023-12-06 at 3 14 00 PM"" src=""https://github.com/ray-project/ray/assets/39287272/c324db7b-6057-4c9a-804a-290215503ca7"">
<img width=""856"" alt=""Screenshot 2023-12-06 at 3 14 20 PM"" src=""https://github.com/ray-project/ray/assets/39287272/34771c5e-b7ae-45e0-894e-eb755660cad4"">
",example lambda single task total output normal could also estimation error since,issue,negative,positive,neutral,neutral,positive,positive
1843838492,Close as planned metrics are already added.,close metric already added,issue,negative,neutral,neutral,neutral,neutral,neutral
1843790165,"@amogkam also made this request on June 5th, 2023: 
[@Max]
 is it possible to link to the example gallery with pre-selected tags?
Also, is there a way to do ""or"" based filtering (give me all the examples with workload X or workload Y) instead of ""and"" based?
I'm looking to see if in Ray Data we can link to the global examples gallery with pre-selected filters, rather than maintain a separate, independent gallery. But this would require being able to select multiple workloads",also made request june th possible link example gallery also way based filtering give instead based looking see ray data link global gallery rather maintain separate independent gallery would require able select multiple,issue,negative,positive,neutral,neutral,positive,positive
1843779216,Removing the release-blocker tag for now. De-flaking for real would require a major refactor but moving it back to v1 CI might make it less flaky for now.,removing tag real would require major moving back might make le flaky,issue,negative,positive,neutral,neutral,positive,positive
1843778614,"This looks like a real bug but not a regression. Basically the issue is that [autoscaler.py](https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/_private/autoscaler.py) has some problematic concurrency bugs that have likely been in the codebase for a long time.

The autoscaler state is spread out across a bunch of different Python objects, e.g., `pending_launches` describes nodes that are pending launch, while `non_terminated_nodes` describes nodes that are alive. Individually they are thread-safe but not together. So when the autoscaler decides how many nodes to add, we can see some inconsistent state, like ""a launched node does not appear in either pending_launches or non_terminated_nodes"". Then, the autoscaler will accidentally start more nodes. Eventually, the autoscaler will update and remove the nodes but temporarily it can do the wrong thing.

",like real bug regression basically issue problematic concurrency likely long time state spread across bunch different python pending launch alive individually together many add see inconsistent state like node appear either accidentally start eventually update remove temporarily wrong thing,issue,negative,positive,neutral,neutral,positive,positive
1843769980,"Based on @matthewdeng's position about Many Model Training, I can remove both of these cards:

- `/tune/examples/batch_tuning` that is called, ""Batch Training and Tuning using Ray Tune`
- `/data/examples/batch_training`; there are actually two cards for this same example, `Batch Training with Ray Data` and `Many Model Training with Ray Data`",based position many model training remove batch training tuning ray tune actually two example batch training ray data many model training ray data,issue,negative,positive,positive,positive,positive,positive
1843768866,"> @c21 @scottjlee Could you quickly clarify why this should be a release blocker

@zhe-thoughts the PR fixes a bug, which affects unit tests (not previously caught because test wasn't triggered in CI)",could quickly clarify release blocker bug unit previously caught test triggered,issue,negative,positive,neutral,neutral,positive,positive
1843764760,@c21 @scottjlee Could you quickly clarify why this should be a release blocker,could quickly clarify release blocker,issue,negative,positive,positive,positive,positive,positive
1843724991,"@raulchen I think in the case where the read stage is fused with the actor transform stage and the flaky error happens in the s3 reading, a try catch in actor transform is not suffice to handle this error? ",think case read stage fused actor transform stage flaky error reading try catch actor transform suffice handle error,issue,negative,neutral,neutral,neutral,neutral,neutral
1843667843,"@edoakes yes, the grpc context is scoped to the lifetime of a request. It contains data for that request and allows the deployment to set code and details messages back to the request client :) ",yes context lifetime request data request deployment set code back request client,issue,negative,neutral,neutral,neutral,neutral,neutral
1843665143,"@GeneDer I'm not totally sure what `get_grpc_context` is for, but we already have a `get_replica_context` -- should we just make the grpc context a field of that? Or is it scoped to the lifetime of a request?",totally sure already make context field lifetime request,issue,negative,positive,positive,positive,positive,positive
1843633820,"Right now Ray Data is focused on offline batch inference and online preprocessing for training. Both of these read from bounded datasets, so we're not really focused on the streaming problem at the moment.

External contributions or designs on this would be welcome, however. The Ray Data execution model supports reading from unbounded sources in principle, it's just not exposed in the API.",right ray data batch inference training read bounded really streaming problem moment external would welcome however ray data execution model reading unbounded principle exposed,issue,negative,positive,positive,positive,positive,positive
1843630015,"Added a draft of windows/tests.env.Dockerfile, defer full implementation to the top of this stack when that file is used",added draft defer full implementation top stack file used,issue,negative,positive,positive,positive,positive,positive
1843598688,I'm re-opening this at P1 as there's been a string of failures the last couple of days.,string last couple day,issue,negative,neutral,neutral,neutral,neutral,neutral
1843594873,double checked - still retaining green here - we are good.,double checked still retaining green good,issue,negative,positive,positive,positive,positive,positive
1843561438,"> Unless you are inquiring about creating the blocks based on the columns.

Yes, please! This is an old request, but the feature would still be useful. :) I have a dataset where certain sets of rows must be written out to separate files for later processing. Currently, I add a column to the dataset that's effectively a computed grouping key, use it in `Dataset.groupby()`, then iterate over groups via `GroupedDataset.map_groups()`, writing each to file via the batch format's native functionality. The problem with this is that this isn't really what map-groups is for, and I can't use the `Dataset.write_*()` methods for easy I/O and customized block file naming. I'd guess that it's also slower.

Would be really nice to have something like `Dataset.repartition(by=colname).write_parquet(PATH, block_path_provider=MY_BPP)` . I suppose, for this particular case, being able to specify a partitioning scheme for `Dataset.write_*` would also work nicely.",unless inquiring based yes please old request feature would still useful certain must written separate later currently add column effectively grouping key use iterate via writing file via batch format native functionality problem really ca use easy block file naming guess also would really nice something like path suppose particular case able specify partitioning scheme would also work nicely,issue,positive,positive,positive,positive,positive,positive
1843532587,Closing for now. Need to see if this is still an issue with the new docs.,need see still issue new,issue,negative,positive,positive,positive,positive,positive
1843473261,@mattip followed up with @scottjlee and assigned to you; can you please take up the action item that Scott laid out two comments up?,assigned please take action item laid two,issue,negative,positive,neutral,neutral,positive,positive
1843454997,"Could you clarify what you mean by ""in cluster mode""? ",could clarify mean cluster mode,issue,negative,negative,negative,negative,negative,negative
1843451321,"It depends how the dataset is handled in your train function. Your train function may be copying it from shared memory, and increase the memory used.",handled train function train function may memory increase memory used,issue,negative,neutral,neutral,neutral,neutral,neutral
1843446980,"Hey @goncamateus, we've actually recently removed `NevergradSearch` due to low usage. Would you be able to use one of the other search algorithms?",hey actually recently removed due low usage would able use one search,issue,negative,positive,neutral,neutral,positive,positive
1843440751,"Please check lint: https://buildkite.com/ray-project/premerge/builds/13793#018c3e28-1d52-4436-ac52-7b6aec579539/186-445

You can run https://github.com/ray-project/ray/blob/master/setup_hooks.sh to install a hook that automatically runs the linter before pushing to github.",please check lint run install hook automatically linter pushing,issue,negative,neutral,neutral,neutral,neutral,neutral
1843412312,"Because there are still open CI failures and release blockers, I will mark this PR as do-not-merge. When all these are fixed and we have a final green wheel, I will create a new PR with the performance logs from that wheel, and that's the PR we will actually merge.

It's still a P0 release blocker to review the logs in the current PR.",still open release mark fixed final green wheel create new performance wheel actually merge still release blocker review current,issue,negative,positive,neutral,neutral,positive,positive
1843392053,"latest ray has ""--redirect-output-deprecated""
`Successfully installed ray-2.8.1
(base) sam@Samuels-MacBook-Pro-16 ~ % ray start --no-redirect-output
Traceback (most recent call last):
  File ""/Users/sam/miniforge3/bin/ray"", line 8, in <module>
    sys.exit(main())
  File ""/Users/sam/miniforge3/lib/python3.8/site-packages/ray/scripts/scripts.py"", line 2498, in main
    return cli()
  File ""/Users/sam/miniforge3/lib/python3.8/site-packages/click/core.py"", line 1130, in __call__
    return self.main(*args, **kwargs)
  File ""/Users/sam/miniforge3/lib/python3.8/site-packages/click/core.py"", line 1055, in main
    rv = self.invoke(ctx)
  File ""/Users/sam/miniforge3/lib/python3.8/site-packages/click/core.py"", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/Users/sam/miniforge3/lib/python3.8/site-packages/click/core.py"", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/Users/sam/miniforge3/lib/python3.8/site-packages/click/core.py"", line 760, in invoke
    return __callback(*args, **kwargs)
  File ""/Users/sam/miniforge3/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py"", line 856, in wrapper
    return f(*args, **kwargs)
  File ""/Users/sam/miniforge3/lib/python3.8/site-packages/ray/scripts/scripts.py"", line 644, in start
    ray_params = ray._private.parameter.RayParams(
  File ""/Users/sam/miniforge3/lib/python3.8/site-packages/ray/_private/parameter.py"", line 256, in __init__
    self._check_usage()
  File ""/Users/sam/miniforge3/lib/python3.8/site-packages/ray/_private/parameter.py"", line 448, in _check_usage
    raise DeprecationWarning(""The redirect_output argument is deprecated."")
DeprecationWarning: The redirect_output argument is deprecated.
`",latest ray successfully base sam ray start recent call last file line module main file line main return file line return file line main file line invoke return file line invoke return file line invoke return file line wrapper return file line start file line file line raise argument argument,issue,negative,positive,neutral,neutral,positive,positive
1843386581,@pyrito < believe this should be resolved; if you are you still seeing CI errors on ray28 can you pop them here so we can take a look at it?,believe resolved still seeing ray pop take look,issue,negative,neutral,neutral,neutral,neutral,neutral
1843382885,@AmirBFar sorry we didn't get back here; can you try with latest ray28 and if it's reproing provide the repro code so we can try?,sorry get back try latest ray provide code try,issue,negative,neutral,neutral,neutral,neutral,neutral
1843375047,"@rickyyx The test is failing in the release branch. https://buildkite.com/ray-project/postmerge/builds?branch=releases%2F2.9.0 Can we cherry-pick this PR to Ray 2.9.0?

",test failing release branch ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1843202721,"> Thank you so much @anmyachev !

Thanks everyone for the review!",thank much thanks everyone review,issue,positive,positive,positive,positive,positive,positive
1843186789,"Not super familiar with bazel, but can't we actually just vendor all deps to thirdparty_files using the same way as we vendor cpp deps? https://github.com/ray-project/ray/blob/6948c204055def067af598cdbd37d6399b8731ae/bazel/ray_deps_setup.bzl#L135. ",super familiar ca actually vendor way vendor,issue,positive,positive,positive,positive,positive,positive
1843126507,"cc @jjyao not sure if it can impact any performance, but please take a close look in regression!",sure impact performance please take close look regression,issue,positive,positive,positive,positive,positive,positive
1843121841,The priority of the PR is not clear yet (whether or not it is a blocker). Let's wait one more day for the feedback,priority clear yet whether blocker let wait one day feedback,issue,negative,positive,positive,positive,positive,positive
1842862777,"Reason for failure was:
config yaml file still had the old: `_enable_learner_api` key in it, even though the test was designed for the new API stack. Not a release blocker as there is a simple fix for it. Removed label.",reason failure file still old key even though test designed new stack release blocker simple fix removed label,issue,negative,negative,neutral,neutral,negative,negative
1842841221,"Fails due to framework not set correctly to ""tf2"" (using ""tf"", which is not supported anymore on new stack). Removing release blocker label.",due framework set correctly new stack removing release blocker label,issue,negative,positive,neutral,neutral,positive,positive
1842836487,Failure due to wrong config setting used. Removing release blocker label.,failure due wrong setting used removing release blocker label,issue,negative,negative,negative,negative,negative,negative
1842451515,"@jjyao Thank you for getting back. Yes I already reviewed this: https://docs.ray.io/en/latest/ray-core/api/doc/ray.cancel.html before. I already followed the steps in the doc and still not working with me. Would you please share the reason of the issue or anything wrong in my code attached:

<img width=""760"" alt=""Screenshot 2023-12-04 at 16 58 32"" src=""https://github.com/ray-project/ray/assets/36266903/2aec2705-7170-4314-97f8-f299eae5f8d9"">
",thank getting back yes already already doc still working would please share reason issue anything wrong code attached,issue,positive,negative,negative,negative,negative,negative
1842447402,"The problem is that when running the bento service (only bentoml no ray) using docker on local it works so it isn't a ressource issue I think.
I will deploy it on an AKS, though obviously would like to try it first",problem running service ray docker local work issue think deploy though obviously would like try first,issue,negative,positive,neutral,neutral,positive,positive
1842210244,No timeline yet. The team is pretty overloaded at the moment. Contribution is welcome.,yet team pretty moment contribution welcome,issue,positive,positive,positive,positive,positive,positive
1842110694,@sip-aravind-g are you able to connect to `kuberay-head-svc.kuberay:10001` without using Ray (e.g. using grpc client directly)? are you able to run the same ray client code inside the head pod?,able connect without ray client directly able run ray client code inside head pod,issue,negative,positive,positive,positive,positive,positive
1842040120,"@JakeSummers @bakeryproducts so as Edward already called out in both of your use-cases underlying root-cause of the ""delay"" in results deliver is the fact that your request execution is inherently _synchronous_, therefore blocking the asyncio Event Loop used by Ray Serve ([this is a simplified example](https://stackoverflow.com/questions/61358303/how-can-we-block-event-loop) of what that practically might mean for your application).

In the context of your application what this means is following:

1. You're submitting requests to a Replica trying to handle these requests
2. While requests handling completes (which you're able to confirm indirectly from the logs), sending response requires additional handling as well, however
3. Before result of handling of the previous request has been sent out, new requests handling starts (occupying, blocking the event-loop) therefore preventing response to be sent back
4. This persists until all requests are handled, event-loop is unblocked and response handling gets a chance to be executed

That's the reason why you seeing response times scaling up with the number of requests that you submit.

As have already been called out to avoid this from occurring you'd avoid executing large, uninterrupted blocks (for ex, long loops) on the event-loop.",already underlying delay deliver fact request execution inherently therefore blocking event loop used ray serve simplified example practically might mean application context application following replica trying handle handling able confirm indirectly sending response additional handling well however result handling previous request sent new handling blocking therefore response sent back handled unblocked response handling chance executed reason seeing response time scaling number submit already avoid avoid large uninterrupted ex long,issue,negative,positive,neutral,neutral,positive,positive
1841970714,"I think 1~2 days of work... But the team is very overloaded lately.

@luxunxiansheng Would you be interested in contribution here? ",think day work team lately would interested contribution,issue,negative,negative,neutral,neutral,negative,negative
1841955998,@rickyyx that's pretty weird... do you still have Actor without __init__ as well? ,pretty weird still actor without well,issue,negative,negative,negative,negative,negative,negative
1841953658,"> Ok this is the pr that introduces the regression: https://github.com/ray-project/ray/pull/40451

@rickyyx since you fixed this issue, can we close it now? ",regression since fixed issue close,issue,negative,positive,neutral,neutral,positive,positive
1841874187,"```
The raylet exited immediately because one Ray agent failed, agent_name = runtime_env_agent.
```

When this happens, do you have the runtime env agent log? Want to see why it failed.",raylet immediately one ray agent agent log want see,issue,negative,neutral,neutral,neutral,neutral,neutral
1841872847,"After we fix this issue, https://github.com/ray-project/ray/pull/40116 should be reverted. 

When iterating over a dataset with `iter_batches`, the execution happens on streaming split coordinator actor. The issue should be fixed by https://github.com/ray-project/ray/pull/41569. 
However, some API may trigger execution on the local training worker process. For example, `to_tf` will call `schema` and trigger local execution. In this case, the DataContext being used is still incorrect. ",fix issue execution streaming split actor issue fixed however may trigger execution local training worker process example call schema trigger local execution case used still incorrect,issue,negative,positive,neutral,neutral,positive,positive
1841867063,"We've merged the changes to make Ray compatible with Pydantic 2.5+. You can start using Pydantic 2.5+ with Ray 2.9, which should be out at the end of December.

These changes should also be in the Ray nightly, so feel free to try them out!",make ray compatible start ray end also ray nightly feel free try,issue,positive,positive,positive,positive,positive,positive
1841863176,"We've merged the changes to make Ray compatible with Pydantic 2.5+. You can start using Pydantic 2.5+ with Ray 2.9, which should be out at the end of December.

This issue should no longer happen starting in Ray 2.9, so I'll close it.",make ray compatible start ray end issue longer happen starting ray close,issue,negative,neutral,neutral,neutral,neutral,neutral
1841841599,"Still work in progress, some of documentations not updated yet.",still work progress yet,issue,negative,neutral,neutral,neutral,neutral,neutral
1841839141,"If you have a custom LogicalOperator called `CustomLogicalOp`, you can modify the code in `planner.py` as you did above. Should look something like this:

```
elif isinstance(logical_op, Limit):
            assert len(physical_children) == 1
            physical_op = plan_limit_op(logical_op, physical_children[0])
elif isinstance(logical_op, CustomLogicalOp):
            physical_op = plan_custom_op(logical_op, ...) # translate custom op to physical op
else:
            raise ValueError(
                f""Found unknown logical operator during planning: {logical_op}""
            )
```
where `plan_custom_op` should return a corresponding `PhysicalOperator` for the input `CustomLogicalOp`.

Out of curiosity, what does your custom operator do? Is it something that you think would be useful for the broader open source community, and would you be willing to submit a contribution?",custom modify code look something like limit assert translate custom physical else raise found unknown logical operator return corresponding input curiosity custom operator something think would useful open source community would willing submit contribution,issue,positive,positive,positive,positive,positive,positive
1841838111,"Hi @TracebaK, we've done a lot of work addressing memory leaks in Serve recently. Most of the leaks– including ones in the `HTTPProxyActor`– were fixed in Ray 2.8, and there's one lingering leak that will be fixed in Ray 2.9. I'll close this issue for now, but if you can reproduce it on `master`, please feel free to re-open it.",hi done lot work memory serve recently fixed ray one leak fixed ray close issue reproduce master please feel free,issue,negative,positive,positive,positive,positive,positive
1841831614,Serve now lets you configure the placement group in the [`@serve.deployment`](https://docs.ray.io/en/releases-2.8.0/serve/api/doc/ray.serve.Deployment.html) options. Does that satisfy your use case @Lorien2027?,serve configure placement group satisfy use case,issue,negative,neutral,neutral,neutral,neutral,neutral
1841813652,"This results in errors similar to:
```
pyarrow.lib.ArrowInvalid: offset overflow while concatenating arrays
```

With an increasing number of users reading large datasets (e.g. https://github.com/ray-project/ray/issues/41411), we should prioritize this work. ",similar offset overflow increasing number reading large work,issue,negative,positive,positive,positive,positive,positive
1841811212,"Yeah, unfortunately the error of this type
```
pyarrow.lib.ArrowInvalid: offset overflow while concatenating arrays
```
is resulting from the aforementioned Arrow bug (there are also several related nested issues, such as https://github.com/apache/arrow/issues/28850, which is more directly related to our internal Arrow exchange). 

I found discussion of several potential workarounds which could work depending on the use case (e.g. setting `writer_batch_size` parameter for [HF datasets](https://github.com/huggingface/datasets/issues/5783#issuecomment-1705361235)). 

For now, I propose that we consolidate further discussion and updates in https://github.com/ray-project/ray/issues/29492, since this is tracking the root problem. Please feel free to follow up with additional context or questions in that issue. Thanks!",yeah unfortunately error type offset overflow resulting arrow bug also several related directly related internal arrow exchange found discussion several potential could work depending use case setting parameter propose consolidate discussion since root problem please feel free follow additional context issue thanks,issue,negative,positive,neutral,neutral,positive,positive
1841786414,"Thanks for the minimal repro! I took a look, and this is expected behavior. The logs appear even if Serve is operating normally.

For context, Serve actors use a long poller internally to detect changes in other actors. During normal operation, this long poller periodically times out and reconnects in order to clear connections from dead actors. After timing out, the long poller logs [a debug message](https://github.com/ray-project/ray/blob/ba53d2f332654ffe2da98935f25241b0a79cd0da/python/ray/serve/_private/long_poll.py#L151-L154).

Currently, Serve replicas only use a LongPoll client if they hold a handle to another Serve deployment. The reason you see the message isn't because the Serve deployment uses FastAPI; it's actually because the FastAPI `Service2` deployment in your repro holds a `ServeHandle` for the `Service1` deployment. If you remove the FastAPI integration in `Service2`, you'll still see the timeout messages.

Note that the timeout messages are debug logs. They're appearing in the first place only because the `Service2` replica sets `self._logger.setLevel(logging.DEBUG)` in its constructor. Removing this should also make the logs stop appearing.

Since there's no bug, I'll close this issue. Let us know if you have any questions or concerns!",thanks minimal took look behavior appear even serve operating normally context serve use long poller internally detect normal operation long poller periodically time order clear dead timing long poller message currently serve use client hold handle another serve deployment reason see message serve deployment actually service deployment service deployment remove integration service still see note first place service replica constructor removing also make stop since bug close issue let u know,issue,negative,positive,neutral,neutral,positive,positive
1841772568,"With `read_parquet`, each read task may generate multiple output blocks, depending on the size of the output -- blocks that exceed the max target block size will be split into multiple blocks. So, this is expected behavior with the current API, as Ray Data will dynamically adjust the number/size of blocks. 

If you want to force the output to contain N blocks, one workaround here is to increase the target max block size to effectively disable the block splitting logic (set `DataContext.target_max_block_size` to a large number).",read task may generate multiple output depending size output exceed target block size split multiple behavior current ray data dynamically adjust want force output contain one increase target block size effectively disable block splitting logic set large number,issue,negative,positive,positive,positive,positive,positive
1841759321,Functionally looks OK; error message looks safe to ignore for now. We'll dig deeper into the root cause cc @scottjlee ,functionally error message safe ignore dig root cause,issue,negative,positive,positive,positive,positive,positive
1841701298,Manually tested either https://github.com/ray-project/ray/pull/41637 or https://github.com/ray-project/ray/pull/41603 can fix this issue.,manually tested either fix issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1841700027,"Per suggestion from @stephanie-wang, also implemented a more general fix for StreamingOutputsBackpressure https://github.com/ray-project/ray/pull/41637",per suggestion also general fix,issue,negative,positive,neutral,neutral,positive,positive
1841760559,"Hi @anyscalesam, would you mind transferring this issue back to the Ray repository since this is a Ray client issue? See https://github.com/ray-project/kuberay/issues/1687#issuecomment-1828701166 for more details. Thanks!",hi would mind transferring issue back ray repository since ray client issue see thanks,issue,negative,positive,neutral,neutral,positive,positive
1841606495,"@DanMcInerney let us know if you have follow-up questions. Otherwise, we can close it?",let u know otherwise close,issue,negative,neutral,neutral,neutral,neutral,neutral
1841603786,@alanwguo do you know if it's a known issue given it's related to Pydantic?,know known issue given related,issue,negative,neutral,neutral,neutral,neutral,neutral
1841601786,Dup of https://github.com/ray-project/ray/issues/30182. Closing it. Let's continue to the discussion there,dup let continue discussion,issue,negative,neutral,neutral,neutral,neutral,neutral
1841549993,"Hi @matthew29tang, can you help take a look of this PR? Thanks!",hi help take look thanks,issue,positive,positive,positive,positive,positive,positive
1841539307,"> Or are you talking about changing the whitespace on both the left and right sides of the page content?

Yes! This exactly! We want to remove the excess whitespace and the content should fill the whole page. 
",talking left right side page content yes exactly want remove excess content fill whole page,issue,negative,positive,positive,positive,positive,positive
1841518764,"@aslonnie thanks for the careful review.  I have updated the PR to match https://github.com/ray-project/ray/pull/40515, with the following exceptions:

New in 2.9.0
-> test_reporter.py.  You mentioned this should be fixed by using version in _version.py instead of hardcoding. This makes sense, I think we can just fix it on master and only fix it here if it breaks a test, but let me know if you think otherwise.

Changed from 2.8.0 to 2.9.0
-> the __init__.py update is now in _version.py instead.",thanks careful review match following new fixed version instead sense think fix master fix test let know think otherwise update instead,issue,positive,positive,neutral,neutral,positive,positive
1841500578,"* GKE
  - [x] GKE Node Auto-Provisioning is responsible for creating a new node pool for each new multi-host TPU PodSlice(https://github.com/ray-project/ray/issues/39781#issuecomment-1823241432).
  - [x] Provide a TPU webhook to ensure all Pods on the same TPU PodSlice belong to the same RayCluster custom resource (https://github.com/ray-project/ray/issues/39781#issuecomment-1823241432).
  

* TPU
  * TPU team should provide two utility functions:
    - [x] `get_podslice_id`: Get the unique ID of a PodSlice that this TPU Pod belongs to. I chatted with @jjyao yesterday. I am not sure how it is implemented, but Ray can currently set a unique ID for different TPU PodSlices, so I marked it as done.
    - [x] `get_tpu_worker_id`: Get a unique integer for each TPU Pod in the same PodSlice. This is used to set the environment variable `TPU_WORKER_ID`. If Ray offers an alternative method to set the environment, we can consider this task as completed. 
      * [Update]: Mark it as done. See [link1](https://github.com/ray-project/ray/blob/968aac2e19b45c2221edaa189333710dc777e9e3/python/ray/_private/accelerators/tpu.py#L258) and [link2](https://github.com/ray-project/ray/blob/968aac2e19b45c2221edaa189333710dc777e9e3/python/ray/util/accelerators/tpu.py#L7) for more details.

* KubeRay
  * Scheduling
    * The webhook provided by GKE should promise all Pods on the same TPU PodSlice belong to the same RayCluster custom resource. See the section ""GKE"" above.
    - [ ] KubeRay may need to add some configurations to enable the webhook to schedule all Pods in a RayCluster TPU worker group should be scheduled to the same TPU PodSlice.
  * Autoscaling
    * Ray Autoscaler should add/delete TPU worker groups to scale up/down TPU PodSlices.
    * KubeRay always respects the decisions made by Ray Autoscaler to create or delete specific Pods. It should not determine whether the Ray Pod is a TPU Pod, nor whether these four Ray Pods belong to the same TPU PodSlice. Ray Autoscaler should be the single source of the truth. Hence, the change should be in Ray Autoscaler rather than KubeRay.

* Ray Core
  - [ ] Setup environments
    - [ ] `TPU_WORKER_ID`: See [Deploy TPU workloads in GKE](https://cloud.google.com/kubernetes-engine/docs/how-to/tpus) for more details.
    - [ ] `TPU_WORKER_HOSTNAMES`: See [Deploy TPU workloads in GKE](https://cloud.google.com/kubernetes-engine/docs/how-to/tpus) for more details.
  - [x] Scheduling
    - [ ] `podslice_id` 
  - [ ] Autoscaling
    - [x] VM: #40463 
    - [ ] KubeRay

cc @jjyao @richardsliu @allenwang28 @ryanaoleary @richardliaw Does this summary make sense to you? Could you also check the status of each item? I think some items may have already been completed. Thanks!",node responsible new node pool new provide ensure belong custom resource team provide two utility get unique id pod yesterday sure ray currently set unique id different marked done get unique integer pod used set environment variable ray alternative method set environment consider task update mark done see link link provided promise belong custom resource see section may need add enable schedule worker group ray worker scale always made ray create delete specific determine whether ray pod pod whether four ray belong ray single source truth hence change ray rather ray core setup see deploy see deploy summary make sense could also check status item think may already thanks,issue,positive,positive,positive,positive,positive,positive
1841492030,It looks like the release process instruction has not been updated.  I will use https://github.com/ray-project/ray/pull/40515 as a reference and update the release instruction.,like release process instruction use reference update release instruction,issue,negative,neutral,neutral,neutral,neutral,neutral
1841488265,Let me break this in multiple PR to introduce sub-classes,let break multiple introduce,issue,negative,neutral,neutral,neutral,neutral,neutral
1841421293,"Sorry, I'm still not clear on what left hand margin you mean. The pytorch-lightning docs have lots of left hand margin on the primary sidebar:

![image](https://github.com/ray-project/ray/assets/14017872/6896989b-7c2c-4051-be96-2e5fd4809510)

Or are you talking about changing the whitespace on both the left and right sides of the page content?
![bitmap](https://github.com/ray-project/ray/assets/14017872/4f78abd0-7841-466d-9d90-84b02d3baa8b)
",sorry still clear left hand margin mean lot left hand margin primary image talking left right side page content,issue,positive,negative,neutral,neutral,negative,negative
1841406154,"> Thanks for this, we'll get this implemented. I had a few questions, though:
> 
> > Add icons to the nav bar
> 
> It looks like there are two designs here, one with icons and one without. Which one do you want to use? Can we get rid of the one you don't want?
> 
> ![image](https://private-user-images.githubusercontent.com/14017872/288163251-bbc0469b-4020-479e-b398-f8af155cd465.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDE4MDE5ODYsIm5iZiI6MTcwMTgwMTY4NiwicGF0aCI6Ii8xNDAxNzg3Mi8yODgxNjMyNTEtYmJjMDQ2OWItNDAyMC00NzllLWIzOTgtZjhhZjE1NWNkNDY1LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjA1VDE4NDEyNlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTUwNWQ1MzA1ZjRhOGFlZjFmMDEwNTI2MjZmYWY2ZGVmYzJkYzk1NGY5NDc4Mjk3NTI2MWVhZjM5MzI2YTcwMjgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.DLZLJOXBAe-2EPlj8BxVNJdErXmfZumKUi9IZSNdhAo)
> 
> > The nav bar should not have any margins on the left side as shown in the figma
> 
> It looks like there is lots of margin on the side here. You're asking for it to be removed when I implement it?
> 
> ![image](https://private-user-images.githubusercontent.com/14017872/288163004-6ad70246-c7c4-419b-b5a2-7990c4b9c527.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDE4MDE5ODYsIm5iZiI6MTcwMTgwMTY4NiwicGF0aCI6Ii8xNDAxNzg3Mi8yODgxNjMwMDQtNmFkNzAyNDYtYzdjNC00MTliLWI1YTItNzk5MGM0YjljNTI3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjA1VDE4NDEyNlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTFlNTg4OTRjNmVmMjMwMmFiYjY4Y2E3MmU5MDYyZDE1MzY4NWJkM2EwYzU2ZWI2NjEzYzJhNjM1ZDBhY2UyNmEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.J_XyvYc61GQI7Udn8ljJKD-MqwHC8Fr9p1GVXezFe6Y)

1. Let's start with the nav with icons and then we can get feedback and remove later. 
2. My bad. Let's reduce the margin. I image the nav bar looking like this - https://lightning.ai/docs/pytorch/stable/",thanks get though add bar like two one one without one want use get rid one want image bar left side shown like lot margin side removed implement image let start get feedback remove later bad let reduce margin image bar looking like,issue,positive,negative,negative,negative,negative,negative
1841401269,"Thanks for this, we'll get this implemented. I had a few questions, though:

> Add icons to the nav bar

It looks like there are two designs here, one with icons and one without. Which one do you want to use? Can we get rid of the one you don't want?

![image](https://github.com/ray-project/ray/assets/14017872/bbc0469b-4020-479e-b398-f8af155cd465)

> The nav bar should not have any margins on the left side as shown in the figma

It looks like there is lots of margin on the side here. You're asking for it to be removed when I implement it?

![image](https://github.com/ray-project/ray/assets/14017872/6ad70246-c7c4-419b-b5a2-7990c4b9c527)
",thanks get though add bar like two one one without one want use get rid one want image bar left side shown like lot margin side removed implement image,issue,positive,positive,neutral,neutral,positive,positive
1841390034,@tomchify are you still running into issues? Is there anything we can help with?,still running anything help,issue,negative,neutral,neutral,neutral,neutral,neutral
1841386882,"Thanks for the request @xuanbozeng!

I'm not sure if I fully understand the issue. My understanding is that the Prometheus server scrapes logs that are exposed by an endpoint in Ray. Ray is not pushing the metrics to Prometheus. At what point does Ray need to pass authentication info to Prometheus?",thanks request sure fully understand issue understanding server exposed ray ray pushing metric point ray need pas authentication,issue,positive,positive,positive,positive,positive,positive
1841363152,"For many model training, my personal preference would be to recommend Ray Core. It works well and offers the most about of flexibility for the user, which I think is practical for many model training because it doesn't have a one-size-fits-all solution.",many model training personal preference would recommend ray core work well flexibility user think practical many model training solution,issue,positive,positive,positive,positive,positive,positive
1841351045,"> ping on ^
Yes the changes are still WIP. Working on Fixing the UT failures and will update with a newer version. ",ping yes still working fixing ut update version,issue,negative,neutral,neutral,neutral,neutral,neutral
1841306749,"Actually I don't understand why this happens -- shouldn't Train/Tune be creating a placement group? Why do we think that the CPUs are still available?

Also, we need to have a solution that is independent of Train/Tune. Seems to me that this problem can definitely come up in other ways too. For example, if you are not using Train/Tune but instead create some actors that consume the Dataset.

The requirement should be that we never hang, even if the initial resource calculation is incorrect.",actually understand placement group think still available also need solution independent problem definitely come way example instead create consume requirement never even initial resource calculation incorrect,issue,positive,positive,neutral,neutral,positive,positive
1841274177,👍 Planning to update existing tests and also add a test that asserts that we only have POSIX-compliant characters in all file names in the `serve/` directory,update also add test file directory,issue,negative,neutral,neutral,neutral,neutral,neutral
1841270525,"> @Zandew what is the next step here? did we confirm root cause is #41258? If so are we going to target fixing the root cause so these tests will be passing in time for the ray29 cherry pick deadline?

It looks like #41461 introduce some other issues but they look simple to fix, just some incompatible args. But #41258 still exists too, and I don't know the root cause for that yet.",next step confirm root cause going target fixing root cause passing time ray cherry pick deadline like introduce look simple fix incompatible still know root cause yet,issue,negative,neutral,neutral,neutral,neutral,neutral
1841196274,"This is fixed for latest ray 2.8.1 in conda-forge/ray-packages-feedstock#133 where I merged the ray-dashboard package with ray-default so there is no longer a need to install something separate. I also added a passing test that this works: 
```
python -c ""import ray; ray.init(include_dashboard=True)""
```

Thanks for the report. Please reopen or open a new issue if you notice problems with the conda-forge packaging.",fixed latest ray package longer need install something separate also added passing test work python import ray thanks report please reopen open new issue notice,issue,positive,positive,positive,positive,positive,positive
1841123517,"@shrekris-anyscale, thank you for your support!
This is the minimal code
```
import logging

from fastapi import FastAPI
from fastapi.responses import JSONResponse
from ray import serve
from starlette.requests import Request

API_PREFIX = '/'

app = FastAPI()


@serve.deployment
class Service1:
    def __init__(self):
        self._service = None


@serve.deployment
@serve.ingress(app)
class Service2:
    def __init__(self, service1_deployment):
        self._service1 = service1_deployment.options(use_new_handle_api=True)

        self._logger = logging.getLogger(""ray.serve"")
        self._logger.setLevel(logging.DEBUG)

    @app.post('/call')
    async def call(self, request: Request) -> JSONResponse:
        payload = await request.json()
        self._logger.info(f'Input: {payload}')
        response = ''
        result = JSONResponse(status_code=200, content=dict(error=None,
                                                            response=response))
        return result


service1 = Service1.options(
    num_replicas=2,
    ray_actor_options=dict(num_cpus=.5),
    max_concurrent_queries=1,
).bind()

service2 = Service2.options(
    route_prefix=API_PREFIX,
    ray_actor_options=dict(num_cpus=.5),
    num_replicas=2,
    max_concurrent_queries=20,
).bind(service1)

if __name__ == '__main__':
    from time import sleep

    serve.run(service2, host='0.0.0.0', port=8002)

    sleep(120)
```

There are logs:
```
2023-12-05 17:12:58,388	WARNING deployment.py:404 -- DeprecationWarning: `route_prefix` in `@serve.deployment` has been deprecated. To specify a route prefix for an application, pass it into `serve.run` instead.
/Users/msokolov/Documents/PycharmProjects/ReCam/ML_backend/venv/lib/python3.9/site-packages/ray/serve/api.py:479: UserWarning: Specifying host and port in `serve.run` is deprecated and will be removed in a future version. To specify custom HTTP options, use `serve.start`.
  warnings.warn(
2023-12-05 17:13:00,457	INFO worker.py:1664 -- Started a local Ray instance. View the dashboard at 127.0.0.1:8265 
(ProxyActor pid=34234) INFO 2023-12-05 17:13:07,717 proxy 127.0.0.1 proxy.py:1072 - Proxy actor 1c5ba1470fdc5029ccae45ba01000000 starting on node b379b09c00bc7ef8fae5ec2462b4956e6983009ab0a2047ccf9ba2e1.
(ProxyActor pid=34234) INFO 2023-12-05 17:13:07,726 proxy 127.0.0.1 proxy.py:1257 - Starting HTTP server on node: b379b09c00bc7ef8fae5ec2462b4956e6983009ab0a2047ccf9ba2e1 listening on port 8002
(ProxyActor pid=34234) INFO:     Started server process [34234]
(ServeController pid=34231) INFO 2023-12-05 17:13:07,844 controller 34231 deployment_state.py:1379 - Deploying new version of deployment Service1 in application 'default'.
(ServeController pid=34231) INFO 2023-12-05 17:13:07,847 controller 34231 deployment_state.py:1379 - Deploying new version of deployment Service2 in application 'default'.
(ServeController pid=34231) INFO 2023-12-05 17:13:07,952 controller 34231 deployment_state.py:1668 - Adding 2 replicas to deployment Service1 in application 'default'.
(ServeController pid=34231) INFO 2023-12-05 17:13:07,962 controller 34231 deployment_state.py:1668 - Adding 2 replicas to deployment Service2 in application 'default'.
(ServeReplica:default:Service2 pid=34238) DEBUG 2023-12-05 17:13:55,421 Service2 default#Service2#IYHHWo long_poll.py:151 - LongPollClient polling timed out. Retrying.
```
And `LongPollClient polling timed out. Retrying.` appears every X seconds

Libraries:
ray                                      2.8.0
starlette                             0.27.0
fastapi                                0.103.2
",thank support minimal code import logging import import ray import serve import request class service self none class service self call self request request await response result return result service service service time import sleep service sleep warning specify route prefix application pas instead host port removed future version specify custom use local ray instance view dashboard proxy proxy actor starting node proxy starting server node listening port server process controller new version deployment service application controller new version deployment service application controller deployment service application controller deployment service application default service service default service polling timed polling timed every ray,issue,negative,positive,neutral,neutral,positive,positive
1841120384,"Same problem with `ray==2.8.0`, but manually found the module `/usr/local/lib/python3.10/site-packages/ray/_private/workers/default_worker.py` everywhere.
",problem manually found module everywhere,issue,negative,neutral,neutral,neutral,neutral,neutral
1840680949,Will this be fixed in an upcomming patch? Would gladly use Ray in my Thesis:),fixed patch would gladly use ray thesis,issue,negative,positive,positive,positive,positive,positive
1840434448,"Thanks for the clarification, have I missed that in the documentation? 

What is the intended use case for the streaming operator then? Can it be used from node to node? Or do I have to use dynamic (if that work for clients)?",thanks clarification documentation intended use case streaming operator used node node use dynamic work,issue,positive,positive,neutral,neutral,positive,positive
1840263097,@Zandew what is the next step here? did we confirm root cause is #41258? If so are we going to target fixing the root cause so these tests will be passing in time for the ray29 cherry pick deadline?,next step confirm root cause going target fixing root cause passing time ray cherry pick deadline,issue,negative,neutral,neutral,neutral,neutral,neutral
1840205616,"Thanks, Our cluster's read performance is very good, so I set the custom resources. The `_sample_block` is not coming from `map_groups` calls, it is from the `groupby` calls. I inspected the code and ray dashboard, and I believe the problem is that the sampling process before sort(to determine the range of each output blocks) somehow went into a serial execution, likely due to query the stats stored only once across the cluster.",thanks cluster read performance good set custom coming code ray dashboard believe problem sampling process sort determine range output somehow went serial execution likely due query across cluster,issue,negative,positive,positive,positive,positive,positive
1840079154,"(Additional question, not very related to the fail-to-load issue)
Is it possible to continue the same checkpoint directory and checkpoint index when using `resume_from_checkpoint`

For example, if I load from `Checkpoint(filesystem=local, path=/mnt/NAS/sda/ShareFolder/lidawei/ExperimentNotebook/ray_debug/XGBoost_ResumeExperiment/XGBoostTrainer_96983_00000_0_2023-12-05_13-53-18/checkpoint_000017)`

I am expecting if I create another XGBoostTrainer with `resume_from_checkpoint` pointing to this checkpoint and set `num_boost_round=20`, it will continue training another 20 rounds and get checkpoint like `checkpoint_000037`.
But instead, it will create another folder under `/mnt/NAS/sda/ShareFolder/lidawei/ExperimentNotebook/ray_debug/XGBoost_ResumeExperiment/`. And I will have to increase `num_boost_round` to like 40 (> 20) to continue the training. And the checkpoint index will start over from 0.

This would be inconvenient if I want to monitor this in TensorBoard which I expected it was the ""same experiment"".

I think this experience is a little bit weird.
Since if I want to restore from a failure I would use [`ray.train.xgboost.XGBoostTrainer.restore`](https://docs.ray.io/en/master/train/api/doc/ray.train.xgboost.XGBoostTrainer.restore.html) and do something like

```python
# NOTE: the datasets are the same here. But when using `resume_from_checkpoint` I would use incremental data.
trainer_restore = MyXGBoostTrainer.restore(os.path.dirname(os.path.dirname(checkpoint.path)), datasets={""train"": train_dataset, ""valid"": valid_dataset})
result_restore = trainer_restore.fit()
```",additional question related issue possible continue directory index example load create another pointing set continue training another get like instead create another folder increase like continue training index start would inconvenient want monitor experiment think experience little bit weird since want restore failure would use something like python note would use incremental data train valid,issue,positive,negative,negative,negative,negative,negative
1840067131,"Ok - I can constantly repro it now with this ^

Looks like the `Actor.__init__` is there forever with `Actor` in the component metrics. cc @rkooo567 ",constantly like forever actor component metric,issue,negative,neutral,neutral,neutral,neutral,neutral
1840066518,"```
diff --git a/python/ray/tests/test_metrics_agent.py b/python/ray/tests/test_metrics_agent.py
index 85c3c8aeb5..2fe5434c73 100644
--- a/python/ray/tests/test_metrics_agent.py
+++ b/python/ray/tests/test_metrics_agent.py
@@ -557,6 +557,8 @@ def test_per_func_name_stats(shutdown_only):
         def __init__(self):
             self.arr = np.random.rand(5 * 1024 * 1024)  # 40 MB
             self.shared_arr = ray.put(np.random.rand(5 * 1024 * 1024))
+            import time
+            time.sleep(5)
 
         def pid(self):
             return os.getpid()
```",git index self import time self return,issue,negative,neutral,neutral,neutral,neutral,neutral
1840055317,"Quick workaround now

```python
import xgboost

# Replace all XGBoostTrainer with MyXGBoostTrainer
class MyXGBoostTrainer(XGBoostTrainer):
    @staticmethod
    def get_model(checkpoint: Checkpoint) -> xgboost.Booster:
        """"""Retrieve the XGBoost model stored in this checkpoint.""""""
        with checkpoint.as_directory() as checkpoint_path:
            booster = xgboost.Booster()
            booster.load_model(
                os.path.join(checkpoint_path, 'model')
            )
            return booster

    def _save_model(self, model: xgboost.Booster, path: str) -> None:
        model.save_model(os.path.join(path, 'model'))
```",quick python import replace class retrieve model booster return booster self model path none path,issue,negative,positive,positive,positive,positive,positive
1840045341,"[ray.train.xgboost.xgboost_trainer — Ray 2.8.0](https://docs.ray.io/en/latest/_modules/ray/train/xgboost/xgboost_trainer.html#XGBoostTrainer.get_model)

https://github.com/ray-project/ray/blob/8362c507a18bd557eff94d2881f186d5257c36d7/python/ray/train/xgboost/xgboost_trainer.py#L92-L109

The filename is ""model.json""

https://github.com/ray-project/ray/blob/8362c507a18bd557eff94d2881f186d5257c36d7/python/ray/train/xgboost/xgboost_checkpoint.py#L18

But somehow the dump checkpoints are still binary

![image](https://github.com/ray-project/ray/assets/1515662/baecfe74-6c3b-43ec-97d3-198d90a37046)
",ray somehow dump still binary image,issue,negative,neutral,neutral,neutral,neutral,neutral
1840028544,@justinvyu @matthewdeng Could you clarify why this is critical enough to be picked into 2.9.0 (instead of in 2.10)?,could clarify critical enough picked instead,issue,negative,neutral,neutral,neutral,neutral,neutral
1840027814,"@justinvyu Nit:

> Users expect different failures types to be handled differently in step 4 above:
We need to update this part of the PR description (at the very top). At this point there are no 4 steps listed",nit expect different handled differently step need update part description top point listed,issue,negative,positive,positive,positive,positive,positive
1839913562,Hi @pcmoritz @thomasdesr could one of you review the telemetry changes?,hi could one review telemetry,issue,negative,neutral,neutral,neutral,neutral,neutral
1839886447,"The workaround is a poll action, I cannot use it because it will cause a heavy loop when I want get the result timely. But it's ok, I will wait for the new future :-)",poll action use cause heavy loop want get result timely wait new future,issue,negative,positive,neutral,neutral,positive,positive
1839885969,"@woshiyyya thanks, I added a new method for setting training resources in DataConfig instead. ",thanks added new method setting training instead,issue,negative,positive,positive,positive,positive,positive
1839879486,"We will add a cache size protection mechanism in the later PRs. Because we have several other code changes in the queue to be contributed, if we make the change here, then the following changes may have conflicts to resolve. Could we append the ""enhancement for the cache"" to the tail of our contribution queue?",add cache size protection mechanism later several code queue make change following may resolve could append enhancement cache tail contribution queue,issue,negative,neutral,neutral,neutral,neutral,neutral
1839834852,"Thanks for doing this, this looks great (I made some comments).

One thing that is a little unresolved: On the many model training, ideally we would have one way we recommend that works well and remove the others. Since this is a training workload, maybe @matthewdeng should make the call here.

View numbers from the docs (Sep 1 2023 - Dec 3 2023) -- this is very different from what we are currently recommending:

- tune/examples/batch_tuning.html (our current recommended way per https://docs.ray.io/en/master/ray-overview/use-cases.html#how-do-i-do-many-model-training-on-ray): 381
- data/examples/batch_training: 1425
- /ray-core/examples/batch_training: 703",thanks great made one thing little unresolved many model training ideally would one way recommend work well remove since training maybe make call view different currently current way per,issue,positive,positive,positive,positive,positive,positive
1839811102,"> The cluster has 16 CPUs. 1 CPU is used by the trainers.

The 1 CPU is used by Ray Train trainer, but not the Ray Train workers. By default we allocate Trainer with 1 CPU [[ref]](https://github.com/ray-project/ray/blob/990d87e43dc78f22d2c5ec1cd3180035a1f2858d/python/ray/air/config.py#L105).

We may need toalso  pass the `trainer_resources` into `BackendExecutor` here: https://github.com/ray-project/ray/blob/f0ac970dbe9d9f045dc10866110ccddf12bf3022/python/ray/train/data_parallel_trainer.py#L437

```
backend_executor = self._backend_executor_cls(
            backend_config=self._backend_config,
            trial_info=trial_info,
            num_workers=scaling_config.num_workers,
            num_cpus_per_worker=scaling_config.num_cpus_per_worker,
            num_gpus_per_worker=scaling_config.num_gpus_per_worker,
            additional_resources_per_worker=additional_resources_per_worker,
            max_retries=0,
           + trainer_resources=scaling_config._trainer_resources_not_none,
        )

```

then pass it to `DataConfig`, and subtract that amount of resources from the total resource limit.

**Update**: According to the offline discussion, we can directly pass `ScalingConfig.total_resources` to `DataConfig`.",cluster used used ray train trainer ray train default allocate trainer ref may need pas pas subtract amount total resource limit update according discussion directly pas,issue,negative,positive,neutral,neutral,positive,positive
1839803711,"> pyarrow 14.

Thanks @justinvyu - this is good to know!

> Have you also been seeing that the failure is flaky, but once it happens, every subsequent pyarrow fs call will give the same error?

Yes, this is the behavior we observed as well.

We have a variety of workloads distributed via Ray actors and tasks and a combination of both direct pyarrow and ray.data  - this error primarily occurs with direct pyarrow access.",thanks good know also seeing failure flaky every subsequent call give error yes behavior well variety distributed via ray combination direct error primarily direct access,issue,negative,positive,positive,positive,positive,positive
1839799761,"Thanks for following up @somal! Could you minimize the application-related code in the repro? It's not possible for us to run this now because the model is (understandably) redacted, and it's a tough to understand all the application-specific logic.

In general, [minimal reproductions](https://stackoverflow.com/help/minimal-reproducible-example) should provide as little code as possible that still produces the problem.",thanks following somal could minimize code possible u run model understandably tough understand logic general minimal provide little code possible still problem,issue,negative,negative,neutral,neutral,negative,negative
1839777145,"Hi @yanxiaod123 - Ray Data flattens the outermost dimension when storing numpy ndarray. So that's why you get each row as a `224*3` ndarray, and totally there're 458752 rows (`1024*448`). If you want to store each ndarray as whole, you can add an extra dimension of 1:

```py
>>> arr = [np.random.randint(10, size=(1, 448, 224, 3)) for _ in range(1024)]
>>> ds = ray.data.from_numpy(arr)
>>> ds
MaterializedDataset(
   num_blocks=1024,
   num_rows=1024,
   schema={data: numpy.ndarray(shape=(448, 224, 3), dtype=int64)}
)
```",hi ray data outermost dimension get row totally want store whole add extra dimension range data,issue,negative,positive,neutral,neutral,positive,positive
1839774908,"These experiments were done with the latest version as of 11/14/23 which was pyarrow 14.

@ykoyfman Have you also been seeing that the failure is flaky, but once it happens, every subsequent pyarrow fs call will give the same error?

What other Ray jobs are you running? And do you use pyarrow directly or is pyarrow used implicitly in the implementation of Ray (ex: Ray Data)?",done latest version also seeing failure flaky every subsequent call give error ray running use directly used implicitly implementation ray ex ray data,issue,negative,positive,neutral,neutral,positive,positive
1839770362,@rickyyx can you follow up with ^ (cpp regression test). I merged it as branch cut coming soon,follow regression test branch cut coming soon,issue,negative,neutral,neutral,neutral,neutral,neutral
1839764684,This test is unstable so it won't run during the release process. Can we mark it as stable if it's release blocking. Thankkks.,test unstable wo run release process mark stable release blocking,issue,negative,neutral,neutral,neutral,neutral,neutral
1839672706,"Can you try as a temporary workaround:

```
async def wait(obj_ref):
    while True:
      done, _ = ray.wait([obj_ref], timeout=0, fetch_local=False)
      if not done:
        await asyncio.sleep(0.1)
      else:
        return
```

We will try to support it natively in the future.",try temporary wait true done done await else return try support natively future,issue,positive,positive,positive,positive,positive,positive
1839665358,"> @rickyyx yes, this is a doc-specific lint check; still the right job to look at and has been failing for half a year. devprod is moving docinfra into its scope so we suppose to look at this. But agree on the P1.

Sounds good - i am assigning this back to you then? LMK if you need me to own this.",yes lint check still right job look failing half year moving scope suppose look agree good back need,issue,positive,positive,positive,positive,positive,positive
1839655429,@tvildo do you want to create a PR for it? We are happy to review it.,want create happy review,issue,positive,positive,positive,positive,positive,positive
1839646933,StreamingObjectRefGenerator doesn't work with Ray client for now so the error is expected.,work ray client error,issue,negative,neutral,neutral,neutral,neutral,neutral
1839601269,"Ah i see, i believe the `_sample_block` calls are coming from the `map_groups` calls here. Are you manually setting the cpu resources in the various Ray Data API calls in order to relieve bottlenecks? Is the issue worse if you remove the custom resourcing?

Another note, the Ray team is working on improvements around backpressure for Ray 2.9 (you can also try these on Ray nightly, but there may be some outstanding bugs). You can find updated information on these features here: https://github.com/ray-project/ray/issues/40754",ah see believe coming manually setting various ray data order relieve issue worse remove custom another note ray team working around ray also try ray nightly may outstanding find information,issue,positive,positive,neutral,neutral,positive,positive
1839578401,"@justinvyu What version of pyarrow are you seeing this with? We have also been running into this error with pyarrow 11.0-13.0. (Not with Ray train, but other Ray jobs.)",version seeing also running error ray train ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1839454509,Look like test_metrics_agent is still flaky,look like still flaky,issue,negative,neutral,neutral,neutral,neutral,neutral
1839451787,"This issue was opened before the major docs upgrade was completed, so my focus was there. I'll fix it this afternoon.",issue major upgrade focus fix afternoon,issue,negative,positive,neutral,neutral,positive,positive
1839430779,"@rickyyx yes, this is a doc-specific lint check; still the right job to look at and has been failing for half a year. devprod is moving docinfra into its scope so we suppose to look at this. But agree on the P1.",yes lint check still right job look failing half year moving scope suppose look agree,issue,negative,positive,neutral,neutral,positive,positive
1839427674,Is this still the right lint job to look at tho? @can-anyscale ,still right lint job look tho,issue,negative,positive,positive,positive,positive,positive
1839365592,@ddelange for this issue unfortunately no; we reserve backporting to critical issues as we release forward fairly often. please upgrade to ray29 and/or mitigate via the workaround documented above.,issue unfortunately reserve critical release forward fairly often please upgrade ray mitigate via,issue,negative,positive,neutral,neutral,positive,positive
1839303646,Yes it's https://github.com/ray-project/ray/pull/41194. But it's not ready to merge yet. Going down the rabbit hole...,yes ready merge yet going rabbit hole,issue,positive,positive,positive,positive,positive,positive
1839269132,"> @Zandew have you started work on this yet?

I have not started to work on this, this looks like a core issue. On the data side was fixed by locking the remote call #41299.",work yet work like core issue data side fixed locking remote call,issue,negative,neutral,neutral,neutral,neutral,neutral
1839246055,reviewed last week in core sprint review; don't have bandwidth right now but team is still planning to fix this for 29 release. cc @rkooo567 @jjyao @xieus ,last week core sprint review right team still fix release,issue,negative,positive,positive,positive,positive,positive
1839241426,"this is still all red for the last couple of hundred of runs and has been open for more than half a year now. i propose jailing this and marking this ticket as a p1. we have bigger fish to fry.

@maxpumperla @rickyyx agree?",still red last couple hundred open half year propose marking ticket bigger fish fry agree,issue,negative,negative,neutral,neutral,negative,negative
1839239876,"@angelinalg is this truly p0 (it means it's gotta make it into the current target ray release _and_ most likely get in a couple of days if possible into nightly).

in the context of docs basically ASAP. This has been open for 3w with no update still...",truly got ta make current target ray release likely get couple day possible nightly context basically open update still,issue,negative,neutral,neutral,neutral,neutral,neutral
1839211507,"docker build in windows is significantly slower than linux when creating layers: https://buildkite.com/ray-project/premerge/builds/13523#018c315f-55ca-4c59-8912-5561e499468e/6-5799 (20 minutes after ray build). I'm still investigating this, but this problem will alleviate for the most part when we port wanda to windows",docker build significantly ray build still investigating problem alleviate part port,issue,negative,positive,positive,positive,positive,positive
1839210293,"> can you tell me what file within prometheus client you changed?

metrics_core.py: Adding new `SumMetricFamily` with type counter 
exposition.py: in `generate_latest`  check if Metric has type counter and is `SumMetricFamily`, don't append the `_total` to metric name",tell file within client new type counter check metric type counter append metric name,issue,negative,positive,positive,positive,positive,positive
1839188351,"> hmm for some reasons, the window failure https://buildkite.com/ray-project/premerge/builds/12710#018c1ba3-c93a-4044-8e52-c36b89269752 lasts.
> 
> can you merge the latest master?

Sure, will do",window failure merge latest master sure,issue,negative,positive,positive,positive,positive,positive
1839185347,thanks! I will merge the PR shortly. We may need a short RFC for the API semantics.,thanks merge shortly may need short semantics,issue,negative,positive,neutral,neutral,positive,positive
1839184811,"hmm for some reasons, the window failure https://buildkite.com/ray-project/premerge/builds/12710#018c1ba3-c93a-4044-8e52-c36b89269752 lasts.

can you merge the latest master? ",window failure merge latest master,issue,negative,positive,neutral,neutral,positive,positive
1839039379,RLlib tests are failing. Please don’t merge unless those are resolved. ,failing please merge unless resolved,issue,negative,neutral,neutral,neutral,neutral,neutral
1838757680,merge conflict + there are bunch of test failures. ,merge conflict bunch test,issue,negative,neutral,neutral,neutral,neutral,neutral
1837894509,"I think this issue has relatively low priority. If QS has higher priorities, we should do those first.",think issue relatively low priority higher first,issue,negative,positive,positive,positive,positive,positive
1837794502,"@jjyao Your PR works fine, it can fix the memory leak",work fine fix memory leak,issue,negative,positive,positive,positive,positive,positive
1837781191,"@edoakes @zcin All relevant tests are passing. If there's no more feedback, this change is ready to merge.",relevant passing feedback change ready merge,issue,negative,positive,positive,positive,positive,positive
1837721610,"> * Can the cache size grow without bound? Is it a concern?

Thanks for this comment, it makes a good point. I'll enhance this on next MR. Will open a new jira to track this.


>     * We should have unit tests for the cache.

Yes, unit tests is on latter MR, will send it to you also.",cache size grow without bound concern thanks comment good point enhance next open new track unit cache yes unit latter send also,issue,negative,positive,positive,positive,positive,positive
1837620131,"Most of these recent commits were to fix flaking tests. I also added 3 tests:

1. [`test_autoscaling_status_changes`](https://github.com/ray-project/ray/pull/41317/files#diff-414ce0a58693afa133401329dd7f612a5b0523e998c92853c233885cb66931beR1400): integration test that checks the status behavior for plain autoscaling deployments when they're redeployed.
2. [`test_deployment_info_serialization`](https://github.com/ray-project/ray/pull/41317/files#diff-10c79dc7c66488239ec58263dc13467a70805410334094c9376b8b5cd7e41879R17): unit test that checks `DeploymentInfo` serialization and deserialization.
3. [`test_redeploy_different_num_replicas`](https://github.com/ray-project/ray/pull/41317/files#diff-10c79dc7c66488239ec58263dc13467a70805410334094c9376b8b5cd7e41879R17): unit test that checks the status behavior for static deployments when they're redeployed.

The code logic still functions the same. Here's the diff between the current commit ([dabc52c](https://github.com/ray-project/ray/pull/41317/commits/dabc52c2d3c65b4b8c328b1da3eec30ce7cd0f54)) and [49becb8](https://github.com/ray-project/ray/pull/41317/commits/49becb8f68ae6a25a56446cfe5ec2f595ad858e0): [see diff](https://github.com/ray-project/ray/pull/41317/files/88f47001e600db11bf93060accd07123301edf76..dabc52c2d3c65b4b8c328b1da3eec30ce7cd0f54).",recent fix also added integration test status behavior plain unit test serialization unit test status behavior static code logic still current commit see,issue,negative,positive,neutral,neutral,positive,positive
1837505870,"This error actually stems from the mismatch between the signature of the decorated function `wrapped` and the original function `func`, which can be fixed by manually modify the signature as (inside `wrapper` before return):

```
    original_signature = inspect.signature(fn)
    new_params = list(original_signature.parameters.values()) + [
        inspect.Parameter(""kw"", inspect.Parameter.POSITIONAL_OR_KEYWORD)
    ]
    wrapped.__signature__ = original_signature.replace(parameters=new_params)
```

However, the user might expect that a function call works in Ray as well as it works in Python. I would suggest Ray should not raise such errors, let Python raise.",error actually mismatch signature decorated function wrapped original function fixed manually modify signature inside wrapper return list however user might expect function call work ray well work python would suggest ray raise let python raise,issue,negative,positive,positive,positive,positive,positive
1837383823,@vonsago Just checking to see if you get a chance to verify whether my PR fixes the memory leak?,see get chance verify whether memory leak,issue,negative,neutral,neutral,neutral,neutral,neutral
1837368018,https://buildkite.com/ray-project/release/builds/2876#018c2761-92b1-44e3-bb6b-fe847d4c1682 Runs good. The first try failed due to infra error. Close this ticket.,good first try due infra error close ticket,issue,negative,positive,positive,positive,positive,positive
1837309490,"Would love to see this as well, a CLI option like `--force-setup-update` that triggers this behavior would be wonderful!",would love see well option like behavior would wonderful,issue,positive,positive,positive,positive,positive,positive
1837301002,@raychen911  you are right. Do you want to create a PR to fix it? I'll help the review.,right want create fix help review,issue,positive,positive,positive,positive,positive,positive
1837204625,"I think this is more of a Ray Data issue, not Ray Core. Ray Data uses Ray Core's shared memory to store outputs (DataIterator/Trainer inputs)

Ray Data currently does not pin memory. It might not make sense to do so for shared memory right now because we store Arrow blocks, not the final tensor batches in shared memory (although this may change in the future, see #41571). Also, shared memory blocks are usually read-once (unless `ds.materialize()` is used).

But it should be easy enough to plug in something here. The way to do it would be to write a custom [collate_fn](https://docs.ray.io/en/latest/data/api/doc/ray.data.DataIterator.iter_torch_batches.html#ray.data.DataIterator.iter_torch_batches) that manages a pinned buffer and allocates the returned tensor batches there. Are you interested in contributing this? We'd be interested in seeing the results!",think ray data issue ray core ray data ray core memory store ray data currently pin memory might make sense memory right store arrow final tensor memory although may change future see also memory usually unless used easy enough plug something way would write custom pinned buffer returned tensor interested interested seeing,issue,positive,positive,positive,positive,positive,positive
1837076395,"Ran into this error when attempting to iter_batches over a hugging face dataset 
```python
import ray
import datasets

hf_dataset = datasets.load_dataset(""food101"")
ray_ds = ray.data.from_huggingface(hf_dataset[""train""])
print(""Data:"")
print(ray_ds.take(1))


for i, batch in enumerate(ray_ds.iter_batches(batch_size=32)):
    print(i)


print(""finished"")
```

<img width=""1708"" alt=""Screenshot 2023-12-02 at 8 40 02 PM"" src=""https://github.com/ray-project/ray/assets/50382570/b2c60d28-9266-4287-b3be-d4f9dc623bfe"">
",ran error hugging face python import ray import food train print data print batch enumerate print print finished,issue,negative,neutral,neutral,neutral,neutral,neutral
1837018058,"> @aslonnie I prefer to use `rebase` not `merge`, so I update the commits.

I will squash and merge it for you.",prefer use rebase merge update squash merge,issue,negative,neutral,neutral,neutral,neutral,neutral
1837017906,"fyi, you do not need to worry about merging the PR. I will squash and merge it for you.",need worry squash merge,issue,negative,neutral,neutral,neutral,neutral,neutral
1836981811,"> What about the places where we use DataContext.get_current() during planning, e.g., [here](https://github.com/ray-project/ray/blob/master/python/ray/data/_internal/logical/rules/set_read_parallelism.py#L24C24-L24C24)? Don't we need to propagate the DataContext through to those?

Good point. So this PR can fix the case for training jobs, where be different datasets will be proposed to different processes (the SplitCoordinator actors) for execution.
But if they run in the driver process, it's still a bug. I created an issue https://github.com/ray-project/ray/issues/41573 for tracking. 


 ",use need propagate good point fix case training different different execution run driver process still bug issue,issue,negative,positive,positive,positive,positive,positive
1836979928,"@raulchen I used [`parquet_metadata_resolution`](https://buildkite.com/ray-project/release/builds/2911#018c2806-f506-4818-953d-6bbe64501281) release test to debug the case you mentioned, I found that there is spilling/eventual OOM with the previous approach. Instead of executing the plan containing only `InputDataBuffer`, I tried a new approach by directly using the Read logical op's `_datasource_or_legacy_reader.get_read_tasks()` to construct the output `LazyBlockList` (logic in `get_legacy_lazy_block_list_read_only()`). Any concerns overall with this approach?

[Test run of `parquet_metadata_resolution`](https://buildkite.com/ray-project/release/builds/2911#018c2806-f506-4818-953d-6bbe64501281/548-569) took 37.18 seconds, which is consistent with the current runtimes. Also checked runtime and throughput of `torch_batch_inference_1_gpu_10gb_raw`, they are also consistent with current results.",used release test case found previous approach instead plan tried new approach directly read logical construct output logic overall approach test run took consistent current also checked throughput also consistent current,issue,negative,positive,neutral,neutral,positive,positive
1836961826,"Otherwise looks good to me. Since you mentioned we don't want to merge this in Ray 2.9, I've added a label. We can merge it after the Ray 2.9 branch cut.",otherwise good since want merge ray added label merge ray branch cut,issue,negative,positive,positive,positive,positive,positive
1836939900,All comments are addressed except for this request - https://github.com/ray-project/ray/pull/41461#discussion_r1412666733 . Can you help take another look? Thanks @ericl and @stephanie-wang.,except request help take another look thanks,issue,positive,positive,positive,positive,positive,positive
1836928496,Changed to require `concurrency` or `compute` to be set per offline discussion with @ericl. So we no longer have the default `currency=None` behavior.,require concurrency compute set per discussion longer default behavior,issue,negative,neutral,neutral,neutral,neutral,neutral
1836923735,The original reason for the revert should be fixed by https://github.com/ray-project/ray/pull/41565,original reason revert fixed,issue,negative,positive,positive,positive,positive,positive
1836878893,"Another way is to add the following to the list of imports in the beginning of the file. (No need to import inside the train_func)
```
from torch.backends.cudnn import benchmark as cudnn_benchmark

def train_func(train_loop_config):
      ...
      cudnn_benchmark = True
      ...

```
",another way add following list beginning file need import inside import true,issue,negative,positive,positive,positive,positive,positive
1836874023,"I noticed that Vale didn't catch ""MacOS"" > ""macOS"" and ""HomeBrew"" > ""Homebrew"". Could we add that to the wordlist?",vale catch could add,issue,negative,neutral,neutral,neutral,neutral,neutral
1836871841,"Bug seems to be in worker_pool.cc here, We have to escape storage url before passing it 

https://github.com/ray-project/ray/blob/f07e319ac7ea3ec3764599a519f59db3f55c8e8c/src/ray/raylet/worker_pool.cc#L299-L319",bug escape storage passing,issue,negative,neutral,neutral,neutral,neutral,neutral
1836850349,Linking the upstream issue https://github.com/apache/arrow/issues/25822 here as well to increase awareness for this :),linking upstream issue well increase awareness,issue,positive,neutral,neutral,neutral,neutral,neutral
1836829037,"[Not Ray team, but I used map_groups quite a lot lately] You may find previous discussions about `map_groups` useful (search on github). 

> all the data will be gathered to one single node, as `batch_size` is `None` 

According to Ray team, the data are materialized, but not necessarily gathered into one node. (After all, sorting is distributed)

>  what is the motivation that we use map_batchs(DEMO) and map_groups(DEMO()),

I believe this is simply because they haven't implemented for callable class yet. This should be a straight-forward PR unless there are some limitations (in that case Ray team knows better)

>  the difference that we use one map_batch + group_fn in map_group and we use [map_batch(group) for group in grouped data].

It's about the block boundary (in Ray) vs group boundary (in data). We want the data that goes into the mapping function contains the whole group.
",ray team used quite lot lately may find previous useful search data one single node none according ray team data necessarily one node distributed motivation use believe simply callable class yet unless case ray team better difference use one use group group grouped data block boundary ray group boundary data want data go function whole group,issue,positive,positive,neutral,neutral,positive,positive
1836608716,Yes this is fixed in the new checkpoint implementation,yes fixed new implementation,issue,negative,positive,positive,positive,positive,positive
1836608132,"Closing this ticket as stale, please reopen/create a new ticket if this is still happening in a later Ray version",ticket stale please new ticket still happening later ray version,issue,negative,negative,negative,negative,negative,negative
1836603760,"> Ray Serve doesn't do any physical memory isolation itself.

I was a little off about this– it turns out Ray does provide _some_ physical isolation for GPUs. Ray automatically sets the `CUDA_VISIBLE_DEVICES` env var when an actor sets `num_gpus` to attempt to limit which GPUs the actor can access. If actors are using fractional GPUs, then Ray will attempt to pack the actors into the same GPU before moving onto another GPU. See [these docs](https://docs.ray.io/en/releases-2.8.0/ray-core/tasks/using-ray-with-gpus.html#using-gpus-in-tasks-and-actors) for more info.

If multiple actors are packed into the same GPU, it's still their responsibility to make sure that the GPU memory doesn't overflow.",ray serve physical memory isolation little turn ray provide physical isolation ray automatically actor attempt limit actor access fractional ray attempt pack moving onto another see multiple still responsibility make sure memory overflow,issue,negative,positive,neutral,neutral,positive,positive
1836593470,New release test run here: https://buildkite.com/ray-project/release/builds/2855#_,new release test run,issue,negative,positive,positive,positive,positive,positive
1836494633,@michaelhly they might be just deleted. I think the best place to add tests is under `python/ray/dags/tests`. You don't need to rely on serve.,might think best place add need rely serve,issue,positive,positive,positive,positive,positive,positive
1836491428,"```diff
Unmerged paths:
  (use ""git add/rm <file>..."" as appropriate to mark resolution)
	deleted by them: python/ray/serve/tests/test_gradio_visualization.py
```

@jjyao Any idea where these tests got moved to?",unmerged use git file appropriate mark resolution idea got,issue,negative,positive,positive,positive,positive,positive
1836488116,"@michaelhly Thanks for the contribution and sorry for missing this PR. Could you mark it as ready for review and rebase with master to fix conflicts. After that, I'll merge the PR.",thanks contribution sorry missing could mark ready review rebase master fix merge,issue,negative,negative,neutral,neutral,negative,negative
1836296341,"@wingkitlee0 Also what is the motivation that we use
`map_batchs(DEMO)` and `map_groups(DEMO())`, can we unified them? I think map_groups and map_batchs should have similiar parameter and the difference is map_groups is performced on grouped data. Something I want to understand is the difference that we use one map_batch + group_fn in map_group and we use [map_batch(group) for group in grouped data].",also motivation use unified think parameter difference grouped data something want understand difference use one use group group grouped data,issue,positive,neutral,neutral,neutral,neutral,neutral
1836263211,"@wingkitlee0 I see! Thanks. Based on my search results, the `map_groups` function is indeed based on the `map_batch` function, but the `batch_size` parameter is set to `None`. 
This means that if the original data is stored across different nodes in a distributed way, all the data will be gathered to one single node, as `batch_size` is `None` . Therefore, it is possible that the memory requirements could be very large, depending on the size of the data being processed. Also the max number of parallelism is also depended on single node?  Or it's possible one batch of data can be across different nodes?(but batch_to_block is used in side the group_fn)?
Thanks!

```
 # Note we set batch_size=None here, so it will use the entire block as a batch,
    # which ensures that each group will be contained within a batch in entirety.
    return sorted_ds.map_batches(
        group_fn,
        batch_size=None,
        compute=compute,
        batch_format=batch_format,
        fn_args=fn_args,
        fn_kwargs=fn_kwargs,
        **ray_remote_args,
    )
```",see thanks based search function indeed based function parameter set none original data across different distributed way data one single node none therefore possible memory could large depending size data also number parallelism also single node possible one batch data across different used side thanks note set use entire block batch group within batch entirety return,issue,negative,positive,neutral,neutral,positive,positive
1836181032,"@jjyao could I bring some attention to this? I'd really appreciate this PR being merged, its seems simple enough and fixes high priority issue for us.",could bring attention really appreciate simple enough high priority issue u,issue,negative,positive,neutral,neutral,positive,positive
1836150614,"I was able to use `tune.run()` instead of `tune.Tuner().fit()` but it stil seems to be not working. The way I asses that is by visualizing an episode run  of 3 environement:

1. The initial one I want to retrieve
2. an environment after attempt to restore weights
3. an environment after one step

And 2. and 3. have similar behavior, different from 1.


Side problem is that [tune.run is absent from documentation](https://docs.ray.io/en/master/tune/api/api.html). So I first thought that it was being deprecated. I finally found the info I needed [in the function implementation in the repo](https://github.com/ray-project/ray/blob/master/python/ray/tune/tune.py#L234) but wasn't straightforward at all. 

Questions still remains:
1. Is `tune.run` absent from the docs because it's being deprecated ?
2. The weight retrieval still doesn't work with `tune.run` and `tune.Tuner().fit()` + callbacks but works with `Algorithm.from_checkpoint(path_to_checkpoint)`",able use instead working way episode run initial one want retrieve environment attempt restore environment one step similar behavior different side problem absent documentation first thought finally found function implementation straightforward still remains absent weight retrieval still work work,issue,negative,positive,positive,positive,positive,positive
1835987631,"I've made some progress, will post more over the w/e - it's CPU-usage related...",made progress post related,issue,negative,neutral,neutral,neutral,neutral,neutral
1835701629,"> Don't we need to remove the standalone page too?

Ah, yeah, we do 🤦.",need remove page ah yeah,issue,negative,neutral,neutral,neutral,neutral,neutral
1835686460,The test failures are unrelated. @edoakes this PR is ready to merge.,test unrelated ready merge,issue,negative,positive,positive,positive,positive,positive
1835673348,is this something that qualifies for a bugfix/cherry-pick release on existing minor versions? Maybe even going back to 2.6?,something release minor maybe even going back,issue,negative,negative,neutral,neutral,negative,negative
1835607395,"For PPO algorithm the picture is a bit different. 

**TrainPPO.py**

import gymnasium as gym
import numpy as np
from tqdm import trange
from ray.rllib.algorithms.ppo import PPOConfig

env_name = ""CartPole-v1""

config = PPOConfig()
config = config.training(
    gamma=0.9, 
    lr=0.01, 
    kl_coeff=0.3,
    train_batch_size=128)
config = config.resources(num_gpus=0)
onfig = config.rollouts(num_rollout_workers=1)

algo = config.build(env=""CartPole-v1"")

iterator = trange(100)
for epoch in iterator:
    result = algo.train()
    iterator.set_postfix({
        ""reward_max"": result[""episode_reward_max""],
        ""reward_mean"": result[""episode_reward_mean""],
    })

print(result)
save_result = algo.save(""-"".join((""checkpointPPO"", env_name)))
path_to_checkpoint = save_result.checkpoint.path
print(
    ""An Algorithm checkpoint has been created inside directory: ""
    f""'{path_to_checkpoint}'.""
)
algo.stop()

**RestorePPO.py**

from tqdm import trange
import numpy as np
from ray.rllib.algorithms import Algorithm

env_name = ""CartPole-v1""
algo1 = Algorithm.from_checkpoint(""-"".join((""checkpointPPO"", env_name)))
ppo_policy1= algo1.workers.local_worker().get_policy()
ppo_model_weights1 = ppo_policy1.get_weights()
print(ppo_model_weights1)  # Do something with

algo2 = Algorithm.from_checkpoint(""-"".join((""checkpointPPO"", env_name)))
ppo_policy2= algo2.workers.local_worker().get_policy()
ppo_model_weights2 = ppo_policy1.get_weights()
print(ppo_model_weights2)  # Do something with

for key in ppo_model_weights1.keys(): 
    comparison_result = np.array_equal(ppo_model_weights1[key], ppo_model_weights2[key])
    print(f""Weights of layer {key} comparison = {comparison_result}"")


for epoch in range(10):
    result = algo1.train()
    print(""epoch=%(epoch)d reward_max=%(reward_max)f reward_mean=%(reward_mean)f"" % {
        ""epoch"": epoch,
        ""reward_max"": result[""episode_reward_max""],
        ""reward_mean"": result[""episode_reward_mean""],
    })

**Results:** 

Weights of layer encoder.actor_encoder.net.mlp.0.weight comparison = True
Weights of layer encoder.actor_encoder.net.mlp.0.bias comparison = True
Weights of layer encoder.actor_encoder.net.mlp.2.weight comparison = True
Weights of layer encoder.actor_encoder.net.mlp.2.bias comparison = True
Weights of layer encoder.critic_encoder.net.mlp.0.weight comparison = True
Weights of layer encoder.critic_encoder.net.mlp.0.bias comparison = True
Weights of layer encoder.critic_encoder.net.mlp.2.weight comparison = True
Weights of layer encoder.critic_encoder.net.mlp.2.bias comparison = True
Weights of layer pi.net.mlp.0.weight comparison = True
Weights of layer pi.net.mlp.0.bias comparison = True
Weights of layer vf.net.mlp.0.weight comparison = True
Weights of layer vf.net.mlp.0.bias comparison = True",algorithm picture bit different import gymnasium gym import import import epoch result result result print result print algorithm inside directory import import import algorithm print something print something key key key print layer key comparison epoch range result print epoch epoch epoch result result layer weight comparison true layer bias comparison true layer weight comparison true layer bias comparison true layer weight comparison true layer bias comparison true layer weight comparison true layer bias comparison true layer weight comparison true layer bias comparison true layer weight comparison true layer bias comparison true,issue,positive,positive,positive,positive,positive,positive
1835602341," Good morning! 

I also have found out some strange things with model weights after restoring algorithm from checkpoint . 

With the updated **Restore.py:** 

from tqdm import trange
import numpy as np
from ray.rllib.algorithms import Algorithm

env_name = ""CartPole-v1""
algo = Algorithm.from_checkpoint(""-"".join((""checkpoint"", env_name)))
rl_module = algo.workers.local_worker().module
weights=weights2={}
for layer in rl_module.layers:
    weights[layer.name] = layer.get_weights()  # returns a list of numpy arrays
    print(f""Weights of layer {layer.name}: {weights[layer.name]}"")

algo2 = Algorithm.from_checkpoint(""-"".join((""checkpoint"", env_name)))
rl_module2 = algo2.workers.local_worker().module
for layer in rl_module2.layers:
    weights2[layer.name] = layer.get_weights()  # returns a list of numpy arrays
    print(f""Weights of layer {layer.name}: {weights[layer.name]}"")

for layer in rl_module2.layers:
    comparison_result = np.array_equal(weights[layer.name], weights2[layer.name])
    print(f""Weights of layer {layer.name} comparison = {comparison_result}"")

for epoch in range(10):
    result = algo.train()
    print(""epoch=%(epoch)d reward_max=%(reward_max)f reward_mean=%(reward_mean)f"" % {
        ""epoch"": epoch,
        ""reward_max"": result[""episode_reward_max""],
        ""reward_mean"": result[""episode_reward_mean""],
    })

**I have following results:** 

Weights of layer vector_encoder comparison = False
Weights of layer vector_decoder comparison = False
Weights of layer world_model comparison = False
Weights of layer actor comparison = False
Weights of layer critic comparison = False
Weights of layer dreamer_model comparison = False
",good morning also found strange model algorithm import import import algorithm layer list print layer layer list print layer layer print layer comparison epoch range result print epoch epoch epoch result result following layer comparison false layer comparison false layer comparison false layer actor comparison false layer critic comparison false layer comparison false,issue,negative,negative,negative,negative,negative,negative
1835566921,"Thanks for the quick response!
I created a request to add [a round robin replica balancing feature](https://github.com/ray-project/ray/issues/41555).
",thanks quick response request add round robin replica balancing feature,issue,negative,positive,positive,positive,positive,positive
1835504829,"@jjyao Thank you for solving the problem quickly. I will try to verify your solution later. My solution: when idle receives the pushtask request, it establishes a connection and requests the driver worker to release the connection when the idle exits. [#41395](https://github.com/ray-project/ray/pull/41395). I wonder if this approach is more concise.",thank problem quickly try verify solution later solution idle request connection driver worker release connection idle wonder approach concise,issue,positive,positive,positive,positive,positive,positive
1835502866,@vonsago can you try this wheel (https://ray-ci-artifact-pr-public.s3.amazonaws.com/cbd696e980a700a88799aea8e43cde67cdc25431/tmp/artifacts/.whl/.whl/ray-3.0.0.dev0-cp38-cp38-manylinux2014_x86_64.whl) and see if it fixes the leak?,try wheel see leak,issue,negative,neutral,neutral,neutral,neutral,neutral
1835478175,"> btw do we not use eventfd anymore? Cannot find code.

Yup I got rid of it!",use find code got rid,issue,negative,neutral,neutral,neutral,neutral,neutral
1835372493,"Ya, I think `map_groups` does not support ""callable class"". It currently only supports running the mapping function as actors (e.g., `compute=ray.data.ActorPoolStrategy()`.
In your example, using `DEMO()` would work if you are okay with creating the instance first..
```
sorted_ds = ds.groupby(key=""a"").map_groups(DEMO(),
        compute=ray.data.ActorPoolStrategy(size=10),
        batch_format  = 'pandas')
```",ya think support callable class currently running function example would work instance first,issue,negative,positive,positive,positive,positive,positive
1835316818,@Zandew thanks for this! Could you post a link to the successful run for the new test?,thanks could post link successful run new test,issue,positive,positive,positive,positive,positive,positive
1835279659,"The trick of passing the checkpoint via `""start_from_checkpoint""` parameter to `tune.Tuner().param_space` found [here](https://docs.ray.io/en/latest/tune/faq.html#how-can-i-continue-training-a-completed-tune-experiment-for-longer-and-with-new-configurations-iterative-experimentation) doesn't work either :/ ",trick passing via parameter found work either,issue,negative,neutral,neutral,neutral,neutral,neutral
1835273755,"> So currently Ray Serve won't be able to actually run multiple models on one GPU, keeping the last one in memory unless there is memory pressure, is that correct?

If the sum of required memory for all models does not exceed the memory of 1 GPU, you can run multiple models on one GPU in Ray Serve without releasing memory of any model.

> Or is it possible to keep a variable of the last model used, and make remote calls to delete the pipes from unused models?

It's possible to do that. At least, it can complete your task.
",currently ray serve wo able actually run multiple one keeping last one memory unless memory pressure correct sum memory exceed memory run multiple one ray serve without memory model possible keep variable last model used make remote delete unused possible least complete task,issue,negative,positive,neutral,neutral,positive,positive
1835267868,"Related to #40777, #32751, #36761, #36830, #41290 and #40347
All on loading previously train Model/Policies. ",related loading previously train,issue,negative,negative,neutral,neutral,negative,negative
1835153790,"> One more request: with this API change, can we disallow specifying unbounded autoscaling, e.g., None? The reason is that the vast majority of the time, users get a better experience using a fixed sized pool. So we want to encourage this as the default, and make sure that autoscaling is an advanced feature users explicitly opt into.

@ericl - what's the good default value you are thinking? I am thinking we can use the fixed actor pool size with 1 actor. But this would be quite slow, and not usable, so users have to set a `concurrency` by themselves anyway.

Another option is to always request users to provide a `concurrency` setting. For 2.9, either `compute` or `concurrency` has to be set (to maintain backward compatibility). For 2.10, `concurrency` has to be set by itself.",one request change disallow unbounded none reason vast majority time get better experience fixed sized pool want encourage default make sure advanced feature explicitly opt good default value thinking thinking use fixed actor pool size actor would quite slow usable set concurrency anyway another option always request provide concurrency setting either compute concurrency set maintain backward compatibility concurrency set,issue,positive,positive,positive,positive,positive,positive
1835080643,"Closing this PR because it is no longer needed for https://github.com/ray-project/ray/pull/41466. 

For any future readers, there is an issue related to pickling the operator when creating a deepcopy of it (e.g. in MongoDatasource, use of MongoClient results in pickling a  `_thread.lock` object).",longer future issue related operator use object,issue,negative,neutral,neutral,neutral,neutral,neutral
1834703554,"@rickyyx  it's still single thread. There are two threads:
- gRPC thread
- Ray thread

All work modifying the context is in Ray thread, like checking whether it's disconnected.

Managing RPC lifecycle is done in gRPC thread. Switching between the thread is the place where things go wrong.

I definitely feel there is a huge gap to make these two work efficiently and and safely. This means the API in gRPC is not easy to use and we need some wrapper on it.",still single thread two thread ray thread work context ray thread like whether disconnected done thread switching thread place go wrong definitely feel huge gap make two work efficiently safely easy use need wrapper,issue,positive,positive,neutral,neutral,positive,positive
1834620507,"The `vendoring` readme states:

> This tool has no stability promises -- it has only one intended user: pip. There may be unannounced changes to this codebase at any time, as long as the intended user (i.e. the pip project) is prepared for those changes.
> 
> As a general rule of thumb, if the project is going to be a PyPI package, it should not use this tool.

so I guess we need something else.",tool stability one intended user pip may unannounced time long intended user pip project prepared general rule thumb project going package use tool guess need something else,issue,negative,positive,neutral,neutral,positive,positive
1834618082,I added the path to the os $PATH in `ray/_private/internal_third_party.py` when you `import ray._private.internal_third_party`,added path o path import,issue,negative,neutral,neutral,neutral,neutral,neutral
1834596486,"> It does not look like we can effortlessly add big packages like aiohttp to the in-tree vendors. We need to fix all import aiohttp.a.b in the package itself to relative imports import .aiohttp.a.b notice the . before aiohttp. This applies transitively to all its dependencies. And we will bear the burden to do this every time we upgrade the deps.

Yes, I guess that's why pip developed a tool called `vendoring` for this purpose.

Actually in your PR, how do you manage to not change all imports?",look like effortlessly add big like need fix import package relative import notice transitively bear burden every time upgrade yes guess pip tool purpose actually manage change,issue,positive,neutral,neutral,neutral,neutral,neutral
1834594351,It does not look like we can effortlessly add big packages like aiohttp to the in-tree vendors. We need to fix all `import aiohttp.a.b` in the package itself to relative imports `import .aiohttp.a.b` notice the `.` before `aiohttp`. This applies transitively to all its dependencies. And we will bear the burden to do this every time we upgrade the deps.,look like effortlessly add big like need fix import package relative import notice transitively bear burden every time upgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
1834591187,"So now we have these 4 kinds of third party codes:


|                          | where                          | visible to users        | used in                                                  | pkgs                                  |
|--------------------------|--------------------------------|-------------------------|----------------------------------------------------------|---------------------------------------|
| declared dependencies    | $PYTHONPATH                    | yes                     | everywhere                                               | a lot, e.g. jsonschema and protobuf   |
| in-tree vendors          | ray/_private/thirdparty        | no?                     | everywhere                                               | dacite, pathspec, pynvml, tabulate    |
| ""public vendors""         | ray/thirdparty_files           | yes after  `import ray` | everywhere                                               | psutil, setproctitle==1.2.2, colorama |
| (NEW!) ""private vendors"" | ray/_private/third_party_files | no                      | only Ray internal processes (Dashboard, RuntimeEnvAgnet) | aiohttp, aiohttp_cors, aiosignal      |


I agree this is a little bit scattered. How do we reconcile? @pcmoritz @aslonnie @jjyao ",third party visible used declared yes everywhere lot everywhere dacite tabulate public yes import ray everywhere new private ray internal dashboard agree little bit scattered reconcile,issue,positive,negative,neutral,neutral,negative,negative
1834585442,Seems pip has a tool https://pypi.org/project/vendoring/ that it uses to vendor its dependencies. Something we can see if we can leverage.,pip tool vendor something see leverage,issue,negative,neutral,neutral,neutral,neutral,neutral
1834527252,"@anyscalesam not yet, have some other work i am trying to get in before branch cut. talked to Cheng and he said we can investigate this after branch cut",yet work trying get branch cut cheng said investigate branch cut,issue,negative,neutral,neutral,neutral,neutral,neutral
1834464995,"@vonsago I guess it's hard to know which module should call it since you don't know if you still need the connection in the near future. Instead I have #41535 that tries to fix it in another way. Could you try it out and see if it fixes the memory leak of your workload. Let me know which python version you are using, I can give you the corresponding wheel.",guess hard know module call since know still need connection near future instead fix another way could try see memory leak let know python version give corresponding wheel,issue,negative,negative,neutral,neutral,negative,negative
1834448947,"Could you provide a minimal repro and the Ray version, so we can reproduce the issue?",could provide minimal ray version reproduce issue,issue,negative,negative,neutral,neutral,negative,negative
1834445641,"Ray Serve doesn't do any physical memory isolation itself. The `num_gpus` is used a logic resource to determine where to schedule Ray tasks and actors (such as the deployment replica). Setting `num_gpus=0.5` on your replica tells Ray that it can schedule 2 replicas on a node with 1 GPU. However, it's the replicas' responsibilities ensure that they don't consume too much memory and trigger an OOM.",ray serve physical memory isolation used logic resource determine schedule ray deployment replica setting replica ray schedule node however ensure consume much memory trigger,issue,negative,positive,neutral,neutral,positive,positive
1834434600,"> but if you run job submission, the local machine and cluster can easily have the different dependencies (same issue as ray client), which can bring lots of headache. I think it is possible to make it work, but it'd be pretty rough user experience

hmmm, i might be misunderstanding the worker setup hook's usecase, but Isn't the worker setup function expected to be run on the remote cluster tho? Like it's just some part of code in the entrypoint script moved before task is run? ",run job submission local machine cluster easily different issue ray client bring lot headache think possible make work pretty rough user experience might misunderstanding worker setup hook worker setup function run remote cluster tho like part code script task run,issue,positive,positive,neutral,neutral,positive,positive
1834416222,"The current version of ray installs `grpcio==1.59.3`, I don't think we should downgrade to `1.56.2`. Reproducing the failure exactly as reported requires a `fed_config.yaml` file. @Adjei-Mensah has not responded. I will close this, assuming it is fixed by #35472 and associated changes. Please re-open if it still reproduces.",current version ray think downgrade failure exactly file close assuming fixed associated please still,issue,negative,positive,neutral,neutral,positive,positive
1834412987,@scottjlee did you get a chance to look into this yesterday? Do we have a root cause on why this test is failing?,get chance look yesterday root cause test failing,issue,negative,neutral,neutral,neutral,neutral,neutral
1834326369,"Does the file you uploaded reproduce the issue for you? I get a binary string out of it. 

In any case, could you explain more about your workflow: how exactly are you saving the model to transfer it (a reproducer script would be the best way to convey the idea)? Pickle is a fragile option for storage. Maybe use a cross-platform serialization technique like the ones described in point 1 of [the documentation for checkpointing](https://docs.ray.io/en/latest/train/user-guides/checkpoints.html#saving-checkpoints-during-training)?",file reproduce issue get binary string case could explain exactly saving model transfer reproducer script would best way convey idea pickle fragile option storage maybe use serialization technique like point documentation,issue,positive,positive,positive,positive,positive,positive
1834307203,"```
conda create --name raycheck python=3.10
conda activate raycheck
conda install pip
pip install ""ray[air]""
python -c ""import ray; dataset = ray.data.read_csv('s3://anonymous@air-example-data/breast_cancer.csv')""
```

The dataset cleanly loads. Closing, please reopen if the problem returns.",create name activate install pip pip install ray air python import ray cleanly please reopen problem,issue,negative,positive,positive,positive,positive,positive
1834287955,"Thank for that context Pring. So currently Ray Serve won't be able to actually run multiple models on one GPU, keeping the last one in memory unless there is memory pressure, is that correct? 

Or is it possible to keep a variable of the last model used, and make remote calls to delete the pipes from unused models?",thank context currently ray serve wo able actually run multiple one keeping last one memory unless memory pressure correct possible keep variable last model used make remote delete unused,issue,negative,positive,neutral,neutral,positive,positive
1834282380,@mattip can you try the latest standard ray docker image and run it with his repro script to cross out any possibility it's an issue on the ray source code side and it's only dependency mud?,try latest standard ray docker image run script cross possibility issue ray source code side dependency mud,issue,negative,positive,positive,positive,positive,positive
1834244816,"I will close this, the need for GPutil has been removed and the detection code refactored. Please open a new issue if this reappears.",close need removed detection code please open new issue,issue,negative,positive,neutral,neutral,positive,positive
1834216334,"All comments are addressed, please take another look whenever you have time, thanks!",please take another look whenever time thanks,issue,positive,positive,positive,positive,positive,positive
1834145037,"I have realised that it is not a matter of pre-processing after all. But nevertheless it turns out that balancing is not clear how it works.
I raised 4 replicas and ran a load test with 5 users. You can see that the load on the replicas is not evenly distributed.  Tried fixing this with `RAY_scheduler_spread_threshold=0.0`. Nothing changed.

![5dcf694c372295bb024a4ec0f95fe17fcc24f0ef](https://github.com/ray-project/ray/assets/47638600/f1052eab-5c59-4bcb-9517-f03f18763901)

If you run multiple ray containers with 1 replica each and do nginx balancing, then everything is fine. But this should be provided by ray, right? Can you tell me how to set up round-robin balancing?",matter nevertheless turn balancing clear work raised ran load test see load evenly distributed tried fixing nothing run multiple ray replica balancing everything fine provided ray right tell set balancing,issue,positive,positive,positive,positive,positive,positive
1833949014,"I fell like it might be due to me using `tune.Tuner().fit() `and not `tune.run()`. In [this other example](https://discuss.ray.io/t/how-to-pretrain-a-model-with-behavior-cloning/278/4) with train(), it seem to work for the person that tried. Is there a way that fit reinitialize the weights ? Can you actually prevent that ? ",fell like might due example train seem work person tried way fit actually prevent,issue,positive,positive,neutral,neutral,positive,positive
1833920852,Closing because I couldn't reproduce with Python 3.9 and Ray 2.3. Feel free to re-open if you run into this again.,could reproduce python ray feel free run,issue,positive,positive,positive,positive,positive,positive
1833885573,"Closing because you can't disable block splitting anymore, so this isn't an issue. See https://github.com/ray-project/ray/pull/38235",ca disable block splitting issue see,issue,negative,neutral,neutral,neutral,neutral,neutral
1833865374,"@aslonnie I prefer to use `rebase` not `merge`, so I update the commits.",prefer use rebase merge update,issue,negative,neutral,neutral,neutral,neutral,neutral
1833850526,"To add more context, Vale works by converting RST files to HTML with `rst2html`. There are two major problems with this implementation:
1. We can't lint API references because they're defined in Python files.
2. We can't lint content in non-standard RST directives like `tabbed`.

If we build the documentation and then run Vale on the built HTML, we can lint both API references and content in widgets. The disadvantage of this approach is that it's a lot slower than running Vale directly on the RST files.",add context vale work converting two major implementation ca lint defined python ca lint content like build documentation run vale built lint content disadvantage approach lot running vale directly,issue,negative,positive,neutral,neutral,positive,positive
1833779515,"Closing because I don't think this is an issue anymore:

```python
import numpy as np
from PIL import Image

import ray

image0 = Image.fromarray(np.zeros((512, 512, 3), dtype=np.uint8))
image1 = Image.fromarray(np.zeros((100, 100, 3), dtype=np.uint8))
image2 = Image.fromarray(np.zeros((100, 100), dtype=np.uint8))
image0.save(""/tmp/image0.png"")
image1.save(""/tmp/image1.png"")
image2.save(""/tmp/image2.png"")


ds = ray.data.read_images(""/tmp"").materialize()
```",think issue python import import image import ray image image image,issue,negative,neutral,neutral,neutral,neutral,neutral
1833644160,@lundybernard Did your build from source idea ever work?,build source idea ever work,issue,negative,neutral,neutral,neutral,neutral,neutral
1833532431,"FYI: we (polars) now serialise via IPC by default as of `0.19.7` (which was released back in early October), so I'd expect this to be _much_ faster by default now (~20x or so?) 🤔",via default back early expect faster default,issue,negative,positive,neutral,neutral,positive,positive
1833390961,"The issue is that `UnionOperator.inputs_done` assumes that task are completed in the order of their indices:

https://github.com/ray-project/ray/blob/42c8e0bd3c52fe9036a57dbba4b3bd9e3829075d/python/ray/data/_internal/execution/operators/union_operator.py#L67-L81
",issue task order index,issue,negative,neutral,neutral,neutral,neutral,neutral
1833269576,"I've updated test:

```python
import base64
import concurrent.futures
import logging
import time

import numpy as np
import requests
import torch
from ray import serve
from torchaudio.transforms import Resample as TResample
from transformers import pipeline

logger = logging.getLogger(""ray.serve"")

torch.set_num_threads(1)


@serve.deployment(ray_actor_options={""num_gpus"": 0.1, ""num_cpus"": 1}, num_replicas=2)
class Preprocessor:
    def __init__(self):
        self.default_resampler = TResample(
            orig_freq=8000,
            new_freq=16000,
            dtype=torch.float32,
            resampling_method=""sinc_interp_kaiser"",
        ).to(""cuda"")

    def __call__(self, audio):
        raw_audio = base64.b64decode(audio)
        samples = np.frombuffer(raw_audio, dtype=np.int16)
        samples = samples.astype(np.float32) / np.iinfo(samples.dtype).max
        with torch.inference_mode():
            samples = torch.from_numpy(samples).to(""cuda"")
            samples = self.default_resampler(samples)
            samples = samples.to(""cpu"")
            return samples.numpy()


@serve.deployment(
    ray_actor_options={""num_gpus"": 0.33, ""num_cpus"": 2},
    autoscaling_config={""min_replicas"": 2, ""max_replicas"": 3},
)
class Translator:
    def __init__(self, process):
        self.process = process.options(use_new_handle_api=True)
        self.pipe = pipeline(
            ""automatic-speech-recognition"",
            model=""openai/whisper-medium"",
            chunk_length_s=30,
            device=""cuda"",
        )

    async def translate(self, request) -> str:
        samples = await self.process.remote(request[""audio""])
        with torch.inference_mode():
            transcription = self.pipe(samples, generate_kwargs={""language"": ""russian""})[
                ""text""
            ]
        return [{""text"": transcription}]

    async def __call__(self, request):
        request = await request.json()
        return await self.translate(request)


app = Translator.options(route_prefix=""/translate"").bind(Preprocessor.bind())
serve.run(app)


def send_request(blob):
    s = time.time()
    resp = requests.post(f""http://127.0.0.1:8000/translate"", json={""audio"": blob})
    print(resp.json())
    return time.time() - s


with open(
    ""/tests/resources/test.wav"", ""rb""
) as f:
    blob = f.read()
    blob = base64.b64encode(blob).decode(""ascii"")

print(send_request(blob))
print(send_request(blob))
print(send_request(blob))
print(send_request(blob))

time.sleep(3)
print(""TEST"")
with concurrent.futures.ThreadPoolExecutor() as executor:
    results = []
    for i in range(2):
        results.append(executor.submit(send_request, blob))
    print(""RESULTS"")
    for future in concurrent.futures.as_completed(results):
        result = future.result()
        print(result)

```

Output:
```bash
2023-11-30 13:00:33,350 WARNING deployment.py:404 -- DeprecationWarning: `route_prefix` in `@serve.deployment` has been deprecated. To specify a route prefix for an application, pass it into `serve.run` instead.
2023-11-30 13:00:34,964 INFO worker.py:1664 -- Started a local Ray instance. View the dashboard at 127.0.0.1:8265 
(ProxyActor pid=3197416) INFO 2023-11-30 13:00:36,631 proxy 10.80.0.21 proxy.py:1072 - Proxy actor 7f31fcd30dd1ad253d5c382c01000000 starting on node 6bb61fbb55ab6769c9a44e80afde55c19a5a660a435d26bfe02aa3a9.
(ProxyActor pid=3197416) INFO 2023-11-30 13:00:36,635 proxy 10.80.0.21 proxy.py:1257 - Starting HTTP server on node: 6bb61fbb55ab6769c9a44e80afde55c19a5a660a435d26bfe02aa3a9 listening on port 8000
(ProxyActor pid=3197416) INFO:     Started server process [3197416]
(ServeController pid=3197339) INFO 2023-11-30 13:00:36,755 controller 3197339 deployment_state.py:1379 - Deploying new version of deployment Preprocessor in application 'default'.
(ServeController pid=3197339) INFO 2023-11-30 13:00:36,759 controller 3197339 deployment_state.py:1379 - Deploying new version of deployment Translator in application 'default'.
(ServeController pid=3197339) INFO 2023-11-30 13:00:36,862 controller 3197339 deployment_state.py:1668 - Adding 2 replicas to deployment Preprocessor in application 'default'.
(ServeController pid=3197339) INFO 2023-11-30 13:00:36,871 controller 3197339 deployment_state.py:1668 - Adding 2 replicas to deployment Translator in application 'default'.
(ServeReplica:default:Translator pid=3197448) Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
(ServeReplica:default:Preprocessor pid=3197446) INFO 2023-11-30 13:00:45,819 Preprocessor default#Preprocessor#AQQHpC 39ed7197-32fd-4a07-9968-bdb7291cea5f /translate default replica.py:726 - __CALL__ OK 73.2ms
[{'text': ' test'}]
0.9717648029327393
(ServeReplica:default:Translator pid=3197449) INFO 2023-11-30 13:00:46,664 Translator default#Translator#XTFRyI 39ed7197-32fd-4a07-9968-bdb7291cea5f /translate default replica.py:726 - __CALL__ OK 924.5ms
(ServeReplica:default:Preprocessor pid=3197446) INFO 2023-11-30 13:00:46,682 Preprocessor default#Preprocessor#AQQHpC 92b3ae19-ab35-4031-82d9-a3e4c662f743 /translate default replica.py:726 - __CALL__ OK 0.7ms
[{'text': ' test'}]
0.8590915203094482
(ServeReplica:default:Preprocessor pid=3197446) INFO 2023-11-30 13:00:47,537 Preprocessor default#Preprocessor#AQQHpC 4b6625d2-1820-4230-be75-d8c67aa84373 /translate default replica.py:726 - __CALL__ OK 0.7ms
(ServeReplica:default:Translator pid=3197448) INFO 2023-11-30 13:00:47,526 Translator default#Translator#YRwxid 92b3ae19-ab35-4031-82d9-a3e4c662f743 /translate default replica.py:726 - __CALL__ OK 851.7ms
[{'text': ' test'}]
0.7534689903259277
(ServeReplica:default:Translator pid=3197449) INFO 2023-11-30 13:00:48,280 Translator default#Translator#XTFRyI 4b6625d2-1820-4230-be75-d8c67aa84373 /translate default replica.py:726 - __CALL__ OK 747.7ms
(ServeReplica:default:Preprocessor pid=3197447) INFO 2023-11-30 13:00:48,364 Preprocessor default#Preprocessor#SSdZWA b73ebbd5-bee3-426e-9b82-a748fcdb0c1d /translate default replica.py:726 - __CALL__ OK 73.3ms
[{'text': ' test'}]
0.8116359710693359
(ServeReplica:default:Translator pid=3197448) INFO 2023-11-30 13:00:49,092 Translator default#Translator#YRwxid b73ebbd5-bee3-426e-9b82-a748fcdb0c1d /translate default replica.py:726 - __CALL__ OK 805.6ms
TEST
RESULTS
(ServeReplica:default:Preprocessor pid=3197447) INFO 2023-11-30 13:00:52,110 Preprocessor default#Preprocessor#SSdZWA 98dd517a-c5f7-4ff6-a37c-1c61d1f94db9 /translate default replica.py:726 - __CALL__ OK 0.8ms
(ServeReplica:default:Preprocessor pid=3197447) INFO 2023-11-30 13:00:52,111 Preprocessor default#Preprocessor#SSdZWA 461e56eb-6b0b-4799-9af3-30ea17eae4da /translate default replica.py:726 - __CALL__ OK 0.7ms
(ServeReplica:default:Translator pid=3197449) Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[{'text': ' test'}]
[{'text': ' test'}]
1.2893760204315186
1.2903039455413818
(ServeReplica:default:Translator pid=3197449) INFO 2023-11-30 13:00:53,385 Translator default#Translator#XTFRyI 98dd517a-c5f7-4ff6-a37c-1c61d1f94db9 /translate default replica.py:726 - __CALL__ OK 1280.6ms
(ServeReplica:default:Translator pid=3197448) INFO 2023-11-30 13:00:53,385 Translator default#Translator#YRwxid 461e56eb-6b0b-4799-9af3-30ea17eae4da /translate default replica.py:726 - __CALL__ OK 1280.1ms
```",test python import base import import logging import time import import import torch ray import serve import resample import pipeline logger class self self audio audio return class translator self process pipeline translate self request await request audio transcription language text return text transcription self request request await return await request blob resp audio blob print return open blob blob blob ascii print blob print blob print blob print blob print test executor range blob print future result print result output bash warning specify route prefix application pas instead local ray instance view dashboard proxy proxy actor starting node proxy starting server node listening port server process controller new version deployment application controller new version deployment translator application controller deployment application controller deployment translator application default translator special added vocabulary make sure associated word trained default default default test default translator translator default translator default default default default test default default default default translator translator default translator default test default translator translator default translator default default default default test default translator translator default translator default test default default default default default default default translator special added vocabulary make sure associated word trained test test default translator translator default translator default default translator translator default translator default,issue,positive,positive,neutral,neutral,positive,positive
1833171644,"When you define an actor class using ray_actor_options={""num_gpus"": 1 ""num_cpus"": 0},  the actor created from this class must have one GPU reserved for it for the duration of the actor's lifetime. Since you have only one GPU, you will only be able to create one such actor. 

In Ray's internal implementation, it seems that the GPU card and memory occupation are applied using num_gpus, so you can set the num_gpus to 0.5, so that in the case of only 1 GPU card, the application can be successful, so as to avoid the log problem you sent above. However, you should note that 0.5 cards is not enough to load your model, you need to initialize your model at each call and empty your gpu memory usage at the end, which can be done by (for example):

```
del self.pipe
del self.refiner
gc.collect()
torch.cuda.empty_cache()
```",define actor class actor class must one reserved duration actor lifetime since one able create one actor ray internal implementation card memory occupation applied set case card application successful avoid log problem sent however note enough load model need initialize model call empty memory usage end done example,issue,negative,positive,positive,positive,positive,positive
1833116887,"Thanks for your quick feedback. I have solved this problem recently. It might be triggered by the memory error due to the dataset I input was very large (~ 40G). I used the 'tune.with_parameters' to wrapper the trainable module with the static dataset, and found the bug gone. I hope it could give you some ideas. As the bug is gone, so I closed the issue. 

Best,
Yuheng.",thanks quick feedback problem recently might triggered memory error due input large used wrapper trainable module static found bug gone hope could give bug gone closed issue best,issue,positive,positive,positive,positive,positive,positive
1833103610,"Since this is just cherry picking the PRs, I think we can just merge it whenever the tests look good -- I also kicked off the macOS ones but we probably don't need to block on that :)",since cherry think merge whenever look good also probably need block,issue,negative,positive,positive,positive,positive,positive
1833067685,@thomasdesr @edoakes this one should be ready to merge. Can one of you merge for me?,one ready merge one merge,issue,negative,positive,positive,positive,positive,positive
1833040806,"@jjyao I know it's because of this, but I'm not sure which module should call `disconnect`.",know sure module call disconnect,issue,negative,positive,positive,positive,positive,positive
1833020241,"Mac os tests ran for 3hr before timing out for an unrelated issue. I've manually tested that the broken tests are passing w/ this change and also verified that all of the symptoms observed in CI failures were the issue that this was targeting to fix.

Going to merge this to unblock making a cherry-pick PR.",mac o ran timing unrelated issue manually tested broken passing change also issue fix going merge unblock making,issue,negative,negative,negative,negative,negative,negative
1833018925,initial decision: we disallow every other runtime env other than container when container runtime env is used. We will follow up upon user requests,initial decision disallow every container container used follow upon user,issue,negative,neutral,neutral,neutral,neutral,neutral
1833015580,btw we should make sure this fix won't be missed to be picked..,make sure fix wo picked,issue,negative,positive,positive,positive,positive,positive
1832964826,"fork actually doesn't work very well with Ray (mostly due to gRPC, but it could have so many things that can go wrong). I think using spawn is the best approach here. 

",fork actually work well ray mostly due could many go wrong think spawn best approach,issue,negative,positive,positive,positive,positive,positive
1832939755,"@c21 w00t awesome, thanks, I'll close this one then",awesome thanks close one,issue,positive,positive,positive,positive,positive,positive
1832935343,"but if you run job submission, the local machine and cluster can easily have the different dependencies (same issue as ray client), which can bring lots of headache. I think it is possible to make it work, but it'd be pretty rough user experience",run job submission local machine cluster easily different issue ray client bring lot headache think possible make work pretty rough user experience,issue,positive,positive,neutral,neutral,positive,positive
1832934115,"> 1. otherwise we should serialize function in user's local machine and send it to the cluster)

Why don't we do this then? This is what the runtime env already doing with ray.init right? ",otherwise serialize function user local machine send cluster already right,issue,negative,positive,positive,positive,positive,positive
1832932939,Gave my suggested priority above. We should fix it relatively sooner as this has been a confusing thing for a long time.,gave priority fix relatively sooner thing long time,issue,negative,negative,neutral,neutral,negative,negative
1832929093,"> lmk when you finish ^

Actually looks like each task would result in 2-3 KiB data in GCS:
- 100K -> 300MiB
- 1M -> 3GiB 

Wonder if that would be ok. Or we could do 500K -> 1.5GiB max. 

Also, without running tasks - GCS already uses up 1.7 GiB. 
",finish actually like task would result data mib gib wonder would could also without running already gib,issue,negative,neutral,neutral,neutral,neutral,neutral
1832921658,"> Does it need some documentation or if this was actually already documented with worker_process_setup_hook's usage?

We need more doc. I think we should actually refine this API.. ",need documentation actually already usage need doc think actually refine,issue,negative,neutral,neutral,neutral,neutral,neutral
1832905692,"More thoughts;

```
Can we make the legend easier to understand? If possible, users shouldn’t have to open up the doc to understand what they are. Can we add some simple explanation within each legend:
SPILLED: objects spilled to disk or remote storage.
MMAP_DISK: objects stored as memory-mapped page?
MMAP_SHM: objects stored as memory-mapped page?
WORKER_HEAP: objects stored in the memory of worker processes

Improve the doc of the metrics page for each of the metric above

Improve the doc about the object store (the whitepaper has a good description of it but the doc has too little about it)
the different components of it
how they work together
how to configure each of the component
```",make legend easier understand possible open doc understand add simple explanation within legend disk remote storage page page memory worker improve doc metric page metric improve doc object store good description doc little different work together configure component,issue,positive,positive,neutral,neutral,positive,positive
1832901574,we should write the correct error msg & documentation,write correct error documentation,issue,negative,neutral,neutral,neutral,neutral,neutral
1832901230,"I think it is kind of expected. Feel like the best practice is 

1. users are supposed to use this in ray.init instead of job submission (otherwise we should serialize function in user's local machine and send it to the cluster)
2. if users use job submission, they should use the string-based API",think kind feel like best practice supposed use instead job submission otherwise serialize function user local machine send cluster use job submission use,issue,positive,positive,positive,positive,positive,positive
1832883946,I will accept the revert if you create a release blocker that requires reverting this revert back to close it.,accept revert create release blocker revert back close,issue,positive,neutral,neutral,neutral,neutral,neutral
1832875765,@aslonnie let's revert this first and fix forward with another PR to keep signals on these tests; it might be more complicated than a bucket permission change (user is anonymous),let revert first fix forward another keep might complicated bucket permission change user anonymous,issue,negative,negative,negative,negative,negative,negative
1832875685,"I should add that I checked that the checkpoint were correctly saved. If I do use 
```

path_to_checkpoint = ""/blablabla/ray_results/PPO_2023-11-29_02-51-09/PPO_CustomEnvironment_c4c87_00000_0_2023-11-29_02-51-09/checkpoint_000008""

algo = Algorithm.from_checkpoint(path_to_checkpoint)

```
and then use `algo.compute_single_action()`/ run the environment for several steps and then visualize the agents. I get the correct output. 

It's really when trying to keep training those previous policies using the method described above that it fails. ",add checked correctly saved use use run environment several visualize get correct output really trying keep training previous method,issue,negative,positive,neutral,neutral,positive,positive
1832875202,"> Can you add unit tests to check for the concurrency set but fn is not callable class cases?

@stephanie-wang, yes plan to add all unit test for the new `concurrency` argument. Looks like we have a general consensus on the API change now. Will start adding unit test.",add unit check concurrency set callable class yes plan add unit test new concurrency argument like general consensus change start unit test,issue,negative,positive,neutral,neutral,positive,positive
1832874362,@architkulkarni oh this a release test infra issue. It's on me. https://github.com/ray-project/ray/pull/41489 is a revert that will fix this but I'm still investigate since we cannot really revert anyscale.,oh release test infra issue revert fix still investigate since really revert,issue,negative,positive,positive,positive,positive,positive
1832861535,"@can-anyscale could it be an issue with the release test infra?  

The underlying job succeeded: 
- https://buildkite.com/ray-project/release/builds/2507#018c1a87-5d1b-45b6-9663-b71856eade0d/463-494
- https://console.anyscale-staging.com/o/anyscale-internal/jobs/prodjob_dyelqm5q85stabtgm2mkb9ehxk  

As far as I can tell the release test fails because after the test, the test infra tries to download some kidn of `output.json` file.
```
[ERROR 2023-11-29 10:18:59,229] util.py: 137  Retry function call failed due to 404 GET https://storage.googleapis.com/download/storage/v1/b/anyscale-oss-dev-bucket/o/working_dirs%2Fgcp_cluster_launcher_nightly_image%2Fdesfobmomh%2Ftmp%2Foutput.json?alt=media: No such object: anyscale-oss-dev-bucket/working_dirs/gcp_cluster_launcher_nightly_image/desfobmomh/tmp/output.json: ('Request failed with status code', 404, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PARTIAL_CONTENT: 206>) in 10 seconds...
```

The blamed commit looks unrelated.",could issue release test infra underlying job far tell release test test test infra file error retry function call due get object status code one blamed commit unrelated,issue,negative,negative,neutral,neutral,negative,negative
1832829955,"@rkooo567  it's not about the default worker. The problem here is that the GPU is set in the runtime when you execute the task. So the only way to do is to wrapper the actual function.

In this way, we need to update @ray.remote implementation which I'm not sure whether it's the right implementation.

If we shall do this, then the flow is like:

- Setup the special OS env when launching ray worker
- Always wrap the actual function and check the OS env and call mpi init.

In this way, the user doesn't need to do anything. But the downside is that the implementation is not clean.

The best way to do this is to make runtime env able to rewrite the function. But the difficulity is that runtime env can also be cluster layer and doesn't fit into the scenario here. So in this way, we'll have runtime env specific for functions, which might also be good.


I think we can do this, make this experimental in the documentation. If people complaint, and we see places where wrapper the function in runtime env is useful, we shall update it.",default worker problem set execute task way wrapper actual function way need update implementation sure whether right implementation shall flow like setup special o ray worker always wrap actual function check o call way user need anything downside implementation clean best way make able rewrite function also cluster layer fit scenario way specific might also good think make experimental documentation people complaint see wrapper function useful shall update,issue,positive,positive,positive,positive,positive,positive
1832778984,"Core logic LGTM, will approve once comments on the tests are addressed and Cindy confirms there are no unintended effects of changing the `clear_target_state` behavior.",core logic approve unintended effect behavior,issue,negative,neutral,neutral,neutral,neutral,neutral
1832693568,Congrats on your first PR with Ray! Keep more coming....,first ray keep coming,issue,negative,positive,positive,positive,positive,positive
1832682897,"FYI: this is the console log I see after trying to run one deployment after a different deployment on the same server:

(ServeReplica:default:APIIngress pid=1346403) mark_task_as_started text-to-image__e67367c330dff62d6418ccab51afc903
(ServeController pid=1346323) INFO 2023-11-29 20:49:22,491 controller 1346323 deployment_state.py:1481 - Autoscaling replicas for deployment TextToImage in application default from 0 to 1. Current ongoing requests: [], current handle queued queries: 1.
(ServeController pid=1346323) INFO 2023-11-29 20:49:22,495 controller 1346323 deployment_state.py:1668 - Adding 1 replica to deployment TextToImage in application 'default'.
(raylet) E1129 20:49:22.899110778 1346720 thread_manager.cc:42]       Could not create grpc_sync_server worker-thread
(raylet) E1129 20:49:22.899130657 1346720 thread_manager.cc:145]      assertion failed: worker->created()
(raylet) *** SIGABRT received at time=1701290962 on cpu 5 ***
(raylet) PC: @     0x7f16772d400b  (unknown)  raise
(raylet)     @     0x7f16775f1420  (unknown)  (unknown)
(raylet)     @     0x7f167624ac68        144  grpc::Server::Start()
(raylet)     @     0x7f1676251c71        288  grpc::ServerBuilder::BuildAndStart()
(raylet)     @     0x7f1676123a22        560  ray::rpc::GrpcServer::Run()
(raylet)     @     0x7f1675e84f98        960  ray::core::CoreWorker::CoreWorker()
(raylet)     @     0x7f1675e8b447        624  ray::core::CoreWorkerProcessImpl::CoreWorkerProcessImpl()
(raylet)     @     0x7f1675e8c54f         80  ray::core::CoreWorkerProcess::Initialize()
(raylet)     @     0x7f1675d4ba1f       2016  __pyx_pw_3ray_7_raylet_10CoreWorker_1__cinit__()
(raylet)     @     0x7f1675d4ccc9         48  __pyx_tp_new_3ray_7_raylet_CoreWorker()
(raylet)     @           0x4f6083  (unknown)  _PyObject_MakeTpCall
(raylet)     @           0x740ce0  (unknown)  (unknown)
(raylet) [2023-11-29 20:49:22,911 E 1346720 1346720] logging.cc:361: *** SIGABRT received at time=1701290962 on cpu 5 ***
(raylet) [2023-11-29 20:49:22,911 E 1346720 1346720] logging.cc:361: PC: @     0x7f16772d400b  (unknown)  raise
(raylet) [2023-11-29 20:49:22,911 E 1346720 1346720] logging.cc:361:     @     0x7f16775f1420  (unknown)  (unknown)
(raylet) [2023-11-29 20:49:22,911 E 1346720 1346720] logging.cc:361:     @     0x7f167624ac68        144  grpc::Server::Start()
(raylet) [2023-11-29 20:49:22,911 E 1346720 1346720] logging.cc:361:     @     0x7f1676251c71        288  grpc::ServerBuilder::BuildAndStart()
(raylet) [2023-11-29 20:49:22,911 E 1346720 1346720] logging.cc:361:     @     0x7f1676123a22        560  ray::rpc::GrpcServer::Run()
(raylet) [2023-11-29 20:49:22,911 E 1346720 1346720] logging.cc:361:     @     0x7f1675e84f98        960  ray::core::CoreWorker::CoreWorker()
(raylet) [2023-11-29 20:49:22,911 E 1346720 1346720] logging.cc:361:     @     0x7f1675e8b447        624  ray::core::CoreWorkerProcessImpl::CoreWorkerProcessImpl()
(raylet) [2023-11-29 20:49:22,911 E 1346720 1346720] logging.cc:361:     @     0x7f1675e8c54f         80  ray::core::CoreWorkerProcess::Initialize()
(raylet) [2023-11-29 20:49:22,911 E 1346720 1346720] logging.cc:361:     @     0x7f1675d4ba1f       2016  __pyx_pw_3ray_7_raylet_10CoreWorker_1__cinit__()
(raylet) [2023-11-29 20:49:22,911 E 1346720 1346720] logging.cc:361:     @     0x7f1675d4ccc9         48  __pyx_tp_new_3ray_7_raylet_CoreWorker()
(raylet) [2023-11-29 20:49:22,911 E 1346720 1346720] logging.cc:361:     @           0x4f6083  (unknown)  _PyObject_MakeTpCall
(raylet) [2023-11-29 20:49:22,911 E 1346720 1346720] logging.cc:361:     @           0x740ce0  (unknown)  (unknown)
(raylet) Fatal Python error: Aborted
(raylet)
(raylet) Stack (most recent call first):
(raylet)   File ""/home/user/wrkai-vastai/.env/lib/python3.10/site-packages/ray/_private/worker.py"", line 2284 in connect
(raylet)   File ""/home/user/wrkai-vastai/.env/lib/python3.10/site-packages/ray/_private/workers/default_worker.py"", line 247 in <module>
(raylet)
(raylet) Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, uvloop.loop, ray._raylet, charset_normalizer.md (total: 9)
(ServeController pid=1346323) WARNING 2023-11-29 20:49:52,536 controller 1346323 deployment_state.py:1974 - Deployment 'TextToImage' in application 'default' has 1 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {}, total resources available: {}. Use `ray status` for more details.",console log see trying run one deployment different deployment server default controller deployment application default current ongoing current handle controller replica deployment application raylet could create raylet assertion raylet received raylet unknown raise raylet unknown unknown raylet raylet raylet ray raylet ray raylet ray raylet ray raylet raylet raylet unknown raylet unknown unknown raylet received raylet unknown raise raylet unknown unknown raylet raylet raylet ray raylet ray raylet ray raylet ray raylet raylet raylet unknown raylet unknown unknown raylet fatal python error aborted raylet raylet stack recent call first raylet file line connect raylet file line module raylet raylet extension total warning controller deployment application taken may due waiting cluster environment replica total available use ray status,issue,negative,negative,neutral,neutral,negative,negative
1832513567,"> > What is the issue with ActorPoolStrategy?
> 
> This was coming from batch inference CUJ feedback, users found it's unnecessary to learn a separate class and import `ActorPoolStrategy`, and we don't really need to have a class here. This would also make code simpler.

Also, normal Data users may not know the differences between Ray tasks and actors. Would be better to not expose the implementation details. 
",issue coming batch inference feedback found unnecessary learn separate class import really need class would also make code simpler also normal data may know ray would better expose implementation,issue,negative,positive,positive,positive,positive,positive
1832496192,New chaos release test passing with expected node fail: https://buildkite.com/ray-project/release/builds/2559,new chaos release test passing node fail,issue,negative,negative,negative,negative,negative,negative
1832472094,"> What is the issue with ActorPoolStrategy?

This was coming from batch inference CUJ feedback, users found it's unnecessary to learn a separate class and import `ActorPoolStrategy`, and we don't really need to have a class here. This would also make code simpler.
",issue coming batch inference feedback found unnecessary learn separate class import really need class would also make code simpler,issue,negative,negative,neutral,neutral,negative,negative
1832458053,"Sorry, I don't have the context here.

>  it would go back to have same issue with ActorPoolStrategy.

What is the issue with ActorPoolStrategy?",sorry context would go back issue issue,issue,negative,negative,negative,negative,negative,negative
1832423362,"Ok, release test passed. Something in the new dataplane made the gcloud users become anonymous. ",release test something new made become anonymous,issue,negative,positive,positive,positive,positive,positive
1832402558,"@inpefess @sven1977 @avnishn I was browsing through this PR from earlier this year, and wanted to verify if my assumption is correct about the changes in this PR: 

the env_check for parametric actions (with action mask) will **only** work if the underlying environment is a `gym.Env`, and will not work if the environment is say, a `VectorEnv` or `MultiAgentEnv` since the logic is only added to the [check_gym_environments()](https://github.com/ray-project/ray/blob/64e53731df4fad877da6e6ceef9ac432e0e85ad0/rllib/utils/pre_checks/env.py#L109) method?",browsing year verify assumption correct parametric action mask work underlying environment work environment say since logic added method,issue,negative,positive,neutral,neutral,positive,positive
1832393975,"Alternative solution, set `evaluation_duration_unit` to `""timesteps""` instead of `""episodes""`:

> `evaluation_duration_unit=""timesteps""`",alternative solution set instead,issue,negative,neutral,neutral,neutral,neutral,neutral
1832370616,"> Q: consider adding a unit test to avoid regression? (i.e., import serialization_addon.py and make sure libs are not imported

Sure - but i think what's more important is how we prevent this from happening again. Don't we have tests that track worker start-up time? ",consider unit test avoid regression import make sure sure think important prevent happening track worker time,issue,positive,positive,positive,positive,positive,positive
1832361193,"yeah, it might be that the upgraded `gsutil` now needs additional permissions to perform `cp`. this can be fixed by just granting the required permissions to the GCS bucket.",yeah might need additional perform fixed bucket,issue,negative,positive,neutral,neutral,positive,positive
1832357729,"Well the problem occurred when I used ray-lib to train a model on Google Cloud that runs linux-python3.9 and then I transferred the trained model to my windows machine to do inference. I debugged the issue and I found that is related to the the place where `cloudpickle` loading is happening. To keep things simple I described the problem in the above format.

This seems to be a common problem that maybe faced by lot of other users of ray-lib. Fortunately as a workaround with WSL I am able to infer it presently. I saw that the original data file got removed, so I will provide it [here](https://filebin.net/s21jsls4rgb8eaql) if you would like to investigate. ",well problem used train model cloud transferred trained model machine inference issue found related place loading happening keep simple problem format common problem maybe faced lot fortunately able infer presently saw original data file got removed provide would like investigate,issue,negative,positive,neutral,neutral,positive,positive
1832351299,"I just assume that something has to change in the dataplane for this to work, not in ray repo?",assume something change work ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1832350550,I'm running the tests to confirm https://buildkite.com/ray-project/release/builds/2563#_. I see similar failures when I work on py311 upgrades - I thought the failures was caused by py311 but it's likely due to the dataplane itself.,running confirm see similar work thought likely due,issue,negative,negative,neutral,neutral,negative,negative
1832341328,"how do you know it is caused by this?

looking at the logs, it seems like some other permissions issues.

maybe consider fix forward:

````
2023-11-29 09:50:20,655 INFO worker.py:1639 -- Connected to Ray cluster. View the dashboard at http://10.138.0.24:8265 
Shared connection to 35.247.96.39 closed.
======================================
Ray verification successful.
======================================
Cleaning up cluster...
======================================
Finished executing script successfully.
Subprocess return code: 0
[INFO 2023-11-29 01:51:01,146] anyscale_job_wrapper.py: 190  Process 2724 exited with return code 0.
[INFO 2023-11-29 01:51:01,146] anyscale_job_wrapper.py: 292  Finished with return code 0. Time taken: 185.24580742799998
[WARNING 2023-11-29 01:51:01,146] anyscale_job_wrapper.py: 68  Couldn't upload to cloud storage: '/tmp/release_test_out.json' does not exist.
WARNING: There was an error checking the latest version of pip.
ServiceException: 401 Anonymous caller does not have storage.objects.list access to the Google Cloud Storage bucket. Permission 'storage.objects.list' denied on resource (or it may not exist).
````

we cannot release with this change reverted anyways.",know looking like maybe consider fix forward connected ray cluster view dashboard connection closed ray verification successful cleaning cluster finished script successfully return code process return code finished return code time taken warning could cloud storage exist warning error latest version pip anonymous caller access cloud storage bucket permission resource may exist release change anyways,issue,negative,positive,positive,positive,positive,positive
1832205802,"I think it's not only a problem of the gpu + batch_size
Currently, I cannot run the map_groups + actor.
```
sorted_ds = ds.groupby(key=""a"").map_groups(DEMO,
        compute=ray.data.ActorPoolStrategy(size=10),
        batch_format  = 'pandas')
```
it will failed with error.
```
2023-11-29 11:03:55,624  WARNING actor_pool_map_operator.py:271 -- To ensure full parallelization across an actor pool of size 10, the Dataset should consist of at least 10 distinct blocks. Consider increasing the parallelism when creating the Dataset.
Traceback (most recent call last):
  File ""python/ray/_raylet.pyx"", line 347, in ray._raylet.StreamingObjectRefGenerator._next_sync
  File ""python/ray/_raylet.pyx"", line 4643, in ray._raylet.CoreWorker.try_read_next_object_ref_stream
  File ""python/ray/_raylet.pyx"", line 447, in ray._raylet.check_status
ray.exceptions.ObjectRefStreamEndOfStreamError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/zhilong/miniconda3/envs/ray28/lib/python3.10/site-packages/ray/data/_internal/execution/interfaces/physical_operator.py"", line 80, in on_data_ready
    meta = ray.get(next(self._streaming_gen))
  File ""python/ray/_raylet.pyx"", line 302, in ray._raylet.StreamingObjectRefGenerator.__next__
  File ""python/ray/_raylet.pyx"", line 365, in ray._raylet.StreamingObjectRefGenerator._next_sync
StopIteration

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/zhilong/ray_28_bug/gpb.py"", line 48, in <module>
    sorted_ds.show(5)
  File ""/home/zhilong/miniconda3/envs/ray28/lib/python3.10/site-packages/ray/data/dataset.py"", line 2466, in show
    for row in self.take(limit):
  File ""/home/zhilong/miniconda3/envs/ray28/lib/python3.10/site-packages/ray/data/dataset.py"", line 2390, in take
    for row in limited_ds.iter_rows():
  File ""/home/zhilong/miniconda3/envs/ray28/lib/python3.10/site-packages/ray/data/iterator.py"", line 219, in _wrapped_iterator
    for batch in batch_iterable:
  File ""/home/zhilong/miniconda3/envs/ray28/lib/python3.10/site-packages/ray/data/iterator.py"", line 164, in _create_iterator
    block_iterator, stats, blocks_owned_by_consumer = self._to_block_iterator()
  File ""/home/zhilong/miniconda3/envs/ray28/lib/python3.10/site-packages/ray/data/_internal/iterator/iterator_impl.py"", line 32, in _to_block_iterator
    block_iterator, stats, executor = ds._plan.execute_to_iterator()
  File ""/home/zhilong/miniconda3/envs/ray28/lib/python3.10/site-packages/ray/data/_internal/plan.py"", line 548, in execute_to_iterator
    block_iter = itertools.chain([next(gen)], gen)
  File ""/home/zhilong/miniconda3/envs/ray28/lib/python3.10/site-packages/ray/data/_internal/execution/legacy_compat.py"", line 54, in execute_to_legacy_block_iterator
    for bundle in bundle_iter:
  File ""/home/zhilong/miniconda3/envs/ray28/lib/python3.10/site-packages/ray/data/_internal/execution/interfaces/executor.py"", line 37, in __next__
    return self.get_next()
  File ""/home/zhilong/miniconda3/envs/ray28/lib/python3.10/site-packages/ray/data/_internal/execution/streaming_executor.py"", line 141, in get_next
    raise item
  File ""/home/zhilong/miniconda3/envs/ray28/lib/python3.10/site-packages/ray/data/_internal/execution/streaming_executor.py"", line 201, in run
    while self._scheduling_loop_step(self._topology) and not self._shutdown:
  File ""/home/zhilong/miniconda3/envs/ray28/lib/python3.10/site-packages/ray/data/_internal/execution/streaming_executor.py"", line 252, in _scheduling_loop_step
    process_completed_tasks(topology, self._backpressure_policies)
  File ""/home/zhilong/miniconda3/envs/ray28/lib/python3.10/site-packages/ray/data/_internal/execution/streaming_executor_state.py"", line 365, in process_completed_tasks
    num_blocks_read = task.on_data_ready(
  File ""/home/zhilong/miniconda3/envs/ray28/lib/python3.10/site-packages/ray/data/_internal/execution/interfaces/physical_operator.py"", line 88, in on_data_ready
    ex = ray.get(block_ref)
  File ""/home/zhilong/miniconda3/envs/ray28/lib/python3.10/site-packages/ray/_private/auto_init_hook.py"", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File ""/home/zhilong/miniconda3/envs/ray28/lib/python3.10/site-packages/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
  File ""/home/zhilong/miniconda3/envs/ray28/lib/python3.10/site-packages/ray/_private/worker.py"", line 2563, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): ray::MapBatches(group_fn)() (pid=3943892, ip=10.218.163.85, actor_id=501d51070249f810d044d51801000000, repr=MapWorker(MapBatches(group_fn)))
    yield from _map_task(
  File ""/home/zhilong/miniconda3/envs/ray28/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_operator.py"", line 416, in _map_task
    for b_out in map_transformer.apply_transform(iter(blocks), ctx):
  File ""/home/zhilong/miniconda3/envs/ray28/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py"", line 371, in __call__
    for data in iter:
  File ""/home/zhilong/miniconda3/envs/ray28/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py"", line 215, in __call__
    yield from self._batch_fn(input, ctx)
  File ""/home/zhilong/miniconda3/envs/ray28/lib/python3.10/site-packages/ray/data/_internal/planner/plan_udf_map_op.py"", line 190, in transform_fn
    res = fn(batch)
  File ""/home/zhilong/miniconda3/envs/ray28/lib/python3.10/site-packages/ray/data/_internal/planner/plan_udf_map_op.py"", line 120, in fn
    return op_fn(item, *fn_args, **fn_kwargs)
  File ""/home/zhilong/miniconda3/envs/ray28/lib/python3.10/site-packages/ray/data/grouped_data.py"", line 362, in group_fn
    builder.add_batch(applied)
  File ""/home/zhilong/miniconda3/envs/ray28/lib/python3.10/site-packages/ray/data/_internal/delegating_block_builder.py"", line 39, in add_batch
    return self.add_block(block)
  File ""/home/zhilong/miniconda3/envs/ray28/lib/python3.10/site-packages/ray/data/_internal/delegating_block_builder.py"", line 42, in add_block
    accessor = BlockAccessor.for_block(block)
  File ""/home/zhilong/miniconda3/envs/ray28/lib/python3.10/site-packages/ray/data/block.py"", line 402, in for_block
    raise TypeError(""Not a block type: {} ({})"".format(block, type(block)))
TypeError: Not a block type: <__main__.DEMO object at 0x7f112590b490> (<class '__main__.DEMO'>)

```
it works for the function as the input
```
sorted_ds = ds.groupby(key=""a"").map_groups(test,
        batch_format  = 'pandas')
```
here are the whole codes

```
import ray
ray.init(address = ""10.218.163.85:6274"")
from ray.data import ActorPoolStrategy

ctx = ray.data.context.DatasetContext.get_current()
use_push_based_shuffle = True
num_items = 100001
parallelism = 200
import pandas as pd
import numpy as np
import time


t1 = time.time()
original = ctx.use_push_based_shuffle
ctx.use_push_based_shuffle = use_push_based_shuffle

a = list(reversed(range(num_items)))
a = [x%10 for x in a]
shard = int(np.ceil(num_items / parallelism))
b = [1]*1
offset = 0
dfs = []
while offset < num_items:
    dfs.append(
        pd.DataFrame(
            {""a"": a[offset : offset + shard], ""b"": [b]*len(a[offset : offset + shard])}
        )
    )
    offset += shard
if offset < num_items:
    dfs.append(pd.DataFrame({""a"": a[offset:], ""b"": b[offset:]}))
ds = ray.data.from_pandas(dfs)

class DEMO:
    def __init__(self,*args, **kwargs):
        print(args)
        #print(kwargs)
        pass
    
    def __call__(self, a):
        return a
def test(a):
    return a
sorted_ds = ds.groupby(key=""a"").map_groups(DEMO,
        compute=ray.data.ActorPoolStrategy(size=10),
        batch_format  = 'pandas')
sorted_ds.show(5)
print(f""time used : \n{time.time()-t1}"")


```

",think problem currently run actor error warning ensure full parallelization across actor pool size consist least distinct consider increasing parallelism recent call last file line file line file line handling exception another exception recent call last file line meta next file line file line handling exception another exception recent call last file line module file line show row limit file line take row file line batch file line file line executor file line next gen gen file line bundle file line return file line raise item file line run file line topology file line file line ex file line return file line wrapper return file line get raise ray yield file line iter file line data iter file line yield input file line batch file line return item file line applied file line return block file line block file line raise block type block type block block type object class work function input test whole import ray address import true parallelism import import import time original list reversed range shard parallelism offset offset offset offset shard offset offset shard offset shard offset offset offset class self print print pas self return test return print time used,issue,negative,positive,neutral,neutral,positive,positive
1832109997,"Hello,
I am running into this same problem and am not able to fix it. Were you able to solve it?
I noticed this error at the `ray.shutdown()` portion of [this script](https://github.com/rstrivedi/Melting-Pot-Contest-2023/blob/b3d4d30ca8cb258be3ff51dd7ce97cb9ee19651a/baselines/train/run_ray_train.py#L178)",hello running problem able fix able solve error portion script,issue,negative,positive,positive,positive,positive,positive
1832093919,I will assign this to you for now (but feel free to find a person delegate it!),assign feel free find person delegate,issue,positive,positive,positive,positive,positive,positive
1832061455,"@pcmoritz @ericl this PR will have a breaking change to num_returns=""dynamic"" because it changes the name of the class to ObjectRefGenerator -> DynamicObjectRefGenerator, so that `ObjectRefGenerator` can be used for the streaming generator. (we are going to deprecate num_returns=""dynamic"" as we discussed in the public API proposal)

If this is concerning, we can instead use `ObjRefGenerator` for num_returns=""streaming"" instead. Let me know if you guys prefer this option. ",breaking change dynamic name class used streaming generator going deprecate dynamic public proposal concerning instead use streaming instead let know prefer option,issue,positive,neutral,neutral,neutral,neutral,neutral
1831800139,"@jjyao I found that there is no connection established from idle worker to driver worker. I want to confirm whether this understanding is correct. If so, fixing the problem involves establishing a connection with a client, calling `dissconnect` and then disconnecting the client.",found connection established idle worker driver worker want confirm whether understanding correct fixing problem connection client calling client,issue,negative,neutral,neutral,neutral,neutral,neutral
1831601291,"Yeah, I think this is ok to add. However, I think that accessing the restored loggers from the run config do not keep any state from the run. Neither are the callbacks unless you call `callback.restore_from_dir` manually.",yeah think add however think run keep state run neither unless call manually,issue,negative,neutral,neutral,neutral,neutral,neutral
1831587048,ugh. at least the test case is properly checking things. Okay. I've updated the test cases and it should be passing now...,ugh least test case properly test passing,issue,negative,negative,negative,negative,negative,negative
1831386052,"I got the same issue with simple command as below:

`ray job submit --address $RAY_ADDRESS --runtime-env-json='{""pip"": [""requests==2.26.0""]}' --working-dir ./ -- python test_runtime_env.py`

I tried to test this because my ray cluster could not go to internet for installing packages.

So, how can I stop/delete this summit ? because I did not see JOB_ID
Or I have to delete cluster ?

Thanks and regards

",got issue simple command ray job submit address pip python tried test ray cluster could go summit see delete cluster thanks,issue,negative,positive,neutral,neutral,positive,positive
1831380105,"verified again with latest changes. 
This should be good to merge as soon as tests pass",latest good merge soon pas,issue,negative,positive,positive,positive,positive,positive
1831337483,verified that multi-node log viewing continues to work. will fix the faiiling test,log work fix test,issue,negative,neutral,neutral,neutral,neutral,neutral
1831312711,"Simpler repro: 

```
import ray

ray.init()

@ray.remote(max_calls=1)
def f():
  pass

while True:
  ray.get(f.remote())
```",simpler import ray pas true,issue,negative,positive,positive,positive,positive,positive
1831282673,"> I was trying to load a trained agent in rllib ...

I am not sure loading a trained agent on a different system than the one used to save the agent is meant to work. @anyscalesam is that a scenario that rllib supports?",trying load trained agent sure loading trained agent different system one used save agent meant work scenario,issue,positive,positive,positive,positive,positive,positive
1831279683,"So if I understand correctly, you are saving a cloudpickle on linux-python3.9, and it is failing to load into windows-python3.11 or windows-python3.10. I think the problem is that the problem is around including code objects, so a reproducer that includes the code to create the cloudpickle object is required.",understand correctly saving failing load think problem problem around code reproducer code create object,issue,negative,neutral,neutral,neutral,neutral,neutral
1831274272,"@alanwguo Thanks a lot for doing this -- there is still a failing test, can you fix it? https://buildkite.com/ray-project/premerge/builds/13040#018c197d-5f5b-4abd-999b-08be7f0af010",thanks lot still failing test fix,issue,negative,positive,positive,positive,positive,positive
1831227619,"> Can I have my Mac as dev environment? Or is it better if I clone the repo on a VM(with Ubuntu) running on Mac?

Yes, mac would work for dev. 

> What's the best resource to understand the code structure in the repo?

Adding print statements would help. 

Here are a list of related files:
- https://github.com/ray-project/ray/blob/40073ee036a9c13f9bc5c96c357ee1a707ecb728/python/ray/util/state/api.py#L712-L717
- https://github.com/ray-project/ray/blob/9670b3c2ec5c5d43660182df609c212637db50da/dashboard/modules/state/state_head.py#L324-L328
- https://github.com/ray-project/ray/blob/40073ee036a9c13f9bc5c96c357ee1a707ecb728/python/ray/util/state/state_manager.py#L284-L290
",mac dev environment better clone running mac yes mac would work dev best resource understand code structure print would help list related,issue,positive,positive,positive,positive,positive,positive
1831225159,yeah - I guess I prob have the most context of v1 now. ,yeah guess prob context,issue,negative,neutral,neutral,neutral,neutral,neutral
1831202965,https://buildkite.com/ray-project/release/builds/2492#018c1923-c5d0-4e09-a67e-b45cf2c3b553 release test perf back to 530 level.,release test back level,issue,negative,neutral,neutral,neutral,neutral,neutral
1831175496,"@vonsago Thanks for the detailed explanation. I checked the code seems we, indeed, don't remove dead workers from `client_map_`.",thanks detailed explanation checked code indeed remove dead,issue,negative,positive,positive,positive,positive,positive
1831156719,"@jjyao This is also a point of confusion for me. It is easy to reproduce this problem in version 2.8.0, but in the master branch I need to run fastapi.py and test.py at the same time and maintain the same qps as the total resource amount to reproduce it. But it can be reproduced in a similar way anyway. The key point seems to be options.num_cpus=0.5 instead of 1. For specific phenomena, you can see that ray list workers continue to increase.

I add one debug log at core_worker_pool.cc CoreWorkerClientPool::GetOrConnect `RAY_LOG(DEBUG) << ""Connected to "" << addr_proto.ip_address() << "":"" << addr_proto.port() << "" size: "" << client_map_.size();`

here is log of driver worker in head node which is built by branch of master.
[2023-11-27 22:30:55,540 D 1182 1217] core_worker_client_pool.cc:42: Connected to 10.23.176.225:10004 size: 1
// ...            10 min interval, size 1 to 1914
[2023-11-27 22:41:54,014 D 1182 1217] core_worker_client_pool.cc:42: Connected to 10.23.176.225:13439 size: 1914

This `client_map_` will continue to grow and will not be released.

At the same time, I also noticed your comment in my code. That was my mistake. I am still looking for a way to let the `idle worker` on the `worker node` notify the `driver worker` on the `head node` that its `idle worker` has exited and destroyed the `client_map_` record to ensure the size.",also point confusion easy reproduce problem version master branch need run time maintain total resource amount reproduce similar way anyway key point instead specific phenomenon see ray list continue increase add one log connected size log driver worker head node built branch master connected size min interval size connected size continue grow time also comment code mistake still looking way let idle worker worker node notify driver worker head node idle worker record ensure size,issue,negative,positive,neutral,neutral,positive,positive
1831148529,🤝 Thanks @kevin85421 and @anyscalesam help to ping the relative maintainer.,handshake thanks help ping relative maintainer,issue,positive,positive,neutral,neutral,positive,positive
1831048679,I'm guessing the build failure is solved by https://github.com/ray-project/ray/pull/41250. Can you try retriggering CI?,guessing build failure try,issue,negative,negative,negative,negative,negative,negative
1831044545,"@nate-bush do you have a complete reproducible example, including s3 paths which we can use? I tried a similar approach as the one you described above, but cannot reproduce the error on ray 2.7.1 or ray master:

```
>>> import ray
>>> paths = [
...     ""s3://air-example-data/sql/train.jsonl"",
...     ""s3://air-example-data/sql/missing.jsonl"",
...     ""s3://air-example-data/missing-directory""
... ]

# Also tried this to artificially increase the number of files to read, no issues.
>>> # paths = paths * 50

>>> ds = ray.data.read_text(paths, ignore_missing_paths=True)
Dataset(num_blocks=20, num_rows=30000, schema={text: string})
```",complete reproducible example use tried similar approach one reproduce error ray ray master import ray also tried artificially increase number read text string,issue,negative,negative,negative,negative,negative,negative
1831042474,"```
(base) ~/ray ⑂deflakey-ray-syncer* $ ./bazel-bin/src/ray/common/test/ray_syncer_test --gtest_filter=SyncerReactorTest.TestReactorFailure
Note: Google Test filter = SyncerReactorTest.TestReactorFailure
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from SyncerReactorTest
[ RUN      ] SyncerReactorTest.TestReactorFailure
[2023-11-29 01:21:30,054 I 3177941 3177941] (ray_syncer_test) ray_syncer_test.cc:845: Waiting: 0
[2023-11-29 01:21:31,054 I 3177941 3177941] (ray_syncer_test) ray_syncer_test.cc:845: Waiting: 1
[2023-11-29 01:21:31,057 I 3177941 3177979] (ray_syncer_test) ray_syncer-inl.h:308: Failed to read the message from: cf096515cad2db75f705be47bea8ae9872867aa2051e2f86b0752a32
E1129 01:21:31.057783254 3177979 callback_common.h:165]                ASSERTION FAILED: call_ == nullptr
*** SIGABRT received at time=1701220891 on cpu 3 ***
PC: @     0x7f4a4865400b  (unknown)  raise
    @     0x7f4a48b6f420       2144  (unknown)
    @     0x7f4a48e36b47        192  gpr_assertion_failed
    @     0x558f2d349088        160  grpc::internal::CallbackBidiHandler<>::ServerCallbackReaderWriterImpl::Finish()
    @     0x7f4a4ac37d09        240  std::_Function_handler<>::_M_invoke()
    @     0x7f4a4abd2cb2        176  EventTracker::RecordExecution()
    @     0x7f4a4abf9c9f        160  instrumented_io_context::dispatch()
    @     0x7f4a4ac380b8        144  ray::syncer::RayServerBidiReactor::DoDisconnect()
    @     0x7f4a4abd2cb2        176  EventTracker::RecordExecution()
    @     0x7f4a4abf968b        160  boost::asio::detail::completion_handler<>::do_complete()
    @     0x7f4a4a750543        128  boost::asio::detail::scheduler::do_run_one()
    @     0x7f4a4a755ce1        288  boost::asio::detail::scheduler::run()
    @     0x7f4a4a7571f6         96  boost::asio::io_context::run()
    @     0x7f4a488f4df4  (unknown)  (unknown)
    @     0x558f2d986c20  (unknown)  (unknown)
    @     0x558f2d342ec0  (unknown)  (unknown)
    @ 0x4810c083480004ff  (unknown)  (unknown)
[2023-11-29 01:21:31,072 E 3177941 3177979] (ray_syncer_test) logging.cc:361: *** SIGABRT received at time=1701220891 on cpu 3 ***
[2023-11-29 01:21:31,072 E 3177941 3177979] (ray_syncer_test) logging.cc:361: *** SIGABRT received at time=1701220891 on cpu 3 ***
```
",base note test filter running test test suite global test environment test run waiting waiting read message assertion received unknown raise unknown ray boost boost boost boost unknown unknown unknown unknown unknown unknown unknown unknown received received,issue,positive,negative,negative,negative,negative,negative
1831023409,Could you also share the full code you are using? `read_parquet_bulk` should not be calling `_sample_block` by itself,could also share full code calling,issue,negative,positive,positive,positive,positive,positive
1830931915,The HA tests look good but look like the memory pressure test will fail with this change?,ha look good look like memory pressure test fail change,issue,negative,positive,neutral,neutral,positive,positive
1830916328,"My 2c, anecdote:  this would be impactful for my team since we use status codes and errors offered in HTTP as a means to communicate error messages.

So having an analogue for status codes and error descriptions via the standard method of GRPC context sounds ideal.


Intended use case: In our deployments we have validation that checks for data quality (ex. detecting video corruption) that would normally return an `INVALID_INPUT` style error. This helps on client side in how the business logic triages that such errors are non-retryable.",anecdote would team since use status communicate error analogue status error via standard method context ideal intended use case validation data quality ex video corruption would normally return style error client side business logic,issue,negative,positive,positive,positive,positive,positive
1830881419,"So the original model was saved in linux running python `3.9.x`. I am having the issue on `3.11.6` on windows. I tried on python `3.10.11` on windows and getting the following error message when executing this `states = cloudpickle.load(loaded_object)`

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: file must have 'read' and 'readline' attributes
``

",original model saved running python issue tried python getting following error message recent call last file line module file must,issue,negative,positive,neutral,neutral,positive,positive
1830854677,"Will update all documentation + unit test code after we agree on the API change (o.w. too many places to change).
PTAL and let me know your thoughts, thanks @raulchen, @ericl, @stephanie-wang and @amogkam.",update documentation unit test code agree change many change let know thanks,issue,positive,positive,positive,positive,positive,positive
1830842232,"I didn't move this because I thought we only own autoscaler v2.

I can move it to the core team's task. Does it sound okay? (realistically, you will own this)",move thought move core team task sound realistically,issue,negative,positive,positive,positive,positive,positive
1830839065,"Yes. 

Before;
- Children of RayError becomes RayTaskError, and everything else becomes the original error

After:
- everything becomes the original error",yes becomes everything else becomes original error everything becomes original error,issue,negative,positive,positive,positive,positive,positive
1830836473,I have also renamed the newely added property to `num_paused_threads` to be explicit and not mix it with tasks,also added property explicit mix,issue,negative,neutral,neutral,neutral,neutral,neutral
1830833066,"Hi @rickyyx , can I work on this? Seems like a good place to start contributing.
If you've not started working on it already, I'd be happy to work on this.
Thank you!",hi work like good place start working already happy work thank,issue,positive,positive,positive,positive,positive,positive
1830825074,"I wanted to work on this. Needed clarification on following things..
1. Can I have my Mac as dev environment? Or is it better if I clone the repo on a VM(with Ubuntu) running on Mac?
2. What's the best resource to understand the code structure in the repo?",work clarification following mac dev environment better clone running mac best resource understand code structure,issue,positive,positive,positive,positive,positive,positive
1830815149,"The scrollbar is enabled by default with the newest version of `pydata-sphinx-theme`, so this is fixed by #41115. Closing as complete.

![image](https://github.com/ray-project/ray/assets/14017872/fb3f7564-9f7d-463e-b1b9-695701bb47d6)
",default version fixed complete image,issue,negative,positive,neutral,neutral,positive,positive
1830799940,"> I may have complained about this several times, but I really hate how our doc navigation works. You have to click a parent page, wait for it to load, and then check out the child pages.
> this is super slow and it sometimes shifts the UI up and down
> other products’ docs have a way to let people expand/collapse quickly to navigate.
> 
> What we want is that the drop-down carat expands/collapses the drop-down without loading the page.

Since #41115 got merged, we now have a sidebar that has more sensible navigation behavior, so I'm going to close this issue. There's a another issue here that hasn't been addressed, though:

> We currently need to refresh the page on-click, because we can't generate the whole nav for every page on our docs.
> if you remember, introducing detailed API reference pages blew up the number of documents we have on our docs and having a full nav for every single document means that we incur docs build times of about 40-60 minutes, which is infeasible for [readthedocs.com](http://readthedocs.com/) (and frankly for us, too).
> we've gone through several options many times, and within the standard sphinx ecosystem there's not much we can do here, unfortunately.

This is hugely irritating to me right now. Sphinx's read/write performance is so slow that it's IMO totally unreasonable for a project of this size. There's absolutely no reason why reading some text and generating some HTML from it should take 50 minutes unless you're parsing gigabytes of files. I'd really like to write a sphinx builder that is a compiled python extension that can do this faster. Given that the builtin HTML builder is only ~1000 lines of Python, this wouldn't be a huge project - @angelinalg let me know if you'd be interested in pursuing this.

On my local machine builds complete in just over 3 minutes (the job is run in parallel), but on RTD they take ~50 :disappointed: (because by default RTD build jobs run serially). As a stopgap measure, I want to try to enable multi-process builds on RTD. I'll make an issue for this right now.

> what we can do for sure is to separately generate a LHS nav bar and embed it at runtime. this would not work with vanilla sphinx, but would instead be a rather invasive/custom procedure.

This is in fact the approach taken in #41115, where the primary sidebar is cached the first time an HTML page is generated, then reused for every other page (we still need to update which sections automatically open up on each page, but that's a relatively ""small"" overhead).",may several time really hate doc navigation work click parent page wait load check child super slow sometimes way let people quickly navigate want carat without loading page since got sensible navigation behavior going close issue another issue though currently need refresh page ca generate whole every page remember detailed reference number full every single document incur build time infeasible frankly u gone several many time within standard sphinx ecosystem much unfortunately hugely irritating right sphinx performance slow totally unreasonable project size absolutely reason reading text generating take unless really like write sphinx builder python extension faster given builder python would huge project let know interested local machine complete job run parallel take disappointed default build run serially stopgap measure want try enable make issue right sure separately generate bar embed would work vanilla sphinx would instead rather procedure fact approach taken primary first time page every page still need update automatically open page relatively small overhead,issue,negative,positive,neutral,neutral,positive,positive
1830799048,"Hi, I can confirm that I still get the same error in Ray 2.8.0. I changed my code to read JSON so you don’t need to transform it to Parquet. I believe the key point is a large dataset or long string in the data. I know this is a long-standing issue and the performance is for
ray 2.4.0: cannot load the data large than 2GB, 200000 lines
ray 2.8.0: cannot load the data large than 4GB, 400000 lines, but can load the data less than 4GB, 400000 lines.
I hope this test can give you some clues. @scottjlee 
![tmp](https://github.com/ray-project/ray/assets/121425509/a99f18fc-b1fe-47eb-868c-5534fe252360)

",hi confirm still get error ray code read need transform parquet believe key point large long string data know issue performance ray load data large ray load data large load data le hope test give,issue,negative,positive,positive,positive,positive,positive
1830794454,"Addressed. Good idea on the static assert, I was trying to get `constexpr` to work but this is much more straightforward.",good idea static assert trying get work much straightforward,issue,negative,positive,positive,positive,positive,positive
1830775906,"@jjyao I am still seeing this issue. I get the bug with zsh and bash.

```
Python 3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:42:03) [Clang 12.0.1 ] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import ray
>>> ray.init()
2023-11-28 16:25:21,601	ERROR services.py:1329 -- Failed to start the dashboard , return code 1
2023-11-28 16:25:21,601	ERROR services.py:1354 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure' to find where the log file is.
2023-11-28 16:25:21,606	ERROR services.py:1398 --
The last 20 lines of /tmp/ray/session_2023-11-28_16-25-20_035505_19613/logs/dashboard.log (it contains the error message from the dashboard):
  File ""/Users/ari/opt/miniconda3/envs/mamba_oa_env/lib/python3.10/site-packages/ray/dashboard/dashboard.py"", line 75, in run
    await self.dashboard_head.run()
  File ""/Users/ari/opt/miniconda3/envs/mamba_oa_env/lib/python3.10/site-packages/ray/dashboard/head.py"", line 325, in run
    modules = self._load_modules(self._modules_to_load)
  File ""/Users/ari/opt/miniconda3/envs/mamba_oa_env/lib/python3.10/site-packages/ray/dashboard/head.py"", line 219, in _load_modules
    head_cls_list = dashboard_utils.get_all_modules(DashboardHeadModule)
  File ""/Users/ari/opt/miniconda3/envs/mamba_oa_env/lib/python3.10/site-packages/ray/dashboard/utils.py"", line 121, in get_all_modules
    importlib.import_module(name)
  File ""/Users/ari/opt/miniconda3/envs/mamba_oa_env/lib/python3.10/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1050, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 1027, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 1006, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 688, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 883, in exec_module
  File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed
  File ""/Users/ari/opt/miniconda3/envs/mamba_oa_env/lib/python3.10/site-packages/ray/dashboard/modules/log/log_manager.py"", line 26, in <module>
    class ResolvedStreamFileInfo(BaseModel):
TypeError: NoneType takes no arguments
2023-11-28 16:25:21,721	INFO worker.py:1673 -- Started a local Ray instance.
RayContext(dashboard_url=None, python_version='3.10.4', ray_version='2.8.0', ray_commit='105355bd253d6538ed34d331f6a4bdf0e38ace3a', protocol_version=None)
>>> (raylet) [2023-11-28 16:25:22,490 E 19620 7005385] (raylet) agent_manager.cc:70: The raylet exited immediately because one Ray agent failed, agent_name = dashboard_agent/470211272.
(raylet) The raylet fate shares with the agent. This can happen because
(raylet) - The version of `grpcio` doesn't follow Ray's requirement. Agent can segfault with the incorrect `grpcio` version. Check the grpcio version `pip freeze | grep grpcio`.
(raylet) - The agent failed to start because of unexpected error or port conflict. Read the log `cat /tmp/ray/session_latest/logs/{dashboard_agent|runtime_env_agent}.log`. You can find the log file structure here https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure.
(raylet) - The agent is killed by the OS (e.g., out of memory).

KeyboardInterrupt
>>> exit()
```

Versions:
- modin: 0.25.1
- ray: 2.8.0
- grpcio: 1.47.0
- python: 3.10.4
- os: Mac OS 12.6.4
",still seeing issue get bug bash python main mar clang type help copyright license information import ray error start dashboard return code error error written printing last see find log file error last error message dashboard file line run await file line run file line file line name file line return name level package level file frozen line file frozen line file frozen line file frozen line file frozen line file frozen line file line module class local ray instance raylet raylet raylet immediately one ray agent raylet raylet fate agent happen raylet version follow ray requirement agent incorrect version check version pip freeze raylet agent start unexpected error port conflict read log cat find log file structure raylet agent o memory exit ray python o mac o,issue,negative,positive,neutral,neutral,positive,positive
1830770197,"Looks like this one has been completed. Closing for now, but if this needs to be reopened please feel free to comment.",like one need please feel free comment,issue,positive,positive,positive,positive,positive,positive
1830759755,"@maxpumperla @rkooo567 Now that #41115 is merged, can we close this? To be totally clear, this still won't always work as you might expect - for example, if there are two headings close to the bottom of the page, and you click on the last one, the second-to-last heading might be highlighted in the secondary sidebar because that's the heading the top of the window is closest to. But I think in most cases the behavior is sensible, so can we close this one out?",close totally clear still wo always work might expect example two close bottom page click last one heading might secondary heading top window think behavior sensible close one,issue,positive,positive,neutral,neutral,positive,positive
1830727533,Nightly release test run: https://buildkite.com/ray-project/release/builds/2421,nightly release test run,issue,negative,neutral,neutral,neutral,neutral,neutral
1830721208,"I just added the full code and reran the test with larger memory. My data is only 2GB, but the memory spill is 130GB according to the log from raylet. The file is read as parquet from HDFS and materialized in line 37
![tmp](https://github.com/ray-project/ray/assets/121425509/4ba71664-9a53-4977-bf3c-0313be80715a)

![tmp](https://github.com/ray-project/ray/assets/121425509/3fc7ef1f-3cc2-481f-bea1-618b167eb569)
",added full code test memory data memory spill according log raylet file read parquet line,issue,negative,positive,positive,positive,positive,positive
1830701300,"@Bye-legumes do you get the same error from 2.4.0 in ray 2.8? BTW, `sort()` is a blocking operation, and will need to read the entire dataset before completion, which could be causing the run to get stuck.

For further context, the Arrow offset bug is a long-standing issue from Arrow side, and we have an older issue here: https://github.com/ray-project/ray/issues/29492
",get error ray sort blocking operation need read entire completion could causing run get stuck context arrow offset bug issue arrow side older issue,issue,negative,positive,neutral,neutral,positive,positive
1830673662,"@Bye-legumes can you provide the full code, specifically how you're materializing and reading in the data?",provide full code specifically reading data,issue,negative,positive,positive,positive,positive,positive
1830669400,"after discussion we should change map_groups() to allow (and require) batch_size iff gpu num is set. @scottjlee to follow up on this change,",discussion change allow require set follow change,issue,negative,neutral,neutral,neutral,neutral,neutral
1830660207,"Sorry about that @saswatac, will be fixed in upcoming Ray 2.9 release.",sorry fixed upcoming ray release,issue,negative,negative,negative,negative,negative,negative
1830643478,"This appears to be an async actor issue:

```python
import ray

@ray.remote
class A:
    async def hi(self, task_id: str) -> str:
        return f""Hi from {task_id}""

a = A.remote()
ray.get(a.hi.remote(task_id=""TEST""))
```

Gives:
```
Traceback (most recent call last):
  File ""repro.py"", line 9, in <module>
    ray.get(a.hi.remote(task_id=""TEST""))
  File ""/Users/eoakes/code/ray/python/ray/_private/auto_init_hook.py"", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File ""/Users/eoakes/code/ray/python/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
  File ""/Users/eoakes/code/ray/python/ray/_private/worker.py"", line 2599, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): ray::A.hi() (pid=92789, ip=127.0.0.1, actor_id=4d328729aa8f3436bcc70fd001000000, repr=<repro.A o
bject at 0x104f78dc0>)
TypeError: run_async_func_or_coro_in_event_loop() got multiple values for keyword argument 'task_id'
```

Removing `async def` fixes the issue.",actor issue python import ray class hi self return hi test recent call last file line module test file line return file line wrapper return file line get raise ray got multiple argument removing issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1830642887,"@ax7e can you be a bit more specific on ""quite slow"". What would be your target completion time?",axe bit specific quite slow would target completion time,issue,negative,negative,negative,negative,negative,negative
1830639360,"Thanks for raising this; we are reviewing overall Spark on Ray supportability and will take this into account,",thanks raising overall spark ray supportability take account,issue,positive,positive,neutral,neutral,positive,positive
1830636467,@linshuxi can you supply a repro script and error output; there's not enough information in this ticket.,supply script error output enough information ticket,issue,negative,neutral,neutral,neutral,neutral,neutral
1830631863,The issue is specifically in async actor. Will let core team take over.,issue specifically actor let core team take,issue,negative,neutral,neutral,neutral,neutral,neutral
1830523343,"Yes that is exactly right. At a high level, we should reduce the amount of vendored packages in worker code to the absolute minimum since it can interfere with user dependencies / code (whereas dashboard doesn't).

In addition to what you say, we should also vendor frozenlist in the dashboard and remove it from the dependencies since it is only used in the dashboard :)

Thanks for doing all this work :)",yes exactly right high level reduce amount worker code absolute minimum since interfere user code whereas dashboard addition say also vendor dashboard remove since used dashboard thanks work,issue,positive,positive,positive,positive,positive,positive
1830493020,Closing as complete with #41388. Please reopen if the issue persists.,complete please reopen issue,issue,negative,positive,neutral,neutral,positive,positive
1830490411,cc @anyscalesam Do you know who would be the best person to review this PR? @jiwq has just followed up with me.,know would best person review,issue,positive,positive,positive,positive,positive,positive
1830465221,"Default ubj or JSON satisfies my use case, though I can't speak for other use cases.

Thanks for the heads up on BatchPredictor, I'll move towards that pattern.",default use case though ca speak use thanks move towards pattern,issue,negative,positive,positive,positive,positive,positive
1830460961,"Understood. I think this feature should be implemented in the Ray Autoscaler rather than in KubeRay. If the Ray Autoscaler decides to scale up a multi-host TPU PodSlice, it should update the RayCluster CR to add a new worker group. Then, KubeRay will create new Pods based on the updated CR specification.",understood think feature ray rather ray scale update add new worker group create new based specification,issue,negative,positive,positive,positive,positive,positive
1830447731,"Also as an FYI the `BatchPredictor` interface is also now deprecated, see https://github.com/ray-project/ray/issues/37489. The new recommended pattern should allow you the flexibility to define how you load the checkpoint with your own custom behavior.",also interface also see new pattern allow flexibility define load custom behavior,issue,positive,positive,positive,positive,positive,positive
1830445539,"Hm given that UBJ is now the default for XGBoost, would it be satisfactory if we just updated the checkpoint to `.ubj`, or is there still a need for configurability?",given default would satisfactory still need,issue,negative,neutral,neutral,neutral,neutral,neutral
1830441894,"I think this makes sense, @justinvyu any concerns if we add this as public attributes?",think sense add public,issue,negative,neutral,neutral,neutral,neutral,neutral
1830409197,"@pcmoritz just to confirm, we will do things like this:

- declared deps: like `frozenlist` and `requests`, we use same version as users, as requested in pip pkg definition
- vendored for Ray: [""psutil"", ""setproctitle==1.2.2"", ""colorama""], if user use them in Worker we choose our vendored version
- vendored for Dash: aiohttp, aiosignal, if user use them in Worker they use their own; only Dash and Runtime Env Agent use it",confirm like declared like use version pip definition ray user use worker choose version dash user use worker use dash agent use,issue,positive,neutral,neutral,neutral,neutral,neutral
1830401859,This seems like a good idea to do; @can-anyscale @aslonnie can y'all review @jiwq 's PR please?,like good idea review please,issue,positive,positive,positive,positive,positive,positive
1830390766,"> > Users with more memory constraint could set `RAY_task_events_max_num_task_in_gcs` to a smaller value.
> 
> I couldn't find this in the docs. How does one set this value? Is it an envvar? Can it be set programatically?

Hey @fjeremic - sorry about the lack of doc, we should totally document it better (I will follow-up on this)

But yes, setting it as an env var will work before you start the ray head node (The head node reads it from env var) ",memory constraint could set smaller value could find one set value set hey sorry lack doc totally document better yes setting work start ray head node head node,issue,positive,neutral,neutral,neutral,neutral,neutral
1830382828,"> @JoshTanke Yes, it was. Those integrations were being generated from that yaml file and inserted into `ray-overview/ray-libraries.rst`. Rather than have the extra step, just modify `ray-overview/ray-libraries.rst` directly.

makes sense, thank you for the info!",yes file inserted rather extra step modify directly sense thank,issue,positive,positive,neutral,neutral,positive,positive
1830378858,"@JoshTanke Yes, it was. Those integrations were being generated from that yaml file and inserted into `ray-overview/ray-libraries.rst`. Rather than have the extra step, just modify `ray-overview/ray-libraries.rst` directly.",yes file inserted rather extra step modify directly,issue,negative,positive,neutral,neutral,positive,positive
1830374935,"@vonsago I was not able to reproduce the memory leak.

Could you run the following `test.py` (same as yours but without runtime env)

```
import time
import ray
import uuid
import shlex
import json
import uvicorn
import logging
import subprocess

from subprocess import Popen

@ray.remote
def run_fop(entrypoint, submission_id, metadata):
    cmd = shlex.split(entrypoint)
    logging.info(f""run fop pass serve {cmd}"")
    with Popen(args=cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL) as p:
        ...

metadata = {
        ""biz_metadata"": ""{\""flow\"":\""md5\"",\""fop\"":\""md5\"",\""smid\"":\""id\"",\""upos_uri\"":\""urixxxx\"",\""wrk_random\"":\""189728\""}"",
        ""submission_id"": ""test14""
}

if __name__ == ""__main__"":
    s = ""sdfas12387461982734619823746198237461982374691283746918273649812763498127634987162349876213496128974619287649817263498126734986213846128976349817263498126394876129837461928374691283746918237469123784691823746918732649182736491823764918273649182736491823764198237649182376419283764192873649128736491873246198237461982374619827364198273469182375610237586123908476192387461928374691823469182736498123649123""
    for i in range(100000000000000):
        a = run_fop.options(
            num_cpus=0.5,
            num_gpus=0,
            ).remote(""python3 -c \""import logging; for i in range(100): logging.info(s)\"""", f""tt{i}"", metadata)
        print(a)
        # ray.get(a)
        time.sleep(0.05)
```

and use `top` to monitor the driver process memory usage and see if it has increasing memory usage. You can do something like `watch -n 5 top -p <driver_pid> -n 1`",able reproduce memory leak could run following without import time import ray import import import import import logging import import run fop pas serve test range python import logging range print use top monitor driver process memory usage see increasing memory usage something like watch top,issue,negative,positive,positive,positive,positive,positive
1830371182,@peytondmurray Was doc/source/ray-overview/eco-gallery.yml supposed to be deleted? Is there a new place to add integrations now?,supposed new place add,issue,negative,positive,positive,positive,positive,positive
1830285182,"> Generally lgtm
> 
> 
> 
> Actually, I'd like to discuss a little bit more about num_paused_tasks
> 
> 
> 
> Is it currently used to be displayed for vs code debugger terminal? If not, I feel like it is better removing this.
> 
> 
> 
> 1. Dashboard doesn't display worker state, and we don't have any plan to expose it in a while, so it won't be that useful
> 
> 2. it will simplify a lot of code in this PR

What we need here is a flag to signal a process being paused in the debugger. That flag can not be the port since you might have a port and be either paused or not.

I am using num_paused_tasks counter essentially as a flag, but then I can capture threading paused and unpaused safely.

The current behavior is all threads are unpaused on connect so technically an unpaused call should trump all paused but I would rather not build with that assumption giving that it won't technically simplify things for me.

If you have any suggestions to simply that I am happy to consider.",generally actually like discus little bit currently used displayed code terminal feel like better removing dashboard display worker state plan expose wo useful simplify lot code need flag signal process flag port since might port either counter essentially flag capture safely current behavior connect technically call trump would rather build assumption giving wo technically simplify simply happy consider,issue,positive,positive,positive,positive,positive,positive
1830241766,"in ray 2.4.0, the error for the map_group+ actor is same as this one 
https://github.com/ray-project/ray/issues/26244
![tmp](https://github.com/ray-project/ray/assets/121425509/c5ebed71-e9d4-464c-bcfc-4f6f26d805ea)
",ray error actor one,issue,negative,neutral,neutral,neutral,neutral,neutral
1830228197,"I tried this with pip.

If I pip install `ray[core]` in a clean venv, I see it does not try to start the dashboard:
```
$ python3.10 -m venv /tmp/throwaway
$ source /tmp/throwaway/bin/activate
$ pip install ""ray[core]""
$ python -c ""import ray; ray.init()""
INFO worker.py:1664 -- Started a local Ray instance.
```

Now if I add `ray[default]` it installs many packages, none of them ray:
```
$ pip install ""ray[default]""
...
Successfully installed 
aiohttp-3.9.1 
aiohttp-cors-0.7.0 
async-timeout-4.0.3 
blessed-1.20.0 
cachetools-5.3.2 
colorful-0.5.5 distlib-0.3.7 
google-api-core-2.14.0 
google-auth-2.23.4 
googleapis-common-protos-1.61.0 
gpustat-1.1.1 
grpcio-1.59.3 
multidict-6.0.4 
nvidia-ml-py-12.535.133 
opencensus-0.11.3 
opencensus-context-0.1.3 
platformdirs-3.11.0 
prometheus-client-0.19.0 
psutil-5.9.6 py-spy-0.3.14 
pyasn1-0.5.1 
pyasn1-modules-0.3.0 
pydantic-1.10.13 
rsa-4.9 
six-1.16.0 
smart-open-6.4.0 
typing-extensions-4.8.0 
virtualenv-20.21.0 
wcwidth-0.2.12 yarl-1.9.3

$ python -c ""import ray; ray.init()""
INFO worker.py:1664 -- Started a local Ray instance. View the dashboard at 127.0.0.1:8265
```

Even though no new ray package has been installed, the dashboard is now active. What changed?",tried pip pip install ray core clean see try start dashboard python source pip install ray core python import ray local ray instance add ray default many none ray pip install ray default successfully python import ray local ray instance view dashboard even though new ray package dashboard active,issue,positive,positive,positive,positive,positive,positive
1830145106,"> Users with more memory constraint could set `RAY_task_events_max_num_task_in_gcs` to a smaller value.

I couldn't find this in the docs. How does one set this value? Is it an envvar? Can it be set programatically?",memory constraint could set smaller value could find one set value set,issue,positive,neutral,neutral,neutral,neutral,neutral
1830056688,@jtlz2 The only other thing I remember that helped for me and that wasn't mentioned recently was making sure I allocate at least 2 cores for the slurm job (even if I set `num_cpus=1`). ,thing remember recently making sure allocate least job even set,issue,negative,positive,neutral,neutral,positive,positive
1829867233,"@mgerstgrasser Had high hopes for your solution, but for me I get an unhandled segfault:

```
>>> try_start_ray(1, True)
Trying to start ray.
2023-11-28 13:38:17,224 INFO worker.py:1673 -- Started a local Ray instance.
[2023-11-28 13:38:17,333 E 109 109] core_worker.cc:205: Failed to register worker 01000000ffffffffffffffffffffffffffffffffffffffffffffffff to Raylet. IOError: [RayletClient] Unable to register worker with raylet. No such file or directory
```

Any ideas? :(",high solution get unhandled true trying start ray local ray instance register worker raylet unable register worker raylet file directory,issue,positive,positive,neutral,neutral,positive,positive
1829855641,I have `num_cpus=1` already - how to see how it could be reduced any further......,already see could reduced,issue,negative,neutral,neutral,neutral,neutral,neutral
1829654107,I will review this soon. The core team should start tracking the perf in the perf dashboard as well,review soon core team start dashboard well,issue,negative,neutral,neutral,neutral,neutral,neutral
1829652428,hmm what about we always pass special env var (RAY_MPI_WORKER=1) and invoke this method whenever this env var is set in default_worker.py? ,always pas special invoke method whenever set,issue,negative,positive,positive,positive,positive,positive
1829644815,"Awesome PR! I will take a look shortly

some of window tests seem to fail. Let me retry them.",awesome take look shortly window seem fail let retry,issue,negative,positive,positive,positive,positive,positive
1829627642,client test -> P0.5 (not a release blocker),client test release blocker,issue,negative,neutral,neutral,neutral,neutral,neutral
1829625386,"anything outside flaky list is P1 based on our initial policy. The order is 

P0: linux flaky tests -> By 2.9 release
P0.5: OSX / client tests -> Maybe by a month after 2.9
And we recreate the policy and start fixing issues like this (yellow)
",anything outside flaky list based initial policy order flaky release client maybe month recreate policy start fixing like yellow,issue,negative,neutral,neutral,neutral,neutral,neutral
1829621264,nice! this aligns with my result. Let's keep your benchmark and start tracking the performance stats,nice result let keep start performance,issue,negative,positive,positive,positive,positive,positive
1829413083,"As you mentioned, the problem was found by performing ray.init() without creating all actors and tasks. 
This happened because there was too little memory compared to the number of processors. 
If enough memory is available, ray.init() gradually ramps up to a certain level and then stabilizes. 
Thank you for your answer.",problem found without little memory number enough memory available gradually certain level thank answer,issue,negative,positive,positive,positive,positive,positive
1829312189,"I could fix this issue by passing the output of the custom model as aligned with the DiagGaussian implementation. It should have ideally thrown an error regarding the incorrect shape, but it tends to continue running irrespective with an erroneous output as mentioned above. Closing as this is no longer an issue for me.",could fix issue passing output custom model implementation ideally thrown error regarding incorrect shape continue running irrespective erroneous output longer issue,issue,negative,positive,positive,positive,positive,positive
1829308074,"Same issue here with Ray version 2.8.0 and running 3 workers cluster by CLI.
And got this error when doing HDFS data preprocessing. (Reading multiple parquets as Ray Dataset and applying different filters on them, then union.)

```
(raylet, ip=192.168.222.237) [2023-11-28 16:01:16,836 E 115135 115159] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2023-11-28_09-05-03_302027_73389 is over 95% full, available space: 0; capacity: 588680110080. Object creation will fail if spilling is required.
2023-11-28 16:01:17,004 WARNING worker.py:2074 -- Traceback (most recent call last):
  File ""python/ray/_raylet.pyx"", line 2203, in ray._raylet.spill_objects_handler
  File ""python/ray/_raylet.pyx"", line 2206, in ray._raylet.spill_objects_handler
  File ""/mnt/NAS/sda/ShareFolder/lidawei/ExperimentNotebook/daweilee_research/lib/python3.8/site-packages/ray/_private/external_storage.py"", line 668, in spill_objects
    return _external_storage.spill_objects(object_refs, owner_addresses)
  File ""/mnt/NAS/sda/ShareFolder/lidawei/ExperimentNotebook/daweilee_research/lib/python3.8/site-packages/ray/_private/external_storage.py"", line 304, in spill_objects
    with open(url, ""wb"", buffering=self._buffer_size) as f:
OSError: [Errno 28] No space left on device: '/tmp/ray/session_2023-11-28_09-05-03_302027_73389/ray_spilled_objects/c7aefd14c764412392a2403ace403e00-multi-2000'
An unexpected internal error occurred while the IO worker was spilling objects: [Errno 28] No space left on device: '/tmp/ray/session_2023-11-28_09-05-03_302027_73389/ray_spilled_objects/c7aefd14c764412392a2403ace403e00-multi-2000'
2023-11-28 16:01:17,018 WARNING worker.py:2074 -- Traceback (most recent call last):
  File ""python/ray/_raylet.pyx"", line 2203, in ray._raylet.spill_objects_handler
  File ""python/ray/_raylet.pyx"", line 2206, in ray._raylet.spill_objects_handler
  File ""/mnt/NAS/sda/ShareFolder/lidawei/ExperimentNotebook/daweilee_research/lib/python3.8/site-packages/ray/_private/external_storage.py"", line 668, in spill_objects
    return _external_storage.spill_objects(object_refs, owner_addresses)
  File ""/mnt/NAS/sda/ShareFolder/lidawei/ExperimentNotebook/daweilee_research/lib/python3.8/site-packages/ray/_private/external_storage.py"", line 305, in spill_objects
    return self._write_multiple_objects(f, object_refs, owner_addresses, url)
  File ""/mnt/NAS/sda/ShareFolder/lidawei/ExperimentNotebook/daweilee_research/lib/python3.8/site-packages/ray/_private/external_storage.py"", line 149, in _write_multiple_objects
    written_bytes = f.write(payload)
OSError: [Errno 28] No space left on device
An unexpected internal error occurred while the IO worker was spilling objects: [Errno 28] No space left on device
```

```bash
/tmp/ray $ du -sh *
4.0K    ray_current_cluster
77G     session_2023-11-28_09-05-03_302027_73389
0       session_latest

/tmp/ray/session_2023-11-28_09-05-03_302027_73389 $ du -sh *
32M     logs
4.0K    node_ip_address.json
0       node_ip_address.json.lock
4.0K    ports_by_node.json
0       ports_by_node.json.lock
76G     ray_spilled_objects
929M    runtime_resources
4.0K    sockets
```

Not sure which parameter controls how Ray spills objects. (memory size, disk limit, when to spill, etc.)
And when to add these parameters. (Start cluster or Initial client)

> ~~Maybe `object_store_memory`? [ray.init](https://docs.ray.io/en/latest/ray-core/api/doc/ray.init.html)~~
> 
> In Grafana `Object Store Memory` hint. Found there is no SPILLED on the chart.
> 
> ```
> Object store memory usage by location. The dotted line indicates the object store memory capacity.
> 
> Location: where the memory was allocated, which is MMAP_SHM or MMAP_DISK to indicate memory-mapped page, SPILLED to indicate spillage to disk, and WORKER_HEAP for objects small enough to be inlined in worker memory. Refer to metric_defs.cc for more information.
> ```

- [Object Spilling — Ray 2.8.0](https://docs.ray.io/en/latest/ray-core/objects/object-spilling.html#cluster-mode)

Raylet Log

`/tmp/ray/session_latest/logs/raylet.out`

```txt
[2023-11-28 16:00:46,795 E 115135 115159] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2023-11-28_09-05-03_302027_73389 is over 95% full, available space: 4652392448; capacity: 588680110080. Object creation will fail if spilling is required.
[2023-11-28 16:00:47,498 I 115135 115135] (raylet) node_manager.cc:2211: Triggering object spilling because current usage 80.1694% is above threshold 80%.
```

`/tmp/ray/session_latest/logs/raylet.err`

```txt
[2023-11-28 16:38:10,464 E 115135 115159] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2023-11-28_09-05-03_302027_73389 is over 95% full, available space: 22450171904; capacity: 588680110080. Object creation will fail if spilling is required.
[2023-11-28 16:38:20,477 E 115135 115159] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2023-11-28_09-05-03_302027_73389 is over 95% full, available space: 22450167808; capacity: 588680110080. Object creation will fail if spilling is required.
```

[Object store spilling terabytes of data - Ray Core - Ray](https://discuss.ray.io/t/object-store-spilling-terabytes-of-data/8699/2): The spilling usually happens when you have more objects than the capacity of the shared memory (30% of node memory by default).

- [Cluster Management CLI — Ray 3.0.0.dev0](https://docs.ray.io/en/master/cluster/cli.html#cmdoption-ray-start-object-store-memory)

[Memory Management — Ray 3.0.0.dev0](https://docs.ray.io/en/master/ray-core/scheduling/memory-management.html#concepts): Object store memory: memory used when your application creates objects in the object store via ray.put and when it returns values from remote functions. Objects are reference counted and evicted when they fall out of scope. An object store server runs on each node. By default, when starting an instance, Ray reserves 30% of available memory. The size of the object store can be controlled by [–object-store-memory](https://docs.ray.io/en/master/cluster/cli.html#cmdoption-ray-start-object-store-memory). The memory is by default allocated to /dev/shm (shared memory) for Linux. For MacOS, Ray uses /tmp (disk), which can impact the performance compared to Linux. In Ray 1.3+, objects are [spilled to disk](https://docs.ray.io/en/master/ray-core/objects/object-spilling.html#id1) if the object store fills up.",issue ray version running cluster got error data reading multiple ray different union raylet raylet full available space capacity object creation fail warning recent call last file line file line file line return file line open space left device unexpected internal error io worker space left device warning recent call last file line file line file line return file line return file line space left device unexpected internal error io worker space left device bash sure parameter ray memory size disk limit spill add start cluster initial client object store memory hint found chart object store memory usage location dotted line object store memory capacity location memory indicate page indicate spillage disk small enough worker memory refer information object ray raylet log raylet full available space capacity object creation fail raylet object current usage threshold raylet full available space capacity object creation fail raylet full available space capacity object creation fail object store data ray core ray usually capacity memory node memory default cluster management ray dev memory management ray dev object store memory memory used application object store via remote reference fall scope object store server node default starting instance ray available memory size object store memory default memory ray disk impact performance ray disk object store,issue,negative,positive,neutral,neutral,positive,positive
1829276931,"but it is flaky:
https://buildkite.com/ray-project/premerge/builds/12900#018c14b3-53f1-46e8-902c-affce4dae303/6-379

seems that always fail once first. why?",flaky always fail first,issue,negative,negative,negative,negative,negative,negative
1829239004,"I'm currently pinned to 2.4, though I'll see if my vendor can help us get upgraded.

Even when we do though, I imagine the same difficulty when trying to utilise the .ubj format for model saving.

Are there any plans to make this property configurable? We also hit the same constraints within the Batch predictor.

Thanks for looking into this",currently pinned though see vendor help u get even though imagine difficulty trying format model saving make property also hit within batch predictor thanks looking,issue,negative,positive,neutral,neutral,positive,positive
1829119193,"> Correct me if I'm wrong, but the deserialization happens because we want the python objects be used in a Python thread (e.g. the ""python worker's main thread""?) so we can already offload it to that existing thread?

Currently it will cause deadlock since the existing python thread is blocked waiting for the object to be deserialized and ready. If we post the deserialization to the existing python thread, it will never run.",correct wrong want python used python thread python worker main thread already thread currently cause deadlock since python thread blocked waiting object ready post python thread never run,issue,negative,negative,neutral,neutral,negative,negative
1829038910,Thanks for putting this together -- we need to do it in a way that makes these dependencies only visible in the dashboard and runtime environment agent processes (i.e. it shouldn't be visible to workers or any other processes that the user can put code into -- they should be able to use whichever version of these packages they want) :),thanks together need way visible dashboard environment agent visible user put code able use whichever version want,issue,negative,positive,positive,positive,positive,positive
1828941692,"Likely still failing, I don't see a fix yet",likely still failing see fix yet,issue,negative,neutral,neutral,neutral,neutral,neutral
1828941379,"I'm adding a few more benchmarks in [here](https://github.com/ray-project/ray/pull/41229/files), here are the results:

Before
---

```
# Core Streaming
Core Actors streaming throughput (ASYNC) (num_replicas=1, tokens_per_request=1000, batch_size=10): 12384.79 +- 106.67 tokens/s
(CallerActor pid=66926) Individual request quantiles:
(CallerActor pid=66926) 	P50=807.6792080000002
(CallerActor pid=66926) 	P75=857.1882605000001
(CallerActor pid=66926) 	P99=903.21980678

# Serve Handle Streaming
DeploymentHandle streaming throughput (ASYNC) (num_replicas=1, tokens_per_request=1000, batch_size=10): 10981.91 +- 280.73 tokens/s
(ServeReplica:default:CallerDeployment pid=71672) Individual request quantiles:
(ServeReplica:default:CallerDeployment pid=71672) 	P50=905.3096459999992
(ServeReplica:default:CallerDeployment pid=71672) 	P75=957.0047495000003
(ServeReplica:default:CallerDeployment pid=71672) 	P99=993.9941300000013

```


After
---

```
# Core Streaming
Core Actors streaming throughput (ASYNC) (num_replicas=1, tokens_per_request=1000, batch_size=10): 19252.14 +- 330.47 tokens/s

P50=519.7334164999993
P75=585.4596035
P99=595.3468529799989

# Serve Handle Streaming
DeploymentHandle streaming throughput (ASYNC) (num_replicas=1, tokens_per_request=1000, batch_size=10): 17393.68 +- 644.61 tokens/s

P50=583.9117500000004
P75=633.7538224999998
P99=653.4384627099994
```

Great job @rkooo567 !",core streaming core streaming throughput individual request serve handle streaming streaming throughput default individual request default default default core streaming core streaming throughput serve handle streaming streaming throughput great job,issue,positive,positive,positive,positive,positive,positive
1828932451,Hi could you share a full repro script?,hi could share full script,issue,negative,positive,positive,positive,positive,positive
1828919415,"Hey @farridav , can you share which version of Ray you are using?

If you are using the latest version, I believe it should be saving it with the `.json` prefix already.

https://github.com/ray-project/ray/blob/8919bf0d1f12b6fbf515b6364c873544cf0ca25b/python/ray/train/xgboost/xgboost_trainer.py#L109

https://github.com/ray-project/ray/blob/8919bf0d1f12b6fbf515b6364c873544cf0ca25b/python/ray/train/xgboost/xgboost_checkpoint.py#L18",hey share version ray latest version believe saving prefix already,issue,negative,positive,positive,positive,positive,positive
1828886774,"> Looks good! But let's see if we can make the unit test change before we merge?

done with a larger batch_size",good let see make unit test change merge done,issue,negative,positive,positive,positive,positive,positive
1828882159,Still bisecting - the previously thought PR already had regression in it. (Reverting doesn't work). ,still previously thought already regression work,issue,negative,negative,negative,negative,negative,negative
1828828049,"Thanks, @MissiontoMars, do you think you could take this for now and see if you can repro?",thanks think could take see,issue,negative,positive,positive,positive,positive,positive
1828817879,Will merge after checking the doc once the doc build passes. Thanks for the contribution!,merge doc doc build thanks contribution,issue,negative,positive,positive,positive,positive,positive
1828817333,"Ah makes sense, thanks! If you'd like to open a PR to update the contributor docs, that would also be appreciated, we want those to be as clear as possible. I think other people will run into the same issue",ah sense thanks like open update contributor would also want clear possible think people run issue,issue,positive,positive,neutral,neutral,positive,positive
1828817067,"> LGTM, but can you add the failure stats to `Dataset.stats()`?

All the metrics with `export_metric=True` are already part of stats(). ",add failure metric already part,issue,negative,negative,negative,negative,negative,negative
1828809780,"@architkulkarni I did fork it I just hadn't realized I needed to push to the fork. it should be good to go now though, it just needs a review.",fork push fork good go though need review,issue,negative,positive,positive,positive,positive,positive
1828808401,"its at 19% failure rate but doesn't look too bad in color. i vote we upgrade this to p1 since it is linux and not the ""reddest"" compared to all of our other tests.",failure rate look bad color vote upgrade since,issue,negative,negative,negative,negative,negative,negative
1828804484,"this is still pretty bad but considering it's osx let's keep as p1 for now...


![Image](https://github.com/ray-project/ray/assets/116198444/8adbe397-c9a9-4bac-a1a6-d279ddee8a34)


@rkooo567 that cool? we won't keep this as release-blocker for ray29.",still pretty bad considering let keep image cool wo keep ray,issue,negative,negative,neutral,neutral,negative,negative
1828801757,Still poor - @rkooo567 should we make this a release-blocker and fix this week?,still poor make fix week,issue,negative,negative,negative,negative,negative,negative
1828800911,"it's slightly better but still intermittently failing (yellow blocks). @rkooo567 do you have any opinions on whether it's worth to assign this to someone to look into the root cause for still the intermittent issues.

recommend we keep p0 as this is a linux flaky test.",slightly better still intermittently failing yellow whether worth assign someone look root cause still intermittent recommend keep flaky test,issue,negative,positive,positive,positive,positive,positive
1828721619,"We have a similar request. Something like argo workflow's [onExit](https://argoproj.github.io/argo-workflows/walk-through/exit-handlers/) step would be amazing. For example, we could specify a docker image so KubeRay can launch it with relevant information as arguments when a job is finished. ",similar request something like argo step would amazing example could specify docker image launch relevant information job finished,issue,positive,positive,positive,positive,positive,positive
1828715864,"@WeichenXu123 Ray data doesn't support Ray client. You can warp your dataset code inside a task:

```
import ray
import pandas as pd
ray.init(""ray://<head_node_ip>:10001"")

@ray.remote
def ray_data_task():
    p1 = pd.DataFrame({'a': [3,4] * 10000, 'b': [5,6] * 10000})
    ds = ray.data.from_pandas(p1)
    return ds.repartition(4).to_pandas()

ray.get(ray_data_task.remote())
```",ray data support ray client warp code inside task import ray import ray return,issue,negative,neutral,neutral,neutral,neutral,neutral
1828706515,Let us know if this fixes the issue. ,let u know issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1828631591,@pcmoritz yep waiting for it to generate [here](https://buildkite.com/ray-project/premerge/builds/12798#018c129e-7542-407e-8635-6f93475af7ef) and will update the PR with the update file!,yep waiting generate update update file,issue,negative,neutral,neutral,neutral,neutral,neutral
1828622062,"I think the stack size problem will appear on any thread created via `pthread_create` on macOS, since the default for that platform is 512kb. If I understand the discussion, it is about which thread to use for significant work. Perhaps we could limit this PR to making the stack larger in general, and then in a future PR deal with what activity happens on which thread. Or did I misunderstand the discussion?",think stack size problem appear thread via since default platform understand discussion thread use significant work perhaps could limit making stack general future deal activity thread misunderstand discussion,issue,negative,positive,positive,positive,positive,positive
1828596383,"Ah, I didn't even know about trial_dirname_creator—probably because I'm using the Train API rather than the Tune API, though I've seen in the code that Train uses a lot of Tue code under the hood. From what I can tell, I wouldn't be able to use trial_dirname_creator right now without converting my train code to use `Tuner` and `TuneConfig`.

But either your (1) or (2) solution would work for me! I think 1 feels more appropriate in that it takes a functionality that's common between user-facing Train and Tune APIs and factors it into a config that's shared between them. 

As for 2, I noticed today that it's pretty easy to write multiple results directories [to the same place](https://ray-distributed.slack.com/archives/CSX7HVB5L/p1701114164076439) so having no nesting at all could definitely get messy.

In the meantime, I'll likely have to do something hacky to find the TorchTrainer directory underneath the main directory and that should work for now.",ah even know train rather tune though seen code train lot tue code hood tell would able use right without converting train code use tuner either solution would work think appropriate functionality common train tune today pretty easy write multiple place could definitely get messy likely something hacky find directory underneath main directory work,issue,positive,positive,positive,positive,positive,positive
1828584591,"Correct me if I'm wrong, but the deserialization happens because we want the python objects be used in a Python thread (e.g. the ""python worker's main thread""?) so we can already offload it to that existing thread?",correct wrong want python used python thread python worker main thread already thread,issue,negative,negative,negative,negative,negative,negative
1828568142,"Oh if this has been jailed, you need to close the issue to run it on nightly run.

It will open itself if it fails again on nightly.",oh need close issue run nightly run open nightly,issue,negative,neutral,neutral,neutral,neutral,neutral
1828536267,"Ah, the code you're referring to is correct. This is actually exposed in Tune only as part of the `TuneConfig.trial_dirname_creator` API. 

If we extend the code snippet from above, you can do something like this:

```python

from ray.tune import Tuner, TuneConfig
tuner = Tuner(trainer, tune_config=TuneConfig(trial_dirname_creator=lambda x: ""trial_abc""))
tuner.fit()
```

```
/tmp/storage/run_123
├── basic-variant-state-2023-11-27_12-10-32.json
├── experiment_state-2023-11-27_12-10-32.json
├── trial_abc
│   ├── checkpoint_000000
│   │   └── 0
│   ├── checkpoint_000001
│   │   └── 1
│   ├── checkpoint_000002
│   │   └── 2
│   ├── checkpoint_000003
│   │   └── 3
│   ├── checkpoint_000004
│   │   └── 4
│   ├── events.out.tfevents.1701115835.matt-QM624Q9WT0.local.meter
│   ├── params.json
│   ├── params.pkl
│   ├── progress.csv
│   └── result.json
└── tuner.pkl
```

There are a few options we've discussed about how to improve the usability of this.
1. Move `trial_dirname_creator` to RunConfig`. This would naively be able to expose the desired functionality, but it is more of a workaround. Generally speaking for Ray Train you shouldn't have to know about the Trial concept, which leads to...
2. Update Ray Train to only generate a single non-nested folder, whose name will be defined by `name`. This should generally be cleaner to reason about (Train produces one folder, Tune produces a folder of folders). However, this is a bit more involved of a change that would require more detailed scoping.",ah code correct actually exposed tune part extend code snippet something like python import tuner tuner tuner trainer improve usability move would naively able expose desired functionality generally speaking ray train know trial concept update ray train generate single folder whose name defined name generally cleaner reason train one folder tune folder however bit involved change would require detailed,issue,positive,positive,positive,positive,positive,positive
1828503947,"```
pure virtual method called
[2023-11-25 14:01:36,540 E 853 884] (gcs_server) logging.cc:104: Stack trace: 
 /home/ray/anaconda3/lib/python3.8/site-packages/ray/core/src/ray/gcs/gcs_server(+0x6ca1ca) [0x5650776321ca] ray::operator<<()
/home/ray/anaconda3/lib/python3.8/site-packages/ray/core/src/ray/gcs/gcs_server(+0x6cc8f8) [0x5650776348f8] ray::TerminateHandler()
/lib/x86_64-linux-gnu/libstdc++.so.6(+0xaa37c) [0x7f88c3f3837c]
/lib/x86_64-linux-gnu/libstdc++.so.6(+0xaa3e7) [0x7f88c3f383e7]
/lib/x86_64-linux-gnu/libstdc++.so.6(+0xab145) [0x7f88c3f39145]
/home/ray/anaconda3/lib/python3.8/site-packages/ray/core/src/ray/gcs/gcs_server(+0x263984) [0x5650771cb984] grpc::internal::CallOpSet<>::FillOps()
/home/ray/anaconda3/lib/python3.8/site-packages/ray/core/src/ray/gcs/gcs_server(+0x541ed3) [0x5650774a9ed3] std::_Function_handler<>::_M_invoke()
/home/ray/anaconda3/lib/python3.8/site-packages/ray/core/src/ray/gcs/gcs_server(+0x5d6e8e) [0x56507753ee8e] EventTracker::RecordExecution()
/home/ray/anaconda3/lib/python3.8/site-packages/ray/core/src/ray/gcs/gcs_server(+0x5d03d7) [0x5650775383d7] boost::asio::detail::completion_handler<>::do_complete()
/home/ray/anaconda3/lib/python3.8/site-packages/ray/core/src/ray/gcs/gcs_server(+0x6d96cb) [0x5650776416cb] boost::asio::detail::scheduler::do_run_one()
/home/ray/anaconda3/lib/python3.8/site-packages/ray/core/src/ray/gcs/gcs_server(+0x6db7b9) [0x5650776437b9] boost::asio::detail::scheduler::run()
/home/ray/anaconda3/lib/python3.8/site-packages/ray/core/src/ray/gcs/gcs_server(+0x6dbc72) [0x565077643c72] boost::asio::io_context::run()
/home/ray/anaconda3/lib/python3.8/site-packages/ray/core/src/ray/gcs/gcs_server(+0x262a15) [0x5650771caa15] std::thread::_State_impl<>::_M_run()
/home/ray/anaconda3/lib/python3.8/site-packages/ray/core/src/ray/gcs/gcs_server(+0xccd160) [0x565077c35160] execute_native_thread_routine
/lib/x86_64-linux-gnu/libpthread.so.0(+0x8609) [0x7f88c41d1609] start_thread
/lib/x86_64-linux-gnu/libc.so.6(clone+0x43) [0x7f88c3d9e133] __clone

*** SIGABRT received at time=1700949696 on cpu 7 ***
PC: @     0x7f88c3cc200b  (unknown)  raise
    @     0x7f88c41dd420       4048  (unknown)
    @     0x7f88c3f3837c        176  (unknown)
    @     0x5650774a9ed3        144  std::_Function_handler<>::_M_invoke()
    @     0x56507753ee8e        128  EventTracker::RecordExecution()
    @     0x5650775383d7        160  boost::asio::detail::completion_handler<>::do_complete()
    @     0x5650776416cb        128  boost::asio::detail::scheduler::do_run_one()
    @     0x5650776437b9        288  boost::asio::detail::scheduler::run()
    @     0x565077643c72         96  boost::asio::io_context::run()
    @     0x5650771caa15         48  std::thread::_State_impl<>::_M_run()
    @     0x565077c35160  178409696  execute_native_thread_routine
    @     0x7ffd74c140d0  (unknown)  (unknown)
    @     0x5650771b1ca0  (unknown)  (unknown)
    @ 0x9000a834e1e90789  (unknown)  (unknown)
[2023-11-25 14:01:36,617 E 853 884] (gcs_server) logging.cc:361: *** SIGABRT received at time=1700949696 on cpu 7 ***
[2023-11-25 14:01:36,617 E 853 884] (gcs_server) logging.cc:361: PC: @     0x7f88c3cc200b  (unknown)  raise
[2023-11-25 14:01:36,617 E 853 884] (gcs_server) logging.cc:361:     @     0x7f88c41dd420       4048  (unknown)
[2023-11-25 14:01:36,621 E 853 884] (gcs_server) logging.cc:361:     @     0x7f88c3f3837c        176  (unknown)
[2023-11-25 14:01:36,621 E 853 884] (gcs_server) logging.cc:361:     @     0x5650774a9ed3        144  std::_Function_handler<>::_M_invoke()
[2023-11-25 14:01:36,621 E 853 884] (gcs_server) logging.cc:361:     @     0x56507753ee8e        128  EventTracker::RecordExecution()
[2023-11-25 14:01:36,621 E 853 884] (gcs_server) logging.cc:361:     @     0x5650775383d7        160  boost::asio::detail::completion_handler<>::do_complete()
[2023-11-25 14:01:36,621 E 853 884] (gcs_server) logging.cc:361:     @     0x5650776416cb        128  boost::asio::detail::scheduler::do_run_one()

```",pure virtual method stack trace ray ray boost boost boost boost received unknown raise unknown unknown boost boost boost boost unknown unknown unknown unknown unknown unknown received unknown raise unknown unknown boost boost,issue,positive,negative,neutral,neutral,negative,negative
1828489193,"For the benefit of others, ive managed to solve this problem with the following implementation:

```
        class MyXGBoostTrainer(XGBoostTrainer):
            def _save_model(self, model: xgboost.Booster, path: str) -> None:
                model.save_model(path + "".ubj"")
```

Then using that instead, ill leave this ticket open, in case there is a cleaner, more config driven approach to this, thanks",benefit solve problem following implementation class self model path none path instead ill leave ticket open case cleaner driven approach thanks,issue,positive,negative,neutral,neutral,negative,negative
1828475167,"> @architkulkarni could you test it locally to make sure the usage stats report is accepted by the server.

@jjyao Sure, could you explain how to do that?",could test locally make sure usage report accepted server sure could explain,issue,positive,positive,positive,positive,positive,positive
1828468540,"> I'd like to understand why we should require users to call mpi_init. I feel like it should be possible for us to just call it ourselves under the hood?
> 
> I think we should choose one of them instead;
> 
> 1. raise an exception if mpi_init is not called
> 2. automatically call mpi_init all the time
> 
> I'd like to understand why 2 is not possible here. What's the reason why we can't just call mpi_init when rank is 0?

Hi Sang, thanks for helping reviewing this.

2) is not possible because GPU allocation happens when you start to run the function. And runtime env can't get the actual function, so we can't wrap it.

For example,
```
@ray.remote(runtime_env={...})
de f():
   pass
```

So two way to do it: wrapper f inside runtime env and call it. Or, we do it in ray core's code. but both don't fit.


> raise an exception if mpi_init is not called

I think this is also hard. If this can be done, option 1) should be simple. The reason is the same as above.


",like understand require call feel like possible u call hood think choose one instead raise exception automatically call time like understand possible reason ca call rank hi sang thanks helping possible allocation start run function ca get actual function ca wrap example de pas two way wrapper inside call ray core code fit raise exception think also hard done option simple reason,issue,positive,negative,neutral,neutral,negative,negative
1828458839,"> can we actually move the deserialization to the python threads instead of doing it on IO thread? This looks kinda blocks IO threads and reduces throughput.

I think technically we can create another thread just for running python code but so far, we haven't seen issues of running those python callbacks in the io thread.",actually move python instead io thread io throughput think technically create another thread running python code far seen running python io thread,issue,negative,positive,neutral,neutral,positive,positive
1828453660,@architkulkarni could you test it locally to make sure the usage stats report is accepted by the server.,could test locally make sure usage report accepted server,issue,positive,positive,positive,positive,positive,positive
1828431159,"@jjyao Thanks! Fixed the usagelib code here, e31b07b3d7 please take a look. 

The RFC is already approved, but the more eyes on this PR the better.",thanks fixed code please take look already better,issue,positive,positive,positive,positive,positive,positive
1828412633,This is showing up in release test as well: https://github.com/ray-project/ray/issues/41372#event-11068857068,showing release test well,issue,negative,neutral,neutral,neutral,neutral,neutral
1828400713,can we actually move the deserialization to the python threads instead of doing it on IO thread? This looks kinda blocks IO threads and reduces throughput.,actually move python instead io thread io throughput,issue,negative,neutral,neutral,neutral,neutral,neutral
1828397208,"Hey @angelinalg, I agree with these tags. Only additional one I'd add is `Natural Language Processing`. ",hey agree additional one add natural language,issue,positive,positive,neutral,neutral,positive,positive
1828367619,Your explanation makes sense to me. But I think we need to get some confirmation from cloudpickle devs saying that checking MAGIC_NUMBER is enough? cc @pcmoritz ,explanation sense think need get confirmation saying enough,issue,negative,neutral,neutral,neutral,neutral,neutral
1828352187,"I found some instructions here https://docs.ray.io/en/latest/ray-contribute/development.html#clone-the-repository, but the instructions about forking should really be in the contributor guide here. https://docs.ray.io/en/latest/ray-contribute/getting-involved.html.  I'll make a followup issue about this",found really contributor guide make issue,issue,negative,positive,positive,positive,positive,positive
1828351151,"```
Running with failures
Traceback (most recent call last):
  File ""chaos_test/test_chaos_basic.py"", line 238, in <module>
    main()
  File ""chaos_test/test_chaos_basic.py"", line 209, in main
    node_killer = ray.get_actor(""node_killer"", namespace=""release_test_namespace"")
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/auto_init_hook.py"", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py"", line 2845, in get_actor
    return worker.core_worker.get_named_actor_handle(name, namespace or """")
  File ""python/ray/_raylet.pyx"", line 4028, in ray._raylet.CoreWorker.get_named_actor_handle
  File ""python/ray/_raylet.pyx"", line 453, in ray._raylet.check_status
ValueError: Failed to look up actor with name 'node_killer'. This could because 1. You are trying to look up a named actor you didn't create. 2. The named actor died. 3. You did not use a namespace matching the namespace of the actor.
(raylet) WARNING: 284 PYTHON worker processes have been started on node: 8fe29f0c9af42ce8489d924a3a1321946619053d420e58c9558e48a5 with address: 10.0.17.192. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds). [repeated 7x across cluster]
Subprocess return code: 1
```",running recent call last file line module main file line main file line return file line wrapper return file line return name file line file line look actor name could trying look actor create actor use matching actor raylet warning python worker node address could result large number due blocked see discussion repeated across cluster return code,issue,negative,positive,neutral,neutral,positive,positive
1828349291,"Oh, you need to fork the repo first, did you do that? You can't make branches on our repo, you'll need to do it on your fork.  But you can still make a PR from your fork to our repo.",oh need fork first ca make need fork still make fork,issue,negative,positive,positive,positive,positive,positive
1828346710,"Can you add more details about what command you ran, the error you got, and maybe a screenshot? Anyone should be able to submit a PR even if they're not in the organization.",add command ran error got maybe anyone able submit even organization,issue,negative,positive,positive,positive,positive,positive
1828320969,Thank you @mattip for checking that this fixes it for Windows! You saved me from having to dust off a Windows machine to test this out 🥳 ,thank saved dust machine test,issue,positive,neutral,neutral,neutral,neutral,neutral
1828299177,"@architkulkarni Ok fantastic, I fixed it in the openapi.yml file but when I went to do the PR I got 403 permission denied, I think that means I'm not part of the organization, so I was wondering how I could be added, of if there's a way around this?",fantastic fixed file went got permission think part organization wondering could added way around,issue,positive,positive,positive,positive,positive,positive
1828292957,I can confirm that this solves the problem when applied locally on top of 2.8.0,confirm problem applied locally top,issue,negative,positive,positive,positive,positive,positive
1828260926,@wingkitlee0 Good call out. I reproduced on my side that this bug doesn't apply to local files. I'll update the ticket to reflect that fact.,good call side bug apply local update ticket reflect fact,issue,negative,positive,positive,positive,positive,positive
1828257150,"This looks like the same error as https://github.com/ray-project/ray/issues/40547, which was fixed by adding a seeded random shuffle on the data: https://github.com/ray-project/ray/pull/40648

```
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/utils/nvtx.py"", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/runtime/zero/stage3.py"", line 1896, in _overflow_check_and_loss_scale_update
    self._update_scale(self.overflow)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/runtime/zero/stage3.py"", line 2249, in _update_scale
    self.loss_scaler.update_scale(has_overflow)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/runtime/fp16/loss_scaler.py"", line 175, in update_scale
    raise Exception(
Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
```",like error fixed seeded random shuffle data file line file line file line file line raise exception exception current loss scale already minimum decrease scale run,issue,negative,negative,negative,negative,negative,negative
1828065259,@angelinalg could you please help shepherd this? 🙏 ,could please help shepherd,issue,positive,neutral,neutral,neutral,neutral,neutral
1828028925,"@rkooo567 it's not user code, it's our callback. In the io thread, we will try to deserialize objects (inside get_async_callback) which will create SerializationContext, which will then run `arrow_serialization.py` and import numpy.",user code io thread try inside create run import,issue,negative,neutral,neutral,neutral,neutral,neutral
1827811397,"It seems that tests are failing that are not related to my changes. Considering that the tests take a long time, it would be great if someone restarted only those tests that failed.",failing related considering take long time would great someone,issue,negative,positive,positive,positive,positive,positive
1827515585,"Closing because I don't need this now.

It's really a disappointing contributing experience given the lack of feedbacks in months.",need really disappointing experience given lack,issue,negative,negative,negative,negative,negative,negative
1827475183,what's the plan here? Are we going to refactor with 1s delay by default first? ,plan going delay default first,issue,negative,positive,positive,positive,positive,positive
1827289588,"nice investigation! 

Btw, do you know where it is imported in io thread? Io thread should not execute any user code theoretically (except callback we registered).

I will approve the PR after ^ is resolved",nice investigation know io thread io thread execute user code theoretically except registered approve resolved,issue,positive,positive,positive,positive,positive,positive
1827286546,"I believe if you merge master now, it should pass!",believe merge master pas,issue,negative,neutral,neutral,neutral,neutral,neutral
1827227534,"Seems there is a ray streaming implementation https://github.com/ray-project/mobius, will it be an official ray streaming implementation? @bveeramani @anyscalesam @ericl 
<img width=""895"" alt=""image"" src=""https://github.com/ray-project/ray/assets/11898942/abb5c8ca-5675-4fc9-97d2-84f248d03acf"">

",ray streaming implementation official ray streaming implementation image,issue,negative,neutral,neutral,neutral,neutral,neutral
1827170240,"@architkulkarni I can take this on, I just want to clarify, is this an issue with just the documentation or is this also a code issue in the API",take want clarify issue documentation also code issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1827003438,Hey @PhilippWillms Thanks so much for pointing this out! I completely forgot that Pathlib Paths directly interact with the underlying FileSystem. I think I just need to switch from a `Path` to a `PurePosixPath` and using posix-specific resolution!,hey thanks much pointing completely forgot directly interact underlying think need switch path resolution,issue,negative,positive,positive,positive,positive,positive
1826939702,"@rkooo567 I took a first pass at what we discussed. Internal metrics logging is disabled by default so should not impact nominal use case. I did confirm the cumulative histogram outputs match what Prometheus outputs for the same inputs. 

The Prometheus utility function might be out of scope for Ray, and maybe there is a cleaner way to implement this. Let me know what you think and I can make changes. Otherwise I can lint + add tests + update docs. ",took first pas internal metric logging disabled default impact nominal use case confirm cumulative histogram match utility function might scope ray maybe cleaner way implement let know think make otherwise lint add update,issue,negative,positive,neutral,neutral,positive,positive
1826822167,"xref the discussion on https://discuss.ray.io/t/error-403-for-ray-dashboard-on-localhost-in-ray-2-8-0-on-windows11/12717

The problem came from #39018 where it is assumed that a path like `/static/*` can be converted via pathlib.resolve() to see if it is outside parent. These lines don't work as desired on windows, note the excess `C:` https://github.com/ray-project/ray/blob/790c5064f82e973d9d964f6e962a27e8fa8cd79d/dashboard/http_server_head.py#L137-L142

variable | example
---|---
request.path | `/static/js/main.4e04a38d.js`
request_path | `WindowsPath('C:/static/css/main.388a904b.css')`
parent | `WindowsPath('/static')`

I debugged this by adding some logging before raising the error, which seems like it might help in general:
```
logger.info(f""FORBIDDEN: {request.path=} became {request_path=} != {parent=}"")
```

cc @ijrsvt",discussion problem came assumed path like converted via see outside parent work desired note excess variable example parent logging raising error like might help general forbidden,issue,negative,positive,neutral,neutral,positive,positive
1826704240,"I ran into a similar and related issue with a different import in setup-dev.py. For me though, I was using Python 3.8.3 and all of these fixes were for Python 3.8.6 and above if I interpret this correctly. Anyway, what worked for me was to update my version of Python to 3.9.18 and use that for my virtual environment.",ran similar related issue different import though python python interpret correctly anyway worked update version python use virtual environment,issue,negative,neutral,neutral,neutral,neutral,neutral
1826195563,"FYI, it runs fine on local files
```
Python 3.8.18 (default, Nov 14 2023, 22:56:19)
Type 'copyright', 'credits' or 'license' for more information
IPython 8.12.3 -- An enhanced Interactive Python. Type '?' for help.

In [1]: import ray

In [2]: ray.__version__
Out[2]: '2.7.1'

In [3]: ray.data.read_text([""a"", ""b"", ""c""], ignore_missing_paths=True).take_all()
...
Out[3]: [{'text': 'this is a cat.'}, {'text': 'this is not a cat.'}]
```",fine local python default type information enhanced interactive python type help import ray cat cat,issue,positive,positive,positive,positive,positive,positive
1826191158,"![image](https://github.com/ray-project/ray/assets/8759816/1b4a66ae-10ad-4038-ac0c-09936efa3d93)

except process, data should be clean up too

",image except process data clean,issue,negative,positive,positive,positive,positive,positive
1826190821,"![image](https://github.com/ray-project/ray/assets/8759816/58e0bbc6-a418-4345-b11c-8867711a2c96)

suggestion: I think we can deploy different ray clusters for this requirement.  we can have another component to support schedule different request of different ray version  to different ray clsuter.",image suggestion think deploy different ray requirement another component support schedule different request different ray version different ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1826190326,"![image](https://github.com/ray-project/ray/assets/8759816/c0cb6980-50fe-49d6-8b1a-fa0b4d34d1c3)

Suggestion: ray need have more security policy,  fox example, 
 1. data security:  user A can not attack user B jobs/tasks/actors, can not get the data of another user.
 2. available,  user A can not attack the ray cluster,  can not cause cluster unavailability， user B job can not work if ray clsuter is unavailable.",image suggestion ray need security policy fox example data security user attack user get data another user available user attack ray cluster cause cluster user job work ray unavailable,issue,negative,positive,positive,positive,positive,positive
1826189158,"![image](https://github.com/ray-project/ray/assets/8759816/4e266cc8-3338-41cd-87d6-7c8ec2d3e857)

I check kuberay already integrate volcano: https://docs.ray.io/en/master/cluster/kubernetes/k8s-ecosystem/volcano.html , and RayCluster CRD already support Priority scheduling by integrate volcano, can we support Priority scheduling for Ray job by integrate volcano?",image check already integrate volcano already support priority integrate volcano support priority ray job integrate volcano,issue,positive,neutral,neutral,neutral,neutral,neutral
1826188785,"We need ray support multi-tenancy in cloud compute domian.
We want to provide ray service to different customers, if ray can support multi-tanacy, it will have more efficient for resource schedule,  lower costs.",need ray support cloud compute want provide ray service different ray support efficient resource schedule lower,issue,positive,neutral,neutral,neutral,neutral,neutral
1826038158,@suquark there is PR resolving this issue here: https://github.com/ray-project/ray/pull/40337. Any chance for merging it in the near feature?,issue chance near feature,issue,negative,positive,neutral,neutral,positive,positive
1825695956,"@SebastianMorawiec I'm just some random contributor, you will have to ping someone on the team to get this resolved.",random contributor ping someone team get resolved,issue,negative,negative,negative,negative,negative,negative
1825546669,"Hello @michaelhly - This PR would resolve high-impact issue we are currently having with Ray. I'd really appreciate if this could be merged until the next release. If you do not have time for that, I could take over due diligence for that PR.",hello would resolve issue currently ray really appreciate could next release time could take due diligence,issue,positive,positive,neutral,neutral,positive,positive
1825514501,"> @veryhannibal thank you for submitting the pr; @stephanie-wang left some comments can you advise on them in the PR so we can than merge and close this ticket?

Thank you for your attention, I will reply to all comments.😁",thank left advise merge close ticket thank attention reply,issue,positive,neutral,neutral,neutral,neutral,neutral
1825205354,"I should have found the cause of the leak, I'm trying to verify and fix it. The reason may be that the function release `Disconnect` is not called in [this place](https://github.com/ray-project/ray/blob/master/src/ray/rpc/worker/core_worker_client_pool.cc#L40) after worker exit",found cause leak trying verify fix reason may function release disconnect place worker exit,issue,negative,neutral,neutral,neutral,neutral,neutral
1824995321,"i've done something like this to avoid decorators and it seems to work:

```
async_tasks=[]
inst_ref = ray.remote(MyClass).remote()
async_tasks.append(inst_ref.process.remoste())
```",done something like avoid work,issue,negative,neutral,neutral,neutral,neutral,neutral
1824791364,"@justinvyu I took a look at the diff highlighting issue and it wasn't actually too difficult to update the style for diffs. I made the change to match github's colorblind (deuteranopia) setting:

![image](https://github.com/ray-project/ray/assets/14017872/ba6d3e59-e162-4444-a387-bb974d884cb5)
",took look issue actually difficult update style made change match deuteranopia setting image,issue,negative,negative,negative,negative,negative,negative
1824678252,@mattip Thanks for the pointer. I confirmed the issue is fixed after I manually increase the stack size.,thanks pointer confirmed issue fixed manually increase stack size,issue,positive,positive,positive,positive,positive,positive
1824509070,"it works, thanks @jjyao. 
Should I raise a PR to add this to the documentation ? ",work thanks raise add documentation,issue,negative,positive,positive,positive,positive,positive
1823976619,Took me a while to figure out that these warnings are unrelated to my code during an Upgrade to Ray 2.8. Super annoying.,took figure unrelated code upgrade ray super annoying,issue,negative,negative,negative,negative,negative,negative
1823900404,"Who is opening the thread? I assume it is generic pthread code?

A similar issue was [reported in numpy](https://github.com/numpy/numpy/issues/11551#issuecomment-405380887) a few years ago, and points to a shortcoming with Apple's default stack size for pthread_createed threads, which is [512 KB](https://developer.apple.com/library/archive/qa/qa1419/_index.html)). Since then, OpenBLAS requires less stack memory, but 512k is way too small still. The link has some sample code for pthreads to set the stack size to something more reasonable.",opening thread assume generic code similar issue ago shortcoming apple default stack size since le stack memory way small still link sample code set stack size something reasonable,issue,negative,negative,neutral,neutral,negative,negative
1823875224,"> Possible it's a duplicate of #41267

does seem to be the case",possible duplicate seem case,issue,negative,neutral,neutral,neutral,neutral,neutral
1823871405,"@mattip we found that if core_worker.cc io_service thread (a c++ thread, not the main python thread) runs a python function (through cython) that imports numpy for the first time, sigbus will happen. 

This can be mitigated by setting `OPENBLAS_NUM_THREADS=1` or importing numpy in the main thread first.

Do you have any ideas and whether it's a known issue with numpy?",found thread thread main python thread python function first time happen setting main thread first whether known issue,issue,negative,positive,positive,positive,positive,positive
1823859640,"Happy to see this merged, thanks for pushing it forward @shrekris-anyscale ",happy see thanks pushing forward,issue,positive,positive,positive,positive,positive,positive
1823858109,@rkooo567 can you please document the change in performance on the Serve streaming microbenchmarks on this PR for posterity?,please document change performance serve streaming posterity,issue,negative,neutral,neutral,neutral,neutral,neutral
1823844182,"@justinvyu I don't see this on local builds:

![image](https://github.com/ray-project/ray/assets/14017872/f093a9ef-f56d-4d02-b517-3934e5d13596)

I'm going to leave this one unaddressed, as I don't think it's a blocker - maybe it's something with RTD injecting styles?

**EDIT**: On closer inspection, it looks like nothing is wrong with the pygments highlighting - it's just that only the lines that start with `-` have CSS styles defined for them. I can update to match github's themes if you'd like.",see local image going leave one unaddressed think blocker maybe something edit closer inspection like nothing wrong start defined update match like,issue,positive,negative,negative,negative,negative,negative
1823756226,"> Can you provide a repro script please?

This issue occurred in the production environment, and we have attempted to reproduce it but were unsuccessful.",provide script please issue production environment reproduce unsuccessful,issue,negative,neutral,neutral,neutral,neutral,neutral
1823728020,Note this PR does not work because we default to set the delay time to be 0ms. Previously we use internal heart beat so we at least wait 1s and now we immediately retry. This causes the retry counts be eaten multiple times for a same actor death.,note work default set delay time previously use internal heart beat least wait immediately retry retry eaten multiple time actor death,issue,negative,negative,negative,negative,negative,negative
1823721503,"Hey @matthewdeng, thanks for the detailed response! This is basically the solution/workaround I've arrived at thus far, but it feels a little fragile, relying on the assumption that there will never be two checkpoints in a run for example. 

I *think* the relevant bit of code that generates the directory name (though I may be way off) is [here](https://github.com/ray-project/ray/blob/28fdcb6f6b5141a953b5d3a02167d12996f66bfb/python/ray/tune/experiment/trial.py#L176), and there's some related functionality to have a [custom directory name](https://github.com/ray-project/ray/blob/28fdcb6f6b5141a953b5d3a02167d12996f66bfb/python/ray/tune/experiment/trial.py#L923) that's not exposed in a user-facing API. I'm not sure how easy it would be to make this override-able with something from the user.

If that's not an option, is it possible that this could be done with a callback? Maybe I could have a callback in my Lightning trainer that, at the end of training, renames the checkpoint directory, which would get synced by Ray? Or I could modify the Ray checkpoint callback? It's not the most elegant thing but might work. ",hey thanks detailed response basically thus far little fragile assumption never two run example think relevant bit code directory name though may way related functionality custom directory name exposed sure easy would make something user option possible could done maybe could lightning trainer end training directory would get ray could modify ray elegant thing might work,issue,positive,positive,positive,positive,positive,positive
1823705383,"Basically we cannot run `import numpy` from the core_worker.cc io_service thread, otherwise it will sigbus",basically run import thread otherwise,issue,negative,neutral,neutral,neutral,neutral,neutral
1823699614,I think we have to do lazy registration with other fixes. It makes no sense we import any module that's probably not used (and this kind of issue is the culprit that it is a bad behavior) ,think lazy registration sense import module probably used kind issue culprit bad behavior,issue,negative,negative,negative,negative,negative,negative
1823683995,Another simple fix is just `import pyarrow` in the main thread.,another simple fix import main thread,issue,negative,positive,neutral,neutral,positive,positive
1823676258,"In general, if there is very low usage, very low doc views and/or the project seems abandoned, let's just remove it without deprecation. If you are unsure just use your best judgement :)",general low usage low doc project abandoned let remove without deprecation unsure use best,issue,negative,positive,positive,positive,positive,positive
1823671896,"For things like dragonfly which are not even really maintained upstream any more https://github.com/dragonfly/dragonfly (last substantial development was in 2020), we can just remove it without any deprecation.",like dragonfly even really upstream last substantial development remove without deprecation,issue,positive,positive,neutral,neutral,positive,positive
1823664656,"@pcmoritz sure thing, can I get your thoughts on what the deprecation policy should be for these types of removals?",sure thing get deprecation policy,issue,negative,positive,positive,positive,positive,positive
1823650183,"As a solution for the current implementation, would it suffice to use `RunConfig.name` to define the parent folder of your experiment?

```python
import tempfile
import ray.train
from ray.train.torch import TorchTrainer
from ray.train import Checkpoint, ScalingConfig, RunConfig


def train_func():
    for i in range(5):
        temp_dir = tempfile.mkdtemp()
        with open(f""{temp_dir}/{i}"", 'w') as fp: 
            pass
        ray.train.report({""i"": i}, checkpoint = Checkpoint.from_directory(temp_dir))
    pass

run_config = RunConfig(storage_path=""/tmp/storage"",
                       name=""run_123"",
                       )

trainer = TorchTrainer(train_func, 
                       run_config=run_config,
                       scaling_config=ScalingConfig(num_workers=1))

trainer.fit()
```

This will produce outputs in the following structure:
```
tree /tmp/storage/run_123 
```
```
/tmp/storage/run_123
├── TorchTrainer_90734_00000_0_2023-11-22_15-33-39
│   ├── checkpoint_000000
│   │   └── 0
│   ├── checkpoint_000001
│   │   └── 1
│   ├── checkpoint_000002
│   │   └── 2
│   ├── checkpoint_000003
│   │   └── 3
│   ├── checkpoint_000004
│   │   └── 4
│   ├── events.out.tfevents.1700696021.matt-QM624Q9WT0.local.meter
│   ├── params.json
│   ├── params.pkl
│   ├── progress.csv
│   └── result.json
├── basic-variant-state-2023-11-22_15-33-39.json
├── experiment_state-2023-11-22_15-33-39.json
├── trainer.pkl
└── tuner.pkl
```

Note that this will still create a nested `TorchTrainer_90734_00000_0_2023-11-22_15-33-39` directory, but if you're using Ray Train you can assume that there will be one folder, so you can do something like:
1. Define your `storage_path` and `name` (e.g. `/tmp/storage`, `run_123`) for your Train job.
2. Pass these values to your test job.
3. In your test job, traverse this directory to find the single  `TorchTrainer_{digits}` folder.",solution current implementation would suffice use define parent folder experiment python import import import import range open pas pas trainer produce following structure tree note still create directory ray train assume one folder something like define name train job pas test job test job traverse directory find single folder,issue,positive,negative,neutral,neutral,negative,negative
1823642146,@matthewdeng Can you make a follow up PR that removes the `dragonfly-opt` package from the dependencies (and any extra code / tests we have)?,make follow package extra code,issue,negative,neutral,neutral,neutral,neutral,neutral
1823620053,@can-anyscale [[jailed]serve_handle_wide_ensemble.aws (None)](https://buildkite.com/ray-project/release/builds/2080#018bf94a-1144-4c00-ba4c-05be25761a40) Ran the release test and seeing it succeeding now,none ran release test seeing succeeding,issue,negative,neutral,neutral,neutral,neutral,neutral
1823600286,"Thanks, @archit, for investigating the failure. I've accepted your suggestion to kick off a new build.",thanks investigating failure accepted suggestion kick new build,issue,negative,positive,neutral,neutral,positive,positive
1823596467,"@daturkel i have literally the exact same use case, and am running into this same issue. ",literally exact use case running issue,issue,negative,positive,positive,positive,positive,positive
1823581622,"@angelinalg doc build fails with 

> /home/docs/checkouts/readthedocs.org/user_builds/anyscale-ray/checkouts/41342/doc/source/cluster/running-applications/job-submission/quickstart.rst:312: WARNING: Inline interpreted text or phrase reference start-string without end-string.

Any ideas?",doc build warning text phrase reference without,issue,negative,neutral,neutral,neutral,neutral,neutral
1823577594,"Of course, I think the real fix is for conda defaults to fix themselves. Everything else is just a patch.",course think real fix fix everything else patch,issue,negative,positive,positive,positive,positive,positive
1823557151,"@mattip For the windows server with lots of cores I got this when I did a `ray.init(num_cpus=2)` on python `3.10.11`

```python
2023-11-22 16:47:09,371 INFO worker.py:1673 -- Started a local Ray instance.
RayContext(dashboard_url='', python_version='3.10.11', ray_version='2.8.0', ray_commit='105355bd253d6538ed34d331f6a4bdf0e38ace3a', protocol_version=None)
>>> [33m(raylet)[0m [2023-11-22 16:47:12,284 E 68664 94064] (raylet.exe) agent_manager.cc:70: The raylet exited immediately because one Ray agent failed, agent_name = runtime_env_agent.
[33m(raylet)[0m The raylet fate shares with the agent. This can happen because
[33m(raylet)[0m - The version of `grpcio` doesn't follow Ray's requirement. Agent can segfault with the incorrect `grpcio` version. Check the grpcio version `pip freeze | grep grpcio`.
[33m(raylet)[0m - The agent failed to start because of unexpected error or port conflict. Read the log `cat /tmp/ray/session_latest/logs/{dashboard_agent|runtime_env_agent}.log`. You can find the log file structure here https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure.
[33m(raylet)[0m - The agent is killed by the OS (e.g., out of memory).
[33m(raylet)[0m *** SIGTERM received at time=1700689632 ***
[33m(raylet)[0m     @   00007FF6A41785C6  (unknown)  (unknown)
[33m(raylet)[0m     @   00007FF6A418EE86  (unknown)  (unknown)
[33m(raylet)[0m     @   00007FF6A418E5BE  (unknown)  (unknown)
[33m(raylet)[0m     @   00007FF8C242268A  (unknown)  o_exp
[33m(raylet)[0m     @   00007FF8C2797AC4  (unknown)  BaseThreadInitThunk
[33m(raylet)[0m     @   00007FF8C551A351  (unknown)  RtlUserThreadStart
[33m(raylet)[0m [2023-11-22 16:47:12,303 E 68664 94064] (raylet.exe) logging.cc:361: *** SIGTERM received at time=1700689632 ***
[33m(raylet)[0m [2023-11-22 16:47:12,303 E 68664 94064] (raylet.exe) logging.cc:361:     @   00007FF6A41785C6  (unknown)  (unknown)
[33m(raylet)[0m [2023-11-22 16:47:12,303 E 68664 94064] (raylet.exe) logging.cc:361:     @   00007FF6A418EE86  (unknown)  (unknown)
[33m(raylet)[0m [2023-11-22 16:47:12,303 E 68664 94064] (raylet.exe) logging.cc:361:     @   00007FF6A418E5BE  (unknown)  (unknown)
[33m(raylet)[0m [2023-11-22 16:47:12,303 E 68664 94064] (raylet.exe) logging.cc:361:     @   00007FF8C242268A  (unknown)  o_exp
[33m(raylet)[0m [2023-11-22 16:47:12,303 E 68664 94064] (raylet.exe) logging.cc:361:     @   00007FF8C2797AC4  (unknown)  BaseThreadInitThunk
[33m(raylet)[0m [2023-11-22 16:47:12,303 E 68664 94064] (raylet.exe) logging.cc:361:     @   00007FF8C551A351  (unknown)  RtlUserThreadStart
2023-11-22 16:47:26,277 WARNING worker.py:2074 -- The node with node id: 4fa93e7451f6a84361e9c2a042e02e8cfe57ae10e06bc9cf428a4ddf and address: 127.0.0.1 and node name: 127.0.0.1 has been marked dead because the detector has missed too many heartbeats from it. This can happen when a        (1) raylet crashes unexpectedly (OOM, preempted node, etc.)
        (2) raylet has lagging heartbeats due to slow network or busy workload.
```
Then I interrupted using keyboard and did a `ray.shutdown()` and tried initializing again and then I got

```python
2023-11-22 16:48:22,439 INFO worker.py:1673 -- Started a local Ray instance.
RayContext(dashboard_url='', python_version='3.10.11', ray_version='2.8.0', ray_commit='105355bd253d6538ed34d331f6a4bdf0e38ace3a', protocol_version=None)
```

Although it works fine on windows laptop with smaller number of cores.

For python `3.11` I can't even install ray on a windows based environment due to this issue https://github.com/ray-project/ray/issues/39727 https://github.com/ray-project/ray/issues/38300",server lot got python python local ray instance raylet raylet immediately one ray agent raylet raylet fate agent happen raylet version follow ray requirement agent incorrect version check version pip freeze raylet agent start unexpected error port conflict read log cat find log file structure raylet agent o memory raylet received raylet unknown unknown raylet unknown unknown raylet unknown unknown raylet unknown raylet unknown raylet unknown raylet received raylet unknown unknown raylet unknown unknown raylet unknown unknown raylet unknown raylet unknown raylet unknown warning node node id address node name marked dead detector many happen raylet unexpectedly node raylet lagging due slow network busy interrupted keyboard tried got python local ray instance although work fine smaller number python ca even install ray based environment due issue,issue,negative,negative,neutral,neutral,negative,negative
1823408833,Oh I think we can just point them to 2.8 docs... but I guess it would be missing this 🙉 ,oh think point guess would missing,issue,negative,negative,negative,negative,negative,negative
1823369679,"Oh ok I can close this if so. Some users still seemed to be migrating, do you think it makes sense to keep for 1 more release?",oh close still think sense keep release,issue,negative,neutral,neutral,neutral,neutral,neutral
1823276646,"Mac builds are failing b/c we're trying to upload too much to BK:

<img width=""1140"" alt=""Screenshot 2023-11-22 at 10 32 17 AM"" src=""https://github.com/ray-project/ray/assets/428277/e4dbecc3-976c-4aaa-b0db-2ff0a09ffe8f"">
",mac failing trying much,issue,negative,positive,positive,positive,positive,positive
1823241432,"> > In GKE today, each TPU pod slice is mapped to one node pool. So the node pool can only be resized from 0 to x to 0.
> 
> Sorry for the lack of clarity. I knew the atomicity for the multi-host TPU node pool from the [GKE doc](https://cloud.google.com/kubernetes-engine/docs/concepts/tpus#machine_type) referenced in [#39781 (comment)](https://github.com/ray-project/ray/issues/39781#issuecomment-1800328451). My question is: Who should create a new TPU node pool when KubeRay attempts to create new TPU Pods?

This can be done from GKE Node Auto-Provisioning. The user should not need to manually create the node pools.

> 
> > @ryanaoleary is working on a TPU webhook which can validate this.
> 
> Do you mean that validating that all Pods in a multi-host TPU node pool should be in the same RayCluster?

Validating that when a RayCluster reserves worker pods in a TPU pod slice, the number of workers should match the number of hosts in the slice. Also the pods should be co-scheduled with node affinity such that all workers get placed on the same slice. This should guarantee that all pods in the pod slice are assigned to the same Ray Cluster.",today pod slice one node pool node pool sorry lack clarity knew atomicity node pool doc comment question create new node pool create new done node user need manually create node working validate mean node pool worker pod slice number match number slice also node affinity get slice guarantee pod slice assigned ray cluster,issue,positive,negative,negative,negative,negative,negative
1823222144,"> In GKE today, each TPU pod slice is mapped to one node pool. So the node pool can only be resized from 0 to x to 0.

Sorry for the lack of clarity. I knew the atomicity for the multi-host TPU node pool from the [GKE doc](https://cloud.google.com/kubernetes-engine/docs/concepts/tpus#machine_type) referenced in https://github.com/ray-project/ray/issues/39781#issuecomment-1800328451. My question is: Who should create a new TPU node pool when KubeRay attempts to create new TPU Pods?

> @ryanaoleary is working on a TPU webhook which can validate this.

Do you mean that validating that all Pods in a multi-host TPU node pool should be in the same RayCluster?",today pod slice one node pool node pool sorry lack clarity knew atomicity node pool doc question create new node pool create new working validate mean node pool,issue,positive,negative,negative,negative,negative,negative
1823204405,"> It's a big change. Let's merge after the 2.9 branch cut?

Since branch cut is one week delayed, let's try to get this in 2.9. Thanks!",big change let merge branch cut since branch cut one week let try get thanks,issue,negative,positive,neutral,neutral,positive,positive
1823001957,"same here, ray is last library that is holding us on 3.11",ray last library holding u,issue,negative,neutral,neutral,neutral,neutral,neutral
1822924694,"PR in review: https://github.com/ray-project/ray/pull/41297

1) Link had been fixed already (in master)
2) ray/rllib-contrib repo has been erased to avoid confusion.
3) See 2
4) Good point, will include this in the above PR.
5) Good point, will include this in the above PR.
6) This has already been addressed.
7) Same (see 6)

Downgrading this issue to a P1.",review link fixed already master erased avoid see good point include good point include already see issue,issue,positive,positive,positive,positive,positive,positive
1822681345,"> Hey @gtarcoder, I don't think feature is currently on our roadmap. In the meantime, have you tried implementing a custom datasource?
> 
> https://github.com/ray-project/ray/blob/81c169bde2414fe4237f3d2f05fc76fccfd52dee/python/ray/data/datasource/datasource.py#L18-L19

Hi @bveeramani ，seems to support streaming data in ray like flink by a custom datasource is not easy. 
And here is the plan for streaming in ray https://github.com/ray-project/ray/issues/6184, seems no progress.
So, really hope ray can provide streaming feature like flink.",hey think feature currently tried custom hi support streaming data ray like custom easy plan streaming ray progress really hope ray provide streaming feature like,issue,positive,positive,positive,positive,positive,positive
1822207846,"In order to exclude fastapi factors, I use this test.py has the same problem.

```
import time
import ray
import uuid
import shlex
import json
import uvicorn
import logging
import subprocess

from subprocess import Popen

@ray.remote
def run_fop(entrypoint, submission_id, metadata):
    cmd = shlex.split(entrypoint)
    logging.info(f""run fop pass serve {cmd}"")
    with Popen(args=cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL) as p:
        ...

runtime_env = {
        ""working_dir"": ""https://git.[somethins]"",
        ""pip"": [""pydantic==2.3.0""],
        ""env_vars"": {
            ""RUN_ENV"": ""uat""
        }
}


metadata = {
        ""biz_metadata"": ""{\""flow\"":\""md5\"",\""fop\"":\""md5\"",\""smid\"":\""id\"",\""upos_uri\"":\""urixxxx\"",\""wrk_random\"":\""189728\""}"",
        ""submission_id"": ""test14""
 }

if __name__ == ""__main__"":
    s = ""sdfas12387461982734619823746198237461982374691283746918273649812763498127634987162349876213496128974619287649817263498126734986213846128976349817263498126394876129837461928374691283746918237469123784691823746918732649182736491823764918273649182736491823764198237649182376419283764192873649128736491873246198237461982374619827364198273469182375610237586123908476192387461928374691823469182736498123649123""
    for i in range(100000):
        a = run_fop.options(
            num_cpus=0.5,
            num_gpus=0,
            runtime_env=runtime_env
            ).remote(""python3 -c \""import logging; for i in range(100): logging.info(s)\"""", f""tt{i}"", metadata)
        print(a)
        # ray.get(a)
        time.sleep(0.05)
```

after about 1 hour:
 607501 root      20   0 14.421g 168.2m  47.8m S  67.7  0.3  59:37.83 /usr/local/bin/python3.10 /usr/local/bin/memray run --native -+

![image](https://github.com/ray-project/ray/assets/22726829/cd308fc9-2f80-4fbf-b04c-08f3e3a29836)
![image](https://github.com/ray-project/ray/assets/22726829/ec4ab8fa-ed4e-4fa6-b4c3-d2bf93c1c3ae)

file detail: [memray-flamegraph-test1.py.633207.html.zip](https://github.com/ray-project/ray/files/13434402/memray-flamegraph-test1.py.633207.html.zip)
",order exclude use problem import time import ray import import import import import logging import import run fop pas serve pip test range python import logging range print hour root run native image image file detail,issue,negative,neutral,neutral,neutral,neutral,neutral
1822203273,"@rkooo567 it's ready to merge, getting some weird build failures unrelated to the change",ready merge getting weird build unrelated change,issue,negative,negative,negative,negative,negative,negative
1821876992,"Hey @gtarcoder, I don't think feature is currently on our roadmap. In the meantime, have you tried implementing a custom datasource? 

https://github.com/ray-project/ray/blob/81c169bde2414fe4237f3d2f05fc76fccfd52dee/python/ray/data/datasource/datasource.py#L18-L19",hey think feature currently tried custom,issue,negative,neutral,neutral,neutral,neutral,neutral
1821841458,things can be much more simplified if you review and approve this: https://github.com/ray-project/rayci/pull/138,much simplified review approve,issue,negative,positive,positive,positive,positive,positive
1821841324,"> > In GKE today, each TPU pod slice is mapped to one node pool. So the node pool can only be resized from 0 to x to 0.
> 
> Yeah this piece is still unclear to me - we would want it to be such that we could resize from 0 to x to 2x to 3x, etc. back to 0. But I am not sure if the change should be within KubeRay (i.e. through KubeRay, explore if it's possible scale up multiple NodePools) or through GKE.

Yeah this is the reason for my proposal above - each worker group would map to one GKE node pool, so autoscaling from 0 to x to 2x would end up as creating new worker groups in Kuberay. As far as I know, a TPU node pool in GKE cannot be resized from x to 2x.",today pod slice one node pool node pool yeah piece still unclear would want could resize back sure change within explore possible scale multiple yeah reason proposal worker group would map one node pool would end new worker far know node pool,issue,positive,positive,positive,positive,positive,positive
1821827612,"Thanks justin
I tried to reproduce by issuing `num_samples=3` of the hyperparameters of the failing run, but they succeeded.",thanks tried reproduce issuing failing run,issue,negative,positive,positive,positive,positive,positive
1821823367,"> In GKE today, each TPU pod slice is mapped to one node pool. So the node pool can only be resized from 0 to x to 0.

Yeah this piece is still unclear to me - we would want it to be such that we could resize from 0 to x to 2x to 3x, etc. back to 0. But I am not sure if the change should be within KubeRay (i.e. through KubeRay, explore if it's possible scale up multiple NodePools) or through GKE.",today pod slice one node pool node pool yeah piece still unclear would want could resize back sure change within explore possible scale multiple,issue,negative,positive,positive,positive,positive,positive
1821803738," 
> * Who should scale up the node pools?

In GKE today, each TPU pod slice is mapped to one node pool. So the node pool can only be resized from 0 to x to 0.

> * How can we ensure that all TPU hosts in the PodSlice are assigned to the same RayCluster?

When the Ray Cluster is created, it should create X workers where X is the number of TPU hosts in a PodSlice. We also should add node affinity labels to ensure that all workers are co-scheduled on the same slice. @ryanaoleary is working on a TPU webhook which can validate this.",scale node today pod slice one node pool node pool ensure assigned ray cluster create number also add node affinity ensure slice working validate,issue,positive,neutral,neutral,neutral,neutral,neutral
1821801570,This is the long-running version of this test. The smoke test version of this works just fine.,version test smoke test version work fine,issue,negative,positive,positive,positive,positive,positive
1821769627,"@vitobellini Could you try using the `Tuner` API instead?

See below:
* https://docs.ray.io/en/latest/tune/getting-started.html#using-search-algorithms-in-tune
* https://docs.ray.io/en/latest/tune/api/suggestion.html",could try tuner instead see,issue,negative,neutral,neutral,neutral,neutral,neutral
1821768518,"@anyscalesam Hi, I tested the following in a Ray 2.8.0 cluster.

Here is a piece of code in my test cluster that reproduces the original report (raises `pyarrow.lib.ArrowInvalid: Invalid column index to set field.`):

```python
TRAIN_BASE_PATH = [
    ""gs://ray-ml-nonprod/pctr-poc/hive-debug-sample/key1=value1/key2=value2/15/000000000000.parquet.gz"",
    ""gs://ray-ml-nonprod/pctr-poc/hive-debug-sample/key1=value1/key2=value2/15/000000000001.parquet.gz""
]
train = (
    ray.data.read_parquet(TRAIN_BASE_PATH, columns=list(set(feature_dict.keys())))
    .to_tf(feature_columns=list(set(feature_dict.keys())), label_columns=""clicks"", batch_size=512)
)
for batch in train:
    pass
```

Here is me trying the workaround:

```python
TRAIN_BASE_PATH = [
    ""gs://ray-ml-nonprod/pctr-poc/hive-debug-sample/key1=value1/key2=value2/15/000000000000.parquet.gz"",
    ""gs://ray-ml-nonprod/pctr-poc/hive-debug-sample/key1=value1/key2=value2/15/000000000001.parquet.gz""
]
train = (
    ray.data.read_parquet(TRAIN_BASE_PATH, columns=list(set(feature_dict.keys())) + [""key1"", ""key2""])
    .to_tf(feature_columns=list(set(feature_dict.keys())) + [""key1"", ""key2""], label_columns=""clicks"", batch_size=512)
)
for batch in train:
    pass
```

```
File ~/anaconda3/lib/python3.10/site-packages/pyarrow/types.pxi:326, in pyarrow.lib.DataType.to_pandas_dtype()

File ~/anaconda3/lib/python3.10/site-packages/pyarrow/types.pxi:162, in pyarrow.lib._to_pandas_dtype()

NotImplementedError: dictionary<values=string, indices=int32, ordered=0>
```

[Here is a link to the full traceback](https://gist.github.com/ResidentMario/cca1cee3cfc5c0fc2858d7cdb179b00a). I can test the patched Ray 2.9.0 code path once that is released and we have upgraded our test cluster wrapper.",hi tested following ray cluster piece code test cluster original report invalid column index set python train set set batch train pas trying python train set key key set key key batch train pas file file dictionary link full test ray code path test cluster wrapper,issue,negative,positive,positive,positive,positive,positive
1821761815,"Closing this issue, since the change has already been made, but feel free to keep posting questions here!",issue since change already made feel free keep posting,issue,positive,positive,positive,positive,positive,positive
1821756023,"Nice eagle eyes, @architkulkarni. Thanks for the quick response, @peytondmurray!",nice eagle thanks quick response,issue,positive,positive,positive,positive,positive,positive
1821755847,We should remove the other conda version and just use one for all. Probably too risky for 2.9 but let's do it for 2.10.,remove version use one probably risky let,issue,negative,neutral,neutral,neutral,neutral,neutral
1821750518,Yes. We have a release test that runs the workload every night and it is working fine. Not sure what the issue is tho. ,yes release test every night working fine sure issue tho,issue,positive,positive,positive,positive,positive,positive
1821727361,"For now, you should work around this with `tune_config=tune.TuneConfig(reuse_actors=False)`. It would help a lot if anyone could provide a minimal reproduction here!",work around would help lot anyone could provide minimal reproduction,issue,negative,negative,neutral,neutral,negative,negative
1821698629,"Also finding this issue on batch of runs, version 2.8.0.
Causes about 1/5 runs to error which is a lot!",also finding issue batch version error lot,issue,negative,neutral,neutral,neutral,neutral,neutral
1821679800,@anyscalesam @kouroshHakha  I can't reproduce this issue anymore as I don't have access to servers.  are you guys able to run on your end without any issue?,ca reproduce issue access able run end without issue,issue,negative,positive,positive,positive,positive,positive
1821669762,Reviewed with @c21 this we think should keep as a ray release-blocker; we should unjail this; and make it pass before we release ray29,think keep ray make pas release ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1821661806,"Reviewed - next step here Andrew please investigate into the prior flakiness and determine if it's bad test, bad code, or bad infra.",next step please investigate prior flakiness determine bad test bad code bad infra,issue,negative,negative,negative,negative,negative,negative
1821661760,@Zandew - can you help take a look if anything abnormal for the test recently? Looks like become stable now.,help take look anything abnormal test recently like become stable,issue,positive,neutral,neutral,neutral,neutral,neutral
1821606243,@sven1977 can you review and determine whether this is really p0 (in which case we should address asap),review determine whether really case address,issue,negative,positive,positive,positive,positive,positive
1821603597,@veryhannibal thank you for submitting the pr; @stephanie-wang left some comments can you advise on them in the PR so we can than merge and close this ticket?,thank left advise merge close ticket,issue,negative,neutral,neutral,neutral,neutral,neutral
1821602042,@c21 please triage. Discussed; Cheng to follow up with mtm here.,please triage cheng follow,issue,negative,neutral,neutral,neutral,neutral,neutral
1821600108,@bussrakrkmz can you please attach your full log output so we can look into this further?,please attach full log output look,issue,negative,positive,positive,positive,positive,positive
1821580825,"This is fixed by #41244. Will the test automatically be unjailed, or do we need to change something?",fixed test automatically unjailed need change something,issue,negative,positive,neutral,neutral,positive,positive
1821468732,"> I kicked off a `long_running_serve.aws` run on https://github.com/ray-project/ray/pull/41237 just before the pydantic version was updated: [Buildkite link](https://buildkite.com/ray-project/release/builds/1578).

The run failed with the same error as this PR. It's unlikely that this PR is the root cause. There are no remaining test failures connected to this PR.

@architkulkarni This PR is ready to merge.",run version link run error unlikely root cause test connected ready merge,issue,negative,negative,negative,negative,negative,negative
1821450744,"Hey @aloysius-lim, were you able to get a `adlfs` custom filesystem to work? Were you encountering any pickling/serialization issues like this issue: https://github.com/ray-project/ray/issues/41125",hey able get custom work like issue,issue,negative,positive,positive,positive,positive,positive
1821369680,"LinkCheck is failing with:

```

-rate limited-   https://towardsdatascience.com/modern-parallel-and-distributed-python-a-quick-tutorial-on-ray-99f8d70369b8 \| sleeping...
--
  | -rate limited-   https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205 \| sleeping...
  | -rate limited-   https://medium.com/distributed-computing-with-ray/machine-learning-serving-is-broken-f59aff2d607f \| sleeping...
  | (ray-core/examples/map_reduce: line 180002) broken    https://medium.com/distributed-computing-with-ray/executing-adistributed-shuffle-without-a-mapreduce-system-d5856379426c - 429 Client Error: Too Many Requests for url: https://medium.com/distributed-computing-with-ray/executing-adistributed-shuffle-without-a-mapreduce-system-d5856379426c


```",failing sleeping sleeping sleeping line broken client error many,issue,negative,positive,neutral,neutral,positive,positive
1821346396,"![Screenshot 2023-11-21 at 12-21-22 FL Research Presentation](https://github.com/ray-project/ray/assets/72274532/8b3f5bcd-e53f-4331-9abd-e04596ce1471)

This is the screenshot of the printout here. Nothing in the logs either. @jjyao ",research presentation nothing either,issue,negative,neutral,neutral,neutral,neutral,neutral
1821094634,@alexeykudinkin the PR description is equivalent to the previous fix I remember. Did you just do refactoring + add tests or added additional functionalities here? ,description equivalent previous fix remember add added additional,issue,negative,negative,negative,negative,negative,negative
1820837898,Hi @bveeramani @anyscalesam @ericl any suggestion to this feature? is there a schedule to support this?,hi suggestion feature schedule support,issue,negative,neutral,neutral,neutral,neutral,neutral
1820679740,"Maybe related https://github.com/nghttp2/nghttp2/issues/148

https://github.com/ray-project/ray/issues/11285

Looks like its used on mac at least

",maybe related like used mac least,issue,negative,negative,negative,negative,negative,negative
1820504535,"Seems the startup script doesnt matter even if I write:
```
ray start --head --port=6373 --dashboard-host 0.0.0.0
```

Nightly build as of 11/21 also fails",script doesnt matter even write ray start head nightly build also,issue,negative,neutral,neutral,neutral,neutral,neutral
1820448400,Yeah let's get approval and merge tomorrow.,yeah let get approval merge tomorrow,issue,positive,neutral,neutral,neutral,neutral,neutral
1820379328,"I also have the same problem with the ray 2.8.0 version. And these three time points that memory is growing likely to cause this problem?
![image](https://github.com/ray-project/ray/assets/22726829/3173b654-e4d2-4af1-b25c-ae37ae193ee4)
![image](https://github.com/ray-project/ray/assets/22726829/996d4ab9-e6c6-45b8-8288-99d791d58aeb)
![image](https://github.com/ray-project/ray/assets/22726829/1b7dd632-0605-4e51-ac13-db55807d0791)

",also problem ray version three time memory growing likely cause problem image image image,issue,negative,neutral,neutral,neutral,neutral,neutral
1820327662,"This generally happens if I run:

```
PROXY_HEALTH_CHECK_TIMEOUT_S=100 RAY_SERVE_DEBUG_MODE=1 RAY_SERVE_PROXY_READY_CHECK_TIMEOUT_S=100 RAY_ROTATION_MAX_BYTES=52428800 RAY_RUNTIME_ENV_LOG_TO_DRIVER_ENABLED=1 RAY_ENABLE_RECORD_ACTOR_TASK_LOGGING=1 ray start --head --port=6373   --ray-debugger-external --dashboard-host 0.0.0.0
```
and do any execution in the ray server

```
Exception in thread ray_print_logs:
Traceback (most recent call last):
  File ""/Users/andrewgrosser/.pyenv/versions/3.10.12/lib/python3.10/threading.py"", line 1016, in _bootstrap_inner
    self.run()
  File ""/Users/andrewgrosser/.pyenv/versions/3.10.12/lib/python3.10/threading.py"", line 953, in run
    self._target(*self._args, **self._kwargs)
  File ""/Users/andrewgrosser/Library/Caches/pypoetry/virtualenvs/ml-Yqfv2jYI-py3.10/lib/python3.10/site-packages/ray/_private/worker.py"", line 804, in print_logs
    data = subscriber.poll()
  File ""python/ray/_raylet.pyx"", line 2840, in ray._raylet.GcsLogSubscriber.poll
  File ""python/ray/_raylet.pyx"", line 457, in ray._raylet.check_status
ray.exceptions.RaySystemError: System error: Missing :authority header
```
",generally run ray start head execution ray server exception thread recent call last file line file line run file line data file line file line system error missing authority header,issue,negative,negative,neutral,neutral,negative,negative
1820309845,"The 2 systems are cloud Instances with all ports open (Since No information was available on what all ports needs to be exposed for ray to commuicate). Head starts are works as expected. Workers sometimes gets connected, sometimes it doesn't.",cloud open since information available need exposed ray head work sometimes connected sometimes,issue,negative,positive,positive,positive,positive,positive
1820308081,"`
cluster_name: default

docker:
    image: rayproject/ray:2.8.0-py311-gpu    
    # head_image: str
    worker_image: rayproject/ray:2.8.0-py311
    container_name: ""ray_container""
    pull_before_run: True
    run_options:   # Extra options to pass into ""docker run""
        - --ulimit nofile=65536:65536

provider:
    type: local
    head_ip: 164.52.204.242
    worker_ips: [216.48.179.215]
auth:
    ssh_user: user
    ssh_private_key: ~/.ssh/id_rsa
    
upscaling_speed: 1.0
idle_timeout_minutes: 30

file_mounts: {
""/app/requirements.txt"": ""/Users/ajaichemmanam/Downloads/ray/requirements.txt"",
""~/.ssh/id_rsa"": ""/Users/ajaichemmanam/.ssh/id_rsa"",
#some more file mounts
}

cluster_synced_files: []

file_mounts_sync_continuously: False

rsync_exclude:
    - ""**/.git""
    - ""**/.git/**""

rsync_filter:
    - "".gitignore""

initialization_commands: []

setup_commands:
    - sudo apt-get update
    - sudo apt-get install gcc ffmpeg libsm6 libxext6  -y
    - pip install -r ""/app/requirements.txt""

head_setup_commands: []

worker_setup_commands: 
    - ""echo setup_command was run >> /tmp/ray_worker_output.txt""

head_start_ray_commands:
    - ray stop
    - ulimit -c unlimited && export RAY_health_check_timeout_ms=30000 && ray start --head --node-ip-address=164.52.204.242 --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml --dashboard-host=0.0.0.0 --disable-usage-stats --log-color=auto -v 
 
worker_start_ray_commands:
    - ray stop
    - ray start --address=$RAY_HEAD_IP:8379
`",default docker image true extra pas docker run provider type local user file false update install pip install echo run ray stop unlimited export ray start head ray stop ray start,issue,negative,negative,neutral,neutral,negative,negative
1820300944,"I still do not quite understand how the dockerhub part works.

also do we always need to push to gcr?",still quite understand part work also always need push,issue,negative,neutral,neutral,neutral,neutral,neutral
1820298854,"> @aslonnie resolve conflict, this `--upload` flag is defined on the container base class so it works for both ray_docker_container (which uploads to dockerhub) and anyscale_docker_container (which uploads to ecr)

for uploading to dockerhub, what is the `ecr` being set?",resolve conflict flag defined container base class work set,issue,negative,negative,negative,negative,negative,negative
1820207499,"> Thanks for the replies @JingChen23 !
> 
> Any comments about this?
> 
> > Isn't it equivalent to just generate a cluster YAML with a list of N worker node types whose CPU requirements are 2^i?
> 
> Maybe it's not equivalent, I just want to make sure I understand the behavior correctly.

Hi Archit, thanks for asking the question.

I have updated the description part, with some content which can answer this question:

```
The DRA stategy is various, for example:

We can strictly fit each in-the-waiting-list Ray task's requirments.
We can enlarge the VM size to fulfill the lagest guy in the waiting list.
We can leave more margin above the requirement of the largest guy in the waiting list.
In this change we just implement the 3rd one.
```

Thus, back to your question. For the 3rd one, your idea is an equivalent, as long as we enumerate all the combinations of the GPU/memory value (power of 2) pairs, and autogenerate the yaml file before launching the Ray cluster.

For the 1st and 2nd strategies, there could be too many enumeration cases so the generated yaml will be very long.

Also, another problem is if we generate such a long list of worker nodes, the scaling range for each of them is a little tricky to define.",thanks equivalent generate cluster list worker node whose maybe equivalent want make sure understand behavior correctly hi thanks question description part content answer question various example strictly fit ray task enlarge size fulfill guy waiting list leave margin requirement guy waiting list change implement one thus back question one idea equivalent long enumerate value power file ray cluster st could many enumeration long also another problem generate long list worker scaling range little tricky define,issue,positive,positive,positive,positive,positive,positive
1820146704,"> @wingkitlee0 okay, sounds good. Could we `pytest.skip` the test when the PyArrow version is 6, and add a note explaining why we're skipping?

@bveeramani Done. Thanks",good could test version add note explaining skipping done thanks,issue,positive,positive,positive,positive,positive,positive
1820135407,"@jjyao I have tried some ways which is `memray run fastapi.py` and `memray run --native fastapi.py`. As you said, I checked the memory of the process through the top command and it is indeed continuing to grow.",tried way run run native said checked memory process top command indeed grow,issue,negative,positive,positive,positive,positive,positive
1820108717,"The script passed: 

```
[...]
Counter({('ip-10-0-218-191', 'ip-10-0-228-193'): 8, ('ip-10-0-225-188', 'ip-10-0-225-188'): 8, ('ip-10-0-225-188', 'ip-10-0-218-191'): 8, ('ip-10-0-218-191', 'ip-10-0-218-191'): 8, ('ip-10-0-225-188', 'ip-10-0-215-149'): 8, ('ip-10-0-218-191', 'ip-10-0-225-188'): 7, ('ip-10-0-225-188', 'ip-10-0-228-193'): 7, ('ip-10-0-215-149', 'ip-10-0-218-191'): 7, ('ip-10-0-228-193', 'ip-10-0-225-188'): 7, ('ip-10-0-215-149', 'ip-10-0-215-149'): 6, ('ip-10-0-215-149', 'ip-10-0-228-193'): 6, ('ip-10-0-218-191', 'ip-10-0-215-149'): 6, ('ip-10-0-215-149', 'ip-10-0-225-188'): 5, ('ip-10-0-228-193', 'ip-10-0-215-149'): 4, ('ip-10-0-228-193', 'ip-10-0-228-193'): 3, ('ip-10-0-228-193', 'ip-10-0-218-191'): 2})
Success!
Shared connection to 44.229.14.251 closed.
```",script counter success connection closed,issue,positive,positive,neutral,neutral,positive,positive
1820055786,"This is interesting, as I thought about a related issue/enhancement when working a recent PR #40778 about groupby. 
Use case: When using `groupby` in a partitioned dataset, it's often that group-key is bounded by the partitions (e.g., a time column in a dataset partitioned by date). Since Ray's `groupby.map_groups` sorts dataset by the group key(s), the ability of specifying the group boundaries is helpful.",interesting thought related working recent use case partitioned often bounded time column partitioned date since ray group key ability group helpful,issue,positive,positive,positive,positive,positive,positive
1820043639,The `long_running_serve.aws` smoke test ran successfully: [Buildkite link](https://buildkite.com/ray-project/release/builds/1775#018bef0f-30bc-4080-ae6d-a7ebeae2a235),smoke test ran successfully link,issue,negative,positive,positive,positive,positive,positive
1820033869,"@ajaichemmanam we haven't been able to reproduce this issue unfortunately.  Let us know if there is a minimal configuration that works for you, and we can try to narrow down what's causing the issue.",able reproduce issue unfortunately let u know minimal configuration work try narrow causing issue,issue,negative,negative,neutral,neutral,negative,negative
1820031695,"> To clarify, a TPU node pool can only schedule a single TPU pod (see [ref](https://cloud.google.com/kubernetes-engine/docs/concepts/tpus#machine_type)) so we can't get our autoscaling from the node pool. That's why @richardsliu suggested the approach in https://github.com/ray-project/ray/issues/39781#issuecomment-1736479133 e.g. scale by nodepools rather than nodes within a nodepool.

Thanks, @allenwang28! I lost the message when I attended KubeCon. Thank @richardsliu for the reminder. I have several questions:

* Who should scale up the node pools?
* How can we ensure that all TPU hosts in the PodSlice are assigned to the same RayCluster?
",clarify node pool schedule single pod see ref ca get node pool approach scale rather within thanks lost message thank reminder several scale node ensure assigned,issue,positive,positive,neutral,neutral,positive,positive
1820017842,"nit: can we add a line here https://github.com/ray-project/ray/blob/master/ci/pipeline/determine_tests_to_run.py#L276 something like:

```
elif changed_file == "".readthedocs.yaml"":
  RAY_CI_DOC_AFFECTED = 1
```

It will help to get CI results quicker. The entire test suite is run on this change despite this is only a doc change. ",nit add line something like help get entire test suite run change despite doc change,issue,positive,neutral,neutral,neutral,neutral,neutral
1819996425,"@aslonnie resolve conflict, this `--upload` flag is defined on the container base class so it works for both ray_docker_container (which uploads to dockerhub) and anyscale_docker_container (which uploads to ecr)",resolve conflict flag defined container base class work,issue,negative,negative,negative,negative,negative,negative
1819992252,"yes, not too bad, feel free to move it back to passing; but I'll open it again if they keep failing on pre/postmerge",yes bad feel free move back passing open keep failing,issue,negative,negative,neutral,neutral,negative,negative
1819968728,Ah thanks. It looks like there is a real memory regression in Data but seems it was also failing in 2.8. I think we can ignore for now and address as p0 for 2.10.,ah thanks like real memory regression data also failing think ignore address,issue,negative,positive,positive,positive,positive,positive
1819954008,"Thanks @karrdy89 for the repro - and the docker image! 

A couple of questions:
1. Looks like there was some tasks/actors running
```
xflow@f2daf60a3632:~/app$ ray list tasks

======== List: 2023-11-21 08:11:43.041798 ========
Stats:
------------------------------
Total: 3

Table:
------------------------------
    TASK_ID                                             ATTEMPT_NUMBER  NAME                                       STATE       JOB_ID  ACTOR_ID                          TYPE                 FUNC_OR_CLASS_NAME        PARENT_TASK_ID                                    NODE_ID                                                   WORKER_ID                                                 ERROR_TYPE
 0  c2668a65bda616c19ab3b7bbf7b302d0734947e801000000                 0  UvicornServer.run_server                   RUNNING   01000000  9ab3b7bbf7b302d0734947e801000000  ACTOR_TASK           UvicornServer.run_server  ffffffffffffffffffffffffffffffffffffffff01000000  ec683b35ddbb1d5a813335e4330184069538a69672c4ab3038c6d0c1  0553fdaa8a079cd92304405e1e8ab394e1d14851f4e1c3e558bcd8c9
 1  ffffffffffffffff9ab3b7bbf7b302d0734947e801000000                 0  ASGI:UvicornServer.__init__                FINISHED  01000000  9ab3b7bbf7b302d0734947e801000000  ACTOR_CREATION_TASK  UvicornServer.__init__    ffffffffffffffffffffffffffffffffffffffff01000000
 2  fffffffffffffffff326d82629dbde7e359248d301000000                 0  pipeline_manager:PipelineManager.__init__  FINISHED  01000000  f326d82629dbde7e359248d301000000  ACTOR_CREATION_TASK  PipelineManager.__init__  ffffffffffffffffffffffffffffffffffffffff01000000

xflow@f2daf60a3632:~/app$ ray list actors

======== List: 2023-11-21 08:11:46.768848 ========
Stats:
------------------------------
Total: 2

Table:
------------------------------
    ACTOR_ID                          CLASS_NAME       STATE      JOB_ID  NAME              NODE_ID                                                     PID  RAY_NAMESPACE
 0  9ab3b7bbf7b302d0734947e801000000  UvicornServer    ALIVE    01000000  ASGI              ec683b35ddbb1d5a813335e4330184069538a69672c4ab3038c6d0c1    859  f49818be-8fb5-4a1b-8340-e8a8853a6e6f
 1  f326d82629dbde7e359248d301000000  PipelineManager  ALIVE    01000000  pipeline_manager  ec683b35ddbb1d5a813335e4330184069538a69672c4ab3038c6d0c1    861  f49818be-8fb5-4a1b-8340-e8a8853a6e6f
```

Do you know what these tasks/actors doing? 


And if they were also running in your repro - this sounds like it's not ""no activity"" strictly speaking then? ",thanks docker image couple like running ray list list total table name state type running finished finished ray list list total table state name alive alive know also running like activity strictly speaking,issue,positive,positive,neutral,neutral,positive,positive
1819941896,What's the `memray` command you ran? Did you monitor the memory of the driver process?,command ran monitor memory driver process,issue,negative,neutral,neutral,neutral,neutral,neutral
1819915386,@marco-aws you need to set the the right certificate I think. Try setting `RAY_REDIS_CA_CERT=/etc/ssl/certs/ca-certificates.crt` and see if it works.,need set right certificate think try setting see work,issue,negative,positive,positive,positive,positive,positive
1819910539,"Encountered the same problem, ModuleNotFound: ray.serve.generated. I checked the main branch, in setup-dev.py, serve will link. Solved by `./python/ray/setup-dev.py -y --skip serve`",problem checked main branch serve link skip serve,issue,negative,positive,positive,positive,positive,positive
1819909717,Normal ray usage shouldn't import tests. We will fix it in 2.9 release.,normal ray usage import fix release,issue,negative,positive,positive,positive,positive,positive
1819904930,this looks pretty green in last 2w moving @can-anyscale - shall we downgrade to p1 but keep unjailed?,pretty green last moving shall downgrade keep unjailed,issue,negative,positive,neutral,neutral,positive,positive
1819904072,This one is pretty bad last 2w - we need to root cause into the test failure or decide if this test isn't important.,one pretty bad last need root cause test failure decide test important,issue,negative,negative,neutral,neutral,negative,negative
1819903498,reviewing this one - this one is more yellow and i think we should keep as p1 and have someone look into it; we should at least root cause into the flakiness.,one one yellow think keep someone look least root cause flakiness,issue,negative,negative,negative,negative,negative,negative
1819902484,"@can-anyscale can we close this actually - I'm looking at 

![Image](https://github.com/ray-project/ray/assets/116198444/d9213732-878f-459a-815d-9d349ee24a00)

and it actually isn't that bad.

key decision point on this next is do we p1 and jail it or p1 and keep running it.

@rickyyx @jjyao @rkooo567 to weigh in here.",close actually looking image actually bad key decision point next jail keep running weigh,issue,negative,negative,negative,negative,negative,negative
1819888086,no end impact - but just inconsistency in the code. ,end impact inconsistency code,issue,negative,neutral,neutral,neutral,neutral,neutral
1819805213,"Data and GPU premerge tests are failing, but I don't think it can be related to this PR.  The only dependency changes are minor version updates to packages that look unrelated.",data failing think related dependency minor version look unrelated,issue,negative,negative,neutral,neutral,negative,negative
1819635106,"I kicked off a `long_running_serve.aws` run on [the commit](https://github.com/ray-project/ray/pull/41237) just before the pydantic version was updated: [Buildkite link](https://buildkite.com/ray-project/release/builds/1578).

<img width=""1220"" alt=""Screen Shot 2023-11-20 at 11 00 06 AM"" src=""https://github.com/ray-project/ray/assets/92341594/e34a963c-480d-4864-9765-4c4e605b8cee"">",run commit version link screen shot,issue,negative,neutral,neutral,neutral,neutral,neutral
1819623917,"> hey @angelinalg could you review this?

Friendly ping, anything I can do to help this land?",hey could review friendly ping anything help land,issue,positive,positive,positive,positive,positive,positive
1819617321,"There seems to be a few cases where autoscaler errors occur. Especially when using kube-ray. We are tracking those items here:
https://github.com/ray-project/ray/issues/41072
https://github.com/ray-project/ray/issues/40911

Both of those issues will result in the symptom we see in this ticket. A blank dashboard UI.",occur especially result symptom see ticket blank dashboard,issue,negative,neutral,neutral,neutral,neutral,neutral
1819585031,"- [x] Some inconsistencies on the main landing page with padding and a permanent light background for the code samples.
- [x] How does the user select the Ray version?
- [x] Footer includes Sphinx version information; do we want this?
- [x] The AskAI button has a very faint background so it doesn't look like a clickable button; missing Ray logo
- [x] In the rightmost column on pages, there's an ""Edit on Github"" button; is that something we want?",main landing page padding permanent light background code user select ray version footer sphinx version information want button faint background look like button missing ray rightmost column edit button something want,issue,negative,negative,neutral,neutral,negative,negative
1819572035,"- [x] The Ask AI button doesn't have a clear border. It should also be black in non-dark-mode. The Ask AI button is black in dark mode, so perhaps the color setting is just reversed.
<img width=""156"" alt=""Screenshot 2023-11-20 at 10 08 39 AM"" src=""https://github.com/ray-project/ray/assets/122562471/e6e41233-ec50-4aec-af5c-0292396d89d3"">
<img width=""127"" alt=""Screenshot 2023-11-20 at 10 23 43 AM"" src=""https://github.com/ray-project/ray/assets/122562471/523c15c4-5331-467b-baba-9f57b6616915"">


- [ ] The search bar changes size/flashes when you click on the side bar. (This is not a blocker, but we should polish it up if we can in a follow up if it's too big of a lift for this PR.)

https://github.com/ray-project/ray/assets/122562471/b7428315-7d6f-45b3-b6c1-a7f325303e19

- [x] The dark mode switch has three modes. Two of them seem to be the same dark mode. Is this intentional? 
- [x] We lost the `Welcome to Ray` page in the side bar, that links to the top level landing page. `Overview` also got a new child page, `Learn more`. I think `Learn more` is included in the `Getting started` page on master. Is there a reason this PR rearranges that?",ask ai button clear border also black ask ai button black dark mode perhaps color setting reversed search bar click side bar blocker polish follow big lift dark mode switch three two seem dark mode intentional lost welcome ray page side bar link top level landing page overview also got new child page learn think learn included getting page master reason,issue,positive,positive,neutral,neutral,positive,positive
1819554001,"Ray depends on `async-timeout`, but it does not list it in its dependencies.  Instead, it has relied it on being installed as a transitive dependency of `aiohttp`.  However, `aiohttp` 3.9.0 was released a few days ago, which removed its dependency on `async-timeout` (https://github.com/aio-libs/aiohttp/issues/7502) when under Python 3.11 and higher.  So now Ray needs to depend on `async-timeout` explicitly if it is going to continue using it.",ray list instead transitive dependency however day ago removed dependency python higher ray need depend explicitly going continue,issue,negative,positive,positive,positive,positive,positive
1819535960,"@genesis-jamin  The linked PR was a prototype that got split into multiple PRs that have already been merged.
1. https://github.com/ray-project/ray/pull/33575
2. https://github.com/ray-project/ray/pull/33605
3. https://github.com/ray-project/ray/pull/33620",linked prototype got split multiple already,issue,negative,neutral,neutral,neutral,neutral,neutral
1819522051,"Should work for docker hub, where ray image is stored as well ;)",work docker hub ray image well,issue,negative,neutral,neutral,neutral,neutral,neutral
1819492664,"Had the same issue, running 'pip install async-timeout' fixed it for me.",issue running install fixed,issue,negative,positive,neutral,neutral,positive,positive
1819451322,"We will remove PG. But for now I just want all the tests to pass. We can do a separate PR that deals with PG and then makes sure all the (few) remaining tests are properly moved to PPO. Main problems remaining with this:
* Some tests use offline data that does NOT contain vf-predictions, hence PPO will fail these.
* Some tests run mysteriously slower for PPO than for PG <- This will require more investigation.",remove want pas separate sure properly main use data contain hence fail run mysteriously require investigation,issue,negative,positive,neutral,neutral,positive,positive
1819128540,"@jjyao @scv119 

Let's get back to merging this PR.

Re; typing. 

- I remove RayWaitable and used Union instead.
- To make generator -> ObjectRefGenerator[int], it requires lots of repetitive code. I will do it in a follow up. I got it working in this commit, https://github.com/ray-project/ray/pull/38784/commits/28d535556865fe75f669a2bc50de458507b4fa9f. But I won't inclde this in this PR

In this PR, I just made sure it doesn't raise an error when using mypy
",let get back remove used union instead make generator lot repetitive code follow got working commit wo made sure raise error,issue,negative,positive,neutral,neutral,positive,positive
1819071767,"Could we use `os.register_at_fork` to destroy all ray related environments in child process? For example:

```
import multiprocessing

import ray


@ray.remote
def act():
    def hook():
        print(f'hook pid: {os.getpid()}')
        import ray
        ray.shutdown()
    
    os.register_at_fork(after_in_child=hook)
    with multiprocessing.get_context(""fork"").Pool(1) as pool:
        return pool.apply(lambda: None)


ray.init()

# Hangs
ray.get(act.remote())
```",could use destroy ray related child process example import import ray act hook print import ray fork pool return lambda none,issue,negative,negative,neutral,neutral,negative,negative
1818890943,"I tested it on version 2.6 and the problem persisted, even though the grpcio dependency was removed.
My python version is 3.9.2

```
n136-146-210(data@rayjob:):tiger# py-spy dump -p 29793
Process 29793: ray::TestActor.test_foo_no_process
Python v3.9.2 (/usr/bin/python3.9)

Thread 29793 (idle): ""MainThread""
    poll (multiprocessing/popen_fork.py:27)
    wait (multiprocessing/popen_fork.py:43)
    join (multiprocessing/process.py:149)
    _terminate_pool (multiprocessing/pool.py:729)
    __call__ (multiprocessing/util.py:224)
    terminate (multiprocessing/pool.py:654)
    __exit__ (multiprocessing/pool.py:736)
    test_foo (foo_funcs.py:6)
    test_foo_no_process (main.py:20)
    _resume_span (ray/util/tracing/tracing_helper.py:464)
    actor_method_executor (ray/_private/function_manager.py:727)
    main_loop (ray/_private/worker.py:779)
    <module> (ray/_private/workers/default_worker.py:279)
n136-146-210(data@rayjob:):tiger# 
n136-146-210(data@rayjob:):tiger# 
n136-146-210(data@rayjob:):tiger# py-spy dump -p 32670
Process 32670: ray::TestActor.test_foo_no_process
Python v3.9.2 (/usr/bin/python3.9)

Thread 32670 (idle): ""MainThread""
    __enter__ (multiprocessing/synchronize.py:95)
    get (multiprocessing/queues.py:365)
    worker (multiprocessing/pool.py:114)
    run (multiprocessing/process.py:108)
    _bootstrap (multiprocessing/process.py:315)
    _launch (multiprocessing/popen_fork.py:71)
    __init__ (multiprocessing/popen_fork.py:19)
    _Popen (multiprocessing/context.py:277)
    start (multiprocessing/process.py:121)
    _repopulate_pool_static (multiprocessing/pool.py:326)
    _repopulate_pool (multiprocessing/pool.py:303)
    __init__ (multiprocessing/pool.py:212)
    Pool (multiprocessing/context.py:119)
    test_foo (foo_funcs.py:5)
    test_foo_no_process (main.py:20)
    _resume_span (ray/util/tracing/tracing_helper.py:464)
    actor_method_executor (ray/_private/function_manager.py:727)
    main_loop (ray/_private/worker.py:779)
    <module> (ray/_private/workers/default_worker.py:279)
```",tested version problem even though dependency removed python version data tiger dump process ray python thread idle poll wait join terminate module data tiger data tiger data tiger dump process ray python thread idle get worker run start pool module,issue,negative,neutral,neutral,neutral,neutral,neutral
1818877439,Any progress on this issue? I meet the same problem.,progress issue meet problem,issue,negative,neutral,neutral,neutral,neutral,neutral
1818745142,"> It's useful for us, can you submit a PR?

Thanks for your attention and suggestion. I have submitted, may you please review my request, thanks. ",useful u submit thanks attention suggestion may please review request thanks,issue,positive,positive,positive,positive,positive,positive
1818435351,"I am facing a similar issue, I am getting correct output when I print out the model output, but I can see similar values when I print out the actions values. Can someone help me with this?",facing similar issue getting correct output print model output see similar print someone help,issue,negative,neutral,neutral,neutral,neutral,neutral
1818285052,"The error should be fixed by https://github.com/ray-project/ray/pull/41036. Let's try to get the PR merged tomorrow, and then we don't need to skip the tests?",error fixed let try get tomorrow need skip,issue,negative,positive,neutral,neutral,positive,positive
1818281055,"@wingkitlee0 okay, sounds good. Could we `pytest.skip` the test when the PyArrow version is 6, and add a note explaining why we're skipping?",good could test version add note explaining skipping,issue,negative,positive,positive,positive,positive,positive
1818192196,Is there any update on this issue? Please let me know if there is any other info that I can provide that might help or anything that I can try meanwhile. This currently blocks me from running any training. ,update issue please let know provide might help anything try meanwhile currently running training,issue,positive,neutral,neutral,neutral,neutral,neutral
1818187319,"Thank you very much, I do need to report directly from the worker. However, it seems that [DataParallelTrainer](https://docs.ray.io/en/latest/train/api/doc/ray.train.data_parallel_trainer.DataParallelTrainer.html) cannot help me with this. Because my project does not try to set up multiple Ray workers for distributed training only, but rather a mixed implementation of training logic on the driver side and worker side, and the main loop is primarily running on the worker side. 
So I need to report directly from the worker to use ray tune Schedular.

In addition to aggregating the results from workers to the driver and then reporting,  or using DataParalleTrainer, are there any other ways to meet my requirements?

",thank much need report directly worker however help project try set multiple ray distributed training rather mixed implementation training logic driver side worker side main loop primarily running worker side need report directly worker use ray tune schedular addition driver way meet,issue,positive,positive,positive,positive,positive,positive
1818097327,"> One typo I think, otherwise looks good to me

Thanks Archit for the review!",one typo think otherwise good thanks review,issue,positive,positive,positive,positive,positive,positive
1818091162,"Was this feature ever implemented? #33510 seems to have been closed without merging, and from viewing the flame graph I'm seeing significant time spent in my custom collate function.",feature ever closed without flame graph seeing significant time spent custom collate function,issue,negative,positive,neutral,neutral,positive,positive
1818078443,"Here's [a link to the Buildkite run](https://buildkite.com/ray-project/release/builds/1672#_) where the rest of the stable/non-flaky/non-jailed tests pass.

Note that this run happened before I made [the `serve_resnet_benchmark` fix](https://github.com/ray-project/ray/pull/41244#issuecomment-1818076319), so the `serve_resnet_benchmark` failure is no longer relevant.",link run rest pas note run made fix failure longer relevant,issue,negative,positive,neutral,neutral,positive,positive
1818077790,"The only remaining stable/non-flaky/non-jailed test that failed is `long_running_serve.aws` ([Buildkite link](https://buildkite.com/ray-project/release/builds/1672#018be3bd-f4b1-4fe3-8da9-6e96e8b20f8c)). This also failed on `master` last night due to an infra error ([link](https://buildkite.com/ray-project/release/builds/1661#018be237-2ee5-4d27-b1e0-0faa47925b65)). I don’t think this test uses pydantic in any unique way, and the error traceback was the same on master and on the PR, so I think the PR run also failed due to an infra error.",test link also master last night due infra error link think test unique way error master think run also due infra error,issue,negative,positive,neutral,neutral,positive,positive
1817866360,Thanks for retrying. Please open a new issue if the problem reappears.,thanks please open new issue problem,issue,negative,positive,positive,positive,positive,positive
1817808762,I encountered a similar problem and suspected that there was a memory leak in this place. https://discuss.ray.io/t/ray-task-fastapi-suspected-memory-leak/12884,similar problem suspected memory leak place,issue,negative,neutral,neutral,neutral,neutral,neutral
1817804137,"No luck (which is a good thing in this case).
I've done various attempts at reproducing the issue using the latest Ray version, but after running the same setup as original I was unable to get any leaked processes. I did see the 'force kill' events in the logs, but those processes were properly killed in this case and there were no dangling processes like last time. Ran the test for a couple of hours to verify. So looks like we can close this issue. Thanks!
",luck good thing case done various issue latest ray version running setup original unable get see kill properly case dangling like last time ran test couple verify like close issue thanks,issue,positive,positive,positive,positive,positive,positive
1817721173,"> I tried running only for 30 episodes by using this ""stop={""episodes_total"": 30}"" but it ran for 40 episodes. I tried changing it to 10 and it ran for 20 episodes. How to control for how many episodes the tne.run() should execute? How to set number of iterastions?

The extra episodes may due to evaluation? You can try setting `evaluation_interval=0` to disable it.",tried running ran tried ran control many execute set number extra may due evaluation try setting disable,issue,negative,positive,positive,positive,positive,positive
1817481242,"I was also getting 1450 error on specific large data given to `ray.put` while having a lot of free RAM without any obvious indications why. After looking through the logs got the idea that ray was writing objects to temp on disk anyway and that was the issue. Basically if you don't have enough free space on disk it will fail caching objects to temp and throw 1450 error no matter how much free RAM you have. I freed space on disk and problem is gone. 
It seems if input object is even bigger I again get an error. So since there is not enough memory in plasma storage the solution is to increase it's size. By default in my case ray was using 17gb storage, but I had 25gb object. Adding `object_store_memory=26*1000*1000*1000` in `ray.init` solved it for me again.",also getting error specific large data given lot free ram without obvious looking got idea ray writing temp disk anyway issue basically enough free space disk fail temp throw error matter much free ram freed space disk problem gone input object even bigger get error since enough memory plasma storage solution increase size default case ray storage object,issue,negative,positive,neutral,neutral,positive,positive
1817455834,"> I think the biggest issue for me is that I'm not entirely sure how is the scale direction calculated and why we need to have different behavior for different scale direction and making this logic even more complicated😅 Is there simpler thing we can do to solve the problem?

That's fair. We decided on this design because it makes the most sense for the semantics of `initial_replicas`. While a Serve config is rolling out, `initial_replicas` should be respected as a lower bound. While a Serve config is being deleted, `initial_replicas` should be ignored.

That results in the somewhat complicated logic in this PR. I'm hoping we can simplify this code when we fully implement improved in-place rollouts inside of Serve.",think biggest issue entirely sure scale direction calculated need different behavior different scale direction making logic even complicated simpler thing solve problem fair decided design sense semantics serve rolling lower bound serve somewhat complicated logic simplify code fully implement inside serve,issue,negative,positive,neutral,neutral,positive,positive
1817382734,let's close to run the test in this weekly run; the issue will re-open itself if the test doesn't pass,let close run test weekly run issue test pas,issue,negative,neutral,neutral,neutral,neutral,neutral
1817339260,"> You're right that in the current design, if the supervisor actor fails, the job is failed. Enabling `max_restarts` would require some thought around whether the constructor is idempotent and things like that.
> 
> Unexpected actor failure should be rare though, how is it failing for you?

like this https://github.com/ray-project/ray/issues/41241 
would fail when no system resource",right current design supervisor actor job would require thought around whether constructor idempotent like unexpected actor failure rare though failing like would fail system resource,issue,negative,negative,neutral,neutral,negative,negative
1817326655,"> if there's perf benchmark? (I guess not becuase it is gcs only but just in case)

I guess we will look at memory usage of the release tests + any perf metrics for these. 

I don't think we have any release tests on GCS specifically. 


> verify the mem usage is 500MB~ish when there are 1 million tasks? (maybe you can verify it locally)

Yeah, this is what I did the last time - I can double-check locally again. ",guess case guess look memory usage release metric think release specifically verify mem usage million maybe verify locally yeah last time locally,issue,negative,neutral,neutral,neutral,neutral,neutral
1817321157,"Note: I had to reduce the version of `watchfiles` to `0.19.0` due to the premerge check. I'd argue the constraints should not be applied to the doc build requirements, as they're separate from the rest of the ray requirements.

Once https://github.com/click-contrib/sphinx-click/pull/129 is merged, we can remove this dependency from the `requirements-doc.txt`.",note reduce version due check argue applied doc build separate rest ray remove dependency,issue,negative,negative,negative,negative,negative,negative
1817310412,"it's still pretty flaky... 
![image](https://github.com/ray-project/ray/assets/116198444/1989989d-995f-4da6-8b18-92abeac04403)
@rkooo567 this is linux do we want to keep p0 for this as something we want to fix and block on for ray29 release?",still pretty flaky image want keep something want fix block ray release,issue,negative,positive,positive,positive,positive,positive
1817308515,"Hi @fy222fy,

Calling `train.report` will not work in processes spawned by yourself in this custom training function. You should have the main `objective` function launch workers as needed and aggregated results (like the `ray.get` you're doing), and that process should be the one reporting.

Does your use case require reporting directly from the worker? If so, take a look at [`DataParallelTrainer`](https://docs.ray.io/en/latest/train/api/doc/ray.train.data_parallel_trainer.DataParallelTrainer.html), which may suit your needs if you're trying to set up distributed training yourself.",hi calling work custom training function main objective function launch like process one use case require directly worker take look may suit need trying set distributed training,issue,negative,positive,neutral,neutral,positive,positive
1817307907,per our new policy than should we downgrade this to p1 and just jail it then? @c21 what say you; how important is this test?,per new policy downgrade jail say important test,issue,negative,positive,positive,positive,positive,positive
1817279415,"how is this going?  it's the only thing that is pinning us to 3.11.x
rebuilding our amazon ami linux based container and seeing this:
```
#0 2.371 ERROR: Could not find a version that satisfies the requirement ray[default] (from versions: none)
```",going thing pinning u ami based container seeing error could find version requirement ray default none,issue,negative,neutral,neutral,neutral,neutral,neutral
1817274062,"Ah, okay. Makes sense that the error is happening with `write_block`.",ah sense error happening,issue,negative,neutral,neutral,neutral,neutral,neutral
1817271904,"@bveeramani here is a concrete error I got. Opening the file stream was successful, but then an exception was raised from `write_block`. 

```
ray.exceptions.RayTaskError(OSError): ray::MapBatches(<lambda>)->Write() (pid=84787, ip=10.0.24.21)
    for b_out in map_transformer.apply_transform(iter(blocks), ctx):
File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/data/_internal/execution/operators/map_transformer.py"", line 232, in __call__
    yield from self._block_fn(input, ctx)
File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/data/_internal/planner/plan_write_op.py"", line 25, in fn
    write_result = datasink_or_legacy_datasource.write(blocks, ctx)
File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/data/datasource/file_datasink.py"", line 97, in write
    self.write_block(block, block_index, ctx)
File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/data/datasource/file_datasink.py"", line 186, in write_block
    self.write_block_to_file(block, file)
File ""pyarrow/io.pxi"", line 122, in pyarrow.lib.NativeFile.__exit__                                                                                                                                                                                                                        File ""pyarrow/io.pxi"", line 189, in pyarrow.lib.NativeFile.close
File ""pyarrow/error.pxi"", line 115, in pyarrow.lib.check_status 
    OSError: When uploading part for key '...' in bucket 'air-example-data-2': AWS Error INTERNAL_FAILURE during UploadPart operation: We encountered an internal error. Please try again.

```",concrete error got opening file stream successful exception raised ray lambda write iter file line yield input file line file line write block file line block file file line file line file line part key bucket error operation internal error please try,issue,negative,positive,positive,positive,positive,positive
1817258442,"Running the below release tests (which have been impacted by task backend recently) to see the impact: 
- microbenchmark
- stress_test_many_tasks
- long_running_many_actor_tasks

",running release impacted task recently see impact,issue,negative,neutral,neutral,neutral,neutral,neutral
1817189494,"@raulchen do you have an easy way to reproduce this? 

We use the same retry mechanism for both reads and writes (`_open_file_with_retry`), so it's not obvious to me why this is only an issue for writes:

https://github.com/ray-project/ray/blob/372befbdfdf8191cb0c6b4207fe38c7f187dbc64/python/ray/data/datasource/file_datasink.py#L180-L185",easy way reproduce use retry mechanism obvious issue,issue,negative,positive,positive,positive,positive,positive
1817177995,"I believe this has been deflaked by https://github.com/ray-project/ray/pull/40875.

I will put up a PR to remove from the list and retry it a few times.",believe put remove list retry time,issue,negative,neutral,neutral,neutral,neutral,neutral
1817165854,Do we know of any paying customers running into this issue yet @bveeramani ?,know paying running issue yet,issue,negative,neutral,neutral,neutral,neutral,neutral
1817147713,"@shrekris-anyscale Looks like premerge is still running, please let me know again when it's ready",like still running please let know ready,issue,positive,positive,positive,positive,positive,positive
1817131883,">Awesome PR! Btw, do you think we can test per-series limit inside test_metrics_batch? I think we can export a couple of multi-tag metrics and verify it is importing data correctly?

Doesn't seem like this test exists",awesome think test limit inside think export couple metric verify data correctly seem like test,issue,positive,positive,positive,positive,positive,positive
1817111018,"New [release test ](https://buildkite.com/ray-project/release/builds/1507)passed, [logs](https://console.anyscale-staging.com/o/anyscale-internal/jobs/prodjob_u8ey1r4kz7szqkxik8gj7vwdc6) show that 10 workers were killed, max to kill was set to 20.",new release test show kill set,issue,negative,positive,positive,positive,positive,positive
1817074901,"Sounds good I'll mark this one as closed, please feel free to try out the new version and re-open if you still have any issues. Thanks!",good mark one closed please feel free try new version still thanks,issue,positive,positive,positive,positive,positive,positive
1816939127,"Totally, thanks for sharing raising pid_max fixed the issue. Yes, I can confirm running this on my local machine with less resource and `ulimit -u` can cash Ray's GCS. Created a separate [issue](https://github.com/ray-project/ray/issues/41241) to implement the job queue to fix it :) ",totally thanks raising fixed issue yes confirm running local machine le resource cash ray separate issue implement job queue fix,issue,positive,positive,positive,positive,positive,positive
1816902758,"Make the gcp registry configurable through a env var. There is a yaml config that also contains this value, to make things configurable in another repo. However, the anyscale build code is this repo specific so I keep things simple for now. This might change on further unification.",make registry also value make another however build code specific keep simple might change unification,issue,negative,neutral,neutral,neutral,neutral,neutral
1816889624,"I synced with @iycheng offline yesterday. I will also set the environment variables in the KubeRay repository, instead of solely relying on the default configuration in the Ray repository, as it may be updated by others in the future. The test will also be added in the KubeRay repository. This PR is ready for merge.",yesterday also set environment repository instead solely default configuration ray repository may future test also added repository ready merge,issue,negative,positive,neutral,neutral,positive,positive
1816704897,"I'm running into this issue as well. I'm also using Istio which does not support custom protocols AFAIK. Looking at the ClientBuilder code, it looks like it uses the protocol to specify the module to import, so they can support `ray://` and `anyscale://` protocols.

I think a monkey patch like this one may work for this use case:

```python
import ray
import sys

sys.modules['https'] = sys.modules['ray']
ray.init(""https://ray-cluster.<domain_name>"")
```

I haven't tested it myself, but I'm planning to do it soon. I'll post my results here.",running issue well also support custom looking code like protocol specify module import support ray think monkey patch like one may work use case python import ray import tested soon post,issue,positive,negative,neutral,neutral,negative,negative
1816600594,can you fix core test failures first? ,fix core test first,issue,negative,positive,positive,positive,positive,positive
1816410087,"> These errors seem to be coming from Tune's old execution engine in ray<2.5 -- are you also running into this with the latest version of Ray (2.7.1 as of now)? @jakemdaly @grizzlybearg @llkongs

Hi! I am using Ray version 2.8 and I got this problems from time to time too during my trainings",seem coming tune old execution engine ray also running latest version ray hi ray version got time time,issue,negative,positive,positive,positive,positive,positive
1816340442,"Oh, I searched in the issues to see if it was reported but I did not notice the pull request. Feel free to close this issue.",oh see notice pull request feel free close issue,issue,positive,positive,positive,positive,positive,positive
1815722574,"> > [docs/readthedocs.com:anyscale-ray](https://readthedocs.com/projects/anyscale-ray/builds/1848237/) due to a warning. The warning seems to be unrelated to this PR. @angelinalg do you have any idea? Thanks!
> 
> I guess this is the warning you're talking about: https://buildkite.com/ray-project/premerge/builds/11949#018bd7f5-dc11-4198-8ba4-e9e9b175a5ad/6-102
> 
> ```
> �_bk;t=1700137490258�WARNING: failed to reach any of the inventories with the following issues:
> 
> �_bk;t=1700137490258�intersphinx inventory 'https://docs.scipy.org/doc/scipy/objects.inv' not fetchable due to <class 'requests.exceptions.SSLError'>: HTTPSConnectionPool(host='docs.scipy.org', port=443): Max retries exceeded with url: /doc/scipy/objects.inv (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1131)')))
> ```
> 
> I'm guessing it's a transient error, because `premerge` seems to be passing on master. I'll merge master to restart CI

Hi @architkulkarni, thank you for merging the master branch. And your suggestion is also applied. ",due warning warning unrelated idea thanks guess warning talking reach following inventory due class certificate verify certificate guessing transient error passing master merge master restart hi thank master branch suggestion also applied,issue,negative,negative,neutral,neutral,negative,negative
1815699403,"I confirmed that using ""ray-client-server-port"" instead of head node port addresses the issue.",confirmed instead head node port issue,issue,negative,positive,positive,positive,positive,positive
1815664088,"> Feel free to merge this until #41139 is fixed but I do think we should make sure to fix that in a timely manner -- it will only get worse as time goes on (and more regressions might pile up).

totally. I am trusting that part is on @bveeramani and @anyscalesam :) ",feel free merge fixed think make sure fix timely manner get worse time go might pile totally trusting part,issue,positive,positive,positive,positive,positive,positive
1815656849,Hi! Looking forward to your reply.,hi looking forward reply,issue,negative,neutral,neutral,neutral,neutral,neutral
1815636748,"> We can probably implement some kind of job queue for the Jobs to fix the issue, does that sound good to you? I'm not really sure what else we can help here without able to reproduce the issue on our machines😅

we have modified the system config pid_max to a largger val.
here we just report this issue or fault: ray used so many threads that may easily crash on hundreds of job.

I prefer to give more details to help you reproduce the issue.",probably implement kind job queue fix issue sound good really sure else help without able reproduce issue system report issue fault ray used many may easily crash job prefer give help reproduce issue,issue,positive,positive,positive,positive,positive,positive
1815631656,Feel free to merge this until https://github.com/ray-project/ray/issues/41139 is fixed but I do think we should make sure to fix that in a timely manner -- it will only get worse as time goes on (and more regressions might pile up).,feel free merge fixed think make sure fix timely manner get worse time go might pile,issue,negative,positive,positive,positive,positive,positive
1815628331,Test memory pressure is very flaky and almost certainly unrelated to this PR (since this PR doesn't change anything on x86_64),test memory pressure flaky almost certainly unrelated since change anything,issue,negative,positive,positive,positive,positive,positive
1815556622,"Here's what we tried:
1. Patch the storage context to save the file with some contents. Our hypothesis was the empty file causing some issues. [Rejected ❌]
2. Reinit the filesystem `storage.storage_filesystem = pyarrow.fs.S3FileSystem()`. Our hypothesis was that the pickled version of the pyarrow filesystem from the head node (on AWS) is incompatible on the worker node. The issue was reproducible even with creating a new instance of `pyarrow.fs.S3FileSystem()` in the debug session. [Rejected ❌]
3. Try some different s3 operations on the pyarrow filesystem. Once it fails with the error, running any other commands in the same process (via a debug session) will result in the same error.
4. Use s3fs instead of pyarrow s3. This worked in 5/5 runs, so we can workaround with it. [Workaround ✅ ] 
5. Increasing the max attempts of the pyarrow s3 filesystem, but this just led to the command hanging due to retrying and 
failing with the same error. The hypothesis was that maybe it doesn't get enough retries. [Rejected ❌]
6. We tried to patch the ray code to set the pyarrow s3 filesystem logging level, but this didn't work. TODO: We are putting a pin on this until pyarrow 15 gets released with an environment variable that can help debug the problem. (see https://github.com/apache/arrow/pull/38267)",tried patch storage context save file content hypothesis empty file causing hypothesis version head node incompatible worker node issue reproducible even new instance session try different error running process via session result error use instead worked increasing led command hanging due failing error hypothesis maybe get enough tried patch ray code set logging level work pin environment variable help problem see,issue,negative,negative,neutral,neutral,negative,negative
1815494318,i think the issue is for when the training is happening and the models are filling up the tmp folder,think issue training happening filling folder,issue,negative,neutral,neutral,neutral,neutral,neutral
1815366651,"hi @JakeSummers , For filtering out ray serve internal logs by requests, ray serve introduce the new logging api (will be out to 2.9). After enable the flag, you are able to disable all the serve internal logs **by requests**, so that it will be eaiser to check your application log.",hi filtering ray serve internal ray serve introduce new logging enable flag able disable serve internal check application log,issue,negative,positive,positive,positive,positive,positive
1815365379,"@sven1977 sorry for the spam, let me know if in the future you just want to get one master task for all of these",sorry let know future want get one master task,issue,negative,negative,negative,negative,negative,negative
1815342886,"@justinvyu It looks like the last time this happened, a `tune-sklearn` version bump was reverted in the external `tune-sklearn` repo so the Ray build could go through, and then it was unreverted.  Is it feasible to do this again so the 2.8 docs can be updated, or alternatively can you recommend a way to fix this directly on the Ray 2.8 release branch?",like last time version bump external ray build could go unreverted feasible alternatively recommend way fix directly ray release branch,issue,positive,positive,neutral,neutral,positive,positive
1815336941,"Ok, merging now. The memray issue is othorgonal to this migration. Also I don't think users will be hard-blocked. It's the Ci environments that use a couple of too-old packages that cause the issues.",issue migration also think use couple cause,issue,negative,neutral,neutral,neutral,neutral,neutral
1815329017,According to RTD support the build is timing out (??). This takes 8m locally...troubleshooting now.,according support build timing locally,issue,negative,neutral,neutral,neutral,neutral,neutral
1815292683,"Another issue I observe is very slow scaling (and task submission) with the `ActorPoolStrategy`.  
My job is now running for 18 minutes. It definitely requires hundreds of actors. Only 25 have been requested so far, and it took it around 15 minutes to request 5. The task submission speed seems to improve over time tho. Anyway, I don't see why it shouldn't happen immediately. `max_tasks_in_flight_per_actor` is set to 3. Setting it to 1 causes the AutoScaler to crash. 

Edit: the low number of actors is due to Node Group max size limitation. But the task submission is still slow.",another issue observe slow scaling task submission job running definitely far took around request task submission speed improve time tho anyway see happen immediately set setting crash edit low number due node group size limitation task submission still slow,issue,negative,negative,negative,negative,negative,negative
1815286083,@architkulkarni p1 implies fix in the next 1-2 ray releases; let's go further down to p2.,fix next ray let go,issue,negative,neutral,neutral,neutral,neutral,neutral
1815271011,"To your questions, there is yes, but it's not language/test-framework agonistic (https://stackoverflow.com/questions/60233576/how-do-we-run-a-single-test-using-google-bazel). We want a solution that works across python, c, pytest, gtest, etc. to reduce the complexity, so I think inserting a code-blob to skip it is a better solution.",yes agonistic want solution work across python reduce complexity think skip better solution,issue,positive,positive,positive,positive,positive,positive
1815267414,"Similar to what you did for python, in the beginning of the test function just do 

```
if CI_SKIP_FLAKY_TEST:
  assert true
  return
``` ",similar python beginning test function assert true return,issue,negative,positive,positive,positive,positive,positive
1815160637,"https://ray-distributed.slack.com/archives/CNCKBBRJL/p1700162038008149?thread_ts=1697707213.372609&cid=CNCKBBRJL 

Looks might not be a python version thing. ",might python version thing,issue,negative,neutral,neutral,neutral,neutral,neutral
1815160278,"Hmm, I see, the readthedocs build is actually failing with the same error: https://readthedocs.com/projects/anyscale-ray/builds/1849173/

```
ERROR: Cannot install -r doc/requirements-doc.txt (line 3) and ray[tune]==2.8.0 because these package versions have conflicting dependencies.

The conflict is caused by:
    tune-sklearn 0.5.0 depends on ray>=2.7.1
    ray[tune] 2.8.0 depends on ray 2.8.0 (from https://files.pythonhosted.org/packages/ec/a9/45948b0cc252eef6c06c941d780042d1fffd0ba4a856febaebd5a5f8bf94/ray-2.8.0-cp310-cp310-manylinux2014_x86_64.whl (from https://pypi.org/simple/ray/))

To fix this you could try to:
1. loosen the range of package versions you've specified
2. remove package versions to allow pip attempt to solve the dependency conflict
```

So even though we merged this PR, the doc update isn't being reflected because readthedocs can't build it.  @justinvyu can you recommend any workaround for this?",see build actually failing error error install line ray tune package conflicting conflict ray ray tune ray fix could try loosen range package remove package allow pip attempt solve dependency conflict even though doc update reflected ca build recommend,issue,negative,neutral,neutral,neutral,neutral,neutral
1815086404,"@can-anyscale no we don't need any review from ray-docs, this PR was already reviewed by ray-docs when it was merged to master.  If you could merge that would be great!",need review already master could merge would great,issue,positive,positive,positive,positive,positive,positive
1815045722,"@vitsai @can-anyscale Is it possible to bypass premerge for this? The failure is due to 

```
#7 153.0   WARNING: Requested tune-sklearn==0.4.6 from git+https://github.com/ray-project/tune-sklearn@master (from -r /ray/ci/env/../../python/requirements/ml/tune-requirements.txt (line 20)), but installing version 0.5.0
--
  | #7 153.0 ERROR: Cannot install tune-sklearn==0.4.6 because these package versions have conflicting dependencies.
  | #7 153.0
  | #7 153.0 The conflict is caused by:
  | #7 153.0     The user requested tune-sklearn==0.4.6
  | #7 153.0     The user requested (constraint) tune-sklearn

```
and
```
  ERROR: Cannot install tune-sklearn 0.5.0 (from git+https://github.com/ray-project/tune-sklearn@master#tune-sklearn) and tune-sklearn==0.4.6 because these package versions have conflicting dependencies.
  ```
  
  This is due to an update in an different repo (`tune-sklearn`).",possible bypass failure due warning line version error install package conflicting conflict user user constraint error install package conflicting due update different,issue,negative,negative,negative,negative,negative,negative
1815038722,"@Harsh-Maheshwari The videos seem broken. 
`read_parquet` will also fetch metadata and sample some files. It looks like because of those tasks.
If the memory is from `ray::IDLE`, it should go away after a while. those Ray workers will be garbage collected if being idle for some time. ",seem broken also fetch sample like memory ray go away ray garbage collected idle time,issue,negative,negative,negative,negative,negative,negative
1814995681,"Flaky test functions are run: https://buildkite.com/ray-project/premerge/builds/11966#018bd930-4bb0-47cb-b937-cc0638af3624/6-495
Removed test targets are run: https://buildkite.com/ray-project/premerge/builds/11966#018bd930-4b63-423f-9e00-db7391947bf0/6-1020",flaky test run removed test run,issue,negative,neutral,neutral,neutral,neutral,neutral
1814995183,"> [docs/readthedocs.com:anyscale-ray](https://readthedocs.com/projects/anyscale-ray/builds/1848237/) due to a warning. The warning seems to be unrelated to this PR. @angelinalg do you have any idea? Thanks!

I guess this is the warning you're talking about: https://buildkite.com/ray-project/premerge/builds/11949#018bd7f5-dc11-4198-8ba4-e9e9b175a5ad/6-102

```
_bk;t=1700137490258WARNING: failed to reach any of the inventories with the following issues:

_bk;t=1700137490258intersphinx inventory 'https://docs.scipy.org/doc/scipy/objects.inv' not fetchable due to <class 'requests.exceptions.SSLError'>: HTTPSConnectionPool(host='docs.scipy.org', port=443): Max retries exceeded with url: /doc/scipy/objects.inv (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1131)')))
```
I'm guessing it's a transient error, because `premerge` seems to be passing on master.  I'll merge master to restart CI",due warning warning unrelated idea thanks guess warning talking reach following inventory due class certificate verify certificate guessing transient error passing master merge master restart,issue,negative,negative,neutral,neutral,negative,negative
1814985727,@rkooo567 this is linux and a prior p0; should we add this as a 4th flaky test we prioritize in fixing in the near term?,prior add th flaky test fixing near term,issue,negative,positive,neutral,neutral,positive,positive
1814979729,"Can someone merge; we have a user study later today and it'll likely hit this issue agian. Still happening as of now:
<img width=""819"" alt=""image"" src=""https://github.com/ray-project/ray/assets/116198444/f7b5ee45-684e-450f-83d1-4a6dadcca810"">
",someone merge user study later today likely hit issue still happening image,issue,negative,neutral,neutral,neutral,neutral,neutral
1814960669,"i am using the version 3.0.0 using specific commit 036439609cda989407bb8a35902418876904d8c4 which is from last night it is still not deleting the tmp folders i double checked the local code file for keras.py and the changes are there! i am running an model with 1000 samples and the tmp files from the beginning of the run are still in the folder it filled up the whole space even though there are checkpoints being made




`    model.fit(train_data, batch_size=config[""batch_size""] ,epochs=120,validation_data=val_data,verbose=False,  
              callbacks=[ReportCheckpointCallback(metrics={""mean_accuracy"": ""accuracy"",""mean_loss"":""loss"", ""val_loss"":""val_loss"",""val_acc"":""val_accuracy""})])
`

this is the code i have for the report checkpoint callback",version specific commit last night still double checked local code file running model beginning run still folder filled whole space even though made accuracy loss code report,issue,negative,positive,positive,positive,positive,positive
1814946909,"Thanks for your review @kouroshHakha !
I admit some of the example removals are debateable. The main thought was that some of the more advanced features that we don't really want users to use anymore b/c they will very soon be on the new stack, we can take out already. However, I undid all those examples removals that you questioned.

And yes, we will replace most of our examples with new stack equivalents, but also clean up this folder well (it's become pretty messy over time :( ).",thanks review admit example main thought advanced really want use soon new stack take already however undid yes replace new stack also clean folder well become pretty messy time,issue,positive,positive,positive,positive,positive,positive
1814945126,"Thanks for the replies @JingChen23 ! 

Any comments about this?

> Isn't it equivalent to just generate a cluster YAML with a list of N worker node types whose CPU requirements are 2^i? 

Maybe it's not equivalent, I just want to make sure I understand the behavior correctly.",thanks equivalent generate cluster list worker node whose maybe equivalent want make sure understand behavior correctly,issue,positive,positive,positive,positive,positive,positive
1814939827,[docs/readthedocs.com:anyscale-ray](https://readthedocs.com/projects/anyscale-ray/builds/1848237/) due to a warning. The warning seems to be unrelated to this PR. @angelinalg do you have any idea? Thanks!,due warning warning unrelated idea thanks,issue,negative,positive,neutral,neutral,positive,positive
1814928842,@architkulkarni This PR is ready to merge! Checked the linkcheck build and confirmed none of the errors are related,ready merge checked build confirmed none related,issue,negative,positive,positive,positive,positive,positive
1814926334,"@peytondmurray and @mattip , please review and comment, or will merge this at the end of the day.",please review comment merge end day,issue,negative,neutral,neutral,neutral,neutral,neutral
1814896851,@can-anyscale will you be making a PR to enable this flag and actual remove the test names from the flakey test file? ,making enable flag actual remove test test file,issue,negative,neutral,neutral,neutral,neutral,neutral
1814893766,"I believe this was fixed since the issue was opened.
Currently the code has [here](https://github.com/ray-project/ray/blob/master/rllib/policy/policy.py#L1421C10-L1421C10):
```python
        sample_batch_size = min(
            max(self.batch_divisibility_req * 4, 32),
            self.config[""train_batch_size""],  # Don't go over the asked batch size.
        )
```",believe fixed since issue currently code python min go batch,issue,negative,positive,neutral,neutral,positive,positive
1814881286,"Hey I think this should be addressed in the nightly version of Ray, which includes https://github.com/ray-project/ray/pull/41033",hey think nightly version ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1814881114,"We can probably implement some kind of job queue for the Jobs to fix the issue, does that sound good to you? I'm not really sure what else we can help here without able to reproduce the issue on our machines😅",probably implement kind job queue fix issue sound good really sure else help without able reproduce issue,issue,positive,positive,positive,positive,positive,positive
1814878613,Hey what version of Ray are you using. https://github.com/ray-project/ray/pull/41033 is included in the nightly wheel and should delete the temp folder,hey version ray included nightly wheel delete temp folder,issue,negative,neutral,neutral,neutral,neutral,neutral
1814824672,"https://buildkite.com/ray-project/premerge/builds/11954#018bd8cc-885b-4933-9490-7eaaea6b9000/5631-6348
https://buildkite.com/ray-project/premerge/builds/11954#018bd8cc-885b-4933-9490-7eaaea6b9000/5631-6395

````
src/ray/raylet/scheduling/local_resource_manager_test.cc(267): error: Expected: (absl::Now() - *idle_time) >= (absl::Seconds(1)), actual: 999.758427ms vs 1s
````

and:

https://buildkite.com/ray-project/premerge/builds/11954#018bd8cc-885b-4933-9490-7eaaea6b9000/5631-6442

````
src/ray/raylet/scheduling/local_resource_manager_test.cc(219): error: Expected: (dur) >= (1), actual: 0 vs 1
````
",error actual error actual,issue,negative,neutral,neutral,neutral,neutral,neutral
1814629693,"> Short term: We will likely allow configuerable limit > 10K for returning results (at the cost of higher latency) for users.

Thanks! That seems like a reasonable approach. For our usecase, we don't envision creating more than about 40k tasks, and so a configurable `MAX_LIMIT` would solve our problem until pagination is built out.

By the way we also tried `list_tasks` with the filter argument, but it too is handicapped by the 10k limit.",short term likely allow limit cost higher latency thanks like reasonable approach envision would solve problem pagination built way also tried filter argument handicapped limit,issue,negative,positive,positive,positive,positive,positive
1814293470,"> Made some comments to improve clarity. Let me know if you have questions.

All suggestions applied. Thank you @angelinalg!",made improve clarity let know applied thank,issue,positive,neutral,neutral,neutral,neutral,neutral
1814245265,Thanks for the review @kouroshHakha ! Waiting for tests to pass ...,thanks review waiting pas,issue,negative,positive,positive,positive,positive,positive
1813997133,"just saying, it might be a better fix to make the return value of `input_files()` deterministic.

instead of:

````
metadata = self._plan.execute().get_metadata()
files = set()
for m in metadata:
    for f in m.input_files:
        files.add(f)
return list(files)
````

do this:

````
metadata = self._plan.execute().get_metadata()
fileset = set()
files = list()
for m in metadata:
    for f in m.input_files:
        if f not in fileset:
           fileset.add(f)
           files.append(f)
return files
````
",saying might better fix make return value deterministic instead set return list set list return,issue,positive,positive,positive,positive,positive,positive
1813852001,I create a release blocker for this https://github.com/ray-project/ray/issues/41198. We should at least re-visit if we can undo this change before the next release.,create release blocker least undo change next release,issue,negative,negative,negative,negative,negative,negative
1813847644,"Actually, yeah - now that I think of it, I guess we would just need a function that checks whether the endpoints in the `intersphinx_mapping` are reachable, and removes them from the mapping if they aren't.

> do you know what is broken in https://anyscale-ray--41197.com.readthedocs.build/en/41197/ given that doc page is broken?

Yeah, any type annotations that include types defined in `scipy`, or references in _our_ docs which point to pages in _scipy_'s docs will be broken links at the moment.",actually yeah think guess would need function whether reachable know broken given doc page broken yeah type include defined point broken link moment,issue,negative,negative,negative,negative,negative,negative
1813844007,"@peytondmurray my understanding on the impact of this is limited, do you know what is broken in https://anyscale-ray--41197.com.readthedocs.build/en/41197/ given that doc page is broken? `scipy` also a part of `autodoc_mock_imports`, does it help with anything? ",understanding impact limited know broken given doc page broken also part help anything,issue,negative,negative,negative,negative,negative,negative
1813840310,"I'm really not sure what the best thing to do here is. On one hand, I don't want broken docs to prevent builds from succeeding, but on the other hand if intersphinx doesn't query the docs of the dependency, it doesn't know where to link different types to.

It would be cool to have a mechanism that tests the intersphinx mapping, and if it doesn't work it automatically adds it to `autodoc_mock_modules` or something like that. Anyway, that's probably out of scope here.",really sure best thing one hand want broken prevent succeeding hand query dependency know link different would cool mechanism work automatically something like anyway probably scope,issue,positive,positive,positive,positive,positive,positive
1813833461,"but you cannot really say that ""I cannot reproduce it, so I cannot fix it"". the statement is just false.

but it is totally reasonable to say, we have limited bandwidth, so I won't be able to fix it now. if it is important, then we can block release, or ask engineer managers to allocate more resources.

like in the worse case, I or Cuong can even allocate time to fix it for you.

or maybe it is not an important bug, and we just mark it as not PR blocking, not release blocking let it fail for some more time, or just stop running it for a while, that is totally okay.
",really say reproduce fix statement false totally reasonable say limited wo able fix important block release ask engineer allocate like worse case even allocate time fix maybe important bug mark blocking release blocking let fail time stop running totally,issue,negative,positive,neutral,neutral,positive,positive
1813831453,you probably cannot merge because the docbuild is broken on master; let me know if you want to bypass and merge it ,probably merge broken master let know want bypass merge,issue,negative,negative,negative,negative,negative,negative
1813828343,"> we had no issues reproducing flaky tests

first of all, you can still reproduce now. like create a PR, disable all other tests on the pipeline, and rerun the job with a buildkite job matrix of like 20 times, and I am pretty sure you can reproduce it. otherwise it won't even show up as flaky on the flaky dashboard. it is more like the change of test environment revealed more flaky test issues. 

> ask us to rewrite all tests

that is not what we asked. ultimately, there is a bug, you can decide if it is important, and based on the bandwidth you have, decide to fix it or not.

",flaky first still reproduce like create disable pipeline rerun job job matrix like time pretty sure reproduce otherwise wo even show flaky flaky dashboard like change test environment revealed flaky test ask u rewrite ultimately bug decide important based decide fix,issue,positive,positive,positive,positive,positive,positive
1813818936,"Previously, we had no issues reproducing flaky tests. I understand we are rewriting infra to the right way, but you cannot just change all infra and ask us to rewrite all tests. Yes, ray has too many integration tests, and we should improve it, but We have limited bandwidth, and we should find practical iterative approach. ",previously flaky understand infra right way change infra ask u rewrite yes ray many integration improve limited find practical iterative approach,issue,positive,positive,positive,positive,positive,positive
1813805510,"> we can't debug issues that are not reproducible

you should try to rewrite/redesign the test to make the test's behavior more deterministic or easier to debug/reproduce, so that it relies less on the specific test environment.

a unit test is a test that runs under certain resource assumptions, like overall time out, like memory size or number of cores it has, basic file system and networks, etc. and that's it.

a unit test should be able to run with other stuff running concurrently. it is not part of the spec that when running unit tests, one have to clear the system and exit all other programs.

the timing behavior of the OS or machine is also not part of the spec. unit tests should pass on cpus faster or slower with only adjustments on the overall timeout.

the test env container and bazel is supposed to capture all the dependencies and the setup of test already, if we run a bazel unit test in the same test container, and it passes on one machine, but fails on another machine, or if it passes when running by itself, but fails when running with other tests, that normally means that the test itself is not very well designed or written, and the code should be re-examined on its assumptions of the environment.

----

at the end of the day, ray code will need to run in all kinds of different environments, when a user reports a bug that something does not work for them in their environment, and we tell user that ""we cannot debug or fix it because we cannot reproduce the error by ourselves"", the user will probably just stop using ray.

it is ultimately the code maintainer's job to make test failures easier to reproduce. the CI team will try our best to make the ""test environment"" part easier to reproduce, but we cannot help that much on reproducing a particular flaky test failure.

----

for example, a racing condition issue might only happen under stress, and even when under stress, it might only happen in 0.1% of the time. it is very very hard to reproduce, but still can be a serious bug when the consequence is for example losing the training progress of a big AI model, or crash a production system of vital importance.

and in cases like these, sometimes the best way to debug, is not trying to reproduce it, but just read the code carefully, comment it with texts, try to understand it, refactor it into a structure where its behavior and concurrency pattern is easier to comprehend, double check all the assumptions of the environment, and try to find the potential racing condition based on the clues we have in hand. form a theory, and tune the stress level of different components (e.g. by injecting intentional spins, or manipulate the cache) and see if it is possible to make the racing condition easier to trigger.
",ca reproducible try test make test behavior deterministic easier le specific test environment unit test test certain resource like overall time like memory size number basic file system unit test able run stuff running concurrently part spec running unit one clear system exit timing behavior o machine also part spec unit pas faster overall test container supposed capture setup test already run unit test test container one machine another machine running running normally test well designed written code environment end day ray code need run different user bug something work environment tell user fix reproduce error user probably stop ray ultimately code maintainer job make test easier reproduce team try best make test environment part easier reproduce help much particular flaky test failure example racing condition issue might happen stress even stress might happen time hard reproduce still serious bug consequence example losing training progress big ai model crash production system vital importance like sometimes best way trying reproduce read code carefully comment try understand structure behavior concurrency pattern easier comprehend double check environment try find potential racing condition based hand form theory tune stress level different intentional manipulate cache see possible make racing condition easier trigger,issue,positive,positive,neutral,neutral,positive,positive
1813772102,"> Overall looking good. The total machine time is 2X compared to before though so it's difficult to tell if this will incur more or less cost.

will optimize, will optimize it.",overall looking good total machine time though difficult tell incur le cost optimize optimize,issue,positive,positive,neutral,neutral,positive,positive
1813710366,"> Yea, I think it's expected. If you need that many job running at the same time, would be a good idea to setup some worker nodes to do it if they won't all fit into one single node. But at least to your original issue, you are able to run 30 jobs and can continue to submit jobs on your 20 core machine now right?

it is the head node that will crash, not the worker node. actually we use one head node and many work node in production. to locate and reproduce bug, test ray in one node. 
i think **crash** is a **bug**. ray used too many threads for just 500 jobs. @GeneDer ",yea think need many job running time would good idea setup worker wo fit one single node least original issue able run continue submit core machine right head node crash worker node actually use one head node many work node production locate reproduce bug test ray one node think crash bug ray used many,issue,negative,positive,positive,positive,positive,positive
1813710044,"> Yea, I think it's expected. If you need that many job running at the same time, would be a good idea to setup some worker nodes to do it if they won't all fit into one single node. But at least to your original issue, you are able to run 30 jobs and can continue to submit jobs on your 20 core machine now right?

This bug only occurs occasionally in production，we reproduced the crash as reported here. @GeneDer 
",yea think need many job running time would good idea setup worker wo fit one single node least original issue able run continue submit core machine right bug occasionally crash,issue,negative,positive,positive,positive,positive,positive
1813687359,"Can we also skip this one https://buildkite.com/ray-project/postmerge/builds/1690#018bd506-9d94-4111-bd86-c64149eb997a/6-565; then I don't need this PR (https://github.com/ray-project/ray/pull/41189), thankks",also skip one need,issue,negative,neutral,neutral,neutral,neutral,neutral
1813585261,"Updates on the 2 new backpressure policies:
* StreamingOutput backpressure works well on benchmarks. It will be enabled by default in 2.9. 
* ConcurrencyCap backpressure still needs more improvements. It will be enabled by default in 2.10. https://github.com/ray-project/ray/issues/41193 ",new work well default still need default,issue,negative,positive,positive,positive,positive,positive
1813566952,"Yea, I think we should still add tests.",yea think still add,issue,negative,neutral,neutral,neutral,neutral,neutral
1813562947,"> I think we should try to add tests now (here instead of kuberay repo) if we want to move this plugin out of experimental/alpha.

Agree that if we want to move this out of experimental/alpha, that is necessary. In Serve we are planning to document this as an experimental feature though, since it definitely isn't stable yet from what I've tested. Do you think we need to add the test if it remains experimental?",think try add instead want move agree want move necessary serve document experimental feature though since definitely stable yet tested think need add test remains experimental,issue,positive,positive,neutral,neutral,positive,positive
1813542286,"> @jjyao I'll be adding a test to the Kuberay repo for this (will link here when I have it up). To add a test to the Ray CI, the docker container that we run tests in needs to be started with privileged permissions. I think when this was originally contributed, no tests were enabled in the CI.

I think we should try to add tests now (here instead of kuberay repo) if we want to move this plugin out of experimental/alpha. ",test link add test ray docker container run need privileged think originally think try add instead want move,issue,negative,positive,positive,positive,positive,positive
1813539215,@rkooo567 some of the workflow tests use 4 core and we run them only a medium size machine (with 4 core); we should run these tests serially; updated the PR to do so,use core run medium size machine core run serially,issue,negative,neutral,neutral,neutral,neutral,neutral
1813538759,"@ericl needs your approval here as code owner.

Context: we are removing GPUtil and gpustats dependencies with a vendored in pyvnml single file library (the offical nvidia library that other libraries use internally) so that it works with minimal ray and it also removes unnecessary transitive dependencies included by gpustats for terminal display.",need approval code owner context removing single file library library use internally work minimal ray also unnecessary transitive included terminal display,issue,negative,negative,negative,negative,negative,negative
1813515341,"we can't debug issues that are not reproducible. what do you mean by ""different environment"" here? Are you saying the tests are running concurrently vs serially? What's the exact difference between tests in flaky build vs not? ",ca reproducible mean different environment saying running concurrently serially exact difference flaky build,issue,negative,negative,neutral,neutral,negative,negative
1813511708,"Yes, please help investigate the error logs, they are still flaky in some certain environments",yes please help investigate error still flaky certain,issue,positive,positive,positive,positive,positive,positive
1813510689,"<img width=""1641"" alt=""Screenshot 2023-11-16 at 9 25 10 AM"" src=""https://github.com/ray-project/ray/assets/18510752/79415525-35f9-48b9-9dea-e9085be8b880"">

hmm interesting. This seems to become flaky after I take it out from the list. ",interesting become flaky take list,issue,negative,positive,positive,positive,positive,positive
1813510029,"tests passed.

some runtime env related tests are flaky though. @rickyyx 

I am particularly not very happy about the `test_invalid_conda_env` one:

````
# Sleep to wait bad runtime env cache removed.
--
  | time.sleep(bad_runtime_env_cache_ttl_seconds)
  |  
  | # The third time this runs it should be slower as the error isn't cached.
  | start = time.time()
  | with pytest.raises(RuntimeEnvSetupError, match=""ResolvePackageNotFound""):
  | ray.get(f.options(runtime_env=bad_env).remote())
  |  
  | >       assert (time.time() - start) > (first_time / 2.0)
  | E       assert (1700085106.696519 - 1700085092.9776516) > (28.069849491119385 / 2.0)
  | E        +  where 1700085106.696519 = <built-in function time>()
  | E        +    where <built-in function time> = time.time

````


",related flaky though particularly happy one sleep wait bad cache removed third time error start assert start assert function time function time,issue,negative,positive,neutral,neutral,positive,positive
1813505944,"Still flaky it seems, have been blocking postmerge twice since this is un-flake",still flaky blocking twice since,issue,negative,neutral,neutral,neutral,neutral,neutral
1813503185,"@bveeramani 

To recap, one of the bugs is about the failure to detect partitioning when using one file or a list of one file (after filtering). In recent `pyarrow` versions, this is not an issue. Hence, this PR is to fix Ray.
However, in the older `pyarrow` 6, it actually had the same bug: https://github.com/apache/arrow/blob/release-6.0.1/python/pyarrow/parquet.py#L1686
So the unit test for partition_filter is expected to fail and Ray is consistent with `pyarrow` regarding this bug.

Maybe this test should simply be skipped for pyarrow 6

",recap one failure detect partitioning one file list one file filtering recent issue hence fix ray however older actually bug unit test fail ray consistent regarding bug maybe test simply,issue,negative,negative,neutral,neutral,negative,negative
1813450893,This test has been marked as flaky for a while. I just open an issue now to track it.,test marked flaky open issue track,issue,negative,positive,neutral,neutral,positive,positive
1813445233,"I tried this out and I believe this should actually be fixed by streaming generator backpressure as well.

When you make the following change, backpressure works and the execution halts when object store memory reaches the limit (e.g., `1.74 GiB / 1.74 GiB`):

```
N = 10000
S = 10

ds = ray.data.range(N, parallelism=N)
```

I guess without generator backpressure, the generator task will keep producing rows for the block until the entire input for the task is done, ignoring the memory quota.",tried believe actually fixed streaming generator well make following change work execution object store memory limit gib gib guess without generator generator task keep block entire input task done memory quota,issue,negative,positive,neutral,neutral,positive,positive
1813412167,"I have the same issue too. In my case, my neural network works with batch size 1. Is there any workaround for this I can use? Thanks for the help",issue case neural network work batch size use thanks help,issue,positive,positive,positive,positive,positive,positive
1813351710,"> Do we have tests?

@jjyao I'll be adding a test to the Kuberay repo for this (will link here when I have it up). To add a test to the Ray CI, the docker container that we run tests in needs to be started with privileged permissions. I think when this was originally contributed, no tests were enabled in the CI.",test link add test ray docker container run need privileged think originally,issue,negative,positive,positive,positive,positive,positive
1813233313,Open this issue for book keeping that I moved the test to flaky state in https://github.com/ray-project/ray/pull/41165,open issue book keeping test flaky state,issue,negative,neutral,neutral,neutral,neutral,neutral
1813216352,"Ok, the `mock_s3_bucket_uri` fixture is actually not in the conftest for `ray.air.tests`. Let's move this unit test to `ray.train.tests` instead and add the conftest as a dependency.

1. Move https://github.com/ray-project/ray/blob/2e7448e83521d3130738825fd71a50b6aff5c698/python/ray/air/BUILD#L253 to https://github.com/ray-project/ray/blob/master/python/ray/train/BUILD (sorted by alphabetical order)
2. Add conftest as a dependency:

```python
py_test(
    name = ""test_result"",
    size = ""medium"",
    srcs = [""tests/test_result.py""],
    tags = [""team:ml"", ""exclusive""],
    deps = ["":ml_lib"", "":conftest""]
)
```",fixture actually let move unit test instead add dependency move sorted alphabetical order add dependency python name size medium team exclusive,issue,negative,neutral,neutral,neutral,neutral,neutral
1813141644,"> can you merge it now? I believe it should be working now...

@rkooo567 I noticed we have access to this button now, I think it's a new thing...  I'm going to hit it now just to speed things up and start CI again
<img width=""884"" alt=""Screenshot 2023-11-15 at 11 31 01 AM"" src=""https://github.com/ray-project/ray/assets/5459654/64277e33-0bb5-49d0-9e0b-e176808d285a"">

",merge believe working access button think new thing going hit speed start,issue,negative,positive,positive,positive,positive,positive
1813127921,"Are you using the same version of python everywhere? What version of python did you use to create the pickle?
I found this [CPython issue](https://github.com/python/cpython/issues/100316) about loading code objects which led to my question about python versions",version python everywhere version python use create pickle found issue loading code led question python,issue,negative,neutral,neutral,neutral,neutral,neutral
1813002444,"Yes, I can verify this is a bug. If the client dropped for a batch request, any following requests will not be resolved. Looking into a fix now ",yes verify bug client batch request following resolved looking fix,issue,positive,neutral,neutral,neutral,neutral,neutral
1813001383,@ciaociaoyu waiting on that traceback from you before we can proceed further; can you advise please?,waiting proceed advise please,issue,negative,neutral,neutral,neutral,neutral,neutral
1812964685,"@ChristosPeridis sorry for missing this one. 

Have you tried `CUDA_VISIBLE_DEVICES=uuid1,uuid2....,uuid14 ray start --num-gpus=14` This will start a Ray node with 14 GPUs (each is one 1g.5gb).",sorry missing one tried ray start start ray node one,issue,negative,negative,negative,negative,negative,negative
1812821942,"I think I will try my best in two weeks, if not achieve the goal, I will close that PR.",think try best two achieve goal close,issue,positive,positive,positive,positive,positive,positive
1812816812,"@anyscalesam welp... I'm an idiot and sorry for wasting your time - it looks like the only pin to 2.11 anymore is in the dockerfile requirements:

https://github.com/ray-project/ray/blob/releases/2.7.1/python/requirements/ml/dl-gpu-requirements.txt#L3
https://github.com/ray-project/ray/blob/releases/2.7.1/python/requirements/ml/dl-cpu-requirements.txt#L4

The conflict I had was coming from another package we develop which pinned to TF 2.11. It has been a while since I messed around on the problem I was having and I misremembered the origin of the package conflict

So sorry :(",idiot sorry wasting time like pin conflict coming another package develop pinned since around problem origin package conflict sorry,issue,negative,negative,negative,negative,negative,negative
1812795064,"Yea, I think it's expected. If you need that many job running at the same time, would be a good idea to setup some worker nodes to do it if they won't all fit into one single node. But at least to your original issue, you are able to run 30 jobs and can continue to submit jobs on your 20 core machine now right? ",yea think need many job running time would good idea setup worker wo fit one single node least original issue able run continue submit core machine right,issue,positive,positive,positive,positive,positive,positive
1812688362,CC @rkooo567 Do you have any idea ? This is a big error on a very basic case ,idea big error basic case,issue,negative,neutral,neutral,neutral,neutral,neutral
1812079205,"maybe the `set()` in the `input_files()` should just be used for dedup, so the order of the file names can be preserved as much as possible?

suggestions?",maybe set used order file much possible,issue,negative,neutral,neutral,neutral,neutral,neutral
1812073007,"and because `Time complexity: O(num input files)` , sorting in the implementation seems to be forbidden. ",time complexity input implementation forbidden,issue,negative,neutral,neutral,neutral,neutral,neutral
1812070173,"````
>>> list(set(['1', '2', '3']))
['2', '3', '1']
````

probably based on hash key / iteration order..",list set probably based hash key iteration order,issue,negative,neutral,neutral,neutral,neutral,neutral
1812017793,// hail to all the code owners of this test file..,hail code test file,issue,negative,neutral,neutral,neutral,neutral,neutral
1811970483,"I do not mind running it again and and again on postmerge (and hoping it can show some differnet result?) but it is currently polluting the banner with all the errors like this:

<img width=""888"" alt=""image"" src=""https://github.com/ray-project/ray/assets/95255098/fe613f49-2fc5-4136-9117-252ffa8f9cb9"">
",mind running show result currently polluting banner like image,issue,negative,neutral,neutral,neutral,neutral,neutral
1811969188,"you can flip it back when it is fixed; based on my shallow understanding, the arrow interface has changed, and it won't be fixed unless we change the code on our part.",flip back fixed based shallow understanding arrow interface wo fixed unless change code part,issue,negative,negative,neutral,neutral,negative,negative
1811938402,"> Just started another instance, set ulimit -u 32768, and submitted another 500 jobs. Don't see any failures on my end. <img alt=""image"" width=""416"" src=""https://user-images.githubusercontent.com/7553988/283017784-bd33a325-ddb9-4180-88cb-fb815eaa42b5.png""> <img alt=""image"" width=""758"" src=""https://user-images.githubusercontent.com/7553988/283017053-2254302b-3002-4df4-b8e6-2713f6d312f0.png""> <img alt=""image"" width=""1631"" src=""https://user-images.githubusercontent.com/7553988/283017183-a88ea7c3-280b-481b-9377-21b903860392.png"">
> 
> When you upgraded, did you restart your Ray cluster? Also, maybe try to set `ulimit -u unlimited` to see if that helps?
> 
> If it continue to fail, can you also share at which point it fails? I only submit 500 jobs because in the initial post you mentioned the jobs starts to fail after 400. Are you still seeing issues at 30 jobs in your 20 core machine?

yes, restart ray, or even restart the system first",another instance set another see end image image image restart ray cluster also maybe try set unlimited see continue fail also share point submit initial post fail still seeing core machine yes restart ray even restart system first,issue,negative,negative,negative,negative,negative,negative
1811933177,"> Just started another instance, set ulimit -u 32768, and submitted another 500 jobs. Don't see any failures on my end. <img alt=""image"" width=""416"" src=""https://user-images.githubusercontent.com/7553988/283017784-bd33a325-ddb9-4180-88cb-fb815eaa42b5.png""> <img alt=""image"" width=""758"" src=""https://user-images.githubusercontent.com/7553988/283017053-2254302b-3002-4df4-b8e6-2713f6d312f0.png""> <img alt=""image"" width=""1631"" src=""https://user-images.githubusercontent.com/7553988/283017183-a88ea7c3-280b-481b-9377-21b903860392.png"">
> 
> When you upgraded, did you restart your Ray cluster? Also, maybe try to set `ulimit -u unlimited` to see if that helps?
> 
> If it continue to fail, can you also share at which point it fails? I only submit 500 jobs because in the initial post you mentioned the jobs starts to fail after 400. Are you still seeing issues at 30 jobs in your 20 core machine?

The fact may be: ray create too many threads, and if over the limit, ray would crash. ",another instance set another see end image image image restart ray cluster also maybe try set unlimited see continue fail also share point submit initial post fail still seeing core machine fact may ray create many limit ray would crash,issue,negative,negative,negative,negative,negative,negative
1811923325,"> Just started another instance, set ulimit -u 32768, and submitted another 500 jobs. Don't see any failures on my end. <img alt=""image"" width=""416"" src=""https://user-images.githubusercontent.com/7553988/283017784-bd33a325-ddb9-4180-88cb-fb815eaa42b5.png""> <img alt=""image"" width=""758"" src=""https://user-images.githubusercontent.com/7553988/283017053-2254302b-3002-4df4-b8e6-2713f6d312f0.png""> <img alt=""image"" width=""1631"" src=""https://user-images.githubusercontent.com/7553988/283017183-a88ea7c3-280b-481b-9377-21b903860392.png"">
> 
> When you upgraded, did you restart your Ray cluster? Also, maybe try to set `ulimit -u unlimited` to see if that helps?
> 
> If it continue to fail, can you also share at which point it fails? I only submit 500 jobs because in the initial post you mentioned the jobs starts to fail after 400. Are you still seeing issues at 30 jobs in your 20 core machine?

en, so many threads is bug?",another instance set another see end image image image restart ray cluster also maybe try set unlimited see continue fail also share point submit initial post fail still seeing core machine en many bug,issue,negative,negative,negative,negative,negative,negative
1811903659,"> Just started another instance, set ulimit -u 32768, and submitted another 500 jobs. Don't see any failures on my end. <img alt=""image"" width=""416"" src=""https://user-images.githubusercontent.com/7553988/283017784-bd33a325-ddb9-4180-88cb-fb815eaa42b5.png""> <img alt=""image"" width=""758"" src=""https://user-images.githubusercontent.com/7553988/283017053-2254302b-3002-4df4-b8e6-2713f6d312f0.png""> <img alt=""image"" width=""1631"" src=""https://user-images.githubusercontent.com/7553988/283017183-a88ea7c3-280b-481b-9377-21b903860392.png"">
> 
> When you upgraded, did you restart your Ray cluster? Also, maybe try to set `ulimit -u unlimited` to see if that helps?
> 
> If it continue to fail, can you also share at which point it fails? I only submit 500 jobs because in the initial post you mentioned the jobs starts to fail after 400. Are you still seeing issues at 30 jobs in your 20 core machine?

strange, you can create threads over the limit, so sysctl -w kernel.pid_max=32768? ",another instance set another see end image image image restart ray cluster also maybe try set unlimited see continue fail also share point submit initial post fail still seeing core machine strange create limit,issue,negative,negative,negative,negative,negative,negative
1811855362,"Just started another instance, set ulimit -u 32768, and submitted another 500 jobs. Don't see any failures on my end.
<img width=""416"" alt=""image"" src=""https://github.com/ray-project/ray/assets/7553988/bd33a325-ddb9-4180-88cb-fb815eaa42b5"">
<img width=""758"" alt=""image"" src=""https://github.com/ray-project/ray/assets/7553988/2254302b-3002-4df4-b8e6-2713f6d312f0"">
<img width=""1631"" alt=""image"" src=""https://github.com/ray-project/ray/assets/7553988/a88ea7c3-280b-481b-9377-21b903860392"">

When you upgraded, did you restart your Ray cluster? Also, maybe try to set `ulimit -u unlimited` to see if that helps? 

If it continue to fail, can you also share at which point it fails? I only submit 500 jobs because in the initial post you mentioned the jobs starts to fail after 400. Are you still seeing issues at 30 jobs in your 20 core machine? ",another instance set another see end image image image restart ray cluster also maybe try set unlimited see continue fail also share point submit initial post fail still seeing core machine,issue,negative,negative,negative,negative,negative,negative
1811846113,"> @wingkitlee0 looks premerge is failing because `test_parquet_read_partitioned_with_partition_filter` doesn't work with Arrow 6

@bveeramani I will take a deeper look later because pyarrow dataset api looks different in version 6.x  ",failing work arrow take look later different version,issue,negative,neutral,neutral,neutral,neutral,neutral
1811830157,"> `ulimit -u` is unlimited for me tho, not sure if that makes a difference? Also, just to share, htop looks like this on my machine when running the job <img alt=""image"" width=""785"" src=""https://user-images.githubusercontent.com/7553988/283012167-000db952-8d16-454f-89b7-8599b8957c42.png"">

if do not have root, you can run in docker: --ulimit nproc=32768",unlimited tho sure difference also share like machine running job image root run docker,issue,positive,positive,positive,positive,positive,positive
1811828669,"> Why do you want to set ulimit to 32768?

on some machine, 32768 is the default limit val",want set machine default limit,issue,negative,neutral,neutral,neutral,neutral,neutral
1811826588,"`ulimit -u` is unlimited for me tho, not sure if that makes a difference? 
Also, just to share, htop looks like this on my machine when running the job 
<img width=""785"" alt=""image"" src=""https://github.com/ray-project/ray/assets/7553988/000db952-8d16-454f-89b7-8599b8957c42"">
",unlimited tho sure difference also share like machine running job image,issue,positive,positive,positive,positive,positive,positive
1811826425,"> I ran your scripts on m5.8xlarge instance again and still seeing all of them succeeded. Not really sure what's the disconnect 😅 <img alt=""image"" width=""1704"" src=""https://user-images.githubusercontent.com/7553988/283011334-b9474b2c-bc84-49f5-8846-14a0027105ba.png"">

what your ulimit -a ?
set ulimit -u =32768 ?",ran instance still seeing really sure disconnect image set,issue,negative,positive,positive,positive,positive,positive
1811823993,"I ran your scripts on m5.8xlarge instance again and still seeing all of them succeeded. Not really sure what's the disconnect 😅
<img width=""1704"" alt=""image"" src=""https://github.com/ray-project/ray/assets/7553988/b9474b2c-bc84-49f5-8846-14a0027105ba"">
",ran instance still seeing really sure disconnect image,issue,negative,positive,positive,positive,positive,positive
1811815131,"![image](https://github.com/ray-project/ray/assets/20881860/9f952019-601f-4d12-be02-2d5b1202c912)
![image](https://github.com/ray-project/ray/assets/20881860/53cb180f-9b6c-4506-9644-9db24071fa0b)
![image](https://github.com/ray-project/ray/assets/20881860/6cb4667e-b653-4ad6-92a5-2b27b62185c4)
![image](https://github.com/ray-project/ray/assets/20881860/a0f8850b-bd54-4315-9ea0-d12bb429b7ad)

still crash and pending: python9, ray2.8",image image image image still crash pending python ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1811791365,"Python 3.8 is still supported, but would suggest a later version if you are upgrading anyways 
",python still would suggest later version anyways,issue,negative,neutral,neutral,neutral,neutral,neutral
1811790609,"> @hefeiyun Are you running this on Python 3.7? We already dropped the support of Python 3.7, would suggest to use 3.9 or 3.10. Also, can you use latest Ray 2.8.0?

python3.8 ok?",running python already support python would suggest use also use latest ray python,issue,negative,positive,positive,positive,positive,positive
1811789281,"> @hefeiyun Are you running this on Python 3.7? We already dropped the support of Python 3.7, would suggest to use 3.9 or 3.10. Also, can you use latest Ray 2.8.0?

ray: 2.7.1",running python already support python would suggest use also use latest ray ray,issue,negative,positive,positive,positive,positive,positive
1811788728,"@GeneDer 
ray used too many thread for this 500 jobs:
![image](https://github.com/ray-project/ray/assets/20881860/a54e6948-1409-4031-8aec-261f683ff109)

",ray used many thread image,issue,negative,positive,positive,positive,positive,positive
1811786862,"@hefeiyun Are you running this on Python 3.7? We already dropped the support of Python 3.7, would suggest to use 3.9 or 3.10. Also, can you use latest Ray 2.8.0?",running python already support python would suggest use also use latest ray,issue,negative,positive,positive,positive,positive,positive
1811784937,"it seems not fixed.

**machine**: 
cpu: 20, mem: 64G
ulimit -u: 32768

**ray**: ray[default]==2.7.1

**code**:
# long_running.py
`import ray
import time

@ray.remote(num_cpus=1, scheduling_strategy=""SPREAD"")
def hello_world(time_diff):
    time.sleep(500)
    return f""hello world: {time_diff}""

ray.init()
start_time = time.time()
diff = time.time() - start_time
while diff < 50:
    print(ray.get(hello_world.remote(diff)))
    time.sleep(1)
    diff = time.time() - start_time`


# long_running.sh
`#!/bin/bash

counter=1
while [ $counter -le 500 ]
do
echo $counter
((counter++))
RAY_ADDRESS='http://172.16.10.61:8888' DEFAULT_JOB_START_TIMEOUT_SECONDS=300 ray job submit --no-wait -- python long_running.py &
done
echo All done
`

**cmd**: 
ray start --head --port=6666 --dashboard-host=0.0.0.0 --dashboard-port=8888

when the 500 jobs submitted, 
some job fail:
![image](https://github.com/ray-project/ray/assets/20881860/2ea77c65-e5cf-4626-8f82-66cabfb2d37b)
the head will fail:
![image](https://github.com/ray-project/ray/assets/20881860/283d308f-21ad-4373-9fbb-9411a58c0aef)
some job pending:
![image](https://github.com/ray-project/ray/assets/20881860/0d9c59e9-5048-4685-8814-b7b54de4c221)

",fixed machine mem ray ray default code import ray import time spread return hello world print counter echo counter ray job submit python done echo done ray start head job fail image head fail image job pending image,issue,negative,negative,negative,negative,negative,negative
1811772070,"Can we say ""Logical CPU usage (current / max)"" or ""Logical GPU usage (current / max)""  then? We can wrap the text to avoid a really wide column, like this:

<img width=""440"" alt=""Screenshot 2023-11-14 at 7 55 38 PM"" src=""https://github.com/ray-project/ray/assets/9677264/61f6e442-f894-47c1-b9c6-18ed56a29c1f"">


Btw, can we use lower case for the ""current"" and ""max""?
",say logical usage current logical usage current wrap text avoid really wide column like use lower case current,issue,negative,positive,neutral,neutral,positive,positive
1811751061,This test and its friends (https://github.com/ray-project/ray/issues/41151) suddenly fails on master. I need to jail them to unbreak master for now https://github.com/ray-project/ray/pull/41149. Please help investigate the unjail them later.,test suddenly master need jail master please help investigate later,issue,positive,negative,neutral,neutral,negative,negative
1811748666,"
> 1. What does cpu usage mean here? Physical cpu usage or?

It is logical, the image is outdated, but there's a tooltip that clarifies this. I thought the header was getting too long so I put it in a tooltip

> 2. The progress bar is getting too short. Can we set a min width for it and enable horizontal scaling if there are too many rows?

👍 

",usage mean physical usage logical image outdated thought header getting long put progress bar getting short set min width enable horizontal scaling many,issue,negative,negative,neutral,neutral,negative,negative
1811746041,"Thanks. Two comments?
1. What does cpu usage mean here? Physical cpu usage or? 
2. The progress bar is getting too short. Can we set a min width for it and enable horizontal scaling if there are too many rows?",thanks two usage mean physical usage progress bar getting short set min width enable horizontal scaling many,issue,positive,positive,neutral,neutral,positive,positive
1811702007,"> Ray is a code execution tool and we need to be compatible with all versions a user wants to use 
> we need to have a solution for it :)

hmm.. maybe consider buying out the arrow library maintainers? :)

kidding aside, reacting fast to third-party changes is a probe monitoring+oncall job. you can even declare an incident or something if arrow nightly is breaking us, but I would prefer premerge/postmerge pipelines not doing Internet probing work.",ray code execution tool need compatible user use need solution maybe consider arrow library aside fast probe job even declare incident something arrow nightly breaking u would prefer work,issue,negative,positive,positive,positive,positive,positive
1811666089,"Having a restricted list of supported versions is not acceptable -- Ray is a code execution tool and we need to be compatible with all versions a user wants to use (otherwise we get into the very painful situation that we were in with grpc a while back). That's why we must pick our dependencies extremely carefully.

Whether you want to solve that problem by running nightly tests in the CI or some other means / different pipeline is up to you all, but we need to have a solution for it :)",restricted list acceptable ray code execution tool need compatible user use otherwise get painful situation back must pick extremely carefully whether want solve problem running nightly different pipeline need solution,issue,negative,negative,negative,negative,negative,negative
1811657181,"Ah, you're right! Turns out I had 14.0.0 installed locally.",ah right turn locally,issue,negative,positive,positive,positive,positive,positive
1811654054,the 14.0.1 was released just a week ago; maybe 14.0.0 works (2 weeks ago),week ago maybe work ago,issue,negative,neutral,neutral,neutral,neutral,neutral
1811653800,"Arrow follows semver, sort-of. Virtually every release is a major release so backwards compatibility isn't guaranteed. 

> for those kind of packages, IMO, the right solution is to maintain a set of versions that work together and recommend this set to users, which we are already kind of doing in a way with constraint files + container images

@pcmoritz do you feel strongly that we keep the nightly tests? I think, given the tradeoffs, it might make sense to remove nightly from pre- and post-merge, and recommend a set of supported Arrow versions to users.

That said, I'm not strongly opposed to keeping them either.",arrow virtually every release major release backwards compatibility kind right solution maintain set work together recommend set already kind way constraint container feel strongly keep nightly think given might make sense remove nightly recommend set arrow said strongly opposed keeping either,issue,positive,positive,positive,positive,positive,positive
1811649637,"Okay, I couldn't reproduce the issue locally with Arrow 14. Also, when I reviewed the Arrow git log, it seems like the offending commit a week ago (which checks out since Arrow nightly only recently starting breaking. If it was an issue with Arrow 14, we would've seen the error earlier).

> I can generate a pip freeze for every job, that will make it easier to know going forward.

That'd be awesome!

",could reproduce issue locally arrow also arrow git log like commit week ago since arrow nightly recently starting breaking issue arrow would seen error generate pip freeze every job make easier know going forward awesome,issue,positive,positive,positive,positive,positive,positive
1811647838,"The test is broken with pyarrow 14 as well, as far as I know. The version is somewhere in the logs, might be hard to see.

I can generate a pip freeze for every job, that will make it easier to know going forward.",test broken well far know version somewhere might hard see generate pip freeze every job make easier know going forward,issue,negative,negative,negative,negative,negative,negative
1811644223,"just some thoughts and general principles:

- the purpose of premerge/postmerge is testing the code in the repo, not probing the Internet.
- we can have another probe pipeline that probes the Internet.
- if a python package does not have an 1.0 or does not follow semver, or does not have any kind of spec at all, we cannot expect things work with a future version.
- for those kind of packages, even if we test against them, when the test break, we have not much to do, other than file an issue and maybe disable the test.
- for those kind of packages, IMO, the right solution is to maintain a set of versions that work together and recommend this set to users, which we are already kind of doing in a way with constraint files + container images
- pip and its ecosystem is basically broken. pip install from pypi heads without constraint file is guaranteed not to work reliably for most of the cases, and we probably just should not endorse/support that workflow. I think there is no point to fight a fundamentally failing workflow. ",general purpose testing code another probe pipeline python package follow kind spec expect work future version kind even test test break much file issue maybe disable test kind right solution maintain set work together recommend set already kind way constraint container pip ecosystem basically broken pip install without constraint file work reliably probably think point fight fundamentally failing,issue,positive,positive,positive,positive,positive,positive
1811644213,"@aslonnie is there any way to confirm which Arrow version is being used in CI (or I guess which package versions are installed in general)? I'd expect it to use Arrow 14, but based on the errors I suspect it's using Arrow nightly.",way confirm arrow version used guess package general expect use arrow based suspect arrow nightly,issue,negative,positive,neutral,neutral,positive,positive
1811640975,"I see. So the core issue is that we want to identify issues with Arrow itself before an Arrow release? 

In that case, we could run to Arrow nightly tests pre-merge and permanently mark it as unstable.",see core issue want identify arrow arrow release case could run arrow nightly permanently mark unstable,issue,negative,neutral,neutral,neutral,neutral,neutral
1811629431,"If it is a relevant regression in Arrow, we can file an issue upstream and get it fixed before the Arrow release goes out. That's the purpose of these nightly tests (we have similar tests for some Ray Core dependencies).",relevant regression arrow file issue upstream get fixed arrow release go purpose nightly similar ray core,issue,negative,positive,positive,positive,positive,positive
1811626119,"> After it is released, it is too late and it will already be shipped to users.

How would we prevent new Arrow versions from breaking the stable Ray release? Even if we fix Arrow-related issues on master, it'd only help users who use Ray nightly or future releases, right?",late already shipped would prevent new arrow breaking stable ray release even fix master help use ray nightly future right,issue,positive,positive,neutral,neutral,positive,positive
1811616033,"This doesn't seem like a good move. It makes it impossible to detect issues before a new arrow version is released (and if it needs to be fixed on the arrow side, submit an issue and make sure it is fixed before the release). After Arrow is released, it is too late and it will already be shipped to users.

Do you have any other mechanisms in place to make sure we detect breakages before the release?",seem like good move impossible detect new arrow version need fixed arrow side submit issue make sure fixed release arrow late already shipped place make sure detect release,issue,positive,positive,positive,positive,positive,positive
1811582260,"We should also handle detached actor when job finishes. Right now, if a job finishes, any running tasks from the detached actor would be marked as failed incorrectly. ",also handle detached actor job right job running detached actor would marked incorrectly,issue,negative,positive,positive,positive,positive,positive
1811559589,"Although this seems to not be taking into account K8s native scalers, for the Keda particular use case, users could still use the [Metric Server](https://keda.sh/docs/2.12/operate/metrics-server/) with a dummy `ScaledObject` to retrieve metrics and scale upon them. May not be ideal, but I thought it was worth mentioning there is a way around it.",although taking account native particular use case could still use metric server dummy retrieve metric scale upon may ideal thought worth way around,issue,positive,positive,positive,positive,positive,positive
1811542827,"@aslonnie ah, yeah. Wanted to confirm that CI passes first.

Will ping you when ready.",ah yeah confirm first ping ready,issue,positive,positive,positive,positive,positive,positive
1811540724,"Already discussed, but wouldn't hurt to get stamp.",already would hurt get stamp,issue,negative,neutral,neutral,neutral,neutral,neutral
1811538456,"SGTM, do you need someone else from data to approve as well, or you all already agree on this?",need someone else data approve well already agree,issue,positive,neutral,neutral,neutral,neutral,neutral
1811535220,"The error is probably caused by https://github.com/apache/arrow/pull/38608.

Here's a prototype PR to fix https://github.com/ray-project/ray/pull/41036.",error probably prototype fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1811506830,"Yes that make sense, we will add the setting to `DataContext` later on.",yes make sense add setting later,issue,negative,neutral,neutral,neutral,neutral,neutral
1811500449,"Hey, that's exactly what I did. 
However, this doesn't seem like the best UX (or practice). Having dedicated settings would be more discoverable and convenient. ",hey exactly however seem like best practice would discoverable convenient,issue,positive,positive,positive,positive,positive,positive
1811500264,"You're right that in the current design, if the supervisor actor fails, the job is failed.  Enabling `max_restarts` would require some thought around whether the constructor is idempotent and things like that.  

Unexpected actor failure should be rare though, how is it failing for you?",right current design supervisor actor job would require thought around whether constructor idempotent like unexpected actor failure rare though failing,issue,negative,positive,neutral,neutral,positive,positive
1811436012,"Hi @danielgafni  - you can directly change the `DEFAULT_WAIT_FOR_MIN_ACTORS_SEC` value in your Python code.

```py
from ray.data._internal.execution.operators import actor_pool_map_operator
actor_pool_map_operator.DEFAULT_WAIT_FOR_MIN_ACTORS_SEC = xxx
```",hi directly change value python code import,issue,negative,positive,neutral,neutral,positive,positive
1811428806,"@vitsai Premerge is failing, but the reason is unrelated to this PR:

`ERROR: Cannot install tune-sklearn 0.5.0 (from git+https://github.com/ray-project/tune-sklearn@master#tune-sklearn) and tune-sklearn==0.4.6 because these package versions have conflicting dependencies.`

 The docs build has succeeded and the original PR didn't break the master build, so I think it's safe to merge this PR.  ",failing reason unrelated error install package conflicting build original break master build think safe merge,issue,negative,positive,positive,positive,positive,positive
1811270880,"> w00t yes, need to remove civ1 build jobs too. look like they build the cuda versions as well: https://github.com/ray-project/ray/blob/master/.buildkite/pipeline.arm64.yml#L44

hah, I have always thought that gpu on arm is not really a thing, it seems that ec2 actually has g5g instances, and I was wrong.",yes need remove build look like build well hah always thought arm really thing actually wrong,issue,negative,negative,negative,negative,negative,negative
1811193727,"That's right. When we fork, all open FDs in the raylet gets inherited to the child process aka the worker. The worker don't need them nor does it know them, so they never close, leading to leaks. Fun point is, if raylet had already connected with worker 1, when it forks worker 2, the new worker 2 would keep the sockets to worker 1, leading to O(n^2) open sockets.

Will make a unit test.",right fork open raylet child process aka worker worker need know never close leading fun point raylet already connected worker worker new worker would keep worker leading open make unit test,issue,negative,positive,positive,positive,positive,positive
1811193681,"w00t yes, need to remove civ1 build jobs too. look like they build the cuda versions as well: https://github.com/ray-project/ray/blob/master/.buildkite/pipeline.arm64.yml#L44",yes need remove build look like build well,issue,positive,neutral,neutral,neutral,neutral,neutral
1811060720,https://github.com/ray-project/ray/pull/41020 is merged. @lejarx or @mattip could you try nightly and verify that your issue is resolved?,could try nightly verify issue resolved,issue,negative,neutral,neutral,neutral,neutral,neutral
1810975084,"Cluster launcher seems to be passing https://buildkite.com/ray-project/release-tests-branch/builds/2356, it may have been a transient issue 

We can reopen if this crops up again.",cluster launcher passing may transient issue reopen,issue,negative,neutral,neutral,neutral,neutral,neutral
1810877236,"I don't think it's relevant anymore, this is the current test job: https://buildkite.com/ray-project/postmerge/builds/1662#018bcd7f-15f9-42e4-a4ef-75f5de9e8bac",think relevant current test job,issue,negative,positive,positive,positive,positive,positive
1810858249,"@vitsai premerge failed, but I think it's unrelated to the PR.  I restarted premerge for now, but if it keeps failing I think we should just merge it if the failure is unrelated",think unrelated failing think merge failure unrelated,issue,negative,negative,negative,negative,negative,negative
1810828099,"Add more details for https://github.com/ray-project/ray/issues/40911#issuecomment-1809367609.

* The monitor process communicates with other Ray processes via (1) RPC calls and (2) files.
* When users enable Ray Autoscaler in KubeRay, the Autoscaler (i.e. monitor process) will be running as a sidecar container in the head Pod.
* All containers in the same Pod share the network namespace, so they can communicate via RPC.
* KubeRay mounts the same volume at `/tmp/ray` for both the Ray container and the Ray Autoscaler sidecar container, enabling them to communicate via log files.
  * We can verify this by running `ray status` and triggering the autoscaling in the Ray container. If you can see `(autoscaler +XXmYYs)` in your console, the Ray container can read the `monitor.log` file.
     <img width=""1440"" alt=""Screen Shot 2023-11-14 at 10 00 30 AM"" src=""https://github.com/ray-project/ray/assets/20109646/f22bb355-718a-4aeb-abf6-e9c1d0fdc044"">


I cannot reproduce the issue on KubeRay. The [reproduction here](https://github.com/ray-project/ray/issues/40911#issuecomment-1795076822) is on Mac rather than K8s. We may need to figure out whether there is a reproduction for KubeRay or not.",add monitor process ray via enable ray monitor process running sidecar container head pod pod share network communicate via volume ray container ray sidecar container communicate via log verify running ray status ray container see console ray container read file screen shot reproduce issue reproduction mac rather may need figure whether reproduction,issue,negative,neutral,neutral,neutral,neutral,neutral
1810826492,downgrading this to p1 since it's a python test and we're prioritizing linux for now; agree @rkooo567 ?,since python test agree,issue,negative,neutral,neutral,neutral,neutral,neutral
1810825114,"Yes, it'll probably be unpinned in a nightly by the end of the month.",yes probably unpinned nightly end month,issue,negative,neutral,neutral,neutral,neutral,neutral
1810790457,"Ok, thank you. Might it be unpinned sooner in a nightly?",thank might unpinned sooner nightly,issue,negative,neutral,neutral,neutral,neutral,neutral
1810722206,"Ok great, thanks, do you have a rough timeline on when that might be released?",great thanks rough might,issue,positive,positive,positive,positive,positive,positive
1810589416,"Yes, we're planning to unpin the dependency for Ray 2.9 (the next Ray release).",yes unpin dependency ray next ray release,issue,negative,neutral,neutral,neutral,neutral,neutral
1810418309,I'm having a similar issue with the dashboard. The resources in the static folder are returning 403: main.4e04a38d.js and main.388a904b.css,similar issue dashboard static folder,issue,negative,positive,positive,positive,positive,positive
1810065558,With the recent release of pydantic 2.5 will you be able to unpin the dependency now? Thanks,recent release able unpin dependency thanks,issue,negative,positive,positive,positive,positive,positive
1810059476,"Can this be implemented inside Ray Train? Like current Ray Train tensorflow with「MultiWorkerMirroredStrategy」
https://docs.ray.io/en/latest/train/distributed-tensorflow-keras.html#train-tensorflow-overview
@matthewdeng ",inside ray train like current ray train,issue,negative,neutral,neutral,neutral,neutral,neutral
1809748127,"Btw, also found this message lol

```
    // TODO(mehrdadn): Use clone() on Linux or posix_spawnp() on Mac to avoid duplicating
    // file descriptors into the child process, as that can be problematic.
```",also found message use clone mac avoid file child process problematic,issue,negative,neutral,neutral,neutral,neutral,neutral
1809744597,"fork happens as a part of starting a new worker process? Did I understand the problem correctly here?

- Whenever we create a new worker, we fork.
- We duplicates fds to workers. 
- Repeat
?

Also can we add a test? I assume we should be able to write pretty determinisitc tests?",fork part starting new worker process understand problem correctly whenever create new worker fork repeat also add test assume able write pretty,issue,negative,positive,positive,positive,positive,positive
1809714363,"I could fix this issue by commenting out the following line from ray\_private\runtime_env\context.py

passthrough_args = [s.replace("" "", r""\ "") for s in passthrough_args]

(line 70 for ray 2.6.3)",could fix issue following line line ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1809690792,"We encountered the same issue with Pydantic 1.8.2 and Ray 2.6. PR #38418 seems a fix to this issue, which has been merged and released with Ray >=2.7 but unfortunately not backported to 2.6.

While waiting for a fix, a temporary workaround that works for me is to upgrade Pydantic to >=1.10.8 (upgrading to 1.9.0 is not enough as another error will ocurr).",issue ray fix issue ray unfortunately waiting fix temporary work upgrade enough another error,issue,negative,negative,negative,negative,negative,negative
1809607127,"@rkooo567 i started prototyping solution to make sure we look at actual serialized payload size when building up a batch in 
https://github.com/ray-project/ray/pull/40581. 

It's mostly there just need a bit of wiring to avoid hard-coding the current constants. Would you have cycles to take it over?",solution make sure look actual size building batch mostly need bit wiring avoid current would take,issue,negative,positive,positive,positive,positive,positive
1809605187,Re-opening this since #40177 haven't addressed this issue -- we still see metrics overflowing now default limit of 20Mb,since issue still see metric overflowing default limit,issue,negative,neutral,neutral,neutral,neutral,neutral
1809601648,"Yes, this can be implemented on top of Ray Core, similar to https://docs.ray.io/en/master/ray-core/examples/plot_parameter_server.html",yes top ray core similar,issue,positive,positive,positive,positive,positive,positive
1809558517,"here is a ray cluster in action:
` ss -x -a -p `

```
Netid              State               Recv-Q               Send-Q                                                                      Local Address:Port                                    Peer Address:Port                                 Process
u_str              LISTEN              0                    4096                  /tmp/ray/session_2023-11-13_17-17-18_580425_67/sockets/plasma_store 28237                                              * 0                                     users:((""ray::fib"",pid=30611,fd=20),(""ray::fib"",pid=30598,fd=20),(""ray::fib"",pid=30527,fd=20),(""ray::fib"",pid=30485,fd=20),
(""ray::fib"",pid=30425,fd=20),(""ray::fib"",pid=30378,fd=20),(""ray::fib"",pid=30335,fd=20),(""ray::fib"",pid=30305,fd=20),(""ray::IDLE"",pid=30256,fd=20),(""ray::fib"",pid=30253,fd=20),(""ray::fib"",pid=30147,fd=20),(""ray::fib"",pid=30102,fd=20),(""ray::fib"",pid=30056,fd=20),(""ray::fib"",pid=30055,fd=20),(""ray::IDLE"",pid=30054,fd=20),(""ray::fib"",pid=29974,fd=20),(""ray::fib"",pi
d=29921,fd=20),(""ray::fib"",pid=29911,fd=20),(""ray::fib"",pid=29907,fd=20),(""ray::fib"",pid=29822,fd=20),(""ray::fib"",pid=29719,fd=20),(""ray::fib"",pid=29716,fd=20),(""ray::fib"",pid=29715,fd=20),(""ray::fib"",pid=29714,fd=20),(""ray::fib"",pid=29597,fd=20),(""ray::fib"",pid=29596,fd=20),(""ray::fib"",pid=29589,fd=20),(""ray::fib"",pid=29535,fd=20),(""ray::fib"",pid=29496,fd=20),(
""ray::fib"",pid=29495,fd=20),(""ray::fib"",pid=29375,fd=20),(""ray::fib"",pid=29367,fd=20),(""ray::fib"",pid=29254,fd=20),(""ray::fib"",pid=29253,fd=20),(""ray::fib"",pid=29252,fd=20),(""ray::fib"",pid=29226,fd=20),(""ray::fib"",pid=29169,fd=20),(""ray::fib"",pid=29113,fd=20),(""ray::fib"",pid=29108,fd=20),(""ray::fib"",pid=29106,fd=20),(""ray::fib"",pid=29001,fd=20),(""ray::fib"",pid=2
8998,fd=20),(""ray::fib"",pid=28894,fd=20),(""ray::fib"",pid=28893,fd=20),(""ray::fib"",pid=28785,fd=20),(""ray::IDLE"",pid=28784,fd=20),(""ray::fib"",pid=28783,fd=20),(""ray::fib"",pid=28782,fd=20),(""ray::fib"",pid=28658,fd=20),(""ray::fib"",pid=28651,fd=20),(""ray::fib"",pid=28650,fd=20),(""ray::fib"",pid=28548,fd=20),(""ray::fib"",pid=28546,fd=20),(""ray::fib"",pid=28544,fd=20),(""r
ay::fib"",pid=28439,fd=20),(""ray::fib"",pid=28437,fd=20),(""ray::fib"",pid=28436,fd=20),(""ray::fib"",pid=28333,fd=20),(""ray::fib"",pid=28312,fd=20),(""ray::fib"",pid=28220,fd=20),(""ray::fib"",pid=28117,fd=20),(""ray::fib"",pid=28097,fd=20),(""ray::fib"",pid=28047,fd=20),(""ray::fib"",pid=27907,fd=20),(""ray::fib"",pid=27879,fd=20),(""ray::fib"",pid=27784,fd=20),(""ray::fib"",pid=277
83,fd=20),(""ray::fib"",pid=27782,fd=20),(""ray::fib"",pid=27742,fd=20),(""ray::fib"",pid=27693,fd=20),(""ray::IDLE"",pid=27618,fd=20),(""ray::fib"",pid=27566,fd=20),(""ray::fib"",pid=27516,fd=20),(""ray::fib"",pid=27471,fd=20),(""ray::fib"",pid=27426,fd=20),(""ray::fib"",pid=27326,fd=20),(""ray::fib"",pid=27232,fd=20),(""ray::fib"",pid=27225,fd=20),(""ray::fib"",pid=27209,fd=20),(""ray
::fib"",pid=27107,fd=20),(""ray::fib"",pid=27094,fd=20),(""ray::fib"",pid=26983,fd=20),(""ray::fib"",pid=26963,fd=20),(""ray::fib"",pid=26922,fd=20),(""ray::fib"",pid=26858,fd=20),(""ray::fib"",pid=26732,fd=20),(""ray::fib"",pid=26682,fd=20),(""ray::fib"",pid=26548,fd=20),(""ray::IDLE"",pid=26535,fd=20),(""ray::fib"",pid=26490,fd=20),(""ray::fib"",pid=26374,fd=20),(""ray::fib"",pid=2633
5,fd=20),(""ray::fib"",pid=26226,fd=20),(""ray::fib"",pid=26225,fd=20),(""ray::fib"",pid=26224,fd=20),(""ray::fib"",pid=26117,fd=20),(""ray::fib"",pid=26116,fd=20),(""ray::fib"",pid=26114,fd=20),(""ray::fib"",pid=26002,fd=20),(""ray::fib"",pid=25919,fd=20),(""ray::fib"",pid=25888,fd=20),(""ray::fib"",pid=25844,fd=20),(""ray::fib"",pid=25742,fd=20),(""ray::fib"",pid=25717,fd=20),(""ray::
fib"",pid=25654,fd=20),(""ray::fib"",pid=25626,fd=20),(""ray::fib"",pid=25601,fd=20),(""ray::fib"",pid=25544,fd=20),(""ray::fib"",pid=25499,fd=20),(""ray::fib"",pid=25464,fd=20),(""ray::fib"",pid=25463,fd=20),(""ray::fib"",pid=25419,fd=20),(""ray::fib"",pid=25338,fd=20),(""ray::IDLE"",pid=25203,fd=20),(""ray::fib"",pid=25202,fd=20),(""ray::fib"",pid=25201,fd=20),(""ray::fib"",pid=25095,
fd=20),(""ray::fib"",pid=25090,fd=20),(""ray::fib"",pid=24980,fd=20),(""ray::fib"",pid=24874,fd=20),(""ray::fib"",pid=24841,fd=20),(""ray::fib"",pid=24840,fd=20),(""ray::fib"",pid=24815,fd=20),(""ray::fib"",pid=24771,fd=20),(""ray::fib"",pid=24734,fd=20),(""ray::fib"",pid=24594,fd=20),(""ray::fib"",pid=24423,fd=20),(""ray::fib"",pid=24388,fd=20),(""ray::fib"",pid=24307,fd=20),(""ray::fi
b"",pid=24288,fd=20),(""ray::fib"",pid=24275,fd=20),(""ray::fib"",pid=24222,fd=20),(""ray::fib"",pid=24209,fd=20),(""ray::fib"",pid=24169,fd=20),(""ray::fib"",pid=24094,fd=20),(""ray::fib"",pid=24033,fd=20),(""ray::fib"",pid=24025,fd=20),(""ray::IDLE"",pid=23963,fd=20),(""ray::fib"",pid=23786,fd=20),(""ray::fib"",pid=23676,fd=20),(""ray::fib"",pid=23569,fd=20),(""ray::fib"",pid=23564,fd
=20),(""ray::fib"",pid=23455,fd=20),(""ray::fib"",pid=23323,fd=20),(""ray::fib"",pid=23208,fd=20),(""ray::fib"",pid=23206,fd=20),(""ray::fib"",pid=23098,fd=20),(""ray::fib"",pid=20513,fd=20),(""ray::fib"",pid=18437,fd=20),(""ray::fib"",pid=17054,fd=20),(""ray::fib"",pid=15301,fd=20),(""ray::IDLE"",pid=15087,fd=20),(""ray::fib"",pid=14330,fd=20),(""ray::fib"",pid=14220,fd=20),(""ray::fib
"",pid=14003,fd=20),(""ray::IDLE"",pid=13785,fd=20),(""ray::fib"",pid=13784,fd=20),(""ray::fib"",pid=13561,fd=20),(""ray::fib"",pid=13387,fd=20),(""ray::fib"",pid=12945,fd=20),(""ray::fib"",pid=12832,fd=20),(""ray::fib"",pid=12690,fd=20),(""ray::IDLE"",pid=12342,fd=20),(""ray::fib"",pid=12341,fd=20),(""ray::fib"",pid=12113,fd=20),(""ray::fib"",pid=12029,fd=20),(""ray::fib"",pid=11928,fd
=20),(""ray::fib"",pid=11774,fd=20),(""ray::fib"",pid=11509,fd=20),(""ray::fib"",pid=11508,fd=20),(""ray::fib"",pid=11418,fd=20),(""ray::fib"",pid=11347,fd=20),(""ray::IDLE"",pid=11186,fd=20),(""ray::fib"",pid=11139,fd=20),(""ray::fib"",pid=10893,fd=20),(""ray::fib"",pid=10409,fd=20),(""ray::fib"",pid=10305,fd=20),(""ray::fib"",pid=10188,fd=20),(""ray::fib"",pid=9850,fd=20),(""ray::fib""
,pid=9535,fd=20),(""ray::fib"",pid=9055,fd=20),(""ray::fib"",pid=8833,fd=20),(""ray::fib"",pid=8773,fd=20),(""ray::fib"",pid=8719,fd=20),(""ray::fib"",pid=8666,fd=20),(""ray::fib"",pid=8653,fd=20),(""ray::fib"",pid=8435,fd=20),(""ray::fib"",pid=8404,fd=20),(""ray::fib"",pid=8350,fd=20),(""ray::fib"",pid=8307,fd=20),(""ray::fib"",pid=8305,fd=20),(""ray::fib"",pid=8236,fd=20),(""ray::IDLE
"",pid=8235,fd=20),(""ray::fib"",pid=8231,fd=20),(""ray::fib"",pid=8087,fd=20),(""ray::fib"",pid=7975,fd=20),(""ray::fib"",pid=7864,fd=20),(""ray::fib"",pid=7862,fd=20),(""ray::IDLE"",pid=7860,fd=20),(""ray::fib"",pid=7859,fd=20),(""ray::fib"",pid=7749,fd=20),(""ray::fib"",pid=7746,fd=20),(""ray::fib"",pid=7677,fd=20),(""ray::fib"",pid=7665,fd=20),(""ray::fib"",pid=7605,fd=20),(""ray::fi
b"",pid=7604,fd=20),(""ray::fib"",pid=7490,fd=20),(""ray::fib"",pid=7461,fd=20),(""ray::fib"",pid=7379,fd=20),(""ray::fib"",pid=7354,fd=20),(""ray::fib"",pid=7232,fd=20),(""ray::fib"",pid=7229,fd=20),(""ray::fib"",pid=7119,fd=20),(""ray::fib"",pid=7108,fd=20),(""ray::fib"",pid=7082,fd=20),(""ray::fib"",pid=7006,fd=20),(""ray::fib"",pid=6998,fd=20),(""ray::fib"",pid=6997,fd=20),(""ray::fi
b"",pid=6970,fd=20),(""ray::fib"",pid=6917,fd=20),(""ray::fib"",pid=6886,fd=20),(""ray::fib"",pid=6885,fd=20),(""ray::fib"",pid=6778,fd=20),(""ray::fib"",pid=6777,fd=20),(""ray::fib"",pid=6730,fd=20),(""ray::fib"",pid=6664,fd=20),(""ray::fib"",pid=6663,fd=20),(""ray::fib"",pid=6662,fd=20),(""ray::fib"",pid=6592,fd=20),(""ray::fib"",pid=6547,fd=20),(""ray::IDLE"",pid=6546,fd=20),(""ray::f
ib"",pid=6491,fd=20),(""ray::fib"",pid=6436,fd=20),(""ray::fib"",pid=6434,fd=20),(""ray::fib"",pid=6284,fd=20),(""ray::fib"",pid=6277,fd=20),(""ray::fib"",pid=6174,fd=20),(""ray::fib"",pid=6169,fd=20),(""ray::fib"",pid=6164,fd=20),(""ray::fib"",pid=6054,fd=20),(""ray::fib"",pid=5939,fd=20),(""ray::fib"",pid=5934,fd=20),(""ray::fib"",pid=5829,fd=20),(""ray::fib"",pid=5828,fd=20),(""ray::f
ib"",pid=5824,fd=20),(""ray::fib"",pid=5823,fd=20),(""ray::fib"",pid=5717,fd=20),(""ray::fib"",pid=5712,fd=20),(""ray::fib"",pid=5698,fd=20),(""ray::fib"",pid=5606,fd=20),(""ray::fib"",pid=5601,fd=20),(""ray::fib"",pid=5469,fd=20),(""ray::fib"",pid=5468,fd=20),(""ray::IDLE"",pid=5371,fd=20),(""ray::fib"",pid=5370,fd=20),(""ray::fib"",pid=5345,fd=20),(""ray::fib"",pid=5263,fd=20),(""ray::
IDLE"",pid=5260,fd=20),(""ray::IDLE"",pid=5108,fd=20),(""python3"",pid=206,fd=20),(""python3"",pid=204,fd=20),(""raylet"",pid=168,fd=20))
```

it looks like all these processes listened on/established to a same socket. It may be inherited from raylet",ray cluster action state local address port peer address port process listen ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray pi ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ay ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray fib ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray ray idle ray python python raylet like socket may raylet,issue,negative,positive,neutral,neutral,positive,positive
1809529632,The new release test pipeline fixed this,new release test pipeline fixed,issue,negative,positive,positive,positive,positive,positive
1809528196,"I can also use another config in the otherworld..

I think the removal of `core_cpp` tags is going to be good anyways.",also use another otherworld think removal going good anyways,issue,negative,positive,positive,positive,positive,positive
1809526926,"it computes tags in the otherworld, and when it computes tags, it does not have an easy way to run everything..",otherworld easy way run everything,issue,negative,positive,positive,positive,positive,positive
1809526388,I think on postmerge the bootstraper doesn't even compute tags which default to run everything regardless of tags,think even compute default run everything regardless,issue,negative,neutral,neutral,neutral,neutral,neutral
1809523888,I am actually not sure why `core_cpp` needs to trigger docker build (but not building the wheel). I feel that it should only trigger building the wheel. the docker build simply extracts the wheel inside the container and does nothing much else.,actually sure need trigger docker build building wheel feel trigger building wheel docker build simply wheel inside container nothing much else,issue,negative,positive,positive,positive,positive,positive
1809490480,I am facing the same question.I can train my project with DQN or DDPG. But I always get nan for rewards with PPO_LSTM.,facing train project always get nan,issue,negative,neutral,neutral,neutral,neutral,neutral
1809460737,"> The code looks good to me. Should there be a corresponding update in the Ray documentation?

Sure. Will update the document in next PR. @architkulkarni ",code good corresponding update ray documentation sure update document next,issue,positive,positive,positive,positive,positive,positive
1809445937,"> Thank you for raising this @kasakun but we typically don't maintain older documentation versions; is this still an issue on latest ray release docs?

No it isn’t since the legacy monitor is completely removed. We can close the ticket",thank raising typically maintain older documentation still issue latest ray release since legacy monitor completely removed close ticket,issue,negative,positive,positive,positive,positive,positive
1809437733,"Spent some time thinking about this, and I'm not sure if there's any easy way to implement this right now. Downgrading to P2",spent time thinking sure easy way implement right,issue,positive,positive,positive,positive,positive,positive
1809433721,"Here's a repro:

```python
import numpy as np
from PIL import Image

import ray

l_image = Image.fromarray(np.zeros((32, 32), dtype=np.uint8))
rgb_image = Image.fromarray(np.zeros((32, 32, 3), dtype=np.uint8))

l_image.save(""/tmp/l.png"")
rgb_image.save(""/tmp/rgb.png"")

ds = ray.data.read_images([""/tmp/l.png"", ""/tmp/rgb.png""])
ds.take_batch(batch_size=2)
```

```
ValueError: ArrowVariableShapedTensorArray only supports tensor elements that all have the same number of dimensions, but got tensor elements with dimensions: 2, 3
```",python import import image import ray tensor number got tensor,issue,negative,neutral,neutral,neutral,neutral,neutral
1809393868,"I am pretty sure that I am running into the same issue here.

Issue description: https://discuss.ray.io/t/no-request-can-complete-until-all-requests-are-ready/12772?u=jacob_summers

Possible workarounds: 

* https://discuss.ray.io/t/no-request-can-complete-until-all-requests-are-ready/12772/8?u=jacob_summers
* https://discuss.ray.io/t/no-request-can-complete-until-all-requests-are-ready/12772/9?u=jacob_summers

Suggestions to resolve this issue: https://discuss.ray.io/t/no-request-can-complete-until-all-requests-are-ready/12772/10?u=jacob_summers",pretty sure running issue issue description possible resolve issue,issue,positive,positive,positive,positive,positive,positive
1809371158,"I think I've addressed all the feedback from the docs team. I'll split this PR into reviewable chunks against a new branch, then we'll merge that branch to `master` at which point I'll close this PR.",think feedback team split reviewable new branch merge branch master point close,issue,negative,positive,positive,positive,positive,positive
1809367609,"You should not use --no-monitor when you do ray start unless you use kuberay (it is a private API, and several features won't work). Kuberay uses this flag it because it starts the monitor separately in a sidecar container. ",use ray start unless use private several wo work flag monitor separately sidecar container,issue,negative,neutral,neutral,neutral,neutral,neutral
1809339043,"So if async -> use async context var

if not -> use worker context (it should work with both threaded/normal actor)",use context use worker context work actor,issue,negative,neutral,neutral,neutral,neutral,neutral
1809338443,I think we can make the API go through raylet.pyx and always manually pass task_id,think make go always manually pas,issue,negative,neutral,neutral,neutral,neutral,neutral
1809335144,"> The worker process setup hook is more of an internal API so it should be okay.

Nit: It is not an internal API. It is a public one and is already documented in for [configuring logging](https://docs.ray.io/en/latest/ray-observability/user-guides/configure-logging.html#structured-logging).

Other changes LGTM.",worker process setup hook internal nit internal public one already logging,issue,negative,neutral,neutral,neutral,neutral,neutral
1809334890,"Currently, you can access the task id from 

```
# This argument is used to obtain the correct task id inside
# an asyncio task. It is because task_id can be obtained
# by the worker_context_ API, which is per thread, not per
# asyncio task. TODO(sang): We should properly fix it.
# Note that the context var is recommended to be defined
# in the top module.
# https://docs.python.org/3/library/contextvars.html#contextvars.ContextVar
# It is thread-safe.
async_task_id = contextvars.ContextVar('async_task_id', default=None)
```

this inside raylet.pyx",currently access task id argument used obtain correct task id inside task per thread per task sang properly fix note context defined top module inside,issue,negative,positive,positive,positive,positive,positive
1809314431,Comparing the processing time for the task events thread with release tests: https://buildkite.com/ray-project/release/builds/1074,time task thread release,issue,negative,neutral,neutral,neutral,neutral,neutral
1809293863,"> My current intuition is that we will need to pass the task id in (rather than getting it from the worker context which is not synchronized with async actor's task switching since async tasks run in the same thread, thus the same worker context since it's now thread local).  
> 
> 

That works, I actually thought of that and tried looking for any example of getting the task Id from the python land to pass it but I couldn't find it.
",current intuition need pas task id rather getting worker context synchronized actor task switching since run thread thus worker context since thread local work actually thought tried looking example getting task id python land pas could find,issue,negative,neutral,neutral,neutral,neutral,neutral
1809278316,"@rickyyx yeah good point, I'm codeowner but I can't approve my own PR... We'll figure out a better solution (maybe @hongchaodeng we can add you as another codeowner for OSS cluster launcher?)",yeah good point ca approve figure better solution maybe add another cluster launcher,issue,positive,positive,positive,positive,positive,positive
1809264405,"> It is because whenever new task is submitted, it overwrites the ""current_task_id"" from the worker context to the newly submitted task (it is a bug)

I thought it's the task id for ""executing task"" that matters here tho? Why would task submission affects current task id. @rkooo567 wanna sync offline for this? I feel this is ultimately the same issue we ran into with task logs for async actors. 

> Can you point me to the work around? I will also add the E2E tests.

My current intuition is that we will need to pass the task id in (rather than getting it from the worker context which is not synchronized with async actor's task switching since async tasks run in the same thread, thus the same worker context since it's now thread local).  
",whenever new task worker context newly task bug thought task id task tho would task submission current task id wan na sync feel ultimately issue ran task point work around also add current intuition need pas task id rather getting worker context synchronized actor task switching since run thread thus worker context since thread local,issue,negative,positive,neutral,neutral,positive,positive
1809254555,">  recall you mentioning elsewhere that Ray will eventually build out pagination support, at which point we can use that to fix our problem. Do you have any other tips for being able to support our scenario of simply aggregating running, finished, passed jobs as above while working around the 10000 reporting limit?

Long term: We are planning to do it maybe next year for pagination. 
Short term: We will likely allow configuerable limit > 10K for returning results (at the cost of higher latency) for users. 
",recall elsewhere ray eventually build pagination support point use fix problem able support scenario simply running finished working around limit long term maybe next year pagination short term likely allow limit cost higher latency,issue,positive,positive,neutral,neutral,positive,positive
1809246794,"Hi yes I fixed this already, just forgot to close the issue. Still yellow but a much more acceptable level.",hi yes fixed already forgot close issue still yellow much acceptable level,issue,positive,positive,positive,positive,positive,positive
1809245500,@anyscalesam we moved it back to civ1 to see if it becomes less flaky but I think it's still flaky; let's investigate otherwise we can decide if we want to jail it in civ2,back see becomes le flaky think still flaky let investigate otherwise decide want jail,issue,negative,negative,neutral,neutral,negative,negative
1809245399,@can-anyscale FYI we're lowering this P1 for now as per go/flaky this test is more yellow than red; to wrap up this CY we're going to focus on just the red flakies.,lowering per test yellow red wrap going focus red,issue,negative,neutral,neutral,neutral,neutral,neutral
1809243714,Client lower than Linux red flaky; lowering to P1.,client lower red flaky lowering,issue,negative,neutral,neutral,neutral,neutral,neutral
1809241607,Discussed - let's at least root cause to know the why of this first. cc @rynewang to take point on this.,let least root cause know first take point,issue,negative,negative,neutral,neutral,negative,negative
1809238873,"Overall ""flakiness"" is yellow; we are going to focus on reds for Linux first.",overall flakiness yellow going focus first,issue,negative,positive,neutral,neutral,positive,positive
1809235935,"Reviewed with @rkooo567 @rynewang @vitsai > let's decide whether we should fix this in autoscaler v1.

This is fixed in v2 but we have an interim state in ray29 where default may still be autosclaer v1 in which case this issue will be there.

Next steps lets decide do we just skip/[ush to autoscaler v2 in ray210 or fix this regression.",let decide whether fix fixed interim state ray default may still case issue next decide ush ray fix regression,issue,negative,positive,neutral,neutral,positive,positive
1809231082,"If you use `OPENBLAS_NUM_THREADS=2 python` you also get the issue.

```
Stack (most recent call first):
  File ""/opt/homebrew/lib/python3.9/site-packages/numpy/linalg/linalg.py"", line 561 in inv
  File ""/opt/homebrew/lib/python3.9/site-packages/numpy/lib/polynomial.py"", line 680 in polyfit
  File ""/opt/homebrew/lib/python3.9/site-packages/numpy/__init__.py"", line 386 in _mac_os_check
  File ""/opt/homebrew/lib/python3.9/site-packages/numpy/__init__.py"", line 392 in <module>
  File ""<frozen importlib._bootstrap>"", line 228 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 850 in exec_module
  File ""<frozen importlib._bootstrap>"", line 680 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 986 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1007 in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 228 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 1181 in exec_module
  File ""<frozen importlib._bootstrap>"", line 680 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 986 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1007 in _find_and_load
  File ""/opt/homebrew/lib/python3.9/site-packages/pyarrow/__init__.py"", line 65 in <module>
  File ""<frozen importlib._bootstrap>"", line 228 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 850 in exec_module
  File ""<frozen importlib._bootstrap>"", line 680 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 986 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1007 in _find_and_load
  File ""/Users/ruiyangwang/gits/ray/python/ray/_private/arrow_serialization.py"", line 46 in _register_custom_datasets_serializers
  File ""/Users/ruiyangwang/gits/ray/python/ray/util/serialization_addons.py"", line 86 in apply
  File ""/Users/ruiyangwang/gits/ray/python/ray/_private/serialization.py"", line 153 in __init__
  File ""/Users/ruiyangwang/gits/ray/python/ray/_private/worker.py"", line 628 in get_serialization_context
  File ""/Users/ruiyangwang/gits/ray/python/ray/_private/worker.py"", line 740 in deserialize_objects
```",use python also get issue stack recent call first file line file line file line file line module file frozen line file frozen line file frozen line file frozen line file frozen line file frozen line file frozen line file frozen line file frozen line file frozen line file line module file frozen line file frozen line file frozen line file frozen line file frozen line file line file line apply file line file line file line,issue,negative,positive,positive,positive,positive,positive
1809230318,Good point. The worker process setup hook is more of an internal API so it should be okay.,good point worker process setup hook internal,issue,negative,positive,positive,positive,positive,positive
1809229250,"Discussed - one possible mitigation is to remove pyarrow dependency for core lib. believe this is only an issue on macos.

cc @rkooo567  to attempt repro > if it repros can you flag as p0; if not p1.",one possible mitigation remove dependency core believe issue attempt flag,issue,negative,neutral,neutral,neutral,neutral,neutral
1809220773,Plasma store memory (SHR) is not expected to be released; we allocate upfront and it is reused (roughly 30% of memory alloc). cc @rkooo567 ,plasma store memory allocate roughly memory,issue,negative,negative,neutral,neutral,negative,negative
1809215264,Thank you for raising this @kasakun but we typically don't maintain older documentation versions; is this still an issue on latest ray release docs?,thank raising typically maintain older documentation still issue latest ray release,issue,negative,positive,positive,positive,positive,positive
1809213196,"Thanks @gresavage - where are you finding the tf2.11 pin for ray271? ray shouldn't have any dependency on a specific 271 version.

here is the 271 lib dependency list https://github.com/ray-project/ray/blob/releases/2.7.1/python/setup.py#L38",thanks finding pin ray ray dependency specific version dependency list,issue,negative,positive,neutral,neutral,positive,positive
1809202126,"Since the team has no bandwidth, and most of common path is fixed, we will dowograde the priority for now. We will address the rest by Ray 2.10",since team common path fixed priority address rest ray,issue,negative,negative,neutral,neutral,negative,negative
1809201066,"Note when I try to run the numpy's _mac_os_check function directly, or to just import it directly it works. There are some stackoverflow issues saying it's related to some thread issues somehow.",note try run function directly import directly work saying related thread somehow,issue,negative,positive,neutral,neutral,positive,positive
1809190373,"Issue from OpenBlas + MacOS. Can be patched by a env var `OPENBLAS_NUM_THREADS=1`.

Side by side running the script in

```
OPENBLAS_NUM_THREADS=1 python
```

works fine, while 

```
python
```

fails as sigbus.

",issue side side running script python work fine python,issue,negative,positive,positive,positive,positive,positive
1809183166,"It's nice to have, but I don't think the team has bandwidth right now",nice think team right,issue,negative,positive,positive,positive,positive,positive
1809126939,Can reproduce the bug error on my macos laptop. Can NOT reproduce on Anyscale workspace (nightly). looking deeper,reproduce bug error reproduce nightly looking,issue,negative,neutral,neutral,neutral,neutral,neutral
1809108742,"This is probably the patch needed. Note that the field already takes a string in the type signature, but it doesn't seem to be properly supported.

```
diff --git a/python/ray/_private/runtime_env/setup_hook.py b/python/ray/_private/runtime_env/setup_hook.py
index afc718dc61..9daee51d51 100644
--- a/python/ray/_private/runtime_env/setup_hook.py
+++ b/python/ray/_private/runtime_env/setup_hook.py
@@ -7,6 +7,7 @@ from typing import Dict, Any, Callable, Union, Optional
 
 import ray
 import ray._private.ray_constants as ray_constants
+from ray._private.storage import _load_class
 import ray.cloudpickle as pickle
 from ray.runtime_env import RuntimeEnv
 
@@ -54,6 +55,8 @@ def upload_worker_process_setup_hook_if_needed(
     if setup_func is None:
         return runtime_env
 
+    if isinstance(setup_func, str):
+        setup_func = _load_class(setup_func)
     if not isinstance(setup_func, Callable):
         raise TypeError(
             ""worker_process_setup_hook must be a function, "" f""got {type(setup_func)}.""
```",probably patch note field already string type signature seem properly git index import callable union optional import ray import import import pickle import none return callable raise must function got type,issue,negative,neutral,neutral,neutral,neutral,neutral
1809102212,"> > How about
> > 
> > * head_setup_commands
> > * worker_setup_commands
> > * head_start_ray_commands
> > * worker_start_ray_commands
> 
> I think in this case we can omit `node` for (1) brevity, and (2) because it's not as confusing. (Commands must be run on a node, it doesn't make as much sense to run commands on a process). What do you think?
> 
> My impression is max_workers and min_workers (out of context) are by far the most confusing, and everything else is pretty minor and not worth changing

My only concern is that people might confuse it with something like `worker_process_setup_hook`. However, fine with not touching it for now and addressing them if we hear feedback",think case omit node brevity must run node make much sense run process think impression context far everything else pretty minor worth concern people might confuse something like however fine touching hear feedback,issue,positive,positive,positive,positive,positive,positive
1809098155,"> RE @scottsun94
> 
> > Before we are able to fix this issue on dashboard side, a workaround will be customizing the formatter of the serve logger and remove the redundant or add metadata you want to? @JakeSummers
> 
> I looked briefly into how to do that, but it wasn't obvious to me. Can you point me in the right direction?

@sihanwang41 do you have any pointer? Or what's the best way to do it?",able fix issue dashboard side serve logger remove redundant add want briefly obvious point right direction pointer best way,issue,positive,positive,positive,positive,positive,positive
1809082497,This was something observed as part of Data CUJ user study; let's triage and prioritize everything once we get the current round of user studies done this week @Wendi-anyscale @raulchen @c21 ,something part data user study let triage everything get current round user done week,issue,negative,negative,neutral,neutral,negative,negative
1809021535,"RE @scottsun94 

> Before we are able to fix this issue on dashboard side, a workaround will be customizing the formatter of the serve logger and remove the redundant or add metadata you want to? @JakeSummers

I looked briefly into how to do that, but it wasn't obvious to me.  Can you point me in the right direction?  

",able fix issue dashboard side serve logger remove redundant add want briefly obvious point right direction,issue,negative,positive,positive,positive,positive,positive
1809006275,"> test_parent_task_id_concurrent_actor

OK, I can reproduce the error.

```
(AsyncActor pid=2581588) [2023-11-13 20:18:27,594 C 2581588 2581653] core_worker.cc:4332:  Check failed: current_task We should have set the current task spec before executing the task.
```


```python3
@ray.remote
class AsyncActor:
    async def paused(self):
        print(""paused"")
        with ray._private.worker.global_worker.task_paused_by_debugger():
            print(""waiting"")
            await asyncio.sleep(60)
        return ""hi""

    async def not_paused(self):
        print(""waiting"")
        await asyncio.sleep(60)
        return ""hi""

a = AsyncActor.remote()


actor = AsyncActor.options(max_concurrency=2).remote()

ref1 = actor.not_paused.remote()
ref2 = actor.paused.remote()

print(ray.get([ref1, ref2]))
```

Can you point me to the work around? I will also add the E2E tests.",reproduce error check set current task spec task python class self print print waiting await return hi self print waiting await return hi actor ref ref print ref ref point work around also add,issue,negative,neutral,neutral,neutral,neutral,neutral
1809004701,"[This change](https://github.com/ray-project/ray/pull/40831) removed the monkeypatch workaround, so this failure shouldn't occur on `master`.",change removed failure occur master,issue,negative,negative,negative,negative,negative,negative
1808999230,"For now, I'd recommend using Pydantic 2.4. We're planning on adding Pydantic 2 support in Ray 2.9, so that should fix this issue. See https://github.com/ray-project/ray/issues/38977 for more info.",recommend support ray fix issue see,issue,positive,neutral,neutral,neutral,neutral,neutral
1808957004,"> How about
> 
> * head_setup_commands
> * worker_setup_commands
> * head_start_ray_commands
> * worker_start_ray_commands

I think in this case we can omit `node` for (1) brevity, and (2) because it's not as confusing.  (Commands must be run on a node, it doesn't make as much sense to run commands on a process). What do you think?

My impression is max_workers and min_workers (out of context) are by far the most confusing, and everything else is pretty minor and not worth changing",think case omit node brevity must run node make much sense run process think impression context far everything else pretty minor worth,issue,positive,positive,positive,positive,positive,positive
1808945042,"@architkulkarni the test runners use anyscale staging; I think if we change the compute config format in ray, it need to sync with anyscale in someway; maybe Lanbo knows?",test use staging think change compute format ray need sync someway maybe,issue,negative,neutral,neutral,neutral,neutral,neutral
1808919520,"confirming that this is a dupe of https://github.com/ray-project/ray/issues/38300; since that issue was created first and has other discussion, we will move to discuss the topic there.",confirming dupe since issue first discussion move discus topic,issue,negative,positive,positive,positive,positive,positive
1808918053,the p0 use case where we drop the columns is done; there is further juice to squeeze... >> we'll decide offline on whether or not to keep additional investment in here for ray210.,use case drop done juice squeeze decide whether keep additional investment ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1808913153,part of trend of aggregating Blocks of different type; for consideration as a larger project for ray210,part trend different type consideration project ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1808894736,"This issue may have been fixed already since we had some failures in related tests. Let me trigger the test now, and will try to investigate if it's still an issue.",issue may fixed already since related let trigger test try investigate still issue,issue,negative,positive,neutral,neutral,positive,positive
1808894522,Reviewed - for Ray 2.10 we should think of a holistic fix for all of these multi block type shuffle issues. cc @c21 ,ray think holistic fix block type shuffle,issue,negative,neutral,neutral,neutral,neutral,neutral
1808881796,@c21 please take a look; let's see if we can aim for ray29.,please take look let see aim ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1808881325,You might already know but those are tagged with `unstable-release-test` are not release blocking.,might already know tagged release blocking,issue,negative,neutral,neutral,neutral,neutral,neutral
1808877095,Not quite started yet; might need to juggle on bandwidth but this is important.,quite yet might need juggle important,issue,negative,positive,positive,positive,positive,positive
1808857986,"> - This PR does **not** update the `min_workers`/`max_workers` fields in the compute templates in the `release` directory.  The reason is that the cluster launcher for these tests do not appear to run on Ray master, so the compute template gets [rejected](https://buildkite.com/ray-project/release-tests-pr/builds/57898#018b922c-8be1-4487-8b89-2affb02b39f8/424-553) with `""body.config.worker_node_types.0.max_worker_nodes: extra fields not permitted.` 

@can-anyscale regarding this: do you know the test runners run on the latest Ray release? If so, I think it would make sense for me to make a follow-up PR after Ray 2.9 is released that updates the fields in all the compute template YAML files in the `release/` directory.",update compute release directory reason cluster launcher appear run ray master compute template extra regarding know test run latest ray release think would make sense make ray compute template directory,issue,negative,positive,positive,positive,positive,positive
1808852387,"Still fixing some lingering tests, but this is ready for review.",still fixing ready review,issue,negative,positive,positive,positive,positive,positive
1808784816,@rkooo567 @vdesai2014 were yall able to meet last week; what were the conclusions?,able meet last week,issue,negative,positive,positive,positive,positive,positive
1808526557,"Hey @anyscalesam, @bveeramani. Sorry for not clearing this as I was invested in some other work. I have done as per requested and if any change is required, do tell.",hey sorry clearing work done per change tell,issue,negative,negative,negative,negative,negative,negative
1808522152,"@rkooo567 thanks for your contributions to the project. We run into the hard 10000 truncation limit which was implemented in #26124. We are using Ray parallel tasks and are trying to use the state API to monitor the tasks at a high level using the `summarize_tasks` API. We run a background context manager which uses the state API to print summary entries every 10 seconds, for example:

```
(_monitor pid=889) [76%] Done: 5846 Total: 7727 Pending: 1761 Running: 120 Passed: 5846 Failed: 0 Unknown: 0
(_monitor pid=889) [79%] Done: 6140 Total: 7727 Pending: 1462 Running: 125 Passed: 6140 Failed: 0 Unknown: 0
(_monitor pid=889) [83%] Done: 6445 Total: 7727 Pending: 1168 Running: 114 Passed: 6445 Failed: 0 Unknown: 0
```

We have additional ""stages"" in our program which run new, large batches of new tasks, so we end up hitting the 10000 max limit, and `summarize_tasks` starts truncating output, so we aren't able to calculate statistics of number of running jobs, etc.

I recall you mentioning elsewhere that Ray will eventually build out pagination support, at which point we can use that to fix our problem. Do you have any other tips for being able to support our scenario of simply aggregating running, finished, passed jobs as above while working around the 10000 reporting limit?

We are looking into using the `list_tasks` API with filters, and that should help, but it is also susceptible to the 10000 hard limit.",thanks project run hard truncation limit ray parallel trying use state monitor high level run background context manager state print summary every example done total pending running unknown done total pending running unknown done total pending running unknown additional program run new large new end limit output able calculate statistic number running recall elsewhere ray eventually build pagination support point use fix problem able support scenario simply running finished working around limit looking help also susceptible hard limit,issue,positive,positive,neutral,neutral,positive,positive
1808466012,Tested 10 times and not seeing it failing anymore :) ,tested time seeing failing,issue,negative,neutral,neutral,neutral,neutral,neutral
1808191079,"i think you can see

test_task_event.py

```
test_parent_task_id_non_concurrent_actor
test_parent_task_id_concurrent_actor
```

Basically what you need to check is

1. For a normal actor, we probably don't need e2e tests, but nice to add it here.
2. For a threaded actor, execute 2 tasks concurrently (just call actor.task.remote() twice with num_concurrency=2, and each task just sleeps long time). Add breakpoint to one of tasks and make sure only one task has paused = true
3. For async actor, I think you can trigger the edge case this way;

```python3

class AsyncActor:
    async def f(self, pause):
        await asyncio.sleep(2)
        if pause:
            breakpoint()
        else:
            await asyncio.sleep(10)

a = AsyncActor.remote()
ref1 = a.f.remote(True)
time.sleep(0.5)
ref2 = a.f.remote(False)
# Check if the first task pause = True
```

It is because whenever new task is submitted, it overwrites the ""current_task_id"" from the worker context to the newly submitted task (it is a bug). And your task event always finds the task id from the current task from worker context
",think see basically need check normal actor probably need nice add threaded actor execute concurrently call twice task long time add one make sure one task true actor think trigger edge case way python class self pause await pause else await ref true ref false check first task pause true whenever new task worker context newly task bug task event always task id current task worker context,issue,positive,positive,positive,positive,positive,positive
1808155891,cc @pcmoritz do you need help taking this issue btw? Or do you think you will have bandwidth to merge it?,need help taking issue think merge,issue,negative,neutral,neutral,neutral,neutral,neutral
1808153701,I will downgrade the priority as V1 fix is less prioritized. Does it sound okay? ,downgrade priority fix le sound,issue,negative,positive,positive,positive,positive,positive
1807452919,"> Code looks good! I think it may not work with async actor (due to ray core's tech debt). We can discuss how to enable it in that case after adding unit tests

@rkooo567 i just added unit tests, what's the case that wouldn't work for Async actors?",code good think may work actor due ray core tech debt discus enable case unit added unit case would work,issue,negative,positive,positive,positive,positive,positive
1807424942,"@rynewang @jjyao instead of waiting for them to repro, can we actively repro it ourselves? Writing a similar computation pattern workload and see if we can repro in a long running cluster? ",instead waiting actively writing similar computation pattern see long running cluster,issue,negative,negative,neutral,neutral,negative,negative
1807408039,hmm some of the premerge failures may be related to failures in the master. I will lyk one more time when you need to merge the latest master...,may related master one time need merge latest master,issue,negative,positive,positive,positive,positive,positive
1807407627,I like the new logic after we add head_node == 1 assertion! ,like new logic add assertion,issue,negative,positive,positive,positive,positive,positive
1807253999,"Yes, I got the same error when running it inside VSCode and the powershell command prompt. Thank you for your effort to help!",yes got error running inside command prompt thank effort help,issue,positive,neutral,neutral,neutral,neutral,neutral
1807251453,"I think that is correct: `AssignProcessToJobObject` will fail inside visualstudio, since it does not like being called when a debugger is active. 

The failure from the powershell command prompt was also inside `AssignProcessToJobObject`?

I am glad you got it working.",think correct fail inside since like active failure command prompt also inside glad got working,issue,negative,negative,negative,negative,negative,negative
1807240488,"I am running it on VSCode and tried to run it in both a python file and the powershell command prompt but was unsuccessful. However, I did get it working in a virtual environment.",running tried run python file command prompt unsuccessful however get working virtual environment,issue,negative,neutral,neutral,neutral,neutral,neutral
1807110695,"Having this problem as well on RLLib 2.7.1. After a few training iterations gradients go to nan:

```
(APPO pid=2911932) ValueError: Expected parameter loc (Tensor of shape (500, 10)) of distribution Normal(loc: torch.Size([500, 10]), scale: torch.Size([500, 10])) to satisfy the constraint Real(), but found invalid values:
(APPO pid=2911932) tensor([[nan, nan, nan,  ..., nan, nan, nan],
(APPO pid=2911932)         [nan, nan, nan,  ..., nan, nan, nan],
(APPO pid=2911932)         [nan, nan, nan,  ..., nan, nan, nan],
(APPO pid=2911932)         ...,
(APPO pid=2911932)         [nan, nan, nan,  ..., nan, nan, nan],
(APPO pid=2911932)         [nan, nan, nan,  ..., nan, nan, nan],
(APPO pid=2911932)         [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',
(APPO pid=2911932)        grad_fn=<SplitBackward0>)
```",problem well training go nan parameter tensor shape distribution normal scale satisfy constraint real found invalid tensor nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan,issue,negative,positive,positive,positive,positive,positive
1807108431,I am getting nan with an APPO - wondering why this issue was closed originally. ,getting nan wondering issue closed originally,issue,negative,positive,positive,positive,positive,positive
1806922059,Is it possible to get the `submission_id` in any way from the running job?,possible get way running job,issue,negative,neutral,neutral,neutral,neutral,neutral
1806884427,"Are you running directly from
- a command prompt inside cmd.exe (this is what I did)
- a powershell command prompt
- visual studio
- jupyter notebook/IDE/something else?",running directly command prompt inside command prompt visual studio else,issue,negative,positive,neutral,neutral,positive,positive
1806831395,"> Thanks for your quick response! I've located the offending line (68) in `python/ray/_private/runtime_env/context.py`. Let me see if I can open a pull request to fix the issue
> 
> https://github.com/ray-project/ray/blob/4444150c2920f7c583a84eadadca649341c80d88/python/ray/_private/runtime_env/context.py#L68

could you please share the code again? Thank you very much!",thanks quick response line let see open pull request fix issue could please share code thank much,issue,positive,positive,positive,positive,positive,positive
1806687290,"I encountered the same problem when running a custom environment and lstm-based model on Ray 2.8.0. I had it print out the batch shapes, and with a bit of cleanup, my error is

```
Failure # 1 (occurred at 2023-11-10_19-43-36)
ray::PPO.train() (pid=10060, ip=10.91.0.26, actor_id=c9854846c60d32b8fc828da401000000, repr=PPO)
  File ""ray/tune/trainable/trainable.py"", line 342, in train
    raise skipped from exception_cause(skipped)
  File ""ray/tune/trainable/trainable.py"", line 339, in train
    result = self.step()
  File ""ray/rllib/algorithms/algorithm.py"", line 853, in step
    results, train_iter_ctx = self._run_one_training_iteration()
  File ""ray/rllib/algorithms/algorithm.py"", line 2854, in _run_one_training_iteration
    results = self.training_step()
  File ""ray/rllib/algorithms/ppo/ppo.py"", line 429, in training_step
    train_batch = synchronous_parallel_sample(
  File ""ray/rllib/execution/rollout_ops.py"", line 101, in synchronous_parallel_sample
    full_batch = concat_samples(all_sample_batches)
  File ""ray/rllib/policy/sample_batch.py"", line 1580, in concat_samples
    return concat_samples_into_ma_batch(samples)
  File ""ray/rllib/policy/sample_batch.py"", line 1731, in concat_samples_into_ma_batch
    out[key] = concat_samples(batches)
  File ""ray/rllib/policy/sample_batch.py"", line 1651, in concat_samples
    raise ValueError(
ValueError: Cannot concat data under key 'obs', b/c sub-structures under that key don't match. `samples`=[SampleBatch(150 (seqs=5): [... snip ...]), SampleBatch(150 (seqs=7): []), SampleBatch(150 (seqs=4): []), SampleBatch(150 (seqs=2): []), SampleBatch(150 (seqs=3): []), SampleBatch(150 (seqs=3): []), SampleBatch(150 (seqs=2): []), SampleBatch(150 (seqs=3): []), SampleBatch(150 (seqs=6): []), SampleBatch(150 (seqs=1): []), SampleBatch(150 (seqs=7): []), SampleBatch(150 (seqs=3): []), SampleBatch(150 (seqs=6): []), SampleBatch(150 (seqs=2): []), SampleBatch(150 (seqs=5): []), SampleBatch(150 (seqs=4): []), SampleBatch(150 (seqs=2): []), SampleBatch(150 (seqs=4): []), SampleBatch(150 (seqs=2): []), SampleBatch(150 (seqs=2): []), SampleBatch(150 (seqs=10): []), SampleBatch(150 (seqs=2): []), SampleBatch(150 (seqs=2): []), SampleBatch(150 (seqs=5): []), SampleBatch(150 (seqs=7): []), SampleBatch(150 (seqs=4): []), SampleBatch(150 (seqs=8): [])]
 Original error:
 all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 99 and the array at index 1 has size 38

Batch shapes: [(5, 99, 1430), (7, 38, 1430), (4, 63, 1430), (2, 122, 1430), (3, 69, 1430), (3, 102, 1430), (2, 104, 1430), (3, 88, 1430), (6, 64, 1430), (1, 150, 1430), (7, 44, 1430), (3, 110, 1430), (6, 36, 1430), (2, 126, 1430), (5, 70, 1430), (4, 99, 1430), (2, 96, 1430), (4, 79, 1430), (2, 123, 1430), (2, 148, 1430), (10, 21, 1430), (2, 132, 1430), (2, 117, 1430), (5, 55, 1430), (7, 51, 1430), (4, 120, 1430), (8, 49, 1430)]
```

The relevant piece of code is

https://github.com/ray-project/ray/blob/8af874e834f8ae9c2bbfa8a9c76d434781c2048c/rllib/policy/sample_batch.py#L1674-L1686

My guess is that it is assuming the T dimension matches across all the tensors when trying to concatenate along B. My understanding is this would work if `s.zero_padded` is True since they'd all have `T = max_seq_len`, but at least in my case this isn't true. The sample batches should then be zero-padded to all have the same T dimension..? (not necessarily `max_seq_len`, but perhaps the max dimension of the batches). Or do I have some configuration wrong that is causing this mismatch?",problem running custom environment model ray print batch bit cleanup error failure ray file line train raise file line train result file line step file line file line file line file line return file line key file line raise data key key match snip original error input array concatenation axis must match exactly along dimension array index size array index size batch relevant piece code guess assuming dimension across trying concatenate along understanding would work true since least case true sample dimension necessarily perhaps dimension configuration wrong causing mismatch,issue,negative,positive,neutral,neutral,positive,positive
1806671869,"@emmyscode 
 
> On the ""Ray Clusters Overview"" page, the ""Learn Key Concepts"" button/card link jumps you down past the title on the ""Key Concepts"" page.

I'm seeing this on <https://docs.ray.io/en/master> as well. This is not due to any change I've made here, but the fact that the `Learn Key Concepts` button links to an anchor that is placed underneath the heading of the page. Really it should be linking to the doc, not an arbitrary sphinx ref. This is a pattern found in many places, but I think it would be best to clean this stuff up after the update is complete.


> On the ""Key Concepts"" page, each heading (i.e. Ray Cluster, Head Node, Worker Node, Autoscaling, and Ray Jobs) when clicked shoots you up to the Ray Cluster heading.

I'm also seeing this on <https://docs.ray.io/en/master>; leaving for future cleanup.

> When I'm on a page, it's not highlighted in the sidenav, which causes me to kind of lose where I am, especially in a deeply nested section like ""Ray Clusters.""
When I go to any subpage of ""API References"" under ""Deploying VMs,"" the sidenav collapses completely and I lose my spot.
> ""API References"" under ""Deploying VMs"" is section, but is not bolded or expandable.
> ""Applications Guide"" is a section, but is not expandable or bolded in the sidenav.
> When I go to a subpage in ""Applications Guide"" the sidenav collapses.
> Accessing any subpage in the ""Ray Cluster Management API"" causes the sidenav to collapse. Also, it's a section, but is not bolded and subitems don't appear in the dropdown.

The rest of these are due to the fact that we don't currently have _every single_ page accessible from the sidebar - it's sort of an arbitrary collection of links at the moment. @angelinalg and I met earlier this week and decided to put every page in the sidebar until we can find a better solution, so for now that's what I'll do. I'll notify once it's ready for you to look at.",ray overview page learn key link past title key page seeing well due change made fact learn key button link anchor underneath heading page really linking doc arbitrary sphinx ref pattern found many think would best clean stuff update complete key page heading ray cluster head node worker node ray ray cluster heading also seeing leaving future cleanup page kind lose especially deeply section like ray go completely lose spot section guide section go guide ray cluster management collapse also section appear rest due fact currently page accessible sort arbitrary collection link moment met week decided put every page find better solution notify ready look,issue,positive,positive,positive,positive,positive,positive
1806581908,@matthewdeng  do you know this this was included in the nightly build? ,know included nightly build,issue,negative,neutral,neutral,neutral,neutral,neutral
1806467502,"Actually, what's wrong with setting a default max_worker to 8 and users can always override it.",actually wrong setting default always override,issue,negative,negative,negative,negative,negative,negative
1806248698,long running many actor tasks passed 5 times: https://buildkite.com/ray-project/release/builds?branch=rickyyx%3Apr-task-gc-gcs,long running many actor time,issue,negative,positive,positive,positive,positive,positive
1806239883,Thanks @justinvyu for the thorough review! I'll try to address your comments early next week.,thanks thorough review try address early next week,issue,negative,positive,positive,positive,positive,positive
1806061312,We do have a check in sphinx called nitpicky mode but we disabled it. Here is the issue that is tracking the task to re-enable it after we clean up the 5000+ broken links: https://github.com/ray-project/ray/issues/39658,check sphinx mode disabled issue task clean broken link,issue,negative,negative,neutral,neutral,negative,negative
1805963986,Give me a week to get this set up and working again and reproduce the test. ,give week get set working reproduce test,issue,negative,neutral,neutral,neutral,neutral,neutral
1805947672,"I ran this to reproduce:
```
conda create -y -n issue29388 python=3.10
conda activate issue29388
conda install -y pip
pip install ""ray[rllib]==2.8.0 gputil
pip install tensorflow torch 
python d:\temp\issue29388.py
```

It installed `tensorflow_intel-2.14.0`, which brought with it `grpcio-1.59.2`. The original reproducer used `gym`, I replaced that with `import gymnasium as gym`. I also added `from gymnasium.wrappers import EnvCompatibility` and changed the `gym.Env` base class to  `EnvCompatibility`. Now the script runs, but I do not see the `Force kill` text in the logs.

@jbedorf could you update the reproducer and see if the issue still occurs with latest software versions?",ran reproduce create issue activate issue install pip pip install ray pip install torch python brought original reproducer used gym import gymnasium gym also added import base class script see force kill text could update reproducer see issue still latest,issue,negative,positive,neutral,neutral,positive,positive
1805946950,"No, I am not using any remote environment or desktop software. When I started ray, it did not open a gui dialog box for me or say anything about opening ports in the firewall.",remote environment ray open box say anything opening,issue,negative,negative,neutral,neutral,negative,negative
1805827089,I just saw that this is fixed in version 2.8.0. I'm closing the ticket.,saw fixed version ticket,issue,negative,positive,neutral,neutral,positive,positive
1805501935,"I could not reproduce the failure. I tried with python3.11 from python.org and python3.11 from the windows app store, both succeeded in running `python -c""import ray;ray.init()""`. In the past starting ray on a fresh machine opens a gui dialog box around opening ports in the firewall. Are you running in a remote environment or using any remote desktop software?",could reproduce failure tried python python store running python import ray past starting ray fresh machine box around opening running remote environment remote,issue,negative,negative,neutral,neutral,negative,negative
1805401322,"@edoakes @shrekris-anyscale, does that mean that Pydantic 2 support will only happen in Ray 2.9, not 2.8 or earlier? ",mean support happen ray,issue,negative,negative,negative,negative,negative,negative
1804974270,"Is this problem solved now? I met the same problem when I using ""Rllib evaluate --render"" 😢 ",problem met problem evaluate render,issue,negative,neutral,neutral,neutral,neutral,neutral
1804946341,"@matthewdeng 
This comment is still remaining in the 2.3-2.6 version.
It looks removed when 2.7 version update.

It is better to remove the comment from each version, but it is not required as functions.

Sorry for bothering you.
Thank you.",comment still version removed version update better remove comment version sorry thank,issue,positive,neutral,neutral,neutral,neutral,neutral
1804895059,"```


!!! No API stability annotation found for:
--
  | ray.util.accelerators.tpu.pod_name
  | ray.util.accelerators.tpu.pod_worker_count

```

You also need to rebase with master.",stability annotation found also need rebase master,issue,negative,neutral,neutral,neutral,neutral,neutral
1804850364,"We don't do this fast-path anymore. We always just give the checkpoint at `storage_path`, as it's sure to exist.",always give sure exist,issue,negative,positive,positive,positive,positive,positive
1804837535,"@architkulkarni Can you help to merge this? The failing tests are flaky and unrelated 
<img width=""1666"" alt=""image"" src=""https://github.com/ray-project/ray/assets/7553988/63d7f85f-bf50-4a0a-bb05-57dfe96051c3"">
",help merge failing flaky unrelated image,issue,negative,neutral,neutral,neutral,neutral,neutral
1804826404,@jjyao Just from testing it seems theres a communication issue with the head node and worker nodes. I can see through testing printouts that both are started however it'll still produce errors.,testing there communication issue head node worker see testing however still produce,issue,negative,neutral,neutral,neutral,neutral,neutral
1804781789,"@architkulkarni ready to merge, windows telemetry is flacky in master.",ready merge telemetry master,issue,negative,positive,positive,positive,positive,positive
1804728993,We're currently busy with other interrupts like the Arrow nightly failure and `read_images_train_multi_node_gpu` performance regression. We can take a look next week,currently busy like arrow nightly failure performance regression take look next week,issue,negative,negative,neutral,neutral,negative,negative
1804626996,"> One question is, do we have plan to add multi-gpu instances for doctests? I saw we disabled a bunch of tests that require more than 1 GPUs.

I think it's just the one PyTorch Lightning snippet that uses multiple GPUs. 

For cost and maintenance reasons, I'm wondering if we should actually hold off on adding a multi-GPU doctest job. Might not be ideal to run a pipeline step to test a single code snippet. 

If we add more multi-GPU snippets in the future, we can always revisit this.",one question plan add saw disabled bunch require think one lightning snippet multiple cost maintenance wondering actually hold job might ideal run pipeline step test single code snippet add future always revisit,issue,positive,positive,positive,positive,positive,positive
1804579299,"As per https://github.com/ray-project/ray/pull/41020#discussion_r1388207822
We will move mig detection support as follow up PR",per move mig detection support follow,issue,negative,neutral,neutral,neutral,neutral,neutral
1804545410,"@angelinalg at the current state, reviewers and PR owners need to inspect the logs. We need to reduce the check to only changes in the PR for this lint to be high signal.",current state need inspect need reduce check lint high signal,issue,negative,positive,neutral,neutral,positive,positive
1804529528,"I think this issue has been present since the very beginning and though it's very confusing, it doesn't block any users. So I will downgrade to P1 for now",think issue present since beginning though block downgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
1804524595,I think so - I'm pretty sure at this point we've deprecated that args as well. cc @matthewdeng  who might know more.,think pretty sure point well might know,issue,positive,positive,positive,positive,positive,positive
1804396908,"@can-anyscale  I notice for some test the logs are in the artifact but for some it's just not.
For example

https://buildkite.com/ray-project/postmerge/builds/1614#018bb0cb-d4b9-4525-9eae-b1d922823762

test_basic_4 is flakey, but the logs are not there.

https://buildkite.com/ray-project/postmerge/builds/1614#018bb0cb-d4d3-4287-9b01-1dd92b97de96

test_basic_4 is flakey and the logs are there.


Some setup seems not right for the first one.",notice test artifact example setup right first one,issue,negative,positive,positive,positive,positive,positive
1804384818,"From what I understand (I've also been trying to get an RLModule to work on PPO), RLlib flattens observations like how you describe to just a box, and before training I believe it passes in dummy batches of data (all 0s, in the shape of your observation space). I don't know if this is the best way of solving the problem, but in my code I do

```python
from ray.rllib.models.modelv2 import restore_original_dimensions

class MyRLModule(TorchPPORLModule):
    def _forward_train(self, batch: NestedDict, compute_vf=True) -> Mapping[str, Any]:
        output = {}
 
        # Unpack our observation and restore it to the proper shape
        obs = batch[SampleBatch.OBS]
        obs = restore_original_dimensions(
            obs,
            self.config.observation_space.original_space,
            tensorlib=torch
        )
  
        input_dict = {
            SampleBatch.OBS: obs,
            STATE_IN: batch[STATE_IN]
        }
  
        # process input_dict as you'd expect
```",understand also trying get work like describe box training believe dummy data shape observation space know best way problem code python import class self batch output unpack observation restore proper shape batch batch process expect,issue,positive,positive,positive,positive,positive,positive
1804383519,"Hey @can-anyscale, could you take a look at the linkcheck errors? How do you recommend that we figure out whether there are real errors or not?",hey could take look recommend figure whether real,issue,negative,positive,positive,positive,positive,positive
1804375391,@can-anyscale  got it. Thank you! This feature is nice!,got thank feature nice,issue,positive,positive,positive,positive,positive,positive
1804323067,@rkooo567 @iycheng It would be nice to keep this from falling through the cracks. @iycheng do you have any information about the questions above?,would nice keep falling information,issue,negative,positive,positive,positive,positive,positive
1804294045,Could we also report final accuracy to add to the perf dashboard?,could also report final accuracy add dashboard,issue,negative,neutral,neutral,neutral,neutral,neutral
1804279759,@architkulkarni Could you help merge this one? 🙏,could help merge one,issue,negative,neutral,neutral,neutral,neutral,neutral
1804190080,Is there any progress on this? I'm working with the API for persistent storage with the `adlfs` library which is a fsspec filesystem implementation but it fails to load. Issue mentioned here: https://github.com/ray-project/ray/issues/41125,progress working persistent storage library implementation load issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1804168463,I figured out how to fix the broken link. My higher level concern is why we don't have checks in place to help us avoid these broken links.,figured fix broken link higher level concern place help u avoid broken link,issue,negative,negative,negative,negative,negative,negative
1804136215,@angelinalg do you mind upload the screenshot again; i was able to click on the link that you sent,mind able click link sent,issue,negative,positive,positive,positive,positive,positive
1804113892,"Breaks some CI jobs with OOM - hard to debug without access to CI machines, so tagging @matthewdeng for awareness and closing for now",hard without access awareness,issue,negative,negative,negative,negative,negative,negative
1803852100,Hello! I would like to know if there is a way to read this information currently? I can't read action and observation in the training loop.@ArturNiederfahrenhorst,hello would like know way read information currently ca read action observation training loop,issue,negative,positive,neutral,neutral,positive,positive
1803777363,"It seems that if I use ""tf2"" as a framework I get the expected crash. ""tf"" is the only one that doesn't crash, maybe it is because my tensorflow version is 2.13, if this is the case it should probably raise an error when using tf instead of tf2.",use framework get crash one crash maybe version case probably raise error instead,issue,negative,neutral,neutral,neutral,neutral,neutral
1803257592,"Hello @icarusin, @jjyao  I also noticed this problem. I think we should first run code `ray.init()` before `# Performance on Ray task`. This way we could compare the Ray core with native python only in the stage of computation. WDYT? If it's acceptable, I'd like to fix this issue.",hello also problem think first run code performance ray task way could compare ray core native python stage computation acceptable like fix issue,issue,negative,positive,positive,positive,positive,positive
1803128336,"The test is run with a lot of different flavors (normal, redis, minimal, minimal-redis). You need to use the 'show flaky tests' pop up to get to the right job ;)

<img width=""1334"" alt=""Screenshot 2023-11-08 at 8 02 03 PM"" src=""https://github.com/ray-project/ray/assets/128072568/154c4b0c-0505-4a42-942c-ea24de842c82"">
",test run lot different normal minimal need use flaky pop get right job,issue,negative,positive,neutral,neutral,positive,positive
1803118878,"I'm working on this ticket. From the dashboard, it's like this:

<img width=""1826"" alt=""image"" src=""https://github.com/ray-project/ray/assets/74173148/b41ea5cd-fbeb-40ab-80a2-d5f8f4b8737c"">


So I check the PR and the build, but it seems test_basic_4 looks ok:

<img width=""1164"" alt=""image"" src=""https://github.com/ray-project/ray/assets/74173148/5f556412-5cfa-4992-b996-0c5111e3f26e"">


[link](https://buildkite.com/ray-project/premerge/builds/10928#_) to the build

[link](https://github.com/ray-project/ray/commit/c3285a8f9ae4d04c25bf16f6d87e4a9ac13e420b) to the commit


@can-anyscale  am I looking at the right thing? Can you help me find the right link for the failure?

I feel the dashboard seems broken for some cases. I checked 3 commits which was claimed to have flakey failure for test_basic_4 but all seems ok.


",working ticket dashboard like image check build image link build link commit looking right thing help find right link failure feel dashboard broken checked failure,issue,negative,negative,neutral,neutral,negative,negative
1803072095,"> Hi @yiwei00000, thanks for posting! Could you provide a minimal repro script?

There are quite a few files in our project, but I have submitted small test samples without any failure in the past. In the error message above, there is a last available logs (truncated to 20000 chars), and I don't understand what this means? I searched for all the log and error files such as raynet, worker, and driver, but couldn't find the corresponding error log information.",hi thanks posting could provide minimal script quite project small test without failure past error message last available truncated understand log error worker driver could find corresponding error log information,issue,positive,negative,neutral,neutral,negative,negative
1803011204,"Other than Sihan's comment above which I will address in a follow up PR, I have addressed all comments! @shrekris-anyscale @sihanwang41 @edoakes please take another look!",comment address follow please take another look,issue,negative,neutral,neutral,neutral,neutral,neutral
1802938794,@rkooo567 I updated the API to avoid the extra worker file. ,avoid extra worker file,issue,negative,neutral,neutral,neutral,neutral,neutral
1802883811,"P2 since it doesn't currently affect performance, but this would greatly improve our ability to extend the optimizer in the future.",since currently affect performance would greatly improve ability extend future,issue,positive,positive,positive,positive,positive,positive
1802821112,"Original issue closed, no longer needed.",original issue closed longer,issue,negative,positive,positive,positive,positive,positive
1802735485,PR merged partial; still additional work to be done here.,partial still additional work done,issue,negative,negative,neutral,neutral,negative,negative
1802730349,@Dv04 for sure - can you pick up from the PR that @hartikainen had started? @c21 has left some comments to correct the implementation/approach.,sure pick left correct,issue,negative,positive,positive,positive,positive,positive
1802723564,Ray Data uses 25% of the total object store memory by default. You can override it with `DataContext.execution_options`,ray data total object store memory default override,issue,negative,neutral,neutral,neutral,neutral,neutral
1802717792,@mattip do you think you'll be able to get the gpustat PR merged by EOTM? Slack me; I can help prioritize reviews as required.,think able get slack help,issue,negative,positive,positive,positive,positive,positive
1802692154,"> This seems like the right direction, and will close #35581 and a number of issues around GPU detection.
> 
> > Additionally, we can remove GPUUtil from auto detection as well.
> 
> Will this be in a follow-up PR?

The auto detection will be in this PR as well as the change is not too big + part of using nvml library. Will update once I'm done with the changes.",like right direction close number around detection additionally remove auto detection well auto detection well change big part library update done,issue,positive,positive,positive,positive,positive,positive
1802690251,#41020 takes a different (and probably better) approach of using pynvml directly.,different probably better approach directly,issue,negative,positive,positive,positive,positive,positive
1802687597,"This seems like the right direction, and will close #35581 and a number of issues around GPU detection.

> Additionally, we can remove GPUUtil from auto detection as well.

Will this be in a follow-up PR?",like right direction close number around detection additionally remove auto detection well,issue,positive,positive,positive,positive,positive,positive
1802683896,"The pynvml package says
> As of version 11.0.0, the NVML-wrappers used in pynvml are identical to those published through [nvidia-ml-py](https://pypi.org/project/nvidia-ml-py/).

PR #41020 uses pynvml instead of gpustat.",package version used identical instead,issue,negative,neutral,neutral,neutral,neutral,neutral
1802530999,@XuehaiPan do you know the difference between https://pypi.org/project/nvidia-ml-py/ and https://pypi.org/project/pynvml/. Which is the official binding provided by Nvidia?,know difference official binding provided,issue,negative,neutral,neutral,neutral,neutral,neutral
1802528172,"Was fixed by https://github.com/ray-project/ray/pull/40932

Latest run pass: https://buildkite.com/ray-project/release-tests-branch/builds/2356#018b7f4e-f6db-4eb7-91ea-1945e3081677/undefined-1",fixed latest run pas,issue,negative,positive,positive,positive,positive,positive
1802523883,"This should be a release blocker for Ray 2.9. The Pydantic 2.5.0 release is imminent and there's a minor amount of work left on our side so we can unpin and support `>= 2.5.0`.

If we get to 2.9 branch cut and there's still no Pydantic release in sight, we can revisit but it should be an explicit call.

I'll be offline next 2 weeks -- @shrekris-anyscale is POC for this in my absence.",release blocker ray release imminent minor amount work left side unpin support get branch cut still release sight revisit explicit call next absence,issue,negative,negative,neutral,neutral,negative,negative
1802488979,I'll wait for @architkulkarni to take a look; otherwise I can stamp tomorrow,wait take look otherwise stamp tomorrow,issue,negative,neutral,neutral,neutral,neutral,neutral
1802482766,"Re-reviewing this with @mattip sounds like it's a memory issue (we have some hypothesis on way in comparison between Windows and Linux).

@grizzlybearg can you try doubling the memory and see if it passes/progresses further? Also can you answer the clarifying questions from @mattip  above?",like memory issue hypothesis way comparison try doubling memory see also answer,issue,negative,neutral,neutral,neutral,neutral,neutral
1802479679,"Totally, @mattip is already doing the right thing here for window tests. Once we migrate windows over to civ2 the process can be more unified. Right now, enable/disable window test is case-by-case basis.",totally already right thing window migrate process unified right window test basis,issue,negative,positive,positive,positive,positive,positive
1802474855,@can-anyscale can you share how one is able to manually kick off a CI test after we believe it's been unflaked? @mattip can help here else @architkulkarni can check tonight if the CI is passing now?,share one able manually kick test believe help else check tonight passing,issue,positive,positive,positive,positive,positive,positive
1802464903,"@PurvangL I see. So let's see what the content of batch is when the exception occurs? It seems to be not finding the key on some entries, right? ",see let see content batch exception finding key right,issue,negative,positive,positive,positive,positive,positive
1802459865,"> Btw, as a follow up we should correctly update the unaccounted tasks from the dashboard. Is it transparently done, or should we make one more PR?

it will be transparent to the unaccounted tasks. ",follow correctly update unaccounted dashboard transparently done make one transparent unaccounted,issue,negative,neutral,neutral,neutral,neutral,neutral
1802456472,Thanks! Will take a look sometime next week,thanks take look sometime next week,issue,negative,positive,neutral,neutral,positive,positive
1802446272,@mattip can you do a quick repro on python311 and see if it happens; lower version of python as well so we can see if that unblocks @Akshajy6.,quick python see lower version python well see,issue,negative,positive,positive,positive,positive,positive
1802445167,"@kouroshHakha 
I was caching exceptions instead of script to terminate to check if there is only one input that is causing issue, but if I don't cache exception, script fails at first step only.",instead script terminate check one input causing issue cache exception script first step,issue,negative,positive,positive,positive,positive,positive
1802432434,won't do since no complains from a global eviction queue. ,wo since global eviction queue,issue,negative,neutral,neutral,neutral,neutral,neutral
1802376079,"Merging now, folks feel free to discuss why some tests need to be omitted and we can adjust as a follow up.",feel free discus need adjust follow,issue,positive,positive,positive,positive,positive,positive
1802258544,"> Looks good, add a section to the monitoring docs?

Hi Ed, I am planning to do doc in separate pr.",good add section hi doc separate,issue,negative,positive,positive,positive,positive,positive
1802249151,"We might or might not need it for weekly release, I'm not sure yet ;)",might might need weekly release sure yet,issue,negative,positive,positive,positive,positive,positive
1802231395,This test is still on CI version 1 which doesn't have the concept of jailed tests,test still version concept,issue,negative,neutral,neutral,neutral,neutral,neutral
1802230184,"I remember testing running 100 jobs on 2.5 on Anyscale platform last week as well and not seeing any issue. But since Ray 2.8.0 is out, you should probably try that first :)",remember testing running platform last week well seeing issue since ray probably try first,issue,negative,positive,positive,positive,positive,positive
1802213839,"Maybe a triple-quoted string could be used to preserve line formatting, and with a touch of dedent to make it pretty? Also, note this is for the windows cmd command prompt. For powershell, additional escaping is needed. Maybe all this should only be added to the help text for windows, to give posix users less of an opportunity to laugh at windows users.
```
ray start --head --resources=""{\""""special_hardware\"""":1, \""""custom_label\"""":1}""
```",maybe string could used preserve line touch make pretty also note command prompt additional maybe added help text give le opportunity laugh ray start head,issue,positive,positive,positive,positive,positive,positive
1802205993,"Sigh. For powershell, I needed to do
```
ray start --head --resources=""{\""""special_hardware\"""":1, \""""custom_label\"""":1}""
```",sigh ray start head,issue,negative,neutral,neutral,neutral,neutral,neutral
1801748568,"I think all the ""common path"" should be handled properly now (as of Ray 2.8). I.e., worker is killed by sigterm or ray. We fixed the bug where SIGTERM is ignored in some cases & add code to kill child processes on a shutdown code path. Handling shutdown gracefully is important as killing child processes is not the only clean up action in many times.

This is the last edge case to handle (kill the subprocess when a parent dies unexpectedly like segfault or sigkill). 

I think the given timeline and the size of the work, it'd be difficult to fix it by Ray 2.9 (which has branch cut in 2 weeks). We should target the end of the year.",think common path handled properly ray worker ray fixed bug add code kill child shutdown code path handling shutdown gracefully important killing child clean action many time last edge case handle kill parent unexpectedly like think given size work difficult fix ray branch cut target end year,issue,negative,positive,neutral,neutral,positive,positive
1801667971,@vitsai has other priorities for 2.9. let's aim for 2.10. I dont think this failure is also that critical (it must be test issue),let aim dont think failure also critical must test issue,issue,negative,negative,negative,negative,negative,negative
1801660719,"I reduced the number of changes to this PR.
- Made the 10sec timout 5sec again, b/c it seems to have no effect either way (`test_actor_manager.py` passes fine regardless; see our flakey test tracker rn).
- Changed PPO back to PG for the one test in test_envs_that_crash.py that seems to take longer only because we are using PPO (not PG). The train batch size is the same (and sgd_iter=1), but maybe there is something else that makes PPO run a little slower than PG (other settings we are not thinking about rn).

I really believe this is it, there is no deeper reason for the flakeyness, and our ActorManager is ok.",reduced number made sec sec effect either way fine regardless see test tracker back one test take longer train batch size maybe something else run little thinking really believe reason,issue,negative,positive,neutral,neutral,positive,positive
1801399035,"Thanks @mattip, this solved the issue on the windows cmd. 
Interestingly this still does not work in windows powershell (but thats a minor issue, at least for me).",thanks issue interestingly still work thats minor issue least,issue,positive,positive,neutral,neutral,positive,positive
1801247554,@architkulkarni agree on priority? if so - ETA on starting a PR for this?,agree priority eta starting,issue,negative,neutral,neutral,neutral,neutral,neutral
1801233912,@can-anyscale shouldn't this be jailed? given it was marked 3w ago as flaky?,given marked ago flaky,issue,negative,positive,neutral,neutral,positive,positive
1801232237,Ok， i will consider that integrate with the lastest version of ray. But I want to confirm first if it's a problem fixed between 2.5 and 2.7 ？,consider integrate version ray want confirm first problem fixed,issue,negative,positive,positive,positive,positive,positive
1801227268,@vitsai > chasing the breadcrumbs > which is the most promising GH issue / PR that we think will resolve this issue?,chasing promising issue think resolve issue,issue,positive,positive,positive,positive,positive,positive
1801225794,@shrekris-anyscale can you please prioritize investigating into the failure of this test?,please investigating failure test,issue,negative,negative,negative,negative,negative,negative
1801191758,"@scv119 @anyscalesam 
Is it right only to correct `@PublicAPI(stability=""beta"")` to `@PublicAPI(stability=""stable"")` at line 74?
I want to send pullrequest about this issue. 

Thank you in advance!",right correct beta stable line want send issue thank advance,issue,positive,positive,positive,positive,positive,positive
1800907496,"Maybe wrap the actor code into a subprocess of a per-actor daemon process. Then no matter of how actor acts, the subprocesses are not leaking beyond the daemon process. When shutting down the actor, the daemon process can terminate all processes.",maybe wrap actor code daemon process matter actor beyond daemon process shutting actor daemon process terminate,issue,negative,neutral,neutral,neutral,neutral,neutral
1800888848,@stephanie-wang I downgraded the priority for now (mostly due to limited resources). Please lmk if this issue blocks the ray data! ,priority mostly due limited please issue ray data,issue,negative,negative,neutral,neutral,negative,negative
1800871758,"Dependabot does not support your Python version. Because of this, Dependabot cannot update this pull request.",support python version update pull request,issue,negative,neutral,neutral,neutral,neutral,neutral
1800859567,"Hi @matthewdeng , Thank you for looking into this issue and the information.

Yes, by default the latest checkpoint is preserved. However, what I mean is an option to keep last checkpoint always in addition to the best checkpoint that is decided based on the monitor metric (checkpoint_score_attribute). For example Lightning checkpointing provides similar option.",hi thank looking issue information yes default latest however mean option keep last always addition best decided based monitor metric example lightning similar option,issue,positive,positive,positive,positive,positive,positive
1800854680,"The script seems to be running the training for at least 62 steps. Then I don't understand when the problem shows up? Can you elaborate on that? 

```
(RayTrainWorker pid=85271, ip=1.1.1.1) [epoch 0 step 62] loss: 2.1053566932678223 step-time: 20.169826984405518   
```",script running training least understand problem elaborate epoch step loss,issue,negative,positive,neutral,neutral,positive,positive
1800715837,"@kouroshHakha provided dict {'input'} is captured from collate_fn only. Below is the entire batch printed in collate_fn
```
(RayTrainWorker pid=85275, ip=1.1.1.1) ***** {'input': array(['<START_Q>Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she 
wants to read half of the remaining pages tomorrow, how many pages should she read?<END_Q><START_A>Maila read 12 x 2 = <<12*2=24>>24 pages today.\nSo she was able to read a total of 12 + 24 = <<12+24=36>>36 pages s
ince yesterday.\nThere are 120 - 36 = <<120-36=84>>84 pages left to be read.\nSince she wants to read half of the remaining pages tomorrow, then she should read 84/2 = <<84/2=42>>42 pages.\n#### 42<END_A>'],       
(RayTrainWorker pid=85275, ip=1.1.1.1)       dtype=object)} [repeated 7x across cluster]                                                                                                                        
(RayTrainWorker pid=85273, ip=1.1.1.1) ***** {'input': array(['<START_Q>The bakers at the Beverly Hills Bakery baked 200 loaves of bread on Monday morning. They sold 93 loaves in the morning and 39 loaves in 
the afternoon. A grocery store returned 6 unsold loaves. How many loaves of bread did they have left?<END_Q><START_A>The Bakery sold 93 + 39 = <<93+39=132>>132 loaves.\nThe Bakery made 200 loaves and sold 132, leav
ing 200 - 132 = <<200-132=68>>68 loaves remaining.\nThe grocery store returned 6 loaves, so there were 6 + 68 = <<6+68=74>>74 loaves left.\n#### 74<END_A>'],                                                         
(RayTrainWorker pid=85277, ip=1.1.1.1) ***** {'input': array(['<START_Q>The file, 90 megabytes in size, downloads at the rate of 5 megabytes per second for its first 60 megabytes, and then 10 megabytes per se
cond thereafter. How long, in seconds, does it take to download entirely?<END_Q><START_A>The first 60 megabytes take 60/5=<<60/5=12>>12 seconds.\nThere are 90-60=<<90-60=30>>30 remaining megabytes.\nThe remaining 3
0 megabytes take 30/10=<<30/10=3>>3 seconds.\nAnd 12+3=<<12+3=15>>15 seconds.\n#### 15<END_A>'],                                                                                                                      
(RayTrainWorker pid=85278, ip=1.1.1.1) ***** {'input': array(['<START_Q>Carly collected 7 starfish with 5 arms each and one seastar with 14 arms. How many arms do the animals she collected have in total?<END_
Q><START_A>First find the total number of starfish arms: 7 starfish * 5 arms/starfish = <<7*5=35>>35 arms\nThen add the number of seastar arms to find the total number of arms: 35 arms + 14 arms = <<35+14=49>>49 ar
ms\n#### 49<END_A>'],                                                                                                                                                                                                 
(RayTrainWorker pid=85272, ip=1.1.1.1) ***** {'input': array(['<START_Q>Royce takes 40 minutes more than double Rob to shingle a house. If Rob takes 2 hours, how many minutes does Royce take?<END_Q><START_A>C
onvert 2 hours to min: 2(60)=120 minutes\nRoyce takes 40+2(120)=280 minutes\n#### 280<END_A>'],                                                                                                                       
(RayTrainWorker pid=85271, ip=1.1.1.1) [epoch 0 step 62] loss: 2.1053566932678223 step-time: 20.169826984405518                                                                                                 
(RayTrainWorker pid=85271, ip=1.1.1.1) ***** {'input': array(['<START_Q>Tara bought 8 packs of 5 canvas bags for $4 each. She painted them and sold them at a craft fair for $8 each. How much profit did she ea
rn on her bags?<END_Q><START_A>The total number of bags is 8*5 = <<8*5=40>>40\nThe cost of the 40 bags is 40*4 = <<40*4=160>>160\nTara sold the 40 bags at 8 each earning her 40*8 = <<40*8=320>>320\nHer profit was 3
20-160 = <<320-160=160>>160\n#### 160<END_A>'],                                                                                                                                                                       
(RayTrainWorker pid=85276, ip=1.1.1.1) ***** {'input': array([""<START_Q>Ralph is going to practice playing tennis with a tennis ball machine that shoots out tennis balls for Ralph to hit. He loads up the mach
ine with 175 tennis balls to start with. Out of the first 100 balls, he manages to hit 2/5 of them. Of the next 75 tennis balls, he manages to hit 1/3 of them. Out of all the tennis balls, how many did Ralph not hi
t?<END_Q><START_A>Out of the first 100 balls, Ralph was able to hit 2/5 of them and not able to hit 3/5 of them, 3/5 x 100 = 60 tennis balls Ralph didn't hit.\nOut of the next 75 balls, Ralph was able to hit 1/3 of
 them and not able to hit 2/3 of them, 2/3 x 75 = 50 tennis balls that Ralph didn't hit.\nCombined, Ralph was not able to hit 60 + 50 = <<60+50=110>>110 tennis balls Ralph didn't hit.\n#### 110<END_A>""],  
 ```",provided entire batch printed array reading book yesterday able read today read twice many yesterday read half tomorrow many read read able read total left read half tomorrow read repeated across cluster array bakery baked bread morning sold morning afternoon grocery store returned unsold many bread left bakery sold bakery made sold ing grocery store returned array file size rate per second first per se cond thereafter long take entirely first take take array collected starfish arm one arm many arm collected total first find total number starfish arm starfish add number arm find total number arm arm arm ar array double rob shingle house rob many take min epoch step loss array tara bought canvas painted sold craft fair much profit ea total number cost sold earning profit array ralph going practice tennis tennis ball machine tennis ralph hit tennis start first hit next tennis hit tennis many ralph hi first ralph able hit able hit tennis ralph next ralph able hit able hit tennis ralph ralph able hit tennis ralph,issue,negative,positive,positive,positive,positive,positive
1800537664,"hmm, that's odd. Can you print `batch` inside the collate_fn and let me know what content do you see? ",odd print batch inside let know content see,issue,negative,negative,negative,negative,negative,negative
1800484166,"Hi @kamal-rahimi,

Actually the default behavior should be to keep the last checkpoint, for the exact reasons you've mentioned. Are you observing something different?

**Example:**
```python

import ray
from ray.train.torch import TorchTrainer
from ray.train import ScalingConfig, RunConfig, Checkpoint, CheckpointConfig

def train_func():
    for i in range(10):
        checkpoint = Checkpoint.from_directory(""."") 
        ray.train.report({""i"": i}, checkpoint = checkpoint)

scaling_config = ScalingConfig(num_workers=2)
checkpoint_config = CheckpointConfig(checkpoint_score_attribute=""i"", checkpoint_score_order=""min"", num_to_keep=2)
run_config = RunConfig(storage_path = ""/tmp/storage"", name=""experiment"", checkpoint_config=checkpoint_config)
torch_trainer = TorchTrainer(train_func, scaling_config=scaling_config, run_config=run_config)

torch_trainer.fit()
```
```bash
$ tree /tmp/storage/experiment
/tmp/storage/experiment
├── TorchTrainer_50ef4_00000_0_2023-11-07_15-49-40
│   ├── checkpoint_000000
│   │   ├── events.out.tfevents.1699400984.g-c83faa40fb9f40001
│   │   ├── params.json
│   │   ├── params.pkl
│   │   └── result.json
│   ├── checkpoint_000001
│   │   ├── events.out.tfevents.1699400984.g-c83faa40fb9f40001
│   │   ├── params.json
│   │   ├── params.pkl
│   │   └── result.json
│   ├── checkpoint_000009
│   │   ├── events.out.tfevents.1699400984.g-c83faa40fb9f40001
│   │   ├── params.json
│   │   ├── params.pkl
│   │   ├── progress.csv
│   │   └── result.json
│   ├── events.out.tfevents.1699400984.g-c83faa40fb9f40001
│   ├── params.json
│   ├── params.pkl
│   ├── progress.csv
│   └── result.json
├── basic-variant-state-2023-11-07_15-49-40.json
├── experiment_state-2023-11-07_15-49-40.json
├── trainer.pkl
└── tuner.pkl
```",hi actually default behavior keep last exact observing something different example python import ray import import range min experiment bash tree,issue,negative,positive,neutral,neutral,positive,positive
1800431627,"@kouroshHakha Thank you for the reply. 
I used [step](https://github.com/ray-project/ray/tree/master/doc/source/templates/04_finetuning_llms_with_deepspeed#:~:text=python%20create_dataset.py), which I believe download dataset in required format. 
After successful completion of above step, I see following files.
```
-rw-r--r-- 1 ray users 752K  test.jsonl
-rw-r--r-- 1 ray users   60  tokens.json
-rw-r--r-- 1 ray users 4.1M  train.jsonl
```

Below is the one of example input
```
{'input': array(['<START_Q>James writes a 3-page letter to 2 different friends twice a week.  How many pages does he write a year?<END_Q><START_A>He writes each fr
iend 3*2=<<3*2=6>>6 pages a week\nSo he writes 6*2=<<6*2=12>>12 pages every week\nThat means he writes 12*52=<<12*52=624>>624 pages a year\n#### 624<END_A>'], 
```
Please let me know if any addition information is needed.
",thank reply used step believe format successful completion step see following ray ray ray one example input array letter different twice week many write year every please let know addition information,issue,positive,positive,positive,positive,positive,positive
1800431102,"Before we are able to fix this issue on dashboard side, a workaround will be customizing the formatter of the serve logger and remove the redundant or add metadata you want to? @JakeSummers ",able fix issue dashboard side serve logger remove redundant add want,issue,negative,positive,positive,positive,positive,positive
1800427417,@PurvangL seems to be a dataset format issue. It is looking for `input` key but cannot find it. Can you share how your dataset looks like?,format issue looking input key find share like,issue,positive,neutral,neutral,neutral,neutral,neutral
1800423496,"I'm going to mark this issue as closed because I am not able reproduce this. Please reopen if the issue is still happening for you.

```
(base) ~ conda create -n ray-39521 python=3.10 -y; conda activate ray-39521; pip install ""ray[tune]""; python -c ""from ray import tune; print('success')""   
```

<details><summary>Output</summary>
<p>

```
Collecting package metadata (current_repodata.json): done
Solving environment: done


==> WARNING: A newer version of conda exists. <==
  current version: 22.11.1
  latest version: 23.10.0

Please update conda by running

    $ conda update -n base -c defaults conda

Or to minimize the number of packages updated during conda update use

     conda install conda=23.10.0



## Package Plan ##

  environment location: /Users/matt/miniconda3/envs/ray-39521

  added / updated specs:
    - python=3.10


The following NEW packages will be INSTALLED:

  bzip2              pkgs/main/osx-arm64::bzip2-1.0.8-h620ffc9_4 
  ca-certificates    pkgs/main/osx-arm64::ca-certificates-2023.08.22-hca03da5_0 
  libffi             pkgs/main/osx-arm64::libffi-3.4.4-hca03da5_0 
  ncurses            pkgs/main/osx-arm64::ncurses-6.4-h313beb8_0 
  openssl            pkgs/main/osx-arm64::openssl-3.0.12-h1a28f6b_0 
  pip                pkgs/main/osx-arm64::pip-23.3-py310hca03da5_0 
  python             pkgs/main/osx-arm64::python-3.10.13-hb885b13_0 
  readline           pkgs/main/osx-arm64::readline-8.2-h1a28f6b_0 
  setuptools         pkgs/main/osx-arm64::setuptools-68.0.0-py310hca03da5_0 
  sqlite             pkgs/main/osx-arm64::sqlite-3.41.2-h80987f9_0 
  tk                 pkgs/main/osx-arm64::tk-8.6.12-hb8d0fd4_0 
  tzdata             pkgs/main/noarch::tzdata-2023c-h04d1e81_0 
  wheel              pkgs/main/osx-arm64::wheel-0.41.2-py310hca03da5_0 
  xz                 pkgs/main/osx-arm64::xz-5.4.2-h80987f9_0 
  zlib               pkgs/main/osx-arm64::zlib-1.2.13-h5a0b063_0 



Downloading and Extracting Packages

Preparing transaction: done
Verifying transaction: \ WARNING conda.core.path_actions:verify(1094): Unable to create environments file. Path not writable.
  environment location: /Users/matt/.conda/environments.txt

done
Executing transaction: | WARNING conda.core.envs_manager:register_env(49): Unable to register environment. Path not writable or missing.
  environment location: /Users/matt/miniconda3/envs/ray-39521
  registry file: /Users/matt/.conda/environments.txt
done
#
# To activate this environment, use
#
#     $ conda activate ray-39521
#
# To deactivate an active environment, use
#
#     $ conda deactivate

Collecting ray[tune]
  Using cached ray-2.8.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (13 kB)
Collecting click>=7.0 (from ray[tune])
  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)
Collecting filelock (from ray[tune])
  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)
Collecting jsonschema (from ray[tune])
  Using cached jsonschema-4.19.2-py3-none-any.whl.metadata (7.9 kB)
Collecting msgpack<2.0.0,>=1.0.0 (from ray[tune])
  Using cached msgpack-1.0.7-cp310-cp310-macosx_11_0_arm64.whl.metadata (9.1 kB)
Collecting packaging (from ray[tune])
  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)
Collecting protobuf!=3.19.5,>=3.15.3 (from ray[tune])
  Using cached protobuf-4.25.0-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)
Collecting pyyaml (from ray[tune])
  Using cached PyYAML-6.0.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.1 kB)
Collecting aiosignal (from ray[tune])
  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)
Collecting frozenlist (from ray[tune])
  Using cached frozenlist-1.4.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.2 kB)
Collecting requests (from ray[tune])
  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)
Collecting numpy>=1.19.3 (from ray[tune])
  Using cached numpy-1.26.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (61 kB)
Collecting pandas (from ray[tune])
  Using cached pandas-2.1.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (18 kB)
Collecting tensorboardX>=1.9 (from ray[tune])
  Using cached tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)
Collecting pyarrow>=6.0.1 (from ray[tune])
  Using cached pyarrow-14.0.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (3.0 kB)
Collecting fsspec (from ray[tune])
  Using cached fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)
Collecting attrs>=22.2.0 (from jsonschema->ray[tune])
  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)
Collecting jsonschema-specifications>=2023.03.6 (from jsonschema->ray[tune])
  Using cached jsonschema_specifications-2023.7.1-py3-none-any.whl.metadata (2.8 kB)
Collecting referencing>=0.28.4 (from jsonschema->ray[tune])
  Using cached referencing-0.30.2-py3-none-any.whl.metadata (2.6 kB)
Collecting rpds-py>=0.7.1 (from jsonschema->ray[tune])
  Using cached rpds_py-0.12.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (3.7 kB)
Collecting python-dateutil>=2.8.2 (from pandas->ray[tune])
  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)
Collecting pytz>=2020.1 (from pandas->ray[tune])
  Using cached pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)
Collecting tzdata>=2022.1 (from pandas->ray[tune])
  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)
Collecting charset-normalizer<4,>=2 (from requests->ray[tune])
  Using cached charset_normalizer-3.3.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (33 kB)
Collecting idna<4,>=2.5 (from requests->ray[tune])
  Using cached idna-3.4-py3-none-any.whl (61 kB)
Collecting urllib3<3,>=1.21.1 (from requests->ray[tune])
  Using cached urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)
Collecting certifi>=2017.4.17 (from requests->ray[tune])
  Using cached certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)
Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->ray[tune])
  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)
Using cached click-8.1.7-py3-none-any.whl (97 kB)
Using cached msgpack-1.0.7-cp310-cp310-macosx_11_0_arm64.whl (231 kB)
Using cached numpy-1.26.1-cp310-cp310-macosx_11_0_arm64.whl (14.0 MB)
Using cached protobuf-4.25.0-cp37-abi3-macosx_10_9_universal2.whl (393 kB)
Using cached pyarrow-14.0.0-cp310-cp310-macosx_11_0_arm64.whl (24.0 MB)
Using cached tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)
Using cached frozenlist-1.4.0-cp310-cp310-macosx_11_0_arm64.whl (46 kB)
Using cached filelock-3.13.1-py3-none-any.whl (11 kB)
Using cached fsspec-2023.10.0-py3-none-any.whl (166 kB)
Using cached jsonschema-4.19.2-py3-none-any.whl (83 kB)
Using cached packaging-23.2-py3-none-any.whl (53 kB)
Using cached pandas-2.1.2-cp310-cp310-macosx_11_0_arm64.whl (10.9 MB)
Using cached PyYAML-6.0.1-cp310-cp310-macosx_11_0_arm64.whl (169 kB)
Using cached ray-2.8.0-cp310-cp310-macosx_11_0_arm64.whl (60.2 MB)
Using cached requests-2.31.0-py3-none-any.whl (62 kB)
Using cached certifi-2023.7.22-py3-none-any.whl (158 kB)
Using cached charset_normalizer-3.3.2-cp310-cp310-macosx_11_0_arm64.whl (120 kB)
Using cached jsonschema_specifications-2023.7.1-py3-none-any.whl (17 kB)
Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)
Using cached referencing-0.30.2-py3-none-any.whl (25 kB)
Using cached rpds_py-0.12.0-cp310-cp310-macosx_11_0_arm64.whl (322 kB)
Using cached urllib3-2.0.7-py3-none-any.whl (124 kB)
Installing collected packages: pytz, urllib3, tzdata, six, rpds-py, pyyaml, protobuf, packaging, numpy, msgpack, idna, fsspec, frozenlist, filelock, click, charset-normalizer, certifi, attrs, tensorboardX, requests, referencing, python-dateutil, pyarrow, aiosignal, pandas, jsonschema-specifications, jsonschema, ray
Successfully installed aiosignal-1.3.1 attrs-23.1.0 certifi-2023.7.22 charset-normalizer-3.3.2 click-8.1.7 filelock-3.13.1 frozenlist-1.4.0 fsspec-2023.10.0 idna-3.4 jsonschema-4.19.2 jsonschema-specifications-2023.7.1 msgpack-1.0.7 numpy-1.26.1 packaging-23.2 pandas-2.1.2 protobuf-4.25.0 pyarrow-14.0.0 python-dateutil-2.8.2 pytz-2023.3.post1 pyyaml-6.0.1 ray-2.8.0 referencing-0.30.2 requests-2.31.0 rpds-py-0.12.0 six-1.16.0 tensorboardX-2.6.2.2 tzdata-2023.3 urllib3-2.0.7
success
```

</p>
</details> ",going mark issue closed able reproduce please reopen issue still happening base create activate pip install ray tune python ray import tune print summary output package done environment done warning version current version latest version please update running update base minimize number update use install package plan environment location added spec following new pip python wheel transaction done transaction warning verify unable create file path writable environment location done transaction warning unable register environment path writable missing environment location registry file done activate environment use activate deactivate active environment use deactivate ray tune click ray tune ray tune ray tune ray tune ray tune ray tune ray tune ray tune ray tune ray tune ray tune ray tune ray tune ray tune ray tune ray tune ray tune ray tune ray tune ray tune ray tune ray tune ray tune ray tune ray tune ray tune six ray tune collected six click ray successfully post success,issue,positive,negative,neutral,neutral,negative,negative
1800413791,"A bit hard to fix the circular dependency issue - this would be much easier if we had the ability to un-fuse operators, but unfortunately we cannot do that right now.

There is a faster fix for shuffle workloads where we just propagate the shuffle block size to all upstream ops before we run the SplitReadOutputBlocks rule.",bit hard fix circular dependency issue would much easier ability unfortunately right faster fix shuffle propagate shuffle block size upstream run rule,issue,negative,positive,neutral,neutral,positive,positive
1800350879,"On the dashboard side, we can potentially filter out some of the repeated metadata when fetching the logs. cc: @alanwguo ",dashboard side potentially filter repeated fetching,issue,negative,neutral,neutral,neutral,neutral,neutral
1800338943,"Could you add some printings to demo the memory was never allocated? I tried this:

```
import ray
from ray.util.multiprocessing import Pool
import numpy as np
import psutil

def txt():
    return f""{psutil.virtual_memory()}""


def calculate_data(i):
    x = np.random.random(100_000_000)
    return x


def parallel_data_acquisition():
    my_pool = Pool(2)
    new_data = my_pool.map(calculate_data, [1, 2, 3, 4])
    print(f""in the pool {txt()}"")
    my_pool.close()
    my_pool.join()
    print(f""after the pool {txt()}"")
    return new_data


def process(data):
    print(sum(sum(datum) for datum in data))


def run():
    for x in range(10):
        print(f'before round {x}: {txt()}')
        process([parallel_data_acquisition() for i in range(5)])
        print(f'after round {x}: {txt()}')

```

and looks like in the middle of running the usage percentage of the physical memory raises (to 37.3), but after a whole round, the usage percentage stays low (18.7 on my laptop).",could add memory never tried import ray import pool import import return return pool print pool print pool return process data print sum sum datum datum data run range print round process range print round like middle running usage percentage physical memory whole round usage percentage stay low,issue,negative,negative,neutral,neutral,negative,negative
1800335931,"This issue is slightly different from #39652, which is doing a copy and paste. @MattEWeber is able to reproduce this one, which uses the ""copy"" button, but not #39652. I suggest we keep both open as they seem to be different mechanisms.",issue slightly different copy paste able reproduce one copy button suggest keep open seem different,issue,negative,positive,positive,positive,positive,positive
1800328451,"https://github.com/ray-project/ray/pull/40463 should enable pod autoscaling for the GCE VM based approach using a similar API laid out in https://github.com/ray-project/ray/issues/39781#issuecomment-1741313233

The Kuberay piece is equally important here though and I am not sure I fully understand if there was a conclusion from https://github.com/ray-project/ray/issues/39781#issuecomment-1736479133 and https://github.com/ray-project/ray/issues/39781#issuecomment-1743658075.

A piece of confusion from my end is that I don't this will work as we expect it to:
> Users should create a TPU node pool for each RayCluster, and each node pool has unique taints/labels for Pod scheduling.

To clarify, a TPU node pool can only schedule a single TPU pod (see [ref](https://cloud.google.com/kubernetes-engine/docs/concepts/tpus#machine_type)) so we can't get our autoscaling from the node pool. That's why @richardsliu suggested the approach in https://github.com/ray-project/ray/issues/39781#issuecomment-1736479133 e.g. scale by nodepools rather than nodes within a nodepool. 




",enable pod based approach similar laid piece equally important though sure fully understand conclusion piece confusion end work expect create node pool node pool unique pod clarify node pool schedule single pod see ref ca get node pool approach scale rather within,issue,negative,positive,positive,positive,positive,positive
1800323965,"Checking with @MattEWeber to see if he has ideas on how to troubleshoot. He is able to reproduce the ""copy"" button behavior.",see able reproduce copy button behavior,issue,negative,positive,positive,positive,positive,positive
1800323417,"The same problem happens when you use the ""copy"" button, per this issue: https://github.com/ray-project/ray/issues/40936",problem use copy button per issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1800268019,"Could you try this out on Ray 2.7.1, and let us know if that fixes the issue?",could try ray let u know issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1800265575,"Hi @yiwei00000, thanks for posting! Could you provide a minimal repro script?",hi thanks posting could provide minimal script,issue,negative,positive,neutral,neutral,positive,positive
1800177304,"Based on feedback from Huaiwei and Angelina, `UPSCALE_COMPLETED` and `DOWNCALE_COMPLETED` does not match with the rest of the status triggers. They would prefer `UPSCALE` and `DOWNSCALE` instead; i.e. once Serve successfully scales the number of replicas to the target number, the state will transition from (UPSCALING, AUTOSCALE) -> (HEALTHY, UPSCALE) ..... or (UPSCALING, CONFIG_UPDATE) -> (HEALTHY, UPSCALE). Please let me know if anyone is against this, otherwise I will go with this naming!",based feedback match rest status would prefer upscale instead serve successfully scale number target number state transition healthy upscale healthy upscale please let know anyone otherwise go naming,issue,positive,positive,positive,positive,positive,positive
1799755431,Update - we are still waiting on the pydantic release. @shrekris-anyscale also able to support integration when it does release.,update still waiting release also able support integration release,issue,negative,positive,positive,positive,positive,positive
1799592526,"# Review of ""Ray Clusters"" section

I'm only pointing our discrepancies between the current state and the upgrade, not necessarily sharing an opinion about how the side navigation should work.

- [x] On the ""Ray Clusters Overview"" page, the ""Learn Key Concepts"" button/card link jumps you down past the title on the ""Key Concepts"" page.
- [x] On the ""Key Concepts"" page, each heading (i.e. Ray Cluster, Head Node, Worker Node, Autoscaling, and Ray Jobs) when clicked shoots you up to the Ray Cluster heading.
- [x] When I'm on a page, it's not highlighted in the sidenav, which causes me to kind of lose where I am, especially in a deeply nested section like ""Ray Clusters.""
- [x] When I go to any subpage of ""API References"" under ""Deploying VMs,"" the sidenav collapses completely and I lose my spot. 
- [x] ""API References"" under ""Deploying VMs"" is section, but is not bolded or expandable.
- [x] ""Applications Guide"" is a section, but is not expandable or bolded in the sidenav.
- [x] When I go to a subpage in ""Applications Guide"" the sidenav collapses.
- [x] Accessing any subpage in the ""Ray Cluster Management API"" causes the sidenav to collapse. Also, it's a section, but is not bolded and subitems don't appear in the dropdown.",review ray section pointing current state upgrade necessarily opinion side navigation work ray overview page learn key link past title key page key page heading ray cluster head node worker node ray ray cluster heading page kind lose especially deeply section like ray go completely lose spot section guide section go guide ray cluster management collapse also section appear,issue,negative,positive,neutral,neutral,positive,positive
1799511175,"Observations:

- On ray.init() each worker has ~3 sockets to plasma store and ~2 to raylet.
- Some connections are linked to the remote inode, some are linked to the file name in `lsof`
- When you have some workloads, the number of sockets and fds grows dramatically
- However in `client_connection.cc` you don't see any fds printed in logs other than the 2 (1 for raylet 1 for plasma store)",worker plasma store raylet linked remote linked file name number dramatically however see printed raylet plasma store,issue,negative,negative,neutral,neutral,negative,negative
1799389061,"Failed tests:
- tests:test_basic unrelated
- serve/tests:test_logging    unrelated
- serve/doc_code/ tests unrelated
- streaming chaos tests unrelated",unrelated unrelated unrelated streaming chaos unrelated,issue,negative,neutral,neutral,neutral,neutral,neutral
1799383022,"Thanks, this is looking really good! I'll do a review sometime later today.",thanks looking really good review sometime later today,issue,positive,positive,positive,positive,positive,positive
1799374881,This has been fixed in `tune-sklearn==0.5.0`. Let me know if that works for you.,fixed let know work,issue,negative,positive,neutral,neutral,positive,positive
1799373501,"@astron8t-voyagerx Thanks for reporting this, do you have a minimal way to reproduce this issue? How often does it happen?

Also, are you certain that the issue goes away when you go back to Ray 2.6? The reason I ask is there are some similar-sounding issues https://github.com/ray-project/ray/issues/39565 and https://github.com/ray-project/ray/issues/38718, but those were reported to also be present in Ray 2.6, not just 2.7.  We haven't been able to reproduce the issue so far",thanks minimal way reproduce issue often happen also certain issue go away go back ray reason ask also present ray able reproduce issue far,issue,positive,positive,positive,positive,positive,positive
1799341005,"Defering to eng to determine final priority. It seems like a P1, to me.",determine final priority like,issue,negative,neutral,neutral,neutral,neutral,neutral
1799340893,"Windows terminal requires double quotes for command line arguments, and json parsing also requires double quotes. So do something like
```
ray start --head --resources=""{\""special_hardware\"":1, \""custom_label\"":1}""
```

Is there a place the `--resources` option is documented that this could be added?",terminal double command line also double something like ray start head place option could added,issue,negative,neutral,neutral,neutral,neutral,neutral
1799292967,"> @sofianhnaide can you ping me when the new window wheel build finishes and doesn't fail?

rebasing",ping new window wheel build fail,issue,negative,negative,negative,negative,negative,negative
1799289935,"Thanks for the tip @anyscalesam, sorry I did not respond sooner - the issue arises with `tensorflow` dependency, not `protobuf` directly. Since `tensorflow==2.11`  depends on `protobuf<3.20` there is no dependency solution for `ray[all]==2.7.1` with `protobuf>=3.20`. So although `ray` doesn't provide an upper bound directly on `protobuf`, by pinning `tensorflow==2.11` then `protobuf` is indirectly pinned to (I believe) `3.19.6` and thus the issue.

If I specify `protobuf>3.20` in `requirements.txt` then there is no valid solution. For `pip` this can obviously be brute-forced however if a project uses something such as `poetry` to resolve dependencies this becomes quite a nuisance to overcome.

I am honestly surprised that not more people have discovered the issue with `tfevents` files not being generated due to this missing `builder.py` in `protobuf`.",thanks tip sorry respond sooner issue dependency directly since dependency solution ray although ray provide upper bound directly pinning indirectly pinned believe thus issue specify valid solution pip obviously however project something poetry resolve becomes quite nuisance overcome honestly people discovered issue due missing,issue,positive,positive,neutral,neutral,positive,positive
1799178620,"I see, could you provide a minimal repro script? It's strange that internet connectivity and closing the laptop lid affects the Ray cluster. Ray clusters shouldn't need to connect to the internet.",see could provide minimal script strange connectivity lid ray cluster ray need connect,issue,negative,negative,neutral,neutral,negative,negative
1799062250,"Hi @matthewdeng Sorry for the late response! I applied your changes, thank you!",hi sorry late response applied thank,issue,negative,negative,negative,negative,negative,negative
1799057691,"> Would you mind providing a YAML file in your PR description and sharing more details about the expected behavior?

~~Sure, I have recorded an experiment based on the example here https://docs.ray.io/en/latest/serve/advanced-guides/advanced-autoscaling.html#attempt-2-autoscale-driver. I will come up with an example YAML later.~~

To better demonstrate the expected behavior, I first wrote a small ray program to figure out how to fill up GCS usage.

Based on my observation from redis side, Ray GCS will store all information in one hash set, and among all hash members, the `<namespace>@KV:@namespace_fun:ActorClass:*` can take most of the redis memory usage since they are pickled from actor definitions.

I used the following program to verify the idea:

```python
import os
import ray
import redis

def new_actor(n):
    @ray.remote(num_cpus=0)
    class MyActor:
        data = bytes(n)
        
    return MyActor

if __name__ == ""__main__"":
    redis_address = os.getenv(""RAY_REDIS_ADDRESS"") # ex. redis://localhost:6379
    redis_client = redis.from_url(redis_address)

    ray.init()

    print(redis_client.memory_usage(""default"", 0))
    for _ in range(30):
        actor = new_actor(1024**2)
        for _ in range(100):
            actor.remote()
            print(redis_client.memory_usage(""default"", 0))
```
This program defined 30 actors and each of them takes about 1MB. And then started 100 replicas for each actor. It printed out redis memory usage after each replica was registered with `actor.remote()`. The usage result was:
![image](https://github.com/ray-project/ray/assets/2727535/c815ee5a-f4a7-4583-832f-7a95fe24fda1)

As you can see from the plot, the memory usage jumps 1MB up whenever a new actor definition is registered. This result shows that users should take their actor definitions, not the number of replicas, into consideration when they want to estimate how much redis memory they have to have.


## Expected behavior of `maxmemory-policy=allkeys-lru`

Then, I will use the following yaml to demonstrate the expected behavior:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: redis
  labels:
    app: redis
spec:
  type: ClusterIP
  ports:
    - name: redis
      port: 6379
  selector:
    app: redis
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  labels:
    app: redis
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
        - name: redis
          image: redis:latest
          command:
            - ""redis-server""
            - ""--bind""
            - ""0.0.0.0""
            - ""--port""
            - ""6379""
            - ""--protected-mode""
            - ""no""
            - ""--maxmemory""
            - ""60mb""
            - ""--maxmemory-policy""
            - ""allkeys-lru""
          ports:
            - containerPort: 6379
---
apiVersion: ray.io/v1
kind: RayCluster
metadata:
  annotations:
    ray.io/ft-enabled: ""true""
    ray.io/external-storage-namespace: ""raycluster-1""
  name: raycluster-1
spec:
  rayVersion: '2.7.0'
  headGroupSpec:
    rayStartParams: {}
    template:
      spec:
        containers:
          - name: ray-head
            image: rayproject/ray:2.7.0
            env:
              - name: RAY_REDIS_ADDRESS
                value: redis://redis:6379
            volumeMounts:
              - mountPath: /home/ray/samples
                name: ray-samples
        volumes:
          - name: ray-samples
            configMap:
              name: ray-samples
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ray-samples
data:
  make_actors_30MB.py: |
    import ray

    def new_actor(n):
        @ray.remote(num_cpus=0)
        class MyActor:
            data = bytes(n)
        return MyActor

    ray.init(namespace=""default_namespace"")
    [new_actor(1024**2).options(name=str(i), lifetime=""detached"").remote() for i in range(30)]
```

This yaml will spin up a redis with `maxmemory=60mb` and `maxmemory-policy=allkeys-lru`, and a `raycluster-1` mounted with a `make_actors_30MB.py` modified from the previous program.

Here are the demonstration procedures:
1. Apply the above yaml to a kuberay operator whose `ENABLE_GCS_FT_REDIS_CLEANUP=false`
2. Fill redis up by running
`kubectl exec -it raycluster-1-head-oooxx -- python /home/ray/samples/make_actors_30MB.py`
3. Delete the cluster by running
`kubectl delete raycluster raycluster-1`
4. Print out the redis memory usage of `raycluster-1`. It should be about ~39MB.
`kubectl exec -it deploy/redis -- redis-cli MEMORY USAGE raycluster-1 SAMPLES 0`
5. Replace all the `raycluster-1` in the yaml to `raycluster-2`, and apply it again to the kuberay operator.
6. Fill redis up again by running the same program on the new cluster
`kubectl exec -it raycluster-2-head-oooxx -- python /home/ray/samples/make_actors_30MB.py`
7. Print out the redis memory usage of `raycluster-1` again. It should be nil now, because it should be evicted by the `maxmemory-policy=allkeys-lru`.",would mind providing file description behavior experiment based example come example better demonstrate behavior first wrote small ray program figure fill usage based observation side ray store information one hash set among hash take memory usage since actor used following program verify idea python import o import ray import class data return ex print default range actor range print default program defined actor printed memory usage replica registered usage result image see plot memory usage whenever new actor definition registered result take actor number consideration want estimate much memory behavior use following demonstrate behavior kind service name spec type name port selector kind deployment name spec selector template spec name image latest command bind port kind true name spec template spec name image name value name name name kind name data import ray class data return detached range spin mounted previous program demonstration apply operator whose fill running python delete cluster running delete print memory usage memory usage replace apply operator fill running program new cluster python print memory usage nil,issue,positive,positive,positive,positive,positive,positive
1799047457,"Hi @woshiyyya Thank you very much for the clarification. It was not obvious to me, that they are soft duplicated. Sorry!",hi thank much clarification obvious soft sorry,issue,negative,negative,neutral,neutral,negative,negative
1798557223,"@simonsays1980 Have you solved this problem? I am facing exactly the same issue.

Although I have num_gpus_per_worker=0.5, the first worker on each GPU always allocates almost the whole GPU memory (e.g. 11.6 GB). Consequently, the subsequent worker allocated to the same GPU encounters a memory availability issue; sometimes it successfully initializes with the remaining 0.6 GB, and other times it fails with the ""Attempting to perform BLAS..."" error.",problem facing exactly issue although first worker always almost whole memory consequently subsequent worker memory availability issue sometimes successfully time perform blas error,issue,negative,positive,positive,positive,positive,positive
1798401670,"Btw, as a follow up we should correctly update the unaccounted tasks from the dashboard. Is it transparently done, or should we make one more PR? ",follow correctly update unaccounted dashboard transparently done make one,issue,negative,neutral,neutral,neutral,neutral,neutral
1798372215,can you merge the latest master? One premerge test failure seems to keep failing ,merge latest master one test failure keep failing,issue,negative,positive,neutral,neutral,positive,positive
1798184968,"> ```python
> ""NCCL_SOCKET_IFNAME"": ""eno1,eth0""
> ```

Thank you thank you and thank you .       I put it outside the env_vars but not nesetd .    

You are my hero! 

Thanks a million. ",python thank thank thank put outside hero thanks million,issue,positive,positive,neutral,neutral,positive,positive
1798070828,This interface is really very useful!  Strongly recommend that the community open this interface.,interface really useful strongly recommend community open interface,issue,positive,positive,positive,positive,positive,positive
1798063611,Hope to open the unified interface to support ray docking different hardware.:heart_eyes:,hope open unified interface support ray different hardware,issue,positive,neutral,neutral,neutral,neutral,neutral
1798027140,Strongly recommend that the community open this interface.,strongly recommend community open interface,issue,positive,positive,positive,positive,positive,positive
1798016610,Strongly recommend that the community open this interface！！！,strongly recommend community open,issue,positive,positive,positive,positive,positive,positive
1797984607,"Strongly recommend ray community provide standard interface to support different hardware, we need support ray run on ascend chip(NPU)",strongly recommend ray community provide standard interface support different hardware need support ray run ascend chip,issue,positive,positive,positive,positive,positive,positive
1797978121,Strongly recommend opening this interface to the community,strongly recommend opening interface community,issue,positive,positive,positive,positive,positive,positive
1797975570,Strongly recommend opening this interface to the community~~~,strongly recommend opening interface,issue,positive,positive,positive,positive,positive,positive
1797966562,This proposal could be really helpful. Pluginability will allow users to use it more flexibly and save troublesome processes. I recommended too.,proposal could really helpful allow use flexibly save troublesome,issue,positive,positive,positive,positive,positive,positive
1797966434,It is highly recommended to open this interface to the community!!,highly open interface community,issue,negative,neutral,neutral,neutral,neutral,neutral
1797870447,"Is your cluster machine the laptop, or is it a separate remote machine that you connect to with the laptop?",cluster machine separate remote machine connect,issue,negative,negative,neutral,neutral,negative,negative
1797822958,"Can you share how you're using your `runtime_env`?

From the original PR description, it looks like the value is not getting propograted:

```
(RayTrainWorker pid=83498) localhost:83498:84059 [0] NCCL INFO NCCL_SOCKET_IFNAME set to ^lo,docker,veth
```

Are you passing it in a nested `env_vars` dictionary?

```python
runtime_env = {""env_vars"": {""NCCL_SOCKET_IFNAME"": ""eno1,eth0""}}
ray.init(runtime_env=runtime_env)
```
",share original description like value getting set docker passing dictionary python,issue,positive,positive,positive,positive,positive,positive
1797800546,Have a single machine cluster setup - happens both running ray start or just in process. We have a similar setup on production so its a little worrying.,single machine cluster setup running ray start process similar setup production little worrying,issue,negative,negative,neutral,neutral,negative,negative
1797718282,"I can spend a little time digging into `sphinx_copybutton` to see if there's something there I can change. For now, I'd propose closing this issue as a duplicate of #39652.",spend little time digging see something change propose issue duplicate,issue,negative,negative,negative,negative,negative,negative
1797715770,"I was never able to reproduce this issue on my local machines, and I expect it to depend on what browser and what version you're using. There are CSS styles which control the selectability of elements, but they're browser-dependent. Even forcibly overriding the styles of the `<span>` elements that contain the numbers didn't seem to work for @angelinalg. Being unable to replicate the problem I'm unsure exactly what the best way to proceed with this issue would be. See https://github.com/ray-project/ray/issues/39652 for more information.",never able reproduce issue local expect depend browser version control even forcibly span contain seem work unable replicate problem unsure exactly best way proceed issue would see information,issue,negative,positive,positive,positive,positive,positive
1797696728,"This is a known problem. The workaround is to turn off line numbers, but in these cases the line numbers are needed to refer to specific lines of code in the docs. This is a docs infrastructure/tooling issue. @peytondmurray, do you have any ideas on how to fix this?",known problem turn line line refer specific code issue fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1797367976,"> Hi, this typically happens if `NCCL_SOCKET_IFNAME` is not configured properly. The provided default may not work based on your setup. Do you know what is the desired value?

here is my head : 
![image](https://github.com/ray-project/ray/assets/26159816/9c1133c2-0515-4aa2-a71e-75a0d4dfa733)
here is my worker node:
![image](https://github.com/ray-project/ray/assets/26159816/56a1f9ac-91a8-4a00-9211-cda2476a145f)
here is my runtime_env:
![image](https://github.com/ray-project/ray/assets/26159816/196253ca-f6ac-46ff-9529-06dbbf012f03)
here is the output. 
![image](https://github.com/ray-project/ray/assets/26159816/5048d583-02e7-44da-a244-6fab8b2e8e0d)


From the output, we can see this line:

(RayTrainWorker pid=33421) localhost:33421:34190 [0] NCCL INFO Bootstrap : Using eno1:192.168.0.107<0>

I have struggled on this issue for a very long time. Any help will be appreciated. ",hi typically properly provided default may work based setup know desired value head image worker node image image output image output see line bootstrap issue long time help,issue,positive,negative,neutral,neutral,negative,negative
1797299707,@wingkitlee0 Nice! I don't have the state at the moment but I can recreate it in the next couple of days (doing an internal demo in a couple of days) and test this fix. It does sound like it's very likely the issue.,nice state moment recreate next couple day internal couple day test fix sound like likely issue,issue,positive,positive,positive,positive,positive,positive
1797237146,I will discuss this with kuberay team and find the right solution by 2.9,discus team find right solution,issue,negative,positive,positive,positive,positive,positive
1797163522,"> Dark mode: Logos don't look good on this page: https://anyscale-ray--39766.com.readthedocs.build/en/39766/train/more-frameworks.html

I look the time to rewrite this: the cards weren't clickable, instead a button was being embedded inside the card leading to some awkward spacing. I added some margin to the tops of the images, this is the result:

![image](https://github.com/ray-project/ray/assets/14017872/8244d910-f290-43fd-af65-e5fb602be0dc)

This looks a little better, but as an alternative we can also replace these with SVGs that have dynamically colored text that can change according to the theme, as we do elsewhere. I think this would be better - what do you think @angelinalg?",dark mode logo look good page look time rewrite instead button inside card leading awkward spacing added margin top result image little better alternative also replace dynamically colored text change according theme elsewhere think would better think,issue,positive,positive,positive,positive,positive,positive
1797163451,@bveeramani Thanks for taking a look. I left some comments and will update the PR in a few days.,thanks taking look left update day,issue,negative,positive,neutral,neutral,positive,positive
1797158330,"Yes, feel free to re-assign it to me. The priority discussion can come into next sprint + headcount updates.",yes feel free priority discussion come next sprint,issue,positive,positive,positive,positive,positive,positive
1797134147,@dioptre could you give more details on your setup? Are you running the Ray cluster entirely on your laptop? How are you connecting to the Ray cluster? cc @rkooo567 ,could give setup running ray cluster entirely ray cluster,issue,negative,neutral,neutral,neutral,neutral,neutral
1797106261,"That's correct!

@vdesai2014 I'd love to talk in person and discuss how to achieve this. would you be open to have a call? ",correct love talk person discus achieve would open call,issue,positive,positive,positive,positive,positive,positive
1797096638,Awesome! Thanks @jrosti Feel free to reopen the issue if you have other question!,awesome thanks feel free reopen issue question,issue,positive,positive,positive,positive,positive,positive
1797094573,"Found this commit https://github.com/ray-project/ray/commit/5ac06e9b5daee18d206726fcdd5c89be633c60d1 which was landed to 2.8.0.

After upgrading, I was not able to reproduce the job table freeze anymore. Additionally, the fix in above commit seems to match perfectly to observations above. ",found commit landed able reproduce job table freeze additionally fix commit match perfectly,issue,positive,positive,positive,positive,positive,positive
1797045886,"@rkooo567 or @jjyao, is :point_up: this something you'd still like to pursue? The fix here is somewhat unclear.",something still like pursue fix somewhat unclear,issue,negative,neutral,neutral,neutral,neutral,neutral
1797042022,"@ResidentMario , I suspect this is the same issue as https://github.com/ray-project/ray/issues/40945 (which I have a fix PR)
can you try including `key` column (i.e., partition columns) in the `columns=FEATURE_COLUMNS` ?",suspect issue fix try key column partition,issue,negative,neutral,neutral,neutral,neutral,neutral
1797007533,Thanks for the info @jrosti! I haven't go through all the code path you mentioned. But maybe it's just a matter of setting a timeout. Will look into it a bit more!,thanks go code path maybe matter setting look bit,issue,negative,positive,positive,positive,positive,positive
1797005514,"For me it looks clear that GCS server tries to communicate with dead worker, and in case the networking appears to be so that it goes to TCP timeouts instead of refused, it will cause large delays. 

Hope this helps!",clear server communicate dead worker case go instead cause large hope,issue,negative,positive,neutral,neutral,positive,positive
1797003962,"Since Ray has observed that all nodes are dead, it should have marked all jobs as dead, so I hope that `void GcsJobManager::HandleGetAllJobInfo` in `gcs_job_manager.cc` goes into this branch:

```
      if (data.second.is_dead()) {
        reply->mutable_job_info_list(i)->set_is_running_tasks(false);
        core_worker_clients_.Disconnect(worker_id);
        (*num_processed_jobs)++;
        ;
        try_send_reply();
      } else {
```

^ but here I'm curious that why it calls `.Disconnect`? Does that communicate with dead worker? 

However, not able to proceed further since I haven't set compilation environment for the Ray GCS Server and can't inject it to my cluster. ",since ray dead marked dead hope void go branch false else curious communicate dead worker however able proceed since set compilation environment ray server ca inject cluster,issue,negative,negative,neutral,neutral,negative,negative
1797000552,"Hi, this typically happens if `NCCL_SOCKET_IFNAME` is not configured properly. The provided default may not work based on your setup. Do you know what is the desired value?",hi typically properly provided default may work based setup know desired value,issue,positive,neutral,neutral,neutral,neutral,neutral
1796999887,Sorry somehow I missed the ping. Will review in morning. You have a merge conflict FYI,sorry somehow ping review morning merge conflict,issue,negative,negative,negative,negative,negative,negative
1796970497,"@Allenyang2, are you able to share logs under `/tmp/ray/session_latest/logs/*`?

> It seems the main issue here is that the head node is not able to communicate with the worker nodes.

How do you know that?",able share main issue head node able communicate worker know,issue,negative,positive,positive,positive,positive,positive
1796968573,"I will assign P2 until it is followed up. Please try. I couldn't repro it with ray 2.6.3 + grpcio 1.57

1. update the grpcio version (1.48 has been yanked)
2. update dashboard_agent.log file contents",assign please try could ray update version update file content,issue,negative,neutral,neutral,neutral,neutral,neutral
1796960925,If anyone from community also wants this/take this issue we are open to it.,anyone community also issue open,issue,negative,neutral,neutral,neutral,neutral,neutral
1796955463,You should be able to specify your required lib versions in requirements.txt (or however you are configuring your dependencies) Ray does not have any upper bound for protobuf.,able specify however ray upper bound,issue,negative,positive,positive,positive,positive,positive
1796886257,"And, as I suspected, before the large delays started to occur, the logs show

```
[2023-11-06 21:57:23,745 W 9 9] (gcs_server) gcs_job_manager.cc:227: Failed to get is_running_tasks from core worker: GrpcUnavailable: RPC Error message: failed to connect to all addresses; last error: UNKNOWN: ipv4:10.xxxx:10003: Failed to connect to remote host: Connection refused; RPC Error details: 
```

Connection refused messages are instantaneously observed as a failure. When GKE scales it away, it tries to reach driver and TCP timeouts starts to accumulate delays.",suspected large occur show get core worker error message connect last error unknown connect remote host connection error connection instantaneously failure scale away reach driver accumulate,issue,negative,negative,neutral,neutral,negative,negative
1796877002,"yea, I agree the example is not ideal, Ray core is not suitable if the task is very short since the overhead of starting worker processes may outweigh the saving from parallel execution.",yea agree example ideal ray core suitable task short since overhead starting worker may outweigh saving parallel execution,issue,positive,positive,positive,positive,positive,positive
1796861940,"Which turns out to wait in `get_all_job_info` in `job/utils.py`, and it goes to GCS Server:

gcs_server.out
```
[2023-11-06 22:17:09,134 I 9 9] (gcs_server) gcs_job_manager.cc:149: Getting all job info.
[2023-11-06 22:18:02,627 W 9 9] (gcs_server) gcs_job_manager.cc:227: Failed to get is_running_tasks from core worker: GrpcUnavailable: RPC Error message: failed to connect to all addresses; last error: UNKNOWN: ipv4:10.xxxx:10003: Failed to connect to remote host: Connection timed out; RPC Error details: 
[2023-11-06 22:18:02,627 W 9 9] (gcs_server) gcs_job_manager.cc:227: Failed to get is_running_tasks from core worker: GrpcUnavailable: RPC Error message: failed to connect to all addresses; last error: UNKNOWN: ipv4:10.xxxx:10003: Failed to connect to remote host: Connection timed out; RPC Error details: 
[2023-11-06 22:18:14,558 W 9 9] (gcs_server) gcs_job_manager.cc:227: Failed to get is_running_tasks from core worker: GrpcUnavailable: RPC Error message: failed to connect to all addresses; last error: UNKNOWN: ipv4:10.xxxxx:10003: tcp handshaker shutdown; RPC Error details: 
[2023-11-06 22:18:14,558 I 9 9] (gcs_server) gcs_job_manager.cc:185: Finished getting all job info.
```
",turn wait go server getting job get core worker error message connect last error unknown connect remote host connection timed error get core worker error message connect last error unknown connect remote host connection timed error get core worker error message connect last error unknown handshaker shutdown error finished getting job,issue,negative,negative,neutral,neutral,negative,negative
1796856738,"@kevin85421 why does autoscaler with kuberay require the --no-monitor flag in order to work?

cc @rkooo567 @rynewang ",require flag order work,issue,negative,neutral,neutral,neutral,neutral,neutral
1796848312,"@emmyscode Would you mind rebasing and if tests pass, please ask @can-anyscale for help merging.",would mind pas please ask help,issue,positive,neutral,neutral,neutral,neutral,neutral
1796843561,@larroy could you submit a PR with fix; should be straightforward and we can review.,could submit fix straightforward review,issue,negative,positive,positive,positive,positive,positive
1796736700,"Hi @sven1977
Can you give us couple advises to investigate the issue further. Do you think it is algorithms specific for PPO?",hi give u couple investigate issue think specific,issue,negative,neutral,neutral,neutral,neutral,neutral
1796516685,"Hi, 

I am able to reproduce issue with 2.7.0 with a condition that I define an entrypoint resource to a node which is scaled away first by Ray autoscaler, an later the underlying node by GKE. The delay does not occur before the GKE node scaleout occurs (likely suggesting that underlying tcp connection semantics changes after that from refused to timeout for some service?)

I produced timing trace from `list_jobs` in job_head.py which calls `get_all_jobs` in `common.py`

All time is consumed in:
```
        driver_jobs, submission_job_drivers = await get_driver_jobs(
            self._dashboard_head.gcs_aio_client
        ) 
```
which takes 111 seconds. 

Everything else what happens in the function takes 1 second. 


",hi able reproduce issue condition define resource node scaled away first ray later underlying node delay occur node likely suggesting underlying connection semantics service produced timing trace time await everything else function second,issue,negative,positive,positive,positive,positive,positive
1796513855,"Ray Train no longer has ""preprocessors"". Metadata field in the Trainers can be used to save arbitrary data with ckpts.",ray train longer field used save arbitrary data,issue,negative,negative,neutral,neutral,negative,negative
1796511990,@yunsangju This issue may have been caused by OOM due to multiple redundant copies of the model checkpoint in memory. Ray 2.7+ should fix this issue -- have you tried it with the latest version of Ray?,issue may due multiple redundant model memory ray fix issue tried latest version ray,issue,negative,positive,neutral,neutral,positive,positive
1796505567,"
> What about the update done in `_scheduling_loop_step` for the execution codepath?

Ah, that one is probably okay since it's running on the driver. For latency-critical scenarios, the iter_batches loop is usually run on a different process. Also, the granularity in the scheduling loop is coarser  - there can be lots of batches produced by one Data op task.
",update done execution ah one probably since running driver loop usually run different process also granularity loop lot produced one data task,issue,negative,negative,negative,negative,negative,negative
1796492991,"> @stephanie-wang regarding the `numpy_load_args` test, I considered a few options:
> 
>     1. Ideally, we mock `np.load` and check that it's called with the keyword-arguments. This doesn't work because mocks don't play well with remote tasks.
> 
>     2. Another option is to configure a specific keyword-argument and check that it applies. The problem is that none of the keyword-arguments make sense: don't think you'd ever want to set `mmap_mode` (and even if you did I'm not sure if there's any way for us to test that it was set); `allow_pickle` is hardcoded as `True`; `fix_imports` and `encoding` only make sense if you're loading Python 2 files; and `max_header_size` is mutually exclusive with `allow_pickle` (which is hardcoded).
> 
> 
> Do you have any ideas abotu how we could test this? One other option is to just remove the parameter altogether. I doubt anyone is getting any value out of it.

Hmm I see, thanks. I think it's fine to leave it in for now (for the sake of not changing too many things in the migration). And I suppose the lack of testing is okay, given that we're probably not covering all the other datasources either...",regarding test considered ideally mock check work play well remote another option configure specific check problem none make sense think ever want set even sure way u test set true make sense loading python mutually exclusive could test one option remove parameter altogether doubt anyone getting value see thanks think fine leave sake many migration suppose lack testing given probably covering either,issue,positive,positive,positive,positive,positive,positive
1796422592,ah ok i was not aware of that! i had noticed if i delete the tmp files that are not in use mid trials that it fails but maybe i did it wrong and deleted some that were mid use!,ah aware delete use mid maybe wrong mid use,issue,negative,negative,neutral,neutral,negative,negative
1796359983,"If you're using Ray Tune, the paused trials should be pointing to checkpoints that are persisted after calling `train.report`. These would be in a location defined in `RunConfig.storage_path`, rather than the temporary ones here. ",ray tune pointing calling would location defined rather temporary,issue,negative,neutral,neutral,neutral,neutral,neutral
1796340109,"This should be resolved in 2.8+ by https://github.com/ray-project/ray/pull/39841.

Feel free to re-open if you're still seeing this issue.",resolved feel free still seeing issue,issue,positive,positive,positive,positive,positive,positive
1796298023,"@stephanie-wang regarding the `numpy_load_args` test, I considered a few options:

1. Ideally, we mock `np.load` and check that it's called with the keyword-arguments. This doesn't work because mocks don't play well with remote tasks.
2. Another option is to configure a specific keyword-argument and check that it applies. The problem is that none of the keyword-arguments make sense: don't think you'd ever want to set `mmap_mode` (and even if you did I'm not sure if there's any way for us to test that it was set); `allow_pickle` is hardcoded as `True`; `fix_imports` and `encoding` only make sense if you're loading Python 2 files; and `max_header_size` is mutually exclusive with `allow_pickle` (which is hardcoded).

Do you have any ideas abotu how we could test this? One other option is to just remove the parameter altogether. I doubt anyone is getting any value out of it.",regarding test considered ideally mock check work play well remote another option configure specific check problem none make sense think ever want set even sure way u test set true make sense loading python mutually exclusive could test one option remove parameter altogether doubt anyone getting value,issue,positive,positive,positive,positive,positive,positive
1796167378,"Hey @dtimokhin12, thanks for creating this issue and sharing your solution with the community!

I'm going to mark this as a duplicate of https://github.com/ray-project/ray/issues/40714. This will be fixed by https://github.com/ray-project/ray/pull/40940.",hey thanks issue solution community going mark duplicate fixed,issue,positive,positive,positive,positive,positive,positive
1796145332,"> @drukoz are you using the `ReportCheckpointCallback`? If so, would it suffice if we deleted the tempdir after the checkpoint is reported?

Hello Matthew,
Yes and maybe i think that would work my only issue that might be that depending on the type of search algo those tmp files might be required to restart paused trials and depending on samples and concurrency/resources it might fill up the tmp folder regardless! 
hence why setting the folder would really be the best solution. this obviously being an optional parameter. 
",would suffice hello yes maybe think would work issue might depending type search might restart depending might fill folder regardless hence setting folder would really best solution obviously optional parameter,issue,positive,positive,positive,positive,positive,positive
1795956779,"> Hmm the 1% overhead is not too bad, but I think it's probably best to put it in a background thread still (to allow shorter interval and make the code a bit more robust).

Sounds good 👍 . 

> No need to start new threads; you can put the stats update in the same loop as `format_batches`, which is already run on a threadpool: https://github.com/ray-project/ray/blob/master/python/ray/data/_internal/block_batching/iter_batches.py#L225

What about the update done in `_scheduling_loop_step` for the execution codepath?",overhead bad think probably best put background thread still allow shorter interval make code bit robust good need start new put update loop already run update done execution,issue,positive,positive,positive,positive,positive,positive
1795927312,"Hmm the 1% overhead is not too bad, but I think it's probably best to put it in a background thread still (to allow shorter interval and make the code a bit more robust).

No need to start new threads; you can put the stats update in the same loop as `format_batches`, which is already run on a threadpool: https://github.com/ray-project/ray/blob/master/python/ray/data/_internal/block_batching/iter_batches.py#L225",overhead bad think probably best put background thread still allow shorter interval make code bit robust need start new put update loop already run,issue,positive,positive,positive,positive,positive,positive
1795766195,"Also timed the overhead for starting a thread on the same test:
```
total time: 104.5556s, thread submission time: 0.0591s
total time: 105.8269s, thread submission time: 0.05925s
total time: 104.9419s,  thread submission time: 0.05904s
total time: 104.4344s, thread submission time: 0.05956s
total time: 67.5454s, thread submission time: 0.03783s
total time: 67.0104s, thread submission time: 0.04410s
total time: 61.5307s, thread submission time: 0.03878s
total time: 61.5185s, thread submission time: 0.05433s
total time: 158.3527s thread submission time: 0.08616s
total time: 175.7912s, thread submission time: 0.1032s
total time: 160.6391s, thread submission time: 0.08617s
total time: 174.0594s, thread submission time: 0.09124s
``` ",also timed overhead starting thread test total time thread submission time total time thread submission time total time thread submission time total time thread submission time total time thread submission time total time thread submission time total time thread submission time total time thread submission time total time thread submission time total time thread submission time total time thread submission time total time thread submission time,issue,negative,neutral,neutral,neutral,neutral,neutral
1795742870,Also updating tags so the dashboard test runs against serve changes,also dashboard test serve,issue,negative,neutral,neutral,neutral,neutral,neutral
1795695628,"w00t, @edoakes , I think this might break the serve dashboard tests on postmerge https://buildkite.com/ray-project/postmerge/builds/1570.

The serve dashboard test was not run during premerge which we should fix as well.",think might break serve dashboard serve dashboard test run fix well,issue,negative,neutral,neutral,neutral,neutral,neutral
1795598545,@edoakes Yes but it's pretty low priority right now,yes pretty low priority right,issue,positive,positive,positive,positive,positive,positive
1795520482,@jrosti Ping again to see if you have more logs and reproduction steps that you can share or if this is consider resolved🙏,ping see reproduction share consider resolved,issue,negative,neutral,neutral,neutral,neutral,neutral
1795514150,@AndreKuu Ping again to see if you have tried it on newer version of Ray and still seeing this issue or if we can consider this resolved?,ping see tried version ray still seeing issue consider resolved,issue,negative,neutral,neutral,neutral,neutral,neutral
1795465780,"Just want to drop my +1 on better documentation of autoscaling behaviour as well as options for providing same resource, different launch types (spot / on-demand) nodes. Current behaviour is to retry the same node type indefinitely which leads to errors if capacity is not available. ",want drop better documentation behaviour well providing resource different launch spot current behaviour retry node type indefinitely capacity available,issue,negative,positive,positive,positive,positive,positive
1795260450,"@smit-kiri you can now set `max_replicas_per_node` to specify how much you'd like to spread your replicas. If you want a strict spread, set this to `1`.",set specify much like spread want strict spread set,issue,negative,positive,positive,positive,positive,positive
1795235048,Unable to reproduce and newly-added Python 3.11 tests are consistently passing; please file a new issue with a reproduction if you run into something similar!,unable reproduce python consistently passing please file new issue reproduction run something similar,issue,negative,negative,neutral,neutral,negative,negative
1795207036,@edoakes no further things you need to do; just closing the issue and have at least one pass run after that moves the test to the passing state ;). The state is in database so you won't see it in the code base.,need issue least one pas run test passing state state wo see code base,issue,negative,negative,negative,negative,negative,negative
1795195870,"Premerge is failing repeatedly with docker: Error response from daemon: manifest for 029272617770.dkr.ecr.us-west-2.amazonaws.com/rayproject/citemp:3664a98e-forge not found: manifest unknown: Requested image not found. https://buildkite.com/ray-project/premerge/builds/10825#018ba553-01e6-4d03-9848-441d8979c14a/185-187

My guess is restarting premerge won't fix this error.  So I'm going to merge master to restart the entire CI build.",failing repeatedly docker error response daemon manifest found manifest unknown image found guess wo fix error going merge master restart entire build,issue,negative,negative,neutral,neutral,negative,negative
1795190996,@can-anyscale is there anything that needs to be done to un-jail the release tests? Still seeing `[jailed]` in the latest test runs but grepping around the codebase and not seeing it,anything need done release still seeing latest test around seeing,issue,negative,positive,positive,positive,positive,positive
1795185701,"test_memory_pressure unrelated
Windows serve tests unrelated
Windows wheels failure unrelated: ""RuntimeError: Detected Python version 3.7, which is not supported. Only Python 3.8, 3.9, 3.10, 3.11 are supported.""",unrelated serve unrelated failure unrelated python version python,issue,negative,negative,negative,negative,negative,negative
1795076822,"It only happens if you do a ray start with the `--no-monitor` argument:
![image](https://github.com/ray-project/ray/assets/9578389/f169a765-412a-40e3-85e4-f8d212be0d77)


Here is when I remove the `--no-monitor` argument:
![image](https://github.com/ray-project/ray/assets/9578389/1aa12ab1-4c6c-4f75-94af-a87c87411f74)

This happens regardless of whether I'm using Kuberay or not. (Though I mention Kuberay because when we are using Kuberay, we want to use the autoscaler, which requires the `--no-monitor` argument).",ray start argument image remove argument image regardless whether though mention want use argument,issue,negative,neutral,neutral,neutral,neutral,neutral
1794230567,"@architkulkarni I have tested with Ray 2.8, and the issue still persists.",tested ray issue still,issue,negative,neutral,neutral,neutral,neutral,neutral
1794172502,"@architkulkarni I'm afraid these two issues might have different causes. The core issue here is that when using ""--min-worker-port=0 --max-worker-port=0"", the driver port is always set to 0 when calling NumPendingTasks, which causes the NumPendingTasks to never succeed.",afraid two might different core issue driver port always set calling never succeed,issue,negative,negative,negative,negative,negative,negative
1794001657,"Hi there,

thanks for your work on this. We just wanted to report that this error is also tripping PyCaret's CI when attempting to add Python 3.11 to the test matrix.

With kind regards,
Andreas.

/cc @Yard1

### References
- https://github.com/ray-project/tune-sklearn/issues/283
- https://github.com/pycaret/pycaret/pull/3814#issuecomment-1794003113
- https://github.com/pycaret/pycaret/pull/3815#issuecomment-1793897533
",hi thanks work report error also tripping add python test matrix kind yard,issue,positive,positive,positive,positive,positive,positive
1793647577,"> > If we only count the number of items in CUDA_VISIBLE_DEVICES, it could lead to wrong results. For example, if you have 8 GPUs on the system but set CUDA_VISIBLE_DEVICES=""0,1,2,3,0"", you will get 0 CUDA devices rather than 5.
> 
> @XuehaiPan, why will I get 0 CUDA devices, currently Ray simply does a `split("","")` with no validations at all.

@jjyao Because there is a duplicate identifier 0 in the `CUDA_VISIBLE_DEVICES`. That invalid `CUDA_VISIBLE_DEVICES` variable will cause no CUDA devices for CUDA programs.

As I commented in https://github.com/ray-project/ray/issues/28064#issuecomment-1793365510, simply splitting the comma-separated list is insufficient to validate the environment variable.

```python
CUDA_VISIBLE_DEVICES = os.getenv('CUDA_VISIBLE_DEVICES', ''.join(map(str, range(num_physical_gpus))))
num_cuda_devices = min(num_physical_gpus, len(CUDA_VISIBLE_DEVICES.split(',')))
```

1. duplicated identifiers

    - `CUDA_VISBLE_DEVICES=""0,1,2,3,0""` will get 0 CUDA devices rather than 5.

2. invalid identifiers

    - `CUDA_VISBLE_DEVICES=""8""` will get 0 CUDA devices on an 8 GPU system.
    - `CUDA_VISBLE_DEVICES=""0,1,8,2,3""` will get 2 CUDA devices rather than 5 on an 8 GPU system.

3. mixing integers and UUIDs

    - `CUDA_VISBLE_DEVICES=""0,1,GPU-a1b2c3d4-e5f6,3""` will get 2 CUDA devices rather than 4.
    - `CUDA_VISBLE_DEVICES=""GPU-a1b2c3d4-e5f6,0,1,3""` will get 1 CUDA device rather than 4.

4. MIG device enumeration (by now, at most one MIG device can be used by a CUDA program)

    - `CUDA_VISBLE_DEVICES=""MIG-a1b2c3d4,MIG-e5f6a7b8""` will get 1 CUDA device rather than 2.

5. if there are both MIG-enabled and MIG-disabled GPUs on the system, CUDA will enumerate the MIG device first (when `CUDA_VISIBLE_DEVICES` is unset or set with MIG-enabled GPUs)

------

You can verify the above cases via [normalize_cuda_visible_devices](https://nvitop.readthedocs.io/en/latest/api/device.html#nvitop.normalize_cuda_visible_devices):

```python
In [1]: from nvitop import normalize_cuda_visible_devices

In [2]: normalize_cuda_visible_devices('0,1,2,3,0')
Out[2]: ''

In [3]: normalize_cuda_visible_devices('8')
Out[3]: ''

In [4]: normalize_cuda_visible_devices('0,1,8,2,3')
Out[4]: 'GPU-d8f503ec-bb34-4304-0053-5d9e62044184,GPU-7758abd0-62e7-a1db-c57e-084f6bc96b11'

In [5]: normalize_cuda_visible_devices('0,1,GPU-1b9855b7,3')
Out[5]: 'GPU-d8f503ec-bb34-4304-0053-5d9e62044184,GPU-7758abd0-62e7-a1db-c57e-084f6bc96b11'

In [6]: normalize_cuda_visible_devices('GPU-1b9855b7,0,1,3')
Out[6]: 'GPU-1b9855b7-640c-7c18-1b53-3d69a2ea51c4'
```",count number could lead wrong example system set get rather get currently ray simply split duplicate identifier invalid variable cause simply splitting list insufficient validate environment variable python map range min get rather invalid get system get rather system get rather get device rather mig device enumeration one mig device used program get device rather system enumerate mig device first unset set verify via python import,issue,negative,negative,neutral,neutral,negative,negative
1793635513,"> If we only count the number of items in CUDA_VISIBLE_DEVICES, it could lead to wrong results. For example, if you have 8 GPUs on the system but set CUDA_VISIBLE_DEVICES=""0,1,2,3,0"", you will get 0 CUDA devices rather than 5.

@XuehaiPan, why will I get 0 CUDA devices, currently Ray simply does a `split("","")` with no validations at all.",count number could lead wrong example system set get rather get currently ray simply split,issue,negative,negative,negative,negative,negative,negative
1793634716,"> Bisect and auto-revert blame PRs

do we still plan to do this part?",bisect blame still plan part,issue,negative,neutral,neutral,neutral,neutral,neutral
1793634174,macos tests are back on line. now sure how it was fixed though..,back line sure fixed though,issue,negative,positive,positive,positive,positive,positive
1793633119,remove devprod tag. does not seem to be devprod related?,remove tag seem related,issue,negative,neutral,neutral,neutral,neutral,neutral
1793632996,"> Unit tests lifecycle? Run more than 1 time in the master for every test
not sure what this means.

> CI: treat timeout as failure
this is already the case?

> We need to check and monitor worker startup time
this seems like a separate issue for core team.

It is unclear what the issue is or what is actionable, either now or in the future. I am closing this.",unit run time master every test sure treat failure already case need check monitor worker time like separate issue core team unclear issue actionable either future,issue,negative,positive,neutral,neutral,positive,positive
1793520788,I am not sure what is causing the tests to fail. Can someone point me in the correct direction to get all of the tests to pass and provide some feedback on possibly merging this PR?,sure causing fail someone point correct direction get pas provide feedback possibly,issue,negative,neutral,neutral,neutral,neutral,neutral
1793467601,"I think all the issues with failing tests were resolved. All RLlib tests are passing for this PR.
cc: @kouroshHakha ",think failing resolved passing,issue,negative,neutral,neutral,neutral,neutral,neutral
1793413831,"@raulchen 
Please find the new recording here, It Seems `ray::IDLE` is taking up most of the Ram, 4% for each process and there are multiple such processes  

[screen-capture 2.webm](https://github.com/ray-project/ray/assets/38434626/08bfd65d-2ae4-4101-9c6a-fb2d580fb277)

[screen-capture 3.webm](https://github.com/ray-project/ray/assets/38434626/83e14c8d-105d-4882-bf1c-dc1db22c5a92)

[screen-capture 1.webm](https://github.com/ray-project/ray/assets/38434626/5fea2eac-d477-4620-9829-0135cde4b127)
",please find new recording ray taking ram process multiple,issue,negative,positive,neutral,neutral,positive,positive
1793365510,"> currently Ray uses GPUtil to detect the number of physical GPU devices and also looks at CUDA_VISIBLE_DEVICES, the **min** is the number of GPU resources available to Ray.

@jjyao That's the number of CUDA visible devices.

If we only count the number of items in `CUDA_VISIBLE_DEVICES`, it could lead to wrong results. For example, if you have 8 GPUs on the system but set `CUDA_VISIBLE_DEVICES=""0,1,2,3,0""`, you will get 0 CUDA devices rather than 5.

Parsing the `CUDA_VISIBLE_DEVICES` via NVML APIs is non-trivial. There are many corner cases:

1. duplicated identifiers
1. invalid identifiers
1. mixing integers and UUIDs
1. UUID abbreviation
1. MIG device enumeration (by now, at most one MIG device can be used by a CUDA program)
1. two UUID formats for MIG devices: `MIG-GPU-<GPU-UUID>/<GI>/<CI>` pre-R470 and `MIG-<MIG-UUID>` (R470+)
1. if there are both MIG-enabled and MIG-disabled GPUs on the system, CUDA will enumerate the MIG device first (when `CUDA_VISIBLE_DEVICES` is unset or set with MIG-enabled GPUs)

    ...

IMO, the best practice is to use the CUDA driver library `libcuda` or the CUDA Runtime library `libcudart` to do this.",currently ray detect number physical also min number available ray number visible count number could lead wrong example system set get rather via many corner invalid abbreviation mig device enumeration one mig device used program two mig system enumerate mig device first unset set best practice use driver library library,issue,positive,positive,positive,positive,positive,positive
1793361942,"> If I understand correctly, we are interested in CUDA devices rather than physical GPU devices.

@XuehaiPan, currently Ray uses GPUtil to detect the number of physical GPU devices and also looks at CUDA_VISIBLE_DEVICES, the **min** is the number of GPU resources available to Ray.",understand correctly interested rather physical currently ray detect number physical also min number available ray,issue,positive,positive,positive,positive,positive,positive
1793350929,@xwjiang2010 is this problem resolved? facing the same issue with using cloud resources. ,problem resolved facing issue cloud,issue,negative,neutral,neutral,neutral,neutral,neutral
1793333612,"> What's your recommended way of auto detecting the number of gpus that works on all platforms?

@jjyao This depends on if we are detecting the number of _GPUs on the system_ or the number of _CUDA visible devices_ (respect the `CUDA_VISIBLE_DEVICES` environment variable). If I understand correctly, we are interested in CUDA devices rather than physical GPU devices.

### To detect GPU devices installed on the system

1. `nvidia-smi` + `subprocess` (NB: `GPUtil` is a wrapper around `nvidia-smi`):

```python
import subprocess

# May need error handling logic
output = subprocess.check_output(['nvidia-smi', '-L']).decode()
num_gpus = output.count('UUID: GPU-')
num_migs = output.count('UUID: MIG-')
```

2. `gpustat`:

```python
import gpustat

num_gpus = gpustat.gpu_count()
# num_migs not supported
```

3. `nvidia-ml-py`:

```python
import pynvml

# May need error handling logic
pynvml.nvmlInit()
num_gpus = pynvml.nvmlDeviceGetCount()

num_migs = 0
for index in range(num_gpus):
    handle = pynvml.nvmlDeviceGetHandleByIndex(index)
    try:
        max_mig_count = pynvml.nvmlDeviceGetMaxMigDeviceCount(handle)
    except pynvml.NVMLError_NotSupported:
        continue
    for mig_index in range(max_mig_count):
        try:
            mig_handle = pynvml.nvmlDeviceGetMigDeviceHandleByIndex(handle, mig_index)
        except pynvml.NVMLError_NotFound:
            break
        num_migs += 1

pynvml.nvmlShutdown()
```

4. `nvitop` (add a new dependency):

```python
from nvitop import Device, MigDevice

num_gpus = Device.count()
num_migs = MigDevice.count()
```

5. Write a C extension with `nvml.h` and link to `libnvidia-ml.so.1` as @wookayin suggested.

### To detect CUDA visible devices

1. `nvidia-smi`, `nvidia-ml-py`, `gpustat`:

    1. Detect the number of GPUs on the system.
    2. Parse the `CUDA_VISIBLE_DEVICES` environment variable.
    3. Verify that the devices specified in `CUDA_VISIBLE_DEVICES` are valid.

2. `torch` (add a huge dependency):

```python
import torch

# Note that torch also uses `nvidia-ml-py + parsing logic` to detect CUDA visible devices.
# This result may not always be correct.
num_cuda_visible_devices = torch.cuda.device_count()
```

3. `tensorflow` (add a huge dependency):

```python
import tensorflow as tf

num_cuda_visible_devices = len(tf.config.list_physical_devices('GPU'))
```

4. `jax` (add a huge dependency):

```python
import jax

num_cuda_visible_devices = jax.device_count('gpu')
```

5. `nvitop` (add a new dependency):

```python
from nvitop import CudaDevice

num_cuda_visible_devices = CudaDevice.count()

# or
from nvitop import Device

num_cuda_visible_devices = Device.cuda.count()

# or
num_cuda_visible_devices = len(Device.parse_cuda_visible_devices())
```

`nvitop` also provides a utility function to parse the `CUDA_VISIBLE_DEVICES` environment variable:

```python
>>> import os
>>> os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
>>> os.environ['CUDA_VISIBLE_DEVICES'] = '6,5'
>>> parse_cuda_visible_devices()        # parse the `CUDA_VISIBLE_DEVICES` environment variable to NVML indices
[6, 5]

>>> parse_cuda_visible_devices('0,4')   # pass the `CUDA_VISIBLE_DEVICES` value explicitly
[0, 4]

>>> parse_cuda_visible_devices('GPU-18ef14e9,GPU-849d5a8d')  # accept abbreviated UUIDs
[5, 6]

>>> parse_cuda_visible_devices(None)    # get all devices when the `CUDA_VISIBLE_DEVICES` environment variable unset
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

>>> parse_cuda_visible_devices('MIG-d184f67c-c95f-5ef2-a935-195bd0094fbd')           # MIG device support (MIG UUID)
[(0, 0)]
>>> parse_cuda_visible_devices('MIG-GPU-3eb79704-1571-707c-aee8-f43ce747313d/13/0')  # MIG device support (GPU UUID)
[(0, 1)]
>>> parse_cuda_visible_devices('MIG-GPU-3eb79704/13/0')                              # MIG device support (abbreviated GPU UUID)
[(0, 1)]

>>> parse_cuda_visible_devices('')      # empty string
[]
>>> parse_cuda_visible_devices('0,0')   # invalid `CUDA_VISIBLE_DEVICES` (duplicate device ordinal)
[]
>>> parse_cuda_visible_devices('16')    # invalid `CUDA_VISIBLE_DEVICES` (device ordinal out of range)
[]
```

6. Write a C extension with `cuda.h` and link to `libcuda.so.1`.

------

> I can think of these two ""simplest"" ways to get the count of GPUs without depending on a high-level library such as gpustat or nvitop (beaware it's GPLv3)

@wookayin About the concern of the `nvitop`'s license. `nvitop` is released under dual license with `Apache-2.0 + GPL-3.0`. The CLI part of `nvitop` is released under the GPL-3.0 license while the API part is released under the Apache-2.0 license. See [nvitop: copyright-notice](https://github.com/XuehaiPan/nvitop#copyright-notice).

```python
import nvitop              # Apache-2.0
from nvitop import *       # Apache-2.0
from nvitop import Device  # Apache-2.0

from nvitop import gui     # GPL-3.0
```",way auto number work number number visible respect environment variable understand correctly interested rather physical detect system wrapper around python import may need error handling logic output python import python import may need error handling logic index range handle index try handle except continue range try handle except break add new dependency python import device write extension link detect visible detect number system parse environment variable verify valid torch add huge dependency python import torch note torch also logic detect visible result may always correct add huge dependency python import add huge dependency python import add new dependency python import import device also utility function parse environment variable python import o parse environment variable index pas value explicitly accept none get environment variable unset mig device support mig mig device support mig device support empty string invalid duplicate device ordinal invalid device ordinal range write extension link think two way get count without depending library concern license dual license part license part license see python import import import device import,issue,positive,positive,positive,positive,positive,positive
1793312284,Probably need advice on where to add unit test. `test_consumption.py`?,probably need advice add unit test,issue,negative,neutral,neutral,neutral,neutral,neutral
1793268421,Also kudos to @mak-454 for bringing that line of code to the team's attention. ,also kudos line code team attention,issue,positive,neutral,neutral,neutral,neutral,neutral
1793237155,"@edoakes it's 3 days (https://github.com/ray-project/ray/blob/master/release/ray_release/test_automation/state_machine.py#L17).

For release tests, jailing just means to stop running them on automatic run, they are still release-blocking as usual. Mostly for cost saving on re-running things that we know will fail.",day release stop running automatic run still usual mostly cost saving know fail,issue,negative,negative,neutral,neutral,negative,negative
1793232436,"> Are there any release tests that we should run on this PR or is CI enough?

@stephanie-wang `read_images_comparison_microbenchmark_single_node` is the only one I'm aware is directly affected. I'm running it on this PR",release run enough one aware directly affected running,issue,negative,positive,positive,positive,positive,positive
1793192084,"I do not see the behavior in Ray 2.7.1. Would you mind providing a detailed KubeRay reproduction?

<img width=""1439"" alt=""Screen Shot 2023-11-03 at 
3 27 33 PM"" src=""https://github.com/ray-project/ray/assets/20109646/995e99f0-7ce5-4c63-b5a8-e0a99aa40ab2"">

<img width=""1920"" alt=""Screen Shot 2023-11-03 at 3 25 34 PM"" src=""https://github.com/ray-project/ray/assets/20109646/fba33088-18a0-4874-a9fd-9781be510a25"">

",see behavior ray would mind providing detailed reproduction screen shot screen shot,issue,negative,positive,positive,positive,positive,positive
1793128205,"Ran `iter_tensor_batches_benchmark_multi_node.aws` [here](https://buildkite.com/ray-project/release/builds/373#018b96bc-3a8a-4b6a-b0b7-9d56c22d70c6). The task submission overheads for `update_stats_actor_iter_metrics`:
```
total time: 104.429s, task submission time: 1.0405s
total time: 105.1504s, task submission time: 0.8603s
total time: 103.7166s, task submission time: 0.7929s
total time: 104.0425s, task submission time: 1.0805s
total time: 65.9899s, task submission time: 0.2991s
total time: 65.9907s, task submission time: 0.29166s
total time: 59.3974s, task submission time: 0.1291s
total time: 59.0898s, task submission time: 0.13269s
total time: 155.84426s, task submission time: 0.5009s
total time: 172.6297s, task submission time: 0.28232s
total time:  157.827s, task submission time: 1.6452s,
total time: 171.9112s, task submission time: 0.28234s
```",ran task submission total time task submission time total time task submission time total time task submission time total time task submission time total time task submission time total time task submission time total time task submission time total time task submission time total time task submission time total time task submission time total time task submission time total time task submission time,issue,negative,neutral,neutral,neutral,neutral,neutral
1793094426,"@kevin85421 you might need to write your own one.

This is what I can find https://github.com/ray-project/ray/blob/master/python/ray/tests/test_advanced_9.py#L357
",might need write one find,issue,negative,neutral,neutral,neutral,neutral,neutral
1793066658,@can-anyscale what's the SLA for fixing before it gets jailed? 2-3 days for a release test seems like not very long given they run nightly,sla fixing day release test like long given run nightly,issue,negative,negative,neutral,neutral,negative,negative
1792983265,"> Looks reasonable to me, LMK when it's fully ready for review w/ tests

hi @edoakes , added tests for config deploy and controller crash. PTAL!",reasonable fully ready review hi added deploy controller crash,issue,negative,positive,positive,positive,positive,positive
1792977990,I gave it P0 because the commands in the next steps just don't work. It's a P0 usability/UX issues. Feel free to downgrade from the resource/priority perspective.,gave next work feel free downgrade perspective,issue,positive,positive,positive,positive,positive,positive
1792977893,"the tests are failing. Let's hold onto merging until the issue is resolved. @can-anyscale Can you tell me what is wrong with the tests? All rllib-tests are complaining about a grpc plugin missing. 
https://buildkite.com/ray-project/premerge/builds/10829#_
",failing let hold onto issue resolved tell wrong missing,issue,negative,negative,negative,negative,negative,negative
1792963033,"Fixed. @kouroshHakha, thanks for the review! Please take another look.",fixed thanks review please take another look,issue,positive,positive,positive,positive,positive,positive
1792921085,"I can think of these two ""simplest"" ways to get the count of GPUs without depending on a high-level library such as gpustat or nvitop (beaware it's GPLv3):

- Reading the output of `nvidia-smi -L | wc -l` (or the # of lines from `nvidia-smi -L` for windows, though this depends on an external binary). It might be possible that GPU drivers are working but this external tool `nvidia-smi` might be just unavailable.
- `pynvml`: `pynvml.nvmlInit()` + `pynvml.nvmlDeviceGetCount()`. Depends on an external python library, `nvidia-ml-py`.

If you prefer neither of these, you can write a native extension that uses `nvml.h` and the shared library from the nvidia driver to directly call the C API [nvmlDeviceGetCount_v2](https://docs.nvidia.com/deploy/nvml-api/group__nvmlDeviceQueries.html#group__nvmlDeviceQueries_1ga93623b195bff04bbe3490ca33c8a42d). No dependencies, but at the cost of all the build hassles.",think two way get count without depending library reading output though external binary might possible working external tool might unavailable external python library prefer neither write native extension library driver directly call cost build,issue,negative,positive,neutral,neutral,positive,positive
1792897906,@MissiontoMars I think this issue might be related: https://github.com/ray-project/ray/issues/39947 It was fixed in Ray 2.8.  Does the issue persist for you in Ray 2.8?,think issue might related fixed ray issue persist ray,issue,negative,positive,neutral,neutral,positive,positive
1792895563,Closing for now as no response @jdonzallaz please re-open if these does repro on latest ray release (28 at time of writing).,response please latest ray release time writing,issue,negative,positive,positive,positive,positive,positive
1792783055,"> Should we still submit the stats update tasks on a different thread? If we're doing it every 10s the overhead is a lot less significant

Could you measure the latency for the task submission? You can use one of the release tests. We just have to be careful here because even if the % time is small, it can block GPU time in training scenarios (i.e. tail latency is important, not just average).

If we put it on a background thread, we could also reduce the interval a bit to get more interactivity in the metrics.",still submit update different thread every overhead lot le significant could measure latency task submission use one release careful even time small block time training tail latency important average put background thread could also reduce interval bit get interactivity metric,issue,negative,positive,neutral,neutral,positive,positive
1792733104,Should we still submit the stats update tasks on a different thread? If we're doing it every 10s the overhead is a lot less significant,still submit update different thread every overhead lot le significant,issue,negative,positive,positive,positive,positive,positive
1792727741,"Discussed offline, we'll just add a better import warning.  @edoakes please take another look",add better import warning please take another look,issue,negative,positive,positive,positive,positive,positive
1792720518,"@XuehaiPan 

I'm catching up here. What's your recommended way of auto detecting the number of gpus that works on all platforms: we have GPUtil, gpustat, nvidia-ml-py, and your nvitop/libcuda.py.",catching way auto number work,issue,negative,positive,positive,positive,positive,positive
1791955179,"It now takes 16.64s down from 138.9s. I'll close the issue, but ideally, we want `repr` to be instant.",close issue ideally want instant,issue,negative,positive,positive,positive,positive,positive
1791950580,Closing in favor of https://github.com/ray-project/ray/pull/40900 because `ParquetDatasource` is coupled with `FileBasedDatasource` and it's hard to split up the PRs,favor coupled hard split,issue,negative,negative,negative,negative,negative,negative
1791891398,"> I will try, but it is frustrating to have to do this work twice. I hope this time it gets a review.

Completely understand the frustration. Sorry again for the late review.

",try work twice hope time review completely understand frustration sorry late review,issue,negative,negative,negative,negative,negative,negative
1791843106,"@architkulkarni Could you please help me take a look at this issue? Thank you.

",could please help take look issue thank,issue,positive,neutral,neutral,neutral,neutral,neutral
1791797017,"Hmm, the doc test harness such as `num_blocks=8,` is literally a string, so it cannot be a programmable value such as `num_blocks=cpu_count()`. Going back to be a concrete string here. I'll leave it to data team if they can improve its robustness.",doc test harness literally string value going back concrete string leave data team improve robustness,issue,positive,positive,neutral,neutral,positive,positive
1791765898,"@aloysius-lim On [ray nightly](https://docs.ray.io/en/latest/ray-overview/installation.html), this logic has been updated so that it should no longer error for you. Let me know if it works out for you.

@fardinabbasi Your issue should also be solved by the same PR.",ray nightly logic longer error let know work issue also,issue,negative,neutral,neutral,neutral,neutral,neutral
1791746972,@angelinalg did we remove this guide? I can't seem to find it in the documentation or source code,remove guide ca seem find documentation source code,issue,negative,neutral,neutral,neutral,neutral,neutral
1791735361,"> Thanks! When troubleshooting I stumbled on a discussion online pointing to [here](https://github.com/ray-project/ray/blob/8d286f03ce39e8856d1ffaa20f377e4bda24dc26/python/ray/serve/deployment.py#L255-L267) and noting the deprecation message. But you're right, yes, serve deploy does seem to be the right way to do it. (Sorry for the confusion. I'm brand new to Ray and still trying to get my footing.)

Ah got it, thanks for elaborating! No worries at all about the confusion btw– that just means we can do a better job with our docs 🙂

> serve run -a http://localhost:8265 serve_config.yml

`serve run` uses Ray client to connect to the Ray cluster. The Ray client address format is `ray://[hostname]:[head node port]`. By default, the head node port is `10001`. For your example, the address would likely be `ray://[public ip address of EC2 instance]:10001`. See [this docs section](https://docs.ray.io/en/releases-2.7.1/serve/advanced-guides/dev-workflow.html#testing-on-a-remote-cluster) for more info.

Note that the EC2 instance should already have a Ray cluster running, so it can respond to the `serve run` command.

> But I guess best practice is not to use port-forwarding but to ssh to the remote cluster and run ray deploy via the remote shell. The follow workflow works for me on AWS EC2.

You should be able to run `serve deploy` without attaching to the remote cluster using `ray attach`. `serve deploy`'s `--address` option takes an address to the remote cluster's dashboard agent. Setting that address should let `serve deploy` submit the config file to the remote cluster. If that doesn't work, then there's probably a bug.

By the way, starting in Ray 2.8 the Serve REST API will also be accessible through the dashboard port (8265). That means `serve deploy` will work with port `8265` as you tried.",thanks discussion pointing deprecation message right yes serve deploy seem right way sorry confusion brand new ray still trying get footing ah got thanks confusion better job serve run serve run ray client connect ray cluster ray client address format ray head node port default head node port example address would likely ray public address instance see section note instance already ray cluster running respond serve run command guess best practice use remote cluster run ray deploy via remote shell follow work able run serve deploy without remote cluster ray attach serve deploy address option address remote cluster dashboard agent setting address let serve deploy submit file remote cluster work probably bug way starting ray serve rest also accessible dashboard port serve deploy work port tried,issue,positive,positive,positive,positive,positive,positive
1791724748,"> > The [documentation](https://docs.ray.io/en/latest/serve/advanced-guides/deploy-vm.html) currently says to use `serve deploy`, but I've heard that is deprecated in favor of `serve run`
> 
> Where did you hear `serve deploy` is deprecated? I'd recommend using `serve deploy` for this case.

Thanks! When troubleshooting I stumbled on a discussion online pointing to [here](https://github.com/ray-project/ray/blob/8d286f03ce39e8856d1ffaa20f377e4bda24dc26/python/ray/serve/deployment.py#L255-L267) and noting the deprecation message. But you're right, yes, `serve deploy` does seem to be the right way to do it. (Sorry for the confusion. I'm brand new to Ray and still trying to get my footing.)

I think this is more of a documentation issue, not a bug in Ray as I originally thought. I was confused about the two ""dashboards"" in Ray one at port 8265 and the other at port 52365, called the ""Dashboard Agent."" The exception I hit above is due to connecting to the wrong one.

To illustrate, the following steps attempt to port-forward to the Dashboard (8265) and will raise the exception above. (This was a user error on my part.)

```
ray dashboard cluster_config.yml
serve run -a http://localhost:8265 serve_config.yml
```

But I guess best practice is not to use port-forwarding but to `ssh` to the remote cluster and run `ray deploy` via the remote shell. The following workflow works for me on AWS EC2.

1. Get a shell on the remote cluster.

```
ray attach cluster_config.yml
```

2. From the remote shell, deploy.

```
serve deploy serve_config.yml
```

It might be good to mention this in the [docs](https://docs.ray.io/en/latest/serve/advanced-guides/deploy-vm.html), which currently don't specify how to access the remote cluster. But that's another issue. There doesn't seem to be a bug here, so I'm gonna close it.",documentation currently use serve deploy favor serve run hear serve deploy recommend serve deploy case thanks discussion pointing deprecation message right yes serve deploy seem right way sorry confusion brand new ray still trying get footing think documentation issue bug ray originally thought confused two ray one port port dashboard agent exception hit due wrong one illustrate following attempt dashboard raise exception user error part ray dashboard serve run guess best practice use remote cluster run ray deploy via remote shell following work get shell remote cluster ray attach remote shell deploy serve deploy might good mention currently specify access remote cluster another issue seem bug gon na close,issue,positive,positive,neutral,neutral,positive,positive
1791677123,Hi @alanwguo - do you wanna do a final review on dashboard side? Thanks.,hi wan na final review dashboard side thanks,issue,negative,neutral,neutral,neutral,neutral,neutral
1791644060,Should be fixed already; attempt repro if unable we can close.,fixed already attempt unable close,issue,negative,negative,negative,negative,negative,negative
1791639592,@c21 pelase triage and assign priority and target ray release.,triage assign priority target ray release,issue,negative,neutral,neutral,neutral,neutral,neutral
1791638943,closing - if we detect a fresh re-run we can assign and look at them.,detect fresh assign look,issue,negative,positive,positive,positive,positive,positive
1791635072,Close - wait on re-run and we can cut one again.,close wait cut one,issue,negative,neutral,neutral,neutral,neutral,neutral
1791628281,let's re-run; if fails and/or is flaky again let's raise another issue.,let flaky let raise another issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1791596467,@bveeramani  current data-oncall can you please take a look at this?,current please take look,issue,negative,neutral,neutral,neutral,neutral,neutral
1791584498,@c21 please review with @pcmoritz on whether performance gap has been addressed,please review whether performance gap,issue,negative,neutral,neutral,neutral,neutral,neutral
1791577447,"@edoakes totally, no rush, let me know if I can help too",totally rush let know help,issue,negative,neutral,neutral,neutral,neutral,neutral
1791567476,"At least one of the failures is connected to this PR. I think it is due to the mocking?
```
::test_gpu_info_parsing 2023-11-02 20:18:07,551	ERROR nvidia_gpu.py:66 -- Could not parse gpu information.
Traceback (most recent call last):
  File ""C:\Miniconda3\lib\site-packages\pynvml.py"", line 1975, in _LoadNvmlLibrary
    nvmlLib = CDLL(""libnvidia-ml.so.1"")
  File ""C:\Miniconda3\lib\ctypes\__init__.py"", line 381, in __init__
    self._handle = _dlopen(self._name, mode)
FileNotFoundError: Could not find module 'libnvidia-ml.so.1' (or one of its dependencies). Try using the full path with constructor syntax.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\install\ray\python\ray\_private\accelerators\nvidia_gpu.py"", line 59, in get_current_node_accelerator_type
    gpu_list = gpustat.new_query()
  File ""C:\Miniconda3\lib\site-packages\gpustat\core.py"", line 745, in new_query
    return GPUStatCollection.new_query()
  File ""C:\Miniconda3\lib\site-packages\gpustat\core.py"", line 402, in new_query
    N.nvmlInit()
  File ""C:\Miniconda3\lib\site-packages\pynvml.py"", line 1947, in nvmlInit
    nvmlInitWithFlags(0)
  File ""C:\Miniconda3\lib\site-packages\pynvml.py"", line 1930, in nvmlInitWithFlags
    _LoadNvmlLibrary()
  File ""C:\Miniconda3\lib\site-packages\pynvml.py"", line 1977, in _LoadNvmlLibrary
    _nvmlCheckReturn(NVML_ERROR_LIBRARY_NOT_FOUND)
  File ""C:\Miniconda3\lib\site-packages\pynvml.py"", line 899, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.NVMLError_LibraryNotFound: NVML Shared Library Not Found
+++ Error creating PyTest summary
[Errno 2] No such file or directory: '::test_gpu_info_parsing.txt'
FAILED

================================== FAILURES ===================================
____________________________ test_gpu_info_parsing ____________________________

mock_listdir = <MagicMock name='listdir' id='2045615548880'>
mock_isdir = <MagicMock name='isdir' id='2045615601360'>
mock_find_spec = <MagicMock name='find_spec' id='2045615679040'>

    @patch(""importlib.util.find_spec"", return_value=False)
    @patch(""os.path.isdir"", return_value=True)
    @patch(""os.listdir"", return_value=[""1""])
    @patch(""sys.platform"", ""linux"")
    def test_gpu_info_parsing(mock_listdir, mock_isdir, mock_find_spec):
        info_string = """"""Model:           Tesla V100-SXM2-16GB
    IRQ:             107
    GPU UUID:        GPU-8eaaebb8-bb64-8489-fda2-62256e821983
    Video BIOS:      88.00.4f.00.09
    Bus Type:        PCIe
    DMA Size:        47 bits
    DMA Mask:        0x7fffffffffff
    Bus Location:    0000:00:1e.0
    Device Minor:    0
    Blacklisted:     No
        """"""
        with patch(""builtins.open"", mock_open(read_data=info_string)):
>           assert NvidiaGPUAcceleratorManager.get_current_node_accelerator_type() == ""V100""
E           AssertionError: assert None == 'V100'
E             +None
E             -'V100'

--



```",least one connected think due error could parse information recent call last file line file line mode could find module one try full path constructor syntax handling exception another exception recent call last file line file line return file line file line file line file line file line raise ret library found error summary file directory patch patch patch patch model video bios bus type size mask bus location device minor patch assert assert none,issue,negative,negative,neutral,neutral,negative,negative
1791539643,"Thanks for the pointer, it looks pretty painful to figure out how to update these, will try to get it working tomorrow",thanks pointer pretty painful figure update try get working tomorrow,issue,negative,negative,neutral,neutral,negative,negative
1791480690,Ran the release test on master [here](https://console.anyscale-staging.com/o/anyscale-internal/workspaces/expwrk_n74v4wdprdeewcwy6wfjh5hgwf/ses_jf1jqs9y1k4wcgnx2ax5ydjpp9?workspaces-tabs=terminal&command-history-section=command_history) and it passed. Closing for now to see if #40750 fixes ,ran release test master see,issue,negative,neutral,neutral,neutral,neutral,neutral
1791479279,"Running Ray on [Railway.app](https://railway.app/), which means **I'm inside a docker (or really `nixpacks`) container**, I hit a similar issue:

```
2023-11-02 20:12:39,831	INFO usage_lib.py:416 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2023-11-02 20:12:39,832	INFO scripts.py:744 -- Local node IP: 172.17.1.43
Traceback (most recent call last):
File ""/opt/venv/bin/ray"", line 8, in <module>
sys.exit(main())
File ""/opt/venv/lib/python3.10/site-packages/ray/scripts/scripts.py"", line 2498, in main
return cli()
File ""/opt/venv/lib/python3.10/site-packages/click/core.py"", line 1157, in __call__
return self.main(*args, **kwargs)
File ""/opt/venv/lib/python3.10/site-packages/click/core.py"", line 1078, in main
rv = self.invoke(ctx)
File ""/opt/venv/lib/python3.10/site-packages/click/core.py"", line 1688, in invoke
return _process_result(sub_ctx.command.invoke(sub_ctx))
File ""/opt/venv/lib/python3.10/site-packages/click/core.py"", line 1434, in invoke
return ctx.invoke(self.callback, **ctx.params)
File ""/opt/venv/lib/python3.10/site-packages/click/core.py"", line 783, in invoke
return __callback(*args, **kwargs)
File ""/opt/venv/lib/python3.10/site-packages/ray/autoscaler/_private/cli_logger.py"", line 856, in wrapper
return f(*args, **kwargs)
File ""/opt/venv/lib/python3.10/site-packages/ray/scripts/scripts.py"", line 771, in start
node = ray._private.node.Node(
File ""/opt/venv/lib/python3.10/site-packages/ray/_private/node.py"", line 307, in __init__
self.start_head_processes()
File ""/opt/venv/lib/python3.10/site-packages/ray/_private/node.py"", line 1404, in start_head_processes
assert self.get_gcs_client() is not None
File ""/opt/venv/lib/python3.10/site-packages/ray/_private/node.py"", line 676, in get_gcs_client
self._init_gcs_client()
File ""/opt/venv/lib/python3.10/site-packages/ray/_private/node.py"", line 721, in _init_gcs_client
raise RuntimeError(
RuntimeError: Failed to start GCS.  Last 0 lines of error files:
```
I can't easily ssh in to cat the error file, sorry about that.

I'm starting ray with a simple `ray.init()` in the main file.

Any advice here? Thanks.",running ray inside docker really container hit similar issue usage collection default without user confirmation terminal disable add command cluster run following command ray starting cluster see local node recent call last file line module main file line main return file line return file line main file line invoke return file line invoke return file line invoke return file line wrapper return file line start node file line file line assert none file line file line raise start last error ca easily cat error file sorry starting ray simple main file advice thanks,issue,negative,positive,neutral,neutral,positive,positive
1791475243,"Yes, look at there `managed library dependencies` in this wiki https://www.notion.so/anyscale-hq/OSS-Release-Tests-9e0b126f7dd542aa80e4a6f936b8267f - you would need to do this in a linux machine as well",yes look library would need machine well,issue,positive,neutral,neutral,neutral,neutral,neutral
1791473456,Is there a similar job that will compile the requirements like the CI requirements? Or how should I update them?,similar job compile like update,issue,negative,neutral,neutral,neutral,neutral,neutral
1791471745,"@edoakes yes, they have their own release test requirement files; check out the release/ray_release/byod/requirements_byod... files ",yes release test requirement check,issue,negative,neutral,neutral,neutral,neutral,neutral
1791468636,@can-anyscale are the release tests running in a different environment than the CI jobs?,release running different environment,issue,negative,neutral,neutral,neutral,neutral,neutral
1791465425,"> @Zandew can you take a look, it's already failing for a week, thankks

Currently looking 👍 ",take look already failing week currently looking,issue,negative,neutral,neutral,neutral,neutral,neutral
1791463835,"> What seems to be happening is that the scheduler loses track of paused trials and never chooses to unpause them. Then, because we have a concurrency limiter, we never choose to suggest any new trials either, so the run just hangs.

Thank you for the explanation!  
It's only the HyperBandForBOHB scheduler that is acting out then. Because the same training function with ASHA works fine, including resuming.  ",happening track never concurrency limiter never choose suggest new either run thank explanation acting training function work fine,issue,positive,positive,positive,positive,positive,positive
1791462957,"@architkulkarni not sure if this has previously been discussed but I'm not sure if we want to do this; this is essentially making the AWS cluster launcher a supported part of the default installation, and opting folks into requiring this dependency even if they're running on other platforms (or not using the cluster launcher). I believe the current ""policy"" is that the cluster launchers are community-maintained, optional extensions to Ray.

I think the better solution would be to have a separate install target for each cluster launcher variant, like `ray[""aws""]`, `ray[""gcp""]`, etc. (though didn't we have that at some point?).",sure previously sure want essentially making cluster launcher part default installation dependency even running cluster launcher believe current policy cluster optional ray think better solution would separate install target cluster launcher variant like ray ray though point,issue,positive,positive,positive,positive,positive,positive
1791462472,"@sven1977 can you take a look, it's already failing for a week, thankks",take look already failing week,issue,negative,neutral,neutral,neutral,neutral,neutral
1791456401,"> Though, is the restarting of the unfinished trial the task of the scheduler or the Algorithm? Because BOHB as a whole has no means to resume a trial if there was a system failure. It can only do warmstarting, (uses prior results to select configs).


Restarting the (paused) trial is actually the job of our internal control loop. The scheduler should be the one to select paused trials to continue (`choose_trial_to_run`), but eventually the control loop will actually execute this decision.

What seems to be happening is that the scheduler loses track of paused trials and never chooses to unpause them. Then, because we have a concurrency limiter, we never choose to suggest any new trials either, so the run just hangs.",though unfinished trial task algorithm whole resume trial system failure prior select trial actually job internal control loop one select continue eventually control loop actually execute decision happening track never concurrency limiter never choose suggest new either run,issue,negative,positive,neutral,neutral,positive,positive
1791453593,"Got it, let me close this issue and comment on the new one.",got let close issue comment new one,issue,negative,positive,positive,positive,positive,positive
1791434343,"Closing this issue because this example was removed on master. 

See #40127 ",issue example removed master see,issue,negative,neutral,neutral,neutral,neutral,neutral
1791427838,"Just wanted to follow up on this issue. Can the Ray Team please provide some information about the versions of `gymnasium`, `pettingzoo`, `pytorch`, and `tensorflow` supported by Ray RLLib? ",follow issue ray team please provide information gymnasium ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1791406105,"I will try, but it is frustrating to have to do this work twice. I hope this time it gets a review.",try work twice hope time review,issue,negative,neutral,neutral,neutral,neutral,neutral
1791401648,"Hi @justinvyu ah, The variable `config` should be passed, not `searchable_lightning_config`. `searchable_lightning_config` was what the older method of Lightning Trained needed. Sorry for the mixup.
I have also tried migrating to the new 2.7+ code and the same issue persisted. I documented that [here](https://github.com/ray-project/ray/issues/39900). I guess this becomes a duplicate issue since I have also put a more complete code there. 

Though, is the restarting of the unfinished trial the task of the scheduler or the Algorithm? Because BOHB as a whole has no means to resume a trial if there was a system failure. It can only do warmstarting, (uses prior results to select configs).",hi ah variable older method lightning trained sorry also tried new code issue guess becomes duplicate issue since also put complete code though unfinished trial task algorithm whole resume trial system failure prior select,issue,negative,negative,neutral,neutral,negative,negative
1791392318,@mak-454 Interesting! Let us repro it on our side and get back to you.,interesting let u side get back,issue,negative,positive,positive,positive,positive,positive
1791382579,"@f2010126 I believe this is not working with `reuse_actors=True` due to the lightning trainer not initializing the model with the correct params. How is the `config` with the layer params connected to the `searchable_lightning_config`?

Also, does this still show up for you when migrating to 2.7+ (using the `TorchTrainer`)?",believe working due lightning trainer model correct layer connected also still show,issue,negative,negative,negative,negative,negative,negative
1791372715,@woshiyyya  It seems to be going fine and GPU utilization seems to be stable after commenting the line https://github.com/ray-project/ray/blob/8504563be5302f424f6efa7ba38c03ddccb3622b/doc/source/templates/04_finetuning_llms_with_deepspeed/finetune_hf_llm.py#L402,going fine utilization stable line,issue,negative,positive,positive,positive,positive,positive
1791363705,"It appears we made the decision to do this explicitly: https://github.com/ray-project/ray/pull/38739

A little odd, but seems we'll break something if it's changed.",made decision explicitly little odd break something,issue,negative,negative,negative,negative,negative,negative
1791328940,"Hi @kira-lin, sorry for the late reply, we are currently internally discussing a more native and non-invasive way to support it. Possibly add new scheduling options in Ray Core. I'll keep you updated.",hi sorry late reply currently internally native way support possibly add new ray core keep,issue,negative,negative,negative,negative,negative,negative
1791283949,"hey @Harsh-Maheshwari thanks for providing the screen recording. But unfortunately, the processes in your htop command are sorted by CPU, not memory. So I wasn't able to find which process uses the most memory. Could you run it again and find the top memory costing processes? thanks",hey thanks providing screen recording unfortunately command sorted memory able find process memory could run find top memory costing thanks,issue,positive,positive,positive,positive,positive,positive
1791241426,@aschuh-hf could you clarify what you mean by this? Can you give an example of the type of handling you want to do?,could clarify mean give example type handling want,issue,negative,negative,negative,negative,negative,negative
1791140078,"Seems like the regression is expected:

- current target block size=128MiB and the test's configured batch size means that each block only produces 1-2 batches, and these batches aren't aligned with the block boundaries. So there is likely some memory copying and remote object fetching overhead.
- reverting to 512MiB block size fixes the perf regression
- the batch prefetching version of the benchmark has no regression",like regression current target block test batch size block block likely memory remote object fetching overhead mib block size regression batch version regression,issue,negative,negative,neutral,neutral,negative,negative
1791127603,"cc @sofianhnaide for visibility.

We're currently working on productionizing and polishing a version of this internally. Will provide an update in the next month!",visibility currently working version internally provide update next month,issue,negative,neutral,neutral,neutral,neutral,neutral
1790965990,@glennmoy it has been reverted now. You no longer need to commit the auto generated proto source code. ,longer need commit auto proto source code,issue,negative,neutral,neutral,neutral,neutral,neutral
1790959649,@mattip sorry for the late review. this looks great to me. Could you rebase with master since I moved the auto detection code to nvidia_gpu.py ,sorry late review great could rebase master since auto detection code,issue,positive,neutral,neutral,neutral,neutral,neutral
1790621055,"I noticed that you have also encountered this issue. Have you resolved it?
(PPO pid=1954927) 2023-08-17 11:19:05,083 WARNING algorithm_config.py:656 -- Cannot create PPOConfig from given config_dict! Property stdout_file not supported.",also issue resolved warning create given property,issue,negative,neutral,neutral,neutral,neutral,neutral
1790614941,"> 嘿@sven1977， 问题`observation = OrderedDict(sorted(observation.items())) AttributeError: 'NoneType'`对象没有属性“items”已在本文中[解决](https://discuss.ray.io/t/register-a-custom-environment-and-runing-ppotrainer-on-that-environment-not-working/6143/7?u=fardinabbasi)。这主要是因为我`self._get_obs() if not terminated else None`在**步骤函数**中返回了。
> 
> 尽管如此，我仍然遇到警告：`WARNING algorithm_config.py:672 -- Cannot create PPOConfig from the provided config_dict! Property __stdout_file__ is not supported`。

Yes, so am I.",observation sorted else none warning create provided property yes,issue,negative,neutral,neutral,neutral,neutral,neutral
1790492672,"There were issues we create a thread per caller for async actor, and thats' been fixed. I think this issue is still not fixed. 

```
33 worker.io
4 grpc_global_tim
1 task_event_buff
4 client.poll0
1 server.poll0
1 grpcpp_sync_ser
1 grpc_health_che
2 resolver-execut
2 default-execut
4 ray :: IDLE
1 timer_manager
32 event_engine
```

event_engine is from grpc, and there's currently no way to control (unless we patch grpc). I think in the reality most of them is idle.

it is mysterious why we have so many worker.io threads  per proc (we are expected to have only 1 per proc). 

In terms of fix timeline, as https://discuss.ray.io/t/too-many-threads-in-ray-worker/10881/12?u=sangcho says, we may not prioritize the fix at least in a while unless there's concrete proof in performance impact. Regarding the system resources limit, we in general recommend to set high ulimit.",create thread per caller actor thats fixed think issue still fixed ray idle currently way control unless patch think reality idle mysterious many per per fix may fix least unless concrete proof performance impact regarding system limit general recommend set high,issue,positive,positive,neutral,neutral,positive,positive
1790343260,"@woshiyyya am using  https://github.com/ray-project/ray/blob/master/doc/source/templates/04_finetuning_llms_with_deepspeed/finetune_hf_llm.py
and running it with --lora option.
**Few more details**
Setup - single node AWS g5.12xlarge. It has 4 a10 nodes.
Batchsize - 4
ND - 4
Block size - 512
Lora config and deepspeed config from the dir - https://github.com/ray-project/ray/blob/master/doc/source/templates/04_finetuning_llms_with_deepspeed",running lora option setup single node block size lora,issue,negative,negative,neutral,neutral,negative,negative
1790045902,"> Do we want _all_ possible links nested (potentially very deeply) in the sidebar?

No, I don't think so. I think the individual API reference pages aren't useful in the sidebar. I think the user guides are helpful, though",want possible link potentially deeply think think individual reference useful think user helpful though,issue,positive,positive,neutral,neutral,positive,positive
1790035707,"> When I click ""User guides"", I don't see the specific user guides in the sidebar. Is this expected? I feel like it's easier to read the sidebar than the list of links in the main page.

@bveeramani Sidebar behavior is something that I think we haven't yet really figured out, and which isn't well sorted in the current version of the docs. On this branch we have the links defined in [sidebar.yml](https://github.com/peytondmurray/ray/blob/docs-deps-upgrade/doc/source/sidebar.yml) and that's it. Do we want _all_ possible links nested (potentially very deeply) in the sidebar? I'm happy to insert whatever links we want, and at the same time we should also think carefully about how deep nesting affects usability.

Thanks for the feedback!",click user see specific user feel like easier read list link main page behavior something think yet really figured well sorted current version branch link defined want possible link potentially deeply happy insert whatever link want time also think carefully deep usability thanks feedback,issue,positive,positive,positive,positive,positive,positive
1789955278,"```
 ds.map_batches(lambda df: df[df[""passenger_count""] > 0])
```
The example was outdated. You will need to add `batch_format=""pandas""` to `map_batches` as the mapping function is a pandas one.",lambda example outdated need add function one,issue,negative,negative,negative,negative,negative,negative
1789882292,"Looks like there are situations where autoscaler can crash and cause the dashboard to crash: https://ray-distributed.slack.com/archives/CP950VC76/p1698862348689599

It'll be nice if we can show these autoscaler errors in the Ray Dashboard. Re-opening to track this work",like crash cause dashboard crash nice show ray dashboard track work,issue,negative,positive,positive,positive,positive,positive
1789854433,"This is a flaky test that failed on an earlier release run, looks like race to erase element in SchedulePendingPlacementGroups and should not be new, we can fix in next release.",flaky test release run like race erase element new fix next release,issue,negative,positive,neutral,neutral,positive,positive
1789830875,"Already successfully passed the release test, so merging.",already successfully release test,issue,negative,positive,positive,positive,positive,positive
1789800484,"so for your case, you don't even specify the min&max workers range? I think we can try repro this issue with your setup. can you give us a specific docker command we can use to start? ",case even specify min range think try issue setup give u specific docker command use start,issue,negative,neutral,neutral,neutral,neutral,neutral
1789793479,No UI change. We had to revert it due to last minute regression from the PR. We are going to merge the PR with a fix in a few days,change revert due last minute regression going merge fix day,issue,negative,negative,neutral,neutral,negative,negative
1789782988,"> It's unclear the difference or relationship between a RayTrainReportCallback and the existing Hugging Face Transformers or PyTorch Lightning Callback

Lighting users are confused when to use the native `ModelCheckpoint` vs. the custom callback that we provide.

We should update docs with our recommendations:
* Happy path: use our callback / a simple subclass of the generic lightning callback + disable model checkpoint (`enable_checkpointing=False`)

For more advanced usage (ex: checkpoint every X seconds):
* You can implement this behavior yourself with a generic callback
* Or, if this functionality is already implemented by `ModelCheckpoint`, you can leverage it by subclassing `ModelCheckpoint` instead. However, this may be as hard as implementing yourself due to required knowledge of lightning internals.",unclear difference relationship hugging face lightning lighting confused use native custom provide update happy path use simple subclass generic lightning disable model advanced usage ex every implement behavior generic functionality already leverage instead however may hard due knowledge lightning internals,issue,positive,positive,neutral,neutral,positive,positive
1789782799,"Hi, normally python/requirements.txt requires an update (documentation is wrong?), but boto3 is already in the compiled list so in this particular case you don't need to update.",hi normally update documentation wrong already list particular case need update,issue,negative,negative,neutral,neutral,negative,negative
1789775218,"> @architkulkarni it's that python/requirements.txt file; you can just add a new comment to create a session of [default] and add boto3 under it
> 
> you might need to update the python/requirements_compiled.txt (auto-generated code) as well (https://www.notion.so/anyscale-hq/OSS-Python-dependency-management-f32633b0018c423f927727807ea9da08)

@can-anyscale I don't see the `pip-compile dependencies` job described in https://www.notion.so/anyscale-hq/OSS-Python-dependency-management-f32633b0018c423f927727807ea9da08.  Does that mean I don't need to update `python/requirements_compiled.txt`?",file add new comment create session default add might need update code well see job mean need update,issue,negative,negative,neutral,neutral,negative,negative
1789760251,"`stage_2_avg_iteration_time` I think there's +-5 seconds regression variance. 

And others metrics look like variance. ",think regression variance metric look like variance,issue,negative,neutral,neutral,neutral,neutral,neutral
1789755827,"Looks like we only mark the actor as detached _after_ the creation task finishes:
https://github.com/ray-project/ray/blob/f5c59745d00982835feb145d14d1f9e0d4b0db6c/src/ray/raylet/node_manager.cc#L2186

This means if the creation task is sent to the actor, then the owner dies before it finishes, the actor may be killed by [HandleUnexpectedWorkerFailure](https://github.com/ray-project/ray/blob/f5c59745d00982835feb145d14d1f9e0d4b0db6c/src/ray/raylet/node_manager.cc#L1059).",like mark actor detached creation task creation task sent actor owner actor may,issue,positive,neutral,neutral,neutral,neutral,neutral
1789746335,"One possible (unsubstantiated) theory: in this test, the controller and the actors it creates are killed repeatedly. It could be that the controller is killed immediately after creating a replica, in which case the raylet may not yet have marked the worker running the replica as being a detached actor.",one possible unsubstantiated theory test controller repeatedly could controller immediately replica case raylet may yet marked worker running replica detached actor,issue,negative,positive,neutral,neutral,positive,positive
1789745140,Here are the full cluster logs for the release test failure run: [logs.zip](https://github.com/ray-project/ray/files/13232118/logs.zip),full cluster release test failure run,issue,negative,positive,neutral,neutral,positive,positive
1789731580,Python 3.12 is out now too. Might want to test that also.,python might want test also,issue,negative,neutral,neutral,neutral,neutral,neutral
1789693321,Please update the PR description. Thanks!,please update description thanks,issue,positive,positive,positive,positive,positive,positive
1789648723,@angelinalg do you have an opinion on when we need to prioritize this? it is p1 so something we should pick up this or next sprint. if untrue we can downgrade this.,opinion need something pick next sprint untrue downgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
1789636040,cc @iycheng Would you mind giving a pointer about which CI tests I should focus on for this PR? I am not familiar with the tests for GCS. Thanks!,would mind giving pointer focus familiar thanks,issue,positive,positive,positive,positive,positive,positive
1789608781,"```
REGRESSION 87.81%: dashboard_p95_latency_ms (LATENCY) regresses from 46.348 to 87.047 (87.81%) in 2.8.0/benchmarks/many_nodes.json
REGRESSION 50.45%: dashboard_p50_latency_ms (LATENCY) regresses from 3.687 to 5.547 (50.45%) in 2.8.0/benchmarks/many_nodes.json
REGRESSION 29.40%: dashboard_p99_latency_ms (LATENCY) regresses from 145.645 to 188.461 (29.40%) in 2.8.0/benchmarks/many_nodes.json
REGRESSION 13.80%: stage_2_avg_iteration_time (LATENCY) regresses from 60.438395738601685 to 68.77781887054444 (13.80%) in 2.8.0/stress_tests/stress_test_many_tasks.json
REGRESSION 9.61%: single_client_tasks_async (THROUGHPUT) regresses from 9563.116886637355 to 8643.833466025399 (9.61%) in 2.8.0/microbenchmark.json
REGRESSION 8.99%: 1000000_queued_time (LATENCY) regresses from 177.93132558000002 to 193.92261782800003 (8.99%) in 2.8.0/scalability/single_node.json
REGRESSION 8.80%: client__tasks_and_get_batch (THROUGHPUT) regresses from 0.9574694946141461 to 0.8732387089551705 (8.80%) in 2.8.0/microbenchmark.json
REGRESSION 8.00%: stage_1_avg_iteration_time (LATENCY) regresses from 23.342886781692506 to 25.210959935188292 (8.00%) in 2.8.0/stress_tests/stress_test_many_tasks.json
REGRESSION 7.44%: single_client_wait_1k_refs (THROUGHPUT) regresses from 5.52141006441801 to 5.110630867571745 (7.44%) in 2.8.0/microbenchmark.json
REGRESSION 7.21%: dashboard_p50_latency_ms (LATENCY) regresses from 6.852 to 7.346 (7.21%) in 2.8.0/benchmarks/many_tasks.json
REGRESSION 7.17%: placement_group_create/removal (THROUGHPUT) regresses from 997.6322375478999 to 926.0840791839338 (7.17%) in 2.8.0/microbenchmark.json
REGRESSION 5.85%: stage_4_spread (LATENCY) regresses from 0.7296295246039273 to 0.7723065878019925 (5.85%) in 2.8.0/stress_tests/stress_test_many_tasks.json
REGRESSION 5.34%: 10000_get_time (LATENCY) regresses from 25.08259386100002 to 26.422835183000018 (5.34%) in 2.8.0/scalability/single_node.json
REGRESSION 4.57%: single_client_tasks_and_get_batch (THROUGHPUT) regresses from 9.129595770052905 to 8.7124898510668 (4.57%) in 2.8.0/microbenchmark.json
REGRESSION 3.93%: dashboard_p99_latency_ms (LATENCY) regresses from 14019.625 to 14570.201 (3.93%) in 2.8.0/benchmarks/many_tasks.json
REGRESSION 3.13%: stage_3_creation_time (LATENCY) regresses from 2.1919972896575928 to 2.260662794113159 (3.13%) in 2.8.0/stress_tests/stress_test_many_tasks.json
REGRESSION 3.07%: avg_iteration_time (LATENCY) regresses from 1.5400808811187745 to 1.5873150753974914 (3.07%) in 2.8.0/stress_tests/stress_test_dead_actors.json
REGRESSION 2.95%: client__put_gigabytes (THROUGHPUT) regresses from 0.12778429919203013 to 0.12401864230452364 (2.95%) in 2.8.0/microbenchmark.json
REGRESSION 2.62%: 1_1_actor_calls_sync (THROUGHPUT) regresses from 2273.2464988756947 to 2213.6033025230176 (2.62%) in 2.8.0/microbenchmark.json
REGRESSION 2.53%: single_client_put_calls_Plasma_Store (THROUGHPUT) regresses from 5845.356422909845 to 5697.447666436941 (2.53%) in 2.8.0/microbenchmark.json
REGRESSION 2.36%: stage_3_time (LATENCY) regresses from 2875.2445271015167 to 2943.001654624939 (2.36%) in 2.8.0/stress_tests/stress_test_many_tasks.json
REGRESSION 2.29%: multi_client_tasks_async (THROUGHPUT) regresses from 27850.61204431569 to 27211.51041454346 (2.29%) in 2.8.0/microbenchmark.json
REGRESSION 2.13%: 10000_args_time (LATENCY) regresses from 17.291354780999995 to 17.66019733799999 (2.13%) in 2.8.0/scalability/single_node.json
REGRESSION 1.63%: 107374182400_large_object_time (LATENCY) regresses from 30.62209055699998 to 31.122242091999965 (1.63%) in 2.8.0/scalability/single_node.json
REGRESSION 1.31%: single_client_tasks_sync (THROUGHPUT) regresses from 1177.0860205196755 to 1161.670131632561 (1.31%) in 2.8.0/microbenchmark.json
REGRESSION 1.30%: tasks_per_second (THROUGHPUT) regresses from 275.25470863736416 to 271.6776615598984 (1.30%) in 2.8.0/benchmarks/many_nodes.json
REGRESSION 0.97%: multi_client_put_calls_Plasma_Store (THROUGHPUT) regresses from 12344.39546658664 to 12224.373147431208 (0.97%) in 2.8.0/microbenchmark.json
REGRESSION 0.94%: 1_n_actor_calls_async (THROUGHPUT) regresses from 9672.982187721544 to 9581.728569086026 (0.94%) in 2.8.0/microbenchmark.json
REGRESSION 0.69%: n_n_async_actor_calls_async (THROUGHPUT) regresses from 24458.207679236828 to 24290.541801601616 (0.69%) in 2.8.0/microbenchmark.json
REGRESSION 0.64%: 1_n_async_actor_calls_async (THROUGHPUT) regresses from 8657.20480299259 to 8601.993472120319 (0.64%) in 2.8.0/microbenchmark.json
REGRESSION 0.30%: client__tasks_and_put_batch (THROUGHPUT) regresses from 11449.695712792654 to 11415.752622212967 (0.30%) in 2.8.0/microbenchmark.json
```",regression latency regression latency regression latency regression latency regression throughput regression latency regression throughput regression latency regression throughput regression latency regression throughput regression latency regression latency regression throughput regression latency regression latency regression latency regression throughput regression throughput regression throughput regression latency regression throughput regression latency regression latency regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput,issue,negative,neutral,neutral,neutral,neutral,neutral
1789597832,@jjyao @scv119 does the new abstraction layer implemented not address this or is there more we can do? cc @xieus ,new abstraction layer address,issue,negative,positive,positive,positive,positive,positive
1789573094,"@grizzlybearg we are blocked on a pydantic release containing the fixes for https://github.com/pydantic/pydantic/issues/6763, then we can support Pydantic 2.5.0+.

This should come with Ray 2.9.",blocked release support come ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1789571249,re-reviewing - this thread died off 7mo ago. @zhe-thoughts is this still high priority currently?,thread mo ago still high priority currently,issue,negative,positive,neutral,neutral,positive,positive
1789374171,@rkooo567 can you take a look at the PR in progress that @pcmoritz cut so we can close this out; it's failing on couple of tests.,take look progress cut close failing couple,issue,negative,neutral,neutral,neutral,neutral,neutral
1789369941,labels added - don't have rights to add the topic @richardliaw can you help here?,added add topic help,issue,negative,neutral,neutral,neutral,neutral,neutral
1789333186,@aslonnie am I understanding correctly that the edge case you're referring to is if `plasma_directory = ray._private.utils.get_user_temp_dir()` happens to return `/dev/shm`? I'm not sure if that can ever even happen.,understanding correctly edge case return sure ever even happen,issue,negative,positive,positive,positive,positive,positive
1788809090,"> Hmm after all info ^, I am really not sure it is the Ray issue. We have logics like
> 
> 1. Before starting a worker, it checks if port is already used (https://github.com/ray-project/ray/blob/8fa15650533f4834ba7ba54360e6d405054b676b/src/ray/raylet/worker_pool.cc#L651
>    )
> 2. If it fails to start a worker (due to that error), it should just create a new worker. It should not hang.
> 3. If there are no available ports, it prints this error;
> 
> ```
> core_worker.cc:205: Failed to register worker 16971350aa0952ee87b054d9189a81aec17941c0b87009805e055d7c to Raylet. Invalid: Invalid: No available ports. Please specify a wider port range using --min-worker-port and --max-worker-port.
> ```
> 
> I verified with local experimentation with small port ranges.
> 
> When do you say ""cluster is not usable"", what does it exactly mean? What's the end symptoms?

Hello!
Do you think it is possible that there might be a race condition at the time between checking the port and creating a worker?

I'm experiencing the same issue. I used 2 servers with 4xT4 GPUs for each server.
Running Ray 2.7.1 Docker ([host network mode](https://docs.docker.com/network/drivers/host/)) with following command in entry point:

```
ray start --include-dashboard=True --head --dashboard-host=0.0.0.0
ray start --address=<head_address> (from 7 container)
```

so that I have 1 head node + 7 worker nodes (excluding head) in total.. the 4 worker nodes are on different server.
The python code I use is like:

```py
@ray.remote(num_gpus=1)
class Worker:
    ...
```

If I spawn 7 workers, it runs well with 1 spare GPU

But if I try to run with 8 workers, it only success when it was first run after the ray cluster is started.
For the second time onwards, I *mostly* get the following error (the conflicting port varies), and then it just hung indefinitely I had to run `ray job stop`. It still blocks the GPU though, making it unusable by another process.

<details><summary>View error logs</summary>

```
(raylet, ip=172.31.29.114) E1101 11:05:43.162699633    1376 chttp2_server.cc:1057]      UNKNOWN:No address added out of total 1 resolved for '0.0.0.0:10014' {created_time:""2023-11-01T11:05:43.162617565+00:00"", children:[UNKNOWN:Failed to add any wildcard listeners {created_time:""2023-11-01T11:05:43.162609282+00:00"", children:[UNKNOWN:Unable to configure socket {fd:12, created_time:""2023-11-01T11:05:43.162586619+00:00"", children:[UNKNOWN:Address already in use {syscall:""bind"", os_error:""Address already in use"", errno:98, created_time:""2023-11-01T11:05:43.162578308+00:00""}]}, UNKNOWN:Unable to configure socket {created_time:""2023-11-01T11:05:43.162606359+00:00"", fd:12, children:[UNKNOWN:Address already in use {created_time:""2023-11-01T11:05:43.162603066+00:00"", errno:98, os_error:""Address already in use"", syscall:""bind""}]}]}]}
(raylet, ip=172.31.29.114) [2023-11-01 11:05:43,220 C 1376 1376] grpc_server.cc:119:  Check failed: server_ Failed to start the grpc server. The specified port is 10014. This means that Ray's core components will not be able to function correctly. If the server startup error message is `Address already in use`, it indicates the server fails to start because the port is already used by other processes (such as --node-manager-port, --object-manager-port, --gcs-server-port, and ports between --min-worker-port, --max-worker-port). Try running sudo lsof -i :10014 to check if there are other processes listening to the port.
(raylet, ip=172.31.29.114) *** StackTrace Information ***
(raylet, ip=172.31.29.114) /opt/conda/envs/vllm/lib/python3.9/site-packages/ray/_raylet.so(+0xf1b91a) [0x7fda30cd391a] ray::operator<<()
(raylet, ip=172.31.29.114) /opt/conda/envs/vllm/lib/python3.9/site-packages/ray/_raylet.so(+0xf1d402) [0x7fda30cd5402] ray::SpdLogMessage::Flush()
(raylet, ip=172.31.29.114) /opt/conda/envs/vllm/lib/python3.9/site-packages/ray/_raylet.so(_ZN3ray6RayLogD1Ev+0x37) [0x7fda30cd5717] ray::RayLog::~RayLog()
(raylet, ip=172.31.29.114) /opt/conda/envs/vllm/lib/python3.9/site-packages/ray/_raylet.so(_ZN3ray3rpc10GrpcServer3RunEv+0x1530) [0x7fda307ef8c0] ray::rpc::GrpcServer::Run()
(raylet, ip=172.31.29.114) /opt/conda/envs/vllm/lib/python3.9/site-packages/ray/_raylet.so(_ZN3ray4core10CoreWorkerC1ERKNS0_17CoreWorkerOptionsERKNS_8WorkerIDE+0xec9) [0x7fda30515629] ray::core::CoreWorker::CoreWorker()
(raylet, ip=172.31.29.114) /opt/conda/envs/vllm/lib/python3.9/site-packages/ray/_raylet.so(_ZN3ray4core21CoreWorkerProcessImplC2ERKNS0_17CoreWorkerOptionsE+0x587) [0x7fda3051be57] ray::core::CoreWorkerProcessImpl::CoreWorkerProcessImpl()
(raylet, ip=172.31.29.114) /opt/conda/envs/vllm/lib/python3.9/site-packages/ray/_raylet.so(_ZN3ray4core17CoreWorkerProcess10InitializeERKNS0_17CoreWorkerOptionsE+0xcf) [0x7fda3051cf3f] ray::core::CoreWorkerProcess::Initialize()
(raylet, ip=172.31.29.114) /opt/conda/envs/vllm/lib/python3.9/site-packages/ray/_raylet.so(+0x6236bf) [0x7fda303db6bf] __pyx_pw_3ray_7_raylet_10CoreWorker_1__cinit__()
(raylet, ip=172.31.29.114) /opt/conda/envs/vllm/lib/python3.9/site-packages/ray/_raylet.so(+0x624969) [0x7fda303dc969] __pyx_tp_new_3ray_7_raylet_CoreWorker()
(raylet, ip=172.31.29.114) ray::IDLE(_PyObject_MakeTpCall+0x1fb) [0x4f064b] _PyObject_MakeTpCall
(raylet, ip=172.31.29.114) ray::IDLE(_PyEval_EvalFrameDefault+0x5263) [0x4ecc93] _PyEval_EvalFrameDefault
(raylet, ip=172.31.29.114) ray::IDLE() [0x4e6b2a] _PyEval_EvalCode
(raylet, ip=172.31.29.114) ray::IDLE(_PyFunction_Vectorcall+0xd4) [0x4f7e54] _PyFunction_Vectorcall
(raylet, ip=172.31.29.114) ray::IDLE(_PyEval_EvalFrameDefault+0x1231) [0x4e8c61] _PyEval_EvalFrameDefault
(raylet, ip=172.31.29.114) ray::IDLE() [0x4e6b2a] _PyEval_EvalCode
(raylet, ip=172.31.29.114) ray::IDLE(_PyEval_EvalCodeWithName+0x47) [0x4e67b7] _PyEval_EvalCodeWithName
(raylet, ip=172.31.29.114) ray::IDLE(PyEval_EvalCodeEx+0x39) [0x4e6769] PyEval_EvalCodeEx
(raylet, ip=172.31.29.114) ray::IDLE(PyEval_EvalCode+0x1b) [0x59466b] PyEval_EvalCode
(raylet, ip=172.31.29.114) ray::IDLE() [0x5c1dc7] run_eval_code_obj
(raylet, ip=172.31.29.114) ray::IDLE() [0x5bddd0] run_mod
(raylet, ip=172.31.29.114) ray::IDLE() [0x45674e] pyrun_file.cold
(raylet, ip=172.31.29.114) ray::IDLE(PyRun_SimpleFileExFlags+0x1a2) [0x5b7ab2] PyRun_SimpleFileExFlags
(raylet, ip=172.31.29.114) ray::IDLE(Py_RunMain+0x37e) [0x5b502e] Py_RunMain
(raylet, ip=172.31.29.114) ray::IDLE(Py_BytesMain+0x39) [0x588719] Py_BytesMain
(raylet, ip=172.31.29.114) /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xea) [0x7fda318a7d0a] __libc_start_main
(raylet, ip=172.31.29.114) ray::IDLE() [0x5885ce]
(raylet, ip=172.31.29.114) 
(Worker pid=1304) Available GPUs: [0]
(Worker pid=1304) Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.
(Worker pid=1368, ip=172.31.29.114) Token is valid (permission: read).
(Worker pid=1368, ip=172.31.29.114) Your token has been saved to /root/.cache/huggingface/token
(Worker pid=1368, ip=172.31.29.114) Login successful
(SplitCoordinator pid=19617) StreamSplitDataIterator(epoch=-1, split=0) blocked waiting on other clients for more than 30s. All clients must read from the DataIterator splits at the same time. This warning will not be printed again for this epoch.
(SplitCoordinator pid=19617) StreamSplitDataIterator(epoch=-1, split=3) blocked waiting on other clients for more than 30s. All clients must read from the DataIterator splits at the same time. This warning will not be printed again for this epoch.
(SplitCoordinator pid=19617) StreamSplitDataIterator(epoch=-1, split=2) blocked waiting on other clients for more than 30s. All clients must read from the DataIterator splits at the same time. This warning will not be printed again for this epoch.
```

</details>",really sure ray issue like starting worker port already used start worker due error create new worker available error register worker raylet invalid invalid available please specify port range local experimentation small port say cluster usable exactly mean end hello think possible might race condition time port worker issue used server running ray docker host network mode following command entry point ray start head ray start container head node worker excluding head total worker different server python code use like class worker spawn well spare try run success first run ray cluster second time onwards mostly get following error conflicting port hung indefinitely run ray job stop still though making unusable another process summary view error raylet unknown address added total resolved unknown add unknown unable configure socket unknown address already use bind address already use unknown unable configure socket unknown address already use address already use bind raylet check start server port ray core able function correctly server error message address already use server start port already used try running check listening port raylet information raylet ray raylet ray raylet ray raylet ray raylet ray raylet ray raylet ray raylet raylet raylet ray raylet ray raylet ray raylet ray raylet ray raylet ray raylet ray raylet ray raylet ray raylet ray raylet ray raylet ray raylet ray raylet ray raylet ray raylet raylet ray raylet worker available worker token saved git credential helper pas want set git credential well worker token valid permission read worker token saved worker login successful blocked waiting must read time warning printed epoch blocked waiting must read time warning printed epoch blocked waiting must read time warning printed epoch,issue,negative,positive,neutral,neutral,positive,positive
1788584088,"@mattip, closing this issue means that Ray is able to run over infiniband and other network interfaces?",issue ray able run network,issue,negative,positive,positive,positive,positive,positive
1788509876,"> This is awesome! Is there a plan to integrate this within Ray directly?

Officially adding to the docs would also be great",awesome plan integrate within ray directly officially would also great,issue,positive,positive,positive,positive,positive,positive
1788480039,If you want we can connect on a call to discuss the issue ,want connect call discus issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1788478572,"These two screen recordings describe the errors , may be these help. 
These are recorded on a smaller machine but Still we can see that It almost uses 10Gb ram starting from 1.7 Gb when nothing was running to 12 GB Ram usage while reading the data set. 

1. Ray read Error
[screen-capture.webm](https://github.com/ray-project/ray/assets/38434626/5ff63c82-fc39-4c78-9885-7cd9966b5887)

2. Below I have used the num_cpu arg
[error with all cpus.webm](https://github.com/ray-project/ray/assets/38434626/a4003514-330b-4135-8f2a-9f75108b91fb)

3. The Below video has files reading linearly with pandas,  With pandas I am not able to read all the files but it is expected since it reads all the data not just meta data. 
**Note**  when I subset the data to the number of files Pandas is able to read successfully, I am still not able to read the files using ray  
https://drive.google.com/file/d/1kDjyh4tNPsQw7amwOwX0wK1leqnEb45-/view?usp=sharing


Below I have listed the file sizes that I am trying to read:
```
2023-10-11-00-00-00.parquet file size = 46M  
2023-10-19-00-00-00.parquet file size = 46M  
2023-10-20-00-00-00.parquet file size = 46M  
2023-10-17-00-00-00.parquet file size = 45M  
2023-10-18-00-00-00.parquet file size = 45M  
2023-10-16-00-00-00.parquet file size = 45M  
2023-10-15-00-00-00.parquet file size = 45M  
2023-10-14-00-00-00.parquet file size = 45M  
2023-10-13-00-00-00.parquet file size = 45M  
2023-10-12-00-00-00.parquet file size = 44M  
2023-10-10-00-00-00.parquet file size = 44M  
2023-10-01-00-00-00.parquet file size = 44M  
2023-10-03-00-00-00.parquet file size = 43M  
2023-10-08-00-00-00.parquet file size = 43M  
2023-09-29-00-00-00.parquet file size = 43M  
2023-09-30-00-00-00.parquet file size = 43M  
2023-10-04-00-00-00.parquet file size = 43M  
2023-10-09-00-00-00.parquet file size = 43M  
2023-09-28-00-00-00.parquet file size = 43M  
2023-10-02-00-00-00.parquet file size = 43M  
2023-10-05-00-00-00.parquet file size = 43M  
2023-09-27-00-00-00.parquet file size = 43M  
2023-09-25-00-00-00.parquet file size = 43M  
2023-09-26-00-00-00.parquet file size = 43M  
2023-10-07-00-00-00.parquet file size = 43M  
2023-09-23-00-00-00.parquet file size = 43M  
2023-09-19-00-00-00.parquet file size = 43M  
2023-09-20-00-00-00.parquet file size = 43M  
2023-09-22-00-00-00.parquet file size = 43M  
2023-09-21-00-00-00.parquet file size = 43M  
2023-09-24-00-00-00.parquet file size = 43M  
2023-10-06-00-00-00.parquet file size = 43M  
2023-09-17-00-00-00.parquet file size = 43M  
2023-09-18-00-00-00.parquet file size = 43M  
2023-09-16-00-00-00.parquet file size = 43M  
2023-09-15-00-00-00.parquet file size = 42M  
2023-09-14-00-00-00.parquet file size = 42M  
2023-09-13-00-00-00.parquet file size = 42M  
2023-09-12-00-00-00.parquet file size = 40M  
2023-09-11-00-00-00.parquet file size = 40M  
2023-09-08-00-00-00.parquet file size = 40M  
2023-09-09-00-00-00.parquet file size = 40M  
2023-09-10-00-00-00.parquet file size = 40M  
2023-09-07-00-00-00.parquet file size = 40M  
2023-09-06-00-00-00.parquet file size = 39M  
2023-09-05-00-00-00.parquet file size = 39M  
2023-09-04-00-00-00.parquet file size = 39M  
2023-09-03-00-00-00.parquet file size = 39M  
2023-09-02-00-00-00.parquet file size = 39M  
2023-08-23-00-00-00.parquet file size = 39M  
2023-08-25-00-00-00.parquet file size = 39M  
2023-08-26-00-00-00.parquet file size = 39M  
2023-08-24-00-00-00.parquet file size = 39M  
2023-09-01-00-00-00.parquet file size = 39M  
2023-08-30-00-00-00.parquet file size = 39M  
2023-08-31-00-00-00.parquet file size = 39M  
2023-08-29-00-00-00.parquet file size = 39M  
2023-08-22-00-00-00.parquet file size = 39M  
2023-08-27-00-00-00.parquet file size = 39M  
2023-08-28-00-00-00.parquet file size = 39M  
2023-08-21-00-00-00.parquet file size = 39M  
2023-08-20-00-00-00.parquet file size = 38M  
2023-08-19-00-00-00.parquet file size = 38M  
2023-08-18-00-00-00.parquet file size = 38M  
2023-08-17-00-00-00.parquet file size = 38M  
2023-08-15-00-00-00.parquet file size = 38M  
2023-08-16-00-00-00.parquet file size = 38M  
2023-08-14-00-00-00.parquet file size = 38M  
2023-08-13-00-00-00.parquet file size = 38M  
2023-08-11-00-00-00.parquet file size = 38M  
2023-08-12-00-00-00.parquet file size = 38M  
2023-08-10-00-00-00.parquet file size = 38M  
2023-08-09-00-00-00.parquet file size = 38M  
2023-08-08-00-00-00.parquet file size = 38M  
2023-08-07-00-00-00.parquet file size = 37M  
2023-08-06-00-00-00.parquet file size = 37M  
2023-08-05-00-00-00.parquet file size = 37M  
2023-08-04-00-00-00.parquet file size = 37M  
2023-08-03-00-00-00.parquet file size = 37M  
2023-08-02-00-00-00.parquet file size = 37M  
2023-08-01-00-00-00.parquet file size = 37M  
2023-07-31-00-00-00.parquet file size = 37M  
2023-07-30-00-00-00.parquet file size = 37M  
2023-07-29-00-00-00.parquet file size = 37M  
2023-07-28-00-00-00.parquet file size = 37M  
2023-07-20-00-00-00.parquet file size = 37M  
2023-07-21-00-00-00.parquet file size = 36M  
2023-07-19-00-00-00.parquet file size = 36M  
2023-07-15-00-00-00.parquet file size = 36M  
2023-07-22-00-00-00.parquet file size = 36M  
2023-07-18-00-00-00.parquet file size = 36M  
2023-07-25-00-00-00.parquet file size = 36M  
2023-07-14-00-00-00.parquet file size = 36M  
2023-07-13-00-00-00.parquet file size = 36M  
2023-07-27-00-00-00.parquet file size = 36M  
2023-07-12-00-00-00.parquet file size = 36M  
2023-07-24-00-00-00.parquet file size = 36M  
2023-07-26-00-00-00.parquet file size = 36M  
2023-07-23-00-00-00.parquet file size = 36M  
2023-07-16-00-00-00.parquet file size = 36M  
2023-07-17-00-00-00.parquet file size = 36M  
2023-07-11-00-00-00.parquet file size = 36M  
2023-07-04-00-00-00.parquet file size = 36M  
2023-07-05-00-00-00.parquet file size = 36M  
2023-07-02-00-00-00.parquet file size = 36M  
2023-07-06-00-00-00.parquet file size = 36M  
2023-07-07-00-00-00.parquet file size = 36M  
2023-07-01-00-00-00.parquet file size = 36M  
2023-07-03-00-00-00.parquet file size = 36M  
2023-07-10-00-00-00.parquet file size = 36M  
2023-07-08-00-00-00.parquet file size = 36M  
2023-07-09-00-00-00.parquet file size = 36M  
2023-06-30-00-00-00.parquet file size = 36M  
2023-06-27-00-00-00.parquet file size = 36M  
2023-06-28-00-00-00.parquet file size = 36M  
2023-06-29-00-00-00.parquet file size = 36M  
2023-06-26-00-00-00.parquet file size = 36M  
2023-06-20-00-00-00.parquet file size = 36M  
2023-06-19-00-00-00.parquet file size = 36M  
2023-06-16-00-00-00.parquet file size = 36M  
2023-06-15-00-00-00.parquet file size = 36M  
2023-06-14-00-00-00.parquet file size = 36M  
2023-06-24-00-00-00.parquet file size = 36M  
2023-06-23-00-00-00.parquet file size = 36M  
2023-06-13-00-00-00.parquet file size = 36M  
2023-06-25-00-00-00.parquet file size = 36M  
2023-06-21-00-00-00.parquet file size = 36M  
2023-06-22-00-00-00.parquet file size = 36M  
2023-06-18-00-00-00.parquet file size = 36M  
2023-06-17-00-00-00.parquet file size = 36M  
2023-06-10-00-00-00.parquet file size = 36M  
2023-06-12-00-00-00.parquet file size = 36M  
2023-06-11-00-00-00.parquet file size = 36M  
2023-06-09-00-00-00.parquet file size = 36M  
2023-06-08-00-00-00.parquet file size = 36M  
2023-06-07-00-00-00.parquet file size = 36M  
2023-06-06-00-00-00.parquet file size = 36M  
2023-06-05-00-00-00.parquet file size = 36M  
2023-06-04-00-00-00.parquet file size = 35M  
2023-06-01-00-00-00.parquet file size = 35M  
2023-05-31-00-00-00.parquet file size = 35M  
2023-06-03-00-00-00.parquet file size = 35M  
2023-06-02-00-00-00.parquet file size = 35M  
2023-05-30-00-00-00.parquet file size = 35M  
2023-05-29-00-00-00.parquet file size = 35M  
2023-05-28-00-00-00.parquet file size = 35M  
2023-05-27-00-00-00.parquet file size = 35M  
2023-05-26-00-00-00.parquet file size = 35M  
2023-05-25-00-00-00.parquet file size = 35M  
2023-05-24-00-00-00.parquet file size = 35M  
2023-05-23-00-00-00.parquet file size = 35M  
2023-05-22-00-00-00.parquet file size = 35M  
2023-05-21-00-00-00.parquet file size = 35M  
2023-05-20-00-00-00.parquet file size = 35M  
2023-05-19-00-00-00.parquet file size = 35M  
2023-05-18-00-00-00.parquet file size = 35M  
2023-05-16-00-00-00.parquet file size = 34M  
2023-05-17-00-00-00.parquet file size = 34M  
2023-05-15-00-00-00.parquet file size = 34M  
2023-04-18-00-00-00.parquet file size = 34M  
2023-04-19-00-00-00.parquet file size = 34M  
2023-05-14-00-00-00.parquet file size = 34M  
2023-05-13-00-00-00.parquet file size = 34M  
2023-04-20-00-00-00.parquet file size = 34M  
2023-04-07-00-00-00.parquet file size = 34M  
2023-04-08-00-00-00.parquet file size = 34M  
2023-04-21-00-00-00.parquet file size = 34M  
2023-04-14-00-00-00.parquet file size = 34M  
2023-04-26-00-00-00.parquet file size = 34M  
2023-04-11-00-00-00.parquet file size = 34M  
2023-04-15-00-00-00.parquet file size = 34M  
2023-04-25-00-00-00.parquet file size = 34M  
2023-05-11-00-00-00.parquet file size = 34M  
2023-05-04-00-00-00.parquet file size = 34M  
2023-05-12-00-00-00.parquet file size = 34M  
2023-04-28-00-00-00.parquet file size = 34M  
2023-04-10-00-00-00.parquet file size = 34M  
2023-05-03-00-00-00.parquet file size = 34M  
2023-04-24-00-00-00.parquet file size = 34M  
2023-04-17-00-00-00.parquet file size = 34M  
2023-04-16-00-00-00.parquet file size = 34M  
2023-04-13-00-00-00.parquet file size = 34M  
2023-04-06-00-00-00.parquet file size = 34M  
2023-04-27-00-00-00.parquet file size = 34M  
2023-05-02-00-00-00.parquet file size = 34M  
2023-05-01-00-00-00.parquet file size = 34M  
2023-05-10-00-00-00.parquet file size = 34M  
2023-04-12-00-00-00.parquet file size = 34M  
2023-05-05-00-00-00.parquet file size = 34M  
2023-04-23-00-00-00.parquet file size = 34M  
2023-05-08-00-00-00.parquet file size = 34M  
2023-04-29-00-00-00.parquet file size = 34M  
2023-04-09-00-00-00.parquet file size = 34M  
2023-05-09-00-00-00.parquet file size = 34M  
2023-04-22-00-00-00.parquet file size = 34M  
2023-04-30-00-00-00.parquet file size = 34M  
2023-04-05-00-00-00.parquet file size = 34M  
2023-04-04-00-00-00.parquet file size = 34M  
2023-05-06-00-00-00.parquet file size = 34M  
2023-05-07-00-00-00.parquet file size = 34M  
2023-04-03-00-00-00.parquet file size = 34M  
2023-04-02-00-00-00.parquet file size = 34M  
2023-04-01-00-00-00.parquet file size = 34M  
2023-03-31-00-00-00.parquet file size = 34M  
2023-03-30-00-00-00.parquet file size = 34M  
2023-03-29-00-00-00.parquet file size = 34M  
2023-03-27-00-00-00.parquet file size = 34M  
2023-03-28-00-00-00.parquet file size = 34M  
2023-03-26-00-00-00.parquet file size = 33M  
2023-03-25-00-00-00.parquet file size = 33M  
2023-03-24-00-00-00.parquet file size = 33M  
2023-03-23-00-00-00.parquet file size = 33M  
2023-03-22-00-00-00.parquet file size = 33M  
2023-03-21-00-00-00.parquet file size = 33M  
2023-03-20-00-00-00.parquet file size = 33M  
2023-03-19-00-00-00.parquet file size = 33M  
2023-03-18-00-00-00.parquet file size = 33M  
2023-03-17-00-00-00.parquet file size = 33M  
2023-03-15-00-00-00.parquet file size = 33M  
2023-03-14-00-00-00.parquet file size = 33M  
2023-03-16-00-00-00.parquet file size = 33M  
2023-03-13-00-00-00.parquet file size = 33M  
2023-03-11-00-00-00.parquet file size = 33M  
2023-03-12-00-00-00.parquet file size = 33M  
2023-03-10-00-00-00.parquet file size = 32M  
2023-03-08-00-00-00.parquet file size = 32M  
2023-03-09-00-00-00.parquet file size = 32M  
2023-03-07-00-00-00.parquet file size = 32M  
2023-03-06-00-00-00.parquet file size = 32M  
2023-03-05-00-00-00.parquet file size = 32M  
2023-03-03-00-00-00.parquet file size = 32M  
2023-03-04-00-00-00.parquet file size = 32M  
2023-03-02-00-00-00.parquet file size = 32M  
2023-01-16-00-00-00.parquet file size = 32M  
2023-01-18-00-00-00.parquet file size = 32M  
2023-01-17-00-00-00.parquet file size = 32M  
2023-01-20-00-00-00.parquet file size = 32M  
2023-01-14-00-00-00.parquet file size = 32M  
2023-03-01-00-00-00.parquet file size = 32M  
2023-01-27-00-00-00.parquet file size = 32M  
2023-01-26-00-00-00.parquet file size = 32M  
2023-01-25-00-00-00.parquet file size = 32M  
2023-01-19-00-00-00.parquet file size = 32M  
2023-01-28-00-00-00.parquet file size = 32M  
2023-01-15-00-00-00.parquet file size = 32M  
2023-01-13-00-00-00.parquet file size = 32M  
2023-02-27-00-00-00.parquet file size = 32M  
2023-01-23-00-00-00.parquet file size = 32M  
2023-02-28-00-00-00.parquet file size = 32M  
2023-01-30-00-00-00.parquet file size = 32M  
2023-01-21-00-00-00.parquet file size = 32M  
2023-01-22-00-00-00.parquet file size = 32M  
2023-01-24-00-00-00.parquet file size = 32M  
2023-01-29-00-00-00.parquet file size = 32M  
2023-02-25-00-00-00.parquet file size = 32M  
2023-01-12-00-00-00.parquet file size = 32M  
2023-02-26-00-00-00.parquet file size = 32M  
2023-02-24-00-00-00.parquet file size = 32M  
2023-02-01-00-00-00.parquet file size = 32M  
2023-02-03-00-00-00.parquet file size = 32M  
2023-02-21-00-00-00.parquet file size = 32M  
2023-01-10-00-00-00.parquet file size = 32M  
2023-02-22-00-00-00.parquet file size = 32M  
2023-02-23-00-00-00.parquet file size = 32M  
2023-02-02-00-00-00.parquet file size = 32M  
2023-01-11-00-00-00.parquet file size = 32M  
2023-01-31-00-00-00.parquet file size = 32M  
2023-01-08-00-00-00.parquet file size = 32M  
2023-01-09-00-00-00.parquet file size = 32M  
2023-01-07-00-00-00.parquet file size = 32M  
2023-01-03-00-00-00.parquet file size = 32M  
2023-02-04-00-00-00.parquet file size = 32M  
2023-01-02-00-00-00.parquet file size = 32M  
2023-02-20-00-00-00.parquet file size = 32M  
2023-02-18-00-00-00.parquet file size = 32M  
2023-02-19-00-00-00.parquet file size = 32M  
2023-01-06-00-00-00.parquet file size = 32M  
2023-01-05-00-00-00.parquet file size = 32M  
2023-01-01-00-00-00.parquet file size = 32M  
2023-01-04-00-00-00.parquet file size = 32M  
2023-02-05-00-00-00.parquet file size = 32M  
2022-12-30-00-00-00.parquet file size = 32M  
2023-02-17-00-00-00.parquet file size = 32M  
2022-12-28-00-00-00.parquet file size = 32M  
2023-02-06-00-00-00.parquet file size = 32M  
2022-12-27-00-00-00.parquet file size = 32M  
2022-12-31-00-00-00.parquet file size = 32M  
2022-12-29-00-00-00.parquet file size = 32M  
2023-02-16-00-00-00.parquet file size = 32M  
2023-02-15-00-00-00.parquet file size = 32M  
2022-12-26-00-00-00.parquet file size = 32M  
2023-02-09-00-00-00.parquet file size = 32M  
2023-02-07-00-00-00.parquet file size = 32M  
2023-02-14-00-00-00.parquet file size = 32M  
2023-02-08-00-00-00.parquet file size = 32M  
2023-02-11-00-00-00.parquet file size = 32M  
2023-02-10-00-00-00.parquet file size = 32M  
2023-02-12-00-00-00.parquet file size = 32M  
2022-12-25-00-00-00.parquet file size = 32M  
2023-02-13-00-00-00.parquet file size = 32M  
2022-12-24-00-00-00.parquet file size = 32M  
2022-12-23-00-00-00.parquet file size = 32M  
2022-12-22-00-00-00.parquet file size = 32M  
2022-12-21-00-00-00.parquet file size = 32M  
2022-12-20-00-00-00.parquet file size = 32M  
2022-12-19-00-00-00.parquet file size = 32M  
2022-12-18-00-00-00.parquet file size = 32M  
2022-12-17-00-00-00.parquet file size = 32M  
2022-12-16-00-00-00.parquet file size = 32M  
2022-12-15-00-00-00.parquet file size = 32M  
2022-12-13-00-00-00.parquet file size = 31M  
2022-12-12-00-00-00.parquet file size = 31M  
2022-12-09-00-00-00.parquet file size = 31M  
2022-12-14-00-00-00.parquet file size = 31M  
2022-12-11-00-00-00.parquet file size = 31M  
2022-12-10-00-00-00.parquet file size = 31M  
2022-12-07-00-00-00.parquet file size = 31M  
2022-12-08-00-00-00.parquet file size = 31M  
2022-12-06-00-00-00.parquet file size = 31M  
2022-12-03-00-00-00.parquet file size = 31M  
2022-12-04-00-00-00.parquet file size = 31M  
2022-11-30-00-00-00.parquet file size = 31M  
2022-12-02-00-00-00.parquet file size = 31M  
2022-12-05-00-00-00.parquet file size = 31M  
2022-12-01-00-00-00.parquet file size = 31M  
2022-11-29-00-00-00.parquet file size = 31M  
2022-11-27-00-00-00.parquet file size = 31M  
2022-11-28-00-00-00.parquet file size = 31M  
2022-11-26-00-00-00.parquet file size = 31M  
2022-11-24-00-00-00.parquet file size = 31M  
2022-11-23-00-00-00.parquet file size = 31M  
2022-11-25-00-00-00.parquet file size = 31M  
2022-11-22-00-00-00.parquet file size = 31M  
2022-11-21-00-00-00.parquet file size = 31M  
2022-11-20-00-00-00.parquet file size = 31M  
2022-11-19-00-00-00.parquet file size = 31M  
2022-11-18-00-00-00.parquet file size = 31M  
2022-11-17-00-00-00.parquet file size = 31M  
2022-11-16-00-00-00.parquet file size = 30M  
2022-11-15-00-00-00.parquet file size = 30M  
2022-11-14-00-00-00.parquet file size = 30M  
2022-11-13-00-00-00.parquet file size = 30M  
2022-11-12-00-00-00.parquet file size = 30M  
2022-11-11-00-00-00.parquet file size = 30M  
2022-11-10-00-00-00.parquet file size = 30M  
2022-11-09-00-00-00.parquet file size = 30M  
2022-11-08-00-00-00.parquet file size = 29M  
2022-11-07-00-00-00.parquet file size = 29M  
2022-11-06-00-00-00.parquet file size = 29M  
2022-11-05-00-00-00.parquet file size = 29M  
2022-11-04-00-00-00.parquet file size = 29M  
2022-11-03-00-00-00.parquet file size = 29M  
2022-11-02-00-00-00.parquet file size = 29M  
```

",two screen describe may help smaller machine still see almost ram starting nothing running ram usage reading data set ray read error used error video reading linearly able read since data meta data note subset data number able read successfully still able read ray listed file size trying read file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size file size,issue,negative,positive,positive,positive,positive,positive
1788400940,Long running many actor tasks passed: https://buildkite.com/ray-project/release-tests-pr/builds/57503#018b83df-a617-4354-87ca-ead5755cd29f,long running many actor,issue,negative,positive,positive,positive,positive,positive
1788377050,"could some one review this? the error message is strictly better than before.

otherwise, in some corner cases, it might show that:

````
The object store is using /dev/shm instead of /dev/shm ...
````
",could one review error message strictly better otherwise corner might show object store instead,issue,negative,positive,positive,positive,positive,positive
1788355947,"FYI @LeonLuttenberger 
I am trying to resolve the error I saw in #40774
",trying resolve error saw,issue,negative,neutral,neutral,neutral,neutral,neutral
1788236671,Relevant issues that you described are closed @stephanie-wang; can we close this issue?,relevant closed close issue,issue,negative,positive,positive,positive,positive,positive
1788235571,"@matthewdeng can you review merged PR by @peytondmurray  above fixes the root issue? If yes, please close this parent ticket.",review root issue yes please close parent ticket,issue,positive,neutral,neutral,neutral,neutral,neutral
1788209756,"- [x]  The search box in the Example Gallery page doesn't seem to work.
- [x]  Top nav: Change default page for Resources button to the Discussion Forum so that it's different from the Docs default page.
- [x] Side nav: Ray Core > User Guides should be expandable with child pages to match master.
- [x] Side nav doesn't reflect the location of the current page. Is this by design? For example, if I navigate to this page: https://anyscale-ray--39766.com.readthedocs.build/en/39766/train/getting-started-pytorch.html, it is not highlighted in the side nav
- [x] Remove `Show source` button in RH gutter
- [ ] Add ""Show source"" button to the RHS gutter
- [x] Tab component under https://anyscale-ray--39766.com.readthedocs.build/en/39766/ray-overview/installation.html#installed-python-dependencies doesn't match the tabs up higher in the page. More examples: 
https://anyscale-ray--39766.com.readthedocs.build/en/39766/train/huggingface-accelerate.html#configure-accelerate, https://anyscale-ray--39766.com.readthedocs.build/en/39766/train/deepspeed.html#code-example, https://anyscale-ray--39766.com.readthedocs.build/en/39766/ray-overview/use-cases.html, https://anyscale-ray--39766.com.readthedocs.build/en/39766/ray-overview/getting-started.html (Ray logo in tabs), 
- [x] Dark mode: Logos don't look good on this page: https://anyscale-ray--39766.com.readthedocs.build/en/39766/train/more-frameworks.html
- [x] Dark mode: This collapsable component with the title, `Show code cell content`, doesn't look right: https://anyscale-ray--39766.com.readthedocs.build/en/39766/ray-contribute/docs.html#creating-a-notebook-example
- [ ] Right hand side gutter: Consider adding a `Report issue` button, like the one we have in master.",search box example gallery page seem work top change default page button discussion forum different default page side ray core user child match master side reflect location current page design example navigate page side remove show source button gutter add show source button gutter tab component match higher page ray dark mode logo look good page dark mode component title show code cell content look right right hand side gutter consider report issue button like one master,issue,positive,positive,positive,positive,positive,positive
1788206650,"Only picked top commit, let me redo. Since original PR not merged to master",picked top commit let redo since original master,issue,positive,positive,positive,positive,positive,positive
1788198955,"- [x] `Hide Search Metrics` button at the top of the page.
<img width=""863"" alt=""Screenshot 2023-10-31 at 5 03 49 PM"" src=""https://github.com/ray-project/ray/assets/122562471/9bd044c3-83b4-4711-83c5-a73f95896ebe"">
",hide search metric button top page,issue,negative,positive,positive,positive,positive,positive
1788182978,"> There is a pound (#) sign that appears at the end of the last paragraph on the RLlib index page that I don't see in master

That's because you're hovering over an image that was inserted as a `figure`. It gives you a way to copy a link to that image; here's the raw `rst`:

```rst
.. figure:: images/rllib-stack.svg
    :align: left
    :width: 650

    **RLlib's API stack:** Built on top of Ray, RLlib offers off-the-shelf, highly distributed
    algorithms, policies, loss functions, and default models (including the option to
    auto-wrap a neural network with an LSTM or an attention net). Furthermore, our library
    comes with a built-in Server/Client setup, allowing you to connect
    hundreds of external simulators (clients) via the network to an RLlib server process,
    which provides learning functionality and serves action queries. User customizations
    are realized via sub-classing the existing abstractions and - by overriding certain
    methods in those sub-classes - define custom behavior.
```

If this is undesirable behavior, I think there's probably ways of using other directives or turning this feature off. Personally this feature falls in line with behavior seen elsewhere in the docs - for example if you hover over a heading, a hash will appear allowing you to link to that particular section. For that reason, I'd vote to keep it.",pound sign end last paragraph index page see master hovering image inserted figure way copy link image raw figure align left width stack built top ray highly distributed loss default option neural network attention net furthermore library come setup connect external via network server process learning functionality action user via certain define custom behavior undesirable behavior think probably way turning feature personally feature line behavior seen elsewhere example hover heading hash appear link particular section reason vote keep,issue,negative,positive,neutral,neutral,positive,positive
1788182015,"- [x] Replace Train top page logo with SVG
![Group 48](https://github.com/ray-project/ray/assets/122562471/c814869e-f0ba-454f-97ad-40215da91d6d)
",replace train top page group,issue,negative,positive,positive,positive,positive,positive
1788179240,"- [x] There is a pound (#) sign that appears at the end of the last paragraph on the RLlib index page that I don't see in master: 

<img width=""724"" alt=""Screenshot 2023-10-31 at 4 35 58 PM"" src=""https://github.com/ray-project/ray/assets/122562471/5065f3fe-e749-4d8a-8806-bf92afb051ee"">
",pound sign end last paragraph index page see master,issue,negative,neutral,neutral,neutral,neutral,neutral
1788162918,"@simran-2797 Thanks for taking a look. I'll implement the color changes you made in the mockups.

> Looks like there are 3 different button states instead of 2 for dark and light

Yes. The tooltip for the widget explains that the switch can be `dark`, `light` or `use whatever the current system theme is` modes. Is this undesirable? It's how all the docs based on `pydata-sphinx-theme` work.

Re: example gallery - this is something I still need to finish up. Thanks for the reminder!",thanks taking look implement color made like different button instead dark light yes switch dark light use whatever current system theme undesirable based work example gallery something still need finish thanks reminder,issue,positive,positive,positive,positive,positive,positive
1788150448,"Driver memory is reduced after #40765 but continues to be higher than in 2.7.1. Most likely, the issue is caused by #40248, which increases the number of sort partitions from 1k to 2k, causing 4x higher memory usage at the driver.",driver memory reduced higher likely issue number sort causing higher memory usage driver,issue,negative,positive,positive,positive,positive,positive
1788139905,"Hi @AndreKuu Can you describe your environment and the script you are running a bit more and maybe try out a newer version of Ray (e.g. 2.7.1)? 

I ran the following scripts
```
# long_running.py
import ray
import time

@ray.remote
def hello_world(time_diff):
    return f""hello world: {time_diff}""

ray.init()
start_time = time.time()
diff = time.time() - start_time
while diff < 600:
    print(ray.get(hello_world.remote(diff)))
    time.sleep(1)
    diff = time.time() - start_time

```
and submit job with 
```
# script.sh
#!/bin/bash

counter=1
while [ $counter -le 500 ]
do
echo $counter
((counter++))
ray job submit -- python long_running.py --no-wait &
done
echo All done

```

Ran `script.sh` on Anyscale Workspace with m5.8xlarge instance and Ray 2.7.1. It has no issue submitting and completing 500 of those jobs in parallel
![image (1)](https://github.com/ray-project/ray/assets/7553988/80a797cf-a7b5-492e-ade6-521199dba458)

Quickly submitting another 100 of the same jobs also completed successfully
![image (2)](https://github.com/ray-project/ray/assets/7553988/ff29719d-ced0-4783-bbfe-a49e7b3a851c)


",hi describe environment script running bit maybe try version ray ran following import ray import time return hello world print submit job counter echo counter ray job submit python done echo done ran instance ray issue parallel image quickly another also successfully image,issue,negative,positive,positive,positive,positive,positive
1788138394,"Few comments from the design side: 
1. I have a design for dark mode for the home page. We can use that - https://www.figma.com/file/fRYp3ZBGbpg9dlu3WlBYY9/Quansight---Example-gallery-and-CSAT?type=design&node-id=288%3A546&mode=design&t=HZiNlsIehbqxIXH5-1
2. Looks like there are 3 different button states instead of 2 for dark and light
![Screenshot 2023-10-31 at 3 45 17 PM](https://github.com/ray-project/ray/assets/23429473/57679b87-6291-49b6-9a0c-b728c1c1ff54)
![Screenshot 2023-10-31 at 3 45 47 PM](https://github.com/ray-project/ray/assets/23429473/39d6f828-88b8-4955-b1ae-ccfdfcbb095d)
![Screenshot 2023-10-31 at 3 46 03 PM](https://github.com/ray-project/ray/assets/23429473/ca2900bf-e6e8-4e85-bf86-4882b8bde897)
3. The example gallery is not on a separate page. The filters are missing. 
![Screenshot 2023-10-31 at 3 47 08 PM](https://github.com/ray-project/ray/assets/23429473/803a3e32-4343-4407-86d3-5639f8522fc7)
4. The search functionality on the example gallery does not work. 


",design side design dark mode home page use like different button instead dark light example gallery separate page missing search functionality example gallery work,issue,negative,negative,neutral,neutral,negative,negative
1788084703,"This is a Ray Core issue, duplicated as https://github.com/ray-project/ray/issues/25457. 
Note, it can be worked around with an application-level try-catch. ",ray core issue note worked around,issue,negative,neutral,neutral,neutral,neutral,neutral
1788080852,Fixed by refresh the base_test build cache,fixed refresh build cache,issue,negative,positive,neutral,neutral,positive,positive
1788060432,"@Harsh-Maheshwari Do you mean the OOM happens at `read_parquest`? As you mentioned, this function only fetches metadata, we haven't seen this function triggers OOM. 
How many files do you have in your `dir_path`? which process OOMs, the driver?",mean function seen function many process driver,issue,negative,positive,neutral,neutral,positive,positive
1788047811,@rkooo567 this was targeted for ray28; did this make fix make it in? If so can you dig up and link the PR and than we can close this issue out?,targeted ray make fix make dig link close issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1788043699,"@hhhvu I guess you will want to parallelize processing these small patches? If so, putting one single huge numpy array won't help. 
This could be a valid bug. But this usage is against our common practice.",guess want parallelize small one single huge array wo help could valid bug usage common practice,issue,positive,negative,neutral,neutral,negative,negative
1788032922,"Passing a MaterializedDataset to multiple workers is very common in Ray Data. We have a lot of test coverage on this case. I'm pretty confident this won't be an issue. 
It should be something else that caused the issue in your case, which I guess is likely to be OOM.
We need more information, such as logs, to further triage this issue. ",passing multiple common ray data lot test coverage case pretty confident wo issue something else issue case guess likely need information triage issue,issue,positive,positive,neutral,neutral,positive,positive
1787940660,"No, I doublechecked the behaviour by execing to the head and running commands over there. Additionally, dashboard access log shows execution time on the server side: '/api/jobs/ HTTP/1.1' 200 2811 bytes **119013670 us** '-' 'python-requests/2.31.0'

I have tried to move our workloads to setting, where driver/job supervisor is not on a node which is scaled away, and I haven't observed the job-table freeze since.

I guess that the trigger and cause is something like:
* driver on a node which gets scaled away quickly after job exits
* some combination of logging and triggering the log tailing function on the dashboard same time when scaledown happens
* and after these events dead nodes are tried to reach when job metadata is retrieved -> job list/delete take some multiples of internal timeout

I can try to add some debugging logs to the dashboard to see what it actually waits. ",behaviour head running additionally dashboard access log execution time server side u tried move setting supervisor node scaled away freeze since guess trigger cause something like driver node scaled away quickly job combination logging log tailing function dashboard time dead tried reach job job take internal try add dashboard see actually,issue,negative,positive,neutral,neutral,positive,positive
1787905041,"I was able to reproduce this issue. I'm using ray 2.7.0, python 3.8 on a mac with OS Darwin. ",able reproduce issue ray python mac o,issue,negative,positive,positive,positive,positive,positive
1787870084,"@jrosti Just want to add some notes. Thanks for sharing detailed script and steps. I tried to reproduce this by running local Ray cluster and it doesn't have issue for nightly, 2.7.1, nor 2.4.0. Then I tried this using Kuberay cluster and I do see some issue once control + c is hit after the job is submitted, kubectl port-forward command will show the following error 
```
E1031 19:13:35.896505 1644960 portforward.go:391] error copying from local connection to remote stream: read tcp4 127.0.0.1:8265->127.0.0.1:39572: read: connection reset by peer
E1031 19:13:35.918482 1644960 portforward.go:378] error copying from remote stream to local connection: readfrom tcp4 127.0.0.1:8265->127.0.0.1:39572: write tcp4 127.0.0.1:8265->127.0.0.1:39572: write: broken pipe
```
and the entire dashboard becomes unreachable.
 
Can you confirm this is exactly the behavior you saw? And by restarting `kubectl port-forward`, the issue will go away?
",want add thanks detailed script tried reproduce running local ray cluster issue nightly tried cluster see issue control hit job command show following error error local connection remote stream read read connection reset peer error remote stream local connection write write broken pipe entire dashboard becomes unreachable confirm exactly behavior saw issue go away,issue,negative,positive,neutral,neutral,positive,positive
1787715891,"verified that links work in the doc build: 
* ray-core/api/doc/ray.remote.html
* ray-core/api/doc/ray.actor.ActorClass.options.html#ray.actor.ActorClass.options
* ray-core/api/doc/ray.remote_function.RemoteFunction.options.html#ray-remote-function-remotefunction-options",link work doc build,issue,negative,neutral,neutral,neutral,neutral,neutral
1787669612,"> > That's a separate issue, right now the output from the head node (which is the output of running ray start --head) is streamed to the user laptop (the one running ray up). The output from the head node lists commands which only make sense on the head node. We can add a message in the output to clarify this in a future PR.
> 
> Yes. That makes sense. We need a better solution here. It's really confusing. Can we create an issue to track it? cc: @jjyao

I will create an issue and link it here.

> > Yup, fixed it
> 
> Sorry. What did you fix? It seems that this was the same issues as the one above.

I was referring to this commit: https://github.com/ray-project/ray/pull/40160/commits/18398de6351d13808547a30f7f3d527c020bf8bf So for this command, the screenshot in the description is slightly out of date (it predates this commit).  Basically, it is correct to write ""localhost"" because we instructed the user to do port-forwarding first.

",separate issue right output head node output running ray start head user one running ray output head node make sense head node add message output clarify future yes sense need better solution really create issue track create issue link fixed sorry fix one commit command description slightly date commit basically correct write instructed user first,issue,positive,positive,neutral,neutral,positive,positive
1787639458,"Had another variation here for the same ray train code:
```
base) root@smiral-0:~# python3 hf.py 
2023-10-31 10:09:34.713036: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-31 10:09:35.485444: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
2023-10-31 10:09:35.485523: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
2023-10-31 10:09:35.485529: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
comet_ml is installed but `COMET_API_KEY` is not set.
2023-10-31 10:09:38,678 INFO worker.py:1458 -- Connecting to existing Ray cluster at address: 10.0.0.30:6379...
2023-10-31 10:09:38,685 INFO worker.py:1633 -- Connected to Ray cluster. View the dashboard at 10.0.0.30:8265 
2023-10-31 10:09:38,734 INFO tune.py:228 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Trainer(...)`.
2023-10-31 10:09:38,737 INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949

View detailed results here: /home/ray/ray_results/TorchTrainer_2023-10-31_10-09-38
To visualize your results with TensorBoard, run: `tensorboard --logdir /home/ray/ray_results/TorchTrainer_2023-10-31_10-09-38`





(raylet, ip=10.0.0.19) [2023-10-31 10:10:39,130 E 295 295] (raylet) worker_pool.cc:548: Some workers of the worker process(419) have not registered within the timeout. The process is dead, probably it crashed during start.
(raylet, ip=10.0.0.19) [2023-10-31 10:10:39,132 E 295 295] (raylet) worker_pool.cc:548: Some workers of the worker process(420) have not registered within the timeout. The process is dead, probably it crashed during start.
(raylet, ip=10.0.0.19) [2023-10-31 10:10:39,130 E 295 295] (raylet) worker_pool.cc:548: Some workers of the worker process(419) have not registered within the timeout. The process is dead, probably it crashed during start.
(raylet, ip=10.0.0.19) [2023-10-31 10:10:39,132 E 295 295] (raylet) worker_pool.cc:548: Some workers of the worker process(420) have not registered within the timeout. The process is dead, probably it crashed during start.
(raylet, ip=10.0.0.78) [2023-10-31 10:11:22,088 E 293 293] (raylet) runtime_env_agent_client.cc:256: Runtime Env Agent timed out as NotFound in 30000ms. Status: NotFound: on_connect Connection timed out, address: 100.106.52.141, port: 63107, Suiciding...
(raylet, ip=10.0.0.78) [2023-10-31 10:11:22,088 E 293 293] (raylet) runtime_env_agent_client.cc:216: The raylet exited immediately because the runtime env agent timed out when Raylet try to connect to it. This can happen because the runtime env agent was never started, or is listening to the wrong port. Read the log `cat /tmp/ray/session_latest/logs/runtime_env_agent.log`. You can find the log file structure here https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure.
(raylet, ip=10.0.0.78) 
(raylet, ip=10.0.0.78) [2023-10-31 10:11:22,088 E 293 293] (raylet) runtime_env_agent_client.cc:256: Runtime Env Agent timed out as NotFound in 30000ms. Status: NotFound: on_connect Connection timed out, address: 100.106.52.141, port: 63107, Suiciding...
(raylet, ip=10.0.0.78) [2023-10-31 10:11:22,088 E 293 293] (raylet) runtime_env_agent_client.cc:216: The raylet exited immediately because the runtime env agent timed out when Raylet try to connect to it. This can happen because the runtime env agent was never started, or is listening to the wrong port. Read the log `cat /tmp/ray/session_latest/logs/runtime_env_agent.log`. You can find the log file structure here https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure.
(raylet, ip=10.0.0.78) 
(raylet, ip=10.0.0.78) [2023-10-31 10:11:22,088 E 293 293] (raylet) runtime_env_agent_client.cc:256: Runtime Env Agent timed out as NotFound in 30000ms. Status: NotFound: on_connect Connection timed out, address: 100.106.52.141, port: 63107, Suiciding...
(raylet, ip=10.0.0.78) [2023-10-31 10:11:22,088 E 293 293] (raylet) runtime_env_agent_client.cc:216: The raylet exited immediately because the runtime env agent timed out when Raylet try to connect to it. This can happen because the runtime env agent was never started, or is listening to the wrong port. Read the log `cat /tmp/ray/session_latest/logs/runtime_env_agent.log`. You can find the log file structure here https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure.
(raylet, ip=10.0.0.78) 
(raylet, ip=10.0.0.78) [2023-10-31 10:11:22,088 E 293 293] (raylet) runtime_env_agent_client.cc:256: Runtime Env Agent timed out as NotFound in 30000ms. Status: NotFound: on_connect Connection timed out, address: 100.106.52.141, port: 63107, Suiciding...
(raylet, ip=10.0.0.78) [2023-10-31 10:11:22,088 E 293 293] (raylet) runtime_env_agent_client.cc:216: The raylet exited immediately because the runtime env agent timed out when Raylet try to connect to it. This can happen because the runtime env agent was never started, or is listening to the wrong port. Read the log `cat /tmp/ray/session_latest/logs/runtime_env_agent.log`. You can find the log file structure here https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure.
(raylet, ip=10.0.0.78) 
(raylet, ip=10.0.0.19) [2023-10-31 10:11:22,229 E 295 295] (raylet) runtime_env_agent_client.cc:256: Runtime Env Agent timed out as NotFound in 30000ms. Status: NotFound: on_connect Connection timed out, address: 100.98.192.60, port: 46287, Suiciding...
(raylet, ip=10.0.0.19) [2023-10-31 10:11:22,229 E 295 295] (raylet) runtime_env_agent_client.cc:216: The raylet exited immediately because the runtime env agent timed out when Raylet try to connect to it. This can happen because the runtime env agent was never started, or is listening to the wrong port. Read the log `cat /tmp/ray/session_latest/logs/runtime_env_agent.log`. You can find the log file structure here https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure.
(raylet, ip=10.0.0.19) 
(raylet, ip=10.0.0.19) [2023-10-31 10:11:22,230 E 295 295] (raylet) runtime_env_agent_client.cc:256: Runtime Env Agent timed out as NotFound in 30000ms. Status: NotFound: on_connect Connection timed out, address: 100.98.192.60, port: 46287, Suiciding...
(raylet, ip=10.0.0.19) [2023-10-31 10:11:22,230 E 295 295] (raylet) runtime_env_agent_client.cc:216: The raylet exited immediately because the runtime env agent timed out when Raylet try to connect to it. This can happen because the runtime env agent was never started, or is listening to the wrong port. Read the log `cat /tmp/ray/session_latest/logs/runtime_env_agent.log`. You can find the log file structure here https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure.
(raylet, ip=10.0.0.19) 
(raylet, ip=10.0.0.19) [2023-10-31 10:11:22,229 E 295 295] (raylet) runtime_env_agent_client.cc:256: Runtime Env Agent timed out as NotFound in 30000ms. Status: NotFound: on_connect Connection timed out, address: 100.98.192.60, port: 46287, Suiciding...
(raylet, ip=10.0.0.19) [2023-10-31 10:11:22,229 E 295 295] (raylet) runtime_env_agent_client.cc:216: The raylet exited immediately because the runtime env agent timed out when Raylet try to connect to it. This can happen because the runtime env agent was never started, or is listening to the wrong port. Read the log `cat /tmp/ray/session_latest/logs/runtime_env_agent.log`. You can find the log file structure here https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure.
(raylet, ip=10.0.0.19) 
(raylet, ip=10.0.0.19) [2023-10-31 10:11:22,230 E 295 295] (raylet) runtime_env_agent_client.cc:256: Runtime Env Agent timed out as NotFound in 30000ms. Status: NotFound: on_connect Connection timed out, address: 100.98.192.60, port: 46287, Suiciding...
(raylet, ip=10.0.0.19) [2023-10-31 10:11:22,230 E 295 295] (raylet) runtime_env_agent_client.cc:216: The raylet exited immediately because the runtime env agent timed out when Raylet try to connect to it. This can happen because the runtime env agent was never started, or is listening to the wrong port. Read the log `cat /tmp/ray/session_latest/logs/runtime_env_agent.log`. You can find the log file structure here https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure.
(raylet, ip=10.0.0.19) 
```",another variation ray train code base root python binary deep neural network library use following enable rebuild appropriate compiler could load dynamic library open object file file directory could load dynamic library open object file file directory warning would like use please make sure missing properly set ray cluster address connected ray cluster view dashboard ray automatically cluster usage custom ray call trainer output use new output engine verbosity disable new output use legacy output engine set environment variable information please see view detailed visualize run raylet raylet worker process registered within process dead probably start raylet raylet worker process registered within process dead probably start raylet raylet worker process registered within process dead probably start raylet raylet worker process registered within process dead probably start raylet raylet agent timed status connection timed address port raylet raylet raylet immediately agent timed raylet try connect happen agent never listening wrong port read log cat find log file structure raylet raylet raylet agent timed status connection timed address port raylet raylet raylet immediately agent timed raylet try connect happen agent never listening wrong port read log cat find log file structure raylet raylet raylet agent timed status connection timed address port raylet raylet raylet immediately agent timed raylet try connect happen agent never listening wrong port read log cat find log file structure raylet raylet raylet agent timed status connection timed address port raylet raylet raylet immediately agent timed raylet try connect happen agent never listening wrong port read log cat find log file structure raylet raylet raylet agent timed status connection timed address port raylet raylet raylet immediately agent timed raylet try connect happen agent never listening wrong port read log cat find log file structure raylet raylet raylet agent timed status connection timed address port raylet raylet raylet immediately agent timed raylet try connect happen agent never listening wrong port read log cat find log file structure raylet raylet raylet agent timed status connection timed address port raylet raylet raylet immediately agent timed raylet try connect happen agent never listening wrong port read log cat find log file structure raylet raylet raylet agent timed status connection timed address port raylet raylet raylet immediately agent timed raylet try connect happen agent never listening wrong port read log cat find log file structure raylet,issue,positive,negative,negative,negative,negative,negative
1787637752,"![image](https://github.com/ray-project/ray/assets/56065503/d943bc8c-bf4d-4fff-8d95-9160427dcaff)
Looks like the test is getting greener after it's run separately. Do we still need to investigate? @rkooo567 ",image like test getting greener run separately still need investigate,issue,negative,neutral,neutral,neutral,neutral,neutral
1787521916,@richardliaw Sorry for the direct ping - I wasn't sure who to reach out to. Is there anything blocking this PR?,sorry direct ping sure reach anything blocking,issue,negative,positive,neutral,neutral,positive,positive
1787069780,"> It's not very helpful if it is not happening. Since we made a potential fix, can you also try ray 2.8 (released in a few days)? If it still happens (though I feel like it should be fixed..), we have other root cause in mind. We can prioritize fixing that issue.

Sure, we'll try Ray 2.8 and see if it pops up again.",helpful happening since made potential fix also try ray day still though feel like fixed root cause mind fixing issue sure try ray see,issue,positive,positive,positive,positive,positive,positive
1786692480,@jsdir @shixianc @valtab can you guys tell me more details about the setup? Is it same as the issue here? (you have an intermediate router that's just having async responses)? ,tell setup issue intermediate router,issue,negative,neutral,neutral,neutral,neutral,neutral
1786539639,I will report the concrete progress by eod today,report concrete progress today,issue,negative,positive,positive,positive,positive,positive
1786536885,"Hmm I think it is unlikely, but you can try using ray 2.8 to verify.

Also, other thing you can potentially try is to reduce the port range. I think using the default option should be sufficient to anyone actually (and maybe safer than using all ports). I also found ray uses some of high number ports internally (40000~ ish). Do you think you can also try the default worker port range option? ",think unlikely try ray verify also thing potentially try reduce port range think default option sufficient anyone actually maybe also found ray high number internally think also try default worker port range option,issue,negative,negative,neutral,neutral,negative,negative
1786524565,"Reassigning to core team since it's a runtime env issue
",core team since issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1786520873,"It works with 1 worker, but multiple workers do not work in a distributed manner. Can you look at the status of all your workers in Ray Dashboard and share them here?",work worker multiple work distributed manner look status ray dashboard share,issue,negative,neutral,neutral,neutral,neutral,neutral
1786517343,"The only error message it gives in the logs is ""The actor is dead because it was killed by `ray.kill`."" . Workers die directly before they are started.",error message actor dead die directly,issue,negative,negative,neutral,neutral,negative,negative
1786449016,"> Lint failure:
> 
> ```
> 
> 
> if HPUAcceleratorManager.is_initialized():
> --
>   | -        assert ""Intel-GAUDI"" in HPUAcceleratorManager.get_current_node_accelerator_type()
>   | +        assert (
>   | +            ""Intel-GAUDI"" in HPUAcceleratorManager.get_current_node_accelerator_type()
>   | +        )
>   | else:
>   | assert HPUAcceleratorManager.get_current_node_accelerator_type() is None
> ```

might be nice to have auto corrector",lint failure assert assert else assert none might nice auto corrector,issue,negative,positive,positive,positive,positive,positive
1786443063,"Lint failure:

```


if HPUAcceleratorManager.is_initialized():
--
  | -        assert ""Intel-GAUDI"" in HPUAcceleratorManager.get_current_node_accelerator_type()
  | +        assert (
  | +            ""Intel-GAUDI"" in HPUAcceleratorManager.get_current_node_accelerator_type()
  | +        )
  | else:
  | assert HPUAcceleratorManager.get_current_node_accelerator_type() is None


```",lint failure assert assert else assert none,issue,negative,negative,negative,negative,negative,negative
1786438347,"Lint failure:

```
Tue Oct 31 02:38:26 UTC 2023 Flake8....
--
  | python/ray/util/spark/databricks_hook.py:107:13: F821 undefined name 'db_api_entry'
  | python/ray/util/spark/databricks_hook.py:137:29: F821 undefined name 'db_api_entry'
  | 🚨 Error: The command exited with status 123


```",lint failure tue flake undefined name undefined name error command status,issue,negative,negative,negative,negative,negative,negative
1786358155,"@rynewang There is no actor involved in this repro script

```python
import ray

ray.init()
```",actor involved script python import ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1786350620,"ray status seems to show all resources as expected. even after running the training job and failing with ray object owner exited - ray status still works and shows all resources. None of the worker nodes get evicted or die. running a light weight remote task also works like returning the ips of nodes, this error only seems to pop up when I run a train job. ",ray status show even running training job failing ray object owner ray status still work none worker get die running light weight remote task also work like error pop run train job,issue,negative,positive,positive,positive,positive,positive
1786304592,"It's not very helpful if it is not happening. Since we made a potential fix, can you also try ray 2.8 (released in a few days)? If it still happens (though I feel like it should be fixed..), we have other root cause in mind. We can prioritize fixing that issue. ",helpful happening since made potential fix also try ray day still though feel like fixed root cause mind fixing issue,issue,positive,positive,neutral,neutral,positive,positive
1786251018,"@rkooo567 Thanks for your response. I have tried increasing the range to have 10-20 more ports than n_cpus and in these cases, I don't see the hang issue anymore. However, as you already mentioned this can only mitigate the issue, and does not completely resolve it. I am still unclear on what is causing this odd behavior, can you please elaborate more on this?
For example, why Ray doesn't guarantee the max number of workers and drivers? In other words, a) Why Ray doesn't respect the hard limit set by n_cpus? and b) why Ray creates more drivers? Cause I thought the script that runs ray.init() creates one driver which submits the jobs in the script to the Ray cluster, so why should Ray create more drivers?
Also, regardless, why creating more ports can cause ray.init() to hang?",thanks response tried increasing range see issue however already mitigate issue completely resolve still unclear causing odd behavior please elaborate example ray guarantee number ray respect hard limit set ray cause thought script one driver script ray cluster ray create also regardless cause,issue,positive,positive,neutral,neutral,positive,positive
1786185328,"Not sure... I think you can use `ray status` or `ray.nodes()` or `ray.cluster_resources()` to determine if all the nodes are actually connected to the cluster.  If a node got disconnected during the run, I would expect there to be some mention of that in the logs",sure think use ray status determine actually connected cluster node got disconnected run would expect mention,issue,negative,positive,positive,positive,positive,positive
1786162661,"I believe we already set the device ID for accelerate [here](https://github.com/ray-project/ray/blob/ac65dd10c8ad5703079259268a9fa804868f91ad/python/ray/train/torch/config.py#L141-L145), but if this isn't working then we should certainly fix it as a p0.

If anything, this should improve consistency and usability. ",believe already set device id accelerate working certainly fix anything improve consistency usability,issue,positive,positive,positive,positive,positive,positive
1786161248,"I think it's passing these days? @rkooo567 

<img width=""1334"" alt=""image"" src=""https://github.com/ray-project/ray/assets/56065503/f000b7ed-dd94-40e1-8552-f357e1b60e6f"">
",think passing day image,issue,negative,neutral,neutral,neutral,neutral,neutral
1786154326,"MaterializedDataset should work with multiple workers. I ran your script on my laptop, it works well. 
If you are seeing a worker process being killed. It's most likely due to OOM. consider reducing the size of `ray.data.ActorPoolStrategy(size=6)`.
if you have more detailed error information, please paste it here. ",work multiple ran script work well seeing worker process likely due consider reducing size detailed error information please paste,issue,negative,positive,neutral,neutral,positive,positive
1786127975,Since it's Ray 2.4.0 maybe it's https://github.com/ray-project/ray/issues/33957 . Could you try on Ray 2.7.1?,since ray maybe could try ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1786107529,"can you at least try increasing the port range to something like 50~100 (not like 10K)? I think this number can totally mitigate the issue. This issue is very tricky because port is needed to every driver & worker + Ray doesn't guarantee the max number of workers & driver created (and we don't plan to). Although we fix the issue, you may see a lot of failures at the end. ",least try increasing port range something like like think number totally mitigate issue issue tricky port every driver worker ray guarantee number driver plan although fix issue may see lot end,issue,positive,negative,negative,negative,negative,negative
1786075225,"I would treat it as P0, this also happened several times for deepspeed before. My hypothesis is that these libraries allocate device ids according to the worker's local rank. I'll check their code to confirm it.

Update: confirmed that Accelerate set `device:{local_rank}` by default.

**Accelerate**: https://github.com/huggingface/accelerate/blob/55747318a0f47cdfbc281e11269ba96214e4092d/src/accelerate/state.py#L153

**Deepspeed**: https://github.com/microsoft/DeepSpeed/blob/4199dc25af07ce588c274aff76ed61fb455d003d/deepspeed/runtime/engine.py#L949
What do you think? @matthewdeng ",would treat also several time hypothesis allocate device according worker local rank check code confirm update confirmed accelerate set device default accelerate think,issue,negative,negative,neutral,neutral,negative,negative
1786058221,"> @spolcyn are you guys using Ray HA?
> 
> There have been a bug where HA cluster has slow response time in job page after the head node restarts, but we've fixed that in the master (and Ray 2.8 that will be released this week).

1. Ray HA: No, just single head
2. Full logs: That head has been since restarted; would current head logs be helpful, if it's not currently happening?
3. Repro: Haven't found a way to repro it yet",ray ha bug ha cluster slow response time job page head node fixed master ray week ray ha single head full head since would current head helpful currently happening found way yet,issue,positive,positive,neutral,neutral,positive,positive
1786053725,"Also, can you also share

1. the full logs if possible.
2. the way to repro the issue? 

We will assign P1.5 until the questions are resolved",also also share full possible way issue assign resolved,issue,negative,positive,positive,positive,positive,positive
1786050045,"I am working with whole slide images (WSIs) which can be more than 100,000x100,000 pixels. Since the image scale is huge, we work with smaller patches instead. That's why we have input dimension in the form of (n, 224, 224, 3) with a huge n = number of patches. ",working whole slide since image scale huge work smaller instead input dimension form huge number,issue,positive,positive,positive,positive,positive,positive
1786048946,"@spolcyn are you guys using Ray HA? 

There have been a bug where HA cluster has slow response time in job page after the head node restarts, but we've fixed that in the master (and Ray 2.8 that will be released this week). ",ray ha bug ha cluster slow response time job page head node fixed master ray week,issue,positive,negative,negative,negative,negative,negative
1786027804,"@hhhvu I cannot reproduce this issue. But in your example, the dataset will have only one huge row, which doesn't seem useful in practice. I'm wondering if this is what you actually want. If you want to create a dataset with n rows, you show modify your code to `ray.data.from_numpy([np.random.randint(10, size=(224, 224, 3)) for _ in range(n)])`",reproduce issue example one huge row seem useful practice wondering actually want want create show modify code range,issue,positive,positive,positive,positive,positive,positive
1786012957,`max_tasks_in_flight_per_actor` only prefetches data at the actor task level. The data isn't prefetched into RAM or GRAM. We should support that.,data actor task level data ram gram support,issue,negative,neutral,neutral,neutral,neutral,neutral
1786004515,This should no longer be an issue -- the new checkpoint manager has a unit test that checks this specific issue.,longer issue new manager unit test specific issue,issue,negative,positive,neutral,neutral,positive,positive
1786001957,Preprocessors are deprecated. Can do something similar to this with the `metadata` argument of a trainer.,something similar argument trainer,issue,negative,neutral,neutral,neutral,neutral,neutral
1785998370,Feel free to re-open if still encountering the issue.,feel free still issue,issue,positive,positive,positive,positive,positive,positive
1785996790,These errors seem to be coming from Tune's old execution engine in ray<2.5 -- are you also running into this with the latest version of Ray (2.7.1 as of now)? @jakemdaly @grizzlybearg @llkongs ,seem coming tune old execution engine ray also running latest version ray,issue,negative,positive,positive,positive,positive,positive
1785984151,"> > Can we detect if people use Ray data in a ray job? If so, I'd like to move this table to the top above the ""Task/actor overview"" and hide this table when people don't use Ray Data
> 
> Yeah we can just hide the table if there aren't any datasets.

Great! Can you update the screenshot?",detect people use ray data ray job like move table top overview hide table people use ray data yeah hide table great update,issue,positive,positive,positive,positive,positive,positive
1785979878,Trial artifacts are not uploaded by default anymore. New temporary directory pattern recommended in docs examples avoids this problem: https://docs.ray.io/en/latest/train/user-guides/checkpoints.html#saving-checkpoints-during-training,trial default new temporary directory pattern problem,issue,negative,positive,positive,positive,positive,positive
1785976681,Resolved by new persistence implementation.,resolved new persistence implementation,issue,negative,positive,positive,positive,positive,positive
1785975356,`BatchPredictor` is deprecated in favor of the more lightweight ray data APIs: https://docs.ray.io/en/latest/data/batch_inference.html#using-models-from-ray-train,favor lightweight ray data,issue,negative,neutral,neutral,neutral,neutral,neutral
1785972829,Not planning on supporting artifact restoration -- recommended approach is to download artifacts on restoration manually.,supporting artifact restoration approach restoration manually,issue,negative,positive,positive,positive,positive,positive
1785970993,"This is mostly done, but should be moved to `ray.train` instead since `ray.air` is going to be removed",mostly done instead since going removed,issue,negative,positive,positive,positive,positive,positive
1785968535,This is not currently in the roadmap. The error message has already been fixed -- closing for now. See workaround above.,currently error message already fixed see,issue,negative,positive,neutral,neutral,positive,positive
1785958477,Can be achieved with `Trainer.restore`. `resume_from_checkpoint` is for a *new* experiment starting from a previous run's checkpoint. ,new experiment starting previous run,issue,negative,negative,neutral,neutral,negative,negative
1785956490,Checkpoints no longer contain this empty marker file. Closing as the remaining issue -- feel free to create a new issue if Oracle cloud is still not working properly with the new persistence implementation in Ray 2.7+.,longer contain empty marker file issue feel free create new issue oracle cloud still working properly new persistence implementation ray,issue,positive,positive,positive,positive,positive,positive
1785912212,"> Can we detect if people use Ray data in a ray job? If so, I'd like to move this table to the top above the ""Task/actor overview"" and hide this table when people don't use Ray Data

Yeah we can just hide the table if there aren't any datasets. ",detect people use ray data ray job like move table top overview hide table people use ray data yeah hide table,issue,negative,positive,positive,positive,positive,positive
1785869703,"Hey @prabodh1194, unfortunately this feature didn't make it into 2.8. It'll be included in 2.9 though!",hey unfortunately feature make included though,issue,negative,negative,negative,negative,negative,negative
1785865920,"hey @bussrakrkmz, your assumption is right, converting in-memory numpy data to a Ray Dataset needs to materialize the data in local process memory. This is inefficient for large amount of data. 
Storing the data in files can help. As data can be read in parallel and in a streaming way. Instead of text files, parquet files can be more efficient. 
Glad you worked around the issue. It seems that nothing else needs to be done. I'll close this issue.",hey assumption right converting data ray need materialize data local process memory inefficient large amount data data help data read parallel streaming way instead text parquet efficient glad worked around issue nothing else need done close issue,issue,positive,positive,positive,positive,positive,positive
1785859637,"@architkulkarni do you have any ideas on the issue above? To summarize:


**Head Node Start command:**
```
ray start --head --port=6379 --num-cpus=0 --num-gpus=0 --include-dashboard=true --dashboard-host=0.0.0.0 --node-manager-port=1915 --object-manager-port=1916 --dashboard-agent-grpc-port=1917 --dashboard-agent-listen-port=1918 --disable-usage-stats --block
```

**Worker Node Start command:**
```
ray start --address={public_head_ip}:6379 --node-manager-port=1915 --object-manager-port=1916 --dashboard-agent-grpc-port=1917 --dashboard-agent-listen-port=1918 --disable-usage-stats --block
```

Ray Train fails with the following:
```
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/registry.py"", line 301, in get
    return ray.get(self.references[k])
ray.exceptions.OwnerDiedError: Failed to retrieve object 00ffffffffffffffffffffffffffffffffffffff0300000001e1f505. To see information about where this ObjectRef was created in Python, set the environment variable RAY_record_ref_creation_sites=1 during `ray start` and `ray.init()`.

The object's owner has exited. This is the Python worker that first created the ObjectRef via `.remote()` or `ray.put()`. Check cluster logs (`/tmp/ray/session_latest/logs/*03000000ffffffffffffffffffffffffffffffffffffffffffffffff*` at IP address 10.0.0.117) for more information about the Python worker failure.
```

[Here](https://discuss.ray.io/t/tune-run-works-but-tunegridsearchcv-fit-does-not-work-for-me/6519/15)'s a related thread where the worker node was not actually connecting to the cluster.",issue summarize head node start command ray start head block worker node start command ray start block ray train following file line get return retrieve object see information python set environment variable ray start object owner python worker first via check cluster address information python worker failure related thread worker node actually cluster,issue,negative,negative,neutral,neutral,negative,negative
1785857019,"Hopefully this helps?

```
/tmp/ray/session_latest/logs/03000000ffffffffffffffffffffffffffffffffffffffffffffffff Output:

[2023-10-30 08:48:51,644 I 7500 7500] core_worker_process.cc:107: Constructing CoreWorkerProcess. pid: 7500
[2023-10-30 08:48:51,646 I 7500 7500] io_service_pool.cc:35: IOServicePool is running with 1 io_service.
[2023-10-30 08:48:51,652 I 7500 7500] grpc_server.cc:129: driver server started, listening on port 10006.
[2023-10-30 08:48:51,659 I 7500 7500] core_worker.cc:227: Initializing worker at address: 192.168.16.197:10006, worker ID 06000000ffffffffffffffffffffffffffffffffffffffffffffffff, raylet 0e6da528aae9f90cc09fb22babb40360918121dddd614df018b77792
[2023-10-30 08:48:51,660 I 7500 7500] task_event_buffer.cc:190: Reporting task events to GCS every 1000ms.
[2023-10-30 08:48:51,661 I 7500 7597] core_worker.cc:570: Event stats:


Global stats: 7 total (4 active)
Queueing time: mean = 21.646 us, max = 100.401 us, min = 21.257 us, total = 151.520 us
Execution time:  mean = 31.595 us, total = 221.165 us
Event stats:
	PeriodicalRunner.RunFnPeriodically - 2 total (1 active, 1 running), CPU time: mean = 9.509 us, total = 19.018 us
	UNKNOWN - 1 total (1 active), CPU time: mean = 0.000 s, total = 0.000 s
	NodeInfoGcsService.grpc_client.GetAllNodeInfo - 1 total (1 active), CPU time: mean = 0.000 s, total = 0.000 s
	InternalPubSubGcsService.grpc_client.GcsSubscriberPoll - 1 total (1 active), CPU time: mean = 0.000 s, total = 0.000 s
	InternalPubSubGcsService.grpc_client.GcsSubscriberCommandBatch - 1 total (0 active), CPU time: mean = 167.147 us, total = 167.147 us
	WorkerInfoGcsService.grpc_client.AddWorkerInfo - 1 total (0 active), CPU time: mean = 35.000 us, total = 35.000 us

-----------------
Task Event stats:

IO Service Stats:

Global stats: 2 total (1 active)
Queueing time: mean = 4.523 us, max = 9.046 us, min = 9.046 us, total = 9.046 us
Execution time:  mean = 6.109 us, total = 12.219 us
Event stats:
	CoreWorker.deadline_timer.flush_task_events - 1 total (1 active), CPU time: mean = 0.000 s, total = 0.000 s
	PeriodicalRunner.RunFnPeriodically - 1 total (0 active), CPU time: mean = 12.219 us, total = 12.219 us
Other Stats:
	grpc_in_progress:0
	current number of task events in buffer: 1
	total task events sent: 0 MiB
	total number of task events sent: 0
	num status task events dropped: 0
	num profile task events dropped: 0


[2023-10-30 08:48:51,661 I 7500 7597] accessor.cc:611: Received notification for node id = 75144c5dda31c02a95c6136be651b52e49c1082967eb32e2ef0abe59, IsAlive = 1
[2023-10-30 08:48:51,661 I 7500 7597] core_worker.cc:4262: Number of alive nodes:1
[2023-10-30 08:48:51,661 I 7500 7597] accessor.cc:611: Received notification for node id = 7fb2a236e768fc2ea3f19adf867b211e10ca0af53406a58d024f6312, IsAlive = 1
[2023-10-30 08:48:51,661 I 7500 7597] accessor.cc:611: Received notification for node id = 0e6da528aae9f90cc09fb22babb40360918121dddd614df018b77792, IsAlive = 1
[2023-10-30 08:48:51,661 I 7500 7597] accessor.cc:611: Received notification for node id = 209e8c99aa57aca3e213ac1a924d87c369511a4bf7193d9bc03a6afd, IsAlive = 0
[2023-10-30 08:48:51,661 I 7500 7597] core_worker.cc:306: Node failure from 209e8c99aa57aca3e213ac1a924d87c369511a4bf7193d9bc03a6afd. All objects pinned on that node will be lost if object reconstruction is not enabled.
[2023-10-30 08:48:51,662 I 7500 7500] event.cc:234: Set ray event level to warning
[2023-10-30 08:48:51,662 I 7500 7500] event.cc:342: Ray Event initialized for CORE_WORKER
[2023-10-30 08:48:52,365 I 7500 7500] core_worker.cc:2131: Submitting Placement Group creation to GCS: 6b1e37d945742c16a5b63039cb9906000000
[2023-10-30 08:48:52,680 I 7500 7597] direct_task_transport.cc:289: Connecting to raylet 7fb2a236e768fc2ea3f19adf867b211e10ca0af53406a58d024f6312
[2023-10-30 08:48:54,405 I 7500 7500] direct_actor_task_submitter.cc:36: Set max pending calls to -1 for actor 5a3c8a5d9e25e895f3fa826a06000000
[2023-10-30 08:48:54,409 I 7500 7597] actor_manager.cc:214: received notification on actor, state: DEPENDENCIES_UNREADY, actor_id: 5a3c8a5d9e25e895f3fa826a06000000, ip address: , port: 0, worker_id: NIL_ID, raylet_id: NIL_ID, num_restarts: 0, death context type=CONTEXT_NOT_SET
[2023-10-30 08:48:54,410 I 7500 7597] actor_manager.cc:214: received notification on actor, state: DEPENDENCIES_UNREADY, actor_id: 5a3c8a5d9e25e895f3fa826a06000000, ip address: , port: 0, worker_id: NIL_ID, raylet_id: NIL_ID, num_restarts: 0, death context type=CONTEXT_NOT_SET
[2023-10-30 08:48:54,410 I 7500 7597] actor_manager.cc:214: received notification on actor, state: PENDING_CREATION, actor_id: 5a3c8a5d9e25e895f3fa826a06000000, ip address: , port: 0, worker_id: NIL_ID, raylet_id: NIL_ID, num_restarts: 0, death context type=CONTEXT_NOT_SET
[2023-10-30 08:49:17,327 I 7500 7597] direct_task_transport.cc:55: Actor creation failed and we will not be retrying the creation task, actor id = 5a3c8a5d9e25e895f3fa826a06000000, task id = ffffffffffffffff5a3c8a5d9e25e895f3fa826a06000000
[2023-10-30 08:49:17,327 I 7500 7597] actor_manager.cc:214: received notification on actor, state: ALIVE, actor_id: 5a3c8a5d9e25e895f3fa826a06000000, ip address: 100.121.244.176, port: 10006, worker_id: 085b533d0b5e049ca3e62b56977c27a2db28a7098460cb17c7a6469c, raylet_id: 75144c5dda31c02a95c6136be651b52e49c1082967eb32e2ef0abe59, num_restarts: 0, death context type=CONTEXT_NOT_SET
[2023-10-30 08:49:17,328 I 7500 7597] direct_actor_task_submitter.cc:237: Connecting to actor 5a3c8a5d9e25e895f3fa826a06000000 at worker 085b533d0b5e049ca3e62b56977c27a2db28a7098460cb17c7a6469c
[2023-10-30 08:49:17,462 I 7500 7597] actor_manager.cc:214: received notification on actor, state: DEAD, actor_id: 5a3c8a5d9e25e895f3fa826a06000000, ip address: 100.121.244.176, port: 10006, worker_id: 085b533d0b5e049ca3e62b56977c27a2db28a7098460cb17c7a6469c, raylet_id: 75144c5dda31c02a95c6136be651b52e49c1082967eb32e2ef0abe59, num_restarts: 0, death context type=CreationTaskFailureContext
[2023-10-30 08:49:17,462 I 7500 7597] direct_actor_task_submitter.cc:287: Failing pending tasks for actor 5a3c8a5d9e25e895f3fa826a06000000 because the actor is already dead.
[2023-10-30 08:49:17,463 I 7500 7597] task_manager.cc:829: task 4083cef8db49d2ff5a3c8a5d9e25e895f3fa826a06000000 retries left: 0, oom retries left: 0, task failed due to oom: 0
[2023-10-30 08:49:17,463 I 7500 7597] task_manager.cc:845: No retries left for task 4083cef8db49d2ff5a3c8a5d9e25e895f3fa826a06000000, not going to resubmit.
[2023-10-30 08:49:17,463 I 7500 7597] task_manager.cc:903: Task failed: IOError: Fail all inflight tasks due to actor state change.: Type=ACTOR_TASK, Language=PYTHON, Resources: {}, function_descriptor={type=PythonFunctionDescriptor, module_name=ray.tune.trainable.util, class_name=with_parameters.<locals>._Inner, function_name=__ray_ready__, function_hash=}, task_id=4083cef8db49d2ff5a3c8a5d9e25e895f3fa826a06000000, task_name=_Inner.__ray_ready__, job_id=06000000, num_args=0, num_returns=1, depth=1, attempt_number=0, actor_task_spec={actor_id=5a3c8a5d9e25e895f3fa826a06000000, actor_caller_id=ffffffffffffffffffffffffffffffffffffffff06000000, actor_counter=0}, runtime_env_hash=-898807589, eager_install=1, setup_timeout_seconds=600
[2023-10-30 08:49:18,111 I 7500 7500] core_worker.cc:718: Disconnecting to the raylet.
[2023-10-30 08:49:18,111 I 7500 7500] raylet_client.cc:163: RayletClient::Disconnect, exit_type=INTENDED_USER_EXIT, exit_detail=Shutdown by ray.shutdown()., has creation_task_exception_pb_bytes=0
[2023-10-30 08:49:18,111 I 7500 7500] core_worker.cc:641: Shutting down a core worker.
[2023-10-30 08:49:18,111 I 7500 7500] task_event_buffer.cc:201: Shutting down TaskEventBuffer.
[2023-10-30 08:49:18,111 I 7500 7604] task_event_buffer.cc:183: Task event buffer io service stopped.
[2023-10-30 08:49:18,111 I 7500 7500] core_worker.cc:667: Disconnecting a GCS client.
[2023-10-30 08:49:18,111 I 7500 7597] core_worker.cc:884: Core worker main io service stopped.
[2023-10-30 08:49:18,112 I 7500 7500] core_worker.cc:671: Waiting for joining a core worker io thread. If it hangs here, there might be deadlock or a high load in the core worker io service.
[2023-10-30 08:49:18,118 I 7500 7500] core_worker.cc:684: Core worker ready to be deallocated.
[2023-10-30 08:49:18,118 I 7500 7500] core_worker.cc:632: Core worker is destructed
[2023-10-30 08:49:18,118 I 7500 7500] task_event_buffer.cc:201: Shutting down TaskEventBuffer.
[2023-10-30 08:49:18,119 W 7500 7607] server_call.h:324: [1] Not sending reply because executor stopped.
[2023-10-30 08:49:18,120 I 7500 7500] core_worker_process.cc:148: Destructing CoreWorkerProcessImpl. pid: 7500
[2023-10-30 08:49:18,120 I 7500 7500] io_service_pool.cc:47: IOServicePool is stopped.
[2023-10-30 08:49:18,165 I 7500 7500] stats.h:128: Stats module has shutdown.
```",hopefully output running driver server listening port worker address worker id raylet task every event global total active time mean u u min u total u execution time mean u total u event total active running time mean u total u unknown total active time mean total total active time mean total total active time mean total total active time mean u total u total active time mean u total u task event io service global total active time mean u u min u total u execution time mean u total u event total active time mean total total active time mean u total u current number task buffer total task sent mib total number task sent status task profile task received notification node id number alive received notification node id received notification node id received notification node id node failure pinned node lost object reconstruction set ray event level warning ray event placement group creation raylet set pending actor received notification actor state address port death context received notification actor state address port death context received notification actor state address port death context actor creation creation task actor id task id received notification actor state alive address port death context actor worker received notification actor state dead address port death context failing pending actor actor already dead task left left task due left task going resubmit task fail due actor state change raylet shutting core worker shutting task event buffer io service stopped client core worker main io service stopped waiting joining core worker io thread might deadlock high load core worker io service core worker ready core worker shutting sending reply executor stopped stopped module shutdown,issue,negative,negative,neutral,neutral,negative,negative
1785850154,"Thanks for the help on NCCL , i'll be able to figure this out.
@matthewdeng I was trying to connect 2 vms which are not in the same geographical area - what is the limit and best practices for creating a ray cluster here? I've also tried connecting them over a single private network but had this issue.
Remote ray tasks seem to work fine but this only happens when I launch a ray train job",thanks help able figure trying connect geographical area limit best ray cluster also tried single private network issue remote ray seem work fine launch ray train job,issue,positive,positive,positive,positive,positive,positive
1785817895,These hashes aren't really used for anything sensitive so that they're md5 isn't really an issue. I'll find someone appropriate to review what the consequences are if we change all these paths :D,really used anything sensitive really issue find someone appropriate review change,issue,negative,positive,positive,positive,positive,positive
1785795591,"Can we detect if people use Ray data in a ray job? If so, I'd like to move this table to the top above the ""Task/actor overview"" and hide this table when people don't use Ray Data",detect people use ray data ray job like move table top overview hide table people use ray data,issue,negative,positive,positive,positive,positive,positive
1785789619,"@lukakap I ran into the same `tuner.pkl` is not found error by configuring `RunConfig(local_dir=...)` rather than using the environment variable `RAY_AIR_LOCAL_CACHE_DIR=...`.

Could you try setting this environment variable instead? See here: https://docs.ray.io/en/latest/train/user-guides/persistent-storage.html#setting-the-intermediate-local-directory",ran found error rather environment variable could try setting environment variable instead see,issue,negative,neutral,neutral,neutral,neutral,neutral
1785787206,"For the `NCCL_SOCKET_IFNAME` questions, I'm not too sure - these questions are better suited for NCCL. 

For the Ray issue, this is unrelated to NCCL. I've seen this happen before if the workers cannot properly connect to the head node. Is the issue reproducible?",sure better ray issue unrelated seen happen properly connect head node issue reproducible,issue,positive,positive,positive,positive,positive,positive
1785766569,"@bveeramani move the test to the right job now, thankks",move test right job,issue,negative,positive,positive,positive,positive,positive
1785710427,"@LeonLuttenberger - thanks for the contribution! Shall we also add a unit test to test out `map_groups()`?

Technically after this PR, `ds.groupby(keys).map_groups(fn)` should also work on multiple columns. Thanks.",thanks contribution shall also add unit test test technically also work multiple thanks,issue,positive,positive,neutral,neutral,positive,positive
1785657598,"@angelinalg, @dmatrix, @stephanie-wang  need some stamp on the doc change, thankkks",need stamp doc change,issue,negative,neutral,neutral,neutral,neutral,neutral
1785613106,"Removing ray.shutdown() from the script makes the hang issue more consistent, here is the updated script to reproduce the issue with higher success rate @rkooo567 
`import ray
import subprocess
import os
import numpy as np
import signal

def np_mat(t):
    x = np.random.rand(t,t)
    return x.nbytes

startup_cmd = ""ray stop --force; ray start --head --port 13041 --worker-port-list 13048,13049 --ray-client-server-port 13055 --node-manager-port 13053 --object-manager-port 13054 --runtime-env-agent-port 13056 --num-cpus 1 --include-dashboard False --disable-usage-stats --temp-dir=/dev/shm/ray --object-store-memory=50000000000""

process = subprocess.run(startup_cmd, capture_output=True, shell=True)
if process.returncode:
    print(process.stderr)
    print(process.stdout)
print(f""Ray start completed"")
if not ray.is_initialized():
    print(f""Ray Driver not started"")
    ray.init(address=""127.0.0.1:13041"",
         _temp_dir=""/dev/shm/ray"")
print(f""Ray Driver started"")

remote_func = ray.remote(np_mat)
future = remote_func.remote(t=300)
result = ray.get(future)
print(f""Ray remote job completed, array size {result / 2**30} GB"")

shutdown_cmd = f""ray stop --force""
process = subprocess.run(shutdown_cmd, capture_output=True, shell=True)
print(f""Ray stop completed"")
os.kill(os.getpid(), signal.SIGKILL)`",removing script issue consistent script reproduce issue higher success rate import ray import import o import import signal return ray stop force ray start head port false process print print print ray start print ray driver print ray driver future result future print ray remote job array size result ray stop force process print ray stop,issue,negative,positive,neutral,neutral,positive,positive
1785550891,"Hey, @amogkam I also had object's owner has exited
Is this related , can you check once?

I had run this example from ray docs for huggingface + ray train with some modifications:
https://gist.github.com/smiraldr/be875e09281e4a55fa600da9c143b8bf

Runtime logs:
```
(base) root@gc-unruffled-jemison:~# python3 hf.py 
2023-10-30 08:48:45.472739: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-30 08:48:45.646029: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-10-30 08:48:46.736852: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
2023-10-30 08:48:46.736997: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
2023-10-30 08:48:46.737021: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
comet_ml is installed but `COMET_API_KEY` is not set.
2023-10-30 08:48:51,627 INFO worker.py:1458 -- Connecting to existing Ray cluster at address: 192.168.16.197:6379...
2023-10-30 08:48:51,639 INFO worker.py:1633 -- Connected to Ray cluster. View the dashboard at 192.168.16.197:8265 
2023-10-30 08:48:51,733 INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949

View detailed results here: /home/ray/ray_results/TorchTrainer_2023-10-30_08-48-51
To visualize your results with TensorBoard, run: `tensorboard --logdir /home/ray/ray_results/TorchTrainer_2023-10-30_08-48-51`
(TrainTrainable pid=581, ip=192.168.8.173) Local object store memory usage:
(TrainTrainable pid=581, ip=192.168.8.173) 
(TrainTrainable pid=581, ip=192.168.8.173) (global lru) capacity: 12456522547
(TrainTrainable pid=581, ip=192.168.8.173) (global lru) used: 0%
(TrainTrainable pid=581, ip=192.168.8.173) (global lru) num objects: 0
(TrainTrainable pid=581, ip=192.168.8.173) (global lru) num evictions: 0
(TrainTrainable pid=581, ip=192.168.8.173) (global lru) bytes evicted: 0
(TrainTrainable pid=581, ip=192.168.8.173) 
(TrainTrainable pid=581, ip=192.168.8.173) Exception raised in creation task: The actor died because of an error raised in its creation task, ray::_Inner.__init__() (pid=581, ip=100.121.244.176, actor_id=5a3c8a5d9e25e895f3fa826a06000000, repr=TorchTrainer)
(TrainTrainable pid=581, ip=192.168.8.173)   File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/trainable.py"", line 185, in __init__
(TrainTrainable pid=581, ip=192.168.8.173)     self.setup(copy.deepcopy(self.config))
(TrainTrainable pid=581, ip=192.168.8.173)   File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/util.py"", line 304, in setup
(TrainTrainable pid=581, ip=192.168.8.173)     setup_kwargs[k] = parameter_registry.get(prefix + k)
(TrainTrainable pid=581, ip=192.168.8.173)   File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/registry.py"", line 301, in get
(TrainTrainable pid=581, ip=192.168.8.173)     return ray.get(self.references[k])
(TrainTrainable pid=581, ip=192.168.8.173) ray.exceptions.OwnerDiedError: Failed to retrieve object 00ffffffffffffffffffffffffffffffffffffff0600000002e1f505. To see information about where this ObjectRef was created in Python, set the environment variable RAY_record_ref_creation_sites=1 during `ray start` and `ray.init()`.
(TrainTrainable pid=581, ip=192.168.8.173) 
(TrainTrainable pid=581, ip=192.168.8.173) The object's owner has exited. This is the Python worker that first created the ObjectRef via `.remote()` or `ray.put()`. Check cluster logs (`/tmp/ray/session_latest/logs/*06000000ffffffffffffffffffffffffffffffffffffffffffffffff*` at IP address 192.168.16.197) for more information about the Python worker failure.
2023-10-30 08:49:17,465 ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_d2bc9_00000
Traceback (most recent call last):
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py"", line 110, in resolve_future
    result = ray.get(future)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/auto_init_hook.py"", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/worker.py"", line 2549, in get
    raise value
ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, ray::_Inner.__init__() (pid=581, ip=100.121.244.176, actor_id=5a3c8a5d9e25e895f3fa826a06000000, repr=TorchTrainer)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/trainable.py"", line 185, in __init__
    self.setup(copy.deepcopy(self.config))
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/util.py"", line 304, in setup
    setup_kwargs[k] = parameter_registry.get(prefix + k)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/registry.py"", line 301, in get
    return ray.get(self.references[k])
ray.exceptions.OwnerDiedError: Failed to retrieve object 00ffffffffffffffffffffffffffffffffffffff0600000002e1f505. To see information about where this ObjectRef was created in Python, set the environment variable RAY_record_ref_creation_sites=1 during `ray start` and `ray.init()`.

The object's owner has exited. This is the Python worker that first created the ObjectRef via `.remote()` or `ray.put()`. Check cluster logs (`/tmp/ray/session_latest/logs/*06000000ffffffffffffffffffffffffffffffffffffffffffffffff*` at IP address 192.168.16.197) for more information about the Python worker failure.

Training errored after 0 iterations at 2023-10-30 08:49:17. Total running time: 25s
Error file: /home/ray/ray_results/TorchTrainer_2023-10-30_08-48-51/TorchTrainer_d2bc9_00000_0_2023-10-30_08-48-52/error.txt

2023-10-30 08:49:17,470 ERROR tune.py:1139 -- Trials did not complete: [TorchTrainer_d2bc9_00000]
2023-10-30 08:49:17,474 WARNING experiment_analysis.py:205 -- Failed to fetch metrics for 1 trial(s):
- TorchTrainer_d2bc9_00000: FileNotFoundError('Could not fetch metrics for TorchTrainer_d2bc9_00000: both result.json and progress.csv were not found at /home/ray/ray_results/TorchTrainer_2023-10-30_08-48-51/TorchTrainer_d2bc9_00000_0_2023-10-30_08-48-52')
TuneError: Failure # 1 (occurred at 2023-10-30_08-49-17)
The actor died because of an error raised in its creation task, [36mray::_Inner.__init__()[39m (pid=581, ip=100.121.244.176, actor_id=5a3c8a5d9e25e895f3fa826a06000000, repr=TorchTrainer)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/trainable.py"", line 185, in __init__
    self.setup(copy.deepcopy(self.config))
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/util.py"", line 304, in setup
    setup_kwargs[k] = parameter_registry.get(prefix + k)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/registry.py"", line 301, in get
    return ray.get(self.references[k])
ray.exceptions.OwnerDiedError: Failed to retrieve object 00ffffffffffffffffffffffffffffffffffffff0600000002e1f505. To see information about where this ObjectRef was created in Python, set the 
environment variable RAY_record_ref_creation_sites=1 during `ray start` and `ray.init()`.

The object's owner has exited. This is the Python worker that first created the ObjectRef via `.remote()` or `ray.put()`. Check cluster logs 
(`/tmp/ray/session_latest/logs/*06000000ffffffffffffffffffffffffffffffffffffffffffffffff*` at IP address 192.168.16.197) for more information about the Python worker failure.


The above exception was the direct cause of the following exception:

╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮
│ /home/ray/hf.py:83 in <module>                                                                   │
│                                                                                                  │
│   80 ray_trainer = TorchTrainer(                                                                 │
│   81 │   train_func, scaling_config=ScalingConfig(num_workers=2, use_gpu=True)                   │
│   82 )                                                                                           │
│ ❱ 83 ray_trainer.fit()                                                                           │
│                                                                                                  │
│ /home/ray/anaconda3/lib/python3.9/site-packages/ray/train/base_trainer.py:668 in fit             │
│                                                                                                  │
│   665 │   │   if result.error:                                                                   │
│   666 │   │   │   # Raise trainable errors to the user with a message to restore                 │
│   667 │   │   │   # or configure `FailureConfig` in a new run.                                   │
│ ❱ 668 │   │   │   raise TrainingFailedError(                                                     │
│   669 │   │   │   │   ""\n"".join([restore_msg, TrainingFailedError._FAILURE_CONFIG_MSG])          │
│   670 │   │   │   ) from result.error                                                            │
│   671 │   │   return result                                                                      │
╰──────────────────────────────────────────────────────────────────────────────────────────────────╯
TrainingFailedError: The Ray Train run failed. Please inspect the previous error messages for a cause. After fixing the issue (assuming that the error is not caused by your own application logic, but 
rather an error such as OOM), you can restart the run from scratch or continue this run.
To continue this run, you can use: `trainer = TorchTrainer.restore(""/home/ray/ray_results/TorchTrainer_2023-10-30_08-48-51"")`.
To start a new run that will retry on training failures, set `train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the Trainer's `run_config` with `max_failures > 0`, or `max_failures
= -1` for unlimited retries.
```


/tmp/ray/session_latest/logs/*06000000ffffffffffffffffffffffffffffffffffffffffffffffff* Output:
```
[2023-10-30 08:48:51,644 I 7500 7500] core_worker_process.cc:107: Constructing CoreWorkerProcess. pid: 7500
[2023-10-30 08:48:51,646 I 7500 7500] io_service_pool.cc:35: IOServicePool is running with 1 io_service.
[2023-10-30 08:48:51,652 I 7500 7500] grpc_server.cc:129: driver server started, listening on port 10006.
[2023-10-30 08:48:51,659 I 7500 7500] core_worker.cc:227: Initializing worker at address: 192.168.16.197:10006, worker ID 06000000ffffffffffffffffffffffffffffffffffffffffffffffff, raylet 0e6da528aae9f90cc09fb22babb40360918121dddd614df018b77792
[2023-10-30 08:48:51,660 I 7500 7500] task_event_buffer.cc:190: Reporting task events to GCS every 1000ms.
[2023-10-30 08:48:51,661 I 7500 7597] core_worker.cc:570: Event stats:


Global stats: 7 total (4 active)
Queueing time: mean = 21.646 us, max = 100.401 us, min = 21.257 us, total = 151.520 us
Execution time:  mean = 31.595 us, total = 221.165 us
Event stats:
	PeriodicalRunner.RunFnPeriodically - 2 total (1 active, 1 running), CPU time: mean = 9.509 us, total = 19.018 us
	UNKNOWN - 1 total (1 active), CPU time: mean = 0.000 s, total = 0.000 s
	NodeInfoGcsService.grpc_client.GetAllNodeInfo - 1 total (1 active), CPU time: mean = 0.000 s, total = 0.000 s
	InternalPubSubGcsService.grpc_client.GcsSubscriberPoll - 1 total (1 active), CPU time: mean = 0.000 s, total = 0.000 s
	InternalPubSubGcsService.grpc_client.GcsSubscriberCommandBatch - 1 total (0 active), CPU time: mean = 167.147 us, total = 167.147 us
	WorkerInfoGcsService.grpc_client.AddWorkerInfo - 1 total (0 active), CPU time: mean = 35.000 us, total = 35.000 us

-----------------
Task Event stats:

IO Service Stats:

Global stats: 2 total (1 active)
Queueing time: mean = 4.523 us, max = 9.046 us, min = 9.046 us, total = 9.046 us
Execution time:  mean = 6.109 us, total = 12.219 us
Event stats:
	CoreWorker.deadline_timer.flush_task_events - 1 total (1 active), CPU time: mean = 0.000 s, total = 0.000 s
	PeriodicalRunner.RunFnPeriodically - 1 total (0 active), CPU time: mean = 12.219 us, total = 12.219 us
Other Stats:
	grpc_in_progress:0
	current number of task events in buffer: 1
	total task events sent: 0 MiB
	total number of task events sent: 0
	num status task events dropped: 0
	num profile task events dropped: 0


[2023-10-30 08:48:51,661 I 7500 7597] accessor.cc:611: Received notification for node id = 75144c5dda31c02a95c6136be651b52e49c1082967eb32e2ef0abe59, IsAlive = 1
[2023-10-30 08:48:51,661 I 7500 7597] core_worker.cc:4262: Number of alive nodes:1
[2023-10-30 08:48:51,661 I 7500 7597] accessor.cc:611: Received notification for node id = 7fb2a236e768fc2ea3f19adf867b211e10ca0af53406a58d024f6312, IsAlive = 1
[2023-10-30 08:48:51,661 I 7500 7597] accessor.cc:611: Received notification for node id = 0e6da528aae9f90cc09fb22babb40360918121dddd614df018b77792, IsAlive = 1
[2023-10-30 08:48:51,661 I 7500 7597] accessor.cc:611: Received notification for node id = 209e8c99aa57aca3e213ac1a924d87c369511a4bf7193d9bc03a6afd, IsAlive = 0
[2023-10-30 08:48:51,661 I 7500 7597] core_worker.cc:306: Node failure from 209e8c99aa57aca3e213ac1a924d87c369511a4bf7193d9bc03a6afd. All objects pinned on that node will be lost if object reconstruction is not enabled.
[2023-10-30 08:48:51,662 I 7500 7500] event.cc:234: Set ray event level to warning
[2023-10-30 08:48:51,662 I 7500 7500] event.cc:342: Ray Event initialized for CORE_WORKER
[2023-10-30 08:48:52,365 I 7500 7500] core_worker.cc:2131: Submitting Placement Group creation to GCS: 6b1e37d945742c16a5b63039cb9906000000
[2023-10-30 08:48:52,680 I 7500 7597] direct_task_transport.cc:289: Connecting to raylet 7fb2a236e768fc2ea3f19adf867b211e10ca0af53406a58d024f6312
[2023-10-30 08:48:54,405 I 7500 7500] direct_actor_task_submitter.cc:36: Set max pending calls to -1 for actor 5a3c8a5d9e25e895f3fa826a06000000
[2023-10-30 08:48:54,409 I 7500 7597] actor_manager.cc:214: received notification on actor, state: DEPENDENCIES_UNREADY, actor_id: 5a3c8a5d9e25e895f3fa826a06000000, ip address: , port: 0, worker_id: NIL_ID, raylet_id: NIL_ID, num_restarts: 0, death context type=CONTEXT_NOT_SET
[2023-10-30 08:48:54,410 I 7500 7597] actor_manager.cc:214: received notification on actor, state: DEPENDENCIES_UNREADY, actor_id: 5a3c8a5d9e25e895f3fa826a06000000, ip address: , port: 0, worker_id: NIL_ID, raylet_id: NIL_ID, num_restarts: 0, death context type=CONTEXT_NOT_SET
[2023-10-30 08:48:54,410 I 7500 7597] actor_manager.cc:214: received notification on actor, state: PENDING_CREATION, actor_id: 5a3c8a5d9e25e895f3fa826a06000000, ip address: , port: 0, worker_id: NIL_ID, raylet_id: NIL_ID, num_restarts: 0, death context type=CONTEXT_NOT_SET
[2023-10-30 08:49:17,327 I 7500 7597] direct_task_transport.cc:55: Actor creation failed and we will not be retrying the creation task, actor id = 5a3c8a5d9e25e895f3fa826a06000000, task id = ffffffffffffffff5a3c8a5d9e25e895f3fa826a06000000
[2023-10-30 08:49:17,327 I 7500 7597] actor_manager.cc:214: received notification on actor, state: ALIVE, actor_id: 5a3c8a5d9e25e895f3fa826a06000000, ip address: 100.121.244.176, port: 10006, worker_id: 085b533d0b5e049ca3e62b56977c27a2db28a7098460cb17c7a6469c, raylet_id: 75144c5dda31c02a95c6136be651b52e49c1082967eb32e2ef0abe59, num_restarts: 0, death context type=CONTEXT_NOT_SET
[2023-10-30 08:49:17,328 I 7500 7597] direct_actor_task_submitter.cc:237: Connecting to actor 5a3c8a5d9e25e895f3fa826a06000000 at worker 085b533d0b5e049ca3e62b56977c27a2db28a7098460cb17c7a6469c
[2023-10-30 08:49:17,462 I 7500 7597] actor_manager.cc:214: received notification on actor, state: DEAD, actor_id: 5a3c8a5d9e25e895f3fa826a06000000, ip address: 100.121.244.176, port: 10006, worker_id: 085b533d0b5e049ca3e62b56977c27a2db28a7098460cb17c7a6469c, raylet_id: 75144c5dda31c02a95c6136be651b52e49c1082967eb32e2ef0abe59, num_restarts: 0, death context type=CreationTaskFailureContext
[2023-10-30 08:49:17,462 I 7500 7597] direct_actor_task_submitter.cc:287: Failing pending tasks for actor 5a3c8a5d9e25e895f3fa826a06000000 because the actor is already dead.
[2023-10-30 08:49:17,463 I 7500 7597] task_manager.cc:829: task 4083cef8db49d2ff5a3c8a5d9e25e895f3fa826a06000000 retries left: 0, oom retries left: 0, task failed due to oom: 0
[2023-10-30 08:49:17,463 I 7500 7597] task_manager.cc:845: No retries left for task 4083cef8db49d2ff5a3c8a5d9e25e895f3fa826a06000000, not going to resubmit.
[2023-10-30 08:49:17,463 I 7500 7597] task_manager.cc:903: Task failed: IOError: Fail all inflight tasks due to actor state change.: Type=ACTOR_TASK, Language=PYTHON, Resources: {}, function_descriptor={type=PythonFunctionDescriptor, module_name=ray.tune.trainable.util, class_name=with_parameters.<locals>._Inner, function_name=__ray_ready__, function_hash=}, task_id=4083cef8db49d2ff5a3c8a5d9e25e895f3fa826a06000000, task_name=_Inner.__ray_ready__, job_id=06000000, num_args=0, num_returns=1, depth=1, attempt_number=0, actor_task_spec={actor_id=5a3c8a5d9e25e895f3fa826a06000000, actor_caller_id=ffffffffffffffffffffffffffffffffffffffff06000000, actor_counter=0}, runtime_env_hash=-898807589, eager_install=1, setup_timeout_seconds=600
[2023-10-30 08:49:18,111 I 7500 7500] core_worker.cc:718: Disconnecting to the raylet.
[2023-10-30 08:49:18,111 I 7500 7500] raylet_client.cc:163: RayletClient::Disconnect, exit_type=INTENDED_USER_EXIT, exit_detail=Shutdown by ray.shutdown()., has creation_task_exception_pb_bytes=0
[2023-10-30 08:49:18,111 I 7500 7500] core_worker.cc:641: Shutting down a core worker.
[2023-10-30 08:49:18,111 I 7500 7500] task_event_buffer.cc:201: Shutting down TaskEventBuffer.
[2023-10-30 08:49:18,111 I 7500 7604] task_event_buffer.cc:183: Task event buffer io service stopped.
[2023-10-30 08:49:18,111 I 7500 7500] core_worker.cc:667: Disconnecting a GCS client.
[2023-10-30 08:49:18,111 I 7500 7597] core_worker.cc:884: Core worker main io service stopped.
[2023-10-30 08:49:18,112 I 7500 7500] core_worker.cc:671: Waiting for joining a core worker io thread. If it hangs here, there might be deadlock or a high load in the core worker io service.
[2023-10-30 08:49:18,118 I 7500 7500] core_worker.cc:684: Core worker ready to be deallocated.
[2023-10-30 08:49:18,118 I 7500 7500] core_worker.cc:632: Core worker is destructed
[2023-10-30 08:49:18,118 I 7500 7500] task_event_buffer.cc:201: Shutting down TaskEventBuffer.
[2023-10-30 08:49:18,119 W 7500 7607] server_call.h:324: [1] Not sending reply because executor stopped.
[2023-10-30 08:49:18,120 I 7500 7500] core_worker_process.cc:148: Destructing CoreWorkerProcessImpl. pid: 7500
[2023-10-30 08:49:18,120 I 7500 7500] io_service_pool.cc:47: IOServicePool is stopped.
[2023-10-30 08:49:18,165 I 7500 7500] stats.h:128: Stats module has shutdown.
```",hey also object owner related check run example ray ray train base root python binary deep neural network library use following enable rebuild appropriate compiler custom may see slightly different numerical due different computation turn set environment variable could load dynamic library open object file file directory could load dynamic library open object file file directory warning would like use please make sure missing properly set ray cluster address connected ray cluster view dashboard output use new output engine verbosity disable new output use legacy output engine set environment variable information please see view detailed visualize run local object store memory usage global capacity global used global global global exception raised creation task actor error raised creation task ray file line file line setup prefix file line get return retrieve object see information python set environment variable ray start object owner python worker first via check cluster address information python worker failure error trial task trial recent call last file line result future file line return file line wrapper return file line get raise value actor error raised creation task ray file line file line setup prefix file line get return retrieve object see information python set environment variable ray start object owner python worker first via check cluster address information python worker failure training total running time error file error complete warning fetch metric trial fetch metric found failure actor error raised creation task file line file line setup prefix file line get return retrieve object see information python set environment variable ray start object owner python worker first via check cluster address information python worker failure exception direct cause following exception recent call last module fit raise trainable user message restore configure new run raise return result ray train run please inspect previous error cause fixing issue assuming error application logic rather error restart run scratch continue run continue run use trainer start new run retry training set trainer unlimited output running driver server listening port worker address worker id raylet task every event global total active time mean u u min u total u execution time mean u total u event total active running time mean u total u unknown total active time mean total total active time mean total total active time mean total total active time mean u total u total active time mean u total u task event io service global total active time mean u u min u total u execution time mean u total u event total active time mean total total active time mean u total u current number task buffer total task sent mib total number task sent status task profile task received notification node id number alive received notification node id received notification node id received notification node id node failure pinned node lost object reconstruction set ray event level warning ray event placement group creation raylet set pending actor received notification actor state address port death context received notification actor state address port death context received notification actor state address port death context actor creation creation task actor id task id received notification actor state alive address port death context actor worker received notification actor state dead address port death context failing pending actor actor already dead task left left task due left task going resubmit task fail due actor state change raylet shutting core worker shutting task event buffer io service stopped client core worker main io service stopped waiting joining core worker io thread might deadlock high load core worker io service core worker ready core worker shutting sending reply executor stopped stopped module shutdown,issue,negative,negative,neutral,neutral,negative,negative
1785458163,"Just ran into the same issue with Ray 2.7. The solution within Ray could be to specify a minimum version for fastapi which is at least fastapi >= 0.1.19, because from there fast-api has upper bound restrictions for the starlette version which prevents the issue.",ran issue ray solution within ray could specify minimum version least upper bound version issue,issue,negative,negative,negative,negative,negative,negative
1785245284,"The head node memory utilization now looks like this, the utilization increases for some time after we deploy a new `RayService` and then it levels off.

![Screenshot 2023-10-30 at 9 30 08 AM](https://github.com/ray-project/ray/assets/98415378/468e0e21-27d8-4da0-979d-b864f079171d)

^Here, I'm using the `ray_node_mem_used` metric, I'm not sure if that is correct. 

![Screenshot 2023-10-30 at 9 48 20 AM](https://github.com/ray-project/ray/assets/98415378/e03f4095-cad1-44e1-afb6-964840d797f1)

^This graph compares the two metrics `sum(ray_node_mem_used{pod=""head-pod-name""}) by (pod)` and `sum(ray_component_rss_mb{pod=""head-pod-name""} * 1e6) by (pod)`

But in general, looks like the memory leak is fixed!
",head node memory utilization like utilization time deploy new metric sure correct graph two metric sum pod sum pod general like memory leak fixed,issue,positive,positive,positive,positive,positive,positive
1785244622,"Hi @justinvyu , I'm not quite set up to make contributions to Ray from my machine, so please go ahead and add to your backlog. Thanks!",hi quite set make ray machine please go ahead add backlog thanks,issue,positive,positive,positive,positive,positive,positive
1784665184,It'd be great if this could be fixed. I'm also happy to submit a PR if that'd be helpful.,great could fixed also happy submit helpful,issue,positive,positive,positive,positive,positive,positive
1784526000,This is a high priority item to fix it in 2.9,high priority item fix,issue,negative,positive,positive,positive,positive,positive
1784498890,"I think this is still a problem and it is impacting the performance of Ray Tune. 

Like @robertnishihara mentioned above, the gpus exposed to the remote function can be overridden by setting the CUDA_VISIBLE_DEVICES environment variable. However, in the case of Ray Tune, we specify the number of gpus exposed per worker in advance, and each worker basically can not have data sharing with more than the number of gpus per worker specified; thus limiting the benefit of data parallelism within a single node.

For example, I am running 8 workers (each with num_gpus=1) in a single node machine with 8 gpus. Each worker will not utilize all the gpus available to it because num_gpus=1 isolates the available gpu for each worker. The ideal case is to allow each worker to use all gpus for the sake of data parallelism.

One possible solution is to blow up the logical gpu count by the factor of parallelism (logical gpu count = physical gpu count * factor of parallelism) you want to run so that each worker can run with num_gpus=(ideal parallelism a.k.a number of physical gpus) and we can further specify the machines with CUDA_VISIBLE_DEVICES.",think still problem performance ray tune like exposed remote function setting environment variable however case ray tune specify number exposed per worker advance worker basically data number per worker thus limiting benefit data parallelism within single node example running single node machine worker utilize available available worker ideal case allow worker use sake data parallelism one possible solution blow logical count factor parallelism logical count physical count factor parallelism want run worker run ideal parallelism number physical specify,issue,positive,positive,positive,positive,positive,positive
1784472975,"Lint failure:

```


def test_get_current_process_visible_accelerator_ids():
--
  | os.environ[hpu.HABANA_VISIBLE_DEVICES_ENV_VAR] = ""0,1,2""
  | -    assert HPUAcceleratorManager.get_current_process_visible_accelerator_ids() == [""0"", ""1"", ""2""]  # noqa: E501
  | +    assert HPUAcceleratorManager.get_current_process_visible_accelerator_ids() == [
  | +        ""0"",
  | +        ""1"",
  | +        ""2"",
  | +    ]  # noqa: E501


```",lint failure assert assert,issue,negative,negative,negative,negative,negative,negative
1784443989,"hi, @bveeramani will we be getting this in ray 2.8 ? really excited 🤩 for this 😄 ",hi getting ray really excited,issue,negative,positive,positive,positive,positive,positive
1784435706,"So on the same commit: 
- One run has 182: https://buildkite.com/ray-project/release-tests-branch/builds/2352#018b73b3-65ea-451d-9345-0ad9492a9bc1/353-471
- One run has 200: https://buildkite.com/ray-project/release-tests-branch/builds/2353#018b73b3-8760-46e9-976e-0306bd2aa551/352-470

I think we should ignore this as variance. ",commit one run one run think ignore variance,issue,negative,neutral,neutral,neutral,neutral,neutral
1784393444,"Discussed with @jjyao and @rickyyx 

The PR has been reverted from Ray 2.8. We decided to bear the perf regression in the master since it improves the observability greatly, and the impact is very minimal (small regression from extremely stress tests, and other tests didn't seem to be affected)",ray decided bear regression master since observability greatly impact minimal small regression extremely stress seem affected,issue,negative,positive,neutral,neutral,positive,positive
1784391936,We will close the issue until there's a follow up. We made a speculative fix in the master. Please follow up if you see this issue again. ,close issue follow made speculative fix master please follow see issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1784166148,"same error:
python3.10/site-packages/ray/dashboard/modules/dashboard_sdk.py"", line 264, in _check_connection_and_version_with_url
    raise RuntimeError(
RuntimeError: Version check returned 404. Jobs API is not supported on the Ray cluster. Please ensure the cluster is running Ray 1.9 or higher.",error line raise version check returned ray cluster please ensure cluster running ray higher,issue,negative,positive,positive,positive,positive,positive
1784124696,"I suggest we can remove the following part code :

```
        db_api_entry = _get_db_api_entry()
        try:
            db_api_entry.registerBackgroundSparkJobGroup(
                ray_cluster_handler.spark_job_group_id
            )
        except Exception:
            _logger.warning(
                ""Registering Ray cluster spark job as background job failed. ""
                ""You need to manually call `ray.util.spark.shutdown_ray_cluster()` ""
                ""before detaching your Databricks notebook.""
            )
```

because when databricks notebook REPL terminates, it will kill Ray head node process, and then Ray worker nodes will exit when they detect Ray head node dies. Then it triggers the spark job exit.

So that we can get rid of calling databricks internal API `entry_point`",suggest remove following part code try except exception ray cluster spark job background job need manually call notebook notebook kill ray head node process ray worker exit detect ray head node spark job exit get rid calling internal,issue,negative,neutral,neutral,neutral,neutral,neutral
1784029556,"> wheels link is not support for some time.
> 
> will leave it for @can-anyscale to fix / clean up.

I think wheel link is still working? But rather the branch/commit hash is not working. ",link support time leave fix clean think wheel link still working rather hash working,issue,positive,positive,positive,positive,positive,positive
1783937617,"As an orthogonal idea @thomasdesr what do you think about simply redacting JWT tokens from `runtime_env` when serializing in string or json form? 
Looks like I am doing everything just to avoid my JWT token in `env_var` being exposed to ray dashboard and logs. I am fine with keeping the token in memory. Encryption mechanisms add more complexity and printing/showing a JWT token somewhere is not really a feature. So I think just not showing those tokens will be a good default behavior for everyone and perfectly solves my problem.

Following is an example of how this might look like,
```python
import json
import re

def redact_jwt_tokens(payload):
    # Regular expression pattern to match JWT tokens
    jwt_pattern = r'[\w-]+\.[\w-]+\.[\w-]+'
    # Replace JWT tokens with a redacted string (e.g., ""[REDACTED]"")
    jwt_matches = re.findall(jwt_pattern, payload)
    redacted_payload = re.sub(jwt_pattern, '[REDACTED]', payload)

    return redacted_payload, jwt_matches

# Sample JSON payload containing a JWT
json_payload = '{""user"": ""john.doe"", ""token"": ""eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.5Tc5MnBsB1aIzZqZVu_UjNtVcGc7Y9AlteG3eJuXTOE""}'

# Detect and redact JWT tokens
redacted_payload, jwt_tokens = redact_jwt_tokens(json_payload)

# Print the redacted payload and extracted JWT tokens
print(f""Original payload: {json_payload}"")
print(f""\nRedacted Payload: {redacted_payload}"")
print(f""\nExtracted JWT Tokens: {jwt_tokens}"")
```

If the regex is seems too wide and can cause unintended values to be redacted, we could add a feature flag to disable this behavior when needed.",orthogonal idea think simply string form like everything avoid token exposed ray dashboard fine keeping token memory encryption add complexity token somewhere really feature think showing good default behavior everyone perfectly problem following example might look like python import import regular expression pattern match replace string return sample user token detect redact print extracted print original print print wide cause unintended could add feature flag disable behavior,issue,positive,positive,positive,positive,positive,positive
1783871607,Curious if my edge case may be from https://github.com/ray-project/ray/pull/40210 ??,curious edge case may,issue,negative,negative,neutral,neutral,negative,negative
1783801249,"I thought this problem led to the ""pthread_create resource temporarily unavailable..."" error when there are too many parallel tasks, say 32 tasks. The logging part in Ray itself and the application code (in my case, some OpenMP code) suffer from it.

When the task is running, every ray::xxx process gets a 100+ nTH count.",thought problem led resource temporarily unavailable error many parallel say logging part ray application code case code suffer task running every ray process nth count,issue,negative,positive,positive,positive,positive,positive
1783766010,"```
2023-10-28 01:50:44.876886: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-28 01:50:45.785911: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
2023-10-28 01:50:45.786080: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
2023-10-28 01:50:45.786095: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
comet_ml is installed but `COMET_API_KEY` is not set.
--------------------------------------------------------------------------
                 Aim collects anonymous usage analytics.                 
                        Read how to opt-out here:                         
    https://aimstack.readthedocs.io/en/latest/community/telemetry.html    
--------------------------------------------------------------------------
2023-10-28 01:50:49,176 INFO worker.py:1458 -- Connecting to existing Ray cluster at address: 10.0.0.117:6379...
2023-10-28 01:50:49,191 INFO worker.py:1633 -- Connected to Ray cluster. View the dashboard at 10.0.0.117:8265 
2023-10-28 01:50:49,277 INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949

View detailed results here: /home/ray/ray_results/TorchTrainer_2023-10-28_01-50-49
To visualize your results with TensorBoard, run: `tensorboard --logdir /home/ray/ray_results/TorchTrainer_2023-10-28_01-50-49`
2023-10-28 01:51:13,998 ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_179ad_00000
Traceback (most recent call last):
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py"", line 110, in resolve_future
    result = ray.get(future)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/auto_init_hook.py"", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/worker.py"", line 2549, in get
    raise value
ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, ray::_Inner.__init__() (pid=600, ip=100.114.112.156, actor_id=5086820ed9f9668a13d6a31503000000, repr=TorchTrainer)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/trainable.py"", line 185, in __init__
    self.setup(copy.deepcopy(self.config))
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/util.py"", line 304, in setup
    setup_kwargs[k] = parameter_registry.get(prefix + k)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/registry.py"", line 301, in get
    return ray.get(self.references[k])
ray.exceptions.OwnerDiedError: Failed to retrieve object 00ffffffffffffffffffffffffffffffffffffff0300000001e1f505. To see information about where this ObjectRef was created in Python, set the environment variable RAY_record_ref_creation_sites=1 during `ray start` and `ray.init()`.

The object's owner has exited. This is the Python worker that first created the ObjectRef via `.remote()` or `ray.put()`. Check cluster logs (`/tmp/ray/session_latest/logs/*03000000ffffffffffffffffffffffffffffffffffffffffffffffff*` at IP address 10.0.0.117) for more information about the Python worker failure.

Training errored after 0 iterations at 2023-10-28 01:51:14. Total running time: 24s
Error file: /home/ray/ray_results/TorchTrainer_2023-10-28_01-50-49/TorchTrainer_179ad_00000_0_2023-10-28_01-50-49/error.txt

2023-10-28 01:51:14,004 ERROR tune.py:1139 -- Trials did not complete: [TorchTrainer_179ad_00000]
2023-10-28 01:51:14,007 WARNING experiment_analysis.py:205 -- Failed to fetch metrics for 1 trial(s):
- TorchTrainer_179ad_00000: FileNotFoundError('Could not fetch metrics for TorchTrainer_179ad_00000: both result.json and progress.csv were not found at /home/ray/ray_results/TorchTrainer_2023-10-28_01-50-49/TorchTrainer_179ad_00000_0_2023-10-28_01-50-49')
(TrainTrainable pid=600, ip=10.0.0.196) Local object store memory usage:
(TrainTrainable pid=600, ip=10.0.0.196) 
(TrainTrainable pid=600, ip=10.0.0.196) (global lru) capacity: 18768886579
(TrainTrainable pid=600, ip=10.0.0.196) (global lru) used: 0%
(TrainTrainable pid=600, ip=10.0.0.196) (global lru) num objects: 0
(TrainTrainable pid=600, ip=10.0.0.196) (global lru) num evictions: 0
(TrainTrainable pid=600, ip=10.0.0.196) (global lru) bytes evicted: 0
(TrainTrainable pid=600, ip=10.0.0.196) 
(TrainTrainable pid=600, ip=10.0.0.196) Exception raised in creation task: The actor died because of an error raised in its creation task, ray::_Inner.__init__() (pid=600, ip=100.114.112.156, actor_id=5086820ed9f9668a13d6a31503000000, repr=TorchTrainer)
(TrainTrainable pid=600, ip=10.0.0.196)   File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/trainable.py"", line 185, in __init__
(TrainTrainable pid=600, ip=10.0.0.196)     self.setup(copy.deepcopy(self.config))
(TrainTrainable pid=600, ip=10.0.0.196)   File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/util.py"", line 304, in setup
(TrainTrainable pid=600, ip=10.0.0.196)     setup_kwargs[k] = parameter_registry.get(prefix + k)
(TrainTrainable pid=600, ip=10.0.0.196)   File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/registry.py"", line 301, in get
(TrainTrainable pid=600, ip=10.0.0.196)     return ray.get(self.references[k])
TuneError: Failure # 1 (occurred at 2023-10-28_01-51-14)
The actor died because of an error raised in its creation task, [36mray::_Inner.__init__()[39m (pid=600, ip=100.114.112.156, actor_id=5086820ed9f9668a13d6a31503000000, repr=TorchTrainer)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/trainable.py"", line 185, in __init__
    self.setup(copy.deepcopy(self.config))
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/util.py"", line 304, in setup
    setup_kwargs[k] = parameter_registry.get(prefix + k)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/registry.py"", line 301, in get
    return ray.get(self.references[k])
ray.exceptions.OwnerDiedError: Failed to retrieve object 00ffffffffffffffffffffffffffffffffffffff0300000001e1f505. To see information about where this ObjectRef was created in Python, set the 
environment variable RAY_record_ref_creation_sites=1 during `ray start` and `ray.init()`.

The object's owner has exited. This is the Python worker that first created the ObjectRef via `.remote()` or `ray.put()`. Check cluster logs 
(`/tmp/ray/session_latest/logs/*03000000ffffffffffffffffffffffffffffffffffffffffffffffff*` at IP address 10.0.0.117) for more information about the Python worker failure.


The above exception was the direct cause of the following exception:

╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮
│ /home/ray/hf.py:81 in <module>                                                                   │
│                                                                                                  │
│   78 ray_trainer = TorchTrainer(                                                                 │
│   79 │   train_func, scaling_config=ScalingConfig(num_workers=4, use_gpu=True)                   │
│   80 )                                                                                           │
│ ❱ 81 ray_trainer.fit()                                                                           │
│                                                                                                  │
│ /home/ray/anaconda3/lib/python3.9/site-packages/ray/train/base_trainer.py:668 in fit             │
│                                                                                                  │
│   665 │   │   if result.error:                                                                   │
│   666 │   │   │   # Raise trainable errors to the user with a message to restore                 │
│   667 │   │   │   # or configure `FailureConfig` in a new run.                                   │
│ ❱ 668 │   │   │   raise TrainingFailedError(                                                     │
│   669 │   │   │   │   ""\n"".join([restore_msg, TrainingFailedError._FAILURE_CONFIG_MSG])          │
│   670 │   │   │   ) from result.error                                                            │
│   671 │   │   return result                                                                      │
╰──────────────────────────────────────────────────────────────────────────────────────────────────╯
TrainingFailedError: The Ray Train run failed. Please inspect the previous error messages for a cause. After fixing the issue (assuming that the error is not caused by your own application 
logic, but rather an error such as OOM), you can restart the run from scratch or continue this run.
To continue this run, you can use: `trainer = TorchTrainer.restore(""/home/ray/ray_results/TorchTrainer_2023-10-28_01-50-49"")`.
To start a new run that will retry on training failures, set `train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the Trainer's `run_config` with `max_failures > 0`, or 
`max_failures = -1` for unlimited retries.
(TrainTrainable pid=600, ip=10.0.0.196) ray.exceptions.OwnerDiedError: Failed to retrieve object 00ffffffffffffffffffffffffffffffffffffff0300000001e1f505. To see information about where this ObjectRef was created in Python, set the environment variable RAY_record_ref_creation_sites=1 during `ray start` and `ray.init()`.
(TrainTrainable pid=600, ip=10.0.0.196) 
(TrainTrainable pid=600, ip=10.0.0.196) The object's owner has exited. This is the Python worker that first created the ObjectRef via `.remote()` or `ray.put()`. Check cluster logs (`/tmp/ray/session_latest/logs/*03000000ffffffffffffffffffffffffffffffffffffffffffffffff*` at IP address 10.0.0.117) for more information about the Python worker failure.
```

Is there a way that we can set for ray to use a specific network interface as well? not sure what happened here.",binary deep neural network library use following enable rebuild appropriate compiler could load dynamic library open object file file directory could load dynamic library open object file file directory warning would like use please make sure missing properly set aim anonymous usage analytics read ray cluster address connected ray cluster view dashboard output use new output engine verbosity disable new output use legacy output engine set environment variable information please see view detailed visualize run error trial task trial recent call last file line result future file line return file line wrapper return file line get raise value actor error raised creation task ray file line file line setup prefix file line get return retrieve object see information python set environment variable ray start object owner python worker first via check cluster address information python worker failure training total running time error file error complete warning fetch metric trial fetch metric found local object store memory usage global capacity global used global global global exception raised creation task actor error raised creation task ray file line file line setup prefix file line get return failure actor error raised creation task file line file line setup prefix file line get return retrieve object see information python set environment variable ray start object owner python worker first via check cluster address information python worker failure exception direct cause following exception recent call last module fit raise trainable user message restore configure new run raise return result ray train run please inspect previous error cause fixing issue assuming error application logic rather error restart run scratch continue run continue run use trainer start new run retry training set trainer unlimited retrieve object see information python set environment variable ray start object owner python worker first via check cluster address information python worker failure way set ray use specific network interface well sure,issue,negative,positive,neutral,neutral,positive,positive
1783731056,"ip link show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: ens3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000
    link/ether fa:16:3e:4b:d8:29 brd ff:ff:ff:ff:ff:ff
3: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default 
    link/ether 02:42:bd:78:6c:54 brd ff:ff:ff:ff:ff:ff
17: vethb29f745@if16: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default 
    link/ether 52:55:92:03:b3:07 brd ff:ff:ff:ff:ff:ff link-netnsid 3
19: vethd6c7a39@if18: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default 
    link/ether a2:f2:5c:2e:64:f1 brd ff:ff:ff:ff:ff:ff link-netnsid 5
32: netmaker: <POINTOPOINT,NOARP,UP,LOWER_UP> mtu 1420 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/none 

I used 
import ray
ray.init(runtime_env={{'env_vars': {'NCCL_SOCKET_IFNAME': 'ens3'}}}` and it works.

Also what if I had another VM with following config:
ip link show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc fq_codel state UP mode DEFAULT group default qlen 1000
    link/ether fa:16:3e:f0:7d:ae brd ff:ff:ff:ff:ff:ff
    altname enp0s3
    altname ens3
3: br-996f4fc0cad0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default 
    link/ether 02:42:c9:f2:61:e4 brd ff:ff:ff:ff:ff:ff
4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default 
    link/ether 02:42:28:f4:0d:44 brd ff:ff:ff:ff:ff:ff

1. how would i set different nodes to use different network interfaces for NCCL?
2. how would I know as a user which NCCL_SOCKET_IFNAME to use if the infra is abstracted?",link show lo state unknown mode default group default en broadcast state mode default group default fa docker broadcast state mode default group default broadcast master docker state mode default group default broadcast master docker state mode default group default netmaker state unknown mode default group default used import ray work also another following link show lo state unknown mode default group default broadcast state mode default group default fa ae en broadcast state mode default group default docker broadcast state mode default group default would set different use different network would know user use infra abstracted,issue,negative,negative,neutral,neutral,negative,negative
1783729069,"I have a question here , that if I am clustering different machines which are not from the same provider or have different config, then how could i know which interface works for each of the nodes?
I wouldn't know the NCCL_SOCKET_IFNAME which would work by default here. Can you suggest how to find that ?",question clustering different provider different could know interface work would know would work default suggest find,issue,negative,neutral,neutral,neutral,neutral,neutral
1783727570,"why does ens3 fail here, when it is implicitly inferred by NCCL? When I used ens3 explicitly as suggested by you - it seemed to work.",en fail implicitly used en explicitly work,issue,negative,negative,negative,negative,negative,negative
1783727175,"Here are the logs after adding NCCL_DEBUG to INFO.
```
(base) root@smiral-0:~# python3 hf.py 
2023-10-27 23:58:19.762740: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-27 23:58:20.600691: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
2023-10-27 23:58:20.600784: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
2023-10-27 23:58:20.600794: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
comet_ml is installed but `COMET_API_KEY` is not set.
--------------------------------------------------------------------------
                 Aim collects anonymous usage analytics.                 
                        Read how to opt-out here:                         
    https://aimstack.readthedocs.io/en/latest/community/telemetry.html    
--------------------------------------------------------------------------
2023-10-27 23:58:25,053 INFO worker.py:1458 -- Connecting to existing Ray cluster at address: 10.0.0.30:6379...
2023-10-27 23:58:25,066 INFO worker.py:1633 -- Connected to Ray cluster. View the dashboard at 10.0.0.30:8265 
2023-10-27 23:58:25,129 INFO tune.py:228 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Trainer(...)`.
2023-10-27 23:58:25,132 INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949

View detailed results here: /home/ray/ray_results/TorchTrainer_2023-10-27_23-58-25
To visualize your results with TensorBoard, run: `tensorboard --logdir /home/ray/ray_results/TorchTrainer_2023-10-27_23-58-25`
(TrainTrainable pid=249, ip=10.0.0.19) comet_ml is installed but `COMET_API_KEY` is not set.
(TrainTrainable pid=249, ip=10.0.0.19) 2023-10-27 23:58:29.393245: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
(TrainTrainable pid=249, ip=10.0.0.19) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
(TrainTrainable pid=249, ip=10.0.0.19) 2023-10-27 23:58:30.300658: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
(TrainTrainable pid=249, ip=10.0.0.19) 2023-10-27 23:58:30.300742: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
(TrainTrainable pid=249, ip=10.0.0.19) 2023-10-27 23:58:30.300749: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
(TrainTrainable pid=249, ip=10.0.0.19) --------------------------------------------------------------------------
(TrainTrainable pid=249, ip=10.0.0.19)                  Aim collects anonymous usage analytics.                 
(TrainTrainable pid=249, ip=10.0.0.19)                         Read how to opt-out here:                         
(TrainTrainable pid=249, ip=10.0.0.19)     https://aimstack.readthedocs.io/en/latest/community/telemetry.html    
(TrainTrainable pid=249, ip=10.0.0.19) --------------------------------------------------------------------------

Training started without custom configuration.
(TorchTrainer pid=249, ip=10.0.0.19) Starting distributed worker processes: ['301 (10.0.0.19)', '192 (10.0.0.78)']
(RayTrainWorker pid=301, ip=10.0.0.19) Setting up process group for: env:// [rank=0, world_size=2]
(RayTrainWorker pid=301, ip=10.0.0.19) comet_ml is installed but `COMET_API_KEY` is not set.
(RayTrainWorker pid=301, ip=10.0.0.19) 2023-10-27 23:58:37.036058: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
(RayTrainWorker pid=301, ip=10.0.0.19) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
(RayTrainWorker pid=301, ip=10.0.0.19) 2023-10-27 23:58:38.061058: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
(RayTrainWorker pid=301, ip=10.0.0.19) 2023-10-27 23:58:38.061139: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
(RayTrainWorker pid=301, ip=10.0.0.19) 2023-10-27 23:58:38.061146: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
(RayTrainWorker pid=192, ip=10.0.0.78) -------------------------------------------------------------------------- [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)
(RayTrainWorker pid=192, ip=10.0.0.78)                  Aim collects anonymous usage analytics.                 
(RayTrainWorker pid=192, ip=10.0.0.78)                         Read how to opt-out here:                         
(RayTrainWorker pid=192, ip=10.0.0.78)     https://aimstack.readthedocs.io/en/latest/community/telemetry.html    
(RayTrainWorker pid=192, ip=10.0.0.78) comet_ml is installed but `COMET_API_KEY` is not set.
Downloading builder script: 4.39kB [00:00, 11.9MB/s]                   
(RayTrainWorker pid=192, ip=10.0.0.78) 2023-10-27 23:58:37.081356: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
(RayTrainWorker pid=192, ip=10.0.0.78) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
(RayTrainWorker pid=301, ip=10.0.0.19) Downloading and preparing dataset yelp_review_full/yelp_review_full (download: 187.06 MiB, generated: 496.94 MiB, post-processed: Unknown size, total: 684.00 MiB) to /home/ray/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/13c31a618ba62568ec8572a222a283dfc29a6517776a3ac5945fb508877dde43...
Downloading metadata: 2.13kB [00:00, 6.15MB/s]                   
Downloading data:   0%|          | 0.00/196M [00:00<?, ?B/s]
(RayTrainWorker pid=192, ip=10.0.0.78) 2023-10-27 23:58:38.176368: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 [repeated 2x across cluster]
(RayTrainWorker pid=192, ip=10.0.0.78) 2023-10-27 23:58:38.176376: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Downloading data:   0%|          | 226k/196M [00:00<01:26, 2.26MB/s]
Downloading data:  92%|█████████▏| 181M/196M [00:03<00:00, 62.2MB/s]
Downloading data: 100%|██████████| 196M/196M [00:03<00:00, 50.7MB/s]
Downloading builder script: 4.39kB [00:00, 9.37MB/s]                   
Generating train split:   0%|          | 0/650000 [00:00<?, ? examples/s]
Downloading metadata: 2.13kB [00:00, 15.3MB/s]                   
Downloading data:   0%|          | 0.00/196M [00:00<?, ?B/s]
Downloading data:  90%|████████▉ | 176M/196M [00:03<00:00, 59.8MB/s] [repeated 52x across cluster]
Generating train split:   1%|          | 3263/650000 [00:00<00:19, 32627.93 examples/s]
Downloading data: 100%|██████████| 196M/196M [00:03<00:00, 50.2MB/s] [repeated 3x across cluster]
Generating train split:   0%|          | 0/650000 [00:00<?, ? examples/s]
Generating train split:  25%|██▌       | 164261/650000 [00:05<00:15, 31205.57 examples/s] [repeated 96x across cluster]
Generating train split:  49%|████▉     | 317500/650000 [00:10<00:10, 32682.20 examples/s] [repeated 91x across cluster]
Generating train split:  66%|██████▌   | 427209/650000 [00:13<00:07, 29431.57 examples/s]
Generating train split:  71%|███████   | 458535/650000 [00:15<00:06, 31793.58 examples/s] [repeated 88x across cluster]
Generating train split:  91%|█████████▏| 593923/650000 [00:19<00:01, 32115.99 examples/s]
Generating train split:  92%|█████████▏| 597255/650000 [00:19<00:01, 32463.23 examples/s]
Generating train split:  92%|█████████▏| 600516/650000 [00:19<00:01, 31005.58 examples/s]
Generating train split:  93%|█████████▎| 603957/650000 [00:19<00:01, 31976.24 examples/s]
Generating train split:  93%|█████████▎| 607420/650000 [00:19<00:01, 32743.04 examples/s]
Generating train split:  91%|█████████ | 591739/650000 [00:19<00:01, 31008.19 examples/s] [repeated 78x across cluster]
Generating test split:   0%|          | 0/50000 [00:00<?, ? examples/s]                  
Generating test split:   7%|▋         | 3484/50000 [00:00<00:01, 34823.93 examples/s]
Generating test split:  14%|█▍        | 6967/50000 [00:00<00:01, 34325.75 examples/s]
Generating test split:  21%|██        | 10401/50000 [00:00<00:01, 31644.46 examples/s]
Generating test split:  27%|██▋       | 13717/50000 [00:00<00:01, 32207.62 examples/s]
Generating test split:  34%|███▍      | 17075/50000 [00:00<00:01, 32684.78 examples/s]
Generating test split:  94%|█████████▍| 46891/50000 [00:01<00:00, 32385.22 examples/s]
(RayTrainWorker pid=301, ip=10.0.0.19) Dataset yelp_review_full downloaded and prepared to /home/ray/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/13c31a618ba62568ec8572a222a283dfc29a6517776a3ac5945fb508877dde43. Subsequent calls will reuse this data.
(RayTrainWorker pid=192, ip=10.0.0.78) Downloading and preparing dataset yelp_review_full/yelp_review_full (download: 187.06 MiB, generated: 496.94 MiB, post-processed: Unknown size, total: 684.00 MiB) to /home/ray/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/13c31a618ba62568ec8572a222a283dfc29a6517776a3ac5945fb508877dde43...
100%|██████████| 2/2 [00:00<00:00, 602.20it/s]                                        
Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 113kB/s]
Downloading: 100%|██████████| 570/570 [00:00<00:00, 4.35MB/s]
Downloading: 100%|██████████| 208k/208k [00:00<00:00, 14.8MB/s]
Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]
(RayTrainWorker pid=301, ip=10.0.0.19) [23:59:14] WARNING  Parameter 'function'=<function            fingerprint.py:328
(RayTrainWorker pid=301, ip=10.0.0.19)                     train_func.<locals>.tokenize_function at                    
(RayTrainWorker pid=301, ip=10.0.0.19)                     0x7f0e012c7940> of the transform                            
(RayTrainWorker pid=301, ip=10.0.0.19)                     datasets.arrow_dataset.Dataset._map_singl                   
(RayTrainWorker pid=301, ip=10.0.0.19)                     e couldn't be hashed properly, a random                     
(RayTrainWorker pid=301, ip=10.0.0.19)                     hash was used instead. Make sure your                       
(RayTrainWorker pid=301, ip=10.0.0.19)                     transforms and parameters are                               
(RayTrainWorker pid=301, ip=10.0.0.19)                     serializable with pickle or dill for the                    
(RayTrainWorker pid=301, ip=10.0.0.19)                     dataset fingerprinting and caching to                       
(RayTrainWorker pid=301, ip=10.0.0.19)                     work. If you reuse this transform, the                      
(RayTrainWorker pid=301, ip=10.0.0.19)                     caching mechanism will consider it to be                    
(RayTrainWorker pid=301, ip=10.0.0.19)                     different from the previous calls and                       
(RayTrainWorker pid=301, ip=10.0.0.19)                     recompute everything. This warning is                       
(RayTrainWorker pid=301, ip=10.0.0.19)                     only showed once. Subsequent hashing                        
(RayTrainWorker pid=301, ip=10.0.0.19)                     failures won't be showed.                                   
  0%|          | 0/1 [00:00<?, ?ba/s]) 
Downloading: 100%|██████████| 208k/208k [00:00<00:00, 13.6MB/s]
100%|██████████| 1/1 [00:00<00:00,  2.71ba/s]
100%|██████████| 1/1 [00:00<00:00,  4.06ba/s]
Generating train split: 100%|█████████▉| 647512/650000 [00:21<00:00, 30351.26 examples/s] [repeated 29x across cluster]
Downloading:   0%|          | 728k/416M [00:00<00:58, 7.45MB/s]
Downloading:   2%|▏         | 6.50M/416M [00:00<00:11, 38.8MB/s]
Downloading:   3%|▎         | 14.5M/416M [00:00<00:07, 59.2MB/s]
Downloading:   6%|▌         | 23.0M/416M [00:00<00:05, 71.0MB/s]
Downloading:   8%|▊         | 31.4M/416M [00:00<00:05, 77.3MB/s]
Downloading:  10%|▉         | 39.9M/416M [00:00<00:04, 81.3MB/s]
Generating test split:   0%|          | 0/50000 [00:00<?, ? examples/s]                  
Generating test split:  88%|████████▊ | 43773/50000 [00:01<00:00, 31757.86 examples/s] [repeated 21x across cluster]
Generating test split:  94%|█████████▍| 47166/50000 [00:01<00:00, 32384.76 examples/s]
100%|██████████| 2/2 [00:00<00:00, 494.79it/s]                                        
Downloading: 100%|██████████| 426k/426k [00:00<00:00, 13.8MB/s] [repeated 4x across cluster]
Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s] [repeated 2x across cluster]
  0%|          | 0/1 [00:00<?, ?ba/s] [repeated 3x across cluster]
100%|██████████| 1/1 [00:00<00:00,  4.03ba/s] [repeated 2x across cluster]
Downloading:  92%|█████████▏| 384M/416M [00:04<00:00, 86.6MB/s]
Downloading:  94%|█████████▍| 393M/416M [00:04<00:00, 87.0MB/s]
Downloading:  96%|█████████▋| 401M/416M [00:04<00:00, 86.7MB/s]
Downloading: 100%|██████████| 416M/416M [00:05<00:00, 85.3MB/s]
Downloading:  23%|██▎       | 97.1M/416M [00:04<00:11, 28.4MB/s] [repeated 63x across cluster]
(RayTrainWorker pid=301, ip=10.0.0.19) Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']
(RayTrainWorker pid=301, ip=10.0.0.19) - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
(RayTrainWorker pid=301, ip=10.0.0.19) - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
(RayTrainWorker pid=301, ip=10.0.0.19) Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
(RayTrainWorker pid=301, ip=10.0.0.19) You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Downloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 12.8MB/s]
(RayTrainWorker pid=301, ip=10.0.0.19) The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
(RayTrainWorker pid=301, ip=10.0.0.19) /home/ray/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
(RayTrainWorker pid=301, ip=10.0.0.19)   warnings.warn(
(RayTrainWorker pid=301, ip=10.0.0.19) smiral-2:301:351 [0] NCCL INFO Bootstrap : Using ens3:10.0.0.19<0>
(RayTrainWorker pid=301, ip=10.0.0.19) smiral-2:301:351 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
(RayTrainWorker pid=301, ip=10.0.0.19) smiral-2:301:351 [0] NCCL INFO cudaDriverVersion 12030
(RayTrainWorker pid=301, ip=10.0.0.19) NCCL version 2.14.3+cuda11.8
(RayTrainWorker pid=192, ip=10.0.0.78) Dataset yelp_review_full downloaded and prepared to /home/ray/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/13c31a618ba62568ec8572a222a283dfc29a6517776a3ac5945fb508877dde43. Subsequent calls will reuse this data.
(RayTrainWorker pid=192, ip=10.0.0.78) [23:59:14] WARNING  Parameter 'function'=<function            fingerprint.py:328
(RayTrainWorker pid=192, ip=10.0.0.78)                     train_func.<locals>.tokenize_function at                    
(RayTrainWorker pid=192, ip=10.0.0.78)                     0x7f027d6c0040> of the transform                            
(RayTrainWorker pid=192, ip=10.0.0.78)                     datasets.arrow_dataset.Dataset._map_singl                   
(RayTrainWorker pid=192, ip=10.0.0.78)                     e couldn't be hashed properly, a random                     
(RayTrainWorker pid=192, ip=10.0.0.78)                     hash was used instead. Make sure your                       
(RayTrainWorker pid=192, ip=10.0.0.78)                     transforms and parameters are                               
(RayTrainWorker pid=192, ip=10.0.0.78)                     serializable with pickle or dill for the                    
(RayTrainWorker pid=192, ip=10.0.0.78)                     dataset fingerprinting and caching to                       
(RayTrainWorker pid=192, ip=10.0.0.78)                     work. If you reuse this transform, the                      
(RayTrainWorker pid=192, ip=10.0.0.78)                     caching mechanism will consider it to be                    
(RayTrainWorker pid=192, ip=10.0.0.78)                     different from the previous calls and                       
(RayTrainWorker pid=192, ip=10.0.0.78)                     recompute everything. This warning is                       
(RayTrainWorker pid=192, ip=10.0.0.78)                     only showed once. Subsequent hashing                        
(RayTrainWorker pid=192, ip=10.0.0.78)                     failures won't be showed.                                   
Downloading:  56%|█████▌    | 231M/416M [00:09<00:07, 25.9MB/s] [repeated 28x across cluster]
Downloading:  90%|█████████ | 374M/416M [00:14<00:02, 16.4MB/s] [repeated 29x across cluster]
Downloading:  92%|█████████▏| 383M/416M [00:15<00:02, 13.5MB/s]
Downloading:  93%|█████████▎| 385M/416M [00:15<00:02, 13.2MB/s]
Downloading:  94%|█████████▍| 391M/416M [00:16<00:01, 16.0MB/s]
Downloading:  94%|█████████▍| 392M/416M [00:16<00:01, 13.5MB/s]
Downloading:  96%|█████████▌| 398M/416M [00:16<00:01, 13.3MB/s]
Downloading:  96%|█████████▋| 400M/416M [00:16<00:01, 13.5MB/s]
Downloading:  98%|█████████▊| 406M/416M [00:17<00:00, 19.6MB/s]
Downloading:  98%|█████████▊| 409M/416M [00:17<00:00, 18.0MB/s]
Downloading: 100%|██████████| 416M/416M [00:17<00:00, 25.0MB/s]
(RayTrainWorker pid=192, ip=10.0.0.78) Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
(RayTrainWorker pid=192, ip=10.0.0.78) - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
(RayTrainWorker pid=192, ip=10.0.0.78) - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
(RayTrainWorker pid=192, ip=10.0.0.78) Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
(RayTrainWorker pid=192, ip=10.0.0.78) You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Downloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 12.0MB/s]
(RayTrainWorker pid=192, ip=10.0.0.78) The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
(RayTrainWorker pid=192, ip=10.0.0.78) /home/ray/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
(RayTrainWorker pid=192, ip=10.0.0.78)   warnings.warn(
2023-10-27 23:59:36,277 ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_63c79_00000
Traceback (most recent call last):
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py"", line 110, in resolve_future
    result = ray.get(future)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/auto_init_hook.py"", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/worker.py"", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(DistBackendError): ray::_Inner.train() (pid=249, ip=10.0.0.19, actor_id=84a1163f1b5ae7a10e32c1e201000000, repr=TorchTrainer)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/trainable.py"", line 400, in train
    raise skipped from exception_cause(skipped)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/_internal/utils.py"", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(DistBackendError): ray::_RayTrainWorker__execute.get_next() (pid=301, ip=10.0.0.19, actor_id=a7efbd6c8cc674d17445260101000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7f0f66c040a0>)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/_internal/worker_group.py"", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/_internal/utils.py"", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File ""/home/ray/hf.py"", line 67, in train_func
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/transformers/trainer.py"", line 1317, in train
    return inner_training_loop(
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/transformers/trainer.py"", line 1402, in _inner_training_loop
    model = self._wrap_model(self.model_wrapped)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/transformers/trainer.py"", line 1230, in _wrap_model
    model = nn.parallel.DistributedDataParallel(
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/distributed.py"", line 674, in __init__
    _verify_param_shape_across_processes(self.process_group, parameters)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/torch/distributed/utils.py"", line 118, in _verify_param_shape_across_processes
    return dist._verify_params_across_processes(process_group, tensors, logger)
torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1275, internal error, NCCL version 2.14.3
ncclInternalError: Internal check failed.
Last error:
Proxy Call to rank 0 failed (Connect)

Training errored after 0 iterations at 2023-10-27 23:59:36. Total running time: 1min 10s
Error file: /home/ray/ray_results/TorchTrainer_2023-10-27_23-58-25/TorchTrainer_63c79_00000_0_2023-10-27_23-58-25/error.txt

2023-10-27 23:59:36,290 ERROR tune.py:1139 -- Trials did not complete: [TorchTrainer_63c79_00000]
RayTaskError(DistBackendError): [36mray::_Inner.train()[39m (pid=249, ip=10.0.0.19, actor_id=84a1163f1b5ae7a10e32c1e201000000, repr=TorchTrainer)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/trainable.py"", line 400, in train
    raise skipped from exception_cause(skipped)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/_internal/utils.py"", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(DistBackendError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=301, ip=10.0.0.19, actor_id=a7efbd6c8cc674d17445260101000000, 
repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7f0f66c040a0>)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/_internal/worker_group.py"", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/_internal/utils.py"", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File ""/home/ray/hf.py"", line 67, in train_func
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/transformers/trainer.py"", line 1317, in train
    return inner_training_loop(
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/transformers/trainer.py"", line 1402, in _inner_training_loop
    model = self._wrap_model(self.model_wrapped)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/transformers/trainer.py"", line 1230, in _wrap_model
    model = nn.parallel.DistributedDataParallel(
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/distributed.py"", line 674, in __init__
    _verify_param_shape_across_processes(self.process_group, parameters)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/torch/distributed/utils.py"", line 118, in _verify_param_shape_across_processes
    return dist._verify_params_across_processes(process_group, tensors, logger)
torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1275, internal error, NCCL version 2.14.3
ncclInternalError: Internal check failed.
Last error:
Proxy Call to rank 0 failed (Connect)

The above exception was the direct cause of the following exception:

╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮
│ /home/ray/hf.py:74 in <module>                                                                   │
│                                                                                                  │
│   71 ray_trainer = TorchTrainer(                                                                 │
│   72 │   train_func, scaling_config=ScalingConfig(num_workers=2, use_gpu=True)                   │
│   73 )                                                                                           │
│ ❱ 74 ray_trainer.fit()                                                                           │
│                                                                                                  │
│ /home/ray/anaconda3/lib/python3.9/site-packages/ray/train/base_trainer.py:668 in fit             │
│                                                                                                  │
│   665 │   │   if result.error:                                                                   │
│   666 │   │   │   # Raise trainable errors to the user with a message to restore                 │
│   667 │   │   │   # or configure `FailureConfig` in a new run.                                   │
│ ❱ 668 │   │   │   raise TrainingFailedError(                                                     │
│   669 │   │   │   │   ""\n"".join([restore_msg, TrainingFailedError._FAILURE_CONFIG_MSG])          │
│   670 │   │   │   ) from result.error                                                            │
│   671 │   │   return result                                                                      │
╰──────────────────────────────────────────────────────────────────────────────────────────────────╯
TrainingFailedError: The Ray Train run failed. Please inspect the previous error messages for a cause. After fixing the issue (assuming that the error is not caused by your own application 
logic, but rather an error such as OOM), you can restart the run from scratch or continue this run.
To continue this run, you can use: `trainer = TorchTrainer.restore(""/home/ray/ray_results/TorchTrainer_2023-10-27_23-58-25"")`.
To start a new run that will retry on training failures, set `train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the Trainer's `run_config` with `max_failures > 0`, or 
`max_failures = -1` for unlimited retries.
Downloading:  91%|█████████ | 377M/416M [00:15<00:02, 15.7MB/s]
```",base root python binary deep neural network library use following enable rebuild appropriate compiler could load dynamic library open object file file directory could load dynamic library open object file file directory warning would like use please make sure missing properly set aim anonymous usage analytics read ray cluster address connected ray cluster view dashboard ray automatically cluster usage custom ray call trainer output use new output engine verbosity disable new output use legacy output engine set environment variable information please see view detailed visualize run set binary deep neural network library use following enable rebuild appropriate compiler could load dynamic library open object file file directory could load dynamic library open object file file directory warning would like use please make sure missing properly aim anonymous usage analytics read training without custom configuration starting distributed worker setting process group set binary deep neural network library use following enable rebuild appropriate compiler could load dynamic library open object file file directory could load dynamic library open object file file directory warning would like use please make sure missing properly repeated across cluster ray default set disable log deduplication see aim anonymous usage analytics read set builder script binary deep neural network library use following enable rebuild appropriate compiler mib mib unknown size total mib data could load dynamic library open object file file directory repeated across cluster warning would like use please make sure missing properly data data data builder script generating train split data data repeated across cluster generating train split data repeated across cluster generating train split generating train split repeated across cluster generating train split repeated across cluster generating train split generating train split repeated across cluster generating train split generating train split generating train split generating train split generating train split generating train split repeated across cluster generating test split generating test split generating test split generating test split generating test split generating test split generating test split prepared subsequent reuse data mib mib unknown size total mib warning parameter function transform could properly random hash used instead make sure pickle dill fingerprinting work reuse transform mechanism consider different previous recompute everything warning subsequent wo generating train split repeated across cluster generating test split generating test split repeated across cluster generating test split repeated across cluster repeated across cluster repeated across cluster repeated across cluster repeated across cluster model used model trained another task another architecture model model model expect exactly identical model model model newly probably train model task able use inference builder script following training set corresponding argument text text safely ignore message implementation removed future version use implementation instead set disable warning bootstrap en found internal implementation version prepared subsequent reuse data warning parameter function transform could properly random hash used instead make sure pickle dill fingerprinting work reuse transform mechanism consider different previous recompute everything warning subsequent wo repeated across cluster repeated across cluster model used model trained another task another architecture model model model expect exactly identical model model model newly probably train model task able use inference builder script following training set corresponding argument text text safely ignore message implementation removed future version use implementation instead set disable warning error trial task trial recent call last file line result future file line return file line wrapper return file line get raise ray file line train raise file line ray object file line raise file line file line file line train return file line model file line model file line file line return logger error internal error version internal check last error proxy call rank connect training total running time min error file error complete file line train raise file line object file line raise file line file line file line train return file line model file line model file line file line return logger error internal error version internal check last error proxy call rank connect exception direct cause following exception recent call last module fit raise trainable user message restore configure new run raise return result ray train run please inspect previous error cause fixing issue assuming error application logic rather error restart run scratch continue run continue run use trainer start new run retry training set trainer unlimited,issue,negative,positive,neutral,neutral,positive,positive
1783664566,"@stephanie-wang I think this issue is also related to #39710.
Assume the datasource outputs N blocks. then the shuffle map tasks will output N^2 blocks. 
When we reduce the target blocks size, we increase N from 2000 to 8000. Then number of shuffle map outputs becomes 16x, making the average size smaller than the 100k. 

Increasing the target blocks size can fix it `ray.data.DataContext.get_current().target_max_block_size = 512 * 1024 * 1024`",think issue also related assume shuffle map output reduce target size increase number shuffle map becomes making average size smaller increasing target size fix,issue,negative,negative,neutral,neutral,negative,negative
1783658082,"Confirmed the driver RAM is because the returned blocks of the these map tasks are inlined. Average size of these blocks are 15k, which is less than the 100k limit. Reducing the limit can fix the issue `max_direct_call_object_size`. ",confirmed driver ram returned map average size le limit reducing limit fix issue,issue,negative,positive,positive,positive,positive,positive
1783654566,"cc @woshiyyya @justinvyu 

Would you mind taking a look when you have time? Thanks!",would mind taking look time thanks,issue,negative,positive,positive,positive,positive,positive
1783643643,everything is basically building from source with caching now,everything basically building source,issue,negative,neutral,neutral,neutral,neutral,neutral
1783643473,"wheels link is not support for some time.

will leave it for @can-anyscale to fix / clean up.",link support time leave fix clean,issue,positive,positive,positive,positive,positive,positive
1783642850,"I didn't notice any performance changes after manually reverting #40248. But it might be because I've been testing on single node.

I did find a ~10% perf regression in the same benchmark but for single node. Oddly it does not show up on the data dashboard, maybe there is something wrong with the test setup. Offending commit is #40541, which makes sense since it adds an (async) actor task submission to each batch submission

So i'm re-triggering the test right now on:
- [revert of #40541](https://buildkite.com/ray-project/release-tests-pr/builds/57323)
- [revert of the block slicing PR (#40248)](https://buildkite.com/ray-project/release-tests-pr/builds/57322#018b73a5-44bf-4e27-9dfa-7c66380c8eba)

And we'll see from these results... it's possible there were actually two perf regressions introduced at the same time.",notice performance manually might testing single node find regression single node oddly show data dashboard maybe something wrong test setup commit sense since actor task submission batch submission test right revert revert block slicing see possible actually two time,issue,negative,negative,neutral,neutral,negative,negative
1783635533,"I should probably raise this as a separate issue, but `_check_if_diag_gaussian` fails for `MultiActionDistribution`s even though the child distribution will end up being `DiagGaussian`s

I think `_check_if_diag_gaussian` could handle this by instantiating a throw-away class (not ideal) or it could be handled in `Distribution.get_partial_dist_cls` with class attributes somehow and a slight modification to the `assert` statements in `_check_if_diag_gaussian`.

At the time the partial distribution is constructed it is possible to know the types of the final distributions - perhaps make a class attribute or property `flat_distribution_types: List[Type[Distribution]]` for `DistributionPartial` and then check:

```python
assert issubclass(action_distribution_cls, TorchDiagGaussian) or (issubclass(action_distribution_cls, DistributionPartial) and all(issubclass(x, TorchDiagGaussian) for x in action_distribution_cls.flat_distribution_types))
```

Of course, I am sure you will come up with something more clever.",probably raise separate issue even though child distribution end think could handle class ideal could handled class somehow slight modification assert time partial distribution possible know final perhaps make class attribute property list type distribution check python assert course sure come something clever,issue,positive,positive,positive,positive,positive,positive
1783611435,"Got it, thanks for flagging this -- I'll re-open and rename the issue. Do you want to put up a fix for this?

Should be pretty small:
1. Remove that line
2. Improve the usage example so that it's wrapped with Ray Tune, uses checkpoint frequency, and shows how to separately checkpoint at the end of training

I can also add this to my backlog - let me know!",got thanks flagging rename issue want put fix pretty small remove line improve usage example wrapped ray tune frequency separately end training also add backlog let know,issue,positive,positive,neutral,neutral,positive,positive
1783610899,"I think it would be variance: 177 is on the low end, while 199 is on the high end for this metric: 
<img width=""777"" alt=""image"" src=""https://github.com/ray-project/ray/assets/11676094/186dfd4e-262d-4d51-a9ea-6aceba59403f"">

<img width=""752"" alt=""image"" src=""https://github.com/ray-project/ray/assets/11676094/69c294d8-80b0-47f7-afe1-a85307e5c0ae"">

",think would variance low end high end metric image image,issue,negative,positive,neutral,neutral,positive,positive
1783610120,"I think we can close this; re-reading through the Datasource documentation now it seems pretty clear that Datasource is the base class in which you can inherit and define any custom datasource, where, after you also inherit and write the read and write functions; becomes a Dataset that you can use the same ol' distributed capabilities of Ray Data to iterate/read/ingest from/to.",think close documentation pretty clear base class inherit define custom also inherit write read write becomes use distributed ray data,issue,positive,negative,negative,negative,negative,negative
1783595249,@matthewdeng has this gap been addressed as part of train ga in ray27?,gap part train ga ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1783594543,@bveeramani is this still an issue on latest ray?,still issue latest ray,issue,negative,positive,positive,positive,positive,positive
1783589791,@justinvyu have we addressed this as part of the checkpoint documentation revamp shipped with ray27?,part documentation revamp shipped ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1783589019,@matthewdeng have we addressed this as part of the new checkpoint implementation as part of ray27?,part new implementation part ray,issue,negative,positive,positive,positive,positive,positive
1783587181,"@matthewdeng have we fixed this with the new checkpoint implementation as part of ray27?
",fixed new implementation part ray,issue,negative,positive,positive,positive,positive,positive
1783570513,@woshiyyya @justinvyu this still relevant in latest implementation of checkpointing as part of AI Libraries GA?,still relevant latest implementation part ai ga,issue,negative,positive,positive,positive,positive,positive
1783565384,@xwjiang2010 - old - but can we close this given the context ticket is also closed?,old close given context ticket also closed,issue,negative,neutral,neutral,neutral,neutral,neutral
1783547786,"cc @zhe-thoughts I merged this a bit prematurely, please take a look and let me know if any follow-up action is required, thanks!",bit prematurely please take look let know action thanks,issue,positive,positive,positive,positive,positive,positive
1783528859,"The test was jailed since August, but it looks like there was a regression introduced since 2.7.1.",test since august like regression since,issue,negative,neutral,neutral,neutral,neutral,neutral
1783516945,Just tested those still existing. I think we need some kinda of default response in order to resolve those. Still think it's lower priority as of now. ,tested still think need default response order resolve still think lower priority,issue,negative,neutral,neutral,neutral,neutral,neutral
1783514234,"I have given some more thoughts on this, and here is a summary
1. From ray side this looks like a bigger project, viewed as supporting a secret store inside the ray cluster.
2. If we want to do a good job a protecting secrets we need to use encryption, not encoding.
3. The challenging part with encryption is the key management, and supporting a scheme that works for everyone is going to be involved work.
4. If I am going to use the encryption, I may not need to add `secret_env_vars`, I can just use the `env_vars` and pass encrypted values. That way the encryption and decryption can be handled outside of ray. The client (or a proxy service between the client and the ray cluster) needs to encrypt the secret vars and the ray job needs to be able to decrypt it. I agree this puts more work on the ray users, but the upside is that everyone can manage the encryption part according to what fits well with the rest of the infrastructure they have.

In my case, I already have a proxy service (api gateway) that sits between the ray cli/sdk and ray cluster (head service). This is a standard practice at my firm. Currently this already enforces authentication by requiring users to pass their auth token in the `--headers` input to the `ray job submit` call. This headers do not reach the ray job, which was the premise of my feature request, and I was hoping to add some plumbing to make this reach. In the first discussion with the ray team it looked like that headers is not intended to be used by the ray jobs so we should add a new runtime_env called `secret_env_var`. But now it sounds like I am coming to a full circle and going to rely on the headers, but without making any changes to ray code.

Here is my plan,
1. The proxy service will read the auth token from headers, encrypt it (using a public key) and inject it into the request at `runtime_env.env_vars.AUTH_TOKEN`. This request gets forwarded to the ray cluster (head service).
2. The ray job will call another service to decrypt the token. E.g. `token = ray_encryption_service.decrypt(os.environ['AUTH_TOKEN'])`. The job can then use this token to pull secrets from the secrets manager and it will have the same privileges as the owner of the auth token.
3. Since the ray infrastructure will never see the decrypted token there is no risk with printing in logs or showing it on the dashboard.
4. The value stored in environment variable on the ray nodes is encrypted as well.

NOTE: In this case requiring the user to pass the encrypted auth token in the env_var input will not work, if this ray cluster is shared between multiple users. This is because the encrypted value will show up on the ray dashboard in `runtime_env`, and another user can use that in their request to impersonate the first user. The encryption needs to happen in the proxy service to make sure the user has access to the raw auth token. And of course, we are assuming the plain text token is transmitted securely from the ray cli/sdk to the proxy service (e.g. using TLS).",given summary ray side like bigger project supporting secret store inside ray cluster want good job protecting need use encryption part encryption key management supporting scheme work everyone going involved work going use encryption may need add use pas way encryption handled outside ray client proxy service client ray cluster need encrypt secret ray job need able agree work ray upside everyone manage encryption part according well rest infrastructure case already proxy service gateway ray ray cluster head service standard practice firm currently already authentication pas token input ray job submit call reach ray job premise feature request add plumbing make reach first discussion ray team like intended used ray add new like coming full circle going rely without making ray code plan proxy service read token encrypt public key inject request request ray cluster head service ray job call another service token token job use token pull manager owner token since ray infrastructure never see token risk printing showing dashboard value environment variable ray well note case user pas token input work ray cluster multiple value show ray dashboard another user use request impersonate first user encryption need happen proxy service make sure user access raw token course assuming plain text token securely ray proxy service,issue,positive,positive,neutral,neutral,positive,positive
1783512394,"Also if you know which interface you want to use, you can propagate the value across your cluster by adding this (example) to the top of your script:

```python
import ray
ray.init(runtime_env={{'env_vars': {'NCCL_SOCKET_IFNAME': 'ens5'}}}`
```

There is some logic in Ray Train that sets this by [default](https://github.com/ray-project/ray/blob/eabd18efb84e27c0285a3e4fde31737135727963/python/ray/train/constants.py#L95-L96), but it may not match your setup. ",also know interface want use propagate value across cluster example top script python import ray logic ray train default may match setup,issue,positive,positive,positive,positive,positive,positive
1783510186,hi @smit-kiri / @Lorien2027 we fixed most of the memory leaks in 2.7. Could you try upgrading and let us know if the issues still persist?,hi fixed memory could try let u know still persist,issue,negative,positive,neutral,neutral,positive,positive
1783503194,"Can you try setting `NCCL_DEBUG` to `INFO` in your training script to see if any additional information is there?

```python
def train_func(config):
   import os
   os.environ[""NCCL_DEBUG""] = ""INFO""
   ...
```",try setting training script see additional information python import o,issue,negative,neutral,neutral,neutral,neutral,neutral
1783436784,"```
REGRESSION 123.41%: dashboard_p50_latency_ms (LATENCY) regresses from 35.742 to 79.852 (123.41%) in 2.8.0/benchmarks/many_actors.json
REGRESSION 72.90%: dashboard_p95_latency_ms (LATENCY) regresses from 46.348 to 80.137 (72.90%) in 2.8.0/benchmarks/many_nodes.json
REGRESSION 54.82%: stage_3_creation_time (LATENCY) regresses from 2.1919972896575928 to 3.393756628036499 (54.82%) in 2.8.0/stress_tests/stress_test_many_tasks.json
REGRESSION 20.13%: dashboard_p99_latency_ms (LATENCY) regresses from 145.645 to 174.969 (20.13%) in 2.8.0/benchmarks/many_nodes.json
REGRESSION 11.04%: 1000000_queued_time (LATENCY) regresses from 177.93132558000002 to 197.57540863000003 (11.04%) in 2.8.0/scalability/single_node.json
REGRESSION 11.01%: dashboard_p50_latency_ms (LATENCY) regresses from 3.687 to 4.093 (11.01%) in 2.8.0/benchmarks/many_nodes.json
REGRESSION 9.68%: client__1_1_actor_calls_concurrent (THROUGHPUT) regresses from 961.1926184467325 to 868.1661626750067 (9.68%) in 2.8.0/microbenchmark.json
REGRESSION 7.45%: n_n_actor_calls_async (THROUGHPUT) regresses from 29270.036133623737 to 27088.724228974872 (7.45%) in 2.8.0/microbenchmark.json
REGRESSION 7.14%: placement_group_create/removal (THROUGHPUT) regresses from 997.6322375478999 to 926.429178856758 (7.14%) in 2.8.0/microbenchmark.json
REGRESSION 5.88%: stage_4_spread (LATENCY) regresses from 0.7296295246039273 to 0.7725406967524532 (5.88%) in 2.8.0/stress_tests/stress_test_many_tasks.json
REGRESSION 5.67%: 1_n_async_actor_calls_async (THROUGHPUT) regresses from 8657.20480299259 to 8166.318822443878 (5.67%) in 2.8.0/microbenchmark.json
REGRESSION 5.25%: actors_per_second (THROUGHPUT) regresses from 738.330085638146 to 699.5362497544337 (5.25%) in 2.8.0/benchmarks/many_actors.json
REGRESSION 4.40%: 1_1_actor_calls_async (THROUGHPUT) regresses from 7456.112509761211 to 7127.879723065892 (4.40%) in 2.8.0/microbenchmark.json
REGRESSION 4.39%: client__1_1_actor_calls_async (THROUGHPUT) regresses from 982.5793246271417 to 939.4658556651513 (4.39%) in 2.8.0/microbenchmark.json
REGRESSION 4.26%: 1_1_actor_calls_sync (THROUGHPUT) regresses from 2273.2464988756947 to 2176.3519388689724 (4.26%) in 2.8.0/microbenchmark.json
REGRESSION 3.88%: n_n_async_actor_calls_async (THROUGHPUT) regresses from 24458.207679236828 to 23510.071149146344 (3.88%) in 2.8.0/microbenchmark.json
REGRESSION 3.60%: stage_2_avg_iteration_time (LATENCY) regresses from 60.438395738601685 to 62.61348090171814 (3.60%) in 2.8.0/stress_tests/stress_test_many_tasks.json
REGRESSION 3.44%: 10000_args_time (LATENCY) regresses from 17.291354780999995 to 17.886716769999992 (3.44%) in 2.8.0/scalability/single_node.json
REGRESSION 3.13%: 1_1_actor_calls_concurrent (THROUGHPUT) regresses from 4554.146655326606 to 4411.655031271767 (3.13%) in 2.8.0/microbenchmark.json
REGRESSION 2.22%: single_client_put_calls_Plasma_Store (THROUGHPUT) regresses from 5845.356422909845 to 5715.47419892146 (2.22%) in 2.8.0/microbenchmark.json
REGRESSION 2.22%: dashboard_p50_latency_ms (LATENCY) regresses from 6.852 to 7.004 (2.22%) in 2.8.0/benchmarks/many_tasks.json
REGRESSION 1.94%: multi_client_tasks_async (THROUGHPUT) regresses from 27850.61204431569 to 27311.034390113982 (1.94%) in 2.8.0/microbenchmark.json
REGRESSION 1.68%: stage_1_avg_iteration_time (LATENCY) regresses from 23.342886781692506 to 23.734616684913636 (1.68%) in 2.8.0/stress_tests/stress_test_many_tasks.json
REGRESSION 1.61%: single_client_wait_1k_refs (THROUGHPUT) regresses from 5.52141006441801 to 5.432281070822073 (1.61%) in 2.8.0/microbenchmark.json
REGRESSION 1.35%: client__put_calls (THROUGHPUT) regresses from 821.9975673342932 to 810.9298682126814 (1.35%) in 2.8.0/microbenchmark.json
REGRESSION 0.93%: avg_iteration_time (LATENCY) regresses from 1.5400808811187745 to 1.5543466377258301 (0.93%) in 2.8.0/stress_tests/stress_test_dead_actors.json
REGRESSION 0.61%: 10000_get_time (LATENCY) regresses from 25.08259386100002 to 25.235181824999998 (0.61%) in 2.8.0/scalability/single_node.json
REGRESSION 0.17%: tasks_per_second (THROUGHPUT) regresses from 429.0546317074886 to 428.3314400052605 (0.17%) in 2.8.0/benchmarks/many_tasks.json
```",regression latency regression latency regression latency regression latency regression latency regression latency regression throughput regression throughput regression throughput regression latency regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression latency regression latency regression throughput regression throughput regression latency regression throughput regression latency regression throughput regression throughput regression latency regression latency regression throughput,issue,negative,neutral,neutral,neutral,neutral,neutral
1783431556,Looks like this may need a manual merge from the latest branch to unblock it,like may need manual merge latest branch unblock,issue,negative,positive,positive,positive,positive,positive
1783388599,"> This makes a lot of sense. One reason we haven't gotten around to doing this is that there's a bit of a complexity under the hood mapping that converts between these configurations, Ray Tune-able parameters, and results. For the time being, I would recommend converting between dataclass/pydantic model and dictionary outside of the Ray Train code.

That makes sense to me. As I'm currently using Train without Tune, I've actually just been abusing the current system by passing my dataclass as a value in the train config dictionary. It doesn't serialize correctly in the results, but it makes its way to the trainer intact. However, once we start doing tuning jobs, I'll likely need to do as you say and start passing real dicts. It just becomes unwieldy once that dict gets really big (and one advantage of the dataclass is that it can have methods which help select exactly the args i need for individual functions inside the training loop). I'll have to think about this a little more.",lot sense one reason gotten around bit complexity hood ray time would recommend converting model dictionary outside ray train code sense currently train without tune actually current system passing value train dictionary serialize correctly way trainer intact however start tuning likely need say start passing real becomes unwieldy really big one advantage help select exactly need individual inside training loop think little,issue,positive,positive,neutral,neutral,positive,positive
1783384516,"Hey @matthewdeng , my main issue right now is creating a workflow where I can easily run a training job, then run a separate job to load the artifacts from training and evaluate them on a held out set. Having some control over the location of the artifacts would make it slightly easier to orchestrate this, though I'm also open to ideas on how I might do this better within the current limitations.

Right now, the final location of the artifacts is not known until the torchtrainer spits out the result at the end of training, and I then need to take the path (with the full `TorchTrainer_(lots-of-digits-here)` dir name) and pass it to the test job. However, if I was fully in control of where the artifacts landed, then that path name could be both a bit of configuration that is known ahead of time and doesn't need to be dynamically passed from one job to the next.",hey main issue right easily run training job run separate job load training evaluate set control location would make slightly easier orchestrate though also open might better within current right final location known result end training need take path full name pas test job however fully control landed path name could bit configuration known ahead time need dynamically one job next,issue,positive,positive,positive,positive,positive,positive
1783307631,"This makes a lot of sense. One reason we haven't gotten around to doing this is that there's a bit of a complexity under the hood mapping that converts between these configurations, Ray Tune-able parameters, and results. For the time being, I would recommend converting between dataclass/pydantic model and dictionary outside of the Ray Train code.",lot sense one reason gotten around bit complexity hood ray time would recommend converting model dictionary outside ray train code,issue,negative,neutral,neutral,neutral,neutral,neutral
1783305451,"@kadisi Thanks for the info, it would be helpful if you could share a zip of the logs on each node. By default they're in `/tmp/ray/session_latest/logs` on each node.",thanks would helpful could share zip node default node,issue,positive,positive,positive,positive,positive,positive
1783300723,Hey! Thanks for posting this -  there's some ongoing discussion about simplifying this. Could you share a little more about what your end desired directory structure might look like?,hey thanks posting ongoing discussion could share little end desired directory structure might look like,issue,positive,positive,neutral,neutral,positive,positive
1783287340,"@justinvyu -- Thanks for your quick reply. It does indeed work, however, I believe the bug in this case is a [documentation bug](https://docs.ray.io/en/latest/tune/api/doc/ray.tune.integration.xgboost.TuneReportCheckpointCallback.html#ray.tune.integration.xgboost.TuneReportCheckpointCallback):
> A checkpoint is always saved at the end of training.",thanks quick reply indeed work however believe bug case documentation bug always saved end training,issue,positive,positive,positive,positive,positive,positive
1783260710,"@dioptre I have added some CI builds that are testing Ray Serve (including FastAPI support) and am not seeing any issues. Could you please try with the nightly builds and let me know if you still see any problems?

https://docs.ray.io/en/latest/ray-overview/installation.html#daily-releases-nightlies",added testing ray serve support seeing could please try nightly let know still see,issue,positive,neutral,neutral,neutral,neutral,neutral
1783251300,@fardinabbasi I believe your issue is a different one that will be solved by https://github.com/ray-project/ray/pull/40647,believe issue different one,issue,negative,neutral,neutral,neutral,neutral,neutral
1783247721,"I'll close this for now, feel free to re-open if this solution doesn't work for you.",close feel free solution work,issue,positive,positive,positive,positive,positive,positive
1783114750,"Lint failure:

```
Fri Oct 27 12:06:51 UTC 2023 Flake8....
--
  | python/ray/_private/utils.py:338:89: E501 line too long (108 > 88 characters)
  | python/ray/tests/accelerators/test_hpu.py:3:1: F401 'subprocess' imported but unused
  | python/ray/tests/accelerators/test_hpu.py:110:74: E711 comparison to None should be 'if cond is None:'


```",lint failure flake line long unused comparison none cond none,issue,negative,negative,negative,negative,negative,negative
1782806256,"Hello, I wonder if it is possible to load the checkpoint trained with 0.8.5 into a model created with 2.6.0 now?",hello wonder possible load trained model,issue,negative,neutral,neutral,neutral,neutral,neutral
1782347687,"Hey @sven1977 ,
The issue `observation = OrderedDict(sorted(observation.items())) AttributeError: 'NoneType'` object has no attribute 'items' has been addressed in this [post](https://discuss.ray.io/t/register-a-custom-environment-and-runing-ppotrainer-on-that-environment-not-working/6143/7?u=fardinabbasi). It primarily occurred because I returned `self._get_obs() if not terminated else None` in the **step function**.

Nonetheless, I am still encountering the warning: `WARNING algorithm_config.py:672 -- Cannot create PPOConfig from the provided config_dict! Property __stdout_file__ is not supported`.
",hey issue observation sorted object attribute post primarily returned else none step function nonetheless still warning warning create provided property,issue,negative,positive,positive,positive,positive,positive
1782332534,"> I needed to add a new Firewall rule opening the ports

Might be worth adding a note to the documentation somewhere about this. I have seen the firewall pop-ups when starting `ray init` the first time.

> it was able to make a connection for 1 minute

It may not have actually connected, that may have been a time out while it tried unsuccessfully to connect.",add new rule opening might worth note documentation somewhere seen starting ray first time able make connection minute may actually connected may time tried unsuccessfully connect,issue,negative,positive,positive,positive,positive,positive
1782285943,"> Can you add a non-windows machine to the cluster or does it also die after a minute?

Hello there. The problem was that I needed to add a new Firewall rule opening the ports. Now the node doesn't die and everything works. 

I still think it's quite weird that it was able to make a connection for 1 minute, from my point of view it shouldn't have been able to connect at all.",add machine cluster also die minute hello problem add new rule opening node die everything work still think quite weird able make connection minute point view able connect,issue,negative,positive,positive,positive,positive,positive
1782179005,Thanks for the contribution! We will take a look at this PR shotly :) ,thanks contribution take look,issue,negative,positive,positive,positive,positive,positive
1782178310,"@can-anyscale the test_dashboard.py is not supposed to be manual (I am not sure why it was manual). So I just moved it to a core test.

For the redis_blahblah test, I found it doesn't exist at all, so I completely removed it. ",supposed manual sure manual core test test found exist completely removed,issue,negative,positive,positive,positive,positive,positive
1782175094,"we fixed this in the master, but the fix wasn't included in Ray 2.8 due to the risk of last-minute changes. The fix should be available in the master and will be available from Ray 2.9",fixed master fix included ray due risk fix available master available ray,issue,negative,positive,positive,positive,positive,positive
1782174650,This was a really awesome investigation! Great job! ,really awesome investigation great job,issue,positive,positive,positive,positive,positive,positive
1782173525,"Okay, sounds good! ETA for the final go today",good eta final go today,issue,negative,positive,positive,positive,positive,positive
1782161128,"> Does this happen every time, or only sometimes? I'm not sure if it's possible to get more information about why the actor died, since the error ""The actor died unexpectedly before finishing this task."" is very general (@jjyao do you know?), but if you could zip up and attach the logs, that would potentially be helpful.
> 
> The `AttributeError: 'NoneType' object has no attribute 'status'` is a separate issue which I believe shouldn't actually cause the job to fail. The error message is the same as #29632 so the root cause might be similar.

it happened every time.",happen every time sometimes sure possible get information actor since error actor unexpectedly finishing task general know could zip attach would potentially helpful object attribute separate issue believe actually cause job fail error message root cause might similar every time,issue,negative,positive,neutral,neutral,positive,positive
1782085030,"@sjhermanek Adding the `save_model` to the end of the script as you show seems like the easiest way to do this.

If you want to do everything with a callback, feel free to create a subclass of `xgboost.callback.TrainingCallback`. See here for the `after_training` hook, which is what you'd want to use: [xgboost docs](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.callback.TrainingCallback.after_training)

For example, extend the `TuneReportCheckpointCallback` with:

```python
class MyTuneReportCheckpointCallback(TuneReportCheckpointCallback):
    def after_training(model):
        with tempfile.TemporaryDirectory() as checkpoint_dir:
            model.save_model(os.path.join(checkpoint_dir, self._filename))
            checkpoint = Checkpoint.from_directory(checkpoint_dir)
            ray.train.report({}, checkpoint)
```
",end script show like easiest way want everything feel free create subclass see hook want use example extend python class model,issue,positive,positive,positive,positive,positive,positive
1782074286,"On python 3.11.5 with Ray 2.7.1,
With `ray.init(local_mode=True)`, `get_gpu_ids()` returned` ['0']`
Whereas with `ray.init()`, `get_gpu_ids()` returned` []`",python ray returned whereas returned,issue,negative,neutral,neutral,neutral,neutral,neutral
1782065319,"Neither does “tune multinode tests”. I think it’s unrelated to this commit

On Thu, Oct 26, 2023 at 4:25 PM Archit Kulkarni ***@***.***> wrote:

> What’s the other failed test? The kubernetes operator test doesn’t go
> through the vsphere cluster launcher code path
>
> On Thu, Oct 26, 2023 at 4:07 PM Victoria Tsai ***@***.***>
> wrote:
>
>> Two tests started failing with this commit:
>> [image: Screenshot 2023-10-26 at 4 06 35 PM]
>> <https://user-images.githubusercontent.com/7005244/278495519-6c2b3ded-b4bf-48d3-819a-1324756554dd.png>
>>
>> —
>> Reply to this email directly, view it on GitHub
>> <https://github.com/ray-project/ray/pull/40717#issuecomment-1782031234>,
>> or unsubscribe
>> <https://github.com/notifications/unsubscribe-auth/ABJU5RURC5ISQC5IYXXJYOLYBLULNAVCNFSM6AAAAAA6RVTJ5SVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTOOBSGAZTCMRTGQ>
>> .
>> You are receiving this because you authored the thread.Message ID:
>> ***@***.***>
>>
>
",neither tune think unrelated commit kulkarni wrote test operator test go cluster launcher code path wrote two failing commit image reply directly view id,issue,negative,positive,neutral,neutral,positive,positive
1782062879,"What’s the other failed test? The kubernetes operator test doesn’t go
through the vsphere cluster launcher code path

On Thu, Oct 26, 2023 at 4:07 PM Victoria Tsai ***@***.***>
wrote:

> Two tests started failing with this commit:
> [image: Screenshot 2023-10-26 at 4 06 35 PM]
> <https://user-images.githubusercontent.com/7005244/278495519-6c2b3ded-b4bf-48d3-819a-1324756554dd.png>
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/ray-project/ray/pull/40717#issuecomment-1782031234>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ABJU5RURC5ISQC5IYXXJYOLYBLULNAVCNFSM6AAAAAA6RVTJ5SVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTOOBSGAZTCMRTGQ>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>
",test operator test go cluster launcher code path wrote two failing commit image reply directly view id,issue,negative,positive,neutral,neutral,positive,positive
1781992198,"@aloysius-lim Thanks for filing this issue. This has highlighted some problematic sync-down logic that happens on restoration, but it actually only turned up now because of the custom `adlfs` fs that you are providing. 

`pyarrow`'s wrapper of `fsspec` filesystems uses [`fs.find`](https://github.com/apache/arrow/blob/fcbcb7dab71b14ff3436af7b21bd23132877e2fa/python/pyarrow/fs.py#L358C34-L360) to perform a list operation at a directory.

[The fsspec `find` interface accepts `maxdepth`](https://filesystem-spec.readthedocs.io/en/latest/api.html#fsspec.spec.AbstractFileSystem.find), but the `adlfs` implementation doesn't actually use this parameter -- so it always recursively lists all files in the directory, which results in this error.

See here:

https://github.com/fsspec/adlfs/blob/f15c37a43afd87a04f01b61cd90294dd57181e1d/adlfs/spec.py#L1128

Compare this to the `s3fs` implementation, which we test in our CI:

https://github.com/fsspec/s3fs/blob/2c074502c2d6a9be0d3f05eb678f4cc5add2e7e5/s3fs/core.py#L787

I can put up a fix on our end to generally make the sync-down logic more robust, but this is actually something that `adlfs` should correct in the implementation -- perhaps you could open up a PR for them?

Edit: I've posted an issue on their repo with a recommended fix -- maybe you can continue from that? https://github.com/fsspec/adlfs/issues/435",thanks filing issue problematic logic restoration actually turned custom providing wrapper perform list operation directory find interface implementation actually use parameter always directory error see compare implementation test put fix end generally make logic robust actually something correct implementation perhaps could open edit posted issue fix maybe continue,issue,negative,positive,neutral,neutral,positive,positive
1781969813,"Since the other [PR](https://github.com/ray-project/ray/pull/40716) is reverted (which together makes an optimization work in https://github.com/ray-project/ray/issues/31280), we will revert this as well. 

This should also close the minor regression: https://github.com/ray-project/ray/issues/40605",since together optimization work revert well also close minor regression,issue,positive,negative,neutral,neutral,negative,negative
1781958173,"Premerge passed; CI civ1 test failed on test_memory_pressure which is already failing on master:

![image](https://github.com/ray-project/ray/assets/56065503/55909456-8534-4dc9-a808-a37b6e74bdf7)
",test already failing master image,issue,negative,neutral,neutral,neutral,neutral,neutral
1781908428,@sihanwang41 the tests are copy-pasted. One of the tests was quite slow so I refactored it to remove the sleeps. Let me take a look at the remaining ones.,one quite slow remove let take look,issue,negative,negative,negative,negative,negative,negative
1781896791,"This is merged into release branch, can we close it soon?",release branch close soon,issue,negative,neutral,neutral,neutral,neutral,neutral
1781894573,"This is not prioritized right now. 

Can people share their feedback on what functionalities they would find helpful to expose in the dashboard, and what's lacking in the console output? Thanks!",right people share feedback would find helpful expose dashboard console output thanks,issue,positive,positive,positive,positive,positive,positive
1781888589,Forge merge since this is time-sensitive and the issue in premerge is unrelated,forge merge since issue unrelated,issue,negative,neutral,neutral,neutral,neutral,neutral
1781850878,"Instead of a cherry-pick, we are doing a direct revert here because a cherry pick of the master's revert (https://github.com/ray-project/ray/pull/40715) would be less trivial due to merge conflict. cc @zhe-thoughts ",instead direct revert cherry pick master revert would le trivial due merge conflict,issue,negative,negative,neutral,neutral,negative,negative
1781839992,The premerge failure is happening on master too. Slack thread: https://anyscaleteam.slack.com/archives/C02J07KKF2A/p1698350839623769 ,failure happening master slack thread,issue,negative,negative,negative,negative,negative,negative
1781835093,"Failed tests:
Serve tests unrelated
Rllib tests unrelated
py310 build images unrelated",serve unrelated unrelated build unrelated,issue,negative,neutral,neutral,neutral,neutral,neutral
1781826286,"Ah sorry, should have closed this. Look at this one instead: https://github.com/ray-project/ray/pull/40587",ah sorry closed look one instead,issue,negative,negative,negative,negative,negative,negative
1781812184,"I'm closing this then, just a simple change if we decide to try this again",simple change decide try,issue,negative,neutral,neutral,neutral,neutral,neutral
1781799245,Update here that PR with the removal is in and picked; awaiting approval to merge into release branch cc @kouroshHakha @sven1977 @can-anyscale ,update removal picked approval merge release branch,issue,negative,neutral,neutral,neutral,neutral,neutral
1781786878,Another question: why isn't there a time for 70B. We don't support it yet?,another question time support yet,issue,negative,neutral,neutral,neutral,neutral,neutral
1781686608,"So it's confirmed https://github.com/ray-project/ray/commit/3e8278dc8809274a7c324797897223b8a5b8bc5b is the root cause. 

Reason: 
- We started tracking all lost tasks in the PR at GCS (so that we could report data loss at task level granularity) 
- With more than 100M actor tasks being generated throughout the lifetime of the job, these become larger and larger
- We do have GC of such info, but only when the job finishes. 
- As a result, when a long running job generated many tasks, it will explode GCS memory gradually. 


The only unknown is why on the metric page, this doesn't show up as a slowly increase but a burst.  But the bisection is kind of conclusive: 
- Pass before this commit: https://buildkite.com/ray-project/release-tests-branch/builds/2326
- Fails at this commit with OOM: https://buildkite.com/ray-project/release-tests-branch/builds/2327
",confirmed root cause reason lost could report data loss task level granularity actor throughout lifetime job become job result long running job many explode memory gradually unknown metric page show slowly increase burst bisection kind conclusive pas commit commit,issue,positive,positive,positive,positive,positive,positive
1781678101,Can you add a non-windows machine to the cluster or does it also die after a minute?,add machine cluster also die minute,issue,negative,neutral,neutral,neutral,neutral,neutral
1781675739,Can you give a more complete reproducer? How do you start the cluster and what do you run on the windows machine? Can the windows machine execute any ray remote code during the minute it is working?,give complete reproducer start cluster run machine machine execute ray remote code minute working,issue,negative,neutral,neutral,neutral,neutral,neutral
1781658963,Could you update the PR description to include why,could update description include,issue,negative,neutral,neutral,neutral,neutral,neutral
1781552850,"> @alanwguo If this is a P1 issue (#40467), we shouldn't pick. So could you add more context? Thanks

My main arguments is that although this is not a blocker issue, it is highly visible and reported many times. 
Second, I believe this is a low risk change but I leave it up to you to make the final decision",issue pick could add context thanks main although blocker issue highly visible many time second believe low risk change leave make final decision,issue,negative,positive,positive,positive,positive,positive
1781522461,"```
REGRESSION 72.90%: dashboard_p95_latency_ms (LATENCY) regresses from 46.348 to 80.137 (72.90%) in 2.8.0/benchmarks/many_nodes.json
REGRESSION 64.43%: dashboard_p99_latency_ms (LATENCY) regresses from 14019.625 to 23052.355 (64.43%) in 2.8.0/benchmarks/many_tasks.json
REGRESSION 27.25%: dashboard_p95_latency_ms (LATENCY) regresses from 8149.28 to 10369.756 (27.25%) in 2.8.0/benchmarks/many_tasks.json
REGRESSION 20.13%: dashboard_p99_latency_ms (LATENCY) regresses from 145.645 to 174.969 (20.13%) in 2.8.0/benchmarks/many_nodes.json
REGRESSION 15.32%: 1000000_queued_time (LATENCY) regresses from 177.93132558000002 to 205.19758366500002 (15.32%) in 2.8.0/scalability/single_node.json
REGRESSION 13.14%: stage_2_avg_iteration_time (LATENCY) regresses from 60.438395738601685 to 68.37787942886352 (13.14%) in 2.8.0/stress_tests/stress_test_many_tasks.json
REGRESSION 11.01%: dashboard_p50_latency_ms (LATENCY) regresses from 3.687 to 4.093 (11.01%) in 2.8.0/benchmarks/many_nodes.json
REGRESSION 9.67%: 1_1_actor_calls_sync (THROUGHPUT) regresses from 2273.2464988756947 to 2053.5000943392597 (9.67%) in 2.8.0/microbenchmark.json
REGRESSION 8.44%: single_client_get_object_containing_10k_refs (THROUGHPUT) regresses from 13.009554585474522 to 11.911115119151876 (8.44%) in 2.8.0/microbenchmark.json
REGRESSION 8.18%: placement_group_create/removal (THROUGHPUT) regresses from 997.6322375478999 to 916.0390933731816 (8.18%) in 2.8.0/microbenchmark.json
REGRESSION 8.05%: single_client_wait_1k_refs (THROUGHPUT) regresses from 5.52141006441801 to 5.077083407402203 (8.05%) in 2.8.0/microbenchmark.json
REGRESSION 7.08%: stage_1_avg_iteration_time (LATENCY) regresses from 23.342886781692506 to 24.99666111469269 (7.08%) in 2.8.0/stress_tests/stress_test_many_tasks.json
REGRESSION 6.84%: stage_4_spread (LATENCY) regresses from 0.7296295246039273 to 0.7795093658217519 (6.84%) in 2.8.0/stress_tests/stress_test_many_tasks.json
REGRESSION 6.82%: 1_n_actor_calls_async (THROUGHPUT) regresses from 9672.982187721544 to 9013.345072517786 (6.82%) in 2.8.0/microbenchmark.json
REGRESSION 6.35%: 1_n_async_actor_calls_async (THROUGHPUT) regresses from 8657.20480299259 to 8107.296817082561 (6.35%) in 2.8.0/microbenchmark.json
REGRESSION 5.14%: 10000_get_time (LATENCY) regresses from 25.08259386100002 to 26.371442345999995 (5.14%) in 2.8.0/scalability/single_node.json
REGRESSION 5.03%: 10000_args_time (LATENCY) regresses from 17.291354780999995 to 18.161911279999998 (5.03%) in 2.8.0/scalability/single_node.json
REGRESSION 4.12%: actors_per_second (THROUGHPUT) regresses from 738.330085638146 to 707.91056858179 (4.12%) in 2.8.0/benchmarks/many_actors.json
REGRESSION 3.61%: tasks_per_second (THROUGHPUT) regresses from 429.0546317074886 to 413.5452010251431 (3.61%) in 2.8.0/benchmarks/many_tasks.json
REGRESSION 3.46%: single_client_put_calls_Plasma_Store (THROUGHPUT) regresses from 5845.356422909845 to 5642.93079417387 (3.46%) in 2.8.0/microbenchmark.json
REGRESSION 3.24%: single_client_tasks_async (THROUGHPUT) regresses from 9563.116886637355 to 9253.350380094456 (3.24%) in 2.8.0/microbenchmark.json
REGRESSION 1.49%: single_client_tasks_sync (THROUGHPUT) regresses from 1177.0860205196755 to 1159.5410057905888 (1.49%) in 2.8.0/microbenchmark.json
REGRESSION 1.38%: single_client_get_calls_Plasma_Store (THROUGHPUT) regresses from 7536.924380935448 to 7432.953617355073 (1.38%) in 2.8.0/microbenchmark.json
REGRESSION 1.29%: stage_3_creation_time (LATENCY) regresses from 2.1919972896575928 to 2.2203712463378906 (1.29%) in 2.8.0/stress_tests/stress_test_many_tasks.json
REGRESSION 1.01%: client__tasks_and_put_batch (THROUGHPUT) regresses from 11449.695712792654 to 11334.501893509201 (1.01%) in 2.8.0/microbenchmark.json
REGRESSION 0.14%: n_n_actor_calls_async (THROUGHPUT) regresses from 29270.036133623737 to 29229.518744061534 (0.14%) in 2.8.0/microbenchmark.json
```",regression latency regression latency regression latency regression latency regression latency regression latency regression latency regression throughput regression throughput regression throughput regression throughput regression latency regression latency regression throughput regression throughput regression latency regression latency regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression latency regression throughput regression throughput,issue,negative,neutral,neutral,neutral,neutral,neutral
1781521644,I believe yesterday's numbers should have reflected the infra fix already cc @rickyyx ,believe yesterday reflected infra fix already,issue,negative,neutral,neutral,neutral,neutral,neutral
1781495253,"This appears to be related to Ray on Spark.  @jjyao do you know who the best person for this is? 

The PR is just one line for something that appears to be untested in CI, so I'm okay with just merging it since it seems to pass their manual test.",related ray spark know best person one line something untested since pas manual test,issue,positive,positive,positive,positive,positive,positive
1781472798,"@kadisi Does this happen every time, or only sometimes? I'm not sure if it's possible to get more information about why the actor died, since the error ""The actor died unexpectedly before finishing this task."" is very general (@jjyao do you know?), but if you could zip up and attach the logs, that would potentially be helpful.

The `AttributeError: 'NoneType' object has no attribute 'status'` is a separate issue which I believe shouldn't actually cause the job to fail. The error message is the same as https://github.com/ray-project/ray/issues/29632 so the root cause might be similar.",happen every time sometimes sure possible get information actor since error actor unexpectedly finishing task general know could zip attach would potentially helpful object attribute separate issue believe actually cause job fail error message root cause might similar,issue,negative,positive,neutral,neutral,positive,positive
1781464227,"From Emil @ Kapa:

Looks like the default basis.css conflicts based on how Sphinx builds static sites for the figure element.
However adding these two CSS selectors to docs/source/_static/custom.css should fix it:
#kapa-widget-container figure {
  padding: 0 !important;
}

.mantine-Modal-root figure {
  padding: 0 !important;
}",kapa like default based sphinx static figure element however two fix figure padding important figure padding important,issue,positive,positive,positive,positive,positive,positive
1781462714,"Hey @anyscalesam It was sporadic before. Since last month, I am experiencing this problem every day as long as we are behind company firewall.",hey sporadic since last month problem every day long behind company,issue,negative,negative,negative,negative,negative,negative
1781446203,"@nvtkaszpir - are you able to pull?
@tedhtchang - did you run into any rate limit issues the last 2mo?",able pull run rate limit last mo,issue,negative,positive,positive,positive,positive,positive
1781439906,"@sven1977 back to you

UPDATE: re-reviewing looks like a Tune issue",back update like tune issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1781398852,"It seems that contents are there. We just need to restructure it. I don't know if @alanwguo and @rkooo567 have time for this.

Maybe @angelinalg can help here?",content need know time maybe help,issue,negative,neutral,neutral,neutral,neutral,neutral
1781398720,"The process does not hang. It does require gui access. The windows firewall does not like the new exe starting up, and pops up some dialog boxes the first time. If that is the reason for this issue the OP should say so. In the mean time I will close this.",process require access like new starting first time reason issue say mean time close,issue,negative,positive,neutral,neutral,positive,positive
1781393083,"@rkooo567 right, they are manual tests and never run on CI before - you can exclude the manual tag in the test jobs",right manual never run exclude manual tag test,issue,negative,positive,positive,positive,positive,positive
1781391947,@anyscalesam it seems that you @ the wrong person above. I don't own docs so it really needs input from @angelinalg and @richardliaw ,wrong person really need input,issue,negative,negative,negative,negative,negative,negative
1781370961,"@alanwguo If this is a P1 issue (https://github.com/ray-project/ray/issues/40467), we shouldn't pick. So could you add more context? Thanks",issue pick could add context thanks,issue,negative,positive,positive,positive,positive,positive
1781329117,"For further context, in the progress.csv within each trial directory, the last row also shows ""done"" as ""False""
",context within trial directory last row also done false,issue,negative,negative,negative,negative,negative,negative
1781288917,"Hi folks, I am currently working with the Pydantic team to make this happen. The underlying issue blocking us is here: https://github.com/pydantic/pydantic/issues/6763.

I've merged fixes for this issue into `pydantic` & `pydantic_core`, so we are now waiting for their next release in order to unpin the dependency on our side.",hi currently working team make happen underlying issue blocking u issue waiting next release order unpin dependency side,issue,negative,neutral,neutral,neutral,neutral,neutral
1781277907,Marked as a draft because I need to merge https://github.com/ray-project/ray/pull/40637 first.,marked draft need merge first,issue,negative,positive,positive,positive,positive,positive
1781252779,@can-anyscale I'm planning to mark the build non-blocking and follow up to fix the test failures separately (and immediately),mark build follow fix test separately immediately,issue,negative,neutral,neutral,neutral,neutral,neutral
1781152724,Reads like something going on with the Job and/or even Core and not necessarily a Dashboard issue (fairly straightforward code path for that...) @sihanwang41 can you take a look first?,like something going job even core necessarily dashboard issue fairly straightforward code path take look first,issue,negative,positive,positive,positive,positive,positive
1781144137,@Wendi-anyscale @scottsun94  does this ticket need triaging? if so what would be the priority/owner to submit the pr?,ticket need would submit,issue,negative,neutral,neutral,neutral,neutral,neutral
1781137309,Hey @vitsai are these number updated one after fixing the infra issue that @rickyyx mentioned? ,hey number one fixing infra issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1781060373,"@can-anyscale do you happen to know why test_dashboard fails here? Do you think this has never been running? https://buildkite.com/ray-project/premerge/builds/9888#018b6b75-00be-44d4-bcee-5b954771a670. It wasn't a part of flaky test before iiuc. 

Or do I need some special work to move this test? The same thing happens for test_ray_cluster_with_external_redis ",happen know think never running part flaky test need special work move test thing,issue,negative,positive,positive,positive,positive,positive
1780812937,"@can-anyscale can you check again? I modified them based on your comment. I found there are some flaky tests left for serverless test (1), so I just kept serverless.rayci.yml as is. Lmk if you want me to change anything. ",check based comment found flaky left test kept want change anything,issue,negative,neutral,neutral,neutral,neutral,neutral
1780802436,@batuhanfaik can you merge the latest master? Seems like premerge cannot pass for some reasons,merge latest master like pas,issue,negative,positive,positive,positive,positive,positive
1780762305,close for fixing commit message.,close fixing commit message,issue,negative,neutral,neutral,neutral,neutral,neutral
1780691475,"Encountered same issue in Ray 2.6.3, there is no obviously reason. Also I can add 'key=' to solve it, weird bug.",issue ray obviously reason also add solve weird bug,issue,negative,negative,negative,negative,negative,negative
1780664830,"@sven1977 Can you please take up the issue and suggest to me how can I improve my performance? Currently, only one trial takes 40-50 seconds. However as there are 500 training iterations followed by hyperparameter optimization, it will take a lot of time. ",please take issue suggest improve performance currently one trial however training optimization take lot time,issue,positive,neutral,neutral,neutral,neutral,neutral
1780437101,Oh I thought it was fixed by the cluster id thing? ,oh thought fixed cluster id thing,issue,negative,positive,neutral,neutral,positive,positive
1780411779,"The number of files that I need to read have increased.
I am able to read the files like this :
```python
import pandas as pd

df = pd.DataFrame()
for j,i in enumerate(sorted(glob.glob(f'{dir_path}/*'))):
    df = pd.concat([df, pd.read_parquet(i)]) 
```

Basically loading one file at a time and then performing a concat on the latest df 
But it is taking a lot of time. 
--------------
I would like to read the files directly via ray, I have tried multiple things  
```python
df = ray.data.read_parquet(dir_path, parallelism=1, ray_remote_args={""num_cpus"": 8})
df = ray.data.read_parquet(dir_path, parallelism=30, ray_remote_args={""num_cpus"": 8})
df = ray.data.read_parquet(dir_path, parallelism=30, ray_remote_args={""num_cpus"": 4})
df = ray.data.read_parquet(dir_path, parallelism=30)
df = ray.data.read_parquet(dir_path, parallelism=200)
```
all of these give OOM issue

when I run the command `ray.data.read_parquet` it is supposed to only read metadata, but it is still blowing up the memory! 

",number need read able read like python import enumerate sorted basically loading one file time latest taking lot time would like read directly via ray tried multiple python give issue run command supposed read still blowing memory,issue,positive,positive,positive,positive,positive,positive
1780387360,"> can you upgrade the grpcio version to 1.42 and see if it fixes the issue?

Yes, with upgraded version, it's working fine.
",upgrade version see issue yes version working fine,issue,positive,positive,positive,positive,positive,positive
1780383788,"It's been a while, just want to check what the status is for bringing back some basic monitoring functionalities ray tune runs on the dashboard. Staring at the table in stdout has been a bit frustrating.",want check status back basic ray tune dashboard staring table bit,issue,negative,neutral,neutral,neutral,neutral,neutral
1780354325,Maybe mark those that fails with @pytest.mark.skipif(sys.version_info.minor == 11) or something along that line,maybe mark something along line,issue,negative,neutral,neutral,neutral,neutral,neutral
1780313998,"> I don't think it fixes the flakiness that @rickyyx is looking for though: https://buildkite.com/ray-project/postmerge/builds/1404#018b694a-1097-4365-bf8e-d401c8b17e21
> 
> If this is not the cause I should rather not changing it

I see - thanks for the change, and helps ruling out that this is related. I will continue root causing. 

I will leave up to you folks to decide if we want to merge this. ",think flakiness looking though cause rather see thanks change ruling related continue root causing leave decide want merge,issue,negative,positive,neutral,neutral,positive,positive
1780266709,"Here is the full range of commits, with this being the most likely culprit: 
3e8278dc88 [core][dashboard] Task backend GC policy - GCS refactor [2/3]  (#38792)



25b57d04c7 add dask version (#40537)
0fc52f7a6e Change version numbers in 2.8 release (#40515)
dfdd43cf40 pick of #40525
e6c67051b5 pick of #40525
96efc33a66 make sure tests run serially within each docker (#40509)
add95611b7 Increase timeout for test basic 4 (#40492)
3e8278dc88 [core][dashboard] Task backend GC policy - GCS refactor [2/3]  (#38792)
cfdc6e0e0b [ml] remove alpa release tests (#40510)
66cfec94d2 Revert ""[core] Fix placement groups scheduling when no resources are specified (#39946)"" (#40506)
a1ac74f7b4 [ci] Change the owner of cluster launcher related tests to clusters team. (#40424)
2d726098cf [Doc] [KubeRay] [RayJob] Add info about submitter pod template (#40158)
3c0476aa9c [ci] support gpu core assignment per test shard
af332f41e9 [core] Fix placement groups scheduling when no resources are specified (#39946)
b5ef0ae7a2 [serve] Add microbenchmarks for streaming HTTP and `DeploymentHandle` calls (#40498)
f9de8555ca [Data] Move `_fetch_metadata_parallel` to `file_meta_provider` (#40295)
318fd579c1 [Data] Fix bug where `_StatsActor` errors with `PandasBlock` (#40481)
d8f2527d5f [ci] move train/serve/default minimal tests to civ2 (#40454)
1fb6147a26 [data] add dataset name (#40430)
f3bc522d76 [Data] Remove deprecated `do_write` (#40422)
7c44833720 [RLlib] Issue 39586: Fix dict space restoration from serialized (ordered dict vs normal dict provided by user). (#39627)
7fa1c28acd [serve] Migrate workflow tests using v1 api (#40472)
f4b5f6b673 [owners] remove code owners that are no longer active. (#40476)
b83b591605 [Cluster launcher] [vSphere] avoid to fetch private ip (#40204)
9ba85ae83e [Serve] Get rid of ray cluster setup for test_schema test (#40469)
bdc9f83ff2 [RLlib] Add `on_checkpoint_loaded` callback AND also store eval workers' `policy_mapping_fn` in algo state. (#40350)
4f6c28f543 [Serve] [Docs] Update Serve docs to use the dashboard head instead of the agent (#40474)
1845f1c9f2 [Serve] Support arg builders with non-Pydantic type hints (#40471)
a6bc5ac0ff [RLlib] Issue 40312: Better documentation on how to do inference with DreamerV3 (once trained). (#40448)
d28f6453f7 [core] Error check Redis get requests (#40333)
56f6adcc97 [core] Fix placement group invariant of PlacementResources being superset of Resources (#40038)
149536d0f4 migrate rllib gpu tests to civ2 (#40439)
f077834217 [ci] support debug builds (#40466)
1a08c16813 [Train][Templates] Add LoRA support to Llama-2 finetuning example (#37794)
d6baf12425 [core] Fix session key check (#40468)
f905171540 [Doc] Fix streaming generator doc code #40447
bf1c581e84 [Data][Docs] Add `Dataset.write_sql` to API reference (#40473)
779c08a26d [Train] Update Lightning RayDDPStrategy docstring (#40376)
819733f0d6 [Data] Improve error message when reading HTTP files (#40462)
8e86f25118 [serve] Fix linkcheck + remove deprecated rest api (#40464)
4d93e37c01 [Data] Deflake Data CI test suites: `test_stats`, `test_streaming_executor`, `test_object_gc` (#40457)
b60c1723be [Core] [runtime env] Fix get_wheel_filename being out of date (#39965)
d80fd1d1a2 [data] link dataset ids in constructor, return correct metrics id for materialize (#40413)
7c5b27516d [ci] move ray on spark test to civ2 (#40438)
bfe026f461 [civ2][gpu/4] migrate rllib multi-gpu tests to civ2 (#40379)
c6347d56a9 [serve] Remove custom FastAPI encoders (#40449)
b5eae24692 minimal (#40433)
3052a8d116 migrate ml tests to civ02 (#40440)
c44765ab34 [ci] mark dataset_shuffle_push_based_random_shuffle_100tb.aws as unstable (#40437)
3e13e7c17a [serve] Remove extra comment (#40441)
8f5cd610b1 [data] ray data dashboard config (#40195)
2da60b7f30 [runtime_env]: Remove hypen from profiler config (#40395)
5f832b3346 [core][streaming][python] Fix asyncio.wait coroutines args deprecated warnings #40292
8a7d674662 [ci] migrate debug + asan core builds to civ2 (#40418)
512e6adb36 [civ2][gpu/3] create rllib gpu builds (#40364)
6a5215ccad [unjail] test_redis_tls (#40423)
6ba659f920 [Data] Cap op concurrency with exponential ramp-up (#40275)
929b445d0a [tune] remove test_client.py  (#40415)
b3c14249e1 move serve release tests to civ2 (#40414)
5205a2d121 [ci] change to oss tags (#40428)
f10c25914d Revert ""[docker image] use buildkit to build ray image (#40365)"" (#40427)
11fb194050 [Data] Move `BlockWritePathProvider` to separate file (#40302)
56b72a542a [Data] Remove out-of-date Data examples (#40127)
c91ee0fecc [tune] remove TuneRichReporter (#40169)
4d0c05b00e release test infra still relies on python 3.7 (#40407)
9d9e7c37c1 [serve] Clean up `test_metrics.py::test_queued_queries_disconnect` (#40410)
58b26141d8 [serve] Migrate v1 api release tests (#40372)
ff58667da5 [serve] Outdated API cleanup in docs (#40404)
c16082fa01 [ci] add special tag for ray and ray-ml image steps (#40394)
bc80271c5e Update `CODEOWNERS` (#40268)
ac73b159d6 [serve] Fix flaky test_autoscaling_policy on windows (#40411)
c4ad3d5a2a [jobs] Fix recovery race condition in `JobManager` (#40068)
7bd1d3a0b2 [Data] Deprecate extraneous `Dataset` parameters and methods (#40385)
405e82ab64 [RLlib] Issue deprecation warnings for all `rllib_contrib` algos. (#40147)
e28e2a65a6 [serve] Deprecate DAG API (#40290)
f787843b7f [Doc][KubeRay] Add a section for Redis cleanup (#40308)
71e893c59e [doc] Add vSphere version requirement in user guide (#40284)
67ec4476e3 [Doc] Logging: Add Fluent Bit DaemonSet and CloudWatch for persistence (#39895)
bde327fcdc [Jobs] append error trace to job driver logs (#40380)
a8b24dae32 [Serve] Fix the benchmark import error (#40381)
574eb54bee [serve] Migrate v1 api tests (#40363)
16da48491e [Core] Introduce AcceleratorManager interface (#40286)
56affb7e4b [RLlib] Fix BC release test failure. (#40371)
dc944fe7d9 [Dependencies] Remove pickle5 backport (#40338)
58cd807bf5 [dashboard] Remove `/api/snapshot` endpoint (#40269)
a2ef28db16 [Core] Bugfix/runtime agent binding (#40092) (#40311)
8fa1565053 [deflakey] Deflakey `test_redis_tls` (#40378)
820aad1836 [civ2][gpu/2] migrate ml gpu tests to civ2 (#40362)
ad7e1fc2ee [Train] Deprecate TransformersTrainer (#40277)
89eb6da181 [Train] Update checkpoint path for RayTrainReportCallbacks. (#40174)
dd6eb71fdd [Dependencies] Remove typing_extensions (#40336)
4113ab42bc [runtime env]: Integrating Nsight to Ray worker process (#39998)
c6baff26d7 [Train] Fix lightning 2.0 import path (#40266)
199b6cacdf [RLlib][Docs] Add mobile-env to RLlib community examples (#37641)
1a286fd255 [Train] Deprecate AccelerateTrainer (#40274)
b3c7af543b fix (#40374)
40275a944e [KubeRay][Autoscaler] Make KubeRay CRD version configurable (#40357)
5c3f100dc3 [serve] Deprecate single app config (#40329)
0c06bb9894 [data] store ray dashboard metrics in _StatsActor (#40118)
941ac71e43  [serve] Fix deploy config edge case bug (#40326)
3d2d4fe816 [data] Allow setting target max block size per-op instead of per-Dataset and reduce for streaming maps (#39710)
da5046e76a [docker image] use buildkit to build ray image (#40365)
d9e24f2d59 [civ2][gpu/1] create ml gpu builds (#40322)
563a9bf32a Jail //python/ray/tests:test_redis_tls (#40366)
09d4f0ab72 [Data] Fix return type and docstring for iter APIs (#40361)
c49b8ed244 [Data] Fix documentation link for local shuffle (#40291)
8310ce11df [Data] Remove `BulkExecutor` code path (#40200)
56337e04b7 [data] Add function arg params to map and flat_map (#40010)
ba581a3d60 [serve] Initial `pydantic>=2.0` compatibility (#40222)
b31a5aaf0d [serve] Remove v1 api (#40218)
4ab0ba0823 [Data] Remove FileMetadataShuffler (#40341)
306c71438c [Doc] Streaming generator alpha doc (#39914)
8d286f03ce [RLlib-contrib] Dreamer(V1) (won't be moved into `rllib_contrib`, b/c we now have DreamerV3). (#36621)
f097cd4512 [RLlib] Remove some deprecation warnings that should not be there. (#39984)
 


<img width=""761"" alt=""image"" src=""https://github.com/ray-project/ray/assets/11676094/dc18a11e-7004-4d9d-80ba-bc0385f18569"">

",full range likely culprit core dashboard task policy add version change version release pick pick make sure run serially within docker increase test basic core dashboard task policy remove release revert core fix placement change owner cluster launcher related team doc add submitter pod template support core assignment per test shard core fix placement serve add streaming data move data fix bug move minimal data add name data remove issue fix space restoration ordered normal provided user serve migrate remove code longer active cluster launcher avoid fetch private serve get rid ray cluster setup test add also store state serve update serve use dashboard head instead agent serve support type issue better documentation inference trained core error check get core fix placement group invariant migrate support train add lora support example core fix session key check doc fix streaming generator doc code data add reference cad train update lightning data improve error message reading serve fix remove rest data data test core fix date data link constructor return correct metric id materialize move ray spark test migrate serve remove custom minimal ad migrate cab mark unstable serve remove extra comment data ray data dashboard remove profiler core streaming python fix ad migrate core create data cap concurrency exponential tune remove move serve release ad change revert docker image use build ray image data move separate file baa data remove data tune remove release test infra still python serve clean serve migrate release serve outdated cleanup add special tag ray image update serve fix flaky fix recovery race condition data deprecate extraneous issue deprecation serve deprecate dag doc add section cleanup doc add version requirement user guide doc logging add fluent bit persistence append error trace job driver serve fix import error serve migrate dae core introduce interface fix release test failure remove pickle dashboard remove core agent binding fa migrate train deprecate train update path remove ray worker process train fix lightning import path add community train deprecate fix ae make version serve deprecate single data store ray dashboard metric ace serve fix deploy edge case bug data allow setting target block size instead reduce streaming docker image use build ray image create jail data fix return type iter data fix documentation link local shuffle data remove code path data add function map serve initial compatibility serve remove data remove doc streaming generator alpha doc dreamer wo remove deprecation image,issue,positive,positive,neutral,neutral,positive,positive
1780244128,"I don't think it fixes the flakiness that @rickyyx is looking for though: https://buildkite.com/ray-project/postmerge/builds/1404#018b694a-1097-4365-bf8e-d401c8b17e21

If this is not the cause I should rather not changing it",think flakiness looking though cause rather,issue,negative,neutral,neutral,neutral,neutral,neutral
1780234346,"The small tests ran fine but I think a better solution is probably for these tests to clean up their object store once it is finished; otherwise as the number of tests growth, I assume /dev/shm will always have issues",small ran fine think better solution probably clean object store finished otherwise number growth assume always,issue,positive,positive,positive,positive,positive,positive
1780220344,I'm running all tests to see how this affect everything overall: https://buildkite.com/ray-project/postmerge/builds/1404,running see affect everything overall,issue,negative,neutral,neutral,neutral,neutral,neutral
1780205051,"Need a stamp from core team as well though, CC: @c21 , @raulchen ",need stamp core team well though,issue,negative,neutral,neutral,neutral,neutral,neutral
1780166626,"This one is not a release blocker, right?",one release blocker right,issue,negative,positive,positive,positive,positive,positive
1780137017,"You probably need to move everything in serverless.tests.yml to core.tests.yml as well; otherwise premerge will likely be very flaky

Also can probably delete a few things in serverless.rayci.yml",probably need move everything well otherwise likely flaky also probably delete,issue,negative,neutral,neutral,neutral,neutral,neutral
1780134997,"@Harsh-Maheshwari oops, my bad, the correct syntax should be `read_parquet(..., ray_remote_args={""num_cpus"": 4})`. 
If you have a follow-up map, use `map(..., num_cpus=4)`",bad correct syntax map use map,issue,negative,negative,negative,negative,negative,negative
1780129994,"I was able to find a different version combination to work around the issue for my Python 3.11 tests, but this still seems like a real bug to address",able find different version combination work around issue python still like real bug address,issue,negative,positive,positive,positive,positive,positive
1780060216,"For TP, I have a workaround that seems to work in my case...

I remove the `num_gpus` option for the deployment itself (`VLLMPredictDeployment`). This option for the deployment seems mostly to be for scheduling (to tell it to run the deployment on a node with GPU available), since under the hood, vllm also creates a RayWorker that actually uses GPU. The AsyncLLMEngine itself doesn't need GPU to run, as it's calling ray.remote to these RayWorkers that run the workload on GPU.

What you could do instead is to add a custom resource to GPU nodes and have the VLLM deployment require that custom resource to get the top-level deployment to get deployed to a node with GPU. Another alternative is to set `num_cpus` in such a way that the scheduler will always try to put the vllm deployment onto a GPU node), without actually reserving a GPU for itself.

However, I am running into another issue, which is that for a long-running VLLM deployment, the `RayWorker` actors that vllm is creating die at some point and are never recreated. In the case (when the `RayWorkers` are all dead) `VLLMPredictDeployment` will give the following error:
```
return (yield from awaitable.__await__())
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
    class_name: RayWorker
    actor_id: 1963424394259fade0c44c5501000000
    pid: 714
    namespace: _ray_internal_dashboard
    ip: 100.64.144.72
The actor is dead because all references to the actor were removed

(...)
raise AsyncEngineDeadError(\nvllm.engine.async_llm_engine.AsyncEngineDeadError: Task finished unexpectedly. This should never happen! Please open an issue on Github. See stack trace above for the actual cause..
```

",work case remove option deployment option deployment mostly tell run deployment node available since hood also actually need run calling run could instead add custom resource deployment require custom resource get deployment get node another alternative set way always try put deployment onto node without actually however running another issue deployment die point never case dead give following error return yield actor unexpectedly finishing task actor dead actor removed raise task finished unexpectedly never happen please open issue see stack trace actual cause,issue,negative,positive,neutral,neutral,positive,positive
1780046604,Thanks for fixing this. This helps make the error handling more predictable.,thanks fixing make error handling predictable,issue,negative,neutral,neutral,neutral,neutral,neutral
1780037652,"@sven1977  I thought so :laughing:  Thanks!

I have a working codebase built on the old API, but I have been porting it over to the new API(s) in preparation for the deprecation. Hence why I've been finding all these issues! Right now I can run my algorithms using both API so it's not a big deal.",thought laughing thanks working built old new preparation deprecation hence finding right run big deal,issue,positive,positive,positive,positive,positive,positive
1779999925,"This is an artifact from being in an intermediary state between the old APIs and new - rest assured there is no actual bug/issue with the code and it will run just fine.

The way I see it, you have three options in the interim until the old APIs are completely deprecated:

a) set `log_level=""ERROR""` in the configuration of your algorithm
b) Create a custom `PPOConfig` and override the `PPOConfig.rl_module` method to set `self.exploration_config={}` before calling `super()`
c) Disable the new APIs by setting `_enable_learner_api=False` and `_enable_rl_module_api=False` in your configuration

Hope this helps",artifact intermediary state old new rest assured actual code run fine way see three interim old completely set error configuration algorithm create custom override method set calling super disable new setting configuration hope,issue,positive,positive,positive,positive,positive,positive
1779999194,"@rkooo567 , perhaps it just needs someone from ray team to approve? Shawn has approved but maybe he does not have the right permissions? Can you approve given Shawn has reviewed the code?",perhaps need someone ray team approve maybe right approve given code,issue,negative,positive,positive,positive,positive,positive
1779975021,"@Mark2000 I restore a stopped tune run with:

```python
tune.run(
  ""TD3"", # same as the original run
  name=""<folder_name_containing_the_trial_data>"", # for instance TD3_2023-10-23_18-17-45 
  local_dir=""<absolute_path_to_the_local_dir_of_the_original_run>"", 
  resume=True
)
```

You can also use `resume=""ERRORED_ONLY""` instead if you need to restart failed instances only. In my experience the restored trial works well in some cases but in others the restored trial behaves very differently from the original one. See for instance the following plot for a trial that was restored at ~600k steps after which the reward curve displays a very different profile. 

<img width=""1354"" alt=""Screenshot 2023-10-25 at 20 00 24"" src=""https://github.com/ray-project/ray/assets/2353002/4b944746-12fb-4caa-8453-a4f019007d83"">


Using Ray 2.7.1 + torch 2.1.0+cu118 on an Ubuntu 22.04 system.
",mark restore stopped tune run python original run instance also use instead need restart experience trial work well trial differently original one see instance following plot trial reward curve different profile ray torch system,issue,positive,positive,positive,positive,positive,positive
1779960504,The correction required is actually discovered by a user.,correction actually discovered user,issue,negative,neutral,neutral,neutral,neutral,neutral
1779938680,"Are you following the `Tuner.restore(...)` message that shows up after interrupting training? There's a known issue that this is outputting the wrong restoration path in the case that you set a custom `storage_path`. What's the `storage_path` you set, and what's the path you're passing into `Tuner.restore`?",following message interrupting training known issue wrong restoration path case set custom set path passing,issue,negative,negative,negative,negative,negative,negative
1779930087,"This will be fixed by https://github.com/ray-project/tune-sklearn/pull/272, I will update this thread once a new release with the patch is in. ",fixed update thread new release patch,issue,negative,positive,positive,positive,positive,positive
1779897375,"Totally, I'll wait for https://buildkite.com/ray-project/release-tests-pr/builds/56964#_ but otherwise I'll merge it",totally wait otherwise merge,issue,negative,neutral,neutral,neutral,neutral,neutral
1779892906,@can-anyscale thanks for the quick explanation! Would you mind merging this PR?,thanks quick explanation would mind,issue,negative,positive,positive,positive,positive,positive
1779860400,@jjyao can you attempt repro with latest ray and see if with the script above we get the same head to worker node comm error?,attempt latest ray see script get head worker node error,issue,negative,positive,positive,positive,positive,positive
1779828689,This is currently blocking me from adding Python 3.11 tests for Serve: https://github.com/ray-project/ray/pull/40637,currently blocking python serve,issue,negative,neutral,neutral,neutral,neutral,neutral
1779821983,"Hey guys, sorry for the confusion. But directly using Ray Data on the client side is not supported. 
Your code may happen to work in a previous version, but there are a lot of other issues. 

When using Ray Data with the client mode, we suggest you put your Data code in a remote task. For example:
```
@ray.remote
def my_data_task():
    # Your data code here
ray.get(my_data_task.remote())
```   

We will improve Ray Client to make this restriction clearer cc @rynewang 
But directly using Ray Data code on the client side won't be supported. ",hey sorry confusion directly ray data client side code may happen work previous version lot ray data client mode suggest put data code remote task example data code improve ray client make restriction clearer directly ray data code client side wo,issue,negative,negative,negative,negative,negative,negative
1779799630,"I'd be happy to try implementing this. At a high-level I would need to understand the stack from python API -> Cython -> C++ implementation of metrics, and bridge access to the low-level metric store with an API call, right? 

I skimmed through the call-stack from ray.util.metrics to metrics.cc (haven't had a chance to understand the metrics_agent yet) and it seems OpenCensus handles the actual data store + exporting to Prometheus. Is my understanding correct there? ",happy try would need understand stack python implementation metric bridge access metric store call right skimmed chance understand yet actual data store understanding correct,issue,positive,positive,positive,positive,positive,positive
1779795577,"> Does it still throw RuntimeError, given we added the fallback?

The fallback doesn't catch the runtime error caused by using ray client, I thought that was a separate issue. If we're using ray client then this won't work at all because `global_node` is not initialized and we can't get the id for the current cluster, so we'll just error out.",still throw given added fallback fallback catch error ray client thought separate issue ray client wo work ca get id current cluster error,issue,negative,neutral,neutral,neutral,neutral,neutral
1779791262,"Thanks, @sven1977 I look forward to the new APIs. So far I'm very pleased with the new APIs especially the RL Module API since it now supports Torch JIT compilation.

I'll take your suggestion!",thanks look forward new far new especially module since torch compilation take suggestion,issue,negative,positive,positive,positive,positive,positive
1779789638,"Hi @can-anyscale I see the test is ""jailed"". Does that mean it's not run by default? How can I unjail it?",hi see test mean run default,issue,negative,negative,negative,negative,negative,negative
1779776031,"CC: @c21 , @scottjlee , @raulchen if you can help review the changes to python/ray/data/tests/test_mongo.py i would appreciate, thankks",help review would appreciate,issue,positive,neutral,neutral,neutral,neutral,neutral
1779758230,@rkooo567 you mean we don't cherry pick to *2.8* right? sgtm,mean cherry pick right,issue,negative,negative,neutral,neutral,negative,negative
1779748906,"Segfault: 
```
2023-10-25 02:24:28,994 ERROR tune.py:1043 -- Trials did not complete: [TorchTrainer_eda84_00000]
*** SIGSEGV received at time=1698225869 on cpu 2 ***
PC: @     0x7fc8b8cdcc10  (unknown)  __pyx_pw_7pyarrow_3_fs_10FileSystem_5equals()
    @     0x7fc8d3205420  1837120176  (unknown)
    @     0x7fc8b8cc514c         80  __Pyx_PyObject_CallOneArg()
    @     0x7fc8b8cca76e        144  __pyx_pf_7pyarrow_3_fs_10FileSystem_6__eq__()
    @     0x7fc8b8ccab28         48  __pyx_tp_richcompare_7pyarrow_3_fs_FileSystem()
    @           0x4e181b  (unknown)  PyObject_RichCompare
    @     0x7fc8b8d02920  (unknown)  (unknown)
[2023-10-25 02:24:29,137 E 683 683] logging.cc:361: *** SIGSEGV received at time=1698225869 on cpu 2 ***
[2023-10-25 02:24:29,137 E 683 683] logging.cc:361: PC: @     0x7fc8b8cdcc10  (unknown)  __pyx_pw_7pyarrow_3_fs_10FileSystem_5equals()
[2023-10-25 02:24:29,138 E 683 683] logging.cc:361:     @     0x7fc8d3205420  1837120176  (unknown)
[2023-10-25 02:24:29,138 E 683 683] logging.cc:361:     @     0x7fc8b8cc514c         80  __Pyx_PyObject_CallOneArg()
[2023-10-25 02:24:29,138 E 683 683] logging.cc:361:     @     0x7fc8b8cca76e        144  __pyx_pf_7pyarrow_3_fs_10FileSystem_6__eq__()
[2023-10-25 02:24:29,138 E 683 683] logging.cc:361:     @     0x7fc8b8ccab28         48  __pyx_tp_richcompare_7pyarrow_3_fs_FileSystem()
[2023-10-25 02:24:29,138 E 683 683] logging.cc:361:     @           0x4e181b  (unknown)  PyObject_RichCompare
[2023-10-25 02:24:29,140 E 683 683] logging.cc:361:     @     0x7fc8b8d02920  (unknown)  (unknown)
Fatal Python error: Segmentation fault
```",error complete received unknown unknown unknown unknown unknown received unknown unknown unknown unknown unknown fatal python error segmentation fault,issue,negative,negative,neutral,neutral,negative,negative
1779746130,Gone after infra issue fixed. see https://github.com/ray-project/ray/pull/40571#issuecomment-1779683445,gone infra issue fixed see,issue,negative,positive,neutral,neutral,positive,positive
1779741118,"I believe there's some regression in `1000000_queued_time` after this PR https://github.com/ray-project/ray/pull/38771, but not as much as 10%, from the historical range, it's more of ~5% (from 185 -> 195). 

The test is testing submitting of 1M tasks from the driver (which overloads the task backend buffer on the driver worker), given other more realistic tests like `microbenchmark` and `stress_test_many_tasks` and the variance of this metric, I would probably propose to accept this. 

<img width=""1308"" alt=""image"" src=""https://github.com/ray-project/ray/assets/11676094/06514764-f263-4484-b0d0-3eb242800cbd"">

",believe regression much historical range test testing driver task buffer driver worker given realistic like variance metric would probably propose accept image,issue,positive,positive,neutral,neutral,positive,positive
1779736314,marking as stale and closing; @robertreaney if this still repros on latest ray271 please reopen,marking stale still latest ray please reopen,issue,negative,neutral,neutral,neutral,neutral,neutral
1779734381,"I see. This is not a dashboard issue then.
Need @akshay-anyscale to triage",see dashboard issue need triage,issue,negative,neutral,neutral,neutral,neutral,neutral
1779733959,@peytondmurray can you  attempt a repro with latest ray271; if unable to repro we can close.,attempt latest ray unable close,issue,negative,neutral,neutral,neutral,neutral,neutral
1779732449,@jdonzallaz still important to revisit this? if so can you attempt repro with latest ray271?,still important revisit attempt latest ray,issue,negative,positive,positive,positive,positive,positive
1779731935,"I think @rkooo567 also assigned the priority which makes sense. However, there is no ETA given the limited bandwidth we have.",think also assigned priority sense however eta given limited,issue,negative,negative,neutral,neutral,negative,negative
1779729900,"`stage_3_creation_time` is high variance (abs value is small but the variance is relatively large)

<img width=""1298"" alt=""image"" src=""https://github.com/ray-project/ray/assets/11676094/c75069e2-24a5-48fd-8ef9-06f983befbfa"">
",high variance value small variance relatively large image,issue,negative,positive,neutral,neutral,positive,positive
1779683445,"This one is against 2.7.1
```
REGRESSION 56.47%: dashboard_p95_latency_ms (LATENCY) regresses from 46.348 to 72.519 (56.47%) in 2.8.0/benchmarks/many_nodes.json
REGRESSION 23.75%: dashboard_p50_latency_ms (LATENCY) regresses from 35.742 to 44.232 (23.75%) in 2.8.0/benchmarks/many_actors.json
REGRESSION 21.50%: dashboard_p50_latency_ms (LATENCY) regresses from 6.852 to 8.325 (21.50%) in 2.8.0/benchmarks/many_tasks.json
REGRESSION 20.44%: stage_3_creation_time (LATENCY) regresses from 2.1919972896575928 to 2.6400458812713623 (20.44%) in 2.8.0/stress_tests/stress_test_many_tasks.json
REGRESSION 15.15%: dashboard_p95_latency_ms (LATENCY) regresses from 8149.28 to 9383.733 (15.15%) in 2.8.0/benchmarks/many_tasks.json
REGRESSION 14.23%: single_client_get_calls_Plasma_Store (THROUGHPUT) regresses from 7536.924380935448 to 6464.1729246449195 (14.23%) in 2.8.0/microbenchmark.json
REGRESSION 13.61%: dashboard_p99_latency_ms (LATENCY) regresses from 145.645 to 165.464 (13.61%) in 2.8.0/benchmarks/many_nodes.json
REGRESSION 11.70%: dashboard_p99_latency_ms (LATENCY) regresses from 14019.625 to 15660.353 (11.70%) in 2.8.0/benchmarks/many_tasks.json
REGRESSION 9.63%: 1000000_queued_time (LATENCY) regresses from 177.93132558000002 to 195.069067863 (9.63%) in 2.8.0/scalability/single_node.json
REGRESSION 6.62%: dashboard_p50_latency_ms (LATENCY) regresses from 3.687 to 3.931 (6.62%) in 2.8.0/benchmarks/many_nodes.json
REGRESSION 4.58%: client__put_gigabytes (THROUGHPUT) regresses from 0.12778429919203013 to 0.12193333038806775 (4.58%) in 2.8.0/microbenchmark.json
REGRESSION 4.43%: 107374182400_large_object_time (LATENCY) regresses from 30.62209055699998 to 31.97824557199999 (4.43%) in 2.8.0/scalability/single_node.json
REGRESSION 4.00%: 10000_args_time (LATENCY) regresses from 17.291354780999995 to 17.983785811000004 (4.00%) in 2.8.0/scalability/single_node.json
REGRESSION 3.72%: tasks_per_second (THROUGHPUT) regresses from 275.25470863736416 to 265.0085938319898 (3.72%) in 2.8.0/benchmarks/many_nodes.json
REGRESSION 3.55%: avg_pg_create_time_ms (LATENCY) regresses from 0.917001803304134 to 0.9495785120136944 (3.55%) in 2.8.0/stress_tests/stress_test_placement_group.json
REGRESSION 3.18%: 10000_get_time (LATENCY) regresses from 25.08259386100002 to 25.881300527000008 (3.18%) in 2.8.0/scalability/single_node.json
REGRESSION 2.85%: stage_4_spread (LATENCY) regresses from 0.7296295246039273 to 0.7504145523663499 (2.85%) in 2.8.0/stress_tests/stress_test_many_tasks.json
REGRESSION 2.58%: actors_per_second (THROUGHPUT) regresses from 738.330085638146 to 719.3018798022547 (2.58%) in 2.8.0/benchmarks/many_actors.json
REGRESSION 2.21%: 1_n_actor_calls_async (THROUGHPUT) regresses from 9672.982187721544 to 9459.166339745669 (2.21%) in 2.8.0/microbenchmark.json
REGRESSION 2.15%: multi_client_tasks_async (THROUGHPUT) regresses from 27850.61204431569 to 27251.785365248128 (2.15%) in 2.8.0/microbenchmark.json
REGRESSION 2.03%: multi_client_put_gigabytes (THROUGHPUT) regresses from 33.620993378733125 to 32.938469438463315 (2.03%) in 2.8.0/microbenchmark.json
REGRESSION 1.80%: time_to_broadcast_1073741824_bytes_to_50_nodes (LATENCY) regresses from 85.80861040199989 to 87.355050575 (1.80%) in 2.8.0/scalability/object_store.json
REGRESSION 1.26%: stage_2_avg_iteration_time (LATENCY) regresses from 60.438395738601685 to 61.202564811706544 (1.26%) in 2.8.0/stress_tests/stress_test_many_tasks.json
REGRESSION 0.55%: single_client_put_calls_Plasma_Store (THROUGHPUT) regresses from 5845.356422909845 to 5813.449193922757 (0.55%) in 2.8.0/microbenchmark.json
REGRESSION 0.27%: single_client_wait_1k_refs (THROUGHPUT) regresses from 5.52141006441801 to 5.506660769810904 (0.27%) in 2.8.0/microbenchmark.json
REGRESSION 0.10%: avg_iteration_time (LATENCY) regresses from 1.5400808811187745 to 1.5415918231010437 (0.10%) in 2.8.0/stress_tests/stress_test_dead_actors.json
```",one regression latency regression latency regression latency regression latency regression latency regression throughput regression latency regression latency regression latency regression latency regression throughput regression latency regression latency regression throughput regression latency regression latency regression latency regression throughput regression throughput regression throughput regression throughput regression latency regression latency regression throughput regression throughput regression latency,issue,negative,neutral,neutral,neutral,neutral,neutral
1779682547,"The 1st example is not a valid example anymore because it was removed. @peytondmurray, following up on our chat on Slack a few weeks back, did you get a chance to add the second link to the example gallery and try the linktest checks? 
cc: @anyscalesam ",st example valid example removed following chat slack back get chance add second link example gallery try,issue,negative,neutral,neutral,neutral,neutral,neutral
1779672146,Closing since no response; @tas17 please reopen if you still run into issues.,since response please reopen still run,issue,negative,neutral,neutral,neutral,neutral,neutral
1779669919,@peytondmurray can you take point on what the next step in terms of solution where we don't need to add arbitrary sleeps to Worker startup.,take point next step solution need add arbitrary worker,issue,negative,negative,neutral,neutral,negative,negative
1779664267,Per check in with @peytondmurray there is still one example that is broken; @simran-2797 were you going to chase down where it should redirect to?,per check still one example broken going chase redirect,issue,negative,negative,negative,negative,negative,negative
1779662452,@peytondmurray per our discussion can you please follow up on next steps for this ticket?,per discussion please follow next ticket,issue,negative,neutral,neutral,neutral,neutral,neutral
1779657835,"@mattip latest on this; per @peytondmurray there were some new configs added to the CLI. If so, is that sufficient to close this ticket out?",latest per new added sufficient close ticket,issue,negative,positive,positive,positive,positive,positive
1779651363,"To my understanding this issue is to how to use working_dir with Ray Serve, and there are a few suggested solutions in this thread to get your code working. If the issue is to change the behavior behind how `working_dir` works, then it's beyond Serve's ability to make the change as it's a core issue that affect all stacks. There are existing proposed design changes as you pointed out in https://github.com/ray-project/ray/issues/26784 and https://github.com/ray-project/ray/issues/33456. Please comment on those existing issues so they don't get duplicated and distributed. Again, this is not a ""bug"", it's a feature designed to prevent users from passing invalid urls. Totally understand there are improvements can be made, so please put your suggestions to those other design proposals so they can be included when the new logics are implemented. ",understanding issue use ray serve thread get code working issue change behavior behind work beyond serve ability make change core issue affect design pointed please comment get distributed bug feature designed prevent passing invalid totally understand made please put design included new,issue,positive,negative,neutral,neutral,negative,negative
1779641370,Currently we have not documented the requirements for a custom docker image but this can be done; I believe some things that are required include ssh and rsync (preferably the same version of rsync as your machine). ,currently custom docker image done believe include preferably version machine,issue,negative,neutral,neutral,neutral,neutral,neutral
1779580220,@scottsun94 can you please take point on this to decide priority and ETA for this enhancement? We can chat more next week with the relevant POCs.,please take point decide priority eta enhancement chat next week relevant,issue,negative,positive,positive,positive,positive,positive
1779577844,@raulchen please take the next step here,please take next step,issue,negative,neutral,neutral,neutral,neutral,neutral
1779541379,"Why is this marked as closed?
I understand that I can have custom image built but this bug report was about working_dir functionality. Which in my opinion has some issues not covered by any existing tickets - "".zip"" issue and missing http protocol issue.",marked closed understand custom image built bug report functionality opinion covered issue missing protocol issue,issue,negative,negative,neutral,neutral,negative,negative
1779505319,Hmm this seems pretty difficult to pass in the small environment like this (with high memory constraint). I think we should just add this one to release tests,pretty difficult pas small environment like high memory constraint think add one release,issue,negative,negative,neutral,neutral,negative,negative
1779474536,Note: We won't cherry pick this for 2.9 as this is not as critical as we should pick it in the last minute. Let's keep it in the master,note wo cherry pick critical pick last minute let keep master,issue,negative,neutral,neutral,neutral,neutral,neutral
1779432330,"Oh hi, @sven1977 , do you mind update this file instead https://github.com/ray-project/ray/blob/master/release/ray_release/byod/byod_rllib_test.sh. Release tests do not use these app_config anymore as they now use byod to run tests.",oh hi mind update file instead release use use run,issue,negative,neutral,neutral,neutral,neutral,neutral
1779370421,"Hello @sven1977 ! The script, provided by @a-zhenya above is working and has the same issue as I had.  You can use it as fully self-sufficient reproduction script that you can just run and debug on your end.

Kind regards, Alexander. ",hello script provided working issue use fully reproduction script run end kind,issue,positive,positive,positive,positive,positive,positive
1779351999,"Thanks for the quick response @alexeykudinkin .
For redis, we are using this via AWS so we don't have full configuration details.
Our infra team has informed me that we are using one of the default parameter groups, `default.redis5.0.cluster.on`. I think details about this can be found [here](https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/ParameterGroups.Redis.html#ParameterGroups.Redis.5.0).
Additionally, we have enabled `Cluster mode` and `Encryption in transit`. 
",thanks quick response via full configuration infra team informed one default parameter think found additionally cluster mode encryption transit,issue,negative,positive,positive,positive,positive,positive
1779338501,oh interesting... why does it receive the malformed data? (is it reported incorrectly from autoscaler?),oh interesting receive malformed data incorrectly,issue,negative,positive,positive,positive,positive,positive
1779243372,"Hey @gresavage , thanks for filing this. Yes, this discrepancy in the signatures is not ideal. However, just a heads up, we will deprecate completely the view requirement API as we more and more complete the new API stack (and replace it with the existing (but enhanced) Connector API). Also, we have yet to fully rollout support for LSTMs and PPO using the new stack.

To re-activate the old stack for PPO, do on your config object:
```
config.rl_module(_enable_rl_module_api=False).training(_enable_learner_api=False)
```",hey thanks filing yes discrepancy ideal however deprecate completely view requirement complete new stack replace enhanced connector also yet fully support new stack old stack object,issue,positive,positive,positive,positive,positive,positive
1779223362,"Hey @fardinabbasi , the repro script you provided is not sufficient. There are several (non trivial) imports missing.
Could you provide a short, self-sufficient, and single reproduction script that I can just run and debug locally and that will just run without me having to add additional code to it?
Thanks!",hey script provided sufficient several non trivial missing could provide short single reproduction script run locally run without add additional code thanks,issue,negative,negative,neutral,neutral,negative,negative
1779061987,get_request_queue test in windows failure seem related ,test failure seem related,issue,negative,negative,negative,negative,negative,negative
1778910958,"This is not an RLlib issue, but a possible bug in one of our dependencies (minigrid and/or matplotlib). I removed minigrid from the release app-config, it's not required anyways.",issue possible bug one removed release anyways,issue,negative,neutral,neutral,neutral,neutral,neutral
1778909317,"PR in review: https://github.com/ray-project/ray/pull/40656
Tests running right now to confirm ...

cc: @zhe-thoughts ",review running right confirm,issue,negative,positive,positive,positive,positive,positive
1778793719,"I am also facing version mismatch issue:

`
RuntimeError: Python minor versions differ between client and server: client is 3.10.12, server is 3.8.13
`

My ray version is `2.7.0`. Can we easily re-build ray image with python 3.10 version or there are some known issues? ",also facing version mismatch issue python minor differ client server client server ray version easily ray image python version known,issue,negative,positive,positive,positive,positive,positive
1778715224,"> @vitsai @jjyao @rickyyx Should we have this in 2.8 or leave to 2.9? The entire ASv2 will be released in 2.9 right?

This is to fix the issue with product autoscaler. And once we have v1 fixed (in another PR) or when we roll out OSS ASv2 in 2.9, we should have resolved this issue for OSS users as well. ",leave entire right fix issue product fixed another roll resolved issue well,issue,negative,positive,positive,positive,positive,positive
1778686989,"> @AndreKuu For the jobs with ""no ray driver"", did you use ray or have a ray driver in your entrypoint script?

These logs in python-core-driver.log which i thought very important:
```
task_manager.cc:800: Task failed: SchedulingCancelled: Actor creation cancelled.: Type=ACTOR_CREATION_TASK, Language=PYTHON, Resources: {}, function_descriptor={type=PythonFunctionDescriptor, module_name=ray.dashboard.modules.job.job_manager, class_name=JobSupervisor, function_name=__init__, function_hash=397a8101a9524dd68f0e1c899d9cc7bd}, task_id=ffffffffffffffff6f87a64af727fa848eb5b6f301000000, task_name=_ray_internal_job_actor_263212545118446096:JobSupervisor.__init__, job_id=01000000, num_args=8, num_returns=1, depth=1, attempt_number=0, actor_creation_task_spec={actor_id=6f87a64af727fa848eb5b6f301000000, max_restarts=0, max_retries=0, max_concurrency=1000, is_asyncio_actor=1, is_detached=1}, runtime_env_hash=-2092952565, eager_install=1, setup_timeout_seconds=600
```
It looks like that the job supervisor actor was teminated or cancelled by some unknown reason.",ray driver use ray ray driver script thought important task actor creation like job supervisor actor unknown reason,issue,positive,positive,positive,positive,positive,positive
1778683561,"> @AndreKuu For the jobs with ""no ray driver"", did you use ray or have a ray driver in your entrypoint script?

There some ray related code in entrypoint script:
```python
import ray
ray.init(runtime_env=runtime_env, log_to_driver=False)
...
```",ray driver use ray ray driver script ray related code script python import ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1778679883,"looks the data received is malformed, not a release blocker.",data received malformed release blocker,issue,negative,neutral,neutral,neutral,neutral,neutral
1778679456,"> n your entrypoint script?

yes, i'm sure using ray in the job entrypoint. All jobs have the same entrypoint, the other jobs work well.",script yes sure ray job work well,issue,positive,positive,positive,positive,positive,positive
1778601933,"I tried using the `num_cpus=4` argument, It does not help
By the way
ray.data.read_parquet  does not except num_cpus arguments directly and so I thought may be it is being passed as kwargs to pyarrow but it seems pyarrow read table also does not use the argument, I was able to run ray.data.read_parquet with `num_cpus=12` when my machine only has 8 cpus

That is why I think the parameter is not being used in any way",tried argument help way except directly thought may read table also use argument able run machine think parameter used way,issue,negative,positive,positive,positive,positive,positive
1778553808,It's high risk change and only happen in rare cases so we will merge it to master but not cherry pick to 2.8.,high risk change happen rare merge master cherry pick,issue,negative,positive,positive,positive,positive,positive
1778543294,"<img width=""1180"" alt=""Screen Shot 2023-10-25 at 2 31 45 PM"" src=""https://github.com/ray-project/ray/assets/18510752/7e44a8ff-a0e7-4677-9a15-fdc89f3878d0"">

need to fix this before merging it",screen shot need fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1778542937,Hi @psklavos1 you need to make sure `client` is in the sys.path of the Ray worker process so that it can be found. Maybe you can try to add it to `PYTHONPATH` env var.,hi need make sure client ray worker process found maybe try add,issue,negative,positive,positive,positive,positive,positive
1778522957,We will update the lower bound of grpcio to avoid this issue in the first place.,update lower bound avoid issue first place,issue,negative,positive,positive,positive,positive,positive
1778522333,@BhautikDonga can you upgrade the grpcio version to 1.42 and see if it fixes the issue?,upgrade version see issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1778517986,I think core team should take it. ,think core team take,issue,negative,neutral,neutral,neutral,neutral,neutral
1778517338,"I think it may be possible, but I don't know what'd be the semantic to get the value for histogram. I think we need some scoping for this issue",think may possible know semantic get value histogram think need issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1778474151,Good to hear ☺️ Can you also take a look at the PR?,good hear also take look,issue,negative,positive,positive,positive,positive,positive
1778458938,I cannot merge this now. I assume we need an approval from frontend owner? ,merge assume need approval owner,issue,negative,neutral,neutral,neutral,neutral,neutral
1778457643,"Was there any failure or sth like that from actors? When actors only run tasks, it should not touch GCS at all",failure like run touch,issue,negative,negative,negative,negative,negative,negative
1778457395,"Btw, for the answers;

> why is OOM killer reporting 22GB vs 27.8GB?

`27.57GB / 28.80GB (0.95745),` -> actually this seems correct.

> why is gcs server eating a lot more mem at that time, since the workload is steady (infinite loop of making hundreds of 1MB method calls)?

Maybe you can also post the log of gcs_server.out when this happens? ",killer actually correct server eating lot mem time since steady infinite loop making method maybe also post log,issue,negative,positive,neutral,neutral,positive,positive
1778455876,We need to see how we can achieve it with pytorch (it is pattern 2 from this doc https://docs.google.com/document/d/1MYM7ImPQmuEfcKMoK0hx_2h4rBesgMTAwalGrSWscgQ/edit). cc @jonathan-anyscale ,need see achieve pattern doc,issue,negative,neutral,neutral,neutral,neutral,neutral
1778454652,"Ray client won't be deprecated. It will be there, but we don't make further improvement and recommend people to switch to job submission ",ray client wo make improvement recommend people switch job submission,issue,positive,neutral,neutral,neutral,neutral,neutral
1778395198,"It is the CI issue, so we won't mark it as a release blocker. ",issue wo mark release blocker,issue,negative,neutral,neutral,neutral,neutral,neutral
1778393962,"@jonathan-anyscale debugged the issue.

Looks like https://github.com/ray-project/ray/pull/35188#pullrequestreview-1439885335 is exactly happening now.

Basically, when we get a ""LIST JOB""  RPC to GCS, it does 2 things.

1. Get the data from Redis
2. Get the pending # of tasks from all alive drivers via RPC (NumPendingTasks)

And it is what's happening;
1. When a node is killed abruptly from k8s cluster, it doesn't clean up sockets properly (this is an expected behavior), and that causes RPC (NumPendingTasks) to be pending until the keepalive timeout reaches (2+ minutes). 
2. After 2+ minutes, NumPendingTasks replies with a failure (connection timeout OR infamous ""failed to connect to all addresses"", but the state API already failed (because it has 30 seconds timeout).

@jonathan-anyscale will make a simple write up by tomorrow, and let's discuss the right solution. 



",issue like exactly happening basically get list job get data get pending alive via happening node abruptly cluster clean properly behavior pending failure connection infamous connect state already make simple write tomorrow let discus right solution,issue,positive,positive,neutral,neutral,positive,positive
1778358504,The modin test is failing on master branch but civ1 didn't run it due to a bug. Since the test is failing I'll keep it in civ1 for now.,test failing master branch run due bug since test failing keep,issue,negative,negative,negative,negative,negative,negative
1778319515,"The problem is due to the driver processes uses too much memory (200+GB for 1000 partitions), and causes OOM. But I haven't figured out which particular components are using that memory. 
For now, we can workaround this by using a large head node, and setting num_cpus=0 for head node. 
Considering it's been failing for long time, and has a workaround. Maybe let's fix it in 2.9.",problem due driver much memory figured particular memory large head node setting head node considering failing long time maybe let fix,issue,negative,positive,neutral,neutral,positive,positive
1778289299,"Need to temporarily disable the closing of progress bar to take the screenshot in action.
<img width=""1500"" alt=""Screenshot 2023-10-25 at 8 33 06 AM"" src=""https://github.com/ray-project/ray/assets/25240528/44fd0ba9-fa6d-4509-bb87-835e87693315"">
",need temporarily disable progress bar take action,issue,negative,positive,neutral,neutral,positive,positive
1778281563,"> That's a separate issue, right now the output from the head node (which is the output of running ray start --head) is streamed to the user laptop (the one running ray up). The output from the head node lists commands which only make sense on the head node. We can add a message in the output to clarify this in a future PR.

Yes. That makes sense. We need a better solution here. It's really confusing. Can we create an issue to track it? cc: @jjyao 

> Yup, fixed it

Sorry. What did you fix? It seems that this was the same issues as the one above.

> Both cluster launcher and dashboard require ray[default] so I don't think this situation will happen.

Got it. Thanks!
",separate issue right output head node output running ray start head user one running ray output head node make sense head node add message output clarify future yes sense need better solution really create issue track fixed sorry fix one cluster launcher dashboard require ray default think situation happen got thanks,issue,positive,positive,positive,positive,positive,positive
1778273381,@sobiodum Can you please make sure there is minimal repro script with proper formatting so that we can reproduce the issue on our end? ,please make sure minimal script proper reproduce issue end,issue,positive,positive,positive,positive,positive,positive
1778270588,"@jjyao @jonathan-anyscale @rkooo567 

In 2.9, can we add the profiling tab in the job detail page and show a list of profiling files there for people to download, including this type of traces? At least, people can download and visualize by themselves.
<img width=""892"" alt=""Screenshot 2023-10-24 at 5 17 43 PM"" src=""https://github.com/ray-project/ray/assets/9677264/86ae0c6b-3e6e-4625-99d0-960d200ea154"">

The visualization of those traces/files could be the next step


",add tab job detail page show list people type least people visualize visualization could next step,issue,negative,negative,negative,negative,negative,negative
1778267553,@fulacse Can you provide a smaller reproducible example? It is hard to reproduce the behavior on our end. ,provide smaller reproducible example hard reproduce behavior end,issue,negative,negative,negative,negative,negative,negative
1778256448,"> The dry run is already done on A2C. It went ok (all green).

👍 

> All contrib algos' tuned_examples have been moved from rllib into the new respective rllib_contrib/[algo name]/tuned_examples folders

All checklist items are addressed. However, I don't see the original tuned_example yamls and folders deleted, is that intentional? You want to do deletion in another round? 

",dry run already done went green new respective name however see original intentional want deletion another round,issue,positive,positive,neutral,neutral,positive,positive
1778250712,"Failed tests:
core+python ""An error occurred during the fetch of repository 'boost':"" unrelated
linkcheck: unrelated
test_memory_pressure unrelated
k8s operator test unrelated
Linux wheel build failure unrelated (`nvm` error ""Binary download failed"")

",error fetch repository unrelated unrelated unrelated operator test unrelated wheel build failure unrelated error binary,issue,negative,negative,negative,negative,negative,negative
1778239037,running side by side workload 2.7.1optimized vs 3.0(a random commit on master),running side side random commit master,issue,negative,negative,negative,negative,negative,negative
1778209541,"> @rynewang can you handle the premerge failures? Also plz double check if the data tests failing in this CI is also flaky in the master (since they could be related )

ok. I think it's due to too far behind the master && a linter complaint. updated",handle also double check data failing also flaky master since could related think due far behind master linter complaint,issue,negative,negative,negative,negative,negative,negative
1778159412,"Is there a PR now? If ^ is the root cause, I think we dont need to mark it as a blocker (let's fix it as p0 in 29)",root cause think dont need mark blocker let fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1778126929,I think the risk is that it is a memory leak introduced in 2.8 ,think risk memory leak,issue,negative,neutral,neutral,neutral,neutral,neutral
1778126639,@rynewang is it possible to run this test in 2.7 and if it shows a similar mem usage? ,possible run test similar mem usage,issue,negative,neutral,neutral,neutral,neutral,neutral
1778125912,no longer a release blocker. We will remove redis requirement from windows in 2.9,longer release blocker remove requirement,issue,negative,neutral,neutral,neutral,neutral,neutral
1778123951,Let's merge this asap since it is high risk change,let merge since high risk change,issue,negative,positive,positive,positive,positive,positive
1778123718,@rynewang can you handle the premerge failures? Also plz double check if the data tests failing in this CI is also flaky in the master (since they could be related ),handle also double check data failing also flaky master since could related,issue,negative,neutral,neutral,neutral,neutral,neutral
1778120407,This seems a valid bug. It can be worked around by carrying the arguments with the class. Marking it as P2 for now.,valid bug worked around carrying class marking,issue,negative,neutral,neutral,neutral,neutral,neutral
1778117164,"Also the `Preprocessor` API is now deprecated, we suggest use `map_batches(preprocess_fn)` instead. 
I'm closing it for now, feel free to reopen if you still have issues with the new API and the latest Ray.",also suggest use instead feel free reopen still new latest ray,issue,positive,positive,positive,positive,positive,positive
1778111880,"Do you also know which process(es) are using the most RAM? 
Most likely, if you set a larger `num_cpus` for your `read_parquet` (something like `read_parquet(..., num_cpus=4)`, default is 1), it will limit the concurrency and reduce memory usage. ",also know process e ram likely set something like default limit concurrency reduce memory usage,issue,negative,neutral,neutral,neutral,neutral,neutral
1778104546,"From the error message, it looks more likely something is wrong with the contents in your data files, rather than the url format. 
Could you provide a minimal reproducible script for us?  ",error message likely something wrong content data rather format could provide minimal reproducible script u,issue,negative,negative,negative,negative,negative,negative
1778084207,"Thanks, that's a good suggestion. We should add the map function name as the process name. ",thanks good suggestion add map function name process name,issue,positive,positive,positive,positive,positive,positive
1778072167,"The test still might be flakey, but the dashboard doesn't show this up due to some bug. @can-anyscale will check what goes one.

Reopen the ticket. ",test still might dashboard show due bug check go one reopen ticket,issue,negative,negative,negative,negative,negative,negative
1778047043,@rkooo567 @alanwguo which one of you are going to first take a look at this?,one going first take look,issue,negative,positive,positive,positive,positive,positive
1778010928,@rkooo567 would have a better idea. The ray.util.metric is implemented using the C++ prometheus client library under the hood. I'm not familiar with that codepath.,would better idea client library hood familiar,issue,negative,positive,positive,positive,positive,positive
1778006841,@alanwguo could you describe how this would be implemented and whether @vdesai2014 could possibly implement this?,could describe would whether could possibly implement,issue,negative,neutral,neutral,neutral,neutral,neutral
1777930306,"Tests failed on windows

```
ESC_bk;t=1698165733313^G================================== FAILURES ===================================
ESC_bk;t=1698165733313^G___________________ test_get_current_node_num_accelerators ____________________
ESC_bk;t=1698165733313^G
ESC_bk;t=1698165733313^G    def test_get_current_node_num_accelerators():
ESC_bk;t=1698165733313^G        old_dpctl = None
ESC_bk;t=1698165733313^G        if ""dpctl"" in sys.modules:
ESC_bk;t=1698165733313^G            old_dpctl = sys.modules[""dpctl""]
ESC_bk;t=1698165733313^G    
ESC_bk;t=1698165733313^G>       sys.modules[""dpctl""] = __import__(""mock_dpctl_1"")
ESC_bk;t=1698165733313^GE       ModuleNotFoundError: No module named 'mock_dpctl_1'
ESC_bk;t=1698165733313^G
ESC_bk;t=1698165733313^G\\?\C:\Users\ContainerAdministrator\AppData\Local\Temp\Bazel.runfiles_s1j40za1\runfiles\com_github_ray_project_ray\python\ray\tests\accelerators\test_intel_gpu.py:38: ModuleNotFoundError
ESC_bk;t=1698165733313^G___________________ test_get_current_node_accelerator_type ____________________
ESC_bk;t=1698165733313^G
ESC_bk;t=1698165733313^G    def test_get_current_node_accelerator_type():
ESC_bk;t=1698165733313^G        old_dpctl = None
ESC_bk;t=1698165733313^G        if ""dpctl"" in sys.modules:
ESC_bk;t=1698165733313^G            old_dpctl = sys.modules[""dpctl""]
ESC_bk;t=1698165733313^G    
ESC_bk;t=1698165733313^G>       sys.modules[""dpctl""] = __import__(""mock_dpctl_1"")
ESC_bk;t=1698165733313^GE       ModuleNotFoundError: No module named 'mock_dpctl_1'
ESC_bk;t=1698165733313^G
ESC_bk;t=1698165733313^G\\?\C:\Users\ContainerAdministrator\AppData\Local\Temp\Bazel.runfiles_s1j40za1\runfiles\com_github_ray_project_ray\python\ray\tests\accelerators\test_intel_gpu.py:53: ModuleNotFoundError

```",none module none module,issue,negative,neutral,neutral,neutral,neutral,neutral
1777908223,"@JacksonCakes does the issue happen consistently, and does it happen with Ray 2.7.1? If if happens again, can you send a zip of all the logs from the head node (`/tmp/ray/session_latest/logs`)?",issue happen consistently happen ray send zip head node,issue,negative,positive,positive,positive,positive,positive
1777905551,Alternate approach on https://github.com/ray-project/ray/pull/40488. Let's see which one is more promising.,alternate approach let see one promising,issue,negative,positive,neutral,neutral,positive,positive
1777784006,"Going to let it run first without the label to see that the test doesn't run, then re-trigger with the label to see that the test runs",going let run first without label see test run label see test,issue,negative,positive,positive,positive,positive,positive
1777760904,"Logs for OOM after a good 6h of working on the `many_actor_tasks.py` work:

```
Memory on the node (IP: 10.0.30.108, ID: 5a2a082d5ae843d5f7a75365957cbc63ea1ee5c18b112b7e568553b8) where the task (actor ID: 7610140d413102328133bef401000000, name=Actor.__init__, pid=1900, memory used=0.06GB) was running was 27.57GB / 28.80GB (0.95745), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a05567e1855b961abd91ad70da14b4c14774c76da94cd9bdcbd8b556) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.30.108`. To see the logs of the worker, use `ray logs worker-a05567e1855b961abd91ad70da14b4c14774c76da94cd9bdcbd8b556*out -ip 10.0.30.108. Top 10 memory users:
PID     MEM(GB) COMMAND
912     22.01   /home/ray/anaconda3/lib/python3.8/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
897     0.23    python workloads/many_actor_tasks.py
179     0.18    /home/ray/anaconda3/lib/python3.8/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
288     0.08    /home/ray/anaconda3/bin/python -u /home/ray/anaconda3/lib/python3.8/site-packages/ray/dashboard/agen...
85      0.07    /home/ray/anaconda3/bin/python /home/ray/anaconda3/bin/anyscale session web_terminal_server --deploy...
290     0.07    /home/ray/anaconda3/bin/python -u /home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/runti...
230     0.06    /home/ray/anaconda3/bin/python /home/ray/anaconda3/lib/python3.8/site-packages/ray/dashboard/dashboa...
836     0.06    ray::JobSupervisor
73      0.06    /home/ray/anaconda3/bin/python /home/ray/anaconda3/bin/jupyter-lab --allow-root --ip=127.0.0.1 --no-...
1899    0.06    ray::Actor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
Unexpected error occurred: Task was killed due to the node running low on memory.
```

The `gcs_server` eats 22GB memory which OOMs the whole system. Looking at the dash:

![image](https://github.com/ray-project/ray/assets/56065503/7b29cd2a-835a-4ebe-8fab-96860e19fbce)

The node has a steady 15.6GB mem usage until the very last minute (2023-10-21 10:38) the mem jumped to 27.8GB and Ray OOMs.

Q:

- why is OOM killer reporting 22GB vs 27.8GB?
- why is gcs server eating a lot more mem at that time, since the workload is steady (infinite loop of making hundreds of 1MB method calls)?

*Note: this grafana screenshot is for the unit test control plane session, not the real workload session.*",good working work memory node id task actor id memory running memory usage threshold ray worker id recently task see information memory usage node use ray see worker use ray top memory mem command python session deploy ray ray refer documentation address memory issue consider memory node reducing task parallelism per task set enable retry task due adjust kill threshold set environment variable starting ray disable worker killing set environment variable zero unexpected error task due node running low memory eats memory whole system looking dash image node steady mem usage last minute mem ray killer server eating lot mem time since steady infinite loop making method note unit test control plane session real session,issue,negative,positive,positive,positive,positive,positive
1777760094,"The dashboard related latency for `many_nodes` has pretty high variance, not a release blocker I think: 

<img width=""1336"" alt=""image"" src=""https://github.com/ray-project/ray/assets/11676094/ffaa685d-3449-4bf1-9e83-3064db71e02b"">


<img width=""1335"" alt=""image"" src=""https://github.com/ray-project/ray/assets/11676094/fafa1cf3-4af6-440e-a839-041970cc8b6b"">


The dashboard latency for `many_tasks` are expected to increase since we are returning more data (versus before simply a count for tasks dropped) 

<img width=""1316"" alt=""image"" src=""https://github.com/ray-project/ray/assets/11676094/0e5cb594-da91-47f5-860a-5742d4c05b4d"">


",dashboard related latency pretty high variance release blocker think image image dashboard latency increase since data versus simply count image,issue,positive,positive,positive,positive,positive,positive
1777749113,"It doesn't show up in the flakey dashboard somehow. So I manually checked it

https://buildkite.com/ray-project/postmerge/builds/1339#018b5e97-16f8-40b3-8335-c5028969de96
https://buildkite.com/ray-project/postmerge/builds/1358#018b6188-aff6-46a7-9b72-babb71d2f592
https://buildkite.com/ray-project/postmerge/builds/1343#018b5ef5-1dba-40df-a846-171fdbe5c1a3

And they all passed. I think we can remove it as a release blocker.",show dashboard somehow manually checked think remove release blocker,issue,negative,neutral,neutral,neutral,neutral,neutral
1777727526,"It's the same root cause. Each GCP cluster launcher test was creating a new SSH key, and we hit the per-project limit.

To mitigate the issue and unblock the release, I have deleted 200 unused SSH keys from the project.  The test passes now (see screenshot) and should pass for at least a month with no further changes, but today or tomorrow I'll merge a PR that makes all the GCP cluster launcher tests use the same SSH key, which will permanently fix the issue.

<img width=""1353"" alt=""Screenshot 2023-10-24 at 10 41 53 AM"" src=""https://github.com/ray-project/ray/assets/5459654/ad9c0cf9-9ea4-45e3-8fee-b4e760e8606b"">

I'll close this issue because the release is no longer blocked, and track the remaining work here https://github.com/ray-project/ray/issues/40635",root cause cluster launcher test new key hit limit mitigate issue unblock release unused project test see pas least month today tomorrow merge cluster launcher use key permanently fix issue close issue release longer blocked track work,issue,negative,negative,neutral,neutral,negative,negative
1777723619,"> ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

https://console.anyscale-staging.com/o/anyscale-internal/jobs/prodjob_e9jfg33ubd76vpgh6vjtuctfft",task due node running low memory,issue,negative,negative,neutral,neutral,negative,negative
1777690607,@scottsun94 can you please evaluate this and set a priority?,please evaluate set priority,issue,negative,neutral,neutral,neutral,neutral,neutral
1777681884,"+1 to this as, by default, containers can't run with root in OpenShift environments, so a non-root-required mechanism for the detection is important.",default ca run root mechanism detection important,issue,negative,positive,positive,positive,positive,positive
1777650307,"The new ones from today. Still waiting on @GeneDer for 2.7.1 release logs to compare against (instead of 2.7.0)
```
REGRESSION 52.83%: dashboard_p95_latency_ms (LATENCY) regresses from 47.45 to 72.519 (52.83%) in 2.8.0/benchmarks/many_nodes.json
REGRESSION 51.28%: dashboard_p95_latency_ms (LATENCY) regresses from 6729.761 to 10180.702 (51.28%) in 2.8.0/benchmarks/many_tasks.json
REGRESSION 50.84%: dashboard_p99_latency_ms (LATENCY) regresses from 13941.91 to 21030.085 (50.84%) in 2.8.0/benchmarks/many_tasks.json
REGRESSION 20.19%: 1000000_queued_time (LATENCY) regresses from 181.82263824499995 to 218.523912088 (20.19%) in 2.8.0/scalability/single_node.json
REGRESSION 19.50%: multi_client_put_gigabytes (THROUGHPUT) regresses from 38.605668097256924 to 31.07867911026417 (19.50%) in 2.8.0/microbenchmark.json
REGRESSION 16.66%: dashboard_p50_latency_ms (LATENCY) regresses from 5.534 to 6.456 (16.66%) in 2.8.0/benchmarks/many_tasks.json
REGRESSION 15.31%: dashboard_p99_latency_ms (LATENCY) regresses from 143.498 to 165.464 (15.31%) in 2.8.0/benchmarks/many_nodes.json
REGRESSION 13.45%: dashboard_p50_latency_ms (LATENCY) regresses from 3.465 to 3.931 (13.45%) in 2.8.0/benchmarks/many_nodes.json
REGRESSION 12.98%: stage_2_avg_iteration_time (LATENCY) regresses from 58.160910558700564 to 65.70907316207885 (12.98%) in 2.8.0/stress_tests/stress_test_many_tasks.json
REGRESSION 10.61%: 10000_get_time (LATENCY) regresses from 23.55671212599998 to 26.055165431999995 (10.61%) in 2.8.0/scalability/single_node.json
REGRESSION 10.54%: single_client_tasks_async (THROUGHPUT) regresses from 10739.407361558973 to 9607.186982064028 (10.54%) in 2.8.0/microbenchmark.json
REGRESSION 10.34%: single_client_get_object_containing_10k_refs (THROUGHPUT) regresses from 14.755162462843568 to 13.228837029499449 (10.34%) in 2.8.0/microbenchmark.json
REGRESSION 10.23%: time_to_broadcast_1073741824_bytes_to_50_nodes (LATENCY) regresses from 70.13534577099995 to 77.31193332200007 (10.23%) in 2.8.0/scalability/object_store.json
REGRESSION 9.35%: 1_n_async_actor_calls_async (THROUGHPUT) regresses from 9124.377528222414 to 8271.416009416063 (9.35%) in 2.8.0/microbenchmark.json
REGRESSION 8.75%: multi_client_tasks_async (THROUGHPUT) regresses from 28423.644858766176 to 25935.554390623118 (8.75%) in 2.8.0/microbenchmark.json
REGRESSION 8.53%: single_client_tasks_sync (THROUGHPUT) regresses from 1311.812164358857 to 1199.8831112257556 (8.53%) in 2.8.0/microbenchmark.json
REGRESSION 8.22%: stage_1_avg_iteration_time (LATENCY) regresses from 23.305240750312805 to 25.220864820480347 (8.22%) in 2.8.0/stress_tests/stress_test_many_tasks.json
REGRESSION 7.63%: 1_n_actor_calls_async (THROUGHPUT) regresses from 10133.72696574923 to 9360.803502103265 (7.63%) in 2.8.0/microbenchmark.json
REGRESSION 7.58%: n_n_actor_calls_async (THROUGHPUT) regresses from 30847.92669705198 to 28510.050783675328 (7.58%) in 2.8.0/microbenchmark.json
REGRESSION 5.89%: stage_3_time (LATENCY) regresses from 2802.1650245189667 to 2967.23273229599 (5.89%) in 2.8.0/stress_tests/stress_test_many_tasks.json
REGRESSION 4.69%: n_n_async_actor_calls_async (THROUGHPUT) regresses from 25688.484755543966 to 24484.174826322916 (4.69%) in 2.8.0/microbenchmark.json
REGRESSION 4.23%: 3000_returns_time (LATENCY) regresses from 5.6602293089999876 to 5.899374877999989 (4.23%) in 2.8.0/scalability/single_node.json
REGRESSION 3.91%: actors_per_second (THROUGHPUT) regresses from 748.5322140167257 to 719.3018798022547 (3.91%) in 2.8.0/benchmarks/many_actors.json
REGRESSION 3.63%: client__1_1_actor_calls_concurrent (THROUGHPUT) regresses from 1080.2139341634759 to 1040.99657229968 (3.63%) in 2.8.0/microbenchmark.json
REGRESSION 3.46%: n_n_actor_calls_with_arg_async (THROUGHPUT) regresses from 3074.0790016310475 to 2967.685662012421 (3.46%) in 2.8.0/microbenchmark.json
REGRESSION 3.40%: avg_iteration_time (LATENCY) regresses from 1.4622855401039123 to 1.5120103502273559 (3.40%) in 2.8.0/stress_tests/stress_test_dead_actors.json
REGRESSION 3.10%: 1_1_actor_calls_concurrent (THROUGHPUT) regresses from 4745.83263563276 to 4598.670938618477 (3.10%) in 2.8.0/microbenchmark.json
REGRESSION 3.03%: single_client_tasks_and_get_batch (THROUGHPUT) regresses from 9.369535279594958 to 9.086079282777337 (3.03%) in 2.8.0/microbenchmark.json
REGRESSION 2.91%: 10000_args_time (LATENCY) regresses from 16.89121779300001 to 17.381954480000005 (2.91%) in 2.8.0/scalability/single_node.json
REGRESSION 2.85%: client__1_1_actor_calls_async (THROUGHPUT) regresses from 1083.9300708022135 to 1053.0442301044923 (2.85%) in 2.8.0/microbenchmark.json
REGRESSION 2.84%: tasks_per_second (THROUGHPUT) regresses from 272.7469880191856 to 265.0085938319898 (2.84%) in 2.8.0/benchmarks/many_nodes.json
REGRESSION 2.68%: stage_4_spread (LATENCY) regresses from 0.7217020493267903 to 0.7410596228217868 (2.68%) in 2.8.0/stress_tests/stress_test_many_tasks.json
REGRESSION 2.60%: client__put_gigabytes (THROUGHPUT) regresses from 0.13283428838343245 to 0.12937928916286368 (2.60%) in 2.8.0/microbenchmark.json
REGRESSION 2.32%: single_client_put_calls_Plasma_Store (THROUGHPUT) regresses from 5766.661045557541 to 5632.668588928769 (2.32%) in 2.8.0/microbenchmark.json
REGRESSION 1.97%: client__tasks_and_get_batch (THROUGHPUT) regresses from 1.002041264301031 to 0.9822717209888673 (1.97%) in 2.8.0/microbenchmark.json
REGRESSION 1.55%: client__1_1_actor_calls_sync (THROUGHPUT) regresses from 573.4457553242221 to 564.5765462438006 (1.55%) in 2.8.0/microbenchmark.json
REGRESSION 0.83%: single_client_wait_1k_refs (THROUGHPUT) regresses from 5.50854610986549 to 5.462989777344944 (0.83%) in 2.8.0/microbenchmark.json
REGRESSION 0.83%: avg_pg_create_time_ms (LATENCY) regresses from 0.9287227387393717 to 0.9363928228234366 (0.83%) in 2.8.0/stress_tests/stress_test_placement_group.json
REGRESSION 0.64%: tasks_per_second (THROUGHPUT) regresses from 443.2356047821634 to 440.39063366884034 (0.64%) in 2.8.0/benchmarks/many_tasks.json
```",new today still waiting release compare instead regression latency regression latency regression latency regression latency regression throughput regression latency regression latency regression latency regression latency regression latency regression throughput regression throughput regression latency regression throughput regression throughput regression throughput regression latency regression throughput regression throughput regression latency regression throughput regression latency regression throughput regression throughput regression throughput regression latency regression throughput regression throughput regression latency regression throughput regression throughput regression latency regression throughput regression throughput regression throughput regression throughput regression throughput regression latency regression throughput,issue,negative,positive,positive,positive,positive,positive
1777630915,"Hi @NripeshN,

Thanks for the contribution. As the first step, we need to support Apple silicon GPU in Ray core. Could you follow https://github.com/ray-project/ray/pull/38553 as an example to update the PR? Thanks!",hi thanks contribution first step need support apple silicon ray core could follow example update thanks,issue,positive,positive,positive,positive,positive,positive
1777621080,I am seeing the same issue with Ray 2.7.1 on multiple node cluster. What's worse is that during this time when I submit jobs (not using the --no-wait flag) the job also hangs for minutes. This is pretty easy to reproduce and is highly visible.,seeing issue ray multiple node cluster worse time submit flag job also pretty easy reproduce highly visible,issue,negative,positive,neutral,neutral,positive,positive
1777612299,"Closing this issue as aim 4 was yanked from pypi, feel free to create a new issue if any new issues come up!",issue aim feel free create new issue new come,issue,positive,positive,positive,positive,positive,positive
1777591455,mobilenet test has passed 4 times now w/o ever being flakey. Seems to have worked.,test time ever worked,issue,negative,neutral,neutral,neutral,neutral,neutral
1777585677,"cc: @kouroshHakha , this is ready for final review. Also tests pending ...",ready final review also pending,issue,negative,positive,neutral,neutral,positive,positive
1777585195,"After latest update:

* All contrib algos' tuned_examples have been moved from rllib into the new respective `rllib_contrib/[algo name]/tuned_examples` folders
* All contrib algos' `tests/test_[algo name].py` files have been moved from rllib into the respective `rllib_contrib/[algo name]/tests` folders
* All contrib algos' learning tests and compilation tests have been removed from rllib's BUILD file and into the respective `rllib_contrib/[algo name]/BUILD` file.
* in `.buildkite/pipeline.ml.yml`, all contrib algos now have a couple of extra commands:
* * copy the `run_regression_tests.py` file from the installed ray[rllib] into a local directory
* * run the rllib_contrib's algos already existing example
* * run the rllib_contrib's algos BUILD file (which includes all learning- and compilation tests) with bazel",latest update new respective name name respective name learning compilation removed build file respective name file couple extra copy file ray local directory run already example run build file compilation,issue,positive,positive,neutral,neutral,positive,positive
1777542888,"After a discussion with @kevin85421, we decided to hold off on updating KubeRay for now. KubeRay v1.1.0 will target Ray 2.8.0 and forwards, so we can add it for that release.

The Serve docs and Serve CLI have been updated to use the new port.",discussion decided hold target ray forward add release serve serve use new port,issue,negative,positive,positive,positive,positive,positive
1777461800,"I’ve  narrowed down my issues to a leaky environment (or at least, my environment leaks enough to probably be more significant than the algorithm). @jesuspc how did you trigger the process restore from checkpoint? Would be a good workaround for me until I’m able to track down my environment leak. ",leaky environment least environment enough probably significant algorithm trigger process restore would good able track environment leak,issue,positive,positive,positive,positive,positive,positive
1777441910,"I also see a memory leak during RLlib training, using SAC in combination with PettingZoo MPE. 

Even for longer runs of over 5 days the memory keeps growing linearly, adding over 70GB on top of the already quite great memory consumption right from the beginning. 

Version Details for my Docker Image: 
- Ray: 2.4.0
- torch: 1.8.1+cu11
- PettingZoo 1.22.3

Reproduction Script for Minimal Example 1): 
```python
import os 

import ray 
from ray import air
from ray.rllib.algorithms.sac import SAC, SACConfig

from ray.rllib.env.wrappers.pettingzoo_env import PettingZooEnv
from pettingzoo.mpe import simple_v2

from ray.tune.registry import register_env

# determine checkpoint directory
Dir = os.getcwd() 
checkpoint_dir = os.path.join(Dir, 'ray_results')

#### Environment Setup
def env_creator(env_config): 
        env = PettingZooEnv(simple_v2.env(**env_config))
        return env

register_env(""simple_v2"", env_creator)

#### Initialize Ray
ray.init(local_mode=False, num_gpus=1, logging_level=""info"")

# hyperparameter search 
config = (SACConfig()
            .training(replay_buffer_config = {
                          ""capacity"": int(1e6) 
                      },
            )
            .rollouts(num_rollout_workers=4) 
            .resources(num_gpus=1)
            .environment(env=""simple_v2"")
            .framework('torch')
)

# ``Tuner.fit()`` allows setting a custom log directory (other than ``~/ray-results``)
tuner = ray.tune.Tuner(
    SAC,
    param_space=config,
    run_config=air.RunConfig(
        stop={""training_iteration"": 500},
        checkpoint_config=air.CheckpointConfig(checkpoint_at_end=True,
                                               checkpoint_frequency=300),
        local_dir = checkpoint_dir
    ),
)

# run the experiment trial until the stopping criteria is met
results = tuner.fit() 

```

Grafana showing the rss memory usage going from 46.9 GB to 53.4 GB, i.e. having an increase of 6.5 GB. 
![image](https://github.com/ray-project/ray/assets/9974081/d2ad9527-f2cf-4bfb-9a46-a56e01b9bcda)

I also tried to reduce the number of workers or the replay memory size, but this didn't make the problem go away: 
2) `num_rollout_workers=0` instead of `4` 
==> Increase of 1.2 GB for the single local worker. 
3) `capacity: int(10)` instead of the default of `int(1e6)`
==> Increase of 1.6 GB over local & remote workers

The order of the execution for the chart was 1) -> 2) -> 3) with starting times of 14:06, 14:38 and 16:17 respectively. 
See the full Grafana chart below: Even with the pretty small replay buffer, the memory is still increasing significantly. 

![image](https://github.com/ray-project/ray/assets/9974081/7a9be371-d1e3-450c-a1fc-aa56c7500c06)

Any ideas what is happening here? 
",also see memory leak training sac combination even longer day memory growing linearly top already quite great memory consumption right beginning version docker image ray torch reproduction script minimal example python import o import ray ray import air import sac import import import determine directory environment setup return initialize ray search capacity setting custom log directory tuner sac run experiment trial stopping criterion met showing memory usage going increase image also tried reduce number replay memory size make problem go away instead increase single local capacity instead default increase local remote order execution chart starting time respectively see full chart even pretty small replay buffer memory still increasing significantly image happening,issue,positive,positive,positive,positive,positive,positive
1777390062,"> Looks all very good already! :) Just a few missing docstrings and other nits.
> 
> Note: I'd be happy to merge this (once comments are addressed) and do the multi-agent episode tests in a follow up PR to not explode this one too much.

@sven1977 Yes I like this approach. I will focus on the comments and make the MAE test in another one ",good already missing note happy merge episode follow explode one much yes like approach focus make mae test another one,issue,positive,positive,positive,positive,positive,positive
1777336720,"Happy for the contribution. I am excited about the new sampling API and how it will improve learning performance and user experience. Thanks for the great input @sven1977, @ArturNiederfahrenhorst and @kouroshHakha ",happy contribution excited new sampling improve learning performance user experience thanks great input,issue,positive,positive,positive,positive,positive,positive
1777088865,"I think that when using RAY_AIR_RICH_LAYOUT=1 
It looks great!
One issue is that the numbers are sometimes displayed like this 0.0999999999 instead of 0.1
I suggest changing line  894 
in python/ray/tune/experimental/output.py
from this:
```
            for trial_info in trial_infos:
                table_trial.add_row(*[str(_) for _ in trial_info])
```
to this:
```
            for trial_info in trial_infos:
                table_trial.add_row(*[f'{_:.6}' if isinstance(_, float) else str(_) for _ in trial_info])
```

Voila! they all looking great!",think great one issue sometimes displayed like instead suggest line float else looking great,issue,positive,positive,positive,positive,positive,positive
1777030132,@woshiyyya would you take a look please,would take look please,issue,negative,neutral,neutral,neutral,neutral,neutral
1776699527,"Seeing memory leaks in TD3 as well...

<img width=""1349"" alt=""Screenshot 2023-10-24 at 07 49 57"" src=""https://github.com/ray-project/ray/assets/2353002/cfa9cab0-74a6-40ce-a873-b533c6971b3f"">

The drop corresponds to a process restore from checkpoint. ",seeing memory well drop process restore,issue,negative,neutral,neutral,neutral,neutral,neutral
1776625931,"The dry run is already done on A2C. It went ok (all green).
I'll do all the others now to see what it would look like in the end.
I'll also try to somehow make `run_release_tests` importable from rllib itself, even though this could be tricky as we have rllib pinned to 2.5, so we cannot change anything anymore.",dry run already done went green see would look like end also try somehow make importable even though could tricky pinned change anything,issue,negative,negative,negative,negative,negative,negative
1776557274,Let me forge merge this and remove the flaky test in another pick,let forge merge remove flaky test another pick,issue,negative,neutral,neutral,neutral,neutral,neutral
1776538716,"Ahh, how to bootstrap initial access. Sharing general advice here for future readers: I'd punt the ""how do I get the first secret"" into your cloud provider because they've already solved that problem.

I'd like to get you unblocked, but that probably involves chatting about the specifics of your setup. Do you mind shooting me an email to <thomas at anyscale.com> so we discuss?",bootstrap initial access general advice future punt get first secret cloud provider already problem like get unblocked probably chatting setup mind shooting discus,issue,negative,negative,neutral,neutral,negative,negative
1776531272,"How can I access the external secret manager from my ray job without the auth token? And I cannot mount the auth token on the pod as they expire. Only way I can achieve this is by creating a new ray cluster for every job, please correct me if I am wrong.

Using external secret manager is what I want to do, I wouldn't trust anything else to store secrets. I am just hoping to add plumbing to the ray jobs so users can pass the auth token to the job and then the job can access the secrets from the secret manager. So what I really mean by ""secrets"" in this change is just the auth token.",access external secret manager ray job without token mount token pod expire way achieve new ray cluster every job please correct wrong external secret manager want would trust anything else store add plumbing ray pas token job job access secret manager really mean change token,issue,negative,negative,negative,negative,negative,negative
1776530966,"Thanks for that clarification :D

Even with that in mind, its still kinda a secret store because something within Ray has to hold onto these values for tasks or hosts that haven't started yet unless the secrets are externalized from Ray and held elsewhere.",thanks clarification even mind still secret store something within ray hold onto yet unless ray elsewhere,issue,negative,negative,neutral,neutral,negative,negative
1776522866,"Okay, finished reading & thinking. I think something like this is going to be pretty hard to build into Ray as a first class citizen.

As briefly mentioned before, Secret management presents a pretty long list of challenges they need to solve. If you're curious there was a nice talk at AppSec 2015 on this ([slides](http://schd.ws/hosted_files/appsecusa2015/a5/Turtles.pdf) [video](https://www.youtube.com/watch?v=OUSvv2maMYI)).

Also, more Ray specific, Ray's current design is built around a single global code execution domain. It is extremely difficult to create effective boundaries inside a code execution domain (as evidenced by the amount of effort that has been spent trying to stop exploits in browsers and hypervisors). So, unfortunately, exposing secrets to a cluster exposes them to everything that has run on the cluster, and probably everything that will run on that cluster. Until we can add some form of effective internal isolation into a Ray, I would discourage exposing a ""job-scoped secret"" concept since it can't actually be achieved. We’ve discussed on and off for years what it would take but this likely isn’t a near-term change. xD

The solution I'd strongly encourage is to lean into whatever existing secret management tools you have at your disposal (if you don’t have one the cloud provider’s tools are decent); keeping in mind that the smallest unit of isolation you can possibly rely on is the cluster level.

If we were to build something and label it a ""secret"" system, I'd want to be able to have a path to doing a decent job at it and I don't see a path to that right now without structural changes to Ray. I don't want perfect to be the enemy of the good enough though so if anyone has any idea they like please share; but I am cautious about having an API that can’t do what it seems to plainly offer.

But if you're happy with the properties the e2e encrypted blob idea presents, you can totally do that today without code changes to Ray. With one caveat, I'd probably toss the encrypted blobs somewhere in RuntimeEnv, but just be cautious about where you're storing the runtime env (while it defaults to in GCS volatile memory, it can also easily become somewhere semi-permanent like a cloud blob store).

That said, I’d still encourage you to use your existing answer to secrets management as you normally would (but again, keeping in mind Ray’s isolation boundary is the cluster lifetime).",finished reading thinking think something like going pretty hard build ray first class citizen briefly secret management pretty long list need solve curious nice talk video also ray specific ray current design built around single global code execution domain extremely difficult create effective inside code execution domain amount effort spent trying stop unfortunately cluster everything run cluster probably everything run cluster add form effective internal isolation ray would discourage secret concept since ca actually would take likely change solution strongly encourage lean whatever secret management disposal one cloud provider decent keeping mind unit isolation possibly rely cluster level build something label secret system want able path decent job see path right without structural ray want perfect enemy good enough though anyone idea like please share cautious plainly offer happy blob idea totally today without code ray one caveat probably toss somewhere cautious volatile memory also easily become somewhere like cloud blob store said still encourage use answer management normally would keeping mind ray isolation boundary cluster lifetime,issue,positive,positive,positive,positive,positive,positive
1776522582,Looks like a real test failure in premerge (for the last 3 tries) preventing merge.,like real test failure last merge,issue,negative,negative,neutral,neutral,negative,negative
1776522274,"Hey @thomasdesr just to be clear, we are not proposing a secret store here, just trying to build a pass-through plumbing in the ray jobs so users have a way to submit secrets to ray jobs. These secrets are not supposed to be stored anywhere, just used in the job and then discarded. ",hey clear secret store trying build plumbing ray way submit ray supposed anywhere used job,issue,negative,negative,negative,negative,negative,negative
1776505535,"> This might alleviate the issue: #40615

Looks reasonable for me. The red failures were from dashboard not being properly setup which failed other test cases.

For the yellow failures, it's caused by `test_dedup_logs()` which still being investigated",might alleviate issue reasonable red dashboard properly setup test yellow still,issue,negative,positive,neutral,neutral,positive,positive
1776424361,@sven1977 could you update an ETA of fixing? Thanks,could update eta fixing thanks,issue,negative,positive,positive,positive,positive,positive
1776321006,"Hey!

Just catching up but I'd prefer if we took a bit more time on this. Maybe start a REP for the design?

Secret stores are a pretty complex space and we should try to have some sort of intentional answer for a number of capabilities beyond confidentiality. E.g. Auditability (who accessed what, when) and Authorization (should X be allowed access to Y), neither of which are currently touched upon in this proposal.",hey catching prefer took bit time maybe start rep design secret pretty complex space try sort intentional answer number beyond confidentiality authorization access neither currently touched upon proposal,issue,positive,positive,neutral,neutral,positive,positive
1776273402,"> @rickyyx did you manage to reproduce this?

Hey - will do. Having a few interrupts with recent releases! Sorry. ",manage reproduce hey recent sorry,issue,negative,negative,negative,negative,negative,negative
1776267639,"I don't think this needs to be a release blocker. When Redis is backing GCS its contents can trigger RCE within the cluster already so it isn't an increase in risk to go unpatched.

But it would still be good to fix this sooner rather than later. While it goes unpatched its going to continue to trigger dependency scanners.",think need release blocker backing content trigger within cluster already increase risk go unpatched would still good fix sooner rather later go unpatched going continue trigger dependency,issue,negative,positive,positive,positive,positive,positive
1776264014,"Obfuscated Base64 encoding is absolutely no substitute for proper cryptography. I'd suggest to either use e.g. [SealedBox](https://pynacl.readthedocs.io/en/latest/public/#nacl-public-sealedbox)[^1] or just assume the risks of plain text and mask[^2] the sensitive values on logs.

[^1]: See for instance [how GitHub uses `libsodium` sealed boxes for Actions Secrets](https://docs.github.com/en/rest/guides/encrypting-secrets-for-the-rest-api?apiVersion=2022-11-28).
[^2]: It would be nice to mask sensitive values on logs and similar information, regardless of the chosen approach. Note, though, that secret masking isn't precisely trivial; see e.g. [this GitHub Actions Runner test](https://github.com/actions/runner/blob/121f080023450ab0ee5fc2d7961d5b630434d41f/src/Test/L0/Worker/ActionCommandManagerL0.cs#L424-L444) for an example.",base absolutely substitute proper cryptography suggest either use assume plain text mask sensitive see instance sealed would nice mask sensitive similar information regardless chosen approach note though secret precisely trivial see runner test example,issue,negative,negative,neutral,neutral,negative,negative
1776256985,"The increase bump e44657390b375e703d1d164b2c26467d8e713e63 to 035224b52558047d2bb21df415ca75b2f91f0d4c (9.13 -> 9.14)

`035224b525 [Serve] Make sure Ray installs uvloop to be used as event-loop implementation for asyncio (#39336)
4ed4b52531 [docs][train]Make Train example titles, heading more consistent (#39606)
77b4cb902d [core] add grpc opencensus plugin (#39082)
1225d52f64 [docs][clusters] Change title of RayService doc to Deploy Ray Serve Apps (#39641)
b00d0299e8 [Doc] Fix Title of the Transformers GLUE example (#39605)
5cd72b9ff9 [tune/docs] typo in suggestion.rst (#39262)
7414a8d877 Downgrade grpc from 1.57.0 to 1.50.2 (#39575)
8094bdab16 [release][core][autoscaler] Autoscaler e2e release tests [1/x]  (#39046)
1da1834c5b [docs] Update KubeRay Ingress Docs (#39635)
3e49f5d768 [data][tests-only] Fix nightly microbenchmark for read_images #39609
ed324503b4 [data] store bytes spilled/restored after plan execution (#39361)
8d80377445 [RLlib] Fixed 'rollout_fragment_length' in pong-example by setting it to 'auto'. (#39552)

So the downgrade doesn't fixes everything I guess? ",increase bump serve make sure ray used implementation train make train example heading consistent core add change title doc deploy ray serve doc fix title glue example typo ad downgrade release core release update ingres data fix nightly data store plan execution fixed setting downgrade everything guess,issue,positive,positive,positive,positive,positive,positive
1776253809,"The drop between 64c25cfe00904cea0ddcad283894a6110bc307e3 and 2913e9b9715d26def3246cddcb43904410bdd60e (from 9.7 -> 9.11) 


2913e9b971 [train] Legacy interface cleanup (`air.Checkpoint`, `LegacyExperimentAnalysis`) (#39289)
fddde50e7e [train] remove _max_cpu_fraction_per_node (#39412)
7da798f5f9 [Doc] Add an ad for Ray Summit 2023 (#39404)
daf0dc18cc polish observability (o11y) docs (#39069)
41cb27392f [2.7] Cleanup all LightningTrainer Mentions in Ray Doc (#39406)
5867f3291e [Data] Unpin pyarrow from test-requirements (#39290)
275fad8fa4 jail //python/ray/data:test_streaming_executor (#39423)
fb4dd924da [train] update Train API references & annotations (#39294)
b6edccfd97 [core] Fix performance regression in single_client_tasks_and_get_batch (#39362)
0f5b6f53b8 Update usage_pb2.py (#39425)
0e77916b04 [release byod] fix ml requirements file selection (#39353)
364df49068 Update metrics.md (#38512)
c2d6f54833 [Dashboard/Client] Update docs to Reflect Best Practices (#39403)
b1356d71e9 [Core] Merge Driver/Job's runtime environment when it conflicts (#39208)
07d6e67a74 [Core] Upgrade grpc from 1.46.6 to 1.57.0 (#39210)
449afc9461 [Telemetry] Add Telemetry for Ray Train Utilities (#39363)
3e8a1dc606 [tune] Make Trainable.save/restore developer APIs (#39391)
3e7c8af1c5 [Serve][Doc] Add handle instruction to send multiplex request (#39274)
fecca87536 [Doc] Add vSphere cluster configuration reference with examples (#39379)
1491937d84 [ci] marking serve:test_websockets as failing (#39375)
c324f38ea1 [RLlib] Fix DDPG learning/release tests and make MARWIL CI test criterium more difficult. (#39386)
f33b8ebba6 [train] Fix issues in migration of tune_cifar_torch_pbt_example (#39158)
8b7fcd7bb8 [Core] Allow to rate limit the max # of workers concurrently started (#39253)
6523d94049 skip gcs-ha-e2e-2 for now (#39359)
77412abfd3 [civ2][serve/1] migrate other variances of serve tests to civ02 (#39045)



I guess grpc is the culprit here? ^",drop train legacy interface cleanup train remove daff doc add ad ray summit polish observability cleanup ray doc fe data unpin jail train update train core fix performance regression update release fix file selection update update reflect best core merge environment core upgrade telemetry add telemetry ray train tune make developer serve doc add handle instruction send multiplex request doc add cluster configuration reference marking serve failing fix make test criterium difficult train fix migration core allow rate limit concurrently skip migrate serve guess culprit,issue,negative,positive,positive,positive,positive,positive
1776235159,"Will run again after cherry picks, should give more signal on those edge of distribution values",run cherry give signal edge distribution,issue,negative,neutral,neutral,neutral,neutral,neutral
1776231178,"> For `avg_iteration_time`, the value of 1.65 on release branch is out of distribution though

Yeah, but given the release branch is just a prefix of the master branch, I belive it's more of flakiness. Could we rerun it?",value release branch distribution though yeah given release branch prefix master branch belive flakiness could rerun,issue,positive,neutral,neutral,neutral,neutral,neutral
1776231017,"we can cram in more if some of the tests are IO intensive and some are CPU intensive, but this is fine.",cram io intensive intensive fine,issue,negative,positive,positive,positive,positive,positive
1776230331,`dashboard_p99_latency_ms` and other regressions are related to the GCS task backend. It's expected since we do have more data to return while it was all dropped from the previous code. ,related task since data return previous code,issue,negative,negative,neutral,neutral,negative,negative
1776227702,"For `avg_iteration_time`, the value of 1.65 on release branch is out of distribution though",value release branch distribution though,issue,negative,neutral,neutral,neutral,neutral,neutral
1776178831,"`single_client_tasks_sync` seems real even though not significant. 

<img width=""1329"" alt=""image"" src=""https://github.com/ray-project/ray/assets/11676094/143035fc-c637-4588-a60f-ffd946b42bf0"">
",real even though significant image,issue,negative,positive,positive,positive,positive,positive
1776177449,"`stage_1_avg_iteration_time` seems there's some regression, even though not huge. But looks obvious. 
<img width=""1341"" alt=""image"" src=""https://github.com/ray-project/ray/assets/11676094/510a9f4d-7050-41c3-8131-e0474728e804"">
",regression even though huge obvious image,issue,negative,positive,positive,positive,positive,positive
1776164887,"Linkcheck and chaos test and HA tests are unrelated, this PR only touches the vSphere cluster launcher.",chaos test ha unrelated cluster launcher,issue,negative,neutral,neutral,neutral,neutral,neutral
1776161538,"Thanks @aslonnie, let me investigate if we still need this in the wheel build in a separate PR so we can unblock the Ray Serve work :)",thanks let investigate still need wheel build separate unblock ray serve work,issue,negative,positive,positive,positive,positive,positive
1776158378,"`multi_client_tasks_async` is similar to `1_n_actor_calls_async` it never recovers from the dip. 

<img width=""1335"" alt=""image"" src=""https://github.com/ray-project/ray/assets/11676094/55d6e71a-2799-428f-aa6c-04348ab8db31"">
",similar never dip image,issue,negative,neutral,neutral,neutral,neutral,neutral
1776156400,"Created an issue to optimize the error message https://github.com/ray-project/ray/issues/40600.
Closing this issue for now. Feel free to reopen if there are other issues. ",issue optimize error message issue feel free reopen,issue,positive,positive,positive,positive,positive,positive
1776154398,"`1_n_actor_calls_async` is probably rpc related, but it never recovers.  There seems to be  ~10% regression 
<img width=""1330"" alt=""image"" src=""https://github.com/ray-project/ray/assets/11676094/59cdb80b-6a9d-4060-b7fb-19473c5e2809"">
",probably related never regression image,issue,negative,neutral,neutral,neutral,neutral,neutral
1776150708,"`avg_iteration_time` seems to be flaky - and looks like not in the master branch, could be infra errors? 
<img width=""1367"" alt=""image"" src=""https://github.com/ray-project/ray/assets/11676094/f501bb26-5ca2-4d8e-b56a-ed9fe06b1e8f"">
",flaky like master branch could infra image,issue,negative,neutral,neutral,neutral,neutral,neutral
1776149097,"`1000000_queued_time` probably does have a regression, but it seems less serious than 20%, seems to be 185 -> 200, ~8%

<img width=""1337"" alt=""image"" src=""https://github.com/ray-project/ray/assets/11676094/a7d8c20e-5570-4160-b397-ac7282357721"">
",probably regression le serious image,issue,negative,negative,negative,negative,negative,negative
1776146379,">   File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/data/_internal/execution/operators/actor_pool_map_operator.py"", line 109, in start
    ray.get(refs, timeout=DEFAULT_WAIT_FOR_MIN_ACTORS_SEC)
    
This error means that the not all actors have started within the 10 minutes timeout. 
It's mostly likely because you don't have enough resources for the requested actor pool. ",file line start error within mostly likely enough actor pool,issue,negative,neutral,neutral,neutral,neutral,neutral
1776128042,"Closing this as stale, please reopen if the issue is still present.",stale please reopen issue still present,issue,negative,negative,negative,negative,negative,negative
1776122775,"Closing this since prefix stripping logic has been removed in 2.7, and the user has full flexibility of what stripped/unstripped data to save in the checkpoint.",since prefix stripping logic removed user full flexibility data save,issue,positive,positive,positive,positive,positive,positive
1776117749,"@Geoffreyvd1 can you clarify what your cluster configuration looks like, and what the output of `ray status` shows when you do this? `memory` is treated as a virtual resource and will only limit the scheduling if it cannot fit any more on your cluster. ",clarify cluster configuration like output ray status memory virtual resource limit fit cluster,issue,positive,positive,positive,positive,positive,positive
1776090435,"> Can't we just have both?

hard to fit in the UI. Do you think both is better? I guess I can do raysubmit_12345 (01000000)",ca hard fit think better guess,issue,positive,positive,positive,positive,positive,positive
1776076918,"@zhe-thoughts yes, the plan is to implement this the basic functionality but disable it by default in 2.8, so it's easier for us to do some internal tests and get feedbacks before officially enabling it in 2.9.
There will be another Data PR. both should be safe to cherry-pick, as the new code is disabled by default. ",yes plan implement basic functionality disable default easier u internal get officially another data safe new code disabled default,issue,positive,positive,neutral,neutral,positive,positive
1776072123,"I believe all failed release tests on the release branch are blockers, yes.",believe release release branch yes,issue,negative,neutral,neutral,neutral,neutral,neutral
1776069977,"The action items to be addressed are:
- [ ] 1. Remove the hard requirement check for Windows pyarrow < 7
- [ ] 2. Add runtime check for Pyarrow version >= 7 in `Dataset.__init__()` (or other TBD appropriate code path)
- [ ] 3. Handle any remaining behavior gap related to `_register_arrow_data_serializer()`, and include an environment variable to override the Pyarrow version check",action remove hard requirement check add check version appropriate code path handle behavior gap related include environment variable override version check,issue,negative,positive,neutral,neutral,positive,positive
1776066396,"For context, many users have brought up the issue: https://github.com/ray-project/ray/issues/36926",context many brought issue,issue,negative,positive,positive,positive,positive,positive
1776057950,"Blamed commit looks unrelated, but sanity checking a revert here https://buildkite.com/ray-project/release-tests-pr/builds/56708",blamed commit unrelated sanity revert,issue,negative,neutral,neutral,neutral,neutral,neutral
1776043296,@larroy are you using ray job submission. Could you also provide a more detailed repro?,ray job submission could also provide detailed,issue,negative,positive,positive,positive,positive,positive
1776033291,@raulchen @rkooo567 @jjyao Is it OK to have this in 2.9 instead? Is it because we want to get user feedback on this feature as part of 2.8?,instead want get user feedback feature part,issue,negative,neutral,neutral,neutral,neutral,neutral
1776024521,"Discussed with @matthewdeng offline, we are thinking we can probably make the version check optional. The only risk you running PyArrow 7+ on Windows, is that you will have significant performance regression when using Ray Data. But if your workload is not using Ray Data, it's fine to ignore.",thinking probably make version check optional risk running significant performance regression ray data ray data fine ignore,issue,negative,positive,positive,positive,positive,positive
1776009318,"re Dashboard: yes, we will do that after this PR is out & we agree on a protocol for those secrets. cc @alanwguo ",dashboard yes agree protocol,issue,positive,neutral,neutral,neutral,neutral,neutral
1776001299,"@rynewang Makes sense. Thanks for the clarity!
Also, we are going to remove the `secret_env_vars` from the ray dashboard, right? Otherwise the base64 will not be sufficient because it is to easy to manually see the encoded secrets. 

Copying from logs require a little more work and we are working on removing those as well, like you said, but showing on the dashboard is just too easy to grab.",sense thanks clarity also going remove ray dashboard right otherwise base sufficient easy manually see require little work working removing well like said showing dashboard easy grab,issue,positive,positive,neutral,neutral,positive,positive
1775990744,"@c21  I think this is technically due to a [requirement](https://github.com/ray-project/ray/issues/29814) from Ray Data?

https://github.com/ray-project/ray/blob/2104d63d274408b1117fd53b573d4b4812eeb46a/python/setup.py#L232-L236

A few options:
1. If this is fixed in Arrow already we can remove the custom serialization logic and remove this limit.
2. Another option if this is specific to Ray Data is to change the upper bound to only apply to Ray Data.",think technically due requirement ray data fixed arrow already remove custom serialization logic remove limit another option specific ray data change upper bound apply ray data,issue,negative,negative,neutral,neutral,negative,negative
1775972558,"Some thoughts:

1. we will not implement an ""end to end encryption"" for those secrets in foreseeable future. Most if not all things are transferred in cleartext.
2. That said, some simple measure of hiding them from accidental prints are welcome. i.e. No waterproof, but foolproof.
3. base64 lies quite well in the foolproof realm: you don't easily leak it out cleartext, but a hacker can still manually read it out. (maybe we can do some little hardening/obfuscation on base64, e.g. like this

```

def encode_with_shift(data, shift=1):
    b64_encoded = base64.b64encode(data.encode())
    shifted = bytearray([b + shift for b in b64_encoded])
    return shifted

def decode_with_shift(data, shift=1):
    unshifted = bytearray([b - shift for b in data])
    return base64.b64decode(unshifted).decode()

```

4. after this PR I will investigate a little more on @pcmoritz 's idea: we can rewrite `__repr__` and `__str__` to something like `return f""<{len(d)} secrets>""` which can tell us if the bits are set without telling us the fields or the values. I need to think more about how this also works in client side, not the runtime env agent side. We can do both base64 and this print-hiding.
5. In raylet (C++ side), we print the runtime envs as a string in DEBUG mode but not in INFO mode. This should be fine (DEBUG mode == no security).

So overall the PR's good. I will approve the PR once it's in good shape.",implement end end encryption foreseeable future transferred said simple measure accidental welcome waterproof foolproof base quite well foolproof realm easily leak hacker still manually read maybe little base like data shift return data unshifted shift data return unshifted investigate little idea rewrite something like return tell u set without telling u need think also work client side agent side base raylet side print string mode mode fine mode security overall good approve good shape,issue,positive,positive,neutral,neutral,positive,positive
1775969193,"Might be an infra issue: for the same commit 5f832b3346270a174a049a5feff3e6b925aca845, it was 87s on 18th but 194s now (https://buildkite.com/ray-project/release-tests-branch/builds/2295)",might infra issue commit th,issue,negative,neutral,neutral,neutral,neutral,neutral
1775957377,"> Looks good! If we plan to cherry-pick this, let's mark it with the appropriate labels

Thanks! I have added the label.",good plan let mark appropriate thanks added label,issue,positive,positive,positive,positive,positive,positive
1775934654,"Release tests passing: https://buildkite.com/ray-project/release-tests-pr/builds/56672#018b5e04-c3db-4f2c-b562-c80c926a3fc0

After this PR I will make another PR to mark them stable & daily again.",release passing make another mark stable daily,issue,negative,neutral,neutral,neutral,neutral,neutral
1775907124,@aslonnie I still need to use at least `bash -ic` for the shell to loads .bash_rc; if there is a better way to do this let me know,still need use least bash shell better way let know,issue,negative,positive,neutral,neutral,positive,positive
1775852704,"I am using it for experiment restore. So when it is not inside experiment path directory, restore is not working. I will try to write down some repro script, but now this problem is inside complex solution so I can not easily send you repro script. ",experiment restore inside experiment path directory restore working try write script problem inside complex solution easily send script,issue,positive,positive,neutral,neutral,positive,positive
1775849487,"anyone got a workaround besides downgrading to 2.5? i didn't have any luck with the following
`ray.data.context.DatasetContext.get_current().use_streaming_executor = False` ",anyone got besides luck following false,issue,negative,negative,negative,negative,negative,negative
1775837585,"Re-ran the failing release test [here](https://buildkite.com/ray-project/release-tests-pr/builds/56578#018b4fad-5673-4e60-9201-33b098908032).

Although the run time still looks a bit higher than before the offending PR, the number of workers killed due to out-of-memory is back to normal, so appears that the test is fixed.",failing release test although run time still bit higher number due back normal test fixed,issue,negative,positive,neutral,neutral,positive,positive
1775829852,I guess we can also split the protos into smaller sized chunks and send the chunks one by one?,guess also split smaller sized send one one,issue,negative,neutral,neutral,neutral,neutral,neutral
1775828738,"This looks like a Core issue that happens when trying to start up the Prometheus client server, before any Tune code is ran. ",like core issue trying start client server tune code ran,issue,negative,neutral,neutral,neutral,neutral,neutral
1775824113,"Instead of `functools.partial`, you can do one of the following:
1. Load your data directly in your training function.
2. Use [ray.tune.with_parameters](https://docs.ray.io/en/releases-2.7.1/tune/api/doc/ray.tune.with_parameters.html) as a replacement - note that this will still require serializing the entire data into object store memory and can be memory/network intensive.
",instead one following load data directly training function use replacement note still require entire data object store memory intensive,issue,negative,positive,neutral,neutral,positive,positive
1775780202,"> Do we need to change the docker build files to install this too?

the dockerfile is using `*` to copy.

I do need to add a line in the install script though.

for all these time (3+ months ?) data-test requirements were not included. not 100% sure if this is the right behavior actually.

would be nice if some data team folks can confirm.",need change docker build install copy need add line install script though time included sure right behavior actually would nice data team confirm,issue,positive,positive,positive,positive,positive,positive
1775776563,Oh it find files using .* so as long as it is here it will install,oh find long install,issue,negative,negative,neutral,neutral,negative,negative
1775772314,Do we need to change the docker build files to install this too?,need change docker build install,issue,negative,neutral,neutral,neutral,neutral,neutral
1775770601,"@lukakap The file should still be generated, but one behavioral change in Ray 2.7 may explain why you're not finding it where you expect:

If you set the `RunConfig(storage_path=""s3://..."")`, the `tuner.pkl` file will now be written directly to S3, without saving a local copy.

Another question: How are you using this `tuner.pkl` file? The contents of this file are technically implementation details and could change arbitrarily across releases.",file still one behavioral change ray may explain finding expect set file written directly without saving local copy another question file content file technically implementation could change arbitrarily across,issue,negative,neutral,neutral,neutral,neutral,neutral
1775753755,"Thinking more about this, I am inclined towards storing encrypted secrets in RuntimeEnv object. Here is my reasoning,

1. We can either allow logging the secrets or not allow. 
2. If we want to do the latter, we have to make sure we change all the current log instances and make sure it is not logged in future. This will require an approach like mentioned above where we raise an error when someone tries to log the `ProtectedString`. This requires a significant labor upfront to correct logs and runs a risk of unintentional failures for edge cases. 
3. If we store the secrets encrypted we do not care about logging them because the printed value is not useful, assuming the encryption key is safe. We can still implement measures to avoid logging secrets, but they do not have to be disruptive.
4. Going with encryption solution is a two-way-door as it does not require too much of upfront investment and still allows us to block secrets logging in future, if needed. It will also be easy to remove the encryption/decryption part as it will be used at one place only.

Being that said, we do need to think about where to store the encryption key, to be able to support the encryption based solution. One way is to assume the encryption key will be made available in an environment variable on the ray cluster, which in kuberay world translates to mounting the key on the pod. It does feel like a little step backwards because mounting secrets on the pod is what we are trying to avoid. But in this case, it is limited to only one secret and that secret does not need to change or expire, but can be changed as needed. The encryption/decryption is all done before the job starts running, so this allows us to manage key in other ways and even change the key frequently.

In the above proposal for storing the key, I am assuming the `RuntimeEnvAgent` http server runs on the ray cluster and not on the client submitting job. Please correct me if I am wrong.

Curious to hear your thoughts and suggestions.",thinking towards object reasoning either allow logging allow want latter make sure change current log make sure logged future require approach like raise error someone log significant labor correct risk unintentional edge store care logging printed value useful assuming encryption key safe still implement avoid logging disruptive going encryption solution require much investment still u block logging future also easy remove part used one place said need think store encryption key able support encryption based solution one way assume encryption key made available environment variable ray cluster world mounting key pod feel like little step backwards mounting pod trying avoid case limited one secret secret need change expire done job running u manage key way even change key frequently proposal key assuming server ray cluster client job please correct wrong curious hear,issue,positive,positive,neutral,neutral,positive,positive
1775752433,"@anyscalesam @raulchen 

I am using ray for the first time so my thinking might be a bit off, Do correct me if I am wrong somewhere 

Let's say ray has started reading all 300 files in parallel and since each file is compressed and there will be some spike in memory per file when it is being uncompressed. So if multiple files have a memory spike it can overload the system. and some of the metadata related spikes are still hanging in the memory.  

If I load the files in a for loop using simple pd.read_parquet, I am able to load the entire data by concatenating to a dataframe (starting from empty dataframe )  in the memory.

So It seems really weird that in loading only the parquet file metadata ray used up all this memory 
    ",ray first time thinking might bit correct wrong somewhere let say ray reading parallel since file compressed spike memory per file uncompressed multiple memory spike overload system related still hanging memory load loop simple able load entire data starting empty memory really weird loading parquet file ray used memory,issue,negative,negative,neutral,neutral,negative,negative
1775734425,"No longer relevant in 2.7+.

Artifact uploading is now off by default. This is also avoided by the temporary directory recommendation for creating checkpoints.",longer relevant artifact default also temporary directory recommendation,issue,negative,positive,positive,positive,positive,positive
1775723021,"The smoke version fails as well, in case it's easier to use that for debugging",smoke version well case easier use,issue,positive,neutral,neutral,neutral,neutral,neutral
1775665670,"@AndreKuu For the jobs with ""no ray driver"", did you use ray or have a ray driver in your entrypoint script?",ray driver use ray ray driver script,issue,negative,neutral,neutral,neutral,neutral,neutral
1775660031,"```
REGRESSION 39.88%: dashboard_p99_latency_ms (LATENCY) regresses from 13941.91 to 19502.436 (39.88%) in 2.8.0/benchmarks/many_tasks.json
REGRESSION 37.76%: dashboard_p95_latency_ms (LATENCY) regresses from 6729.761 to 9271.209 (37.76%) in 2.8.0/benchmarks/many_tasks.json
REGRESSION 22.64%: dashboard_p50_latency_ms (LATENCY) regresses from 5.534 to 6.787 (22.64%) in 2.8.0/benchmarks/many_tasks.json
REGRESSION 21.37%: multi_client_put_gigabytes (THROUGHPUT) regresses from 38.605668097256924 to 30.354050606212375 (21.37%) in 2.8.0/microbenchmark.json
REGRESSION 14.94%: 1000000_queued_time (LATENCY) regresses from 181.82263824499995 to 208.991110036 (14.94%) in 2.8.0/scalability/single_node.json
REGRESSION 13.00%: avg_iteration_time (LATENCY) regresses from 1.4622855401039123 to 1.6523929166793823 (13.00%) in 2.8.0/stress_tests/stress_test_dead_actors.json
REGRESSION 12.51%: client__1_1_actor_calls_sync (THROUGHPUT) regresses from 573.4457553242221 to 501.7198350460272 (12.51%) in 2.8.0/microbenchmark.json
REGRESSION 12.48%: 1_n_actor_calls_async (THROUGHPUT) regresses from 10133.72696574923 to 8869.51837285407 (12.48%) in 2.8.0/microbenchmark.json
REGRESSION 9.56%: multi_client_tasks_async (THROUGHPUT) regresses from 28423.644858766176 to 25705.1703030958 (9.56%) in 2.8.0/microbenchmark.json
REGRESSION 9.52%: 1_n_async_actor_calls_async (THROUGHPUT) regresses from 9124.377528222414 to 8255.28161190285 (9.52%) in 2.8.0/microbenchmark.json
REGRESSION 9.39%: client__1_1_actor_calls_concurrent (THROUGHPUT) regresses from 1080.2139341634759 to 978.7677324282791 (9.39%) in 2.8.0/microbenchmark.json
REGRESSION 9.32%: 10000_get_time (LATENCY) regresses from 23.55671212599998 to 25.751547934 (9.32%) in 2.8.0/scalability/single_node.json
REGRESSION 9.27%: stage_2_avg_iteration_time (LATENCY) regresses from 58.160910558700564 to 63.549897527694704 (9.27%) in 2.8.0/stress_tests/stress_test_many_tasks.json
REGRESSION 9.03%: dashboard_p50_latency_ms (LATENCY) regresses from 3.465 to 3.778 (9.03%) in 2.8.0/benchmarks/many_nodes.json
REGRESSION 8.65%: single_client_tasks_async (THROUGHPUT) regresses from 10739.407361558973 to 9810.522775386338 (8.65%) in 2.8.0/microbenchmark.json
REGRESSION 7.01%: client__tasks_and_get_batch (THROUGHPUT) regresses from 1.002041264301031 to 0.9317720482301839 (7.01%) in 2.8.0/microbenchmark.json
REGRESSION 6.86%: stage_1_avg_iteration_time (LATENCY) regresses from 23.305240750312805 to 24.904260087013245 (6.86%) in 2.8.0/stress_tests/stress_test_many_tasks.json
REGRESSION 6.63%: single_client_tasks_sync (THROUGHPUT) regresses from 1311.812164358857 to 1224.7771019170784 (6.63%) in 2.8.0/microbenchmark.json
REGRESSION 6.47%: 10000_args_time (LATENCY) regresses from 16.89121779300001 to 17.983383886000013 (6.47%) in 2.8.0/scalability/single_node.json
REGRESSION 6.40%: single_client_get_object_containing_10k_refs (THROUGHPUT) regresses from 14.755162462843568 to 13.810261182479115 (6.40%) in 2.8.0/microbenchmark.json
REGRESSION 5.08%: client__1_1_actor_calls_async (THROUGHPUT) regresses from 1083.9300708022135 to 1028.839383358896 (5.08%) in 2.8.0/microbenchmark.json
REGRESSION 4.95%: n_n_async_actor_calls_async (THROUGHPUT) regresses from 25688.484755543966 to 24418.02409630373 (4.95%) in 2.8.0/microbenchmark.json
REGRESSION 4.77%: actors_per_second (THROUGHPUT) regresses from 748.5322140167257 to 712.822673976586 (4.77%) in 2.8.0/benchmarks/many_actors.json
REGRESSION 4.54%: n_n_actor_calls_async (THROUGHPUT) regresses from 30847.92669705198 to 29447.495631692484 (4.54%) in 2.8.0/microbenchmark.json
REGRESSION 3.68%: 3000_returns_time (LATENCY) regresses from 5.6602293089999876 to 5.868746854999998 (3.68%) in 2.8.0/scalability/single_node.json
REGRESSION 3.49%: client__put_calls (THROUGHPUT) regresses from 857.6367908455961 to 827.6636203329824 (3.49%) in 2.8.0/microbenchmark.json
REGRESSION 2.93%: 1_1_actor_calls_async (THROUGHPUT) regresses from 7615.355914488919 to 7392.382505877325 (2.93%) in 2.8.0/microbenchmark.json
REGRESSION 2.75%: stage_4_spread (LATENCY) regresses from 0.7217020493267903 to 0.7415504343465557 (2.75%) in 2.8.0/stress_tests/stress_test_many_tasks.json
REGRESSION 2.40%: stage_3_time (LATENCY) regresses from 2802.1650245189667 to 2869.4522173404694 (2.40%) in 2.8.0/stress_tests/stress_test_many_tasks.json
REGRESSION 2.20%: single_client_put_calls_Plasma_Store (THROUGHPUT) regresses from 5766.661045557541 to 5639.599660122037 (2.20%) in 2.8.0/microbenchmark.json
REGRESSION 1.54%: n_n_actor_calls_with_arg_async (THROUGHPUT) regresses from 3074.0790016310475 to 3026.6872241733467 (1.54%) in 2.8.0/microbenchmark.json
REGRESSION 1.25%: client__put_gigabytes (THROUGHPUT) regresses from 0.13283428838343245 to 0.13117919950925 (1.25%) in 2.8.0/microbenchmark.json
REGRESSION 0.52%: client__tasks_and_put_batch (THROUGHPUT) regresses from 11411.245745812425 to 11351.922193872259 (0.52%) in 2.8.0/microbenchmark.json
REGRESSION 0.36%: 1_1_actor_calls_sync (THROUGHPUT) regresses from 2255.614293958201 to 2247.4937092440123 (0.36%) in 2.8.0/microbenchmark.json
```",regression latency regression latency regression latency regression throughput regression latency regression latency regression throughput regression throughput regression throughput regression throughput regression throughput regression latency regression latency regression latency regression throughput regression throughput regression latency regression throughput regression latency regression throughput regression throughput regression throughput regression throughput regression throughput regression latency regression throughput regression throughput regression latency regression latency regression throughput regression throughput regression throughput regression throughput regression throughput,issue,negative,neutral,neutral,neutral,neutral,neutral
1775657468,"w00h00 thank you, I'll leave this up for your team to review then. Also they normally try something like this https://github.com/ray-project/ray/pull/29615 to make it easier to review.",thank leave team review also normally try something like make easier review,issue,positive,positive,positive,positive,positive,positive
1775638560,"> @vitsai is this blocker for ray28?

yes, the release test is failing.",blocker ray yes release test failing,issue,negative,neutral,neutral,neutral,neutral,neutral
1775625024,"Seems to be an error related to too many SSH keys being created and stored in the GCP project ""common instance metadata"".  I think I'll update the test to reuse a single key instead of generating a new one for each test. ",error related many project common instance think update test reuse single key instead generating new one test,issue,negative,positive,neutral,neutral,positive,positive
1775623183,"@can-anyscale Thanks for the heads up. Looks like a different error this time:

```
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/autoscaler/_private/gcp/config.py"", line 523, in _configure_key_pair
    _create_project_ssh_key_pair(project, public_key, ssh_user, compute)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/autoscaler/_private/gcp/config.py"", line 781, in _create_project_ssh_key_pair
    compute.projects()
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/googleapiclient/_helpers.py"", line 130, in positional_wrapper
    return wrapped(*args, **kwargs)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/googleapiclient/http.py"", line 851, in execute
    raise HttpError(resp, content, uri=self.uri)
googleapiclient.errors.HttpError: <HttpError 413 when requesting https://compute.googleapis.com/compute/v1/projects/anyscale-bridge-cd812d38/setCommonInstanceMetadata?alt=json returned ""Value for field 'metadata.items[0].value' is too large: maximum size 262144 character(s); actual size 262279."">
```
",thanks like different error time file line project compute file line file line return wrapped file line execute raise resp content returned value field large maximum size character actual size,issue,positive,positive,positive,positive,positive,positive
1775620537,Is there any new developments in Ray when it comes to the async event loop? ,new ray come event loop,issue,negative,positive,positive,positive,positive,positive
1775618404,"> Another potential use case: Specify one spot node type and the same node type as on-demand. If the spot request fails, then start an on-demand node in its place.

This is definitely a possible extension. We are actively looking into this and will update once we have an API for review. ",another potential use case specify one spot node type node type spot request start node place definitely possible extension actively looking update review,issue,positive,negative,neutral,neutral,negative,negative
1775617076,"> I’d like to specify multiple spot instance types and if one request fails because of a lack of capacity it tries the next. In general it’s just not clear what happens if the user specifies multiple node types with the same resources. From glancing at the code it will just use the first one that can satisfy the requirements, but I’d like to be sure.
> […](#)
> On Thu, 28 Sep 2023, at 01:28, Ricky Xu wrote: What's the usecases for multiple available_node_types here? Maybe just some high-level examples would be really helpful! — Reply to this email directly, view it on GitHub <[#39788 (comment)](https://github.com/ray-project/ray/issues/39788#issuecomment-1738237143)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AAE2WREDXUNKJM42TYQRXY3X4SZCXANCNFSM6AAAAAA5B524WQ>. You are receiving this because you were assigned.Message ID: ***@***.***>

Yeah, I think there was some pending work to take into account node availability in choosing which node type to launch, but as of now, the autoscaler is naive that it's not aware of this.

It has some heuristics of choosing which is the ""best"" node type here: https://github.com/ray-project/ray/blob/5a6d78ce47ab84ee681d267c0b34c3c5c2bf7b7b/python/ray/autoscaler/_private/resource_demand_scheduler.py#L808-L813",like specify multiple spot instance one request lack capacity next general clear user multiple node glancing code use first one satisfy like sure wrote multiple maybe would really helpful reply directly view comment id yeah think pending work take account node availability choosing node type launch naive aware choosing best node type,issue,positive,positive,positive,positive,positive,positive
1775615877,"Ok, I'll do this now:

* For each contrib algo, there will be a new BUILD file with a) all the old compilation tests and b) all the old learning tests in it
* This file is referenced from the main `pipeline.ml.yml` file and run by bazel.

E.g. for A2C:
<img width=""1136"" alt=""image"" src=""https://github.com/ray-project/ray/assets/5839854/6583dc85-c05c-4bce-81a6-61b3d062bfc6"">
",new build file old compilation old learning file main file run image,issue,negative,positive,positive,positive,positive,positive
1775610881,@richardliaw are customer Docker's powered Ray Clusters a well supported path in the stack?,customer docker powered ray well path stack,issue,negative,neutral,neutral,neutral,neutral,neutral
1775601523,Starting troubleshooting from the top of the stack at the Libraries layer first; @alexeykudinkin can you please take a look?,starting top stack layer first please take look,issue,negative,positive,positive,positive,positive,positive
1775596251,@alexeykudinkin can you please triage this Ray Jobs issue?,please triage ray issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1775594319,"Book linkcheck failure unrelated, this PR does not touch the docs.",book failure unrelated touch,issue,negative,negative,negative,negative,negative,negative
1775593944,"@gvspraveen could you review this and decide how to proceed/support the PR that @paolorechia had started. This is in the area of Ray Clusters.

UPDATE: maybe @architkulkarni ?",could review decide area ray update maybe,issue,negative,neutral,neutral,neutral,neutral,neutral
1775573170,"hi @hahahannes , the duplicated logs here are “soft duplicated”, which means outputs with similar pattern will be aggregated together, but they are not necessarily identical.

you can turn it off by setting `RAY_DEDUP_LOGS=0`",hi soft similar pattern together necessarily identical turn setting,issue,negative,positive,neutral,neutral,positive,positive
1775567682,"> ### What happened + What you expected to happen
> **1. the bug:** I'm running a RLlib reinforcement learning algorithm using ray.tune with a user-specified `storage_path` and `name` in the air.RunConfig, as shown in the minimal example. Instead of having the results in `storage_path/name`, another results directory gets created in `~/ray_results/name`. The latter directory also gets updated with TensorBoard results during the training run. Only at the end of training, the TensorBoard file (as well as the checkpoint directory) appear in the correct directory.
> 
> **2. expected behavior:** the results are only supposed to be created in the specified storage path.
> 
> ### Versions / Dependencies
> Ray: 2.7.0 Python: 3.9.18 OS: Ubuntu 22.04.02
> 
> ### Reproduction script
> ```
> from ray import air, tune
> from ray.rllib.algorithms.ppo import PPOConfig
> 
> 
> algo_config = PPOConfig().environment(""CartPole-v1"").framework(""torch"")
> 
> tuner = tune.Tuner(
>     ""PPO"",
>     param_space=algo_config.to_dict(),
>     run_config=air.RunConfig(
>         name=""run_01"",
>         storage_path=""~/ray_results/CartPole-v1"",
>         stop={""episode_reward_mean"": 475},
> 
>     ),
> )
> 
> tuner.fit()
> ```
> 
> ### Issue Severity
> Low: It annoys or frustrates me.

Same issue!",happen bug running reinforcement learning algorithm name shown minimal example instead another directory latter directory also training run end training file well directory appear correct directory behavior supposed storage path ray python o reproduction script ray import air tune import torch tuner issue severity low issue,issue,negative,negative,neutral,neutral,negative,negative
1775560595,"@mfojtak The dockerfile I use to test looks like this 
```
# Use Anyscale base image
FROM anyscale/ray:nightly-py310

# Install dependencies
RUN pip install --upgrade pip && pip install -U torch==2.0.1 torchvision==0.15.2

WORKDIR /home/ray

# Copy local code including protobuf and service definitions into docker image
COPY . /home/ray

# Add working directory into python path so they are importable
ENV PYTHONPATH=/home/ray
```
You do not need to build your code into a library. Simply copy the code into a python importable path (or adding the working directory into the python path in this example), then you should be able to call it from the `import_path` config yaml. I think if you just leave `runtime_env` as an empty json then it will use local path. 

Since there are existing issues talking about modifying the `working_dir` logics, I don't think we need duplicated tickets for those. If you have more issue related to how Ray Serve, I'm happy to dive deeper. Else let's close this issue and comment on the other issues for the specifics. Feel free to reopen if you have more Serve specific issues 🙂
",use test like use base image install run pip install upgrade pip pip install copy local code service docker image copy add working directory python path importable need build code library simply copy code python importable path working directory python path example able call think leave empty use local path since talking think need issue related ray serve happy dive else let close issue comment feel free reopen serve specific,issue,positive,positive,neutral,neutral,positive,positive
1775545128,"Great, thank you for the update. I look forward to the `logging_config` feature.",great thank update look forward feature,issue,positive,positive,positive,positive,positive,positive
1775506083,"@raulchen I am using `pyarrow==13.0.0`. I chose this specific version of `pyarrow` because it is the first version to include both of the following bugfixes:

* https://github.com/apache/arrow/issues/35318 &mdash; without this, the dataset iterator is flaky, raising occasional network errors sourced to `libcurl`.
* https://github.com/apache/arrow/pull/36376 &mdash; without this, the GCS connection initializer (?) running on the head node fails.",chose specific version first version include following without flaky raising occasional network without connection running head node,issue,negative,positive,neutral,neutral,positive,positive
1775498803,"The first PR for that was merged: https://github.com/ray-project/ray/pull/40465

Implementation coming this week",first implementation coming week,issue,negative,positive,positive,positive,positive,positive
1775498051,"@volks73 we're currently working on adding a `logging_config` that can be set cluster-wide or per-deployment which will have an option to disable this specific log message (among other standard log configs such as level, directory, etc.).",currently working set option disable specific log message among standard log level directory,issue,negative,neutral,neutral,neutral,neutral,neutral
1775483289,"@glesperance this is only the first of a few PRs before we can unpin the dependency and be fully compatible with Pydantic v2.

With this patch, if you manually override the Pydantic version and don't serialize Pydantic model definitions, then Ray _should_ work with Pydantic v2 (but it's sort of use-at-your-own-risk at this point).

We're currently waiting for a release containing the fixes I've made in Pydantic for this issue to be released: https://github.com/pydantic/pydantic/issues/6763.",first unpin dependency fully compatible patch manually override version serialize model ray work sort point currently waiting release made issue,issue,negative,positive,positive,positive,positive,positive
1775479320,"@ddelange here's the link to the Pydantic issue: https://github.com/pydantic/pydantic/issues/6763

After that is merged and released, we will require either `< 2.0` or `>= 2.5` (assuming `2.5` is the the release containing the fixes).",link issue require either assuming release,issue,negative,neutral,neutral,neutral,neutral,neutral
1775373110,"@krfricke Hi, just a quick ping, if I shall reopen the PR ? Totally understand, that this is not an important issue",hi quick ping shall reopen totally understand important issue,issue,negative,positive,positive,positive,positive,positive
1775330671,"Hi, I have drafted a PR to address this improvement. 
I would like to invite @N3XT14 to help working on it together.
Thanks!",hi address improvement would like invite help working together thanks,issue,positive,positive,positive,positive,positive,positive
1775196627,"Thank you!
I added a comment about splitting the util function to be able to support providing an algorithm instance.
https://github.com/ray-project/ray/pull/40452#discussion_r1368549339",thank added comment splitting function able support providing algorithm instance,issue,positive,positive,positive,positive,positive,positive
1774888233,"> FYI to reviewers: this is a WIP, I won't merge it until after the next `pydantic` release that includes changes to make models `cloudpickle`-serializable.

do you have a link for this? will that mean that this PR will have to require `pydantic>=2.5`?",wo merge next release make link mean require,issue,negative,negative,negative,negative,negative,negative
1774262083,"That's a great point @gresavage ! I was a bit unclear about the circumstances under which `progress.csv` would exist but `result.json` wouldn't... Maybe if training failed to complete? It seems to me that `result.json` contains more info (namely, it contains the config information, which `progress.csv` does not).

Anyway, since `progress.csv` doesn't contain the config information, we can't hope to restore the config to the `Result` itself, but your proposal does at least re-create the `latest_metrics` in that branch of the if-else logic so the code would still execute.",great point bit unclear would exist would maybe training complete namely information anyway since contain information ca hope restore result proposal least branch logic code would still execute,issue,positive,positive,positive,positive,positive,positive
1774221769,"one problem I see with the proposed solution is that loading from a `progress.csv` was not considered. May just be a simple matter of changing [L153-L166](https://github.com/ray-project/ray/blob/e19b0eb1cb898f4f38349783fd8f86cbc8c60b83/python/ray/air/result.py#L153-L166) to the following instead:

```python
        if result_json_file.exists():
            with open(result_json_file, ""r"") as f:
                json_list = [json.loads(line) for line in f if line]
                metrics_df = pd.json_normalize(json_list, sep=""/"")
                latest_metrics = json_list[-1] if json_list[-1] else {}
        # Fallback to restore from progress.csv
        elif progress_csv_file.exists():
            metrics_df = pd.read_csv(progress_csv_file)
            latest_metrics = metrics_df.iloc[-1].to_dict() if not metrics_df.empty else {}
        else:
            raise RuntimeError(
                f""Failed to restore the Result object: Neither {EXPR_RESULT_FILE}""
                f"" nor {EXPR_PROGRESS_FILE} exists in the trial folder!""
            )
```

I'm not sure, but I think reading from `progress.csv` might also need to be checked for how results are flattened and restored in the dataframe.

Unfortunately I'm having other issues with AIR corrupting trial checkpoint data so I can't confirm at the moment.",one problem see solution loading considered may simple matter following instead python open line line line else fallback restore else else raise restore result object neither trial folder sure think reading might also need checked unfortunately air corrupting trial data ca confirm moment,issue,negative,neutral,neutral,neutral,neutral,neutral
1774198955,Perhaps a note - I'm not running containers but in conda venv on nodes directly.,perhaps note running directly,issue,negative,positive,neutral,neutral,positive,positive
1774166486,I tried to downgrade Ray to 2.5.1 (as I did in [this](https://github.com/ray-project/ray/issues/40554) example) but that didn't help. ,tried downgrade ray example help,issue,negative,neutral,neutral,neutral,neutral,neutral
1774148719,"I found that explicitly setting the local_dir argument of TuneGridSearchCV() will fix this error. That said, this error still remains for me for similar scripts: https://github.com/ray-project/ray/issues/40554",found explicitly setting argument fix error said error still remains similar,issue,negative,neutral,neutral,neutral,neutral,neutral
1774065558,"Hi, I know this is from almost a year ago, so wanted to check as I am also impacted by this issue - is fixing this on the roadmap?",hi know almost year ago check also impacted issue fixing,issue,negative,neutral,neutral,neutral,neutral,neutral
1774047623,I logged into the worker system via ssh and checked if any containers are running via the command docker ps -a. And couldn't find any related running images on the worker node,logged worker system via checked running via command docker could find related running worker node,issue,negative,neutral,neutral,neutral,neutral,neutral
1773994879,"@rynewang I cannot add a reviewer, but wanted to make sure you see this. Thanks!",add reviewer make sure see thanks,issue,positive,positive,positive,positive,positive,positive
1773994599,"That feels like opening a can of worms though, it is likely going to break a lot of code as we are just raising an error.",like opening though likely going break lot code raising error,issue,negative,neutral,neutral,neutral,neutral,neutral
1773938100,"One way how this could be done is have a datatype like
```python
class ProtectedString:
    def __init__(self, s: Union[str, ""ProtectedString""]):
        if isinstance(s, ProtectedString):
            self._UNSAFE_DO_NOT_USE: str = s._UNSAFE_DO_NOT_USE
        elif isinstance(s, str):
            self._UNSAFE_DO_NOT_USE = s
        else:
            raise ValueError(
                f""{type(s)}:'{repr(s)}' is not a valid type for ProtectedString""
            )

    def __eq__(self, other: Any) -> bool:
        if isinstance(other, ProtectedString):
            return self._UNSAFE_DO_NOT_USE == other._UNSAFE_DO_NOT_USE
        return False

    def __str__(self) -> str:
        error_msg = ""ProtectedStrings should not be serialized directly.""
        raise RuntimeError(error_msg)

    def __repr__(self) -> str:
        return str(self)
```
and use that for the secret values of those environment variables everywhere in the code,  and similar in C++ code to make it less likely for the secrets to become part of log files (also avoid future Ray versions printing them by accident due to programmer errors).

This could be good enough for a good amount of simple applications. If you are really serious, you should of course use a secret store and then give the machines in the cluster that need access the right permissions through an IAM like system and we should point that out in the documentation.",one way could done like python class self union else raise type valid type self bool return return false self directly raise self return self use secret environment everywhere code similar code make le likely become part log also avoid future ray printing accident due programmer could good enough good amount simple really serious course use secret store give cluster need access right like system point documentation,issue,positive,positive,neutral,neutral,positive,positive
1773925681,"One question on your suggestion @rynewang 
> Longer term (but still doable in one PR, your call): we add a method to RuntimeEnvPlugin namely def show(runtime_env) -> str. It defaults to return json.dumps({self.name: runtime_env[self.name]}), but for our plugin, it returns json.dumps{""secret_env_vars"": f""**{len(runtime_env[""secret_env_vars""])} secrets**""}

Th plugin just reads the secret_env_vars from [RuntimeEnv](https://github.com/ray-project/ray/blob/master/python/ray/runtime_env/runtime_env.py#L133) object (dict). So the secrets are stored in the [RuntimeEnv](https://github.com/ray-project/ray/blob/master/python/ray/runtime_env/runtime_env.py#L133) not in the plugin, right? In this case adding a `show()` method on the plugin will not help.

Out problem is code printing entire [RuntimeEnv](https://github.com/ray-project/ray/blob/master/python/ray/runtime_env/runtime_env.py#L133) dict which contains secrets as one entry. And in this class I see methods like `serialize`, `deserialize`, `to_dict()` which make me think it is pretty hard to avoid printing/leaking one entry from this dict.

I think storing the secrets encrypted (instead of base64 encoded) is a way of guaranteeing no leak, but it does add some complexity of storing the encryption key, which is why I am hesitant to take that route. ",one question suggestion longer term still doable one call add method namely show return th object right case show method help problem code printing entire one entry class see like serialize make think pretty hard avoid one entry think instead base way leak add complexity encryption key hesitant take route,issue,negative,negative,neutral,neutral,negative,negative
1773919513,"@rynewang FYI, for logging safety of secret_env_vars, I am thinking we store the values in `RuntimeEnv` dict with base64 encoding. That way we can be sure that we do not print them in plain text unintentionally in the logs or anywhere else. Since the `SecretEnvVarsPlugin` has control over how these secrets are set in the context, it can decode the values before setting. This way the secrets are encoded all the time, until the last moment. 
This is orthogonal to adding the `show()` method to `RuntimeEnv`, because base64 encoding is not a cryptographically safe encryption, but it just covers any accidental logs. The ultimately safe mechanism is, of course, to just not log the secrets anywhere.

This does not guarantee that the some code in worker does not print/log the environment variables, but that needs to be handled separately, IMO.

Let me know if you think otherwise.",logging safety thinking store base way sure print plain text unintentionally anywhere else since control set context decode setting way time last moment orthogonal show method base cryptographically safe encryption accidental ultimately safe mechanism course log anywhere guarantee code worker environment need handled separately let know think otherwise,issue,positive,negative,neutral,neutral,negative,negative
1773887815,"@alexpalms I have been having the same issue with Ray 2.7.1.  As a workaround, I have replaced my MultiDiscrete action with a Tuple of Discrete actions.  For example, in your original script:
```
            env_config={
                ""space"": Dict(
                    {
                        ""a"": Tuple(
                            [Dict({""d"": Box(-10.0, 10.0, ()), ""e"": Discrete(2)})]
                        ),
                        ""b"": Box(-10.0, 10.0, (2,)),
                        ""c"": Tuple([Discrete(3), Discrete(3)]),  # MultiDiscrete([3, 3])),
                    }
                ),
            },
```
Another possible workaround might be to flatten the space with [spaces.utils.flatten_space](https://gymnasium.farama.org/api/spaces/utils/), which will convert MultiDiscrete to a Box.  The catch with this approach is that the sampling from the action space would get distorted, because Boxes are not discrete.  ",issue ray action discrete example original script space box discrete box discrete discrete another possible might flatten space convert box catch approach sampling action space would get distorted discrete,issue,negative,positive,positive,positive,positive,positive
1773875065,@edoakes am I reading this right that ray (inc. ray.data) should be compatible with Pydantic v2?,reading right ray compatible,issue,negative,positive,positive,positive,positive,positive
1773695453,"> @mfojtak So I was testing a bit more with the Kuberay with Ray Serve. You should be able to just use `{ }` as the runtime env like the following. This should use the files in the docker image instead of looking up on the remote location
> 
> ```
>   serveConfigV2: |
>     applications:
>       - name: mobilenet
>         import_path: mobilenet.mobilenet:app
>         runtime_env: { }
> ```
> 
> Then given you are brining in your own docker image built with this code, you should be able to start Serve! Please give it a try!!

It assumes that everytime I change the code - I have to rebuild the image. Also I need to turn my code into library. working_dir is more convenient alternative. But unfortunately, it is not working properly. 
Btw. how can I specify a local working director for the application in RayService if only remote URI values are accepted by working_dir? There is an existing issue here #33456
Would it help to move the issue to the ""Core"" label?",testing bit ray serve able use like following use docker image instead looking remote location name given docker image built code able start serve please give try change code rebuild image also need turn code library convenient alternative unfortunately working properly specify local working director application remote accepted issue would help move issue core label,issue,positive,positive,neutral,neutral,positive,positive
1773629104,I'm pretty sure this should be P1 but @xieus @rkooo567 pls update as you see fit,pretty sure update see fit,issue,positive,positive,positive,positive,positive,positive
1773526109,"@architkulkarni it's that python/requirements.txt file; you can just add a new comment to create a session of [default] and add boto3 under it

you might need to update the python/requirements_compiled.txt (auto-generated code) as well (https://www.notion.so/anyscale-hq/OSS-Python-dependency-management-f32633b0018c423f927727807ea9da08)",file add new comment create session default add might need update code well,issue,negative,positive,positive,positive,positive,positive
1773499460,Looks like I need frontend owner to merge this? ,like need owner merge,issue,negative,neutral,neutral,neutral,neutral,neutral
1773493931,"@jorisvandenbossche thanks, we'll  try removing the upper pin first. ",thanks try removing upper pin first,issue,negative,positive,positive,positive,positive,positive
1773492993,"@ResidentMario What pyarrow version are you using? 
We use pyarrow to download files from cloud storage. ",version use cloud storage,issue,negative,neutral,neutral,neutral,neutral,neutral
1773490503,"This is a known issue and [being worked on](https://github.com/ray-project/ray/pull/40387).  It's planed to be experimental in 2.8, and official in 2.9. ",known issue worked experimental official,issue,negative,positive,neutral,neutral,positive,positive
1773490108,"Yes, my mistake, 101 is the head node (have updated my prev comment).",yes mistake head node comment,issue,negative,neutral,neutral,neutral,neutral,neutral
1773487612,Closing this issue as the above suggestion should help. Feel free to reopen if there are other issues. ,issue suggestion help feel free reopen,issue,positive,positive,positive,positive,positive,positive
1773470145,"I see. OOC, is there a reason why?

And will that include support for util functions like gaussian_noise in [this pull request? ](https://github.com/ray-project/ray/pull/40281) Wondering if I need to change anything there.",see reason include support like pull request wondering need change anything,issue,positive,neutral,neutral,neutral,neutral,neutral
1773462384,@scottsun94 can you please triage and set priority assign ,please triage set priority assign,issue,negative,neutral,neutral,neutral,neutral,neutral
1773461386,@rkooo567 can you please fill out issue severity here as well as add a label for target ray release?,please fill issue severity well add label target ray release,issue,positive,neutral,neutral,neutral,neutral,neutral
1773454824,"@gvspraveen can you please review and triage?

UPDATE: this should go to @peytondmurray @mattip ",please review triage update go,issue,negative,neutral,neutral,neutral,neutral,neutral
1773406495,"I saw it's retried once, and failed at similar time. Looks like it has a OOM error

```
(raylet) [2023-10-20 10:50:23,422 E 285 285] (raylet) node_manager.cc:3026: 17 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (...) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip ...`
(raylet)                                                                                                             
(raylet) Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.         
```",saw similar time like error raylet raylet due memory pressure due node last time period see information node use ray raylet raylet refer documentation address memory issue consider memory node reducing task parallelism per task adjust kill threshold set environment variable starting ray disable worker killing set environment variable zero,issue,negative,negative,neutral,neutral,negative,negative
1773402067,"We don't have a good way to unit test this right now, but I think with better perf introspection, we can check the peak memory usage.",good way unit test right think better introspection check peak memory usage,issue,positive,positive,positive,positive,positive,positive
1773398990,"Unlikely, we don't fuse sort ops with upstream maps.

The linked test failure says infra error.",unlikely fuse sort upstream linked test failure infra error,issue,negative,negative,negative,negative,negative,negative
1773398658,"It says
> Final release test exit code is 19 (infra error). Took 1006s

Should we rerun?",final release test exit code infra error took rerun,issue,negative,neutral,neutral,neutral,neutral,neutral
1773380361,"@c21 totally, sorry it got assigned to me and i missed it, feel free to close this issue to re-enable the test

It seems to be failing on release branch though: https://buildkite.com/ray-project/release-tests-branch/builds/2276#018b4de7-e23e-4671-9191-beaeaa4d1825",totally sorry got assigned feel free close issue test failing release branch though,issue,negative,negative,neutral,neutral,negative,negative
1773378747,"Hi @can-anyscale, this looks missed tracked on Data team. Shall we re-enable the test again?",hi tracked data team shall test,issue,negative,neutral,neutral,neutral,neutral,neutral
1773363673,"Looks like the problem is #40248.

This PR changed output block creation so that when a task produces its output blocks, it will try to slice them before yielding to respect the target block size. Unfortunately, all-to-all ops currently don't support dynamic block splitting. This means that if we try to fuse an upstream map iterator with an all-to-all op, the all-to-all task will still have to fuse all of the sliced blocks back together again. This seems to increase memory usage significantly.                                                                                                                                            ",like problem output block creation task output try slice yielding respect target block size unfortunately currently support dynamic block splitting try fuse upstream map task still fuse sliced back together increase memory usage significantly,issue,negative,positive,neutral,neutral,positive,positive
1773349552,"> @architkulkarni ""Another mystery is why the worker node 192.168.0.108 was able to join in your monitor.log"" If it hels, I start `ray up cluster.yaml` from 192.168.0.110. 192.168.0.108 is the head node (which can SSH into all other nodes). And I don't use any firewalls.

Oh interesting, but in your monitor.log it says two nodes have successfully joined (I think one head and one worker)
and the yaml has 108 as a worker node:

```
    head_ip: 192.168.0.101
    worker_ips:
      - 192.168.0.106
      - 192.168.0.107
      - 192.168.0.108
      - 192.168.0.110
 ```
 
 But maybe it's a different run.
 
 @ajaichemmanam thanks for the additional details, it should be helpful for trying to reproduce the issue on our end. How were you able to determine that the docker container didn't start?",another mystery worker node able join start ray head node use oh interesting two successfully think one head one worker worker node maybe different run thanks additional helpful trying reproduce issue end able determine docker container start,issue,positive,positive,positive,positive,positive,positive
1773334513,"@architkulkarni ""Another mystery is why the worker node 192.168.0.108 was able to join in your monitor.log""
If it hels, I start `ray up cluster.yaml` from 192.168.0.110. 192.168.0.101 is the head node (which can SSH into all other nodes). And I don't use any firewalls.",another mystery worker node able join start ray head node use,issue,negative,positive,positive,positive,positive,positive
1773323374,"Thanks @rynewang, appreciate the details. I will go through them and come back with questions or a PR.",thanks appreciate go come back,issue,positive,positive,neutral,neutral,positive,positive
1773312300,"A `secret_env_vars` in runtime environment looks like the best way to do that: it's a natural extension to what we already offer (`env_vars`) and it's simple.

Behavior:
- part of the runtime env
- applied as environment variables just like `env_vars`
- but never shown in Ray logs or Dashboards.

One can specify them via the job submission client, or via the CLI `ray job submit --runtime-env`.

## Overview 

On client side it's just a plain, unparsed json. It's sent here and there until it reaches https://github.com/ray-project/ray/blob/master/python/ray/_private/runtime_env/agent/runtime_env_agent.py . 

`class RuntimeEnvAgent` is an HTTP server that handles creation and deletion of a runtime env. It's a pluggable structure: each kind of runtime env (e.g. `working_dir` or `pip`) is registered as a [`RuntimeEnvPlugin`](https://github.com/ray-project/ray/blob/master/python/ray/_private/runtime_env/plugin.py) in the Agent's `__init__` constructor. The Agent forwards creation, deletion to the plugins.

On a new runtime env creation: the agent makes a `RuntimeEnvContext` which records what to do to create a worker process. Then it invokes each Plugin to prepare the files and prepend commands and env vars. Then the worker is started with all those things.

On a runtime env deletion: when all workers died, the Agent invokes each Plugin to delete the prepared files.


## The Plugin


We can create a new SecretEnvVars plugin like this:

- `class SecretEnvVars(RuntimeEnvPlugin):`
- `validate`: verifies we have a Dict from str to str.
- `get_uris`: return an empty array `[]`.
- `create`: no-op, return 0
- `modify_context`: extend the context.env_vars with the secret KVs.
- `delete_uri`: no-op, return 0

The new plugin can be a new file called `python/ray/_private/runtime_env/secret_env_vars.py`. Then we can ship it as a first party plugin, by adding it in the `RuntimeEnvAgent.__init__ function](https://github.com/ray-project/ray/blob/master/python/ray/_private/runtime_env/agent/runtime_env_agent.py#L199) as a field and registering it there.

You can use the [working_dir](https://github.com/ray-project/ray/blob/master/python/ray/_private/runtime_env/working_dir.py#L115) for reference. Note you don't have a ""EnvVarPlugin"" because we simply embed all env vars to the context in the first place.

## Logging

I think we have a bunch of logs logging the whole runtime env like [here](https://github.com/ray-project/ray/blob/master/python/ray/_private/runtime_env/agent/runtime_env_agent.py#L354) (search ""logger""). 

Short term: we hard code by never logging serialized runtime_env && only log the dict runtime_env with all entries whose keys contains ""secret"" removed.

Longer term (but still doable in one PR, your call): we add a method to `RuntimeEnvPlugin` namely `def show(runtime_env) -> str`. It defaults to return `json.dumps({self.name: runtime_env[self.name]})`, but for our plugin, it returns `json.dumps{""secret_env_vars"": f""**{len(runtime_env[""secret_env_vars""])} secrets**""}`

Next steps: write a unit test to load some secret env vars, and then read the logs to make sure all accidental logs are removed. 

## Dashboard

Now it shows a serialized runtime env, which would contain our secret. I am talking to dashboard people to seek a solution.",environment like best way natural extension already offer simple behavior part applied environment like never shown ray one specify via job submission client via ray job submit overview client side plain unparsed sent class server creation deletion pluggable structure kind pip registered agent constructor agent forward creation deletion new creation agent create worker process prepare worker deletion agent delete prepared create new like class validate return empty array create return extend secret return new new file ship first party function field use reference note simply embed context first place logging think bunch logging whole like search logger short term hard code never logging log whose secret removed longer term still doable one call add method namely show return next write unit test load secret read make sure accidental removed dashboard would contain secret talking dashboard people seek solution,issue,positive,positive,neutral,neutral,positive,positive
1773256812,"Me too tried. However as I said before, the docker in worker has not even getting started /running. So even putting  ""echo setup_command was run >> /tmp/ray_worker_output.txt"" as the first item in setup_commands of workers does't work.



2023-10-20 11:56:17,855 INFO monitor.py:385 -- Autoscaler has not yet received load metrics. Waiting.
2023-10-20 11:56:22,867 INFO autoscaler.py:141 -- The autoscaler took 0.0 seconds to fetch the list of non-terminated nodes.
2023-10-20 11:56:22,868 INFO autoscaler.py:421 -- 
======== Autoscaler status: 2023-10-20 11:56:22.868239 ========
Node status
---------------------------------------------------------------
Healthy:
 1 local.cluster.node
Pending:
 (no pending nodes)
Recent failures:
 (no failures)

Resources
---------------------------------------------------------------
Usage:
 0.0/12.0 CPU
 0.0/1.0 GPU
 0B/28.58GiB memory
 0B/14.29GiB object_store_memory

Demands:
 (no resource demands)
2023-10-20 11:56:22,869 INFO autoscaler.py:1379 -- StandardAutoscaler: Queue 1 new nodes for launch
2023-10-20 11:56:22,869 INFO autoscaler.py:464 -- The autoscaler took 0.002 seconds to complete the update iteration.
2023-10-20 11:56:22,869 INFO node_launcher.py:177 -- NodeLauncher0: Got 1 nodes to launch.

==> /tmp/ray/session_latest/logs/monitor.out <==

",tried however said docker worker even getting even echo run first item work yet received load metric waiting took fetch list status node status healthy pending pending recent usage memory resource queue new launch took complete update iteration got launch,issue,negative,positive,positive,positive,positive,positive
1773195145,Thanks for the review Sihan! Agreed this is great refactor and use of generators! Credit goes to Ed! ,thanks review agreed great use credit go,issue,positive,positive,positive,positive,positive,positive
1773177785,"Very nice job, @jonathan-anyscale. I just had some nits. I assumed the proper name of the Nsight profiler is Nsight Visual Profiler. If not, please ignore those suggestions. You may also want to use Vale (go/vale) to get quicker feedback on typos and general style guidelines. Let me know if you have any questions. 

Nice feedback, @emmyscode.",nice job assumed proper name profiler visual profiler please ignore may also want use vale get feedback general style let know nice feedback,issue,positive,positive,positive,positive,positive,positive
1773119647,"@pcmoritz If we improve the error message, do you think we still need to add something to the debugging doc?",improve error message think still need add something doc,issue,negative,neutral,neutral,neutral,neutral,neutral
1773116029,"> Will you be touching those? 

No

> If not, I can open a new PR to address that separately.

It will be great!
",touching open new address separately great,issue,positive,positive,positive,positive,positive,positive
1773111777,"I'd really like to suggest some changes to the titles of the questions in this section. Will you be touching those? If not, I can open a new PR to address that separately.",really like suggest section touching open new address separately,issue,negative,positive,positive,positive,positive,positive
1773107276,"@mfojtak So I was testing a bit more with the Kuberay with Ray Serve. You should be able to just use `{ }` as the runtime env like the following. This should use the files in the docker image instead of looking up on the remote location
```
  serveConfigV2: |
    applications:
      - name: mobilenet
        import_path: mobilenet.mobilenet:app
        runtime_env: { }
```
Then given you are brining in your own docker image built with this code, you should be able to start Serve! Please give it a try!!",testing bit ray serve able use like following use docker image instead looking remote location name given docker image built code able start serve please give try,issue,positive,positive,positive,positive,positive,positive
1773089798,"Chaos test, doc test, rllib test unrelated. k8s test failure unrelated (it fails at the step of `kubectl delete`)",chaos test doc test test unrelated test failure unrelated step delete,issue,negative,negative,negative,negative,negative,negative
1773074733,"I see, looks like we can use `pickle_dumps` from `serialization.py`:

```python
def pickle_dumps(obj: Any, error_msg: str):
    """"""Wrap cloudpickle.dumps to provide better error message
    when the object is not serializable.
    """"""
    try:
        return pickle.dumps(obj)
    except TypeError as e:
        sio = io.StringIO()
        inspect_serializability(obj, print_file=sio)
        msg = f""{error_msg}:\n{sio.getvalue()}""
        raise TypeError(msg) from e
```",see like use python wrap provide better error message object try return except raise,issue,negative,positive,positive,positive,positive,positive
1773072304,"@jmakov Thanks! I think this means the command was never run. I don't want to take up too much of your time with the back-and-forth here, but one thing that might help confirm this and narrow things down is if we add something like `""echo setup_command was run >> /tmp/ray_worker_output.txt""` as the first item in `setup_commands`.  

Another mystery is why the worker node `192.168.0.108` was able to join in your `monitor.log` above, but not the other worker nodes.",thanks think command never run want take much time one thing might help confirm narrow add something like echo run first item another mystery worker node able join worker,issue,positive,positive,positive,positive,positive,positive
1773050783,"@vitsai can you also merge/rebase on top of  the branch; it needs a fix it ray ci to build the docker image, thankks",also top branch need fix ray build docker image,issue,negative,positive,positive,positive,positive,positive
1773046710,"@sihanwang41 @zcin @shrekris-anyscale Ping again just in case if the notifications are missed. Please take a look when you have some time, thanks!",ping case please take look time thanks,issue,positive,positive,positive,positive,positive,positive
1773041134,"This has now escalated to be a release-blocker since the branch has been cut, so please help to prioritize. Thankkks, CC: @sven1977 ",since branch cut please help,issue,negative,neutral,neutral,neutral,neutral,neutral
1773038719,"> Given this isn't hooked up/implemented yet, let's wait until after branch cut to merge it.


> Given this isn't hooked up/implemented yet, let's wait until after branch cut to merge it.

the branch is cut already! https://github.com/ray-project/ray/tree/releases/2.8.0",given hooked yet let wait branch cut merge given hooked yet let wait branch cut merge branch cut already,issue,negative,neutral,neutral,neutral,neutral,neutral
1772969539,"w00t, i push https://github.com/ray-project/ray/pull/40525/files directly to 2.8.0 by mistake instead of creating a pick; but well, it's in now. I'll rebase this PR on top of 2.8.0 branch to see if the docker build is fixed.",push directly mistake instead pick well rebase top branch see docker build fixed,issue,negative,positive,positive,positive,positive,positive
1772961192,"Ah, the docker build doesn't work and that's my bad. This should be the fix: https://github.com/ray-project/ray/pull/40525. I'll pick in once tests pass.",ah docker build work bad fix pick pas,issue,negative,negative,negative,negative,negative,negative
1772816633,"I have the same issue. I trained my model on my PC and cloned the results dir. into Collab. now when I try to restored the trained agent, it seems like it cant read trials correctly!
Here is the warning log:

```
2023-10-20 13:39:07,135	WARNING experiment_analysis.py:205 -- Failed to fetch metrics for 1 trial(s):
- TD3_RankingEnv_ce7f8b78: FileNotFoundError('Could not fetch metrics for TD3_RankingEnv_ce7f8b78: both result.json and progress.csv were not found at /mainfs/scratch/sb5e19/RL_LTR/TD3_TRAIN/TD3_TRAIN/TD3_RankingEnv_ce7f8b78_1_AlgorithmConfig__prior_exploration_config=None,disable_action_flattening=False,disable_execution_plan_ap_2023-10-19_18-22-51')

<ray.tune.tuner.Tuner object at 0x7f22d0512200>
```
## Reproduction script
```ruby
class DRLlibv2:
    def __init__(
        self,
        trainable: str | Any,
        params: dict,
        train_env=None,
        run_name: str = ""tune_run"",
        local_dir: str = ""tune_results"",
        search_alg=None,
        concurrent_trials: int = 0,
        num_samples: int = 0,
        scheduler_=None,
        # num_cpus: float | int = 2,
        dataframe_save: str = ""tune.csv"",
        metric: str = ""episode_reward_mean"",
        mode: str | list[str] = ""max"",
        max_failures: int = 0,
        training_iterations: int = 100,
        checkpoint_num_to_keep: None | int = None,
        checkpoint_freq: int = 0,
        reuse_actors: bool = True
    ):
        self.params = params

        # if train_env is not None:
        #     register_env(self.params['env'], lambda env_config: train_env(env_config))


        self.train_env = train_env
        self.run_name = run_name
        self.local_dir = local_dir
        self.search_alg = search_alg
        if concurrent_trials != 0:
            self.search_alg = ConcurrencyLimiter(
                self.search_alg, max_concurrent=concurrent_trials
            )
        self.scheduler_ = scheduler_
        self.num_samples = num_samples
        self.trainable = trainable
        if isinstance(self.trainable, str):
            self.trainable = self.trainable.upper()
        # self.num_cpus = num_cpus
        self.dataframe_save = dataframe_save
        self.metric = metric
        self.mode = mode
        self.max_failures = max_failures
        self.training_iterations = training_iterations
        self.checkpoint_freq = checkpoint_freq
        self.checkpoint_num_to_keep = checkpoint_num_to_keep
        self.reuse_actors = reuse_actors

    def train_tune_model(self):

        # if ray.is_initialized():
        #   ray.shutdown()

        # ray.init(num_cpus=self.num_cpus, num_gpus=self.params['num_gpus'], ignore_reinit_error=True)

        if self.train_env is not None:
            register_env(self.params['env'], lambda env_config: self.train_env)


        tuner = tune.Tuner(
            self.trainable,
            param_space=self.params,
            tune_config=TuneConfig(
                search_alg=self.search_alg,
                scheduler=self.scheduler_,
                num_samples=self.num_samples,
                # metric=self.metric,
                # mode=self.mode,
                **({'metric': self.metric, 'mode': self.mode} if self.scheduler_ is None else {}),
                reuse_actors=self.reuse_actors,

            ),
            run_config=RunConfig(
                name=self.run_name,
                storage_path=self.local_dir,
                failure_config=FailureConfig(
                    max_failures=self.max_failures, fail_fast=False
                ),
                stop={""training_iteration"": self.training_iterations},
                checkpoint_config=CheckpointConfig(
                    num_to_keep=self.checkpoint_num_to_keep,
                    checkpoint_score_attribute=self.metric,
                    checkpoint_score_order=self.mode,
                    checkpoint_frequency=self.checkpoint_freq,
                    checkpoint_at_end=True,
                ),
                verbose=3,#Verbosity mode. 0 = silent, 1 = default, 2 = verbose, 3 = detailed
            ),
        )

        self.results = tuner.fit()
        if self.search_alg is not None:
            self.search_alg.save_to_dir(self.local_dir)
        # ray.shutdown()
        return self.results

    def infer_results(self, to_dataframe: str = None, mode: str = ""a""):

        results_df = self.results.get_dataframe()

        if to_dataframe is None:
            to_dataframe = self.dataframe_save

        results_df.to_csv(to_dataframe, mode=mode)

        best_result = self.results.get_best_result()
        # best_result = self.results.get_best_result()
        # best_metric = best_result.metrics
        # best_checkpoint = best_result.checkpoint
        # best_trial_dir = best_result.log_dir
        # results_df = self.results.get_dataframe()

        return results_df, best_result

    def restore_agent(
        self,
        checkpoint_path: str = """",
        restore_search: bool = False,
        resume_unfinished: bool = True,
        resume_errored: bool = False,
        restart_errored: bool = False,
    ):

        # if restore_search:
        # self.search_alg = self.search_alg.restore_from_dir(self.local_dir)
        if checkpoint_path == """":
            checkpoint_path = self.results.get_best_result().checkpoint._local_path

        restored_agent = tune.Tuner.restore(
            checkpoint_path, trainable = self.trainable,
            param_space=self.params,
            restart_errored=restart_errored,
            resume_unfinished=resume_unfinished,
            resume_errored=resume_errored,
        )
        print(restored_agent)
        self.results = restored_agent.get_results()

        if self.search_alg is not None:
            self.search_alg.save_to_dir(self.local_dir)
        return self.results

    def get_test_agent(self, test_env_name: str=None, test_env=None, checkpoint=None):

        # if test_env is not None:
        #     register_env(test_env_name, lambda config: [test_env])

        if checkpoint is None:
            checkpoint = self.results.get_best_result().checkpoint

        testing_agent = Algorithm.from_checkpoint(checkpoint)
        # testing_agent.config['env'] = test_env_name

        return testing_agent
```
```ruby
drl_agent = DRLlibv2(
    trainable=""TD3"",
    # train_env = RankingEnv,
    # num_cpus = num_cpus,
    run_name = ""TD3_TRAIN"",
    local_dir = local_dir,
    params = train_config.to_dict(),
    num_samples = 1,#Number of samples of hyperparameters config to run
    # training_iterations=5,
    checkpoint_freq=5,
    # scheduler_=scheduler_,
    search_alg=search_alg,
    metric = ""episode_reward_mean"",
    mode = ""max""
    # callbacks=[wandb_callback]
)
```
```ruby
results = drl_agent.restore_agent((local_dir/""TD3_TRAIN"").as_posix())
```",issue trained model try trained agent like cant read correctly warning log warning fetch metric trial fetch metric found object reproduction script ruby class self trainable float metric mode list none none bool true none lambda trainable metric mode self none lambda tuner none else verbosity mode silent default verbose detailed none return self none mode none return self bool false bool true bool false bool false trainable print none return self none lambda none return ruby number run metric mode ruby,issue,positive,negative,neutral,neutral,negative,negative
1772712572,@pcmoritz isn't `cloudpickle.dumps` going through the same codepath as other Ray object serialization? Or do we have some wrappers that are improving the error messages?,going ray object serialization improving error,issue,negative,neutral,neutral,neutral,neutral,neutral
1772710415,"Given this isn't hooked up/implemented yet, let's wait until after branch cut to merge it.",given hooked yet let wait branch cut merge,issue,negative,neutral,neutral,neutral,neutral,neutral
1772643654,"@architkulkarni I've added `ulimit -c unlimited && ray start --address=$RAY_HEAD_IP:6379 --disable-usage-stats >> /tmp/ray_worker_output.txt 2>&1` and get
```shell
ls /tmp/ray_worker_output.txt
ls: cannot access '/tmp/ray_worker_output.txt': No such file or directory
```",added unlimited ray start get shell access file directory,issue,negative,neutral,neutral,neutral,neutral,neutral
1772533106,"Hi @GeneDer I have even created a proxy server such that the url ends with .zip. First problem with this is that serve doesn't even accept ""http"" protocol. So I created https with self signed certificate only to find out serve fails there as well. So I have hit a concrete wall. In my humble opinion - this is a BUG.
Regarding contribution - I do not have expertise to do this. Is there any other option?",hi even proxy server first problem serve even accept protocol self certificate find serve well hit concrete wall humble opinion bug regarding contribution option,issue,negative,positive,neutral,neutral,positive,positive
1772135452,"@pcmoritz  @gresavage   Thanks, I will upgrade my python version to 3.8.x",thanks upgrade python version,issue,negative,positive,positive,positive,positive,positive
1772134848,"Thank @angelinalg for the review! I chatted with @hongchaodeng offline, and we decided to not only improve the wording but also add more details about why users should use RayService for model serving and RayCluster for prototyping. Hence, I convert this PR back to draft.",thank review decided improve wording also add use model serving hence convert back draft,issue,positive,neutral,neutral,neutral,neutral,neutral
1772126716,"> Should the .tune_metadata binary file changes also be reverted?

I think so, double confirm with rllib team.",binary file also think double confirm team,issue,negative,neutral,neutral,neutral,neutral,neutral
1772036469,"Also here are segments of the tracebacks from tune - all for the same experiment. This trial errored initially for other reasons, but when `Tune` tried to resume/continue it resulted in these different errors. I think it's important to note that even if one of my `Tune` trial doesn't error the aforementioned files are empty, and that these errors are referring to the existence of checkpoint files and some strange behavior with `importlib` so they may be a useful breadcrumb:

```python
2023-10-19 23:38:55,946 ERROR tune_controller.py:1502 -- Trial task failed for trial LL-PPO-MT-VFDPPO_CorlMultiAgentEnv_70f3f_00000
Traceback (most recent call last):
  File ""/home/tgresavage/mambaforge/envs/vfd_env_2/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py"", line 110, in resolve_future
    result = ray.get(future)
  File ""/home/tgresavage/mambaforge/envs/vfd_env_2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py"", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File ""/home/tgresavage/mambaforge/envs/vfd_env_2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
  File ""/home/tgresavage/mambaforge/envs/vfd_env_2/lib/python3.10/site-packages/ray/_private/worker.py"", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): ray::VFDPPO.restore() (pid=2211586, ip=192.168.86.56, actor_id=f26abfbcb38b2bc4534378d401000000, repr=VFDPPO)
  File ""/home/tgresavage/mambaforge/envs/vfd_env_2/lib/python3.10/site-packages/ray/tune/trainable/trainable.py"", line 976, in restore
    self.load_checkpoint(checkpoint_dir)
  File ""/home/tgresavage/mambaforge/envs/vfd_env_2/lib/python3.10/site-packages/corl/experiments/rllib_experiment.py"", line 528, in load_checkpoint
    super(trainer_class, cls).load_checkpoint(checkpoint_path)  # type: ignore
  File ""/home/tgresavage/mambaforge/envs/vfd_env_2/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py"", line 2152, in load_checkpoint
    self.__setstate__(checkpoint_data)
  File ""/home/tgresavage/mambaforge/envs/vfd_env_2/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py"", line 2595, in __setstate__
    self.workers.local_worker().set_state(state[""worker""])
  File ""/home/tgresavage/mambaforge/envs/vfd_env_2/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py"", line 1454, in set_state
    self.policy_map[pid].set_state(policy_state)
  File ""/home/tgresavage/mambaforge/envs/vfd_env_2/lib/python3.10/site-packages/ray/rllib/policy/torch_mixins.py"", line 114, in set_state
    super().set_state(state)
  File ""/home/tgresavage/mambaforge/envs/vfd_env_2/lib/python3.10/site-packages/ray/rllib/policy/torch_policy_v2.py"", line 1091, in set_state
    super().set_state(state)
  File ""/home/tgresavage/mambaforge/envs/vfd_env_2/lib/python3.10/site-packages/ray/rllib/policy/policy.py"", line 1059, in set_state
    policy_spec = PolicySpec.deserialize(state[""policy_spec""])
  File ""/home/tgresavage/mambaforge/envs/vfd_env_2/lib/python3.10/site-packages/ray/rllib/policy/policy.py"", line 161, in deserialize
    policy_class = get_policy_class(spec[""policy_class""])
  File ""/home/tgresavage/mambaforge/envs/vfd_env_2/lib/python3.10/site-packages/ray/rllib/algorithms/registry.py"", line 451, in get_policy_class
    module = importlib.import_module(""ray.rllib.algorithms."" + path)
TypeError: can only concatenate str (not ""ABCMeta"") to str

Trial LL-PPO-MT-VFDPPO_CorlMultiAgentEnv_70f3f_00000 errored after 39 iterations at 2023-10-19 23:38:55. Total running time: 26min 52s
Error file: /tmp/data/VFD/PPO-MT/LL/ray_results/LL-PPO-MT/LL-PPO-MT-VFDPPO_CorlMultiAgentEnv_70f3f_00000_0_2023-10-19_23-12-03/error.txt
```

```python
2023-10-19 23:39:34,036 ERROR tune_controller.py:1502 -- Trial task failed for trial LL-PPO-MT-VFDPPO_CorlMultiAgentEnv_70f3f_00000
Traceback (most recent call last):
  File ""/home/tgresavage/mambaforge/envs/vfd_env_2/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py"", line 110, in resolve_future
    result = ray.get(future)
  File ""/home/tgresavage/mambaforge/envs/vfd_env_2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py"", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File ""/home/tgresavage/mambaforge/envs/vfd_env_2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
  File ""/home/tgresavage/mambaforge/envs/vfd_env_2/lib/python3.10/site-packages/ray/_private/worker.py"", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(AttributeError): ray::VFDPPO.restore() (pid=2213824, ip=192.168.86.56, actor_id=06d75ce2034489b567e20a3d01000000, repr=VFDPPO)
  File ""/home/tgresavage/mambaforge/envs/vfd_env_2/lib/python3.10/site-packages/ray/tune/trainable/trainable.py"", line 954, in restore
    if not _exists_at_fs_path(checkpoint.filesystem, checkpoint.path):
AttributeError: 'NoneType' object has no attribute 'filesystem'

Trial LL-PPO-MT-VFDPPO_CorlMultiAgentEnv_70f3f_00000 errored after 39 iterations at 2023-10-19 23:39:34. Total running time: 27min 30s
Error file: /tmp/data/VFD/PPO-MT/LL/ray_results/LL-PPO-MT/LL-PPO-MT-VFDPPO_CorlMultiAgentEnv_70f3f_00000_0_2023-10-19_23-12-03/error.txt
```

Be aware that the `error.txt` file mentioned is also empty.",also tune experiment trial initially tune tried different think important note even one tune trial error empty existence strange behavior may useful python error trial task trial recent call last file line result future file line return file line wrapper return file line get raise ray file line restore file line super type ignore file line file line state worker file line file line super state file line super state file line state file line spec file line module path concatenate trial total running time min error file python error trial task trial recent call last file line result future file line return file line wrapper return file line get raise ray file line restore object attribute trial total running time min error file aware file also empty,issue,negative,positive,neutral,neutral,positive,positive
1772034605,"You and I may be having similar issues.

After following @krfricke comment please also make sure `builder.py` is present in the `site-packages` for `protobuf`. There is an issue with `protobuf<3.20` where `builder.py` is missing which ultimately causes the TF event files to be empty. See [here](https://stackoverflow.com/questions/71759248/importerror-cannot-import-name-builder-from-google-protobuf-internal) for more details.

Unfortunately the dependencies for tensorflow work out in a way such that we are stuck with a problematic release of `protobuf`

If you find you're still having issues after that then I think you and I may be experiencing the same or related issues. In my case, however, I get *some* initial data in the TF event files and progress.csv, but after  a seemingly arbitrary number of iterations all of the files under the trial directory are present but completely empty. I've attached some screenshots to demonstrate how data was being recorded in tensorboard and the TF event files/progress.csv up to a point but now all the files are mysteriously empty.

I will try to get a minimum working example script attached to this thread soon - FWIW I always use `Tune.run()` for my experiments and have recently been doing a lot of testing with the new RL Module and Learner APIs... I cannot remember at this moment whether the issue occurs under the old ModelV2 API

This was not an issue I had experienced prior with Ray 2.6 or below. Please LMK if this seems similar to your issue, otherwise I will open a separate issue for the problems I'm having.

![image](https://github.com/ray-project/ray/assets/7985947/86d106c8-da9b-4273-9fd9-d855a58b9127)

![image](https://github.com/ray-project/ray/assets/7985947/d2ee8f65-8e5a-4067-93c7-8a7d806faba3)
",may similar following comment please also make sure present issue missing ultimately event empty see unfortunately work way stuck problematic release find still think may related case however get initial data event seemingly arbitrary number trial directory present completely empty attached demonstrate data event point mysteriously empty try get minimum working example script attached thread soon always use recently lot testing new module learner remember moment whether issue old issue experienced prior ray please similar issue otherwise open separate issue image image,issue,negative,positive,neutral,neutral,positive,positive
1772018304,"> @lyzyn我上面提到的Windows路径问题在2.7.0中得到了解决（感谢@krfricke），但感觉这不是你遇到的。
Okay, thank you! I am using Ray version 2.7.0, but I have also encountered the following path issue and reported an error. Have you ever encountered this issue?
  **File ""pyarrow\_fs.pyx"", line 348, in pyarrow._fs.FileSystem.from_uri
  File ""pyarrow\error.pxi"", line 143, in pyarrow.lib.pyarrow_internal_check_status
  File ""pyarrow\error.pxi"", line 99, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: URI has empty scheme: '~/ray_result'**",thank ray version also following path issue error ever issue file line file line file line empty scheme,issue,negative,negative,neutral,neutral,negative,negative
1771995817,"Merging with master is an ongoing process. Until this can get some review it probably is not worth the effort. FWIW, this is working well on conda-forge, where it has already been adopted. Is there a reason _not_ to do this?",master ongoing process get review probably worth effort working well already adopted reason,issue,negative,positive,positive,positive,positive,positive
1771903858,"Heavy +1 to a better error message.

Another suggestion to provide more guidance: Can we add comments in the code examples that can concisely explain the best practice that will avoid this gotcha?

```python
import os
# If you have other import statements, put them within the function to avoid possible pickling errors.

# When the env var is updated, users see new return value.
msg = os.getenv(""SERVE_RESPONSE_MESSAGE"", ""Hello world!"")


class HelloWorld:
    def hello(self):
        return msg
```",heavy better error message another suggestion provide guidance add code concisely explain best practice avoid python import o import put within function avoid possible see new return value hello world class hello self return,issue,positive,positive,positive,positive,positive,positive
1771902188,"Same for actors:

```python
In [9]: @ray.remote
   ...: class Actor:
   ...:     def __init__(self):
   ...:         completion()
   ...: 

In [10]: a = Actor.remote()
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
File ~/ray/python/ray/_private/serialization.py:65, in pickle_dumps(obj, error_msg)
     64 try:
---> 65     return pickle.dumps(obj)
     66 except TypeError as e:

File ~/ray/python/ray/cloudpickle/cloudpickle_fast.py:88, in dumps(obj, protocol, buffer_callback)
     87 cp = CloudPickler(file, protocol=protocol, buffer_callback=buffer_callback)
---> 88 cp.dump(obj)
     89 return file.getvalue()

File ~/ray/python/ray/cloudpickle/cloudpickle_fast.py:733, in CloudPickler.dump(self, obj)
    732 try:
--> 733     return Pickler.dump(self, obj)
    734 except RuntimeError as e:

TypeError: cannot pickle 'builtins.CoreBPE' object

The above exception was the direct cause of the following exception:

TypeError                                 Traceback (most recent call last)
Input In [10], in <cell line: 1>()
----> 1 a = Actor.remote()

File ~/ray/python/ray/actor.py:536, in ActorClass.remote(self, *args, **kwargs)
    524 def remote(self, *args, **kwargs):
    525     """"""Create an actor.
    526 
    527     Args:
   (...)
    534         A handle to the newly created actor.
    535     """"""
--> 536     return self._remote(args=args, kwargs=kwargs, **self._default_options)

File ~/ray/python/ray/_private/auto_init_hook.py:24, in wrap_auto_init.<locals>.auto_init_wrapper(*args, **kwargs)
     21 @wraps(fn)
     22 def auto_init_wrapper(*args, **kwargs):
     23     auto_init_ray()
---> 24     return fn(*args, **kwargs)

File ~/ray/python/ray/util/tracing/tracing_helper.py:388, in _tracing_actor_creation.<locals>._invocation_actor_class_remote_span(self, args, kwargs, *_args, **_kwargs)
    386 if not _is_tracing_enabled():
    387     assert ""_ray_trace_ctx"" not in kwargs
--> 388     return method(self, args, kwargs, *_args, **_kwargs)
    390 class_name = self.__ray_metadata__.class_name
    391 method_name = ""__init__""

File ~/ray/python/ray/actor.py:849, in ActorClass._remote(self, args, kwargs, **actor_options)
    844     meta.last_export_session_and_job = worker.current_session_and_job
    845     # After serialize / deserialize modified class, the __module__
    846     # of modified class will be ray.cloudpickle.cloudpickle.
    847     # So, here pass actor_creation_function_descriptor to make
    848     # sure export actor class correct.
--> 849     worker.function_actor_manager.export_actor_class(
    850         meta.modified_class,
    851         meta.actor_creation_function_descriptor,
    852         meta.method_meta.methods.keys(),
    853     )
    855 resources = ray._private.utils.resources_from_ray_options(actor_options)
    856 # Set the actor's default resources if not already set. First three
    857 # conditions are to check that no resources were specified in the
    858 # decorator. Last three conditions are to check that no resources were
   (...)
    861 # when deciding the default CPUs. It is strange, but we keep the original
    862 # semantics in case that it breaks user applications & tests.

File ~/ray/python/ray/_private/function_manager.py:517, in FunctionActorManager.export_actor_class(self, Class, actor_creation_function_descriptor, actor_method_names)
    511 job_id = self._worker.current_job_id
    512 key = make_function_table_key(
    513     b""ActorClass"",
    514     job_id,
    515     actor_creation_function_descriptor.function_id.binary(),
    516 )
--> 517 serialized_actor_class = pickle_dumps(
    518     Class,
    519     f""Could not serialize the actor class ""
    520     f""{actor_creation_function_descriptor.repr}"",
    521 )
    522 actor_class_info = {
    523     ""class_name"": actor_creation_function_descriptor.class_name.split(""."")[-1],
    524     ""module"": actor_creation_function_descriptor.module_name,
   (...)
    528     ""actor_method_names"": json.dumps(list(actor_method_names)),
    529 }
    531 check_oversized_function(
    532     actor_class_info[""class""],
    533     actor_class_info[""class_name""],
    534     ""actor"",
    535     self._worker,
    536 )

File ~/ray/python/ray/_private/serialization.py:70, in pickle_dumps(obj, error_msg)
     68 inspect_serializability(obj, print_file=sio)
     69 msg = f""{error_msg}:\n{sio.getvalue()}""
---> 70 raise TypeError(msg) from e

TypeError: Could not serialize the actor class __main__.Actor.__init__:
===========================================================================
Checking Serializability of <class '__main__._modify_class.<locals>.Class'>
===========================================================================
!!! FAIL serialization: cannot pickle 'builtins.CoreBPE' object
    Serializing '__init__' <function Actor.__init__ at 0x7fc438bd6160>...
    !!! FAIL serialization: cannot pickle 'builtins.CoreBPE' object
    Detected 7 global variables. Checking serializability...
        Serializing '_is_tracing_enabled' <function _is_tracing_enabled at 0x7fc410034d30>...
        Serializing '_opentelemetry' None...
        Serializing '__name__' ray.util.tracing.tracing_helper...
        Serializing '_use_context' <function _use_context at 0x7fc438818550>...
        Serializing '_DictPropagator' <class 'ray.util.tracing.tracing_helper._DictPropagator'>...
        Serializing '_actor_span_consumer_name' <function _actor_span_consumer_name at 0x7fc4388188b0>...
        Serializing '_actor_hydrate_span_args' <function _actor_hydrate_span_args at 0x7fc438818790>...
    Detected 1 nonlocal variables. Checking serializability...
        Serializing 'method' <function Actor.__init__ at 0x7fc438c32af0>...
        !!! FAIL serialization: cannot pickle 'builtins.CoreBPE' object
        Detected 1 global variables. Checking serializability...
            Serializing 'completion' <function client.<locals>.wrapper at 0x7fc450530040>...
            !!! FAIL serialization: cannot pickle 'builtins.CoreBPE' object
===========================================================================
Variable: 

	FailTuple(completion [obj=<function client.<locals>.wrapper at 0x7fc450530040>, parent=<function Actor.__init__ at 0x7fc438c32af0>])

was found to be non-serializable. There may be multiple other undetected variables that were non-serializable. 
Consider either removing the instantiation/imports of these variables or moving the instantiation into the scope of the function/class. 
===========================================================================
Check https://docs.ray.io/en/master/ray-core/objects/serialization.html#troubleshooting for more information.
If you have any suggestions on how to improve this error message, please reach out to the Ray developers on github.com/ray-project/ray/issues/
===========================================================================


In [11]: 
```",python class actor self completion recent call last file try return except file protocol file return file self try return self except pickle object exception direct cause following exception recent call last input cell line file self remote self create actor handle newly return file return file self assert return method self file self serialize class class pas make sure export actor class correct set actor default already set first three check decorator last three check default strange keep original semantics case user file self class key class could serialize actor class module list class actor file raise could serialize actor class class fail serialization pickle object function fail serialization pickle object global function none function class function function nonlocal function fail serialization pickle object global function fail serialization pickle object variable completion function function found may multiple undetected consider either removing moving scope check information improve error message please reach ray,issue,negative,negative,neutral,neutral,negative,negative
1771901147,"Actually for tasks we already give a decent error message for tasks:

```python
In [3]: from litellm import completion

In [4]: completion?
Signature: completion(*args, **kwargs)
Docstring: <no docstring>
File:      ~/anaconda3/lib/python3.8/site-packages/litellm/utils.py
Type:      function

In [6]: import ray

In [7]: @ray.remote
   ...: def f():
   ...:     completion()
   ...: 

In [8]: ray.get(f.remote())
2023-10-19 17:49:16,020	INFO worker.py:1659 -- Started a local Ray instance. View the dashboard at http://127.0.0.1:8265 
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
File ~/ray/python/ray/_private/serialization.py:65, in pickle_dumps(obj, error_msg)
     64 try:
---> 65     return pickle.dumps(obj)
     66 except TypeError as e:

File ~/ray/python/ray/cloudpickle/cloudpickle_fast.py:88, in dumps(obj, protocol, buffer_callback)
     87 cp = CloudPickler(file, protocol=protocol, buffer_callback=buffer_callback)
---> 88 cp.dump(obj)
     89 return file.getvalue()

File ~/ray/python/ray/cloudpickle/cloudpickle_fast.py:733, in CloudPickler.dump(self, obj)
    732 try:
--> 733     return Pickler.dump(self, obj)
    734 except RuntimeError as e:

TypeError: cannot pickle 'builtins.CoreBPE' object

The above exception was the direct cause of the following exception:

TypeError                                 Traceback (most recent call last)
Input In [8], in <cell line: 1>()
----> 1 ray.get(f.remote())

File ~/ray/python/ray/remote_function.py:138, in RemoteFunction.__init__.<locals>._remote_proxy(*args, **kwargs)
    136 @wraps(function)
    137 def _remote_proxy(*args, **kwargs):
--> 138     return self._remote(args=args, kwargs=kwargs, **self._default_options)

File ~/ray/python/ray/_private/auto_init_hook.py:24, in wrap_auto_init.<locals>.auto_init_wrapper(*args, **kwargs)
     21 @wraps(fn)
     22 def auto_init_wrapper(*args, **kwargs):
     23     auto_init_ray()
---> 24     return fn(*args, **kwargs)

File ~/ray/python/ray/util/tracing/tracing_helper.py:310, in _tracing_task_invocation.<locals>._invocation_remote_span(self, args, kwargs, *_args, **_kwargs)
    308     if kwargs is not None:
    309         assert ""_ray_trace_ctx"" not in kwargs
--> 310     return method(self, args, kwargs, *_args, **_kwargs)
    312 assert ""_ray_trace_ctx"" not in kwargs
    313 tracer = _opentelemetry.trace.get_tracer(__name__)

File ~/ray/python/ray/remote_function.py:299, in RemoteFunction._remote(self, args, kwargs, **task_options)
    287 self._function_descriptor = PythonFunctionDescriptor.from_function(
    288     self._function, self._uuid
    289 )
    290 # There is an interesting question here. If the remote function is
    291 # used by a subsequent driver (in the same script), should the
    292 # second driver pickle the function again? If yes, then the remote
   (...)
    297 # first driver. This is an argument for repickling the function,
    298 # which we do here.
--> 299 self._pickled_function = pickle_dumps(
    300     self._function,
    301     f""Could not serialize the function {self._function_descriptor.repr}"",
    302 )
    304 self._last_export_session_and_job = worker.current_session_and_job
    305 worker.function_actor_manager.export(self)

File ~/ray/python/ray/_private/serialization.py:70, in pickle_dumps(obj, error_msg)
     68 inspect_serializability(obj, print_file=sio)
     69 msg = f""{error_msg}:\n{sio.getvalue()}""
---> 70 raise TypeError(msg) from e

TypeError: Could not serialize the function __main__.f:
==========================================================
Checking Serializability of <function f at 0x7fc488b31040>
==========================================================
!!! FAIL serialization: cannot pickle 'builtins.CoreBPE' object
Detected 1 global variables. Checking serializability...
    Serializing 'completion' <function client.<locals>.wrapper at 0x7fc450530040>...
    !!! FAIL serialization: cannot pickle 'builtins.CoreBPE' object
    Detected 11 global variables. Checking serializability...
        Serializing 'datetime' <module 'datetime' from '/Users/pcmoritz/anaconda3/lib/python3.8/datetime.py'>...
        Serializing 'uuid' <module 'uuid' from '/Users/pcmoritz/anaconda3/lib/python3.8/uuid.py'>...
        Serializing 'litellm' <module 'litellm' from '/Users/pcmoritz/anaconda3/lib/python3.8/site-packages/litellm/__init__.py'>...
        Serializing 'BudgetExceededError' <class 'litellm.exceptions.BudgetExceededError'>...
        Serializing 'Cache' <class 'litellm.caching.Cache'>...
        Serializing 'print_verbose' <function print_verbose at 0x7fc458820310>...
        Serializing 'threading' <module 'threading' from '/Users/pcmoritz/anaconda3/lib/python3.8/threading.py'>...
        Serializing 'handle_success' <function handle_success at 0x7fc468c37430>...
        Serializing 'traceback' <module 'traceback' from '/Users/pcmoritz/anaconda3/lib/python3.8/traceback.py'>...
        Serializing 'handle_failure' <function handle_failure at 0x7fc468c373a0>...
        Serializing 'liteDebuggerClient' None...
    Detected 3 nonlocal variables. Checking serializability...
        Serializing 'crash_reporting' <function client.<locals>.crash_reporting at 0x7fc450522f70>...
        Serializing 'function_setup' <function client.<locals>.function_setup at 0x7fc450522dc0>...
        !!! FAIL serialization: cannot pickle 'builtins.CoreBPE' object
        Detected 10 global variables. Checking serializability...
            Serializing 'litellm' <module 'litellm' from '/Users/pcmoritz/anaconda3/lib/python3.8/site-packages/litellm/__init__.py'>...
            Serializing 'print_verbose' <function print_verbose at 0x7fc458820310>...
            Serializing 'callback_list' []...
            Serializing 'set_callbacks' <function set_callbacks at 0x7fc468c37310>...
            Serializing 'add_breadcrumb' None...
            Serializing 'user_logger_fn' None...
            Serializing '__name__' litellm.utils...
            Serializing 'CallTypes' <enum 'CallTypes'>...
            Serializing 'Logging' <class 'litellm.utils.Logging'>...
            Serializing 'traceback' <module 'traceback' from '/Users/pcmoritz/anaconda3/lib/python3.8/traceback.py'>...
        Detected 2 nonlocal variables. Checking serializability...
            Serializing 'crash_reporting' <function client.<locals>.crash_reporting at 0x7fc450522f70>...
            Serializing 'original_function' <function completion at 0x7fc450522ee0>...
            !!! FAIL serialization: cannot pickle 'builtins.CoreBPE' object
==========================================================
Variable: 

	FailTuple(original_function [obj=<function completion at 0x7fc450522ee0>, parent=<function client.<locals>.function_setup at 0x7fc450522dc0>])

was found to be non-serializable. There may be multiple other undetected variables that were non-serializable. 
Consider either removing the instantiation/imports of these variables or moving the instantiation into the scope of the function/class. 
==========================================================
Check https://docs.ray.io/en/master/ray-core/objects/serialization.html#troubleshooting for more information.
If you have any suggestions on how to improve this error message, please reach out to the Ray developers on github.com/ray-project/ray/issues/
==========================================================


In [9]: 

```

So I believe we just need to add the failure case to https://docs.ray.io/en/master/ray-core/objects/serialization.html#troubleshooting.",actually already give decent error message python import completion completion signature completion file type function import ray completion local ray instance view dashboard recent call last file try return except file protocol file return file self try return self except pickle object exception direct cause following exception recent call last input cell line file function return file return file self none assert return method self assert tracer file self interesting question remote function used subsequent driver script second driver pickle function yes remote first driver argument function could serialize function self file raise could serialize function function fail serialization pickle object global function fail serialization pickle object global module module module class class function module function module function none nonlocal function function fail serialization pickle object global module function function none none class module nonlocal function function completion fail serialization pickle object variable function completion function found may multiple undetected consider either removing moving scope check information improve error message please reach ray believe need add failure case,issue,negative,negative,neutral,neutral,negative,negative
1771840808,"@larrylian You are probably using the code from Ray master here, right? As stated above, we are discontinuing Python 3.7 support for the upcoming Ray 2.8 release (similar to many other libraries in the Python ecosystem, since Python 3.7 is EOL). Since Ray 2.8 will be released from current master, we are already cleaning up the code in current master.

Python 3.7 will continue to work for older Ray releases <= 2.7 :)",probably code ray master right stated python support upcoming ray release similar many python ecosystem since python since ray current master already cleaning code current master python continue work older ray,issue,negative,positive,positive,positive,positive,positive
1771821099,"Actually, when I reproduced the issue earlier, I had forgotten to open all the ports.  After opening all ports, I wasn't able to reproduce the issue.

@jmakov or @ajaichemmanam if you're able to reproduce the issue and you have time, it would potentially be very helpful if you could amend your YAML file as follows:

```
worker_start_ray_commands:
    - ray stop
    - ""echo \""Executing: ray start --address=$RAY_HEAD_IP:6379\"" >> ray_worker_output.txt""
    - ray start --address=$RAY_HEAD_IP:6379 >> ray_worker_output.txt 2>&1
```

And share the `ray_worker_output.txt` from the failing worker nodes.  (Or do modify the commands in any way you see fit, as long as we can see the output of `ray start --address=...`)",actually issue forgotten open opening able reproduce issue able reproduce issue time would potentially helpful could amend file ray stop echo ray start ray start share failing worker modify way see fit long see output ray start,issue,negative,positive,positive,positive,positive,positive
1771800177,"GPU doctest failure unrelated. (This PR just adds markdown text)
Linkcheck unreltaed (no links were modified)
Windows tune failures unrelated",failure unrelated markdown text link tune unrelated,issue,negative,negative,negative,negative,negative,negative
1771769132,"> @edoakes Setting `max_concurrent_queries=1` for Deployment will block all `await` calls in my FastAPI Ingress. _For example_ if you modify Ingress __call__ for
> 
> ```
> async def __call__(self, total):
>         # self.refs = [self.handle.remote(i) for i in range(total)] 
>         self.refs = []
>         for i in range(total):
>             dep_han = self.handle.remote(i)
>             self.refs.append(dep_han)
>             # next line will block everything
>             obj_ref = await dep_han._to_object_ref() # any await call, just to demonstrate
>             print('Job #', i, 'ref', obj_ref)
> ```
> 
> At each iteration this will get blocked and wait for a job to end (and unblock event loop probably?). Without `max_concurrent_queries=1` it will not block and create all jobs on the spot.
> 
> Even thou i got a ref to my job result in time, i dont know how to retrieve those results without blocking everything and waiting. Does any of that make any sense or Im doing it wrong?

@bakeryproducts the `self.handle.remote()` call will not block on assignment, so even if the replica cannot currently process the request it will return immediately. `_to_object_ref` does need to wait for the request to be assigned to a replica though.",setting deployment block await ingres modify ingres self total range total range total next line block everything await await call demonstrate print iteration get blocked wait job end unblock event loop probably without block create spot even thou got ref job result time dont know retrieve without blocking everything waiting make sense wrong call block assignment even replica currently process request return immediately need wait request assigned replica though,issue,negative,negative,neutral,neutral,negative,negative
1771686765,@rk0n thanks for reporting this issue. At the moment python 3.11 support is still in experimental. Will add this to the todo when we productionize python 3.11 support in the near future ,thanks issue moment python support still experimental add python support near future,issue,positive,positive,neutral,neutral,positive,positive
1771659030,"Ah yeah, we should move these dependencies into requirements.in to make the test hermetic: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/byod_alpa_test.sh#L5",ah yeah move make test hermetic,issue,negative,neutral,neutral,neutral,neutral,neutral
1771653246,"Seems to be due to a version incompatibility between `transformers` and `numpy`.

```
Traceback (most recent call last):
  File ""train_opt_2_7b_minimum.py"", line 518, in <module>
    main()
  File ""train_opt_2_7b_minimum.py"", line 511, in main
    save_checkpoint(state, model, tokenizer, training_args)
  File ""train_opt_2_7b_minimum.py"", line 307, in save_checkpoint
    model.save_pretrained(training_args.output_dir, params=params)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/transformers/modeling_flax_utils.py"", line 992, in save_pretrained
    shards, index = flax_shard_checkpoint(params if params is not None else self.params, max_shard_size)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/transformers/modeling_flax_utils.py"", line 125, in flax_shard_checkpoint
    weight_size = weights[item].size * dtype_byte_size(weights[item].dtype)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/transformers/modeling_flax_utils.py"", line 85, in dtype_byte_size
    if dtype == np.bool:
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/numpy/__init__.py"", line 305, in __getattr__
    raise AttributeError(__former_attrs__[attr])
AttributeError: module 'numpy' has no attribute 'bool'.
`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:
    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
```",due version incompatibility recent call last file line module main file line main state model file line file line index none else file line item item file line file line raise module attribute alias bool avoid error code use bool modify behavior safe specifically scalar type use originally guidance see original release note,issue,negative,positive,positive,positive,positive,positive
1771591960,"w00h00, thanks @architkulkarni, feel free to close this issue then and that will unjail this test automatically",thanks feel free close issue test automatically,issue,positive,positive,positive,positive,positive,positive
1771569944,"@can-anyscale Added another commit to the PR to retrigger the tests and it passed again:
- nightly https://buildkite.com/ray-project/release-tests-pr/builds/56261#018b44e9-9434-454c-bfa8-b86171e93b35
- latest https://buildkite.com/ray-project/release-tests-pr/builds/56261#018b44e9-9431-4f00-b969-29740086434c ",added another commit nightly latest,issue,negative,positive,positive,positive,positive,positive
1771490739,"@chaowanggg Still experiencing this with ray `2.7.1` can you point me towards a stable release? 
",still ray point towards stable release,issue,negative,neutral,neutral,neutral,neutral,neutral
1771480692,"I asked a similar question [here](https://discuss.ray.io/t/initialize-model-parameters-in-rlmodules/12417)

I would very much like to be able to initialize weights and biases as well - especially since this feature was available in Model V2 API and as @simonsays1980 mentioned good initialization can be the difference between whether or not a deep network learns. I think using the same kind of logic that's in `get_activation_fn` would be helpful in this regard.

In the old API, AFAIK, the initializer was either `xavier`, `normal`, or a custom callable. *As an enhancement* it would be nice to see full support for `torch`, `tf`, and `jax` initializers similar to activations rather than just a subset. I would also like to see the ability to specify configurations to pass to both the activations and initializers as keyword arguments.

I have done this already in a private repository on which I work:

1. using `inspect` to dynamically make a mapping from strings to initializers supported by the framework (thus any time a new initializer is added it is automatically supported)
2. using `inspect` along with decorators to wrap the activation and initializer in a `functools.partial` when a configuration dict is passed.

So an example configuration looks something like:

```yaml
...
framework: 'torch'
...
model:
  fcnet_activation: 'tanh'
  fcnet_activation_config: {}
  fcnet_initializer: 'orthogonal_'
  fcnet_initializer_config:
    gain: 1.41
...
```

I obviously also have custom RL Modules to accept these additional `kwargs` so the added functionality is actually used by the MLP heads.

Another alternative, or in addition, to using `inspect` is using `pydantic` where specifying an object type as `PyObject` allows for automatically mapping a valid import string to a python object. So pydantic will see `'torch.nn.init.orthogonal_'` and attempt to import it.",similar question would much like able initialize well especially since feature available model good difference whether deep network think kind logic would helpful regard old either normal custom callable enhancement would nice see full support torch similar rather subset would also like see ability specify pas done already private repository work inspect dynamically make framework thus time new added automatically inspect along wrap activation configuration example configuration something like framework model gain obviously also custom accept additional added functionality actually used another alternative addition inspect object type automatically valid import string python object see attempt import,issue,positive,positive,positive,positive,positive,positive
1771447203,"> LGTM. After merging this can you do some dogfooding (or schedule a meeting with me for it)?

Did some dogfooding to validate: 
- Running multiple jobs with >100k tasks.
- Observe that
  - failure tasks from earlier jobs are not evicted
  - when data loss happens, no inconsistent task states (e.g. before, when data loss happens, we could report tasks runing 
  - while it’s actually finished since the finish event is dropped), But now the task is simply gone.",schedule meeting validate running multiple observe failure data loss inconsistent task data loss could report actually finished since finish event task simply gone,issue,negative,negative,neutral,neutral,negative,negative
1771427096,"Ray should support python &leq; 3.8 until Ray 2.8 (see [here](https://github.com/ray-project/ray/issues/34863) and [here](https://github.com/ray-project/ray/issues/39395) )

It may be that when you created your `venv` you had Ray installed for a higher python version, and your environment is using those files instead of using a ray appropriate to your current environments version of python.

You may be able to just `pip uninstall`, `pip install` to have `pip` pick up the correct wheel. Otherwise you may have to install Ray from python 3.7 wheels. See [here](https://docs.ray.io/en/latest/ray-overview/installation.html#from-wheels) for details and [here](https://pypi.org/project/ray/#files) for available wheels

E.G. for Linux:

```bash
pip uninstall -y ray
pip install ray[default] @ https://files.pythonhosted.org/packages/52/ae/fa1133402860b83fbff132978db852961d062087b2b8885744547960be87/ray-2.7.1-cp37-cp37m-manylinux2014_x86_64.whl
```",ray support python ray see may ray higher python version environment instead ray appropriate current version python may able pip pip install pip pick correct wheel otherwise may install ray python see available bash pip ray pip install ray default,issue,negative,positive,positive,positive,positive,positive
1771426385,Hey could you share a repro script for this?,hey could share script,issue,negative,neutral,neutral,neutral,neutral,neutral
1771423519,"Hey @mv96, we are not currently planning on adding new algorithms at the time, but external contributions are welcome.",hey currently new time external welcome,issue,negative,positive,positive,positive,positive,positive
1771420213,"Ah thanks for clarifying this is potentially helpful for the single node case, but has caused confusion for users in the past for multi-node, where they were not sure where this was being calculated. 

My recommendation would be similar to what you've done, which is to implement and control this behavior separately. Another more programatic way to do this is by just defining a callback, which can handle this logic independent of the console output.
",ah thanks potentially helpful single node case confusion past sure calculated recommendation would similar done implement control behavior separately another way handle logic independent console output,issue,positive,positive,neutral,neutral,positive,positive
1771404670,"Lint failed:

```


python/ray/_private/accelerators/intel_gpu.py:1:1: F401 're' imported but unused
--
  | python/ray/_private/accelerators/intel_gpu.py:3:1: F401 'sys' imported but unused
  | python/ray/_private/accelerators/intel_gpu.py:5:1: F401 'subprocess' imported but unused
  | python/ray/_private/accelerators/intel_gpu.py:6:1: F401 'importlib' imported but unused
  | python/ray/_private/accelerators/intel_gpu.py:55:18: E711 comparison to None should be 'if cond is not None:'

```",lint unused unused unused unused comparison none cond none,issue,negative,neutral,neutral,neutral,neutral,neutral
1771371345,Hi @mfojtak I can't think of any other workaround at the moment. I'm guessing you are also not allowed to build and upload a wheel as well😅 At this point I would suggest the best way forward it to contribute to Ray's codebase following this [guide](https://docs.ray.io/en/latest/ray-contribute/getting-involved.html) and see how the team feels about dropping this validation.,hi ca think moment guessing also build wheel well point would suggest best way forward contribute ray following guide see team dropping validation,issue,positive,positive,positive,positive,positive,positive
1771362000,"FYI to reviewers: this is a WIP, I won't merge it until after the next `pydantic` release that includes changes to make models `cloudpickle`-serializable.",wo merge next release make,issue,negative,neutral,neutral,neutral,neutral,neutral
1771351877,"The changes LGTM, but this was a joint effort between myself and @GeneDer so others should help review (@sihanwang41 @zcin @shrekris-anyscale)",joint effort help review,issue,negative,neutral,neutral,neutral,neutral,neutral
1771177157,"@jjyao @raulchen requested me to use # of objects instead of the size. So I made some modification.

The impl should be almost identical. I made 2 changes.

1. We don't allow threshold = 0. It makes no sense (it means we don't allow to proceed). The minimal threshold is 1. 
2. All the logic that adds size is changed to be just incremented (as it is just # of objects)",use instead size made modification almost identical made allow threshold sense allow proceed minimal threshold logic size,issue,negative,negative,neutral,neutral,negative,negative
1771143025,"@lyzyn I'm not sure if I have context for your issue. Can you file a separate issue and fill in the details? Also, the way you described it, it seems to be a rllib issue instead of serve?",sure context issue file separate issue fill also way issue instead serve,issue,negative,positive,positive,positive,positive,positive
1771130498,"Tested this again manually and seeing the latency metrics for gRPC requests are now logged fully lifecycle of the request 😄

<img width=""1521"" alt=""image"" src=""https://github.com/ray-project/ray/assets/7553988/b462a90d-957c-4d6c-98b9-69fbbb78c593"">
",tested manually seeing latency metric logged fully request image,issue,negative,neutral,neutral,neutral,neutral,neutral
1771087730,"```
CoreWorkerService.grpc_client.PushTask.OnReplyReceived - 300 total (0 active), Execution time: mean = 57.367 us, total 
= 17.210 ms, Queueing time: mean = 11.280 us, total = 3.384 ms
CoreWorkerService.grpc_client.PushTask - 300 total (0 active), Execution time: mean = 871.657 us, total = 261.497 ms, Q
ueueing time: mean = 0.000 s, total = 0.000 s
```

So active == not replied? 

Or it is CoreWorkerService.grpc_client.PushTask  - CoreWorkerService.grpc_client.PushTask.OnReplyReceived?",total active execution time mean u total time mean u total total active execution time mean u total time mean total active,issue,positive,negative,negative,negative,negative,negative
1770893977,"@lyzyn The Windows path issue I mentioned above was resolved in 2.7.0 (thanks @krfricke), but feel that is not what you come across.",path issue resolved thanks feel come across,issue,positive,positive,positive,positive,positive,positive
1770878087,"> 我目前正在使用不同的自定义文件系统测试新的存储路径 API。如果解决了会通知你
Okay, thank you! I am wondering if it is due to a problem with the Ray version, which I am using with version 2.7.0. Would it change the current issue if we downgrade Ray's version to 2.5. x.
",thank wondering due problem ray version version would change current issue downgrade ray version,issue,negative,negative,neutral,neutral,negative,negative
1770841458,I'm currently testing the new storage path API with a different custom file system. Will let you know if it's solved,currently testing new storage path different custom file system let know,issue,negative,positive,neutral,neutral,positive,positive
1770828218,"I launch a job and get the error as in the first post.  The driver program does not finish and so the cluster is not free for another job until the cluster is restarted.

I understand that there is logic trying to not have this happen with free ports, I am just giving you evidence that it is happening, so some edge case is not being caught.  Nothing else is running on the box except bare minimal redhat OS services, no other packages/servers of any type have been installed, nothing is LISTENing on any ports above 1024 except for ray processes.

I also agree that it is not easy to reproduce.  I can run 100 jobs and not have this happen (or it can happen after a handful of jobs after a restart of the cluster).  The problem is I also can not easily detect when the cluster gets in this state.  On one of my clusters, I have jobs that run from between 1 minute to 1.5 hours so now my fix is if I see a job take more than 1.5 hours I restart the cluster and resubmit the job.  This works on one of my clusters but I have others where the jobs could vary from 1 minute and 24+ hours to run and I can't tell when the cluster has become stuck until I see a list of jobs in my queue and check if the logs have the error above and look to see whether the last job should have taken so long and.

This is a serious reliability issue that albeit rare causes a lot of headaches.


",launch job get error first post driver program finish cluster free another job cluster understand logic trying happen free giving evidence happening edge case caught nothing else running box except bare minimal o type nothing listening except ray also agree easy reproduce run happen happen handful restart cluster problem also easily detect cluster state one run minute fix see job take restart cluster resubmit job work one could vary minute run ca tell cluster become stuck see list queue check error look see whether last job taken long serious reliability issue albeit rare lot,issue,positive,positive,positive,positive,positive,positive
1770681285,"As part of this, are there plans to revert commiting the auto-generated proto files introduced in https://github.com/ray-project/ray/pull/39210

> For now, we check in the auto generated proto source files for python since we need to use an old version of protoc to support python 3.7. Once we deprecate 3.7, we can undo this part.",part revert proto check auto proto source python since need use old version support python deprecate undo part,issue,negative,positive,neutral,neutral,positive,positive
1770655154,"This did not work as expected.

However with 2.7.1, I've solved the initial problem. I removed this key (--node-ip-address=x.x.x.x) out of head start command line and it works now. The head has  binded to 0.0.0.0 so it is reachable with all interfaces.

thanks

",work however initial problem removed key head start command line work head reachable thanks,issue,negative,positive,neutral,neutral,positive,positive
1770583188,"> > > 我也遇到了您的错误报告的问题。你解决了吗？**警告algorithm_config.py：2578 - 设置exploration_config = {}，因为您设置了_enable_rl_module_api = True。当RLModule API启用时，exploration_config不能被设置。如果您想实现自定义探索行为，请修改当前 RLModule 的forward_exploration 方法。在具有默认探索配置的配置上，必须使用 config.exploration_config={} 来完成此操作。**
> > 
> > 
> > 不幸的是，我仍然遇到这个问题，如果您找到解决方案，请告诉我。
> 
> Okay, thank you! I am wondering if it is due to a problem with the Ray version, which I am using with version 2.7.0. Would it change the current issue if we downgrade Ray's version to 2.5. x.

It seems related to #40205 ",thank wondering due problem ray version version would change current issue downgrade ray version related,issue,negative,negative,neutral,neutral,negative,negative
1770534810,"I have also encountered the issue with your error report. Have you resolved it?
**WARNING algorithm_config.py:2574 -- Setting exploration_config={} because you set _enable_rl_module_api=True. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the forward_exploration method of the RLModule at hand. On configs that have a default exploration config, this must be done with config.exploration_config={}.**",also issue error report resolved warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done,issue,positive,neutral,neutral,neutral,neutral,neutral
1770472270,"I see, thanks for the reply! As expected, this issue does not exist when using the Jobs API. So, will the CLI be deprecated with 3.0.0 and if so, when do we expect the 3.0.0 release? ",see thanks reply issue exist expect release,issue,negative,positive,positive,positive,positive,positive
1770386418,"Hi Sven, thanks for looking into this.
I can confirm that not providing an env has still worked with ray 2.5.1, but not with 2.6.

I have to do some refactoring to be able to run it the AIR 2.x way (build/train/save). I'll let you know whether it works as you suggested.",hi thanks looking confirm providing still worked ray able run air way let know whether work,issue,negative,positive,positive,positive,positive,positive
1770350795,"@chaowanggg @GeneDer 

I've upgraded my cluster to 2.7.1 and am still experiencing this issue.

Deployment is working fine except for this dashboard issue and it seems to be a simple front end issue. I'm using something close to the example AWS cluster file with the head startup call:
```
    - ray start --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml --dashboard-host=0.0.0.0
 ```

When I attach the cluster and call `ray status` it gives me the message `No cluster status. It may take a few seconds for the Ray internal services to start up.` 

When I call this I can access the dashboard
```
    - ray start --head  --dashboard-host=0.0.0.0
 ```

Is there a more stable version of ray I should use? I've experienced this on `2.6.3` and `2.7.1`",cluster still issue deployment working fine except dashboard issue simple front end issue something close example cluster file head call ray start head attach cluster call ray status message cluster status may take ray internal start call access dashboard ray start head stable version ray use experienced,issue,negative,positive,positive,positive,positive,positive
1770334521,"> I have also encountered the issue with your error report. Have you resolved it? **WARNING algorithm_config.py:2578 -- Setting exploration_config={} because you set _enable_rl_module_api=True. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the forward_exploration method of the RLModule at hand. On configs that have a default exploration config, this must be done with config.exploration_config={}.**

Unfortunately I still have this issue, please let me know if you find a solution.",also issue error report resolved warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done unfortunately still issue please let know find solution,issue,positive,negative,negative,negative,negative,negative
1770207051,"I have also encountered this problem. Have you resolved it?
**2023-09-19 12:50:17,545	WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.**
",also problem resolved warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done,issue,positive,neutral,neutral,neutral,neutral,neutral
1770138853,"> It's hard to test like this. As a workaround does that api support something like this? [https://dev.azure.com/[org]/[project]/_apis/git/repositories/[repo]/items?versionDescriptor[version]=main&resolveLfs=true&$format=zip&download=true&foo=.zip](https://dev.azure.com/%5Borg%5D/%5Bproject%5D/_apis/git/repositories/%5Brepo%5D/items?versionDescriptor%5Bversion%5D=main&resolveLfs=true&$format=zip&download=true&foo=.zip)

Yes - you can add foo=.zip but .zip needs to be in *path* part of the URL. 
path.endswith("".zip"") throws the same error",hard test like support something like yes add need path part error,issue,positive,negative,negative,negative,negative,negative
1770067271,"@stephanie-wang @c21 This PR is ready for another review. 
Main changes include: 1) migrated to the new BackpressurePolicy framework. 2) resolved the deadlock issue. 
one remaining comment is to make the config based on number of blocks, I'll change that tomorrow. ",ready another review main include new framework resolved deadlock issue one comment make based number change tomorrow,issue,negative,positive,positive,positive,positive,positive
1769810982,"Hey, any update on whether the issue will be resolved or not?",hey update whether issue resolved,issue,negative,neutral,neutral,neutral,neutral,neutral
1769792309,I have also encountered this problem. Have you resolved it? This troubles me a lot.,also problem resolved lot,issue,negative,neutral,neutral,neutral,neutral,neutral
1769737886,"Release tests for GCP failing because of insuficcient resources https://buildkite.com/ray-project/release-tests-pr/builds/56267#_
But aws is going through so safe to merge from that perspective",release failing going safe merge perspective,issue,negative,positive,positive,positive,positive,positive
1769736502,@stephanie-wang is this primarily for setting attributes? I assume you'd get an `AttributeError` if you try to get a non-existent attribute,primarily setting assume get try get attribute,issue,negative,positive,positive,positive,positive,positive
1769674515,`test_gcs_ha_e2e.py` already checks this behavior -- the reason it passed before (which is how the bug wasn't caught) is because the bytes object is not equal to the string object even when they encode the same characters.,already behavior reason bug caught object equal string object even encode,issue,negative,neutral,neutral,neutral,neutral,neutral
1769571666,"My suggestion is to remove the page `doc/source/ray-observability/user-guides/ray-profiling.rst` completely and absorb all of the information into the existing `doc/source/ray-observability/user-guides/debug-apps/optimize-performance.rst` This way, we can avoid having duplicate information and a longer side navigation.",suggestion remove page completely absorb information way avoid duplicate information longer side navigation,issue,negative,positive,neutral,neutral,positive,positive
1769524210,"> @sihanwang41 I think you accidentally deleted the test. Now it takes 0s 😅

lol somehow i forget to add the new file.",think accidentally test somehow forget add new file,issue,negative,positive,positive,positive,positive,positive
1769520527,"Closing this in favor of #40470, which should solve the root issue.",favor solve root issue,issue,positive,neutral,neutral,neutral,neutral,neutral
1769370401,"> 2 high-level questions:
> 
> 1. There are two pieces of ray job submission info and they look different..
> 
> <img alt=""Screenshot 2023-10-05 at 4 01 10 PM"" width=""709"" src=""https://user-images.githubusercontent.com/9677264/273063586-b7edf84f-fa4f-45d5-9f81-9c399137d057.png""> <img alt=""Screenshot 2023-10-05 at 4 01 13 PM"" width=""839"" src=""https://user-images.githubusercontent.com/9677264/273063595-97f47a20-a514-4d4e-aaf6-108c2370b2cd.png"">
> 2. The output in the ""next steps"" section seems to use the local ip. The commands that use the local ip won't work right?
> 3. If ray dashboard is not installed, will we hide the related output?

1. That's a separate issue, right now the output from the head node (which is the output of running `ray start --head`) is streamed to the user laptop (the one running `ray up`).  The output from the head node lists commands which only make sense on the head node.  We can add a message in the output to clarify this in a future PR.
2. Yup, fixed it
3. Both cluster launcher and dashboard require ray[default] so I don't think this situation will happen.",two ray job submission look different output next section use local use local wo work right ray dashboard hide related output separate issue right output head node output running ray start head user one running ray output head node make sense head node add message output clarify future fixed cluster launcher dashboard require ray default think situation happen,issue,negative,positive,neutral,neutral,positive,positive
1769360852,"> @krfricke
> 
> https://github.com/architkulkarni/ray/blob/1fd210222b79fca568bc341f1cd915f842f998ea/python/setup.py#L235-L237
> 
> ```
> # If you're adding dependencies for ray extras, please
> # also update the matching section of requirements/requirements.txt
> # in this directory
> ```
> 
> It seems the file structure has changed. I couldn't find the relevant requirements.txt file for `ray[default]`, do you know where I should add it?

@can-anyscale do you happen to know where to add it?",ray please also update matching section directory file structure could find relevant file ray default know add happen know add,issue,negative,positive,positive,positive,positive,positive
1769352760,"Took a quick look at this. `ray.get()` times out due to lock contention during auto-init when ray client connection is enabled. So, for some reason, `ray.init` is taking longer than 3 minutes.

 One specific failure here (`test_placement_ready`): https://buildkite.com/ray-project/oss-ci-build-branch/builds/6484",took quick look time due lock contention ray client connection reason taking longer one specific failure,issue,negative,negative,neutral,neutral,negative,negative
1769342799,"I just opened a PR with no behavior changes and the release test passed. https://buildkite.com/ray-project/release-tests-pr/builds/56209#018b4482-139c-42e4-b86f-1cffb25daa3a (nightly and latest were mistakenly switched prior to this PR; this PR unswitches them, but they both passed.)

I also ran the release test command 4 times from my laptop (`python launch_and_verify_cluster.py gcp/example-full.yaml --num-expected-nodes 2 --retries 20  --docker-override nightly`) and it passed each time.

I'm guessing it's a transient error in GCP. In the PR I added more debug logs to have a more useful traceback. If we merge the PR, would you consider unjailing the test? The next time it fails it'll have more debug logs for me to figure out the root cause.",behavior release test nightly latest mistakenly switched prior also ran release test command time python nightly time guessing transient error added useful merge would consider test next time figure root cause,issue,negative,positive,positive,positive,positive,positive
1769341778,"see my environment for the reference
```
aiosignal==1.3.1
attrs==23.1.0
box2d-py==2.3.5
certifi==2023.7.22
charset-normalizer==3.3.0
click==8.1.7
cloudpickle==3.0.0
cmake==3.27.5
dm-tree==0.1.8
Farama-Notifications==0.0.4
filelock==3.12.4
frozenlist==1.4.0
fsspec==2023.9.2
gymnasium==0.28.1
idna==3.4
imageio==2.31.5
jax-jumpy==1.0.0
Jinja2==3.1.2
joblib==1.3.2
jsonschema==4.19.1
jsonschema-specifications==2023.7.1
lazy_loader==0.3
lit==17.0.3
lz4==4.3.2
markdown-it-py==3.0.0
MarkupSafe==2.1.3
mdurl==0.1.2
mpmath==1.3.0
msgpack==1.0.7
networkx==3.1
numpy==1.26.1
nvidia-cublas-cu11==11.10.3.66
nvidia-cuda-cupti-cu11==11.7.101
nvidia-cuda-nvrtc-cu11==11.7.99
nvidia-cuda-runtime-cu11==11.7.99
nvidia-cudnn-cu11==8.5.0.96
nvidia-cufft-cu11==10.9.0.58
nvidia-curand-cu11==10.2.10.91
nvidia-cusolver-cu11==11.4.0.1
nvidia-cusparse-cu11==11.7.4.91
nvidia-nccl-cu11==2.14.3
nvidia-nvtx-cu11==11.7.91
packaging==23.2
pandas==2.1.1
Pillow==10.1.0
protobuf==4.24.4
pyarrow==13.0.0
pygame==2.1.3
Pygments==2.16.1
python-dateutil==2.8.2
pytz==2023.3.post1
PyWavelets==1.4.1
PyYAML==6.0.1
ray==2.7.1
referencing==0.30.2
requests==2.31.0
rich==13.6.0
rpds-py==0.10.6
scikit-image==0.21.0
scikit-learn==1.3.1
scipy==1.11.3
six==1.16.0
swig==4.1.1
sympy==1.12
tensorboardX==2.6.2.2
threadpoolctl==3.2.0
tifffile==2023.9.26
torch==2.0.1
tqdm==4.66.1
triton==2.0.0
typer==0.9.0
typing_extensions==4.8.0
tzdata==2023.3
urllib3==2.0.7
```",see environment reference post,issue,negative,neutral,neutral,neutral,neutral,neutral
1769337508,"Hi @sven1977 
I also experience the same issue with **LunarLanderContinuous**. Find the reproduction script below:

### `train.py`
```py
#!/usr/bin/env python3

from tqdm import trange
from ray.rllib.algorithms.ppo import PPOConfig

env_name = ""LunarLanderContinuous-v2""
config = (
    PPOConfig()
    .environment(env_name, env_config={})
    .framework(""torch"")
)

algo = config.build()

iterator = trange(70)
for epoch in iterator:
    result = algo.train()
    iterator.set_postfix({
        ""reward_max"": result[""episode_reward_max""],
        ""reward_mean"": result[""episode_reward_mean""],
    })

checkpoint = algo.save(""-"".join((""checkpoint"", env_name)))

print(""checkpoint"", checkpoint.checkpoint.path)
```

Result: reward_max==57, reward_mean==-54

### `restore.py`
```py
#!/usr/bin/env python3

from tqdm import trange
from ray.rllib.algorithms import Algorithm

env_name = ""LunarLanderContinuous-v2""
algo = Algorithm.from_checkpoint(""-"".join((""checkpoint"", env_name)))

for epoch in range(10):
    result = algo.train()
    print(""epoch=%(epoch)d reward_max=%(reward_max)f reward_mean=%(reward_mean)f"" % {
        ""epoch"": epoch,
        ""reward_max"": result[""episode_reward_max""],
        ""reward_mean"": result[""episode_reward_mean""],
    })
```
Result:
```
epoch=0 reward_max=57.314377 reward_mean=-54.208695
epoch=1 reward_max=57.314377 reward_mean=-201.215229
epoch=2 reward_max=57.314377 reward_mean=-196.686604
epoch=3 reward_max=-1.421870 reward_mean=-236.587598
epoch=4 reward_max=-9.095212 reward_mean=-207.183925
epoch=5 reward_max=-9.095212 reward_mean=-191.485136
epoch=6 reward_max=10.975599 reward_mean=-179.023834
epoch=7 reward_max=10.975599 reward_mean=-170.321037
epoch=8 reward_max=10.975599 reward_mean=-169.739232
epoch=9 reward_max=-5.227497 reward_mean=-159.908437
```

As you see the model starts with the results at the same range as ones in the checkpoint, but quickly deteriorates to the levels similar to a freshly initialized random model
",hi also experience issue find reproduction script python import import torch epoch result result result print result python import import algorithm epoch range result print epoch epoch epoch result result result see model range quickly similar freshly random model,issue,negative,negative,negative,negative,negative,negative
1769286179,What version of Ray are you using? `RunConfig` is part of `ray.train` starting in 2.7.0.,version ray part starting,issue,negative,neutral,neutral,neutral,neutral,neutral
1769094033,"@@xwjiang2010, this is the migration of previous notebook from Ray AIR to Ray Train. :)",migration previous notebook ray air ray train,issue,negative,negative,negative,negative,negative,negative
1769017850,"@architkulkarni totally, https://buildkite.com/ray-project/release-tests-branch/builds/2245#018b1b7e-0b8a-41b7-99ae-a686e4de68b1 should be the last time it failed

The last time it passed was on https://buildkite.com/ray-project/release-tests-branch/builds/2235#018b01c5-b641-4b16-9624-6418f98025b1

`[INFO 2023-10-10 21:40:40,546] ray_test_db.py: 34  Test results: [{""status"": ""error"", ""commit"": ""24012e57c6a083d02990b6648a032a38bb00e8af"", ""url"": ""https://buildkite.com/ray-project/release-tests-branch/builds/2245#018b1b7e-0b8a-41b7-99ae-a686e4de68b1"", ""timestamp"": 1696974039316}, {""status"": ""error"", ""commit"": ""e918a19737e2ba6df52f1c772274dc88a035090b"", ""url"": ""https://buildkite.com/ray-project/release-tests-branch/builds/2239#018b0446-3d82-49dc-b1ab-82f9c2cce589"", ""timestamp"": 1696584484870}, {""status"": ""success"", ""commit"": ""07f3d3ac4036688f83911cc8b9cece18b739b191"", ""url"": ""https://buildkite.com/ray-project/release-tests-branch/builds/2235#018b01c5-b641-4b16-9624-6418f98025b1"", ""timestamp"": 1696543196618}, {""status"": ""success"", ""commit"": ""3286dc933c73619b29a9b21c722c0a7bf879d72b"", ""url"": ""https://buildkite.com/ray-project/release-tests-branch/builds/2213#018af66d-837f-4d0a-a731-6be27412a633"", ""timestamp"": 1696352892331}, {""status"": ""success"", ""commit"": ""9670b3c2ec5c5d43660182df609c212637db50da"", ""url"": ""https://buildkite.com/ray-project/release-tests-branch/builds/2208#018aef36-7b88-47f3-b9a9-19e058a82027"", ""timestamp"": 1696231822766}, {""status"": ""success"", ""commit"": ""3135323aa1f6eb5861c331b24a9209419106c0ed"", ""url"": ""https://buildkite.com/ray-project/release-tests-branch/builds/2202#018ae020-6c7d-4d84-96ba-92e2f56e0c14"", ""timestamp"": 1695978823355}, {""status"": ""success"", ""commit"": ""9c143f63233d5cbde8a6943db31b91fb3b05f017"", ""url"": ""https://buildkite.com/ray-project/release-tests-branch/builds/2195#018ad5c5-faf6-4b20-afe4-bfac6338cdf8"", ""timestamp"": 1695805104050}, {""status"": ""success"", ""commit"": ""3e8aad8177a4e284771eb9b1eb80593ef1a90246"", ""url"": ""https://buildkite.com/ray-project/release-tests-branch/builds/2194#018ad28a-e574-45b6-9334-c07ed6be719b"", ""timestamp"": 1695750891517}, {""status"": ""success"", ""commit"": ""f2eaea027f3744c3008c1a6b7ed6553c46de250b"", ""url"": ""https://buildkite.com/ray-project/release-tests-branch/builds/2192#018acb84-badd-4253-ba11-cfa386e121ba"", ""timestamp"": 1695632605557}, {""status"": ""success"", ""commit"": ""75a68c02bfffdb40011ef442934462c8a17b023e"", ""url"": ""https://buildkite.com/ray-project/release-tests-branch/builds/2189#018abd26-9178-45c9-b6f4-f5245f707086"", ""timestamp"": 1695391974192}]`",totally last time last time test status error commit status error commit status success commit status success commit status success commit status success commit status success commit status success commit status success commit status success commit,issue,positive,positive,positive,positive,positive,positive
1769000883,It's hard to test like this. As a workaround does that api support something like this?  [https://dev.azure.com/[org]/[project]/_apis/git/repositories/[repo]/items?versionDescriptor[version]=main&resolveLfs=true&$format=zip&download=true&foo=.zip](https://dev.azure.com/%5Borg%5D/%5Bproject%5D/_apis/git/repositories/%5Brepo%5D/items?versionDescriptor%5Bversion%5D=main&resolveLfs=true&$format=zip&download=true&foo=.zip ),hard test like support something like,issue,positive,negative,negative,negative,negative,negative
1768992375,Hmm actually it seems odd that we're snapshotting the stats during execution? Should we try to fix that instead?,actually odd execution try fix instead,issue,negative,negative,negative,negative,negative,negative
1768988340,"Oh I see. Is there any better way to see when it started to fail, other than manually scrolling through buildkite?",oh see better way see fail manually,issue,negative,neutral,neutral,neutral,neutral,neutral
1768973445,"I cannot add any step in CI/CD. Do you know how RayService works in Kubernetes - it is enforced there for the working_dir to be remote URI. And it makes sense as there is no local filesystem there.
I cannot share the URL as this is private Azure DevOps repository. 
The path.endswith("".zip"") condition is wrong. It cannot be assumed that remote URLs would always end with "".zip"". Is there any reason why this condition could not be removed to fix the bug and make the code more future-proof?",add step know work enforced remote sense local share private azure repository condition wrong assumed remote would always end reason condition could removed fix bug make code,issue,negative,negative,negative,negative,negative,negative
1768947746,Hmm that still doesn't explain the usage tho. All I can tell is you are probably not running this locally? Is this on a CI/ CD that you can add a step? Also would be nice if you can share one of those url so I can test some things out. ,still explain usage tho tell probably running locally add step also would nice share one test,issue,positive,positive,positive,positive,positive,positive
1768937837,"> @mfojtak So I was reading [the doc](https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#api-reference) a bit more and I think this is by design. I also don't have permission to change this as it's not really a serve issue, this is more on how ray core works.
> 
> To solve your specific issue, can you speak more on how you use it? If this is running locally, you should be able to download the zip file and just use local path? If it's on a pipeline sort of setup, maybe you can have a step before this to download the zip file and/or resolve the url to a .zip url?

I need to use working_dir with Serve. Specifically with KubeRay RayService where the remote URI is the only option. So there is no way to download locally. Therefore it is a show-stopper for me. 
",reading doc bit think design also permission change really serve issue ray core work solve specific issue speak use running locally able zip file use local path pipeline sort setup maybe step zip file resolve need use serve specifically remote option way locally therefore,issue,positive,positive,neutral,neutral,positive,positive
1768911518,"It looks like the test passed on the last two iterations.  Will keep monitoring it to see if it was a transient error on the GCP side.

1. https://buildkite.com/ray-project/release-tests-branch/builds/2269#018b41cb-4200-40a8-acce-8d418f22e8cd
2. https://buildkite.com/ray-project/release-tests-branch/builds/2255#018b3776-4219-4bf9-ac6a-9f60ee51c8e4
",like test last two keep see transient error side,issue,negative,neutral,neutral,neutral,neutral,neutral
1768899857,"> Can you share how we should see the output of event stats to detect leak?

Added a sample event stats dump. By looking at the # active events, we can see if there is leak.",share see output event detect leak added sample event dump looking active see leak,issue,negative,negative,negative,negative,negative,negative
1768863946,Some PR broke the release test pipeline on master so I'm rerunning this..,broke release test pipeline master,issue,negative,neutral,neutral,neutral,neutral,neutral
1768784111,"That line is no-op, it falls back to py38 since py37 is not a valid option",line back since valid option,issue,negative,neutral,neutral,neutral,neutral,neutral
1768698448,"@mfojtak So I was reading [the doc](https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#api-reference) a bit more and I think this is by design. I also don't have permission to change this as it's not really a serve issue, this is more on how ray core works. 

To solve your specific issue, can you speak more on how you use it? If this is running locally, you should be able to download the zip file and just use local path? If it's on a pipeline sort of setup, maybe you can have a step before this to download the zip file and/or resolve the url to a .zip url? ",reading doc bit think design also permission change really serve issue ray core work solve specific issue speak use running locally able zip file use local path pipeline sort setup maybe step zip file resolve,issue,positive,positive,positive,positive,positive,positive
1768481020,"@davzaman how were you able to achieve running with a GPU on `local=True`? I'm adding 
`os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""`  (a single GPU)
but still can't get access to the GPU from the local worker...",able achieve running single still ca get access local worker,issue,negative,positive,positive,positive,positive,positive
1768455868,"> afaik the [last python 3.11 specific bugs](https://github.com/kserve/kserve/pull/3075#issuecomment-1677630840) were fixed starting with the Ray 2.7 release

Ah thanks, so probably all that's needed here is to not mark the build as experimental any more?",last python specific fixed starting ray release ah thanks probably mark build experimental,issue,negative,positive,neutral,neutral,positive,positive
1768452888,"I see, thanks for the feedback @RocketRider and @MatFl , we do see a problem currently with the msgpack conversion tool, in cases where you have spaces somewhere in your config, where RLlib does not expect them.

Making this a P0 now.

The solution is to slightly change the rules of engagement for handling python version moves/upgrades and taking your old checkpoints along:

* We will continue treating pickle as the default format for checkpoints (msgpack will NOT replace this in the future).
* However, if you would like to upgrade your python version, we will provide a fix with which you will be able to do the following:
    * Use a new and improved `convert_to_msgpack_checkpoint` utility, which takes as inputs the pickle checkpoint and converts it into a msgpack checkpoint, however, the msgpack checkpoint will NOT contain non-serializable information. This information must be kept separate by the user (e.g. in the form of python Algorithm config code).
    * Use the existing Algorithm.from_checkpoint and Policy.from_checkpoint utilities to restore, however, now these methods must (mandatory!) take the original config used to create the pickle checkpoint as input. This will relieve the msgpack checkpoints from having to store non-serializable data, such as spaces and other stuff.

With the above slight rule changes, we should be able to support python version moves properly. We are working on a PR right now and will update this issue as soon as it's in review.",see thanks feedback see problem currently conversion tool somewhere expect making solution slightly change engagement handling python version taking old along continue treating pickle default format replace future however would like upgrade python version provide fix able following use new utility pickle however contain information information must kept separate user form python algorithm code use restore however must mandatory take original used create pickle input relieve store data stuff slight rule able support python version properly working right update issue soon review,issue,positive,positive,positive,positive,positive,positive
1768415003,"My pleasure!

Sorry I have been bombarding you all with issues related to the new API - I have recently been prepping my code to be forward compatible so I have been stumbling across a lot of these ""alpha"" bugs",pleasure sorry related new recently code forward compatible stumbling across lot alpha,issue,positive,negative,neutral,neutral,negative,negative
1768352033,"Hey @Alian3785 , thanks for raising this issue. This is an old API vs new API issue, sorry about the lack of documentation here (we are working on it).

In the meantime, can you simply try the following (very similar to what you already had, but with a few tricks added)?

```
import gymnasium as gym
import numpy as np
import tree  # pip install dm_tree

from ray.rllib.algorithms.dreamerv3.dreamerv3 import DreamerV3Config
from ray.rllib.core.models.base import STATE_IN, STATE_OUT
from ray.rllib.policy.sample_batch import SampleBatch
from ray.rllib.utils.framework import try_import_tf

tf1, tf, tfv = try_import_tf()


env_name = ""CartPole-v1""
# Use the vector env API.
env = gym.vector.make(env_name, num_envs=1, asynchronous=False)

episode_reward = 0.0
terminated = truncated = False
# Reset the env.
obs, info = env.reset()
# Every time, we start a new episode, we should set is_first to True for the upcoming
# action inference.
is_first = 1.0

# Create the algorithm from a simple config.
config = (
    DreamerV3Config()
    .environment(""CartPole-v1"")
    .training(model_size=""XS"", training_ratio=1024)
)
algo = config.build()

# Extract the actual RLModule from the local (Dreamer) EnvRunner.
rl_module = algo.workers.local_worker().module
# Get initial states from RLModule:
states = tree.map_structure(
    lambda s: tf.convert_to_tensor(s),
    rl_module.get_initial_state(),
)

while not terminated and not truncated:
    # Use the RLModule for action computations directly.
    # DreamerV3 expects this particular batch format: obs, prev. states and the
    # `is_first` flag.
    batch = {
        STATE_IN: states,  # states is already batched (B=1)
        SampleBatch.OBS: tf.convert_to_tensor(obs),  # obs is already batched (due to vector env)
        ""is_first"": tf.convert_to_tensor([is_first]),  # set to True at beginning of episode.
    }
    outs = rl_module.forward_inference(batch)
    # Extract actions (which are in one hot format) and state-outs from outs
    one_hot_actions = outs[SampleBatch.ACTIONS].numpy()
    actions = np.argmax(one_hot_actions, axis=-1)
    states = outs[STATE_OUT]

    # Perform a step in the env.
    obs, reward, terminated, truncated, info = env.step(actions)
    episode_reward += reward[0]  # reward is batched (vector env)

    # Not at the beginning of the episode anymore.
    is_first = 0.0

print(f""Episode done, total reward={episode_reward}"")
```

I will add this to the DreamerV3 documentation as - you are 100% right - this is not clear right now at all.",hey thanks raising issue old new issue sorry lack documentation working simply try following similar already added import gymnasium gym import import tree pip install import import import import use vector truncated false reset every time start new episode set true upcoming action inference create algorithm simple extract actual local dreamer get initial lambda truncated use action directly particular batch format flag batch already already due vector set true beginning batch extract one hot format perform step reward truncated reward reward vector beginning episode print episode done total add documentation right clear right,issue,positive,positive,neutral,neutral,positive,positive
1768345100,"> Hey @NDR008 , thanks for raising this issue. You need to disable the new API stack in your config by doing:
> ```
> config.training(_enable_learner_api=False).rl_module(_enable_rl_module_api=False)
> ```
> 
> Since you are using PPO (and PPO has the new stack enabled by default), RLlib cannot use the ComplexInputNet as it's only supported on the old API stack.

Hi could you elaborate slightly?
I started to use RLlib on 2.3.0 and I remember at that time most tutorials didn't match because the config API has changed to use example PPOConfif().

1) When you say the old/new API stack, does it mean things have moved on again and I'm now using an old way?

2) When you say RLlib cannot use the ComplexNet does it mean we cannot handle images+parametric inputs without a custom model? Because reading this, I may have misunderstood ComplexNet to be a recent thing (I had found:
https://docs.ray.io/en/latest/rllib/rllib-rlmodule.html)

3) why does the same config pull ComplexNet on ImpalaConfig then?",hey thanks raising issue need disable new stack since new stack default use old stack hi could elaborate slightly use remember time match use example say stack mean old way say use mean handle without custom model reading may misunderstood recent thing found pull,issue,negative,positive,neutral,neutral,positive,positive
1768311927,"Hey @gresavage , thanks for raising this. This seems related to https://github.com/ray-project/ray/issues/39174
We are working on a fix by this issue and PR:
https://github.com/ray-project/ray/issues/39813
https://github.com/ray-project/ray/pull/39732",hey thanks raising related working fix issue,issue,negative,positive,neutral,neutral,positive,positive
1768305987,"Hey @NDR008 , thanks for raising this issue. You need to disable the new API stack in your config by doing:
```
config.training(_enable_learner_api=False).rl_module(_enable_rl_module_api=False)
```

Since you are using PPO (and PPO has the new stack enabled by default), RLlib cannot use the ComplexInputNet as it's only supported on the old API stack.",hey thanks raising issue need disable new stack since new stack default use old stack,issue,negative,positive,positive,positive,positive,positive
1768301505,"Hey @Alex-Golod , could you provide a fully self-sufficient reproduction script that we can just run and debug on our end out of the box? Thanks!",hey could provide fully reproduction script run end box thanks,issue,negative,positive,positive,positive,positive,positive
1768298077,"Hey @AvisP , thanks for raising this issue. We are very sorry, but AlphaZero will be moved for Ray 2.8 into the rllib_contrib repo (outside of RLlib) and will no longer receive further support by the team.
See here for more information on our contrib efforts: https://github.com/ray-project/ray/tree/master/rllib_contrib",hey thanks raising issue sorry ray outside longer receive support team see information,issue,positive,negative,neutral,neutral,negative,negative
1768296449,"Hey @karstenddwx , thanks for raising this issue. I don't actually think that this would have ever worked, not providing an env, b/c of the need for RLlib to know about the spaces for creating the policy/ies in your algorithm.

The code below does work fine. Could you confirm that doing this would help you with the problem?

```
from ray.rllib.algorithms.ppo import PPOConfig
import gymnasium as gym


env = gym.make(""CartPole-v1"")

config = (
    PPOConfig()
    .environment(""CartPole-v1"")
)
algo = config.build()
algo.train()
dir = algo.save()
algo.stop()

# ""Fix"" config: Remove Env, BUT provide spaces information manually, such that RLlib can create the policies correctly.
config.environment(
    None,
    observation_space=env.observation_space,
    action_space=env.action_space,
).rollouts(num_rollout_workers=0)
algo2 = config.build()
algo2.restore(dir)

# print some actions
print(algo2.compute_single_action(env.reset()[0]))  # [0] -> env.reset returns obs and infos
print(algo2.compute_single_action(env.reset()[0]))
print(algo2.compute_single_action(env.reset()[0]))
```

",hey thanks raising issue actually think would ever worked providing need know algorithm code work fine could confirm would help problem import import gymnasium gym fix remove provide information manually create correctly none print print print print,issue,positive,positive,positive,positive,positive,positive
1768278300,"Hey @quarkzou , you need to activate the ""old stack"" behavior by adding this to your config:
```
config.training(_enable_learner_api=False).rl_modules(_enable_rl_module_api=False)
```

Then you should be able to utilize your custom ModelV2 network w/o problems.

It's true what you say:
Either you use a custom ModelV2, then you need to disable the new stack via the above config changes
OR you use the new RLModule APIs (default for PPO, so no config changes required), but then your custom model must be a subclass of RLModule.",hey need activate old stack behavior able utilize custom network true say either use custom need disable new stack via use new default custom model must subclass,issue,negative,positive,positive,positive,positive,positive
1768250769,Seems like this is still failing. @iycheng do you know the answer for questions ^?,like still failing know answer,issue,negative,neutral,neutral,neutral,neutral,neutral
1768229612,afaik the [last python 3.11 specific bugs](https://github.com/kserve/kserve/pull/3075#issuecomment-1677630840) were fixed starting with the Ray 2.7 release,last python specific fixed starting ray release,issue,negative,positive,neutral,neutral,positive,positive
1768050387,Any update on this feature? Sequential termination takes way too long right now for clusters of reasonable size,update feature sequential termination way long right reasonable size,issue,negative,positive,positive,positive,positive,positive
1768039697,"> If we already have some amount of experience with this now, it would be great to say a little more in the note `Python 3.11 support is experimental.` to the degree we are aware, for example something like `Most things are working, known limitations at the moment include X, Y, and Z.` :)

We're considering migrating to 3.11 but Ray is a core dependency for us and the experimental warning is quite off-putting to say the least! It's been marked experimental since January - what are the outstanding issues that mean the wheel is kept in an experimental state?",already amount experience would great say little note python support degree aware example something like working known moment include considering ray core dependency u experimental warning quite say least marked experimental since outstanding mean wheel kept experimental state,issue,positive,positive,positive,positive,positive,positive
1768012407,"Overload resolution bug in the RAY_CHECK; because args and field names had the same names in ActorOptions, it was resolving to the constructor argument and not the field value",overload resolution bug field constructor argument field value,issue,negative,neutral,neutral,neutral,neutral,neutral
1767931396,Thank you @kira-lin! I'll take a look tomorrow!,thank take look tomorrow,issue,negative,neutral,neutral,neutral,neutral,neutral
1767911578,Pin @woshiyyya as we talked in slack. Can you help review this?,pin slack help review,issue,negative,neutral,neutral,neutral,neutral,neutral
1767884049,"this is super annoying when working with nodes on different cloud providers

RuntimeError: Version mismatch: The cluster was started with:
    Ray: 2.6.2
    Python: 3.8.17
This process on node 172.17.0.2 was started with:
    Ray: 2.6.2
    Python: 3.8.18

",super annoying working different cloud version mismatch cluster ray python process node ray python,issue,negative,negative,negative,negative,negative,negative
1767875836,"Resolved by this PR: https://github.com/ray-project/ray/pull/39841

You should be able to use `lightning.pytorch` with Ray Train in 2.8.",resolved able use ray train,issue,negative,positive,positive,positive,positive,positive
1767754694,"Dear @ArturNiederfahrenhorst, thank you for your pull request!  The mbmpo algorithm runs correctly now, but unfortunately it does not train properly. Could you analyze this issue https://github.com/ray-project/ray/issues/40400 and give your recommendation, why this is happening and how to correct this behavior?",dear thank pull request algorithm correctly unfortunately train properly could analyze issue give recommendation happening correct behavior,issue,negative,negative,negative,negative,negative,negative
1767711871,"Hi @GeneDer , this is not about supporting Azure. My Kube cluster is in EKS (AWS) and code in private corporate DevOps. Cannot be migrated to Github. The problem is the path.endswith("".zip"") condition which doesn't make sense as the zip file might be available on URLs not satisfying this condition.",hi supporting azure cluster code private corporate problem condition make sense zip file might available satisfying condition,issue,negative,positive,positive,positive,positive,positive
1767683065,"Hmm actually having idle processes alive while running a script is not an unexpected behavior (it is intended actually). I think the main question here is

1. how many processes do you exactly have? Also note that ray can start a new worker if you call ray.get inside a task.
2. How much memory each IDLE process uses? what's the value of RES - shm (it'd be great if you can show me a screenshot)",actually idle alive running script unexpected behavior intended actually think main question many exactly also note ray start new worker call inside task much memory idle process value great show,issue,positive,positive,positive,positive,positive,positive
1767681260,Hmm seems like there are lots of failures. Is it due to superset checking or using placement resources within a scheduler? ,like lot due placement within,issue,negative,negative,negative,negative,negative,negative
1767675710,"@jjyao @richardliaw  is there a plan to address the issue and the MR? This issue is often causing our environments to break because the most recent version of the packages (like for example numpy) gets installed through a dependency, although the correct version of numpy that we need already exists in the image. ",plan address issue issue often causing break recent version like example dependency although correct version need already image,issue,negative,neutral,neutral,neutral,neutral,neutral
1767644410,We should launch lora and full-param release tests to make sure this PR is ok. Thanks.,launch lora release make sure thanks,issue,positive,positive,positive,positive,positive,positive
1767548698,"## Attention: External code changed

A previous version of this PR changed code that is used or cited in  external sources, e.g. blog posts.

It looks like these changes have been reverted or are otherwise not present in this PR anymore. Please still carefully review the changes to make sure code we use in external sources still works.",attention external code previous version code used external like otherwise present please still carefully review make sure code use external still work,issue,positive,positive,neutral,neutral,positive,positive
1767548157,I mark the test as unstable so it won't create release blocker for 2.8 release: https://github.com/ray-project/ray/pull/40437,mark test unstable wo create release blocker release,issue,negative,neutral,neutral,neutral,neutral,neutral
1767528707,hmm.. seems a bit better now after test skipping on master branch?,bit better test skipping master branch,issue,negative,positive,positive,positive,positive,positive
1767509084,"@kouroshHakha I went through the complete fine-tuning process again, with 7B/13B, multiple nodes, with ""--as-test"", without, and for multiple epochs, merged the lora weights and loaded the model to evaluate it a little afterwards.",went complete process multiple without multiple lora loaded model evaluate little afterwards,issue,negative,negative,neutral,neutral,negative,negative
1767375435,"@rkooo567 

1. Still running. The memory is released only when the script ends or I kill idle workers. I haven't yet tested if IDLE workers persist after script entrypouint ends when head-node is spun up explicitly with `ray start`.
2. Definetly more then number of CPUs assigned to head-node and even available on the server. I'd say 50-ish",still running memory script kill idle yet tested idle persist script spun explicitly ray start number assigned even available server say,issue,negative,positive,positive,positive,positive,positive
1767365011,"@LilDojd thanks for the detailed report! 

> Is this an expected behavior when multiple large DAGs run from the same entry point?

IDLE procesess are supposed to exist while you are running your script, but not after you terminate your job. 

> Is it safe to detect such unaccounted ray::IDLE processes and call SIGKILL on them?

If your job is already finished & you don't use detached actors, it is mostly okay. I think we actually already do this. It is risky to do it while you are running your script because the idle processes can still own some important metadata

I have a couple more questions here;

1. When there are lots of idle processes, was your python script already finished, or is it still running?
2. How many IDLE processes are there when you observe this?
3. ",thanks detailed report behavior multiple large dag run entry point idle supposed exist running script terminate job safe detect unaccounted ray call job already finished use detached mostly think actually already risky running script idle still important couple lot idle python script already finished still running many idle observe,issue,positive,positive,positive,positive,positive,positive
1767351423,"I think unless you set all the ports manually, there's always a small possibility of conflict (unless we start dashboard_agent before starting other procs, but that's not the case). So the ideal case is to set all ports manually. ",think unless set manually always small possibility conflict unless start starting case ideal case set manually,issue,negative,positive,positive,positive,positive,positive
1767341814,"Hi @mfojtak I don't think we support Azure just yet. Currently AWS and GCP are recommended platforms. I don't really have access to Azure as well (but my guess is you can probably do a curl to follow the redirect and get the .zip url). 

I would also recommend you push the code into github.com and then you can pull the source code zip like http://github.com/[user]/[repository]/archive/master.zip",hi think support azure yet currently really access azure well guess probably curl follow redirect get would also recommend push code pull source code zip like,issue,positive,positive,positive,positive,positive,positive
1767338206,"Note: this PR above only supports `nsys` in Ray Core.
If you guys want to use `nsys` with Ray Train, Ray Tune, or other libraries, please create a new github issue and document your requirements there.",note ray core want use ray train ray tune please create new issue document,issue,positive,positive,positive,positive,positive,positive
1767311212,"> lgtm, i also see `do_write` used in `custom_datasource.py`, is that removed in another PR?

Ah, good catch. Should've be removed in #40127.

Rather than opening up a new PR and waiting for CI to run, I'm including it in this PR.",also see used removed another ah good catch removed rather opening new waiting run,issue,negative,positive,positive,positive,positive,positive
1767308523,"Hi, the PR for this support is already merged and available for nightly wheel + next ray release. 
https://github.com/ray-project/ray/pull/39998",hi support already available nightly wheel next ray release,issue,negative,positive,positive,positive,positive,positive
1767277126,"I am having this issue also, and it is blocking. Calling ray.init() within a python script with a class that builds up on DAG API. 

#### Versions tested

- Ray Versions tested: `2.7.1`, `3.0.0dev Nightly build`

Ray is initialized within this method, which is called only once.

```python
def _check_init_ray(self):
    if not ray.is_initialized():
        _context = ray.init(
            ignore_reinit_error=True,
            num_cpus=self._cfg.num_cpus,
            num_gpus=self._cfg.num_gpus,
            storage=str(self._cfg.ray_storage),
        )
```

The class itself does not store any references to Ray objects internally. DAG is built by iterating over topologically sorted nodes and creating FunctionNodes for them. Dendncies are inferred internally and supplied in `**inputs` as top-level arguments:

```python
def _schedule_dag(self, protocoldag: ProtocolDAG) -> 'FunctionNode':

  ...

  for unit in protocoldag.protocol_units:

      requested_resources = self.resolve_resources(unit)

      def store_internal_result(result: Union[ProtocolUnit, ProtocolUnitResult]):
          if isinstance(result, ProtocolUnit):
              return self.nodes[pdag_key][result.key]
          self._internal_client.store_result(result)
          return self._internal_storage.get_object_ref(
              f""results/{'/'.join(result.key.split('-'))}.json""
          )

      inputs = unit.inputs
      inputs = modify_dependencies(
          inputs, store_internal_result, is_my_obj, mode=""encode""
      )  # This modifies inputs in place

      inputs = flatten_dict(inputs)

      node = execute_unit_remote.options(
          name=unit.name, **requested_resources
      ).bind(unit_rbfengine_key, self.client, **inputs)
     
     self.nodes[pdag_key][unit.key] = node
  ... 
     # After building dag and getting terminal node, all internal references to Ray objects are removed:
     for prefix, location in self._internal_client.list_objects():
         self._internal_client.delete(
             f""{prefix}/{'/'.join(location.split('-'))}.json""
         )

     del self.nodes[pdag_key]

     # Last node
     return node
```

The `execute_unit_remote()` itself is a simple ray remote function:

```python
@ray.remote
def execute_unit_remote(
    unit_key: str,
    external_client: ResultClient,
    **dependencies,
) -> ProtocolUnitResult:

    # Get unit from local client
    unit = external_client.load_tokenizable(unit_key)

    dependencies = unflatten_dict(dependencies)

    result = unit.execute(
        **dependencies, raise_error=True, shared=external_client.get_store()
    )

    return _from_dict(result)
```

Then I just call `node.execute()` or `workflow.run_async(node)` for multiple independent DAGs on a single node. 

#### Symptoms

- The tasks finished successfully
- `ray memory` looks good
- I get lots of ray::IDLE in my top (much more than num_cpus) after each finished DAG, and memory continuously grows until getting OOM:

<img width=""1698"" alt=""image"" src=""https://github.com/ray-project/ray/assets/37330594/45019c55-2436-44b3-8ac0-06ad4178ad03"">

Also, this is how memory usage by component looks like (purple is ray::IDLE:

<img width=""1698"" alt=""image"" src=""https://github.com/ray-project/ray/assets/37330594/9163fe19-db17-4aaa-9752-018055a49765"">

- I am almost certain there are no memory leaks in the code running remotely
- Most importantly, I do not see pids of memory-hungry ray::IDLE processes from `top` in ray list workers.

### Questions

- Is this an expected behavior when multiple large DAGs run from the same entry point?
- Is it safe to detect such unaccounted ray::IDLE processes and call SIGKILL on them?

I will be opening a separate issue if I do not find a workaround. Thank you!",issue also blocking calling within python script class dag tested ray tested dev nightly build ray within method python self class store ray internally dag built sorted internally python self unit unit result union result return result return encode place node node building dag getting terminal node internal ray removed prefix location prefix last node return node simple ray remote function python get unit local client unit result return result call node multiple independent dag single node finished successfully ray memory good get lot ray top much finished dag memory continuously getting image also memory usage component like purple ray image almost certain memory code running remotely importantly see ray top ray list behavior multiple large dag run entry point safe detect unaccounted ray call opening separate issue find thank,issue,positive,positive,positive,positive,positive,positive
1767262497,"> What is the timeline for rewriting the examples that are planned to be rewritten? We should make sure we don't leave a gap here (e.g. not having them for Ray 2.8) and also make sure the links will keep working.

With the exception of the custom datasource example, we're rewriting them after the 2.8 branch cut but before the 2.8 release. We'll rewrite the custom datasource example once we finish #40296 ",make sure leave gap ray also make sure link keep working exception custom example branch cut release rewrite custom example finish,issue,negative,positive,positive,positive,positive,positive
1767238269,"> maybe change the 'gpu' tag in ml.rayci.yml to oss too? that tag is only used to skip running, rather than have anything to do with gpu

discussed offline, will keep the gpu tags.",maybe change tag tag used skip running rather anything keep,issue,negative,neutral,neutral,neutral,neutral,neutral
1767220050,"@stephanie-wang that fix doesn't seems super straight-forward. I slightly lean towards merging this PR first and add that fix later (hopefully before 2.8 release). Without that fix, this should still be useful. Because: 1) If one op's tasks won't use all resources, it's fine; 2) the concurrency cap ramp-up will make the resource allocation more balanced and mitigate the issue; 3) for internal tests, we can always tune configs to avoid the issue.
What do you think? 
Other comments are addressed. 

----

Update: per offline discussion, we can check the sum of downstreams' `num_active_tasks` . This would be simpler than checking resources. ",fix super slightly lean towards first add fix later hopefully release without fix still useful one wo use fine concurrency cap make resource allocation balanced mitigate issue internal always tune avoid issue think update per discussion check sum would simpler,issue,positive,positive,positive,positive,positive,positive
1767189983,"I faced the same error when trying to tune a model under Sklearn. Just did a few quick downgrades and seems like it works on 2.6.0, but not on 2.7.0 or after? I don't know how to set and pass `storage_path` and `name` based on reading the documentation.",faced error trying tune model quick like work know set pas name based reading documentation,issue,negative,positive,positive,positive,positive,positive
1767150658,"lgtm, i also see `do_write` used in `custom_datasource.py`, is that removed in another PR?",also see used removed another,issue,negative,neutral,neutral,neutral,neutral,neutral
1767131728,What is the timeline for rewriting the examples that are planned to be rewritten? We should make sure we don't leave a gap here (e.g. not having them for Ray 2.8) and also make sure the links will keep working.,make sure leave gap ray also make sure link keep working,issue,positive,positive,positive,positive,positive,positive
1767104538,"Thank you @sven1977 for the pointer! This is very helpful! We will update our codebase with the `Algorithm.from_checkpoin()`. And we will keep a eyes on the updated documentations / examples on the ""new API stack""!  

Thank you and the Team for keep making RLlib better and more user friendly for the RL community! ",thank pointer helpful update keep new stack thank team keep making better user friendly community,issue,positive,positive,positive,positive,positive,positive
1767104184,"Synced offline. It seems that we have already crashed the program when receiving an error [here](https://github.com/ray-project/ray/blob/bc80271c5e20feb4263634a7334196483a90648f/src/ray/gcs/redis_context.cc#L45-L48) 

So the error might come from Connect [here](https://github.com/ray-project/ray/blob/master/python/ray/includes/global_state_accessor.pxd#L154C1-L158) 

Some simple test without Redis can verify this.",already program error error might come connect simple test without verify,issue,negative,neutral,neutral,neutral,neutral,neutral
1767099305,"Right, it fails for another reason that I'm hoping transient; I get this in now since most release tests are failing now (https://buildkite.com/ray-project/release-tests-branch/builds/2268) and this change is needed for bisect to run properly ",right another reason transient get since release failing change bisect run properly,issue,negative,positive,positive,positive,positive,positive
1767090856,"> Running release test here: https://buildkite.com/ray-project/release-tests-pr/builds/56038

seems that still failing, but for another reason though?",running release test still failing another reason though,issue,negative,neutral,neutral,neutral,neutral,neutral
1767057213,"> I'll add the comment on how streaming gen backpressure works in code. [Sang's PR](https://github.com/ray-project/ray/pull/40285) has a link to the [doc](https://docs.google.com/document/d/1Ugxs7SgCDqUk44SMyO_l7Rj17drHY2Qn6GDd-BxV2Pk/edit#heading=h.mqspm92i4pgv). Regarding the solution to the deadlock issue, I guess the simplest approach we can implement in 2.9 is to detect if the downstream ops have no resources to run. If so, we temporarily disable backpressure for the current op. I guess this should reduce memory usage for most cases. For the edge cases, the behavior should remain the same as before.

I see, I didn't realize we had a way to disable backpressure. That seems fine then.

Can we add that to this PR? Even if the feature is disabled, it's not really useful to merge into 2.8 unless we have something close to the final version that we can test. If we add the deadlock fix, we can test for unknown unknowns, instead of just running into the known deadlock issue.",add comment streaming gen work code sang link doc regarding solution deadlock issue guess approach implement detect downstream run temporarily disable current guess reduce memory usage edge behavior remain see realize way disable fine add even feature disabled really useful merge unless something close final version test add deadlock fix test unknown instead running known deadlock issue,issue,negative,positive,neutral,neutral,positive,positive
1767055298,"All fixed, please take another look. Thanks a ton @ArturNiederfahrenhorst !",fixed please take another look thanks ton,issue,positive,positive,positive,positive,positive,positive
1767049765,"@sven1977 hi yess, let's close this and check the result tonight",hi let close check result tonight,issue,negative,neutral,neutral,neutral,neutral,neutral
1767019979,"I'll add the comment on how streaming gen backpressure works in code. 
[Sang's PR](https://github.com/ray-project/ray/pull/40285) has a link to the [doc](https://docs.google.com/document/d/1Ugxs7SgCDqUk44SMyO_l7Rj17drHY2Qn6GDd-BxV2Pk/edit#heading=h.mqspm92i4pgv).
Regarding the solution to the deadlock issue, I guess the simplest approach we can implement in 2.9 is to detect if the downstream ops have no resources to run. If so, we temporarily disable backpressure for the current op.
I guess this should reduce memory usage for most cases. For the edge cases, the behavior should remain the same as before.",add comment streaming gen work code sang link doc regarding solution deadlock issue guess approach implement detect downstream run temporarily disable current guess reduce memory usage edge behavior remain,issue,negative,neutral,neutral,neutral,neutral,neutral
1766994593,"In case it helps, I was struggling with an issue that had a similar effect as the one described here, where rewards during training were much higher than the ones I was obtaining from fetching actions on the trained model, all in the Python API. I tried `_disable_preprocessor_api=True` with no effect. In particular I was training a TD3 model:

```python
config = (
    TD3Config()
    .framework(""torch"")
    .rollouts(create_env_on_local_worker=True, observation_filter=""MeanStdFilter"")
    .environment(env=MyEnv, env_config=my_env_config)
)
```

And evaluating with:

```python
model = Algorithm.from_checkpoint(path)
env = MyEnv(my_env_config)
obs, _ = env.reset()
state = None
for i in range(env.total_steps):
    action = model.compute_single_action(obs, state, explore=False)
    obs, _reward, _terminated, _, _ = env.step(action)
```

Where `MyEnv` is a custom class inheriting from `gym.Env`.

I reproduced the issue both when running training with tune and directly with `.train()`, and both using the raw model immediately after training and loading it from a checkpoint file. I've seen it on 2.7.0 and 2.7.1, and on an M2 Mac and Ubuntu 22.04.

In this process I noticed that `compute_single_action` was not calling any `MeanStdFilter` instance (by adding log statements in `rllib/utils/filter.py`). I worked around the issue by calling it manually before passing the observation to `compute_single_action`:

```python
c = model.get_policy().agent_connectors.connectors[1]
action = model.compute_single_action(c.filter(obs), state, explore=False)
```

After this the evaluation actions started matching the ones I was getting during training. 
",case struggling issue similar effect one training much higher fetching trained model python tried effect particular training model python torch python model path state none range action state action custom class issue running training tune directly raw model immediately training loading file seen mac process calling instance log worked around issue calling manually passing observation python action state evaluation matching getting training,issue,negative,positive,neutral,neutral,positive,positive
1766984969,"Tested that master branch Ray leaks on this repro script, and Ray with this PR no longer leaks. Next step: make tests fixed and @rkooo567 will merge it",tested master branch ray script ray longer next step make fixed merge,issue,negative,positive,neutral,neutral,positive,positive
1766963685,Should we just use a simple tag that say 'runtime_disabled' for these jobs?,use simple tag say,issue,negative,neutral,neutral,neutral,neutral,neutral
1766963142,"> microbenchmark CI failing

@edoakes I didn't touch the microbenchmark test, I think that will probably be fixed when I merge master because of [Sihan's fix](https://github.com/ray-project/ray/pull/40411). Will wait until the [long running test](https://buildkite.com/ray-project/release-tests-pr/builds/55973#018b3b62-807a-4ea6-b733-de5d08d9e9d5) finishes running, then merge + rerun the microbenchmark test.",failing touch test think probably fixed merge master fix wait long running test running merge rerun test,issue,negative,positive,neutral,neutral,positive,positive
1766895823,"@antoine-galataud We are moving away from EnvRunnerV2, so such efforts should go into https://sourcegraph.com/github.com/ray-project/ray/-/blob/rllib/env/env_runner.py. Thanks for offering your help - can you hold back for 1-2 weeks? After https://github.com/ray-project/ray/pull/39732 is merged, there should be a clearer picture on master about how such Episodes are built in PPO.

Thereafter, there will likely be an EpisodeV3, where these changes should go.

CC @sven1977 @simonsays1980 ",moving away go thanks offering help hold back clearer picture master built thereafter likely go,issue,positive,positive,neutral,neutral,positive,positive
1766891573,"`test_streaming_generator` is failing on master, all other doc tests are passing
<img width=""766"" alt=""Screen Shot 2023-10-17 at 10 50 37 AM"" src=""https://github.com/ray-project/ray/assets/15851518/fdb89c8d-f6d9-47f9-9d1c-24b2eae1024f"">
",failing master doc passing screen shot,issue,negative,neutral,neutral,neutral,neutral,neutral
1766842899,"I run Ray Tune in single-node mode, where the head and worker are the same host. ",run ray tune mode head worker host,issue,negative,neutral,neutral,neutral,neutral,neutral
1766841922,"> Are the ports that do not have defaults and are randomly pre-chosen just the dashboard ports

Took a closer look at the code, looks like it's actually just `dashboard_agent_grpc` and `dashboard_agent_http` that have this property.
Would it make sense to provide reasonable defaults for these two?",randomly dashboard took closer look code like actually property would make sense provide reasonable two,issue,negative,negative,neutral,neutral,negative,negative
1766823018,"> pre-choose a port

Are the ports that _do not have defaults_ **and** are _randomly pre-chosen_ just the dashboard ports
`dashboard_agent_grpc` , `dashboard_agent_http`, and `metrics_export`?
I mean if we set these three manually and correctly, do we remove the possibility of `ray start` non-deterministically conflicting with itself? ",port dashboard mean set three manually correctly remove possibility ray start conflicting,issue,negative,negative,negative,negative,negative,negative
1766813238,"if: build.branch == master ?

BTW, I'll leave it for @sven1977 who is the owner of rllib to decide ;)",master leave owner decide,issue,negative,neutral,neutral,neutral,neutral,neutral
1766812650,"> Hi. Since this issue has not been updated for a long time, can I continue to work on this issue? @architkulkarni

@antimonyGu Sure! Feel free to tag me on your PR.",hi since issue long time continue work issue sure feel free tag,issue,positive,positive,positive,positive,positive,positive
1766812147,"or, we can also skip them by default in test tags or something.",also skip default test something,issue,negative,neutral,neutral,neutral,neutral,neutral
1766800561,"@edoakes This PR is ready to merge, the failing tests are failing on master
<img width=""628"" alt=""Screen Shot 2023-10-17 at 9 46 16 AM"" src=""https://github.com/ray-project/ray/assets/15851518/d76263fc-6cbe-456f-abdc-b9ccde9ba791"">
<img width=""766"" alt=""Screen Shot 2023-10-17 at 9 46 58 AM"" src=""https://github.com/ray-project/ray/assets/15851518/bb3048ae-15d1-411b-8ad1-f25695da0895"">
",ready merge failing failing master screen shot screen shot,issue,negative,positive,positive,positive,positive,positive
1766794632,"test_object_assign_owner_client_mode  unrelated.

Linkcheck failures unrelated:


(tune/examples/tune-aim: line 110002) broken    https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#metrics-explorer - 404 Client Error: Not Found for url: https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html
--
  | (serve/advanced-guides/performance: line   20) broken    https://github.com/ray-project/ray/blob/master/python/ray/serve/benchmarks/README.md - 404 Client Error: Not Found for url: https://github.com/ray-project/ray/blob/master/python/ray/serve/benchmarks/README.md
  | (tune/examples/tune_mnist_keras: line 10006) broken    https://keras.io - HTTPSConnectionPool(host='keras.io', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))
  | (rllib/rllib-torch2x: line    8) broken    https://pytorch.org/docs/stable/dynamo/index.html#torchdynamo-overview - 404 Client Error: Not Found for url: https://pytorch.org/docs/stable/dynamo/index.html
  | (ray-air/getting-started: line   45) broken    https://www.anyscale.com/ray-summit-2022/agenda/sessions/180 - 404 Client Error: Not Found for url: https://www.anyscale.com/ray-summit-2022/agenda/sessions/180



",unrelated unrelated line broken client error found line broken client error found line broken certificate verify unable get local issuer certificate line broken client error found line broken client error found,issue,negative,negative,negative,negative,negative,negative
1766773135,"> There could be a deadlock if an upstream op is backpressured, and all resources are allocated for this op. In this case, we have no resources to schedule downstream op tasks to consume the data. This issue isn't solved in this PR. https://github.com/ray-project/ray/pull/40275can mitigate this issue, but cannot eliminate it completely. There are some potential solution we can try in 2.9:

So we will disable this by default, right?

Long-term, I don't see a way around this except by implementing some sort of task preemption.",could deadlock upstream case schedule downstream consume data issue mitigate issue eliminate completely potential solution try disable default right see way around except sort task,issue,negative,positive,positive,positive,positive,positive
1766762571,"This is a functional change. Since we already released 2.7.1, merging this PR will have no affect. It's already in master, will go out with 2.8.0 today",functional change since already affect already master go today,issue,negative,neutral,neutral,neutral,neutral,neutral
1766754086,"Great to see this! Nice work! @GeneDer 
This will make the runtime debugging experience much easier!",great see nice work make experience much easier,issue,positive,positive,positive,positive,positive,positive
1766661997,"@rkooo567 I've added a test case: `test_job_manager.py::test_actor_creation_error_not_overwritten`.

Verified that this failed consistently w/o the fix.",added test case consistently fix,issue,negative,positive,positive,positive,positive,positive
1766640373,"Hi @woshiyyya, but I expect that there are no duplicated logs because no hyperparameter combination should be run more than once. And I print the chosen hyperparameter for the trial.",hi expect combination run print chosen trial,issue,negative,neutral,neutral,neutral,neutral,neutral
1766604103,"> 1. submit_job, await event.wait
> 2. _recover_running_jobs creates a task (not scheduled) _monitor_job and set event.
> 3. _monitor_job scheduled and starts _monitor_job_internal. await on `self._job_info_client.get_status(job_id)`
> 4. submit_job puts PENDING info (and await on `new_key_added = await self._job_info_client.put_info`)
> 5. _monitor_job_internal returned and await on `job_info = await self._job_info_client.get_info(job_id)`
> 6. submit_job fails to create an actor
> 7. _monitor_job_internal returned from `job_info = await self._job_info_client.get_info(job_id)` and call `job_supervisor = self._get_actor_for_job(job_id)`. It fails (because actors didn't start) and writes the incorrect status.

For a single job manager, this sequence of events cannot happen for a single `job_id` under the current solution. The reason it cannot happen is because `submit_job` can only run _after_ `_recover_running_jobs`, which means the job ID cannot be present in `self._job_info_client.get_all_jobs()`, so `_monitor_job_internal` will not be started for it (until it hits the ""happy path"" in `submit_job`).

Re: your proposed solution about health checking immediately after starting the actor, that is effectively what happens in this case (with a bad `runtime_env`, the `ActorCls.remote()` call fails immediately).",await task set event await pending await await returned await await create actor returned await call start incorrect status single job manager sequence happen single current solution reason happen run job id present happy path solution health immediately starting actor effectively case bad call immediately,issue,positive,positive,neutral,neutral,positive,positive
1766601123,"@edoakes Tests are passing (currently failing tests are failing on master):
<img width=""392"" alt=""Screen Shot 2023-10-17 at 7 59 50 AM"" src=""https://github.com/ray-project/ray/assets/15851518/9c71c932-39bc-4245-97d1-e99c20f13c75"">
",passing currently failing failing master screen shot,issue,negative,neutral,neutral,neutral,neutral,neutral
1766434279,"@krfricke
I'm also having the same issue. Is there any update on this?
I'm using ray 1.11.0, Ubuntu 20.04, python3.6
Thank you!",also issue update ray python thank,issue,negative,neutral,neutral,neutral,neutral,neutral
1766429698,"> Thank you, that makes a lot of sense. What would be the proper way to go about doing that? Seems like the callsbacks are executed too late for the purpose

I have the same issue, still have not found how to modify rewards before processing.",thank lot sense would proper way go like executed late purpose issue still found modify,issue,positive,negative,negative,negative,negative,negative
1766367814,"Here is a workaround, for whom it may concern: when using PPO you can force use of `Episode` (v1) by disabling new RL Module, connectors and learner API.
Sample config:

```python
config = (
    PPOConfig()
    .rl_module(_enable_rl_module_api=False)
    .training(_enable_learner_api=False)
    .rollouts(enable_connectors=False)
    .environment(CustomCartPole)
    .framework(args.framework)
    .callbacks(MyCallbacks)
    .resources(num_gpus=int(os.environ.get(""RLLIB_NUM_GPUS"", ""0"")))
    .reporting(keep_per_episode_custom_metrics=True)
)
```",may concern force use episode new module learner sample python,issue,negative,positive,positive,positive,positive,positive
1766270107,"So, it looks like if a supervisor actor fails to start, I can easily reproduce the error message. Can you add a unit test? (maybe we can provide a bad runtime env to the supervisor actor). I could verify this PR fixes the issue. 

Regarding the problem, I have the impression the root cause is actually that we don't health check actor right after we create it, but we rely on monitor_internal which always calls `get_actor` first (which will always fail if actor fails to be created). In this case, isn't the right solution we just health check actor immediately to check if it is started or failed? For example, https://github.com/ray-project/ray/pull/40402. (and we should avoid overwriting the failed status message if it is already failed). 

Regarding the current fix, I could verify the fix, but I feel like there's still fundamental possibility of race. The reason why I feel this way is because ""monitor_job"" is a task, so theoretically it is still able to be created in the middle of PENDING <> actor creation (since we await on putting the PENDING status RPC). For example, I feel like the following scenario is possible. 

1. submit_job, await event.wait
2. _recover_running_jobs creates a task (not scheduled) _monitor_job and set event. 
3. _monitor_job scheduled and starts _monitor_job_internal. await on `self._job_info_client.get_status(job_id)`
4. submit_job puts PENDING info (and await on `new_key_added = await self._job_info_client.put_info`)
5. _monitor_job_internal returned and await on `job_info = await self._job_info_client.get_info(job_id)`
6. submit_job fails to create an actor
7. _monitor_job_internal returned from `job_info = await self._job_info_client.get_info(job_id)` and call `job_supervisor = self._get_actor_for_job(job_id)`. It fails (because actors didn't start) and writes the incorrect status. ",like supervisor actor start easily reproduce error message add unit test maybe provide bad supervisor actor could verify issue regarding problem impression root cause actually health check actor right create rely always first always fail actor case right solution health check actor immediately check example avoid status message already regarding current fix could verify fix feel like still fundamental possibility race reason feel way task theoretically still able middle pending actor creation since await pending status example feel like following scenario possible await task set event await pending await await returned await await create actor returned await call start incorrect status,issue,positive,positive,neutral,neutral,positive,positive
1766256238,"It's great to hear that your organization/company is moving towards RLlib. We are actively working on finalizing the ""new API stack"" and it will be infinitely simpler and more transparent to use and get started with than the ""old API stack"" (which you are using right now).",great hear moving towards actively working new stack infinitely simpler transparent use get old stack right,issue,positive,positive,positive,positive,positive,positive
1766252397,"Hey @heng2j , if you ""only"" want to run your restored Algorithm inside an evaluation environment loop using only certain policies, you should probably checkout this API here and use the entire Algorithm checkpoint for that:

```
# search for your checkpoint (sub)directory, in which exists a file `algorithm_state.pkl`.
# This one is the ""main"" checkpoint dir that we will use in the below code

checkpoint_dir = [see comments above]

from ray.rllib.algorithms.algorithm import Algorithm

algo = Algorithm.from_checkpoint(checkpoint_dir, policy_ids=[list of policy IDs you would like to recover; all other policies will NOT be included in the restored algorithm])

# Compute actions for individual policies.
action = algo.compute_single_action([obs], [state]?, policy_id=[the policy ID you would like to use])
```

",hey want run algorithm inside evaluation environment loop certain probably use entire algorithm search sub directory file one main use code see import algorithm list policy would like recover included algorithm compute individual action state policy id would like use,issue,positive,positive,neutral,neutral,positive,positive
1766198948,"random means it is chosen when a process starts randomly. Some of procs cannot do this due to some implementation limitation, and they pre-choose a port (or it is hardcoded). Generally, if it is deployed in prod, it is a good idea to set all ports manually. ",random chosen process randomly due implementation limitation port generally prod good idea set manually,issue,negative,positive,neutral,neutral,positive,positive
1766187061,"So if I understand correctly, ray doesn't support RDMA yet? Any one can provide some guides about how to accelerate the data transmission between different nodes.",understand correctly ray support yet one provide accelerate data transmission different,issue,negative,neutral,neutral,neutral,neutral,neutral
1766113435,"Both BC tests now passed:
https://buildkite.com/ray-project/release-tests-pr/builds/55914#018b3cdb-56ba-40a8-97ee-4e7779173050

Merged PR into master. Should be good now.

@can-anyscale , could we unjail the release test? Thanks! :)",master good could release test thanks,issue,positive,positive,positive,positive,positive,positive
1765703826,"By logical resources, I was referring to Ray's resource concept: https://docs.ray.io/en/latest/ray-core/scheduling/resources.html, which is different from the physical hardware usage.

BTW. In your workaround, it only reports the memory usage on the head node (where the ray driver/script runs). How do you use it for capacity planning? What actions will you take base don it?
",logical ray resource concept different physical hardware usage memory usage head node ray use capacity take base,issue,negative,negative,negative,negative,negative,negative
1765691096,"In the printed port information, what does it mean that some ports are `random`, whereas some that we didn't specify (like `metrics_export`) have assigned numbers?",printed port information mean random whereas specify like assigned,issue,negative,negative,negative,negative,negative,negative
1765656961,"I see, looks like we rolled the `ray start` dice enough times to trigger this once :)",see like rolled ray start dice enough time trigger,issue,negative,neutral,neutral,neutral,neutral,neutral
1765617759,"@rkooo567 the race condition happens because the background task is _started_ for the pending job. The fix in this PR will avoid kicking off the `_monitor_internal` task for jobs between when their status is added as `PENDING` and the actor is started.

The `PENDING` check is not sufficient because what happens is the actor actually fails to be created (in the in the linked issue it was because of the runtime_env being invalid). So it passes that check, tries to get the actor, fails, and overwrites the status with the weird system error message.

So sequence of events is:

1. Constructor runs, schedules `_recover_running_jobs` task.
2. `submit_job` runs, marks the job `PENDING` then yields.
3. `_recover_running_jobs` runs, kicking off a `_monitor_internal` task for the job that's `PENDING`.
4. `submit_job` tries to create the actor, fails, and marks the job as `FAILED`.
5. `_monitor_internal` tries to get the actor, fails, and overwrites the status.",race condition background task pending job fix avoid kicking task status added pending actor pending check sufficient actor actually linked issue invalid check get actor status weird system error message sequence constructor task job pending kicking task job pending create actor job get actor status,issue,negative,negative,negative,negative,negative,negative
1765570147,"Hmm after all info ^, I am really not sure it is the Ray issue. We have logics like 

1. Before starting a worker, it checks if port is already used (https://github.com/ray-project/ray/blob/8fa15650533f4834ba7ba54360e6d405054b676b/src/ray/raylet/worker_pool.cc#L651)
2. If it fails to start a worker (due to that error), it should just create a new worker. It should not hang. 
3. If there are no available ports, it prints this error;
```
core_worker.cc:205: Failed to register worker 16971350aa0952ee87b054d9189a81aec17941c0b87009805e055d7c to Raylet. Invalid: Invalid: No available ports. Please specify a wider port range using --min-worker-port and --max-worker-port.
```

I verified with local experimentation with small port ranges. 

When do you say ""cluster is not usable"", what does it exactly mean? What's the end symptoms? ",really sure ray issue like starting worker port already used start worker due error create new worker available error register worker raylet invalid invalid available please specify port range local experimentation small port say cluster usable exactly mean end,issue,negative,positive,neutral,neutral,positive,positive
1765551994,"```
dashboard_agent_http': 52365, 'metrics_export': 52365,
```

I think it is extremely unlucky case. I don't think we made special changes here except using more fixed ports in general in production. 

",think extremely unlucky case think made special except fixed general production,issue,negative,positive,neutral,neutral,positive,positive
1765549906,"I managed to work-around the issue and restore the lost memory usage log line with a `CLIReporter`-derived class:

```
import ray.tune
from psutil import virtual_memory

_GB_MULT = 1024 ** -3

class CLIReporter2(ray.tune.CLIReporter):
    def __init__(self, *a0, **a1):
        os.environ[""RAY_AIR_NEW_OUTPUT""] = ""0""
        super().__init__(*a0, **a1)

    def report(self, trials, done, *sys_info):
        vm = virtual_memory()
        total_gb = vm.total * _GB_MULT
        used_gb = (vm.total - vm.available) * _GB_MULT
        memory_usage_msg = f""Memory usage on this node: {used_gb:.1f}/{total_gb:.1f} GiB.""
        return super().report(trials, done, *sys_info, memory_usage_msg)
```

Output:

```
== Status ==
Current time: 2023-10-16 02:43:26 (running for 00:03:01.77)
Using MedianStoppingRule: num_stopped=0.
Logical resource usage: 2.0/32 CPUs, 0/0 GPUs (0.0/1.0 accelerator_type:G)
Memory usage on this node: 33.5/125.7 GiB.
Current best trial: b6b3a1c9 with ...
Result logdir: /home/max/ray_results/...
Number of trials: 4/4 (4 TERMINATED)
```

I get good mileage out of these status log lines in Google Compute Engine.
",issue restore lost memory usage log line class import import class self super report self done memory usage node gib return super done output status current time running logical resource usage memory usage node gib current best trial result number get good mileage status log compute engine,issue,positive,positive,positive,positive,positive,positive
1765512384,"We just randomly saw this when running `ray start --head --port=6379` in Ray 2.2.0.
```
ValueError: Ray component metrics_export is trying to use a port number 52365 that is used by other components.

Port information: {'gcs': 'random', 'object_manager': 'random', 'node_manager': 'random', 'gcs_server': 6379, 'client_server': 10001, 'dashboard': 8265, 'dashboard_agent_grpc': 64676, 'dashboard_agent_http': 52365, 'metrics_export': 52365, 'redis_shards': 'random', 'worker_ports': '9998 ports from 10002 to 19999'}
```
Has it been solved since Ray 2.2.0? Seems not quite a duplicate of https://github.com/ray-project/ray/issues/25793, since that one was specifically about worker ports.

",randomly saw running ray start head ray ray component trying use port number used port information since ray quite duplicate since one specifically worker,issue,negative,negative,negative,negative,negative,negative
1765499888,@c21 @stephanie-wang thanks for your comments. They are either addressed (resolved threads) or replied (non-resolved threads). Please take a look again. ,thanks either resolved please take look,issue,positive,positive,positive,positive,positive,positive
1765497865,"Need a stamp from one of ml team folks, CC: @justinvyu , @matthewdeng , thankkks",need stamp one team,issue,negative,neutral,neutral,neutral,neutral,neutral
1765476419,This PR is ready for a preliminary review. It depends on https://github.com/ray-project/ray/pull/40285.  I've tested it locally by patching that PR. Unit tests will be added later when that PR is merged. ,ready preliminary review tested locally unit added later,issue,negative,positive,neutral,neutral,positive,positive
1765426253,"@stephanie-wang You are right. The current default config isn't optimal. I also thought of making the default config smarter by taking into consideration the number of ops and their resource requirements. The plan is to implement the basic framework and turn this off in 2.8. In 2.9, we'll need to do more experiments to figure out the best configuration and officially release this feature. ",right current default optimal also thought making default taking consideration number resource plan implement basic framework turn need figure best configuration officially release feature,issue,positive,positive,positive,positive,positive,positive
1765406995,"The root cause is probably excessive task OOM failures, but not sure if that's a regression in Data, regression in some other cluster env, or just an inherent issue with the test setup. Seems we've had worker OOMs in much older runs too, so I don't think it's a release blocker for 2.8.

Since the root cause is excessive task OOM, there are a few options to handle for 2.9:
- There might be a memory leak regression in the data task. This is the most important thing to look into
- Have a smarter way to prevent task OOM failures (this has been discussed in the past but never prioritized) - i.e. wait to retry OOM-killed tasks / adjust Data scheduling policy to account for memory usage
- Update the test to ensure we don't OOM (by adjusting instance type, number of partitions, etc) - recommend we do this anyway so that the test is more stable in the future",root cause probably excessive task sure regression data regression cluster inherent issue test setup worker much older think release blocker since root cause excessive task handle might memory leak regression data task important thing look way prevent task past never wait retry adjust data policy account memory usage update test ensure instance type number recommend anyway test stable future,issue,positive,positive,neutral,neutral,positive,positive
1765402048,"> The [documentation](https://docs.ray.io/en/latest/serve/advanced-guides/deploy-vm.html) currently says to use `serve deploy`, but I've heard that is deprecated in favor of `serve run`

Where did you hear `serve deploy` is deprecated? I'd recommend using `serve deploy` for this case.",documentation currently use serve deploy favor serve run hear serve deploy recommend serve deploy case,issue,positive,neutral,neutral,neutral,neutral,neutral
1765401648,I guess the CI linter != my linter. Will adjust,guess linter linter adjust,issue,negative,neutral,neutral,neutral,neutral,neutral
1765382774,"> I'm using the latest version of [pixi](https://github.com/prefix-dev/pixi) which uses conda deps. Can confirm that using the pip version of torch fixed my issue though.
> 
> Did you check that you're reproducing with gpu torch like: `conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia` though with a new version of pytorch released on conda, can't guarantee this will reproduce.
> 
> I see you're using the CPU only build of torch which might not exhibit this issue.

Gotcha - thanks for the tips. Will do. ",latest version confirm pip version torch fixed issue though check torch like install though new version ca guarantee reproduce see build torch might exhibit issue thanks,issue,positive,positive,positive,positive,positive,positive
1765375789,"Tested locally by running `ray job submit --runtime-env-json='{""pip"": [""requests==3333.26.0""]}' -- python script.py` and seeing the job driver log file created correctly with the correct traceback


<img width=""1705"" alt=""image"" src=""https://github.com/ray-project/ray/assets/7553988/f54f9491-7b46-47d2-9872-7ef53cad1ccc"">
",tested locally running ray job submit pip python seeing job driver log file correctly correct image,issue,negative,neutral,neutral,neutral,neutral,neutral
1765345590,"```




25143:M 16 Oct 2023 21:50:19.847 # Failed to load certificate: /root/.cache/bazel/_bazel_root/1df605deb6d24fc8068f6e25793ec703/execroot/com_github_ray_project_ray/bazel-out/k8-opt/bin/python/ray/tests/test_redis_tls.runfiles/com_github_ray_project_ray/python/ray/tests/tls/redis.crt: error:02001002:system library:fopen:No such file or directory
 

<br class=""Apple-interchange-newline"">
```

Different error. Seems path is not setup correctly.",load certificate error library file directory different error path setup correctly,issue,negative,neutral,neutral,neutral,neutral,neutral
1765314757,"Hi @rickyyx, does https://github.com/ray-project/ray/pull/40357#issuecomment-1765297077 make sense to you? If it makes sense to you, could you please merge this PR? It's blocking the KubeRay release. Thank you!",hi make sense sense could please merge blocking release thank,issue,negative,neutral,neutral,neutral,neutral,neutral
1765312164,Open an issue to track the progress https://github.com/ray-project/kuberay/issues/1499.,open issue track progress,issue,negative,neutral,neutral,neutral,neutral,neutral
1765297077,"@rickyyx The current Ray CI covers part of it (the latest KubeRay release). KubeRay CI tests both the nightly KubeRay and the latest KubeRay release. After v1.0.0 is released, I will add a new CI test in KubeRay CI that only uses CRD v1.",current ray part latest release nightly latest release add new test,issue,negative,positive,positive,positive,positive,positive
1765289949,"> However, @max0x7ba it seems that you are using logical memory resources for scheduling. Can you elaborate on ""I have used memory usage report for automated capacity planning""?

Memory usage numbers come from `psutil.virtual_memory` call, and they are what `cat /proc/meminfo` reports. These are the numbers to use for capacity planning, is that right? What do you mean by _logical memory resources_?",however logical memory elaborate used memory usage report capacity memory usage come call cat use capacity right mean memory,issue,negative,positive,positive,positive,positive,positive
1765261766,"I use these log lines to get maximum RAM utilization on the worker nodes to allocate sufficient amount of RAM for Google Compute Engine instances:

```
gawk -F': |/' '/^Memory usage on this node/ { if(m<$2) m=$2 } END { if(m>0) printf ""ray.tune memory usage maximum was %.1f GiB.\n"", m }'
```

I can see that `JupyterNotebookReporter` still reports RAM usage. But `CLIReporter` no longer does, contrary to its documentation.
",use log get maximum ram utilization worker allocate sufficient amount ram compute engine gawk usage end memory usage maximum see still ram usage longer contrary documentation,issue,negative,neutral,neutral,neutral,neutral,neutral
1765254941,"@scottsun94 Not really.

@Yicheng-Lu-llll are you interested in this issue: (1) explain that the Python version in the head / worker should match (2) specify custom Ray images in [troubleshooting.md](https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/troubleshooting/troubleshooting.md)?",really interested issue explain python version head worker match specify custom ray,issue,negative,positive,positive,positive,positive,positive
1765237851,"Actually not. For the new API, they still need to specify timeouts in `TorchConfig` instead of `RayDDPStrategy`. 

I think we still need to update the docstring for RayDDPStrategy to clarify this.",actually new still need specify instead think still need update clarify,issue,negative,positive,positive,positive,positive,positive
1765232317,Yes we already have a user guide for experiment tracking. Let's close this.,yes already user guide experiment let close,issue,negative,neutral,neutral,neutral,neutral,neutral
1765167298,"Update: I found the issue being the Plasma Client (core worker and raylet's client to the shared memory) never releases mmap'd files. This only happens on high memory pressure when the main memory is not enough and Ray is forced to allocate files and mmap them. After the memory pressure, we never munmap the files even though they are no longer used.

Actively working on PRs to tackle this. Preview: https://github.com/ray-project/ray/pull/40370 (not mature to merge yet)",update found issue plasma client core worker raylet client memory never high memory pressure main memory enough ray forced allocate memory pressure never even though longer used actively working tackle preview mature merge yet,issue,positive,negative,neutral,neutral,negative,negative
1765101213,"> Hi could you share more about how you use this?
> 
> cc @scottsun94 do you remember whether this was intentionally removed with the new console output?

I think this was intentional because based on our experience, people didn't often use logical memory resources for scheduling, especially for train and tune users. Sometimes, people don't really understand what logical memory resource usage means and confuse it with physical hardware memory usage

However, @max0x7ba it seems that you are using logical memory resources for scheduling. Can you elaborate on ""I have used memory usage report for automated capacity planning""?",hi could share use remember whether intentionally removed new console output think intentional based experience people often use logical memory especially train tune sometimes people really understand logical memory resource usage confuse physical hardware memory usage however logical memory elaborate used memory usage report capacity,issue,negative,positive,positive,positive,positive,positive
1765101160,"> wait, this test runs weekly right? are there manual runs for those dates where the 100TB test succeeds? @stephanie-wang

Hmm not sure how the tests were triggered, but I see them [here](https://b534fd88.us1a.app.preset.io/superset/dashboard/19/?native_filters_key=_PivmtbWAj8TtReb9gyPePf2xNmKbqbHQ4QubLBMF95JyDn9yXlrK2ypm7hX_ZhW).",wait test weekly right manual test sure triggered see,issue,negative,positive,positive,positive,positive,positive
1765089107,"> Curious how this would work wrt checkpointing and determinism. If you want to have reproducibility and resume without re-iterating on the same data you've trained on, how would you ensure that?

@kszlim - good question. This PR only enables randomness for training. More design discussion needed to integrate into checkpointing and achieve resumability.",curious would work determinism want reproducibility resume without data trained would ensure good question randomness training design discussion integrate achieve resumability,issue,positive,positive,positive,positive,positive,positive
1765081938,"@stephanie-wang of course; you can also run the test on-demand by creating a new build in https://buildkite.com/ray-project/release-tests-branch/builds?branch=master, without having to wait for the nightly run",course also run test new build without wait nightly run,issue,negative,positive,positive,positive,positive,positive
1765081876,"wait, this test runs weekly right? are there manual runs for those dates where the 100TB test succeeds? @stephanie-wang ",wait test weekly right manual test,issue,negative,positive,positive,positive,positive,positive
1765079365,Also jail the test just blocked my build,also jail test blocked build,issue,negative,negative,neutral,neutral,negative,negative
1765079081,@can-anyscale is it okay to close this to try running the test again? Looks like it passed 10/1-10/6. I'd like to get the metrics on memory usage per node on a fresh run too.,close try running test like like get metric memory usage per node fresh run,issue,positive,positive,positive,positive,positive,positive
1765058430,"Hi. Since this issue has not been updated for a long time, can I continue to work on this issue? @architkulkarni ",hi since issue long time continue work issue,issue,negative,negative,neutral,neutral,negative,negative
1765050272,"I'm using the latest version of [pixi](https://github.com/prefix-dev/pixi) which uses conda deps. Can confirm that using the pip version of torch fixed my issue though.

Did you check that you're reproducing with gpu torch like:
`conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia` though with a new version of pytorch released on conda, can't guarantee this will reproduce.

I see you're using the CPU only build of torch which might not exhibit this issue.",latest version confirm pip version torch fixed issue though check torch like install though new version ca guarantee reproduce see build torch might exhibit issue,issue,positive,positive,positive,positive,positive,positive
1765034406,"@gvspraveen @architkulkarni can you take a look at this?

It seems like failure from cluster launcher.

Please re-assign me if it's a ray core issue.",take look like failure cluster launcher please ray core issue,issue,negative,negative,negative,negative,negative,negative
1765004510,"Hi @hahahannes , yes Ray deduplicate the logs from different workers by default. I'm closing this issue now.

More details here: https://docs.ray.io/en/latest/ray-observability/user-guides/configure-logging.html#log-deduplication. ",hi yes ray different default issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1764985824,Hi @c21. Would you mind taking a look at this?,hi would mind taking look,issue,negative,neutral,neutral,neutral,neutral,neutral
1764968964,"I am observating 12 cores being used when I ran the repro script in the original issue: 

<img width=""928"" alt=""image"" src=""https://github.com/ray-project/ray/assets/11676094/2d54b29c-11f1-4eb3-9209-bcff4fa83b6b"">




Here's my env: 

```
(i-39810) ➜  ~ conda list | grep torch
pytorch                   2.0.1           cpu_py310hdc00b08_0  

(i-39810) ➜  ~ conda --version
conda 23.1.0

(i-39810) ➜  ~ pip list | grep wandb 
wandb                                  0.15.8
(i-39810) ➜  ~ pip list | grep ray      
ray                                    2.6.2
```

",used ran script original issue image list torch version pip list pip list ray ray,issue,negative,positive,positive,positive,positive,positive
1764944418,@sihanwang41 just in case you are still looking into this. I think we just need to add a `__init__.py` into `serve/_private/benchmarks/` to make that module importable. ,case still looking think need add make module importable,issue,negative,neutral,neutral,neutral,neutral,neutral
1764915824,"I put up something similar in case you want to have a look https://github.com/ray-project/ray/pull/40345. The caveat is that the python/requirements_compiled.txt is actually auto-generated code so you cannot modify by hand.

CI produced a copy of this file so you can just download and replace in your local copy: https://buildkite.com/ray-project/premerge/builds/8578#018b33c7-f576-46a5-ac50-1f15bf5d88cd",put something similar case want look caveat actually code modify hand produced copy file replace local copy,issue,positive,neutral,neutral,neutral,neutral,neutral
1764879482,"Hi could you share more about how you use this?

cc @scottsun94 do you remember whether this was intentionally removed with the new console output?",hi could share use remember whether intentionally removed new console output,issue,negative,positive,positive,positive,positive,positive
1764806497,"Piggy backing off of this - I am also seeing the same behavior when attempting to read from 72 differing s3 folder locations in parallel with each folder containing 200 file objects. When attempting to do the above, I encounter at random the libcurl function was given a bad arguement error.

I have attempted to leverage the ray.read_parquet_bulk() function instead, however when leveraging this I am unable to read any data for any folders, with all folders receiving a mixture of the following error codes:

OSError: When reading information for key 'region_id=1/snapshot_day=2022-11-02/warehouse_id=YHM1/part-00095-a03e9b38-8ea7-49d2-804e-437a151cf544.c000.snappy.parquet' in bucket ':cradledata2testpick_stow_transactions_acu_with_gl': AWS Error NETWORK_CONNECTION during HeadObject operation: curlCode: 43, A libcurl function was given a bad argument 

OSError: When reading information for key 'region_id=1/snapshot_day=2022-11-02/warehouse_id=YHM1/part-00026-a03e9b38-8ea7-49d2-804e-437a151cf544.c000.snappy.parquet' in bucket ':cradledata2testpick_stow_transactions_acu_with_gl': AWS Error UNKNOWN (HTTP status 400) during HeadObject operation: No response body. 

OSError: When reading information for key 'region_id=1/snapshot_day=2022-11-01/warehouse_id=OAK4/part-00022-30c0a292-d45d-49af-8d67-d24d2a9ac990.c000.snappy.parquet' in bucket ':cradledata2testpick_stow_transactions_acu_with_gl': AWS Error NETWORK_CONNECTION during HeadObject operation: curlCode: 28, Timeout was reached


This is with manually providing the s3 Reader to leverage. When not providing the s3 Reader, read_parquet_bulk is unable to automatically infer that it needs to leverage the s3 reader even when providing the files with the s3 location.

",piggy backing also seeing behavior read folder parallel folder file encounter random function given bad error leverage function instead however unable read data mixture following error reading information key bucket error operation function given bad argument reading information key bucket error unknown status operation response body reading information key bucket error operation manually providing reader leverage providing reader unable automatically infer need leverage reader even providing location,issue,negative,negative,negative,negative,negative,negative
1764717941,@zhe-thoughts just in case the notification is missed. Can you approve this PR when you have a sec?,case notification approve sec,issue,negative,negative,neutral,neutral,negative,negative
1764703487,"Follow up:

- enhance the API to follow the API proposal
- GC policy
- other profilers ",follow enhance follow proposal policy,issue,negative,neutral,neutral,neutral,neutral,neutral
1764635340,it already answered in the forums. https://discuss.ray.io/t/ray-serve-get-header-dynamic-batching-with-fastapi/12447 . Thanks ray team!,already thanks ray team,issue,negative,positive,positive,positive,positive,positive
1764182042,"@jjyao from other PR (https://github.com/ray-project/ray/pull/40210), I also realized we should keep checking signal while it blocks the thread. I am going to add that feature to this PR",also keep signal thread going add feature,issue,negative,neutral,neutral,neutral,neutral,neutral
1764179751,"@jjyao I think we can do this all together with the exit code work for 2.9. The scope I am thinking

1. Clearly define and document the behavior of exit upon various failure scenario (including sigterm)
2. Add exit code to all different failures
3. Add more details to task failures (https://github.com/ray-project/ray/issues/40359)
4. Potentially revamp retry APis with other requirements. ",think together exit code work scope thinking clearly define document behavior exit upon various failure scenario add exit code different add task potentially revamp retry,issue,negative,negative,neutral,neutral,negative,negative
1764172579,"@jjyao @rickyyx I found another issue from Antoni where sys.exit during ray.get is not properly handled. I also addressed this from this PR. I also updated the PR description and simplified some parts (e.g., sigterm handler now just raises SystemExit). ",found another issue properly handled also also description simplified handler,issue,negative,neutral,neutral,neutral,neutral,neutral
1763988414,"Another potential use case:
Specify one spot node type and the same node type as on-demand. If the spot request fails, then start an on-demand node in its place.",another potential use case specify one spot node type node type spot request start node place,issue,negative,neutral,neutral,neutral,neutral,neutral
1763954572,"I have the same problem, the task in actors has been completed, but the results are delayed（Not always. Sometimes）.Cause Actor not to killed",problem task always actor,issue,negative,neutral,neutral,neutral,neutral,neutral
1763725613,I am still seeing this issue in Ray 2.7.1: https://github.com/ray-project/ray/blob/master/rllib/models/preprocessors.py#L330,still seeing issue ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1763411419,Still working on this. Try to find out why fail.,still working try find fail,issue,negative,negative,negative,negative,negative,negative
1763402475,"Hi again! The issue will be closed because there has been no more activity in the 14 days since the last message.

Please feel free to reopen or open a new issue if you'd still like it to be addressed.

Again, you can always ask for help on our [discussion forum](https://discuss.ray.io) or [Ray's public slack channel](https://github.com/ray-project/ray#getting-involved).

Thanks again for opening the issue!
",hi issue closed activity day since last message please feel free reopen open new issue still like always ask help discussion forum ray public slack channel thanks opening issue,issue,positive,positive,neutral,neutral,positive,positive
1763344012,"Hi, I'm a bot from the Ray team :)

To help human contributors to focus on more relevant issues, I will automatically add the stale label to issues that have had no activity for more than 4 months.

If there is no further activity in the 14 days, the issue will be closed!

- If you'd like to keep the issue open, just leave any comment, and the stale label will be removed!
- If you'd like to get more attention to the issue, please tag one of Ray's contributors.

You can always ask for help on our [discussion forum](https://discuss.ray.io/) or [Ray's public slack channel](https://github.com/ray-project/ray#getting-involved).
",hi bot ray team help human focus relevant automatically add stale label activity activity day issue closed like keep issue open leave comment stale label removed like get attention issue please tag one ray always ask help discussion forum ray public slack channel,issue,positive,negative,neutral,neutral,negative,negative
1763326690,I would find Python 3.12 support including wheels also very welcome!,would find python support also welcome,issue,positive,positive,positive,positive,positive,positive
1763291980,"master was merged back in, there are still a number of failing CI jobs. Is that normal?",master back still number failing normal,issue,negative,positive,neutral,neutral,positive,positive
1763202777,"Curious how this would work wrt checkpointing and determinism. If you want to have reproducibility and resume without re-iterating on the same data you've trained on, how would you ensure that?",curious would work determinism want reproducibility resume without data trained would ensure,issue,positive,negative,neutral,neutral,negative,negative
1763194072,"@edoakes Setting `max_concurrent_queries=1` for Deployment will block all `await` calls in my FastAPI Ingress. *For example* if you modify Ingress \_\_call\__ for 
```
async def __call__(self, total):
        # self.refs = [self.handle.remote(i) for i in range(total)] 
        self.refs = []
        for i in range(total):
            dep_han = self.handle.remote(i)
            self.refs.append(dep_han)
            # next line will block everything
            obj_ref = await dep_han._to_object_ref() # any await call, just to demonstrate
            print('Job #', i, 'ref', obj_ref)
``` 

At each iteration this will get blocked and wait for a job to end (and unblock event loop probably?). Without `max_concurrent_queries=1` it will not block and create all jobs on the spot.

 Even thou i got a ref to my job result in time, i dont know how to retrieve those results without blocking everything and waiting. Does any of that make any  sense or Im doing it wrong? 
",setting deployment block await ingres example modify ingres self total range total range total next line block everything await await call demonstrate print iteration get blocked wait job end unblock event loop probably without block create spot even thou got ref job result time dont know retrieve without blocking everything waiting make sense wrong,issue,negative,negative,neutral,neutral,negative,negative
1763175664,"> I think it's probably related to the `schema()` call inside `groupby` - https://github.com/ray-project/ray/blob/master/python/ray/data/dataset.py#L1872 .

Makes sense. I updated the issue description and priority to reflect this. Probably there is some nice way we can do limit pushdown for at least some all-to-all calls fairly easily, since we know the number of output rows for many of them (e.g., reparititon, sort).",think probably related schema call inside sense issue description priority reflect probably nice way limit least fairly easily since know number output many sort,issue,positive,positive,positive,positive,positive,positive
1762964652,"Having this same issue.  Any updates?  Specifically this occurs with config:  from ray.rllib.algorithms.dqn import DQNConfig but not with from ray.rllib.algorithms.ppo import PPOConfig.  I can save checkpoint but restore fails with above error messages.

many thanks,",issue specifically import import save restore error many thanks,issue,positive,positive,positive,positive,positive,positive
1762595283,"> > > Did you solve the problem? I get the same error.
> > 
> > 
> > No, I can not find the solution, maybe you want to try the new RLModule API since ModelV2 API will be superseded.
> > BTW, I tried many other rl libs, but they all have some problems with custom enviroment and policy.
> 
> Thanks for your reply! Hope it will be fixed soon.

Hi! I found that my issue was due to some termination conditions I included in the run file (rather than the environment file). Also, I wrote a run file by referring to [action_masking.py](https://github.com/ray-project/ray/blob/master/rllib/examples/action_masking.py) and it is currently being trained normally. I hope it helps if the problem still persists.",solve problem get error find solution maybe want try new since tried many custom policy thanks reply hope fixed soon hi found issue due termination included run file rather environment file also wrote run file currently trained normally hope problem still,issue,positive,positive,positive,positive,positive,positive
1762513892,"I think what could be really helpful is: 
- better definition of the labels like `development` and `move to production`. For example, when you say `development`, is it equal to proof of concept or just interactive development stage people need to go through even after there are already Ray workloads running in production
- understand exactly what doc contents people need in different stages: development, move to production, etc.. For example, during development, which part of the docs do they need? Different library users may have different needs. Then it's easier to figure out how we should group the contents.

",think could really helpful better definition like development move production example say development equal proof concept interactive development stage people need go even already ray running production understand exactly doc content people need different development move production example development part need different library may different need easier figure group content,issue,positive,positive,positive,positive,positive,positive
1762428087,"@evalaiyc98 If you're happy to do it, I'm happy to let you! Thanks so much!",happy happy let thanks much,issue,positive,positive,positive,positive,positive,positive
1762427922,"The labels are validated to represent typical stages users go through when developing Ray application. If our current content doesn't fit the structure, we need to consider carefully how we design the content to adapt to users' intuitive mental model vs. letting users try to figure out which part of the content is right for them.

Apparently from the feedback, the current structure is not easy enough for users to find content, I don't have a strong opinion on using these labels as the perfect solution. Welcome ideas and thoughts :D

Regarding the issues mentioned, it is clear we need to decide whether it is better to organize the content by ""development"" and ""move to production"" on the highest level, or group the content for each library or ""Ray cluster"" and ""Ray jobs"" in a way that makes it clear what the user needs to know for ""local development"" vs. ""moving to production.""",represent typical go ray application current content fit structure need consider carefully design content adapt intuitive mental model try figure part content right apparently feedback current structure easy enough find content strong opinion perfect solution welcome regarding clear need decide whether better organize content development move production highest level group content library ray cluster ray way clear user need know local development moving production,issue,positive,positive,positive,positive,positive,positive
1762412737,"Hi @woshiyyya, thanks for your fast response! 
Yes, the trial table is what I would expect but when I print the hyperparameters that are passed to the function (`config`), it logs the same parameter combination (see the `(foo pid=11442) {'q': 2, 'b': 3, 'c': 6} [repeated 5x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)
`).
I might misunderstand the output log though.",hi thanks fast response yes trial table would expect print function parameter combination see foo repeated across cluster ray default set disable log deduplication see might misunderstand output log though,issue,negative,positive,positive,positive,positive,positive
1762363045,please do not merge. I will create another branch for you.,please merge create another branch,issue,positive,neutral,neutral,neutral,neutral,neutral
1762327470,"@bakeryproducts setting `max_concurrent_queries=1` will not cause the `.remote` call to block, only the `await response` call (not sure if that's still an issue for you).

Yeah, you're right that it sort of breaks the actor model. But it turns out that concurrency/batching is quite important for ML serving workloads :)",setting cause call block await response call sure still issue yeah right sort actor model turn quite important serving,issue,negative,positive,positive,positive,positive,positive
1762300614,@can-anyscale Yeah we can close it now. Train team will keep track of the import path issue.,yeah close train team keep track import path issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1762296514,"Hm I don't know if this change actually makes sense, I would prefer to avoid introducing special casing here ",know change actually sense would prefer avoid special casing,issue,negative,positive,positive,positive,positive,positive
1762289391,"The high-level outline makes sense. I would suggest some edits on top of what @alanwguo have
- Get started
  - install ray with dashboard component. Start a local ray cluster. Run a simple script. Open the Ray Dashboard and view related info.
- Key concepts
- Monitor with Ray dashboard
  - each page
- Monitor with CLI or SDK
  - state api reference
- Metrics
  - What are metrics. What is prometheus
  - supported exported metrics
    - how to add custom metrics
  - how to collect/visualize metrics
    - link to setup guides for prometheus and grafana in the cluster section since people who deploy clusters should set up those tools.
- logging
- profiling
- tracing
- debugging and optimization guides
  - common gotchas
  - memory issues
  - hanging applications
  - application failures
  - performance optimization
  - using Ray Debugger


btw, I left my comments about the high-level outline in https://github.com/ray-project/ray/issues/40331.",outline sense would suggest top get install ray dashboard component start local ray cluster run simple script open ray dashboard view related key monitor ray dashboard page monitor state reference metric metric metric add custom metric metric link setup cluster section since people deploy set logging tracing optimization common memory hanging application performance optimization ray left outline,issue,positive,positive,neutral,neutral,positive,positive
1762259131,"Some of the labels don't make sense to me.
I don't think we should use ""Develop"" and ""Move to production"" as labels. We tried it in Anyscale docs but it doesn't make that much sense. Here are the reasons:
- You need ray clusters even during development. Why is ""Clusters"" under ""move to production""? The same applies to Ray jobs
- Users only need to start local ray clusters during local development. How does it fit here?
- libraries like ""Serve"" has guide for how to move to production. So a library's doc can have contents about developments and production.",make sense think use develop move production tried make much sense need ray even development move production ray need start local ray local development fit like serve guide move production library doc content production,issue,positive,positive,positive,positive,positive,positive
1762252412,@woshiyyya I think you already delete these tests right? is this good to close? thankkks,think already delete right good close,issue,negative,positive,positive,positive,positive,positive
1762249861,"The original issue is fixed, now it is just an error specific to this one test, CC: @iycheng who is core oncall",original issue fixed error specific one test core,issue,negative,positive,positive,positive,positive,positive
1762245063,"Hey @hahahannes . 

> All config in foo should be unique. There must not be any repetition.

From the trial table, we can see that Ray Tune generated all 8 combinations, all of which were identical. There's no repetition here. Can you further clarify the issue you met?

",hey foo unique must repetition trial table see ray tune identical repetition clarify issue met,issue,negative,positive,positive,positive,positive,positive
1762170152,Hi @ericl @iycheng @stephanie-wang could I get a review/stamp on the changes in the workflow test files?,hi could get test,issue,negative,neutral,neutral,neutral,neutral,neutral
1762153725,"@edoakes Thanks for a respond (great Serve talk at Ray summit btw ). Indeed `time.sleep` is a placeholder. Option 1 would be good enough, but setting max_concurrent_queries=1 at Downstream deployment would block HTTP /set Ingress method, from where i go with `handle._longpytorchjob.remote(args)`. So my job is a blocking one. Using option 3 should allow me to do the pytorch job, but I struggle to understand how to retrieve results after that? Also doesnt it break Actor logic of doing things one after another (im actually looking for it, as it is a heavy-gpu-resources job), sequentially? With option 3 and non blocking event loop all jobs will try to start *almost* at the same time..",thanks respond great serve talk ray summit indeed option would good enough setting downstream deployment would block ingres method go job blocking one option allow job struggle understand retrieve also doesnt break actor logic one another actually looking job sequentially option non blocking event loop try start almost time,issue,negative,positive,positive,positive,positive,positive
1762137784,"@bakeryproducts thanks for the detailed report. I reproduced your issue and it seems to be related to the fact that the `Downstream` method is blocking the `asyncio` event loop. I confirmed this by running three modified versions of the code, all of which worked as you originally expected:

- Setting `max_concurrent_queries=1` in the `Downstream` deployment decorator. This causes only one of the queries to be scheduled to the replica at a time.
- Using `await asyncio.sleep(idx*2)` instead of `time.sleep(2)` to not block the event loop.
- Using `await asyncio.get_running_loop.run_in_executor(None, lambda: time.sleep(idx*2))` to not block the event loop.

I understand this is a very poor/confusing user experience and I will give some thought as to how we can improve it in the general case.

To unblock you, I'd suggest avoiding blocking the event loop by using one of the above options. I assume that `time.sleep` is a placeholder for some real work you want to do; in that case, the third option above of `run_in_executor` may be your best bet.",thanks detailed report issue related fact downstream method blocking event loop confirmed running three code worked originally setting downstream deployment decorator one replica time await instead block event loop await none lambda block event loop understand user experience give thought improve general case unblock suggest blocking event loop one assume real work want case third option may best bet,issue,negative,positive,positive,positive,positive,positive
1762133665,"@zcin some (seemingly unrelated) test failures, merging master to re-trigger CI",seemingly unrelated test master,issue,negative,neutral,neutral,neutral,neutral,neutral
1762131597,"The high level outline makes sense to me. I re-ordered it a bit and put some of the existing docs under the new headings:

- Monitor with Ray dashboard
  - each page 
- Monitor with CLI or SDK
  - state api reference
- Metrics
  - What are metrics. What is prometheus
  - supported exported metrics
    - how to add custom metrics 
  - how to collect/visualize metrics
    - setup guides for prometheus and grafana 
- How to debug
  - profiling
  - logging
  - tracing

There's also some observability docs in the serve documentation and in the future, the ray data documentation. I think it makes sense to keep those docs there.
",high level outline sense bit put new monitor ray dashboard page monitor state reference metric metric metric add custom metric metric setup logging tracing also observability serve documentation future ray data documentation think sense keep,issue,negative,positive,neutral,neutral,positive,positive
1762092399,"Hello, if this issue still needs solving, I would like to work on this.
",hello issue still need would like work,issue,negative,neutral,neutral,neutral,neutral,neutral
1762048148,Deferring this to @edoakes and @sihanwang41; I can accept in a few days if you get no response ,accept day get response,issue,negative,neutral,neutral,neutral,neutral,neutral
1761796302,"@anyscalesam I am using an autoscaler setup where workers sometimes crash (memory error for example). On these remote nodes I start ray with ""ray start ... ""  in the console and connect to the head node, so the head node distributes job to those worker nodes.  Once the worker on the worker node crashes, it tries to restart it, but then the memory on the GPU is still full on that node, and it crashes again .. and that cycles continues forever, because the GPU is full and it will crash again and again.

My only option at the moment is call ray stop on these remote hosts, and restart it with ""ray start ..."" again",setup sometimes crash memory error example remote start ray ray start console connect head node head node job worker worker worker node restart memory still full node forever full crash option moment call ray stop remote restart ray start,issue,negative,positive,positive,positive,positive,positive
1761783545,"I'm running into this same issue as well. I also only notice it when I do longer train sessions.... I tried reproducing by decreasing the number of epochs and training samples to something more trivial, and I don't notice this issue occurring ",running issue well also notice longer train session tried decreasing number training something trivial notice issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1761738940,Just nuked everything and retried with modifying `.bazelrc` in the ray repo. This totally worked! Thanks @raulchen!!,everything ray totally worked thanks,issue,negative,positive,neutral,neutral,positive,positive
1761553748,Bumping this up -- I'd still be quite interested in a worked example here (particularly in the distributed setting using ray_xgboost),bumping still quite interested worked example particularly distributed setting,issue,negative,positive,positive,positive,positive,positive
1761332204,"net.ipv4.tcp_fin_timeout = 20

It seems to be a ray issue to me.  One ray job opens a port for a worker (the only thing making use of the port ranges in question) and the next job can't get started because the port is still bound. And then the only way forward is a restart of the ray cluster.  

I agree it isn't easy to reproduce, but does really causes instability in our workflow having to detect a cluster that needs restarting and restarting it.",ray issue one ray job port worker thing making use port question next job ca get port still bound way forward restart ray cluster agree easy reproduce really instability detect cluster need,issue,positive,positive,positive,positive,positive,positive
1761220593,"I think the problems are interrelated. When I followed the [guide](https://docs.ray.io/en/master/ray-contribute/development.html#installing-additional-dependencies-for-development), I think there are no dependency conflicts in lint-requirements.txt. The screenshot below displays the result of running `pip install -r python/requirements/lint-requirements.txt`.

![lint-requirements](https://github.com/ray-project/ray/assets/139951533/d17b6712-1688-4f0a-86ec-3213fc4a7be4)

However, there were indeed dependency conflicts in text-requirements.txt, as illustrated in the screenshot below.
Result of running `pip install -c python/requirements.txt -r python/requirements/test-requirements.txt`

![test-requirements](https://github.com/ray-project/ray/assets/139951533/462e63ab-a040-4886-bb3b-a6c7817e93d3)
",think interrelated guide think dependency result running pip install however indeed dependency result running pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
1761011110,"@rkooo567 @rynewang, would you be able to have a look at this? Thanks.",would able look thanks,issue,negative,positive,positive,positive,positive,positive
1760949395,"I rebased off master, maybe it will help move the review forward",master maybe help move review forward,issue,negative,neutral,neutral,neutral,neutral,neutral
1760782696,"This diff will fix the failure in `alloc.c`. There is still a failure to build boringssl. I don't really understand a few things:
- what changed, why does master build boringssl and this version does not?
- why use [boringssl for hiredis](https://github.com/ray-project/ray/blob/65ed62d512720c037f545d52cfc653dc830a4589/bazel/BUILD.hiredis#L48), and [openssl for redis](https://github.com/ray-project/ray/blob/65ed62d512720c037f545d52cfc653dc830a4589/bazel/BUILD.redis#L35)? It seems openssl is more prevalent, especially in the documentation.
- If we are talking about security, why is openssl [pinned to the very old openssl v1.1.1f](https://github.com/ray-project/ray/blob/65ed62d512720c037f545d52cfc653dc830a4589/bazel/ray_deps_setup.bzl#L252) when there are security releases, the final 1.1.1 version [is 1.1.1w](https://github.com/openssl/openssl/tags)

Anyhow, here is what I have so far
```diff
diff --git a/thirdparty/patches/hiredis-windows-msvc.patch b/thirdparty/patches/hiredis-windows-msvc.patch
index 2d05b117c4..733cb75665 100644
--- a/thirdparty/patches/hiredis-windows-msvc.patch
+++ b/thirdparty/patches/hiredis-windows-msvc.patch
@@ -17,7 +17,7 @@ diff --git sds.h sds.h
 diff --git fmacros.h fmacros.h
 --- fmacros.h
 +++ fmacros.h
-@@ -12,0 +12,3 @@
+@@ -13,0 +13,3 @@
 +#if defined(_MSC_VER) && !defined(__clang__) && !defined(strdup)
 +#define strdup _strdup
 +#endif
```",fix failure still failure build really understand master build version use prevalent especially documentation talking security pinned old security final version anyhow far git index git git defined defined defined define,issue,negative,negative,neutral,neutral,negative,negative
1760767331,could you merge master back in? ,could merge master back,issue,negative,neutral,neutral,neutral,neutral,neutral
1760728456,@richardliaw this is needed for ray + python 3.11 on conda-forge. Could we get some eyes on it?,ray python could get,issue,negative,neutral,neutral,neutral,neutral,neutral
1760712480,"It would be great if this could be merged (test or not); it would help debug a long-standing issue (#35383) where we're stuck on not knowing what's going wrong at all. Perhaps after debugging that issue, it'll be easier to write a test.",would great could test would help issue stuck knowing going wrong perhaps issue easier write test,issue,positive,positive,positive,positive,positive,positive
1760600509,"Add the following to your `.bazelrc` for a temporary workaround.
```
build --action_env=CPPFLAGS=""-Wno-strict-prototypes -Wno-implicit-function-declaration -DMAC_OS_X_VERSION_10_6""
build --copt=-Wno-error=deprecated-builtins --copt=-Wno-error=deprecated-declarations
```",add following temporary build build,issue,negative,neutral,neutral,neutral,neutral,neutral
1760584371,"Found the root cause, should be fixed at least for aws",found root cause fixed least,issue,negative,negative,neutral,neutral,negative,negative
1760521117,I think it's probably related to the `schema()` call inside `groupby` - https://github.com/ray-project/ray/blob/master/python/ray/data/dataset.py#L1872 .,think probably related schema call inside,issue,negative,neutral,neutral,neutral,neutral,neutral
1760457934,"It looks like the issue here is that when we have multiple sort passes (e.g., groupby -> map_groups -> groupby), the sort in the second groupby ends up re-executing the first groupby. Not sure yet if this is because it's trying to get the schema, or if it has to do with the sort sampling stage.

It can be easily fixed by putting a .materialize() before the second groupby (groupby -> map_groups -> materialize -> groupby)",like issue multiple sort sort second first sure yet trying get schema sort sampling stage easily fixed second materialize,issue,positive,positive,positive,positive,positive,positive
1760454124,Sorry; pushed to the wrong upstream earlier so the changes weren't reflected. Comments for PlacementResources and RequiredResources are in the `CreateActorOptions` struct.,sorry wrong upstream reflected,issue,negative,negative,negative,negative,negative,negative
1760395036,lmk when it is ready based on our discussion last time!,ready based discussion last time,issue,negative,positive,neutral,neutral,positive,positive
1760202848,"> > Should we use shuffle_files=True instead of shuffle=""files""?
> 
> @stephanie-wang, according to discussion offline earlier, `shuffle_files=True` has the limitation which cannot be extended later, if we need to support shuffle seed, or different granularity of shuffle, we have to add more arguments separately. On the other hand a single `shuffle` argument we can overload the data type later, e.g. to support a `ShuffleOption` class via `shuffle=ShuffleOption(seed=..., ...)`
> 
> > Should we update the train.DataConfig to set this automatically? (in a separate PR)
> 
> That's good question, probably not in 2.8. Need more discussion with Ray Train together. Would prefer to introduce on Data first, and gather users feedback.

Sounds good, thanks for the context.",use instead according discussion limitation extended later need support shuffle seed different granularity shuffle add separately hand single shuffle argument overload data type later support class via update set automatically separate good question probably need discussion ray train together would prefer introduce data first gather feedback good thanks context,issue,positive,positive,positive,positive,positive,positive
1760180209,"> > I think we still need to remove file_metadata_shuffler.py (?), but other than that LGTM
> 
> Let me do the code removal in a separate PR. Several places need to be cleaned up, such as file_metadata_shuffler.py, https://github.com/ray-project/ray/blob/master/python/ray/data/_default_config.py and https://github.com/ray-project/ray/blob/master/python/ray/data/context.py#L174 .


Ah, gotcha. Sounds good",think still need remove let code removal separate several need ah good,issue,negative,positive,positive,positive,positive,positive
1760168047,">I think we still need to remove file_metadata_shuffler.py (?), but other than that LGTM

Let me do the code removal in a separate PR. Several places need to be cleaned up, such as file_metadata_shuffler.py, https://github.com/ray-project/ray/blob/master/python/ray/data/_default_config.py and https://github.com/ray-project/ray/blob/master/python/ray/data/context.py#L174 .",think still need remove let code removal separate several need,issue,negative,neutral,neutral,neutral,neutral,neutral
1760153605,">so we will not be removing the DatasetPipeline class completely for now, just raising the deprecation warning?

The only thing I am worried about is the usage of `DatasetPipeline.from_iterable`. I want to avoid the situation that Python throws the module/class not found, and people shocked about it. Any blocker if we keep the `DatasetPipeline` class around? All actual code inside `DatasetPipeline` is removed.",removing class completely raising deprecation warning thing worried usage want avoid situation python found people blocker keep class around actual code inside removed,issue,negative,positive,neutral,neutral,positive,positive
1760142884,"> Should we use shuffle_files=True instead of shuffle=""files""?

@stephanie-wang, according to discussion offline earlier, `shuffle_files=True` has the limitation which cannot be extended later, if we need to support shuffle seed, or different granularity of shuffle, we have to add more arguments separately. On the other hand a single `shuffle` argument we can overload the data type later, e.g. to support a `ShuffleOption` class via `shuffle=ShuffleOption(seed=..., ...)`

> Should we update the train.DataConfig to set this automatically? (in a separate PR)

That's good question, probably not in 2.8. Need more discussion with Ray Train together. Would prefer to introduce on Data first, and gather users feedback.",use instead according discussion limitation extended later need support shuffle seed different granularity shuffle add separately hand single shuffle argument overload data type later support class via update set automatically separate good question probably need discussion ray train together would prefer introduce data first gather feedback,issue,positive,positive,positive,positive,positive,positive
1760138675,Any chance the google docs in the PR description could be updated with global read access?,chance description could global read access,issue,negative,neutral,neutral,neutral,neutral,neutral
1760051316,Yeah - i think hanging is definitely not ideal. ,yeah think hanging definitely ideal,issue,positive,positive,positive,positive,positive,positive
1760037824,"Unfortunately there is currently not an implemented way of converting Tune Experiment checkpoints. 

If you have a Trial checkpoint (e.g. `.../checkpoints/checkpoint_00000`) you can do this by saving it as a new checkpoint in a newer version.",unfortunately currently way converting tune experiment trial saving new version,issue,negative,positive,neutral,neutral,positive,positive
1759756758,"Is Kourosh also an LLM person today ;). I'm tagging other folks in rllib team @avnishn, @ArturNiederfahrenhorst as an FYI of these changes",also person today team,issue,negative,neutral,neutral,neutral,neutral,neutral
1759560627,Also encountering compatibility challenges but when using vllm that uses ray to serve model.,also compatibility ray serve model,issue,negative,neutral,neutral,neutral,neutral,neutral
1759473749,"From what I can tell, the gist of the issue below.

[Here](https://github.com/ray-project/ray/blob/b696766f6d433fd3fdd9ed524402bfa1dfb303c6/python/ray/util/client/worker.py#L560) there's a handling for when `_num_returns()` returns `""dynamic""` to make sure that it is converted to integer. However, the changes with streaming generator in Ray Data in 2.7.x makes it also possible that `_num_returns()` returns `""streaming""`. This is not handled, so it gets passed all the way downstream to the `range(num_returns_ref)` call, as expected `range(""streaming"")` raises a `TypeError`.",tell gist issue handling dynamic make sure converted integer however streaming generator ray data also possible streaming handled way downstream range call range streaming,issue,positive,positive,positive,positive,positive,positive
1759341257,"Hi @matthewdeng, yes it should be fine to complete using 2.6.3. But is there any way to convert older checkpoints to a newer version of ray? ",hi yes fine complete way convert older version ray,issue,positive,positive,positive,positive,positive,positive
1759218877,"Bumping this issue, it severely affects working with complex graphs in Workflow context",bumping issue severely working complex context,issue,negative,negative,negative,negative,negative,negative
1759199542,"I run RayCluster on k8s  and access ray client server from ingress with ssl:

```python
import grpc
import ray

ray.init(address=""ray://xxxx:443"", _credentials=grpc.ssl_channel_credentials())
```

But I run into other problem:

```plain
E1012 16:43:07.559710570 2789667 hpack_parser.cc:833]                  Error parsing 'content-type' metadata: error=invalid value key=content-type
E1012 16:43:07.637759904 2789667 hpack_parser.cc:833]                  Error parsing 'content-type' metadata: error=invalid value key=content-type
E1012 16:43:07.749863840 2789674 hpack_parser.cc:833]                  Error parsing 'content-type' metadata: error=invalid value key=content-type
E1012 16:43:07.877639829 2789674 hpack_parser.cc:833]                  Error parsing 'content-type' metadata: error=invalid value key=content-type
2023-10-12 16:43:12,903 WARNING dataclient.py:403 -- Encountered connection issues in the data channel. Attempting to reconnect.
Log channel is reconnecting. Logs produced while the connection was down can be found on the head node of the cluster in `ray_client_server_[port].out`

 raise self._exception
ConnectionError: Failed during this or a previous request. Exception that broke the connection: <_MultiThreadedRendezvous of RPC that terminated with:
        status = StatusCode.NOT_FOUND
        details = ""Attempted to reconnect to a session that has already been cleaned up.""
        debug_error_string = ""UNKNOWN:Error received from peer ipv4:10.8.8.50:443 {created_time:""2023-10-12T16:43:59.571742914+08:00"", grpc_status:5, grpc_message:""Attempted to reconnect to a session that has al
ready been cleaned up.""}""
>
```",run access ray client server ingres python import import ray ray run problem plain error value error value error value error value warning connection data channel reconnect log channel produced connection found head node cluster port raise previous request exception broke connection status reconnect session already unknown error received peer reconnect session al ready,issue,negative,negative,neutral,neutral,negative,negative
1759168485,"The use-case is we have an actor that should live in one `K8s` Pod / Ray Worker Node alone, and due to resource constraints and because it suffices for us, we need to allocate a fractional CPU < 1 to it.",actor live one pod ray worker node alone due resource u need allocate fractional,issue,negative,positive,neutral,neutral,positive,positive
1759154050,"@rkooo567 

The scenario unfolds as follows:

1. We created 300 Actors.
2. Data is transmitted to each Actor at regular intervals.
3. The Actor conducts calculations and subsequently returns the results.
4. There is no additional creation or removal of Actors.

Initially, the Head Node begins with a memory consumption of a few hundred units. After creating the Actors, this consumption slightly increases, as you previously explained (which is not problematic). As we send data and receive results from the Actors (as outlined in Steps 2 and 3), the memory consumption of the Head Node increases (which is also acceptable). 

The issue is that the Head Node does not release the memory when there is no communication with the Ray System. It should cool down and remove the held memory. In other words, even when we cease sending data to the System, after a few hours, the memory utilization of the Head Node remains high and fails to decrease.

The script in the worker nodes should be irrelevant, as the memory leak is in the Head Noden especially since no scripts are running on the Head Node.",scenario data actor regular actor subsequently additional creation removal initially head node memory consumption hundred consumption slightly previously problematic send data receive outlined memory consumption head node also acceptable issue head node release memory communication ray system cool remove memory even cease sending data system memory utilization head node remains high decrease script worker irrelevant memory leak head especially since running head node,issue,negative,negative,neutral,neutral,negative,negative
1759051796,"> Seems that this is not related to python 3.11, but the restriction comes from using Windows with Pyarrow 7+ (see [setup.py](https://github.com/ray-project/ray/blob/master/python/setup.py#L245-L246)). It looks like our internal fix for the [Arrow bug](https://github.com/apache/arrow/issues/26685), implemented in #29993, is not compatible with Windows.

Speaking as a pyarrow maintainer here: I personally doubt that this is the case. I would suggest that you actually test this again on CI by removing the upper pin and do a Windows test run with the latest pyarrow to check if this indeed still the case, or whether it's fine to remove this pin. 

Nothing in that original PR for fixing the serialization issue looks as something that might have issues on Windows. Of course, there might have been a Windows-specific bug in pyarrow 7 in the IPC serialization that surfaced by switching to that. But 1) it might be that this is already fixed in the meantime for more recent pyarrow versions, and 2) if there is actually a bug for Windows, it would be nice to report that to pyarrow so we could actually try to fix that.",related python restriction come see like internal fix arrow bug compatible speaking maintainer personally doubt case would suggest actually test removing upper pin test run latest check indeed still case whether fine remove pin nothing original fixing serialization issue something might course might bug serialization surfaced switching might already fixed recent actually bug would nice report could actually try fix,issue,negative,positive,positive,positive,positive,positive
1758941617,"Note: our shutdown path is a bit complex, and it'd be nice to clean this up.  Added to the tech debt item (and some doc regarding how shutdown path works here; https://docs.google.com/document/d/1mr4j5p9a0Ce5oYaUDoaiHUdOsbYfBOmAO4CPQTSjwR4/edit#heading=h.fwxjsbhjhpi5). ",note shutdown path bit complex nice clean added tech debt item doc regarding shutdown path work,issue,negative,positive,positive,positive,positive,positive
1758847050,Failing GCE release tests are due to unrelated authentication issue.,failing release due unrelated authentication issue,issue,negative,negative,negative,negative,negative,negative
1758771156,I think this can also happen when you have not enough ports. Seems like we shouldn't just infinitely hang here but raise an exception or start a driver with less workers. ,think also happen enough like infinitely raise exception start driver le,issue,negative,neutral,neutral,neutral,neutral,neutral
1758769638,I will take a look at the half of it today / other half tmrw,take look half today half,issue,negative,negative,negative,negative,negative,negative
1758769177,"What's the value for sysctl net.ipv4.tcp_fin_timeout?

Besides, I think it is difficult us to start investigation / prioritize the issue unless we have a repro as it seems like not a Ray issue finding the wrong or occupied port (based on your comment). ",value besides think difficult u start investigation issue unless like ray issue finding wrong port based comment,issue,negative,negative,negative,negative,negative,negative
1758757359,"We don't recommend using a ray client anymore https://docs.ray.io/en/master/cluster/running-applications/job-submission/ray-client.html#when-to-use-ray-client, so I don't think we will make any fix sooner or later. Please feel free to create a PR to fix! 
",recommend ray client think make fix sooner later please feel free create fix,issue,positive,positive,positive,positive,positive,positive
1758755611,"@WeichenXu123 ah, gotcha! Didn't realize you were running E2E tests on your side.

Given that we'd likely need extensive mocks to unit tests this feature, I think the additional complexity may outweigh the benefit of the tests.",ah realize running side given likely need extensive unit feature think additional complexity may outweigh benefit,issue,negative,neutral,neutral,neutral,neutral,neutral
1758753934,"@John-Almardeny. What's the rate of the leak, and if it is severe, could you create a new issue with a repro script? also note that Ray stores lots of data in memory, so some degree of memory growing up is not unexpected (E.g., whenever you schedule an actor the head node should store the metadata. If we have more than 10K+ metadata, we delete the oldest dead actor metadata)

",rate leak severe could create new issue script also note ray lot data memory degree memory growing unexpected whenever schedule actor head node store delete dead actor,issue,negative,positive,neutral,neutral,positive,positive
1758746982,"Did some spot checks on the single-node performance benchmarks, and seems like there's on obvious difference.",spot performance like obvious difference,issue,negative,neutral,neutral,neutral,neutral,neutral
1758742609,"Two CVEs affecting this version:

* CVE-2021-32765
* CVE-2020-7105

Neither is dangerous because malicious data in Redis can already cause much more harm via direct access to Ray objects. But we should still patch so they don't cause issues for users.",two affecting version neither dangerous malicious data already cause much harm via direct access ray still patch cause,issue,negative,negative,neutral,neutral,negative,negative
1758617972,"Release test signal loooks good - waiting for ci to merge:

Metric                                               | Master           | PR             
----------------------------------------------------|------------------|-----------------
single_client_get_calls_Plasma_Store                | 7282.77          | 7841.54        
single_client_put_calls_Plasma_Store                | 5756.89          | 5737.71        
multi_client_put_calls_Plasma_Store                 | 12672.54         | 12315.34       
single_client_put_gigabytes                         | 18.45            | 19.93          
single_client_tasks_and_get_batch                   | 8.77             | 10.02          
multi_client_put_gigabytes                          | 33.06            | 30.71          
single_client_get_object_containing_10k_refs        | 12.29            | 13.72          
single_client_wait_1k_refs                          | 5.30             | 5.56           
single_client_tasks_sync                            | 1218.19          | 1167.51        
single_client_tasks_async                           | 9753.34          | 9949.97        
multi_client_tasks_async                            | 27750.93         | 29401.67       
1_1_actor_calls_sync                                | 2238.65          | 2212.49        
1_1_actor_calls_async                               | 7546.64          | 7497.98        
1_1_actor_calls_concurrent                          | 4798.26          | 4503.25        
1_n_actor_calls_async                               | 9732.13          | 9250.60        
n_n_actor_calls_async                               | 30251.36         | 30422.05       
n_n_actor_calls_with_arg_async                      | 2925.57          | 2925.14        
1_1_async_actor_calls_sync                          | 1451.97          | 1479.63        
1_1_async_actor_calls_async                         | 3164.91          | 3027.01        
1_1_async_actor_calls_with_args_async               | 2329.22          | 2260.64        
1_n_async_actor_calls_async                         | 8936.29          | 8174.14        
n_n_async_actor_calls_async                         | 25270.14         | 25046.85       
placement_group_create/removal                      | 1012.41          | 1002.06        
client__get_calls                                   | 1260.04          | 1141.05        
client__put_calls                                   | 928.00           | 958.77         
client__put_gigabytes                               | 0.13             | 0.09           
client__tasks_and_put_batch                         | 11604.27         | 12290.04       
client__1_1_actor_calls_sync                        | 569.51           | 558.57         
client__1_1_actor_calls_async                       | 1051.85          | 1008.41        
client__1_1_actor_calls_concurrent                  | 1041.03          | 978.42         
client__tasks_and_get_batch                         | 0.98             | 1.06           
",release test signal good waiting merge metric master,issue,negative,positive,positive,positive,positive,positive
1758596240,"> Nice fix. Have you run the benchmarks? Would like to learn the perf impact.

Good idea, will do this.",nice fix run would like learn impact good idea,issue,positive,positive,positive,positive,positive,positive
1758427703,@alanwguo / @architkulkarni could you please review this for the job model import changes?,could please review job model import,issue,negative,neutral,neutral,neutral,neutral,neutral
1758397965,"So I found a temporary workaround if you are using ray such that is not using the pyarrow package.

First I uninstalled existing ray installation. Then I got the source code for pyarrow 6.0.1 from [here](https://github.com/apache/arrow/tree/maint-6.0.x). In the package I went to ./python and changed the version dependencies  `numpy==1.21.3` to `numpy>=1.21.3` in the files `requirements-wheel-build.txt`, `requirements-wheel-test.txt` and also in the `setup.py` file changed to `install_requires = ('numpy >= 1.23.1',)` Next I installed `cmake` and `Visual Studio Community` version. I was unsure about the individual components that is  needed to install for `Visual Studio` and just get the usual CRT and compilers. Once all that is done did a `python setup.py install` after navigating to the directory and it installed without further errors! However when I started python and did a `import pyarrow` it failed with a message 

`""....\_init__.py"", line 63 in <module> import pyarrow.lib as _lib
ModuleNotFoundError: NO module named ""pyarrow.lib""`

Now that pyarrow 6.0.1 is installed, when I did a `pip install -U ""ray[rllib]""` it installed without issues and able to use it to run my script. I am not knowledgable with python packages but maybe someone else can provide a better solution that builds the pyarrow package properly.",found temporary ray package first uninstalled ray installation got source code package went version also file next visual studio community version unsure individual install visual studio get usual done python install directory without however python import message line module import module pip install ray without able use run script python maybe someone else provide better solution package properly,issue,negative,positive,positive,positive,positive,positive
1758200159,"Based on this PR, we might be able to turn the auto-wrap with a flag: https://github.com/psf/black/issues/1802",based might able turn flag,issue,negative,positive,positive,positive,positive,positive
1758197326,"Post a fix PR: https://github.com/ray-project/ray/pull/40266
Will share updates soon after the release tests passed.",post fix share soon release,issue,negative,neutral,neutral,neutral,neutral,neutral
1758169510,"> @John-Almardeny can you actually try Ray 2.7? We fixed one major memory leak bug that was caused by gRPC regressions in Ray 2.7. Also cc @jjyao to follow up


@rkooo567 I switched to the latest `Ray==2.7.2`. The memory accumulation rate seems to be less than before, but still, the memory does not go down in the HEAD Node (and other worker nodes) if we leave the system to cool down for hours without sending it any job requests!",actually try ray fixed one major memory leak bug ray also follow switched latest memory accumulation rate le still memory go head node worker leave system cool without sending job,issue,negative,positive,positive,positive,positive,positive
1758120565,Hey @olipinski would you be able to complete this experiment using 2.6.3? Generally restoring across versions isn't well supported.,hey would able complete experiment generally across well,issue,negative,positive,positive,positive,positive,positive
1758100815,"cc @gvspraveen @architkulkarni I believe there is something wrong with the cluster launcher's setup for nightly tests. 

Could you folks help take a look? 

This is related as well: https://github.com/ray-project/ray/issues/40228
",believe something wrong cluster launcher setup nightly could help take look related well,issue,negative,negative,negative,negative,negative,negative
1758079856,"> LGTM. For testing, can you make sure you have the following tests?
> 
> 1. Test profile events per task is respected.
> 2. Test max events per status / profiling is respected properly.
> 3. task_events_dropped_task_attempt_batch_size is respected

Yeah, these are covered in the unit testing: 
1. TestLimitProfileEventsPerTask
2. TestBufferSizeLimit
3. TestBatchedSend

Running microbenchmark: https://buildkite.com/ray-project/release-tests-pr/builds/55447#018b2095-76aa-4146-8a3a-e3dc39eb0313",testing make sure following test profile per task test per status properly yeah covered unit testing running,issue,positive,positive,positive,positive,positive,positive
1757968006,can you try with ray 2.7? We removed grpcio requirement from the dashboard agent which is highly likely a root cause of this issue ,try ray removed requirement dashboard agent highly likely root cause issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1757966247,Good catch! Thanks for the contribution! ,good catch thanks contribution,issue,positive,positive,positive,positive,positive,positive
1757921268,"and before merging, please make sure there's no regression in microbenchmark!",please make sure regression,issue,positive,positive,positive,positive,positive,positive
1757624338,"The [windows error](https://buildkite.com/ray-project/oss-ci-build-pr/builds/38208#018b1c17-6f61-4543-b0cc-575b3902a935/3011-3125) starts with
```
external/com_github_redis_hiredis/alloc.c(40): error C2065: 'strdup': undeclared identifier
```

Does that version work with MSVC?",error error undeclared identifier version work,issue,negative,neutral,neutral,neutral,neutral,neutral
1756807783,I have confirmed that driver log file is there under `Logs` section. So I guess there are some grpc version issues to read that file.,confirmed driver log file section guess version read file,issue,negative,positive,positive,positive,positive,positive
1756482149,Hmm actually all the windows build seem to fail? ,actually build seem fail,issue,negative,negative,negative,negative,negative,negative
1756437694,Yeah it is run in `premerge` job - example [here](https://buildkite.com/ray-project/premerge/builds/8100#018b1b50-80dd-41be-afd6-4e6380e5e8d3).,yeah run job example,issue,negative,neutral,neutral,neutral,neutral,neutral
1756395849,"They are all existing issues ;), the issues only cut today since the release test run has been failing in the last few days",cut today since release test run failing last day,issue,negative,neutral,neutral,neutral,neutral,neutral
1756358885,"1. Sure attached job logs here [job-driver-raysubmit_4sS9Mwpf9g3zuVmy.log](https://github.com/ray-project/ray/files/12862271/job-driver-raysubmit_4sS9Mwpf9g3zuVmy.log)
2. I dont think the backoff is a critical factor here. We saw the same behavior with `tenacity`. The root cause has something to do with exceptions.
3. I am using @ray.remote to make sure the workload runs on a worker node and not on the head node. I have seen behavior where just submitting a python script without any ray calls just runs entirely on the head node. 
",sure attached job dont think critical factor saw behavior tenacity root cause something make sure worker node head node seen behavior python script without ray entirely head node,issue,positive,positive,positive,positive,positive,positive
1756357350,Hmm @rickyyx do you think you will have time in Ray 2.9 to start making a repro script?,think time ray start making script,issue,negative,neutral,neutral,neutral,neutral,neutral
1756355816,@John-Almardeny can you actually try Ray 2.7? We fixed one major memory leak bug that was caused by gRPC regressions in Ray 2.7. Also cc @jjyao to follow up,actually try ray fixed one major memory leak bug ray also follow,issue,negative,positive,neutral,neutral,positive,positive
1756350860,Hmm intersting. That seems an orthogonal issue from this particular issue (probably related to keepalive). Is it possible to create a new issue with a reproducible script? We can start converstaion from there,orthogonal issue particular issue probably related possible create new issue reproducible script start,issue,negative,positive,neutral,neutral,positive,positive
1756350143,yes that's right. It is not just for job endopint. Every network communication will happen through that network interface that is bound to the node ip,yes right job every network communication happen network interface bound node,issue,negative,positive,positive,positive,positive,positive
1756349881,"> Hmm I think it may take some time until it can detect ungraceful failures. For example, detecting a ungraceful node failure would take 30 seconds ~ 1 minute.

Yeah, it cannot detect the ungraceful failure like forever. It always stuck in running status even though the worker node is down.",think may take time detect ungraceful example ungraceful node failure would take minute yeah detect ungraceful failure like forever always stuck running status even though worker node,issue,negative,negative,negative,negative,negative,negative
1756347978,"Hmm I think it may take some time until it can detect ungraceful failures. For example, detecting a ungraceful node failure would take 30 seconds ~ 1 minute. ",think may take time detect ungraceful example ungraceful node failure would take minute,issue,negative,negative,negative,negative,negative,negative
1756329613,"Just want to add my observation. When I run a ray job in the ray cluster, the driver code in the ray head is not able to catch the exception after I terminate the worker node ungracefully.",want add observation run ray job ray cluster driver code ray head able catch exception terminate worker node ungracefully,issue,negative,positive,positive,positive,positive,positive
1756323113,"Having the same issue, so how to solve it? As mentioned here [#39222](https://github.com/ray-project/ray/issues/39222#issue-1877789003) for installing rllib I don't need it but getting the error related to arrow when doing a `pip install ""ray[rllib]""` Is there a way around?",issue solve need getting error related arrow pip install ray way around,issue,negative,neutral,neutral,neutral,neutral,neutral
1756301772,"You can think batch is for the worst of the worst edge case handling. Also if each request has 10+mb, each request will have 100k of metrics which may block thread too long, so break down to multiple requests make sense ",think batch worst worst edge case handling also request request metric may block thread long break multiple make sense,issue,negative,negative,negative,negative,negative,negative
1756226220,@edoakes @sihanwang41 This is now ready for another round of review :) ,ready another round review,issue,negative,neutral,neutral,neutral,neutral,neutral
1756219499,"A couple questions -- 
* do we have any more information here, especially with logs?
* the backoff library is a bit of a smell, not sure if that actually works well with Ray. You should try to use Ray's native backoff
* Any reason why you are using @ray.remote? If you remove it, what happens?",couple information especially library bit smell sure actually work well ray try use ray native reason remove,issue,positive,positive,positive,positive,positive,positive
1756185940,Yes we aligned this to be P0 for the next sprint. It went into go/okr.,yes next sprint went,issue,negative,neutral,neutral,neutral,neutral,neutral
1756151727,failure unrelated. Ping to merge. @edoakes ,failure unrelated ping merge,issue,negative,negative,negative,negative,negative,negative
1756116099,"LGTM! A couple questions:
- Should we use `shuffle_files=True` instead of `shuffle=""files""`?
- Should we update the train.DataConfig to set this automatically? (in a separate PR)",couple use instead update set automatically separate,issue,negative,neutral,neutral,neutral,neutral,neutral
1756096908,Looks like the role configs are not setup properly - @gvspraveen is this something you could hep triage and take a look for the clsuter launchers failures? ,like role setup properly something could hep triage take look,issue,negative,neutral,neutral,neutral,neutral,neutral
1756073918,"@larrylian ldd libREMOTE_LIB.so showed that ray lib was not linked, even though it was linked in the cmake file.

This line in CMakeLists.txt fixes the issue. 
```shell
set(CMAKE_CXX_FLAGS ""-Wl,--no-as-needed"")
``` 
Now it is able to find the remote functions and everything works.

",ray linked even though linked file line issue shell set able find remote everything work,issue,negative,positive,positive,positive,positive,positive
1756031887,@caren-guo could you share how you ended up solving this? Thanks!,could share ended thanks,issue,positive,positive,positive,positive,positive,positive
1756026708,"Actually found a way to do this, sorry about the false alarm!",actually found way sorry false alarm,issue,negative,negative,negative,negative,negative,negative
1756017411,"> @a-zhenya this issue is probably fixed in ray 2.7. Can you try that out? The reason why when you create a new driver, it doesn't pick up the IP address that was given by ray start. It is fixed in 2.7

Hi I am trying to localize the issue and develop a minimum reproducible case. I use a machine with 2 network cards.
I am starting the ray head as ""ray start --head --node-ip-address=x.x.x.x"" where x is desired IP address to bind the ray jobs endpoint. Correct?
",issue probably fixed ray try reason create new driver pick address given ray start fixed hi trying localize issue develop minimum reproducible case use machine network starting ray head ray start head desired address bind ray correct,issue,negative,positive,neutral,neutral,positive,positive
1755997554,"Let's figure out what the root case issue is because @peytondmurray found this related PR that deliberately removed the scrollbar: https://github.com/ray-project/ray/pull/24873
",let figure root case issue found related deliberately removed,issue,negative,neutral,neutral,neutral,neutral,neutral
1755977725,"> @rickyyx limit change by itself is not a solution, right? Irrespective of the limit, workers should not be dropping metrics on the ground -- we have to
> 
> * Have a control over the batch size (@rkooo567 figured out that we have it, but it turned out to be non-working)
> * Make sure that metrics are reported reliably, even in the presence of failures when we cross the max-size of the batch (which this PR is aiming to address)

I guess my assumption is that with a larger limit - we will not be dropping metrics already. If that's the case, yes, it's not gonna work with just the limit change. ",limit change solution right irrespective limit dropping metric ground control batch size figured turned make sure metric reliably even presence cross batch aiming address guess assumption limit dropping metric already case yes gon na work limit change,issue,positive,positive,positive,positive,positive,positive
1755970610,"Will wait for https://github.com/ray-project/ray/pull/40173 to be merged, then we can also emit better warnings based on the block size metrics available per Operator.",wait also emit better based block size metric available per operator,issue,negative,positive,positive,positive,positive,positive
1755970171,"Assigning to @can-anyscale as of now since this looks like a test infra issue. 
",since like test infra issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1755913506,"@rickyyx limit change by itself is not a solution, right? Irrespective of the limit, workers should not be dropping metrics on the ground -- we have to 

 - Have a control over the batch size (@rkooo567 figured out that we have it, but it turned out to be non-working)
 - Make sure that metrics are reported reliably, even in the presence of failures when we cross the max-size of the batch (which this PR is aiming to address)",limit change solution right irrespective limit dropping metric ground control batch size figured turned make sure metric reliably even presence cross batch aiming address,issue,positive,positive,positive,positive,positive,positive
1755892106,"GH does not provide a mechanism for uploading a Parquet file, neither as an attachment to an issue comment nor as a file within a gist. This makes it difficult for me to share the exact data file I used to replicate this issue.

But it is simply a blank Parquet file containing 164 features named `feature_N` and ~20,000 columns per partition file, with all values set to `0` for `int` type features, `0.0` for `float` type features, and `np.nan` for `object` type features.",provide mechanism parquet file neither attachment issue comment file within gist difficult share exact data file used replicate issue simply blank parquet file per partition file set type float type object type,issue,negative,negative,neutral,neutral,negative,negative
1755887097,"> w00t so this never worked?

maybe it is broken by my CI stack upgrade, which is required by adding the trainium machines.",never worked maybe broken stack upgrade,issue,negative,negative,negative,negative,negative,negative
1755853468,"> Can u include a screenshot so that other reviewers can take a look?

Sure, I have attached the screenshots.
",include take look sure attached,issue,negative,positive,positive,positive,positive,positive
1755851470,"It's a bit strange to specify a percentage of a GPU that's required, since you don't know in advance the specs of the GPU the task will be scheduled on.",bit strange specify percentage since know advance spec task,issue,negative,negative,neutral,neutral,negative,negative
1755821768,"Preserving order is not guaranteed in ray streaming by default, you can use the following to preserve ordering.
```python
ctx = ray.data.DataContext.get_current()
ctx.execution_options.preserve_order = True
```
",order ray streaming default use following preserve python true,issue,negative,positive,positive,positive,positive,positive
1755732760,"> another related question. Does this change the behavior of map -> all-to-all fusion?

Good question, it shouldn't change. I'll update the PR description.",another related question change behavior map fusion good question change update description,issue,negative,positive,positive,positive,positive,positive
1754624128,"> ### Description
> We have seen increasing demands for support accelerators in Ray core, including AWS neurons, TPUs, AMD GPUs and Intel accelerators. Currently we are doing it incrementally but we might lose the big picture. We should do a proper design after 2.7.
> 
> ### Use case
> intel #38553 aws #37998 tpu #38669 amd ?
> 
> _No response_

Intel GPU: #38553 
Intel Gaudi: #39759 ",description seen increasing support ray core currently might lose big picture proper design use case,issue,negative,neutral,neutral,neutral,neutral,neutral
1754385490,Thanks. LGTM. Can u include a screenshot so that other reviewers can take a look?,thanks include take look,issue,negative,positive,positive,positive,positive,positive
1754166015,"hi, also encounter this issue, and support and suggestion is appreciate thanks!",hi also encounter issue support suggestion appreciate thanks,issue,positive,positive,positive,positive,positive,positive
1754053151,"> Can you help test how it looks like now on different terminals and different bgs (white, dark, grey)? Don't need to share the results but just do a sanity check and make sure they all have reasonable color contrast.

Of course! All look good. I have also made a PR for it. It is ready for review(The CI failure is not related to my change).
",help test like different different white dark grey need share sanity check make sure reasonable color contrast course look good also made ready review failure related change,issue,positive,positive,neutral,neutral,positive,positive
1753715660,"I am using the latest Ray==2.6. And there is a massive increase in memory in the Head and Worker nodes accumulated over time!
Even if we leave the system to cool down for hours without sending it any job requests, the memory does not go down!  ",latest massive increase memory head worker time even leave system cool without sending job memory go,issue,positive,positive,positive,positive,positive,positive
1753699965,"We have started to observe this. I've not tried to build a repro yet, but we see it on large production clusters where there is a lot of worker churn due to spot availability.

We are running Ray 2.6.3 conda packages.",observe tried build yet see large production lot worker churn due spot availability running ray,issue,negative,positive,neutral,neutral,positive,positive
1753471740,This issue is still present in ray 2.7.1,issue still present ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1753305247,"Status update for this issue:

I've had no luck reproducing this issue locally both on FF and Chrome. The linked PR explicitly adds CSS rules which forbid copying of the line numbers, but from discussions with @angelinalg the fix didn't seem to do the trick. Putting this effort on hold for now due to difficulty in reproducing the issue.",status update issue luck issue locally chrome linked explicitly forbid line fix seem trick effort hold due difficulty issue,issue,negative,negative,neutral,neutral,negative,negative
1753214441,"I am running this on baremetal with redhat 9.2, should be pretty standard socket closing times.  I have not been able to reproduce  with a test on demand but it happens about 1-4 times a day on our cluster.",running pretty standard socket time able reproduce test demand time day cluster,issue,negative,positive,positive,positive,positive,positive
1753208613,"yes, there is no process that uses the specific port and in this state I can not start a job and the cluster is in an unusable state.",yes process specific port state start job cluster unusable state,issue,negative,neutral,neutral,neutral,neutral,neutral
1752999786,"@MMorente 

1. Are you sure you moved the code `RAY_REMOTE(compute)` to remote.cpp?
2. Can you show me `ldd libREMOTE_LIB.so`?",sure code compute show,issue,negative,positive,positive,positive,positive,positive
1752404022,"The above log is for
2023-10-08 23:13:33,485 INFO monitor.py:691 -- Ray version: 2.7.1
2023-10-08 23:13:33,485 INFO monitor.py:692 -- Ray commit: 9f07c12615958c3af3760604f6dcacc4b3758a47

",log ray version ray commit,issue,negative,neutral,neutral,neutral,neutral,neutral
1752402779,"```
`2023-10-09 11:46:28,208 INFO node_provider.py:53 -- ClusterState: Loaded cluster state: ['216.48.179.215', '164.52.201.70']
Fetched IP: 164.52.201.70
Warning: Permanently added '164.52.201.70' (ED25519) to the list of known hosts.
==> /tmp/ray/session_latest/logs/monitor.err <==

==> /tmp/ray/session_latest/logs/monitor.log <==
2023-10-08 23:13:33,485 INFO monitor.py:690 -- Starting monitor using ray installation: /home/ray/anaconda3/lib/python3.11/site-packages/ray/__init__.py
2023-10-08 23:13:33,485 INFO monitor.py:691 -- Ray version: 2.7.1
2023-10-08 23:13:33,485 INFO monitor.py:692 -- Ray commit: 9f07c12615958c3af3760604f6dcacc4b3758a47
2023-10-08 23:13:33,486 INFO monitor.py:693 -- Monitor started with command: ['/home/ray/anaconda3/lib/python3.11/site-packages/ray/autoscaler/_private/monitor.py', '--logs-dir=/tmp/ray/session_2023-10-08_23-13-32_012785_2484/logs', '--logging-rotate-bytes=536870912', '--logging-rotate-backup-count=5', '--gcs-address=164.52.201.70:6379', '--autoscaling-config=/home/ray/ray_bootstrap_config.yaml', '--monitor-ip=164.52.201.70']
2023-10-08 23:13:33,489 INFO monitor.py:159 -- session_name: session_2023-10-08_23-13-32_012785_2484
2023-10-08 23:13:33,490 INFO monitor.py:191 -- Starting autoscaler metrics server on port 44217
2023-10-08 23:13:33,491 INFO monitor.py:216 -- Monitor: Started
2023-10-08 23:13:33,506 INFO node_provider.py:53 -- ClusterState: Loaded cluster state: []
2023-10-08 23:13:33,507 INFO node_provider.py:114 -- ClusterState: Writing cluster state: ['216.48.179.215', '164.52.201.70']
2023-10-08 23:13:33,507 INFO autoscaler.py:274 -- disable_node_updaters:False
2023-10-08 23:13:33,507 INFO autoscaler.py:282 -- disable_launch_config_check:False
2023-10-08 23:13:33,507 INFO autoscaler.py:294 -- foreground_node_launch:False
2023-10-08 23:13:33,507 INFO autoscaler.py:304 -- worker_liveness_check:True
2023-10-08 23:13:33,507 INFO autoscaler.py:312 -- worker_rpc_drain:True
2023-10-08 23:13:33,508 INFO autoscaler.py:362 -- StandardAutoscaler: {'cluster_name': 'default', 'auth': {'ssh_user': 'user', 'ssh_private_key': '~/ray_bootstrap_key.pem'}, 'upscaling_speed': 1.0, 'idle_timeout_minutes': 30, 'docker': {'image': 'rayproject/ray:2.7.1.9f07c1-py311-gpu', 'worker_image': 'rayproject/ray:2.7.1.9f07c1-py311-gpu', 'container_name': 'ray_container', 'pull_before_run': True, 'run_options': ['--ulimit nofile=65536:65536']}, 'initialization_commands': [], 'setup_commands': ['sudo apt-get update', 'sudo apt-get install gcc ffmpeg libsm6 libxext6  -y', 'pip install -r ""/app/requirements-gpu.txt""'], 'head_setup_commands': ['sudo apt-get update', 'sudo apt-get install gcc ffmpeg libsm6 libxext6  -y', 'pip install -r ""/app/requirements-gpu.txt""'], 'worker_setup_commands': ['sudo apt-get update', 'sudo apt-get install gcc ffmpeg libsm6 libxext6  -y', 'pip install -r ""/app/requirements-gpu.txt""'], 'head_start_ray_commands': ['ray stop', 'ulimit -c unlimited && export RAY_health_check_timeout_ms=30000 && ray start --head --node-ip-address=164.52.201.70 --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml --dashboard-host=0.0.0.0 --disable-usage-stats --log-color=auto -v'], 'worker_start_ray_commands': ['ray stop', 'ray start --address=164.52.201.70:6379 --object-manager-port=8076'], 'file_mounts': {'~/.ssh/id_rsa': '/home/ray/.ssh/id_rsa', '/app/requirements-gpu.txt': '/app/requirements-gpu.txt'}, 'cluster_synced_files': [], 'file_mounts_sync_continuously': False, 'rsync_exclude': ['**/.git', '**/.git/**'], 'rsync_filter': ['.gitignore'], 'provider': {'type': 'local', 'head_ip': '164.52.201.70', 'worker_ips': ['216.48.179.215']}, 'available_node_types': {'local.cluster.node': {'node_config': {}, 'resources': {}, 'min_workers': 1, 'max_workers': 1}}, 'head_node_type': 'local.cluster.node', 'max_workers': 1, 'no_restart': False}
2023-10-08 23:13:33,509 INFO monitor.py:385 -- Autoscaler has not yet received load metrics. Waiting.
2023-10-08 23:13:38,522 INFO autoscaler.py:141 -- The autoscaler took 0.0 seconds to fetch the list of non-terminated nodes.
2023-10-08 23:13:38,522 INFO autoscaler.py:421 -- 
======== Autoscaler status: 2023-10-08 23:13:38.522726 ========
Node status
---------------------------------------------------------------
Healthy:
 1 local.cluster.node
Pending:
 (no pending nodes)
Recent failures:
 (no failures)

Resources
---------------------------------------------------------------
Usage:
 0.0/12.0 CPU
 0.0/1.0 GPU
 0B/28.57GiB memory
 0B/14.29GiB object_store_memory

Demands:
 (no resource demands)
2023-10-08 23:13:38,524 INFO autoscaler.py:1379 -- StandardAutoscaler: Queue 1 new nodes for launch
2023-10-08 23:13:38,524 INFO autoscaler.py:464 -- The autoscaler took 0.002 seconds to complete the update iteration.
2023-10-08 23:13:38,524 INFO node_launcher.py:177 -- NodeLauncher0: Got 1 nodes to launch.
2023-10-08 23:13:38,525 INFO monitor.py:415 -- :event_summary:Resized to 12 CPUs, 1 GPUs.
2023-10-08 23:13:38,526 INFO node_provider.py:114 -- ClusterState: Writing cluster state: ['216.48.179.215', '164.52.201.70']
2023-10-08 23:13:38,526 INFO node_launcher.py:177 -- NodeLauncher0: Launching 1 nodes, type local.cluster.node.
2023-10-08 23:13:43,534 INFO autoscaler.py:141 -- The autoscaler took 0.0 seconds to fetch the list of non-terminated nodes.
2023-10-08 23:13:43,534 INFO autoscaler.py:421 -- 
======== Autoscaler status: 2023-10-08 23:13:43.534774 ========
Node status
---------------------------------------------------------------
Healthy:
 1 local.cluster.node
Pending:
 216.48.179.215: local.cluster.node, uninitialized
Recent failures:
 (no failures)

Resources
---------------------------------------------------------------
Usage:
 0.0/12.0 CPU
 0.0/1.0 GPU
 0B/28.57GiB memory
 0B/14.29GiB object_store_memory

Demands:
 (no resource demands)
2023-10-08 23:13:43,537 INFO autoscaler.py:1326 -- Creating new (spawn_updater) updater thread for node 216.48.179.215.`
```",loaded cluster state fetched warning permanently added list known starting monitor ray installation ray version ray commit monitor command starting metric server port monitor loaded cluster state writing cluster state false false false true true true update install install update install install update install install stop unlimited export ray start head stop start false false yet received load metric waiting took fetch list status node status healthy pending pending recent usage memory resource queue new launch took complete update iteration got launch writing cluster state type took fetch list status node status healthy pending recent usage memory resource new thread node,issue,positive,positive,neutral,neutral,positive,positive
1752117797,"@larrylian Apparently, after starting to load the shared lib it skips it.

```shell
[2023-10-08 19:47:33,221 I 1456 1456] event.cc:234: Set ray event level to warning
[2023-10-08 19:47:33,221 I 1456 1456] event.cc:342: Ray Event initialized for CORE_WORKER
[2023-10-08 19:47:33,222 I 1456 1478] accessor.cc:611: Received notification for node id = 5fb26f369578b441f7866b3343d9ea6757cbd3cebdd4f9c98d792f79, IsAlive = 1
[2023-10-08 19:47:33,222 I 1456 1478] core_worker.cc:4100: Number of alive nodes:1
[2023-10-08 19:47:33,222 I 1456 1456] abstract_ray_runtime.cc:55: Native ray runtime started.
[2023-10-08 19:47:33,222 I 1456 1456] function_helper.cc:156: LD_LIBRARY_PATH: /home/theadmin/.local/lib/python3.8/site-packages/ray/cpp/lib:/home/theadmin/test/build/libREMOTE_LIB.so
[2023-10-08 19:47:33,222 I 1456 1456] function_helper.cc:28: Start loading the library ""/home/theadmin/test/build/libREMOTE_LIB.so"".
[2023-10-08 19:47:33,224 I 1456 1456] function_helper.cc:72: The library ""/home/theadmin/test/build/libREMOTE_LIB.so"" isn't integrated with Ray, skip it.

``` ",apparently starting load shell set ray event level warning ray event received notification node id number alive native ray start loading library library ray skip,issue,negative,positive,neutral,neutral,positive,positive
1752022647,"@MMorente   This is quite weird. Now we can only rely on the log to debug. You can follow my tips below to check the specific error.

1. Viewing log files  '/tmp/ray/session_latest/logs/cpp-core-worker-xxxxx.log
![image](https://github.com/ray-project/ray/assets/11072802/a535de45-9922-40f6-9570-2a74dc096543)

2. Check if xx.so have be loaded by log ""Start loading the library""
![image](https://github.com/ray-project/ray/assets/11072802/9128c656-953a-4140-b75b-55f88e9a3f9b)

3. Check if xx.so have remote function by log ""is loaded successfully. The remote functions: xxx""

4. You also can show me your cpp-core-worker-xx.log ",quite weird rely log follow check specific error log image check loaded log start loading library image check remote function log loaded successfully remote also show,issue,negative,negative,negative,negative,negative,negative
1751799666,"What about using [libmamba solver](https://conda.github.io/conda-libmamba-solver/)? Although it has some differences with the classic solver, it won't probably require any change in Ray core. If users want to opt-in for the mamba solver, they can just install it and set it as default in their dockerfiles.",solver although classic solver wo probably require change ray core want mamba solver install set default,issue,negative,positive,positive,positive,positive,positive
1751745433,"@larrylian I have already tried that, I keep getting the same error.",already tried keep getting error,issue,negative,neutral,neutral,neutral,neutral,neutral
1751741935,"@MMorente 
Moving `RAY_REMOTE(compute)` from main.cpp to remote.cpp and try again. 
`comupte` function must register in so file by `RAY_REMOTE(compute)`.
",moving compute try function must register file compute,issue,negative,neutral,neutral,neutral,neutral,neutral
1751656144,"Our project [OpenLLMAI/OpenLLaMA2](https://github.com/OpenLLMAI/OpenLLaMA2) implements a high performance RLHF framework based on Ray and DeepSpeed. With Ray's great flexibility, we can do 34B LLaMA2 PPO training on a single DGX-A100 node with ZeRO-2, which achieve high speed for text generation when make experience.

For anyone interested, you can get started at: https://github.com/OpenLLMAI/OpenLLaMA2/blob/main/examples/train_ppo_ray.py",project high performance framework based ray ray great flexibility llama training single node achieve high speed text generation make experience anyone interested get,issue,positive,positive,positive,positive,positive,positive
1751622527,"Can you help test how it looks like now on different terminals and different bgs (white, dark, grey)? Don't need to share the results but just do a sanity check and make sure they all have reasonable color contrast.",help test like different different white dark grey need share sanity check make sure reasonable color contrast,issue,positive,positive,neutral,neutral,positive,positive
1751622259,"Thanks for the investigation!
Yeah. I think it's fine to remove it.",thanks investigation yeah think fine remove,issue,positive,positive,positive,positive,positive,positive
1751617004,"> Adding argc and argv to the init method makes that warning go away, but the output is still the same:
> 
> ray::internal::RayFunctionNotFound: Executable function not found, the function name compute

@ManuelMorente 
1. Can you show your startup command?
2. Do you set the param (--ray_code_search_path=xxxxx/xxx.so ) ?",method warning go away output still ray executable function found function name compute show command set param,issue,negative,neutral,neutral,neutral,neutral,neutral
1751559216,"The original build may lack some package.
After running`pip install opencensus prometheus_client aiohttp aiohttp-cors pydantic grpcio six`
, then you can use dashboard.",original build may lack package running pip install six use dashboard,issue,negative,positive,positive,positive,positive,positive
1751553597,"> I am not exactly sure why there's such issue. It can happen due to network error (which is not common because both gcs_server and dashboard runs in the same host). 2. The GCS is overloaded and responds slowly (which is also uncommon because you said it happens at idle time).
> 
> you can alternatively modify this behavior by setting env var when you run ray start --head
> 
> GCS_CHECK_ALIVE_MAX_COUNT_OF_RPC_ERROR=40 by default GCS_CHECK_ALIVE_RPC_TIMEOUT=10 (seconds) by default

This fixed my problem, it's still alive after a week。My args are:
```
export GCS_CHECK_ALIVE_MAX_COUNT_OF_RPC_ERROR=100
export GCS_CHECK_ALIVE_RPC_TIMEOUT=90
``` ",exactly sure issue happen due network error common dashboard host slowly also uncommon said idle time alternatively modify behavior setting run ray start head default default fixed problem still alive export export,issue,negative,positive,positive,positive,positive,positive
1751543256,@tiangolo Looks like this is closed by mistake. Could you reopen this? I'm running into this issue.,like closed mistake could reopen running issue,issue,negative,negative,neutral,neutral,negative,negative
1751543013,"> @harborn
> 
> > Is there a separate channel for discussions related to further integration/development (such as slack/discord etc?)
> 
> Could you reach out to me on Ray slack? We should set up a collaboration channel.

OK, reach you on Slack.",separate channel related could reach ray slack set collaboration channel reach slack,issue,negative,neutral,neutral,neutral,neutral,neutral
1751540822,"Hi @scottsun94, I've found it! There indeed exists a setting that also changes the colorama configuration.

Basically, [here](https://github.com/ray-project/ray/blob/master/python/ray/_private/worker.py#L1978) it adds `colorama.Style.DIM`, which is used to create a dim or faint appearance of the text, making it less bright compared to the normal text.

Could I remove `colorama.Style.DIM` and follow your suggestion to also remove the `BRIGHT` style? I think removing colorama.Style.DIM is reasonable because, according to the [colorama documentation](https://github.com/tartley/colorama#description), on Windows, Colorama does not support ANSI 'dim text'; it looks the same as 'normal text'. This can also be confirmed by my previous screenshot, so to make it consistent with Mac and Linux terminals, removing it seems reasonable.

By the way, currently colorama.Style.DIM affects all worker logs, like raylet, autoscaler. But I think removing it is reasonable as on Windows, Colorama does not support ANSI 'dim text'.

I wonder if this seems good to you. Thank you in advance!

Here is the final screenshot(remove `colorama.Style.DIM` and remove the `BRIGHT` style):
<img width=""1920"" alt=""image"" src=""https://github.com/ray-project/ray/assets/51814063/92be3658-1465-4b8b-b2ff-8c532517a67d"">
",hi found indeed setting also configuration basically used create dim faint appearance text making le bright normal text could remove follow suggestion also remove bright style think removing reasonable according documentation support text text also confirmed previous make consistent mac removing reasonable way currently worker like raylet think removing reasonable support text wonder good thank advance final remove remove bright style image,issue,positive,positive,positive,positive,positive,positive
1751531899,"> > Would changing the payload size limit not be efficient?
> 
> I think limit change is not necessary. But 4MB seems too small anyway (especially given ray default is 512MB). I can revert it if you strongly feel like it.

I was actually thinking if limit change is enough to tackle the issue? ",would size limit efficient think limit change necessary small anyway especially given ray default revert strongly feel like actually thinking limit change enough tackle issue,issue,positive,positive,neutral,neutral,positive,positive
1751528611,"> @WeichenXu123 When you're back from vacation, could add tests? My bad -- I should've caught this in an earlier review and before merging

The e2e test requires databricks environment, we will add e2e test in our databricks code repo to monitor the Ray reader works.

For unit test in Ray repo, we can only add mocking test, we can add it if you need it.",back vacation could add bad caught review test environment add test code monitor ray reader work unit test ray add test add need,issue,negative,negative,negative,negative,negative,negative
1751519167,"Ok. Both release tests passed!

Resolved the merge conflicts and run the full CI again.",release resolved merge run full,issue,negative,positive,positive,positive,positive,positive
1751518558,"I read this resource on chunking which was really nice.  This improved the speed of my computations by 2x!  Though my average computation speed is still around 2s.  Another 2x speedup would solve all of my issues.  Basically I create the chunk size such that each core gets one chunk.

https://github.com/ray-project/ray-educational-materials/blob/main/Ray_Core/Ray_Core_5_Best_Practices.ipynb

```
            @ray.remote
            def combine_shader_polygons(
                    start,
                    end,
                    shading_candidates_id,
                    reference_gdf_id
            ):
                shader_polygons = []
                for i in range(start, end):
                    shader_indices = np.flatnonzero(shading_candidates_id[i])
                    if len(shader_indices) == 0:
                        shader_polygon = None
                    elif len(shader_indices) == 1:
                        shader_polygon = reference_gdf_id.loc[shader_indices].iloc[0]
                    else:
                        polys = reference_gdf_id.loc[shader_indices]
                        shader_polygon = polys.unary_union
                    shader_polygons.append(shader_polygon)
                return shader_polygons
            
            shading_candidates_id = ray.put(shading_candidates_np)
            reference_gdf_id = ray.put(reference_gdf)
            chunk_size = int(len(reference_gdf) / num_cores)

            futures = []
            current_start = 0
            current_end = chunk_size
            while current_start <= len(reference_gdf):
                future = combine_shader_polygons.remote(
                    start=current_start,
                    end=current_end,
                    shading_candidates_id=shading_candidates_id,
                    reference_gdf_id=reference_gdf_id
                )
                futures.append(future)

                current_start += chunk_size
                current_end += chunk_size
                if current_end > len(reference_gdf):
                    current_end = len(reference_gdf)

            nested_shader_polygons = ray.get(futures)
            shader_polygons = [item for sublist in nested_shader_polygons for item in sublist]
```",read resource really nice speed though average computation speed still around another would solve basically create chunk size core one chunk start end range start end none else return future future item item,issue,positive,positive,positive,positive,positive,positive
1751497036,"Apologies for the stream of consciousness comments that I have been posting.  I am attempting to get more familiar with Ray and posting observations as I go.


Another thing that I have noticed is that if I add a number of CPU's to the @ray_remove decorator, the speed gets much slower

```
        @ray.remote(num_cpus=19)
        def combine_shader_polygons(i, shading_candidates_id, reference_gdf_id):
            shader_indices = np.flatnonzero(shading_candidates_id[i])
            if len(shader_indices) == 0:
                shader_polygon = None
            elif len(shader_indices) == 1:
                shader_polygon = reference_gdf_id.loc[shader_indices].iloc[0]
            else:
                polys = reference_gdf_id.loc[shader_indices]
                shader_polygon = polys.unary_union
            return shader_polygon
```",stream consciousness posting get familiar ray posting go another thing add number decorator speed much none else return,issue,negative,positive,positive,positive,positive,positive
1751478908,"> Thanks @scottjlee, this solution is cleaner than the previous one.
> 
> Also, can you elaborate more on the RLLib Learner issue?

Yeah, the previous implementation, which added a new parameter into `RayTrainWorker.__init__()`, was incompatible with `Learner` which doesn't have this extra parameter (and we do not want to change this RLLib API).",thanks solution cleaner previous one also elaborate learner issue yeah previous implementation added new parameter incompatible learner extra parameter want change,issue,positive,positive,neutral,neutral,positive,positive
1751472433,"> Would changing the payload size limit not be efficient?

I think limit change is not necessary. But 4MB seems too small anyway (especially given ray default is 512MB). I can revert it if you strongly feel like it. ",would size limit efficient think limit change necessary small anyway especially given ray default revert strongly feel like,issue,positive,positive,neutral,neutral,positive,positive
1751445929,"CI run with ML / RL tests passing: https://buildkite.com/ray-project/oss-ci-build-pr/builds/37993

Going to now revert manual enabling the RL tests trigger.",run passing going revert manual trigger,issue,negative,neutral,neutral,neutral,neutral,neutral
1751436036,the original issue due to this issue; https://github.com/ray-project/ray/issues/40189,original issue due issue,issue,negative,positive,positive,positive,positive,positive
1751432264,"@aslonnie that's right, aside from building the wheel (5 minutes), i think the bulk of the time remained is for pushing 6 images to docker",right aside building wheel think bulk time pushing docker,issue,negative,positive,positive,positive,positive,positive
1751429094,"> @aslonnie they still build sequentially

but the wheel will only be built once I guess.",still build sequentially wheel built guess,issue,negative,neutral,neutral,neutral,neutral,neutral
1751373212,"<div><img src=""https://media2.giphy.com/media/dr6toZX3D1O8/200.gif?cid=5a38a5a222qtcuxm7gjeh96l178ynga38wbbssbz5hk3h3h5&amp;ep=v1_gifs_search&amp;rid=200.gif&amp;ct=g"" style=""border:0;height:162px;width:300px""/><br/>via <a href=""https://giphy.com/gifs/dr6toZX3D1O8"">GIPHY</a></div>",div border height width via,issue,negative,neutral,neutral,neutral,neutral,neutral
1751370435,"## Attention: External code changed

This PR changes code that is used or cited in external sources, e.g. blog posts.

Before merging this PR, please make sure that the code in the external sources is still working, and consider updating them to reflect the changes.

The affected files and the external sources are:
- `doc/external/pytorch_tutorials_hyperparameter_tuning_tutorial.py`: https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html",attention external code code used external please make sure code external still working consider reflect affected external,issue,negative,positive,neutral,neutral,positive,positive
1751295496,"@WeichenXu123 When you're back from vacation, could add tests? My bad -- I should've caught this in an earlier review and before merging",back vacation could add bad caught review,issue,negative,negative,negative,negative,negative,negative
1751286404,"> The new API has the same underlying performance characteristics? I.e. we can merge this without being concerned about a degradation in throughput from this?

~~There might be a slight degradation in throughput~~. `random_shuffle` should have the same performance as `random_shuffle_each_window`, so there's no reason not to use `random_shuffle`
",new underlying performance merge without concerned degradation throughput might slight degradation performance reason use,issue,positive,negative,neutral,neutral,negative,negative
1751165102,"Something that I found interesting is that if I do nothing inside of the ray.remote decorated function, the ray.get still takes 2.7s

```
        # --- Populate Shaders ---
        start = time.time()

        @ray.remote
        def combine_shader_polygons(
                x,
                ref_gdf_ray,
                shading_candidates_ray
        ):
            pass
            # shader_polygon = None
            # shader_indices = np.flatnonzero(shading_candidates_ray[x])
            # if len(shader_indices) == 0:
            #     pass
            # elif len(shader_indices) == 1:
            #     shader_polygon = ref_gdf_ray.loc[shader_indices].iloc[0]
            # else:
            #     polygons = ref_gdf_ray.loc[shader_indices]
            #     shader_polygon = polygons.unary_union
            # return shader_polygon

        # --- Try with Ray: ---
        shading_candidates_id = ray.put(shading_candidates_np)
        reference_gdf_id = ray.put(reference_gdf)
        timer = round(time.time() - start, 2)
        print(f'Checkpoint 0: {timer}s')

        shader_polygons = ray.get([
            combine_shader_polygons.remote(
                x,
                reference_gdf_id,
                shading_candidates_id
            ) for x in range(len(reference_polygons))
        ])

        timer = round(time.time() - start, 2)
        print(f'Ray Speed: {timer}s')
```",something found interesting nothing inside decorated function still populate start pas none pas else return try ray timer round start print timer range timer round start print speed timer,issue,negative,positive,neutral,neutral,positive,positive
1751149073,"> We can address them in follow-up PRs, since this case rarely happens

Sounds good. Thanks for your work on this PR!
",address since case rarely good thanks work,issue,negative,positive,positive,positive,positive,positive
1750969718,"I’d like to specify multiple spot instance types and if one request fails because of a lack of capacity it tries the next.

In general it’s just not clear what happens if the user specifies multiple node types with the same resources. From glancing at the code it will just use the first one that can satisfy the requirements, but I’d like to be sure.

On Thu, 28 Sep 2023, at 01:28, Ricky Xu wrote:
> 
> 
> What's the usecases for multiple available_node_types here? Maybe just some high-level examples would be really helpful!
> 
> 
> —
> Reply to this email directly, view it on GitHub <https://github.com/ray-project/ray/issues/39788#issuecomment-1738237143>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AAE2WREDXUNKJM42TYQRXY3X4SZCXANCNFSM6AAAAAA5B524WQ>.
> You are receiving this because you were assigned.Message ID: ***@***.***>
> 
",like specify multiple spot instance one request lack capacity next general clear user multiple node glancing code use first one satisfy like sure wrote multiple maybe would really helpful reply directly view id,issue,positive,positive,positive,positive,positive,positive
1750740498,"> we don't think there's anything we've left out

The one major exception is that we have not attempted any Actor support at this point (it's on our roadmap but wanted to prove out the feasibility with a less ambitious subset of the Ray Core)",think anything left one major exception actor support point prove feasibility le ambitious subset ray core,issue,positive,positive,positive,positive,positive,positive
1750620029,"> That's very impressive. Where are the Julia implementations of Ray APIs like ray.remote, ray.get?

Thanks! The implementations for `Ray.put` and `Ray.get` are found in [`src/object_store.jl`](https://github.com/beacon-biosignals/Ray.jl/blob/main/src/object_store.jl#L1-L36) while our task submission is currently implemented via [`submit_task`](https://github.com/beacon-biosignals/Ray.jl/blob/db0f98b0c6484ba89834ec9e59cc3a8317a04408/src/runtime.jl#L206-L228) which we plan to clean up / wrap in a Ray-like macro  (`Ray.@remote`)  in due course.
",impressive ray like thanks found task submission currently via plan clean wrap macro ray remote due course,issue,positive,positive,positive,positive,positive,positive
1750600388,"> is this all the changes needed to integrate to https://github.com/beacon-biosignals/Ray.jl?

We believe so. We've successfully run Ray jobs from a julia driver process that launches julia workers that submits tasks and puts/gets objects in the object store so we don't _think_ there's anything we've left out. But we're open to correction if any experienced ray developers spot that we missed something.",integrate believe successfully run ray driver process object store anything left open correction experienced ray spot something,issue,negative,positive,positive,positive,positive,positive
1750394201,"> Maybe we can post this to the docs somewhere?

is there a good beginner guide of ray 
 ",maybe post somewhere good beginner guide ray,issue,negative,positive,positive,positive,positive,positive
1750151574,"Found an issue when supporting autoscaling mode: autoscaling mode requires the notebook REPL process keep running to create / cancel spark job when ray scale up / down.  If user calls “setup_ray_cluster(autoscale=True, global_mode=True)” in a notebook, then detaches the notebook, and then starting a new notebook running ray application, it does not work. I am investigating solutions.",found issue supporting mode mode notebook process keep running create cancel spark job ray scale user notebook notebook starting new notebook running ray application work investigating,issue,positive,positive,positive,positive,positive,positive
1750113821,"The fix should be wait until core worker is initialized before running this; 
```
    # Setup tracing here
    tracing_hook_val = worker.gcs_client.internal_kv_get(
        b""tracing_startup_hook"", ray_constants.KV_NAMESPACE_TRACING
    )
    if tracing_hook_val is not None:
        ray.util.tracing.tracing_helper._enable_tracing()
        if not getattr(ray, ""__traced__"", False):
            _setup_tracing = _import_from_string(tracing_hook_val.decode(""utf-8""))
            _setup_tracing()
            ray.__traced__ = True
```

Btw @alexeykudinkin what's the priority for this? We have pretty big list of P0 bugs now (16), so unless it is critical from your end, we may want to fix it in 2.9",fix wait core worker running setup tracing none ray false true priority pretty big list unless critical end may want fix,issue,negative,positive,neutral,neutral,positive,positive
1750112031,I think it is actually reasonable default as it affects performance quite a bit (if you create a cluster by ray.init you usually schedule tasks immediately). I have never seen users complaining this behavior as well except that particular issue that is very unique (2 CPU with 128 concurrent workers),think actually reasonable default performance quite bit create cluster usually schedule immediately never seen behavior well except particular issue unique concurrent,issue,negative,positive,positive,positive,positive,positive
1750110002,"We have no capacity now to handle this as 2.8 P0. We have several tasks to unflake flaky linux tests scoped for Ray 2.8. For the rest of them, we may need to handle it from Ray 2.9. cc @xieus 

I readjused the priority for now. Note that ""we still have 5 P0s to fix flaky linux tests from flaky dashboard"". So we are still working towards it. ",capacity handle several flaky ray rest may need handle ray priority note still fix flaky flaky dashboard still working towards,issue,negative,neutral,neutral,neutral,neutral,neutral
1750073882,@iycheng we will also send multiple batch requests actually. I am figuring out what's the best default,also send multiple batch actually best default,issue,positive,positive,positive,positive,positive,positive
1750019008,"I see, Will try to find out where it has also been changed.",see try find also,issue,negative,neutral,neutral,neutral,neutral,neutral
1750016840,"> It is possible. I haven't dived deep into it to see if it has also been changed somewhere. If there is such a need, I am happy to look into it! But based on the final appearance, I wonder if you think we should change or keep the original code.

If the log color can be the same yellow as produced solely through Colorama. I think we should just remove the `BRIGHT` style. Yellow itself should work well on all 3 backgrounds (white, grey and dark). However, we need to make sure the log can print the same yellow first.",possible deep see also somewhere need happy look based final appearance wonder think change keep original code log color yellow produced solely think remove bright style yellow work well white grey dark however need make sure log print yellow first,issue,positive,positive,positive,positive,positive,positive
1750009461,"It is possible. I haven't dived deep into it to see if it has also been changed somewhere. If there is such a need, I am happy to look into it! But based on the final appearance, I wonder if you think we should change or keep the original code.",possible deep see also somewhere need happy look based final appearance wonder think change keep original code,issue,positive,positive,positive,positive,positive,positive
1750005851,"> > I see: we actually don't use light/bright yellow. We just use the bright as the style.
> 
> I see!

lol... This is something I just learnt from you I guess? I never read this piece of code before.


> Yes, I have also tried using Colorama previously. For some reason, the color of the real log and the color produced solely through Colorama are different. Below is a screenshot of my Mac terminal using just Colorama. I think we get the same color. <img alt=""image"" width=""1920"" src=""https://user-images.githubusercontent.com/51814063/273111229-1f0a1b0b-6668-4bde-a9ca-72484ca6d808.png"">

Hmm. Then it seems that the colorama setting is changed by ray somewhere else?

",see actually use yellow use bright style see something learnt guess never read piece code yes also tried previously reason color real log color produced solely different mac terminal think get color image setting ray somewhere else,issue,positive,positive,positive,positive,positive,positive
1750000454,"> I see: we actually don't use light/bright yellow. We just use the bright as the style.

I see!

> I played with the colorama a bit on my basic mac terminal. The color looks a bit different from your test.

Yes, I have also tried using Colorama previously. For some reason, the color of the real log and the color produced solely through Colorama are different. Below is a screenshot of my Mac terminal using just Colorama. I think we get the same color.
<img width=""1920"" alt=""image"" src=""https://github.com/ray-project/ray/assets/51814063/1f0a1b0b-6668-4bde-a9ca-72484ca6d808"">",see actually use yellow use bright style see bit basic mac terminal color bit different test yes also tried previously reason color real log color produced solely different mac terminal think get color image,issue,positive,positive,neutral,neutral,positive,positive
1749997302,I think maybe streaming important for transferring big chunks of data.,think maybe streaming important transferring big data,issue,negative,positive,positive,positive,positive,positive
1749990205,"I see: we actually don't use light/bright yellow. We just use the bright as the style.

I played with the colorama a bit on my basic mac terminal. The color looks a bit different from your test.

<img width=""522"" alt=""Screenshot 2023-10-05 at 10 05 57 PM"" src=""https://github.com/ray-project/ray/assets/9677264/34759e83-eaa7-4e09-b0d0-dea00748f5de"">


",see actually use yellow use bright style bit basic mac terminal color bit different test,issue,negative,positive,positive,positive,positive,positive
1749962877,Oh I was talking about core side change (the core side fix),oh talking core side change core side fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1749914550,"> if you have a cluster with more than 750 cores I can see this being an issue. IIRC we ran into some rate limiting errors when we performed internal testing.

> Ray Tasks don't retry application-level exceptions by default. So, if you get a rate limiting error, I don't think it'd get retried.


We can address them in follow-up PRs, since this case rarely happens",cluster see issue ran rate limiting internal testing ray retry default get rate limiting error think get address since case rarely,issue,negative,positive,positive,positive,positive,positive
1749885905,"@stephanie-wang I'm so sorry for not getting to this earlier. I made some copy edits and tried to put it into a commit to make it easier for you, but I must've done something wrong. Here is a PR with my commit: https://github.com/ray-project/ray/pull/40176

The code examples have references to the Ray AIR API which has been move to Ray Train. Are you still planning to merge this PR? If so, let me know and I can work on updating the code examples.",sorry getting made copy tried put commit make easier must done something wrong commit code ray air move ray train still merge let know work code,issue,negative,negative,negative,negative,negative,negative
1749851345,"getting this error trying to install ray[default] on a python:latest docker image

```
root@1b2f0114dfb1:/workspaces/asvc# pip install ray[default]
ERROR: Could not find a version that satisfies the requirement ray[default] (from versions: none)
ERROR: No matching distribution found for ray[default]```",getting error trying install ray default python latest docker image root pip install ray default error could find version requirement ray default none error matching distribution found ray default,issue,negative,positive,positive,positive,positive,positive
1749835539,"> Seems that using `git push -f` will automatically tag `ray-project/ray-ci` for review?

that should not be the case.. no idea..

you can try if you can reproduce it.

it is more likely that there was a messy rebase or something.",git push automatically tag review case idea try reproduce likely messy rebase something,issue,negative,negative,neutral,neutral,negative,negative
1749827593,"So cluster launcher worked for me for the last +2 years using a local cluster (without Docker, just conda env). Think it was 2.6.0 before I made the mistake of upgrading, if I remember correctly. Think I'll just start writing my own tests and run before every upgrade.",cluster launcher worked last local cluster without docker think made mistake remember correctly think start writing run every upgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
1749827537,@aslonnie Seems that using `git push -f` will automatically tag `ray-project/ray-ci` for review?,git push automatically tag review,issue,negative,neutral,neutral,neutral,neutral,neutral
1749826074,"""without the BRIGHT setting"" means I delete `colorama.Style.BRIGHT` in source code.

from
```python
        elif data.get(""pid"") == ""autoscaler"":
            if ""Error:"" in line or ""Warning:"" in line:
                return colorama.Style.BRIGHT + colorama.Fore.YELLOW
            else:
                return colorama.Style.BRIGHT + colorama.Fore.CYAN
```
to

```python
        elif data.get(""pid"") == ""autoscaler"":
            if ""Error:"" in line or ""Warning:"" in line:
                return colorama.Fore.YELLOW
            else:
                return colorama.Fore.CYAN
```",without bright setting delete source code python error line warning line return else return python error line warning line return else return,issue,negative,positive,positive,positive,positive,positive
1749822637,"> only change the autoscaler error color to bright red(see below)

Using ""red"" probably is not the way to. I think there is a reason why we didn't use red initially. Red usually indicates real errors - a stop of partial/complete functionality. However, the autoscaler ""errors"" here are more like warnings that can be resolved by itself via autoscaling. It's more appropriate to use yellow. (a separate thing we should improve is the wording of those messages).


> Log in Mac's terminal: On the left, tests were conducted on Ray 1.13.0 and the master branch of Ray. On the right, tests were done without the BRIGHT setting.

When you say ""without the BRIGHT setting"", what do you mean? You mean using `Yellow` instead of `Bright yellow` or terminal setting like this?

<img width=""182"" alt=""Screenshot 2023-10-05 at 5 16 25 PM"" src=""https://github.com/ray-project/ray/assets/9677264/f4bf2545-e93f-4a79-86c6-3c413d942502"">


",change error color bright red see red probably way think reason use red initially red usually real stop functionality however like resolved via appropriate use yellow separate thing improve wording log mac terminal left ray master branch ray right done without bright setting say without bright setting mean mean yellow instead bright yellow terminal setting like,issue,positive,positive,positive,positive,positive,positive
1749805864,"~~I'm able to reproduce this on AWS using `pip install ""ray[default]""==2.7.0` in the setup commands and using the latest ray master on the client side for the cluster launcher.~~[see below, it was just a port issue on my end]

@jmakov do you happen to remember if this was working for you on a previous version of Ray, and if so which one?",able reproduce pip install ray default setup latest ray master client side cluster see port issue end happen remember working previous version ray one,issue,negative,positive,positive,positive,positive,positive
1749804197,"2 high-level questions:

1. There are two pieces of ray job submission info and they look different..

<img width=""709"" alt=""Screenshot 2023-10-05 at 4 01 10 PM"" src=""https://github.com/ray-project/ray/assets/9677264/b7edf84f-fa4f-45d5-9f81-9c399137d057"">
<img width=""839"" alt=""Screenshot 2023-10-05 at 4 01 13 PM"" src=""https://github.com/ray-project/ray/assets/9677264/97f47a20-a514-4d4e-aaf6-108c2370b2cd"">

2. The output in the ""next steps"" section seems to use the local ip. The commands that use the local ip won't work right?

3. If ray dashboard is not installed, will we hide the related output?",two ray job submission look different output next section use local use local wo work right ray dashboard hide related output,issue,negative,positive,neutral,neutral,positive,positive
1749797321,"Hi, @scottsun94, here's what I found:

Ray uses colorama, so it essentially utilizes ANSI code. The actual color is controlled by the Terminal Emulator (how it interprets the ANSI colors). Mac, Windows and Ubuntu have their own Terminal Emulator. 

Given these considerations, I believe it's reasonable to use the default config and test the colors on Mac, Windows, Ubuntu, and in VSCode(IDEs might also interpret the ANSI colors differently)

For further verification, I tested with both Ray 1.13.0 (the release version when this issue was opened) and the master branch of Ray.

As you can see from the screenshot below:

1.  the current colors appear quite satisfactory. No “too bright” problem(might because mac updates its Terminal Emulator since the issues created).
2. Some cases, logs without BRIGHT looks worse(bright aslo bolds the text, See issue https://github.com/tartley/colorama/issues/158)

Thus, my thought is that we retain the brightness and only change the autoscaler error color to bright red(see below). I'd like to know if this sounds good to you. Thank you in advance!



**Log in Mac's terminal:** On the left, tests were conducted on Ray 1.13.0 and the master branch of Ray. On the right, tests were done without the BRIGHT setting.
<img width=""1728"" alt=""1"" src=""https://github.com/ray-project/ray/assets/51814063/31c2a19b-3200-4ed0-bfd7-bff9e1a6eb9e"">




**Log in Ubuntu’s terminal:** On the left, tests were conducted on Ray 1.13.0 and the master branch of Ray. On the right, tests were done without the BRIGHT setting
<img width=""1728"" alt=""2"" src=""https://github.com/ray-project/ray/assets/51814063/43e868f7-01c4-41d3-a913-751900fd84f8"">


**Log in Windows terminal:** On the left, tests were conducted on Ray 1.13.0 and the master branch of Ray. On the right, tests were done without the BRIGHT setting

<img width=""1728"" alt=""3"" src=""https://github.com/ray-project/ray/assets/51814063/abfe72c9-334e-4690-bf24-5bffe99b47dd"">


**Log in VSCode** **with a gray background:** On the left, tests were conducted on Ray 1.13.0 and the master branch of Ray. On the right, tests were done without the BRIGHT setting
<img width=""1728"" alt=""4"" src=""https://github.com/ray-project/ray/assets/51814063/69b4f9c2-bb29-4496-8f0d-593ca91c0e10"">


**Log after changing the autoscaler error color to red in Mac's terminal with default config:**

The left side is with brightness, while the right side is tested without BRIGHT.
<img width=""1728"" alt=""5"" src=""https://github.com/ray-project/ray/assets/51814063/e9b0161e-c24d-4e5a-bd5b-77268c30c4f9"">



**Log after changing the autoscaler error color to red in Ubuntu’s terminal:**

The left side is with brightness, while the right side is tested without BRIGHT.
<img width=""1728"" alt=""6"" src=""https://github.com/ray-project/ray/assets/51814063/9e1a3481-a01d-4b4c-b55c-c2658c7fd3bb"">


**Log after changing the autoscaler error color to red in Windows terminal:**

The left side is with brightness, while the right side is tested without BRIGHT.
<img width=""1728"" alt=""7"" src=""https://github.com/ray-project/ray/assets/51814063/17eb28e3-f570-428a-82db-262cf99c7c6e"">


**Log after changing the autoscaler error color to red in VSCode** **with a gray background:**

The left side is with brightness, while the right side is tested without BRIGHT.
<img width=""1728"" alt=""8"" src=""https://github.com/ray-project/ray/assets/51814063/a211855c-50e3-4a77-bb9b-f7d7798336f9"">

",hi found ray essentially code actual color terminal emulator color mac terminal emulator given believe reasonable use default test color mac ides might also interpret color differently verification tested ray release version issue master branch ray see current color appear quite satisfactory bright problem might mac terminal emulator since without bright worse bright text see issue thus thought retain brightness change error color bright red see like know good thank advance log mac terminal left ray master branch ray right done without bright setting log terminal left ray master branch ray right done without bright setting log terminal left ray master branch ray right done without bright setting log gray background left ray master branch ray right done without bright setting log error color red mac terminal default left side brightness right side tested without bright log error color red terminal left side brightness right side tested without bright log error color red terminal left side brightness right side tested without bright log error color red gray background left side brightness right side tested without bright,issue,positive,positive,positive,positive,positive,positive
1749768979,"> If we intend to deprecate/remove StatsActor it might be easier if we keep it separate? I am cool with reusing it as well.

Yeah please don't create a new actor in this case then. You can add new methods to the actor and deprecate old methods, without having two similar/duplicate actors show up to users, which will be confusing.",intend might easier keep separate cool well yeah please create new actor case add new actor deprecate old without two show,issue,positive,positive,positive,positive,positive,positive
1749762400,"> Why not re-use the StatsActor?

If we intend to deprecate/remove StatsActor it might be easier if we keep it separate? I am cool with reusing it as well.",intend might easier keep separate cool well,issue,positive,positive,positive,positive,positive,positive
1749722229,"<div><img src=""https://media4.giphy.com/media/UTj6uNKzYFmJGbKdgH/200.gif?cid=5a38a5a27266fyufhbtj8e4cipoprphumrn84d4j7mx1cnj0&amp;ep=v1_gifs_search&amp;rid=200.gif&amp;ct=g"" style=""border:0;height:170px;width:300px""/><br/>via <a href=""https://giphy.com/YouMoveMe/"">You Move Me</a> on <a href=""https://giphy.com/gifs/YouMoveMe-movers-moving-day-you-move-me-UTj6uNKzYFmJGbKdgH"">GIPHY</a></div>",div border height width via move,issue,negative,neutral,neutral,neutral,neutral,neutral
1749712212,"I believe `boto3` now supports specifying the endpoint URL via environment variable (which was the initial reason for creating this issue).  See https://github.com/boto/boto3/issues/2099#issuecomment-1626117882

If we can verify that this workflow can succeed by setting the relevant variables in the `runtime_env` `""env_vars""` field, then perhaps we don't even need to add a new API here.  Instead we can just add a quick section for this workflow in the docs.",believe via environment variable initial reason issue see verify succeed setting relevant field perhaps even need add new instead add quick section,issue,negative,positive,positive,positive,positive,positive
1749710705,"Not sure if it is fixed or not, let's close and figure it out.",sure fixed let close figure,issue,negative,positive,positive,positive,positive,positive
1749686094,"The same mechanism is used by `java_jars`, `py_modules` and `working_dir`. We can find a common way to model these.

We can either type the `working_dir` value to (pseudocode) `str | Dict[""uri"" | **kwargs, str]`, accepting

- `runtime_env={""working_dir"":{""uri"":""s3://my_private_bucket/dir"", ""aws_access_key_id"":""my_key"", ""aws_secret_access_key"":""my_secret""}}`
- `runtime_env={""working_dir"":""s3://public_bucket/dir""}`

Or we can add a special field in runtime_env `_runtime_env_context` like

- `runtime_env={""working_dir"":""s3://my_private_bucket/dir"", ""_runtime_env_context"":{""aws_access_key_id"":""my_key"", ""aws_secret_access_key"":""my_secret""}}`

and let different runtime env plugins to read them.

I personally prefer the former one, but I'd like to hear from you. cc @jjyao @rkooo567 ",mechanism used find common way model either type value add special field like let different read personally prefer former one like hear,issue,positive,positive,neutral,neutral,positive,positive
1749640121,@c21 @raulchen @ericl are we okay with creating a cluster-wide actor to collect these stats (exactly like `_StatsActor`) for the ray data dashboard? I don't think the concerns [here](https://github.com/ray-project/ray/issues/31571) would hold for this case since we're directly emitting the metrics to prometheus.,actor collect exactly like ray data dashboard think would hold case since directly metric,issue,negative,positive,positive,positive,positive,positive
1749637566,"We implemented league play using this pattern in an older version of ray/rllib. But we will adjust if that is not supported.

Thanks ",league play pattern older version adjust thanks,issue,positive,positive,positive,positive,positive,positive
1749625476,I can reproduce but we don't support creating algorithms within algorithms. That's a very funky pattern. What's the use case here?,reproduce support within funky pattern use case,issue,negative,neutral,neutral,neutral,neutral,neutral
1749574258,"> > is there any proposal I can read to understand context better?
> 
> This is basically a refactor of the existing
> 
> https://github.com/ray-project/ray/blob/c21f7a209a0ccbadd3bdf73eb3bd6240fabc629c/python/ray/autoscaler/_private/resource_demand_scheduler.py#L102-L110
> 
> And the refactor is needed since the original implementation doesn't expose infeasible requests and changing it to enable tracking of infeasible constraints and placement group requests would require many forced changes on top it.

",proposal read understand context better basically since original implementation expose infeasible enable infeasible placement group would require many forced top,issue,negative,positive,positive,positive,positive,positive
1749499510,"@robcaulk You're right, not all files in the github repo are included in the package that is built and hosted at PyPI.  Omitting this particular file is an oversight on our end.  We will include this file in future releases, thanks for reporting the issue!",right included package built particular file oversight end include file future thanks issue,issue,negative,positive,positive,positive,positive,positive
1749484914,Let's move it into a separate section called Many Model Training to make it clear. This matches the header we use in the Ray Use Cases section,let move separate section many model training make clear header use ray use section,issue,negative,positive,positive,positive,positive,positive
1749483021,"I'd just make it clear that this is for when you are training thousands or more models, and when the training depends on different slices of data. You can add a link to Train for more monolithic training cases. No need to frame it as a warning, just need to set the context for the use case.",make clear training training different data add link train monolithic training need frame warning need set context use case,issue,negative,positive,neutral,neutral,positive,positive
1749475252,"We'd warn that, if you're doing regular training, you should look at the Ray Train examples. What do you think?",warn regular training look ray train think,issue,negative,neutral,neutral,neutral,neutral,neutral
1749472686,"> Got it. Has many model training been a common use case? Even if we preface this example with a warning, I'm concerned that it'll cause confusion for the majority of users who perform regular training

Yes, it's a frequent ask. Why are we even adding a warning? As far as I know nothing has changed here in regards to use cases and recommendations, so we shouldn't be making any material positioning changes.",got many model training common use case even preface example warning concerned cause confusion majority perform regular training yes frequent ask even warning far know nothing use making material,issue,negative,positive,neutral,neutral,positive,positive
1749469107,"Got it. Has many model training been a common use case? Even if we preface this example with a warning, I'm concerned that it'll cause confusion for the majority of users who perform regular training",got many model training common use case even preface example warning concerned cause confusion majority perform regular training,issue,negative,positive,neutral,neutral,positive,positive
1749461102,"> @ericl to clarify, we're still positioning Ray Data as the recommended solution for many model training?

Yes, nothing has changed here as far as I am aware, and we also don't have any other better solution here. This falls under the category of ""generic bulk parallel processing"".",clarify still ray data solution many model training yes nothing far aware also better solution category generic bulk parallel,issue,positive,positive,positive,positive,positive,positive
1749448824,"> Er, I don't think we've decided to do (2).

@ericl to clarify, we're still positioning Ray Data as the recommended solution for many model training?",er think decided clarify still ray data solution many model training,issue,negative,positive,positive,positive,positive,positive
1749442125,@rkooo567 we can't control application's code. I think the primary issue here is that the Core worker crashes instead of handling it gracefully,ca control application code think primary issue core worker instead handling gracefully,issue,positive,positive,positive,positive,positive,positive
1749433872,"SplitBlocks works within the read task to split the read output into multiple smaller pieces. These will remain as smaller individual blocks for the remainder of the computation unless the dataset is explicitly repartitioned.

Ray Data will automatically insert SplitBlocks to ensure the desired/autodetected parallelism is met after a read.",work within read task split read output multiple smaller remain smaller individual remainder computation unless explicitly ray data automatically insert ensure parallelism met read,issue,negative,neutral,neutral,neutral,neutral,neutral
1749432790,ok then let's either keep the batch training example in a new section called Many Model Training or file an issue for rewrite if it needs to be rewritten,let either keep batch training example new section many model training file issue rewrite need,issue,negative,positive,positive,positive,positive,positive
1749397887,"> cc @c21 for review and merge. EDIT: i see there are some unit test failures. @Zandew to re-assign to c21 once PR is ready.

This was failing for a legacy dataset config test, which is now deprecated.",review merge edit see unit test ready failing legacy test,issue,negative,positive,positive,positive,positive,positive
1749382457,"This is a P2 on our side, but @chappidim if you are able to work on this we are happy to help.",side able work happy help,issue,positive,positive,positive,positive,positive,positive
1749368416,"> Many people did benchmark with ray.init() cluster and if we don't block the driver until workers are started, the initial task scheduling performance will be very slow.

So if it's a benchmark usecase, shouldn't it be a non-default behaviour? I mean there are many configs that could be tuned differently for perf, but the default behaviour should be the more stable and generic one? ",many people cluster block driver initial task performance slow behaviour mean many could tuned differently default behaviour stable generic one,issue,negative,positive,neutral,neutral,positive,positive
1749355138,"Thanks.

1. Can we create issues to track the rewrites?
2. For the batch training example, if we are no longer positioning Ray Data as a solution for Many Model Training, then we should update the corresponding documentation: https://docs.ray.io/en/latest/ray-overview/use-cases.html#many-model-training
3. I think we should keep 1 tabular data + xgboost example",thanks create track batch training example longer ray data solution many model training update corresponding documentation think keep tabular data example,issue,positive,positive,positive,positive,positive,positive
1749337815,"Hi @birgerbr, sorry for the delay. Do you know if the problem persists with Ray 2.7 and kuberay v1.0.0-rc.0?",hi sorry delay know problem ray,issue,negative,negative,negative,negative,negative,negative
1749316385,BTW we are planning to add `trn1.2xlarge` instances to our CI pipeline next week to be able to test this cc @can-anyscale  ,add pipeline next week able test,issue,negative,positive,positive,positive,positive,positive
1749297400,"Got it, I think we should update the documentation in 2.8",got think update documentation,issue,negative,neutral,neutral,neutral,neutral,neutral
1749294326,"lol, i can't get any lucks... let me do rebase again.",ca get let rebase,issue,negative,neutral,neutral,neutral,neutral,neutral
1749291796,The test failures are unrelated. @edoakes this change is ready to merge.,test unrelated change ready merge,issue,negative,positive,positive,positive,positive,positive
1749286156,@aslonnie i'm not exactly sure but pip insists for the latest version of typepy in its resolver,exactly sure pip latest version resolver,issue,negative,positive,positive,positive,positive,positive
1748806057,"Btw, I think I discussed with @alexeykudinkin last time, and I guess the issue is the exception is raised while initializing a core worker. Maybe we can delay rasing an exception or we should run a startup hook after core worker is fully intiialized",think last time guess issue exception raised core worker maybe delay exception run hook core worker fully,issue,negative,neutral,neutral,neutral,neutral,neutral
1748799769,"> Also, I wonder if prestarting all <num_cpu> workers are really the best default behavior here when driver connects? The memory overhead of running an idle process could be significant (70MiB?)

The original motivation is for stable performance. Many people did benchmark with ray.init() cluster and if we don't block the driver until workers are started, the initial task scheduling performance will be very slow.

For this particular issue, it looks like the problem is the workers are not started in 60 seconds? I think this is an abnormal behavior (60 seconds is pretty long to just launch a worker). I think although the driver starts successfully, if the worker startup is this slow, it is probably not functioning as expected.

+1 in improving the error messages here (or we can even raise an exception?)

",also wonder really best default behavior driver memory overhead running idle process could significant mib original motivation stable performance many people cluster block driver initial task performance slow particular issue like problem think abnormal behavior pretty long launch worker think although driver successfully worker slow probably improving error even raise exception,issue,positive,positive,positive,positive,positive,positive
1748755103,"> > Did you solve the problem? I get the same error.
> 
> No, I can not find the solution, maybe you want to try the new RLModule API since ModelV2 API will be superseded.
> 
> BTW, I tried many other rl libs, but they all have some problems with custom enviroment and policy.

Thanks for your reply! Hope it will be fixed soon.",solve problem get error find solution maybe want try new since tried many custom policy thanks reply hope fixed soon,issue,positive,positive,positive,positive,positive,positive
1748709525,"ACtually. I don't see any perf changes, and I just realized this change doesn't address https://github.com/ray-project/ray/issues/15656? Doesn't this mean now we are using 1 thread instead of parallel memcpy actually? (what's the behavior of std::async?)",actually see change address mean thread instead parallel actually behavior,issue,negative,negative,neutral,neutral,negative,negative
1748670279,I think this would be solved by using gpustat everywhere ray wants to find gpu resources. See PR #35581.,think would everywhere ray find see,issue,negative,neutral,neutral,neutral,neutral,neutral
1748352989,"Suppose I have 20 big size files and I implement a specfic datasource for it.   I would like to load the datset by read_datasource with a parallesim ,say , 200.  Now I see the splitblocks function to split each block to many smaller blocks. My question is ,  how does the splitblocks work?    It will split a single big file in each row into many many binary parts and then to coalesce them somewhere in the downstream?",suppose big size implement would like load say see function split block many smaller question work split single big file row many many binary coalesce somewhere downstream,issue,negative,positive,positive,positive,positive,positive
1748288398,"> Did you solve the problem? I get the same error.

No, I can not find the solution, maybe you want to try the new RLModule API since ModelV2 API will be superseded.

BTW, I tried many other rl libs, but they all have some problems with custom enviroment and policy.",solve problem get error find solution maybe want try new since tried many custom policy,issue,negative,positive,positive,positive,positive,positive
1748226877,"Here is output with both pending and running pg tasks:

```
======== Autoscaler status: 2023-10-05 07:13:09.740863 ========
GCS request time: 0.000660s

Node status
---------------------------------------------------------------
Active:
 1 node_7dedc912c22c07df0fd49dc5b6e8450472310ad4041ddd2540e519dc
Idle:
 (no idle nodes)
Pending:
 (no pending nodes)
Recent failures:
 (no failures)

Resources
---------------------------------------------------------------
Total Usage:
 4.0/36.0 CPU (4.0 used of 8.0 reserved in placement groups)
 0B/35.23GiB memory
 0B/17.61GiB object_store_memory

Total Demands:
 {'CPU': 1.0}: 6+ pending tasks/actors (6+ using placement groups)

Node: 7dedc912c22c07df0fd49dc5b6e8450472310ad4041ddd2540e519dc
 Usage:
  4.0/36.0 CPU (4.0 used of 8.0 reserved in placement groups)
  0B/35.23GiB memory
  0B/17.61GiB object_store_memory
 Activity:
  Resource: CPU currently in use.
  Busy workers on node.
  Resource: CPU_group_0_38c7049e01961594ddbd48ae247806000000 currently in use.
  Resource: CPU_group_38c7049e01961594ddbd48ae247806000000 currently in use.
```

I think the only potential change needed is in node activity and that can be a follow up as it is not super noisy.",output pending running status request time node status active idle idle pending pending recent total usage used reserved placement memory total pending placement node usage used reserved placement memory activity resource currently use busy node resource currently use resource currently use think potential change node activity follow super noisy,issue,positive,positive,neutral,neutral,positive,positive
1748103804,"Maybe one way we could support this is by translating gpu_memory into GPU requests of a specific accelerator type label(s) under the hood (i.e., it's syntactic sugar for manually specifying accelerator types). That way we wouldn't have to make changes to the scheduler internals.",maybe one way could support specific accelerator type label hood syntactic sugar manually accelerator way would make internals,issue,negative,neutral,neutral,neutral,neutral,neutral
1748068491,"Hah, okay, here is the reason this bug was not revealed earlier. It's due to a subtle hidden side effect in a nested constructor.

The normal actor creation workflow is like this:
1. Create task spec based on `required_resources` and `required_placement_resources` specified in `actor.py`
2. Eventually core worker sends the `rpc::TaskSpec` to GCS to create and schedule the actor.
3. GCS sends the `rpc::TaskSpec` back to some node to `RequestWorkerLease`.
4. In `NodeManager::HandleRequestWorkerLease`, we create a `ray::Task(rpc::TaskSpec)`. This is the sneaky part! 

What looks like a straightforward cast/copy actually does a hidden `ray::TaskSpecification(rpc::TaskSpec)` (layered inside the `ray::Task(rpc::TaskSpec)`!), which will call `ComputeResources()` in the constructor, which has this line:

```
  auto &required_placement_resources = message_->required_placement_resources().empty()
                                           ? required_resources
                                           : message_->required_placement_resources();
```

and that is how placement_resources magically appears from nothing when originally it was empty.

The implication, of course, is that even when `num_cpus=0`, if `placement_resources` is empty, then we will fix this invariant halfway through the plumbing. However, since https://github.com/ray-project/ray/pull/39946 changed `placement_resources` to be nonempty during the GCS portion of the actor scheduling, it also revealed this issue.",hah reason bug revealed due subtle hidden side effect constructor normal actor creation like create task spec based eventually core worker create schedule actor back node create ray sneaky part like straightforward actually hidden ray layered inside ray call constructor line auto magically nothing originally empty implication course even empty fix invariant halfway plumbing however since portion actor also revealed issue,issue,positive,positive,neutral,neutral,positive,positive
1748001489,"Hi team, i can confirm that this issue still persists. I'm running on version 2.7.0, and my use case is to use vLLM with distributed inference. By default vLLM will use Ray as backend, and if I got GPU OOM etc. and the program exit unexpectedly, the Ray processed can't be killed and will become zombie process. 
![image](https://github.com/ray-project/ray/assets/18288209/4543b9e9-f842-4ed9-9e81-8214ee7a7ff4)
",hi team confirm issue still running version use case use distributed inference default use ray got program exit unexpectedly ray ca become zombie process image,issue,negative,positive,neutral,neutral,positive,positive
1747968129,"This is the ""infinite loop"" (manifests as failure to schedule, meaning `ray.get()` will hang):
1. NodeManager periodically runs `ClusterTaskManager::ScheduleAndDispatchTasks`. 
2. Try to schedule, using PlacementResources to get feasible node, which is local node.
3. Local node tries to schedule, but checks feasibility one more time by trying to allocate with RequiredResources. This fails if RequiredResources is bigger than PlacementResources!
4. Task is placed back in waiting queue. 
5. The next time we schedule, the same thing can happen.

In terms of codepath:
1. `ClusterTaskManager::ScheduleAndDispatchTasks` calls `ClusterResourceScheduler::GetBestSchedulableNode`, which determines an appropriate node using `RequiredPlacementResources`, say Node A.
5. Next step is `ClusterResourceScheduler::ScheduleOnNode` which hands off to `LocalTaskManager::QueueAndScheduleTask` when not spilling back (assume we are on Node A). `LocalTaskManager::QueueAndScheduleTask` calls `LocalTaskManager::ScheduleAndDispatchTasks` calls `LocalTaskManager::DispatchScheduledTasksToWorkers`, which checks that the task is still feasible to schedule by calling `LocalTaskManager::AllocateLocalTaskResources`.
6. Task feasibility here is determined by `RequiredResources`, and not `RequiredPlacementResources`. If `RequiredResources` is not a subset of `RequiredPlacementResources`, it is possible for the task to be considered infeasible at this step. So, it is not scheduled!
7. We `TrySpillback`, which calls `ClusterResourceScheduler::GetBestSchedulableNode`, which returns Node A (current node), so we don't spill back. Instead, we set the task state to WAITING.
8. Rinse and repeat.",infinite loop failure schedule meaning periodically try schedule get feasible node local node local node schedule feasibility one time trying allocate bigger task back waiting queue next time schedule thing happen appropriate node say node next step back assume node task still feasible schedule calling task feasibility determined subset possible task considered infeasible step node current node spill back instead set task state waiting rinse repeat,issue,negative,positive,neutral,neutral,positive,positive
1747965862,"Also, I wonder if prestarting all <num_cpu> workers are really the best default behavior here when driver connects? The memory overhead of running an idle process could be significant (70MiB?) ",also wonder really best default behavior driver memory overhead running idle process could significant mib,issue,positive,positive,positive,positive,positive,positive
1747939171,Did you solve the problem? I get the same error.,solve problem get error,issue,negative,neutral,neutral,neutral,neutral,neutral
1747890385,I think Ray successfully released all object data in the object store; but then Ray did not release the mmap'd files. Further tracking down the release code...,think ray successfully object data object store ray release release code,issue,negative,positive,positive,positive,positive,positive
1747860260,"> @jjyao is p0 right for #19631 ? Asking because this dupe was marked as p0 and in scope for ray28

@anyscalesam  Yes, current plan is still for P0 ray 2.8
",right dupe marked scope ray yes current plan still ray,issue,negative,positive,positive,positive,positive,positive
1747858406,@jjyao is p0 right for #19631 ? Asking because this dupe was marked as p0 and in scope for ray28,right dupe marked scope ray,issue,negative,positive,positive,positive,positive,positive
1747856652,@scv119 have you had a chance to take a look at this yet?,chance take look yet,issue,negative,neutral,neutral,neutral,neutral,neutral
1747844836,"I also would like to be able to schedule on graphics memory as well. I think it provides a better utilization strategy for the Ray users rather than an admin segmenting off certain gpus for one task vs another. My team sees all types of configurations, some very advanced GPU rigs and some simple. GPU memory seems to be the most logical method for requesting / allocating resources.",also would like able schedule graphic memory well think better utilization strategy ray rather certain one task another team advanced simple memory logical method,issue,positive,positive,positive,positive,positive,positive
1747837175,@woshiyyya which ray release do we want to target this doc update for?,ray release want target doc update,issue,negative,neutral,neutral,neutral,neutral,neutral
1747772849,"Looks like this started in happening in https://github.com/ray-project/ray/commit/befad81464d6f9ebe3be03c1d9c0313466c9d000. 

Seems like the placeholder values are not getting resolved here, will keep investigating.",like happening like getting resolved keep investigating,issue,positive,neutral,neutral,neutral,neutral,neutral
1747738534,"@kouroshHakha , @ArturNiederfahrenhorst probably need a stamp from either of you for the change to rllib/BUILD bazel files. Thankkks",probably need stamp either change,issue,negative,neutral,neutral,neutral,neutral,neutral
1747676383,"Thanks @matthewdeng, that seemed to do the trick. 

Will you leave this issue open until the new progress reporter also supports customization, so that an update can be added here when this is added?",thanks trick leave issue open new progress reporter also update added added,issue,positive,positive,positive,positive,positive,positive
1747676372,"> I assume the current issue is that the callback added with add_done_callback is running prior to the coroutine actually completing. If that's the case, then how about we move event.Notify() into a finally block in the async_func just above?

@edoakes this fix makes sense",assume current issue added running prior actually case move finally block fix sense,issue,negative,neutral,neutral,neutral,neutral,neutral
1747674328,This issue is unable to transfer to the product repo. Created new issue there https://github.com/anyscale/product/issues/24133,issue unable transfer product new issue,issue,negative,negative,negative,negative,negative,negative
1747640821,"@rkooo567 I just verified that my suggested change fixes the test case you've added. Without the following diff, I see the segfaults. With the following diff, the test passes consistently (without any of your other changes):
```
diff --git a/python/ray/_raylet.pyx b/python/ray/_raylet.pyx
index af1e777680..5449563478 100644
--- a/python/ray/_raylet.pyx
+++ b/python/ray/_raylet.pyx
@@ -4280,22 +4280,24 @@ cdef class CoreWorker:
             function_descriptor, specified_cgname)
 
         async def async_func():
-            if task_id:
-                async_task_id.set(task_id)
+            try:
+                if task_id:
+                    async_task_id.set(task_id)
 
-            if inspect.isawaitable(func_or_coro):
-                coroutine = func_or_coro
-            else:
-                coroutine = func_or_coro(*args, **kwargs)
+                if inspect.isawaitable(func_or_coro):
+                    coroutine = func_or_coro
+                else:
+                    coroutine = func_or_coro(*args, **kwargs)
 
-            return await coroutine
+                return await coroutine
+            finally:
+                event.Notify()
 
         future = asyncio.run_coroutine_threadsafe(async_func(), eventloop)
         if task_id:
             with self._task_id_to_future_lock:
                 self._task_id_to_future[task_id] = future
 
-        future.add_done_callback(lambda _: event.Notify())
         with nogil:
             (CCoreWorkerProcess.GetCoreWorker()
                 .YieldCurrentFiber(event))
```",change test case added without following see following test consistently without git index class try else else return await return await finally future future lambda event,issue,negative,positive,neutral,neutral,positive,positive
1747598551,"Oh my bad, I should've made this a draft PR. I'm not planning to merge this. It was to share code with @edoakes.",oh bad made draft merge share code,issue,negative,negative,negative,negative,negative,negative
1747598232,@sihanwang41 we can simply replicate what we have in Product right now,simply replicate product right,issue,negative,positive,positive,positive,positive,positive
1747597240,@sihanwang41 can't merge due to premerge failing,ca merge due failing,issue,negative,negative,negative,negative,negative,negative
1747595815,"> can you also include this small fix in this PR

@alanwguo thanks for the find, I added the change.",also include small fix thanks find added change,issue,negative,negative,neutral,neutral,negative,negative
1747557389,Thanks @zhe-thoughts and @GeneDer ! DCO has been fixed. Ready to merge into 2.7.1.,thanks fixed ready merge,issue,positive,positive,positive,positive,positive,positive
1747554600,"Should we close this issue in lieu of a previous, more rich/commented issue: https://github.com/ray-project/ray/issues/19631 ?",close issue lieu previous issue,issue,negative,negative,negative,negative,negative,negative
1747542354,"> @alanwguo to verify that the dashboard works with this change.

verified the fix works.

@shrekris-anyscale , can you also include this small fix in this PR: https://github.com/alanwguo/ray/commit/d697adde2437ca6c3dcbdfc6d2c8ca25dfa713ff

seems like newer versions of prometheus have changed the health check message",verify dashboard work change fix work also include small fix like health check message,issue,negative,negative,negative,negative,negative,negative
1747535626,"Has there been any update on this/is there any way to help here? Use case would be profiling distributed training

Or any workarounds to profile without using RAY_ADDRESS=local?",update way help use case would distributed training profile without,issue,negative,neutral,neutral,neutral,neutral,neutral
1747498291,@alanwguo to verify that the dashboard works with this change.,verify dashboard work change,issue,negative,neutral,neutral,neutral,neutral,neutral
1747488871,"> let me check why the premerge fails..

passing now.

python in the buildkite ami does not support typings.",let check passing python ami support,issue,negative,neutral,neutral,neutral,neutral,neutral
1747476077,"Hey @bwohlberg I just took a look, this seems to be because there was a rollout of a new progress reporter.

If you reduce the logging level (in your script it's set to ERROR), you can see the following: 
```
2023-10-04 11:19:47,962 INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2023-10-04 11:19:49,540 WARNING tune.py:1000 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
```

To use your custom progress reporter right now, you can set the `RAY_AIR_NEW_OUTPUT=0`. In a future release, we'll look into making this configurable without setting this flag.",hey took look new progress reporter reduce logging level script set error see following output use new output engine verbosity disable new output use legacy output engine set environment variable information please see warning set use custom progress reporter right set future release look making without setting flag,issue,negative,positive,positive,positive,positive,positive
1747413677,"This is no longer the case since indexing is now centralized by ""checkpoint index"" rather than iteration. The new feature request is to make this customizable.",longer case since indexing index rather iteration new feature request make,issue,negative,positive,positive,positive,positive,positive
1747405723,"@can-anyscale Looks like this is passing?

<img width=""689"" alt=""Screen Shot 2023-10-04 at 11 13 19 AM"" src=""https://github.com/ray-project/ray/assets/3887863/2c146b45-5c51-4cf5-896f-5365fb1c3cc8"">
",like passing screen shot,issue,negative,neutral,neutral,neutral,neutral,neutral
1747405267,"> premerge failed, also does tags need to be an array?

it supports an array, but also supports a single string.",also need array array also single string,issue,negative,negative,neutral,neutral,negative,negative
1747384165,"@rynewang can you make a full list of features from the current server that aren't supported? I am wondering in particular about things like reference counting and starting separate job ids/runtime_envs per client.

If we were to transparently swap users to the new server, could things continue to work as long as they didn't use libraries directly / stuck to the limited Ray API? If we could guarantee a high degree of backwards compatibility this makes the rollout much more feasible.",make full list current server wondering particular like reference counting starting separate job per client transparently swap new server could continue work long use directly stuck limited ray could guarantee high degree backwards compatibility much feasible,issue,negative,positive,neutral,neutral,positive,positive
1747337633,@matthewdeng can you please set priority and advise on timeline?,please set priority advise,issue,negative,neutral,neutral,neutral,neutral,neutral
1747233250,"@rkooo567 has the most context and reviewed the original PR, but I assigned some extra folks to ensure it gets approved--sorry for the spam",context original assigned extra ensure sorry,issue,positive,negative,neutral,neutral,negative,negative
1747221894,"Windows tune tests unrelated
RLlib tests unrelated
Dashboard test ""serve_dashboard"" unrelated

",tune unrelated unrelated dashboard test unrelated,issue,negative,neutral,neutral,neutral,neutral,neutral
1747143183,"> actually there was a suggestion for it. @Yicheng-Lu-llll if u could take it ovet and finish it, that'll be great!

Sure! Will follow the suggestion here: https://github.com/ray-project/ray/pull/27272#issuecomment-1201993905.",actually suggestion could take finish great sure follow suggestion,issue,positive,positive,positive,positive,positive,positive
1747078651,Also encountering compatibility challenges that necessitate remaining on Pydantic v1 to ensure functionality with Ray Serve. Means i have to re-write a lot of Pydantic v2 code,also compatibility necessitate ensure functionality ray serve lot code,issue,negative,neutral,neutral,neutral,neutral,neutral
1747030290,"After a bit more work; 

1. It seems like the same error happens only when hardware cpu_counts == 0 (which should never happen?), not < 1
2. I couldn't reproduce the agent exiting (it just keeps failing at that point).

I will make a PR to handle cpu counts == 0 handled more gracefully, but maybe not additional work will be done here. 
",bit work like error hardware never happen could reproduce agent failing point make handle handled gracefully maybe additional work done,issue,negative,neutral,neutral,neutral,neutral,neutral
1747017460,Hmm this seems irrelevant in 2.7 now. I couldn't start ray using `ray start --num_cpus=0.5` or  `ray.init(num_cpus=0.5)`,irrelevant could start ray ray start,issue,negative,negative,negative,negative,negative,negative
1746951253,"A couple comments!

- What's the current gap to support popular libraries like ray train or ray data? I can see generator is also not supported from the current impl. 
- `ObjectRefGenerator ` will be deprecated lol... the newer implementation may be harder to support
- We sholud probably handle large data for put/get (either disallow it or stream it)
- reconnection <- are we sending failed requests after reconnecting? Are existing APIs idempotent for this? 

> Now the HTTP server is a dashboard head module. This removes the need to occupy a port per client session; Also it makes the URL deterministic (since the dashboard head HTTP address is known).

I think we should reconsider this part (maybe okay for the prototype). Dashboard is actually pretty slow at scale already and we will probably add new functionalities to it, which could slow down the response time for client even more. Given we are planning to go multi tenancy route, this also could become bottlenecks. 

> There's none. Once a ObjectRef is seen by the ClientSupervisor it's kept forever. Later we can work on it.

Original client solves this using bidi stream IIRC. But we can probably simply send a request from `__del__` maybe? 

> Note: the dashboard module can only access to detached actors due to Ray isolation set ups. This is kind of a loss because we can no longer track a client's whole usage in the dashboard UI as a single driver, but I did not find a better way.

what about we just never exits the driver? and from the client side, we can give a URL to the driver job page



",couple current gap support popular like ray train ray data see generator also current implementation may harder support probably handle large data either disallow stream reconnection sending idempotent server dashboard head module need occupy port per client session also deterministic since dashboard head address known think reconsider part maybe prototype dashboard actually pretty slow scale already probably add new could slow response time client even given go tenancy route also could become none seen kept forever later work original client stream probably simply send request maybe note dashboard module access detached due ray isolation set kind loss longer track client whole usage dashboard single driver find better way never driver client side give driver job page,issue,positive,positive,positive,positive,positive,positive
1746908631,Sorry had to focus on https://github.com/ray-project/ray/pull/40083 today. ETA is tomorrow ,sorry focus today eta tomorrow,issue,negative,negative,negative,negative,negative,negative
1746789254,"@rynewang I feel like it is blocked too long. If the other PR is not merged by tomorrow, I will probably just merge it first (so that we can run chaos tests asap). After we merge your PR, I can work further to use the same pattern",feel like blocked long tomorrow probably merge first run chaos merge work use pattern,issue,negative,positive,neutral,neutral,positive,positive
1746787368,Seems like there's another merge conflict...,like another merge conflict,issue,negative,neutral,neutral,neutral,neutral,neutral
1746613517,"@yiwei00000 these preprocessors are not well written

this is the method from preprocessor class, you can see while using ds.map_batch, they are not passing num_cpus properly, you can check their code, thats why these things are using only single core by default

```
def _transform(
        self, ds: Union[""Dataset"", ""DatasetPipeline""]
    ) -> Union[""Dataset"", ""DatasetPipeline""]:
        # TODO(matt): Expose `batch_size` or similar configurability.
        # The default may be too small for some datasets and too large for others.
        transform_type = self._determine_transform_to_use()

        # Our user-facing batch format should only be pandas or NumPy, other
        # formats {arrow, simple} are internal.
        kwargs = self._get_transform_config()
        if transform_type == BatchFormat.PANDAS:
            return ds.map_batches(
                self._transform_pandas, batch_format=BatchFormat.PANDAS, **kwargs
            )
        elif transform_type == BatchFormat.NUMPY:
            return ds.map_batches(
                self._transform_numpy, batch_format=BatchFormat.NUMPY, **kwargs
            )
        else:
            raise ValueError(
                ""Invalid transform type returned from _determine_transform_to_use; ""
                f'""pandas"" and ""numpy"" allowed, but got: {transform_type}'
            )

```



i was facing same issue with Categorizer, my current workaround is that, i just overrided few methods and it worked



```

class MyCategorizer(Preprocessor):

    def __init__(self, columns: List[str], dtypes: Optional[Dict[str, pd.CategoricalDtype]] = None, ):
        if not dtypes:
            dtypes = {}

        self.columns = columns
        self.dtypes = dtypes

    def _fit(self, dataset: Dataset) -> Preprocessor:
        columns_to_get = [column for column in self.columns if column not in self.dtypes]
        if columns_to_get:
            unique_indices = _get_unique_value_indices(dataset, columns_to_get, drop_na_values=True, key_format=""{0}"")
            unique_indices = {
                column: pd.CategoricalDtype(values_indices.keys())
                for column, values_indices in unique_indices.items()
            }
        else:
            unique_indices = {}
        unique_indices = {**self.dtypes, **unique_indices}
        self.stats_: Dict[str, pd.CategoricalDtype] = unique_indices
        return self

    def _transform(self, ds: Union[""Dataset"", ""DatasetPipeline""]) -> Union[""Dataset"", ""DatasetPipeline""]:
        # TODO(matt): Expose `batch_size` or similar configurability.
        # The default may be too small for some datasets and too large for others.
        transform_type = self._determine_transform_to_use()

        # Our user-facing batch format should only be pandas or NumPy, other
        # formats {arrow, simple} are internal.
        kwargs = self._get_transform_config()
        if transform_type == BatchFormat.PANDAS:
            return ds.map_batches(
                self._transform_pandas,
                batch_format=BatchFormat.PANDAS,
                batch_size=batch_size,
                num_cpus=4,
                # num_gpus=1,
                **kwargs
            )
        elif transform_type == BatchFormat.NUMPY:
            return ds.map_batches(
                self._transform_numpy,
                batch_format=BatchFormat.NUMPY,
                batch_size=batch_size,
                num_cpus=4,
                **kwargs
            )
        else:
            raise ValueError(
                ""Invalid transform type returned from _determine_transform_to_use; ""f'""pandas"" and ""numpy"" allowed, but got: {transform_type}')

    def _transform_pandas(self, df: pd.DataFrame):
        df = df.astype(self.stats_)
        return df

    def __repr__(self):
        return (f""{self.__class__.__name__}(columns={self.columns!r}, ""f""dtypes={self.dtypes!r})"")
```

Now, i can use **MyCategorizer** instead of **Categorizer** class, you can also do something like this. 
",well written method class see passing properly check code thats single core default self union union expose similar default may small large batch format arrow simple internal return return else raise invalid transform type returned got facing issue current worked class self list optional none self column column column column column else return self self union union expose similar default may small large batch format arrow simple internal return return else raise invalid transform type returned got self return self return use instead class also something like,issue,negative,negative,neutral,neutral,negative,negative
1746349143,"@bveeramani 

I got replied from our warehouse engineer about rate limit:

For concurrent request (concurrent fetching chunk data) limiters of EXTERNAL_LINKS type (the type used in my PR), it's around 750 requests per second per workspace.


So I think we rarely hit the rate limit , because one chunk data is at least 10MB size and each request takes some time to complete, if hit rate limit, we can just let ray task retry.",got warehouse engineer rate limit concurrent request concurrent fetching chunk data type type used around per second per think rarely hit rate limit one chunk data least size request time complete hit rate limit let ray task retry,issue,negative,positive,neutral,neutral,positive,positive
1746218271,"> > Cluster launcher still doesn't work (workers uninitialized).
> 
> Does it raise the same error?

No, it starts the head node, but other nodes are [left uninitialized](https://github.com/ray-project/ray/issues/39565#issuecomment-1743780290).",cluster launcher still work raise error head node left,issue,negative,neutral,neutral,neutral,neutral,neutral
1746166087,"> LGTM. one last question. Do you know why the job page says the init is from dashboard.py?

oh it's because it would query GCS for cluster status (the ray status info), and when it initializes a GcsClient, it was trying to get the gcs address from runtime context, which would auto init a worker. ",one last question know job page oh would query cluster status ray status trying get address context would auto worker,issue,negative,neutral,neutral,neutral,neutral,neutral
1746145829,"> As a follow-up, I think we can consolidate the static cluster and autoscaling cluster code by always using the autoscaling code path? A static cluster is just an autoscaling cluster with the same min_workers and max_workers.

This makes sense.",think consolidate static cluster cluster code always code path static cluster cluster sense,issue,negative,positive,positive,positive,positive,positive
1746137101,"https://github.com/ray-project/ray/pull/27272

actually there was a suggestion for it. @Yicheng-Lu-llll if u could take it ovet and finish it, that'll be great!",actually suggestion could take finish great,issue,positive,positive,positive,positive,positive,positive
1746048839,LGTM. one last question. Do you know why the job page says the init is from dashboard.py? ,one last question know job page,issue,negative,neutral,neutral,neutral,neutral,neutral
1746020261,"Seems like a regression, was able to narrow it down between Ray 2.3 and 2.4.

Minimal repro:
```python
from ray import tune, train, air
import random


def trainable(config):
    air.session.report({""metric"": config[""x""] + config[""y""]})


tuner = tune.Tuner(
    trainable,
    param_space={
        ""x"": tune.sample_from(lambda _: random.random()),
        ""y"": tune.grid_search([1, 2, 3]),
    },
)

tuner.fit()
```",like regression able narrow ray minimal python ray import tune train air import random trainable metric tuner trainable lambda,issue,negative,negative,neutral,neutral,negative,negative
1745978462,"@rakataprime  I tried this with a simpler script (the `MosaicTrainer` is being deprecated).

```python
from ray.train.torch import TorchTrainer
from ray.train import ScalingConfig

def train_func():
    from composer.trainer import Trainer

trainer = TorchTrainer(train_func, scaling_config=ScalingConfig(num_workers=2))
trainer.fit() 
```

This fails with this:

```
ray.exceptions.RayTaskError(ValueError): ray::_RayTrainWorker__execute.get_next() (pid=9424, ip=10.0.0.180, actor_id=df0d155b0e8bb7c3d1ad00e707000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7efc18733be0>)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/_internal/worker_group.py"", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File ""/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/_internal/utils.py"", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
  File ""/home/ray/default/sigint_test.py"", line 8, in f
    from composer.trainer import Trainer
  File ""/mnt/cluster_storage/pypi/lib/python3.9/site-packages/composer/__init__.py"", line 7, in <module>
    from composer.core import Algorithm, Callback, DataSpec, Engine, Evaluator, Event, State, Time, Timestamp, TimeUnit
  File ""/mnt/cluster_storage/pypi/lib/python3.9/site-packages/composer/core/__init__.py"", line 14, in <module>
    from composer.core.engine import Engine, Trace
  File ""/mnt/cluster_storage/pypi/lib/python3.9/site-packages/composer/core/engine.py"", line 126, in <module>
    signal.signal(signal.SIGTERM, sigterm_handler)
  File ""/home/ray/anaconda3/lib/python3.9/signal.py"", line 56, in signal
    handler = _signal.signal(_enum_to_int(signalnum), _enum_to_int(handler))
ValueError: signal only works in main thread of the main interpreter
```

This is happening because the `train_func` is being executed on a separate thread. What you can do here is disable the signal, but I'm not entirely sure if there are side effects from doing this:

```python
from ray.train.torch import TorchTrainer
from ray.train import ScalingConfig

def train_func():
    import signal
    signal.signal = lambda *args, **kwargs: None
    from composer.trainer import Trainer

trainer = TorchTrainer(train_func, scaling_config=ScalingConfig(num_workers=2))
trainer.fit() 
``` ",tried simpler script python import import import trainer trainer ray object file line raise file line file line import trainer file line module import algorithm engine event state time file line module import engine trace file line module file line signal handler handler signal work main thread main interpreter happening executed separate thread disable signal entirely sure side effect python import import import signal lambda none import trainer trainer,issue,negative,positive,positive,positive,positive,positive
1745922163,"Got it, @raulchen @ericl updated the PR to match discussion from above, ready for another look.",got match discussion ready another look,issue,negative,positive,positive,positive,positive,positive
1745907948,"> Cluster launcher still doesn't work (workers uninitialized).

Does it raise the same error? ",cluster launcher still work raise error,issue,negative,neutral,neutral,neutral,neutral,neutral
1745906698,Hmm yeah it is pretty odd. Let's just merge this (since it will give a better error message for us upon failure) and not close the issue until the person replies. And we downgrade the priority (until the person replies)?,yeah pretty odd let merge since give better error message u upon failure close issue person downgrade priority person,issue,negative,positive,neutral,neutral,positive,positive
1745879750,"> @rickyyx @chaowanggg can you guys create an issue to root cause it? @rickyyx is it unexpected the cluster status returns an empty string?

Link the issue here https://github.com/ray-project/ray/issues/40076",create issue root cause unexpected cluster status empty string link issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1745872839,@rickyyx @chaowanggg can you guys create an issue to root cause it? @rickyyx is it unexpected the cluster status returns an empty string? ,create issue root cause unexpected cluster status empty string,issue,negative,neutral,neutral,neutral,neutral,neutral
1745858067,"We should root cause it, but we need to mitigate the issue where the entire UI crashes, preferably for ray 2.7.1",root cause need mitigate issue entire preferably ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1745838715,@sihanwang41 the output basically is that in this PR you don't see core or serve tests running because I didn't change any of them: https://buildkite.com/ray-project/premerge/builds/7311,output basically see core serve running change,issue,negative,neutral,neutral,neutral,neutral,neutral
1745827052,"you rock!
what does the output look like?  (Do you have a screenshot)",rock output look like,issue,negative,neutral,neutral,neutral,neutral,neutral
1745822910,"> Fix https://github.com/ray-project/ray/issues/39564, for an unknown reason, the backend router 'cluster_status' may return empty data without 'cluster_status'.

Can we actually root cause this? I am a bit confused if the empty data was expected in the first place. If not so, maybe the right fix includes figuring out why it was empty?",fix unknown reason router may return empty data without actually root cause bit confused empty data first place maybe right fix empty,issue,negative,negative,neutral,neutral,negative,negative
1745811801,Updated the doc and use set operations to make the logic easier to understand. I kept the boolean to keep things simple for now.,doc use set make logic easier understand kept keep simple,issue,negative,neutral,neutral,neutral,neutral,neutral
1745804091,"> > > We have 6 components on the Overview page, and it appears that any error could lead to the crash of the Overview Page. Should we consider adding a React Error Boundary to protect the Overview Page? @alanwguo
> > 
> > 
> > yes, can we add error boundaries?
> 
> maybe we should do that as a follow-up for post 2.7.1

Sounds good, just filed an issue

https://github.com/ray-project/ray/issues/40070",overview page error could lead crash overview page consider react error boundary protect overview page yes add error maybe post good issue,issue,negative,positive,positive,positive,positive,positive
1745767098,"> Hi @zcin, can you help to clarify when to use ""Config"", ""Info"" and ""Schema"" ? When i see `ReplicaConfig` into `ReplicaInfo`, i am a little lost.

(Pending the [conversation](https://github.com/ray-project/ray/pull/40005#discussion_r1344658108) above)
- `ReplicaInitInfo` should only be from user's application code, i.e. (deployment def, init args, init kwargs)
- Deployment config is Serve's internal object used to pass around info about a deployment
- Schema is user-facing, basically only used to define request/response format of REST apis.
",hi help clarify use schema see little lost pending conversation user application code deployment deployment serve internal object used pas around deployment schema basically used define format rest,issue,negative,negative,neutral,neutral,negative,negative
1745722215,"> > We have 6 components on the Overview page, and it appears that any error could lead to the crash of the Overview Page. Should we consider adding a React Error Boundary to protect the Overview Page? @alanwguo
> 
> yes, can we add error boundaries?

maybe we should do that as a follow-up for post 2.7.1",overview page error could lead crash overview page consider react error boundary protect overview page yes add error maybe post,issue,negative,neutral,neutral,neutral,neutral,neutral
1745721401,"> We have 6 components on the Overview page, and it appears that any error could lead to the crash of the Overview Page. Should we consider adding a React Error Boundary to protect the Overview Page? @alanwguo

yes, can we add error boundaries?",overview page error could lead crash overview page consider react error boundary protect overview page yes add error,issue,negative,neutral,neutral,neutral,neutral,neutral
1745716332,"Hi @wuisawesome, I've noticed that this issue has been open for some time and it hasn't been resolved yet. Can I take this on?

",hi issue open time resolved yet take,issue,negative,neutral,neutral,neutral,neutral,neutral
1745710288,"We have 6 components on the Overview page, and it appears that any error could lead to the crash of the Overview Page. Should we consider adding a React Error Boundary to protect the Overview Page? @alanwguo ",overview page error could lead crash overview page consider react error boundary protect overview page,issue,negative,neutral,neutral,neutral,neutral,neutral
1745692618,"Hi  @jjyao, After examining the code more thoroughly, I realized I might have been mistaken in my previous approach. 

Upon further investigation, I believe the correct logic is as follows::
1. Utilize `services.find_gcs_addresses()` to retrieve all GCS addresses on the **local physical node**.
2. when executing `ray start --address='172.31.63.128:6379'`, If `ray_params.gcs_address`(the GCS address the worker tries to connect to) is in `services.find_gcs_addresses()`, it indicates that a worker is trying to initiate on the head node.

Here's the segment of code I'm considering to integrate, with the modifications beginning [here](https://sourcegraph.com/github.com/ray-project/ray@master/-/blob/python/ray/scripts/scripts.py?L919):

```python
if ray_params.gcs_address in services.find_gcs_addresses():
    cli_logger.print(""WARNING: Attempting to start a worker node on the physical head node."")
    cli_logger.newline()
```

Here are the execution results:

```shell
(venv) ubuntu@ip-172-31-63-128:~/workspace/ray/python$ ray start --address='172.31.63.128:6379'
WARNING: Attempting to start a worker node on the physical head node.

Local node IP: 172.31.63.128

--------------------
Ray runtime started.
--------------------

To terminate the Ray runtime, run
  ray stop
```

If we also want to throw the warning when two workers running in the same node:

Currently, I do not think there is a existing API that allows me to detect if there is already a running worker on the local node（especial this two workers may from different ray clusters). So, my thought is add a new function `_ has_active_raylet` that has nearly the same code with part of this existing function: [_find_address_from_flag](https://sourcegraph.com/github.com/ray-project/ray@master/-/blob/python/ray/_private/services.py?L335-351) to detect if there are any raylet process in this local node.





I wonder if this approach seems appropriate to you. Thank you in advance!
",hi examining code thoroughly might mistaken previous approach upon investigation believe correct logic utilize retrieve local physical node ray start address worker connect worker trying initiate head node segment code considering integrate beginning python warning start worker node physical head node execution shell ray start warning start worker node physical head node local node ray terminate ray run ray stop also want throw warning two running node currently think detect already running worker local two may different ray thought add new function nearly code part function detect raylet process local node wonder approach appropriate thank advance,issue,negative,positive,neutral,neutral,positive,positive
1745667498,"The documentation uses Helm now, so we can close this issue.",documentation helm close issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1745660135,"Hi @Benyuel this sounds like a reasonable request, would you be willing to file a PR to add support for this?",hi like reasonable request would willing file add support,issue,positive,positive,positive,positive,positive,positive
1745653400,Yea once windows gets added to the new premerge CI system we will be able to just add a tag for each test. But for now have to add it to that bash script.,yea added new system able add tag test add bash script,issue,negative,positive,positive,positive,positive,positive
1745643111,"Oh cool, I was wondering if there is a way to just skip a file for a specific condition! Let me add this to it",oh cool wondering way skip file specific condition let add,issue,negative,positive,positive,positive,positive,positive
1745642480,"Looking into the stacktrace, the specific problem occurred in the file AutoscalerStatusCards.tsx at line 29, column 46, where an attempt was made to access a property called 'split' on an undefined object.",looking specific problem file line column attempt made access property undefined object,issue,negative,neutral,neutral,neutral,neutral,neutral
1745636270,"@GeneDer you can also add this test build target to `ci/ci.sh`, see `test_python` -- there are a number of tests that are completely skipped",also add test build target see number completely,issue,negative,positive,neutral,neutral,positive,positive
1745558956,"@llidev - and if you have a special ray address, wonder what's the value you passed to other commands like ray start/job submission. ",special ray address wonder value like ray submission,issue,positive,positive,positive,positive,positive,positive
1745536812,The latest run doesn't include the patch yet. Waiting for a successful nightly run to close this. ,latest run include patch yet waiting successful nightly run close,issue,positive,positive,positive,positive,positive,positive
1745441784,Hey @mattip -- just want to make sure you have this on your radar,hey want make sure radar,issue,negative,positive,positive,positive,positive,positive
1745413875,rebase fix the issue. @edoakes ready to merge.,rebase fix issue ready merge,issue,negative,positive,positive,positive,positive,positive
1745361520,"> is there any proposal I can read to understand context better?

This is basically a refactor of the existing https://github.com/ray-project/ray/blob/c21f7a209a0ccbadd3bdf73eb3bd6240fabc629c/python/ray/autoscaler/_private/resource_demand_scheduler.py#L102-L110

And the refactor is needed since the original implementation doesn't expose infeasible requests and changing it was too decoupled. 
 ",proposal read understand context better basically since original implementation expose infeasible,issue,positive,positive,positive,positive,positive,positive
1745356827,"> Hmm feel like it is not very meaningful changes here though. We can merge it, but if the address is malformed like that, it basically means even ray start should already fail (and we should have better format check in the highest layer by e2e principle, i.e., ray start)?

Seems in the original issue, it didn't raise an issue when ray start? But we could wait for OP to gather more info. If that's the case, we should not mark it as P0 as of now?",feel like meaningful though merge address malformed like basically even ray start already fail better format check highest layer principle ray start original issue raise issue ray start could wait gather case mark,issue,positive,positive,positive,positive,positive,positive
1745356677,"@bveeramani 

Your comments are all adressed or answered.

> How do we handle retries, especially for rate limiting errors?

Without considering rate limiting, I think we don't need to add retry code, because Ray task should automatically retry when failed if I understand ray correctly.

If we considered rate limit, I need to ask our warehouse engineer and then I will get back to you.


> How do we handle retries, especially for rate limiting errors?
IIRC Databricks marks a statement as closed when the last statement has been read. Is this still an issue, and if so, do how do we workaround it?

I replied it in https://github.com/ray-project/ray/pull/39852#discussion_r1343262825

",handle especially rate limiting without considering rate limiting think need add retry code ray task automatically retry understand ray correctly considered rate limit need ask warehouse engineer get back handle especially rate limiting statement closed last statement read still issue,issue,negative,negative,neutral,neutral,negative,negative
1745205071,"Discussed with @jjyao. We deciced to handle it in 2.8 because
1. it is not a regression from 2.7
2. while it is a bad issue, job submission is not an officially supported path for HA (we made it clear with the user as well)",handle regression bad issue job submission officially path ha made clear user well,issue,negative,negative,negative,negative,negative,negative
1745046155,"@alanwguo with nightly install I can now run ray manually (cluster launcher still doesn't work). Console output:
<details>
xhr.js:187     GET http://127.0.0.1:8265/api/grafana_health 500 (Internal Server Error)
(anonymous) @ xhr.js:187
e.exports @ xhr.js:13
e.exports @ dispatchRequest.js:51
u.request @ Axios.js:108
r.forEach.u.<computed> @ Axios.js:129
(anonymous) @ bind.js:9
i @ requestHandlers.ts:33
(anonymous) @ utils.ts:29
d @ regeneratorRuntime.js:72
(anonymous) @ regeneratorRuntime.js:55
(anonymous) @ regeneratorRuntime.js:97
r @ asyncToGenerator.js:3
s @ asyncToGenerator.js:22
(anonymous) @ asyncToGenerator.js:27
(anonymous) @ asyncToGenerator.js:19
(anonymous) @ utils.ts:28
(anonymous) @ utils.ts:53
d @ regeneratorRuntime.js:72
(anonymous) @ regeneratorRuntime.js:55
(anonymous) @ regeneratorRuntime.js:97
r @ asyncToGenerator.js:3
s @ asyncToGenerator.js:22
(anonymous) @ asyncToGenerator.js:27
(anonymous) @ asyncToGenerator.js:19
(anonymous) @ utils.ts:44
(anonymous) @ App.tsx:146
d @ regeneratorRuntime.js:72
(anonymous) @ regeneratorRuntime.js:55
(anonymous) @ regeneratorRuntime.js:97
r @ asyncToGenerator.js:3
s @ asyncToGenerator.js:22
(anonymous) @ asyncToGenerator.js:27
(anonymous) @ asyncToGenerator.js:19
(anonymous) @ App.tsx:139
(anonymous) @ App.tsx:157
Ul @ react-dom.production.min.js:262
t.unstable_runWithPriority @ scheduler.production.min.js:18
Va @ react-dom.production.min.js:122
kl @ react-dom.production.min.js:261
bl @ react-dom.production.min.js:243
(anonymous) @ react-dom.production.min.js:123
t.unstable_runWithPriority @ scheduler.production.min.js:18
Va @ react-dom.production.min.js:122
$a @ react-dom.production.min.js:123
Wa @ react-dom.production.min.js:122
Sl @ react-dom.production.min.js:244
oc @ react-dom.production.min.js:289
t.render @ react-dom.production.min.js:296
(anonymous) @ index.tsx:6
(anonymous) @ index.tsx:6
(anonymous) @ index.tsx:6
react-dom.production.min.js:216 TypeError: Cannot read properties of undefined (reading 'split')
    at ti (AutoscalerStatusCards.tsx:29:46)
    at Ja (AutoscalerStatusCards.tsx:12:10)
    at ni (AutoscalerStatusCards.tsx:69:11)
    at oo (react-dom.production.min.js:157:137)
    at zo (react-dom.production.min.js:180:154)
    at Ws (react-dom.production.min.js:269:343)
    at Al (react-dom.production.min.js:250:347)
    at Ol (react-dom.production.min.js:250:278)
    at xl (react-dom.production.min.js:250:138)
    at bl (react-dom.production.min.js:243:163)
us @ react-dom.production.min.js:216
n.callback @ react-dom.production.min.js:216
mi @ react-dom.production.min.js:131
hs @ react-dom.production.min.js:220
Ml @ react-dom.production.min.js:259
t.unstable_runWithPriority @ scheduler.production.min.js:18
Va @ react-dom.production.min.js:122
Dl @ react-dom.production.min.js:252
bl @ react-dom.production.min.js:243
(anonymous) @ react-dom.production.min.js:123
t.unstable_runWithPriority @ scheduler.production.min.js:18
Va @ react-dom.production.min.js:122
$a @ react-dom.production.min.js:123
Wa @ react-dom.production.min.js:122
ml @ react-dom.production.min.js:237
wo @ react-dom.production.min.js:170
(anonymous) @ use-sync-external-store-shim.production.min.js:10
(anonymous) @ index.mjs:102
l @ index.mjs:402
(anonymous) @ index.mjs:40
f @ index.mjs:175
(anonymous) @ index.mjs:306
d @ regeneratorRuntime.js:72
(anonymous) @ regeneratorRuntime.js:55
(anonymous) @ regeneratorRuntime.js:97
r @ asyncToGenerator.js:3
s @ asyncToGenerator.js:22
Promise.then (async)
r @ asyncToGenerator.js:12
s @ asyncToGenerator.js:22
(anonymous) @ asyncToGenerator.js:27
(anonymous) @ asyncToGenerator.js:19
(anonymous) @ index.mjs:308
(anonymous) @ index.mjs:379
hs @ react-dom.production.min.js:219
Ml @ react-dom.production.min.js:259
t.unstable_runWithPriority @ scheduler.production.min.js:18
Va @ react-dom.production.min.js:122
Dl @ react-dom.production.min.js:252
bl @ react-dom.production.min.js:243
ml @ react-dom.production.min.js:237
ec @ react-dom.production.min.js:285
(anonymous) @ react-dom.production.min.js:289
Sl @ react-dom.production.min.js:244
oc @ react-dom.production.min.js:289
t.render @ react-dom.production.min.js:296
(anonymous) @ index.tsx:6
(anonymous) @ index.tsx:6
(anonymous) @ index.tsx:6
react-dom.production.min.js:216 TypeError: Cannot read properties of undefined (reading 'split')
    at ti (AutoscalerStatusCards.tsx:29:46)
    at ei (AutoscalerStatusCards.tsx:25:10)
    at ri (AutoscalerStatusCards.tsx:84:11)
    at oo (react-dom.production.min.js:157:137)
    at zo (react-dom.production.min.js:180:154)
    at Ws (react-dom.production.min.js:269:343)
    at Al (react-dom.production.min.js:250:347)
    at Ol (react-dom.production.min.js:250:278)
    at xl (react-dom.production.min.js:250:138)
    at bl (react-dom.production.min.js:243:163)
us @ react-dom.production.min.js:216
n.callback @ react-dom.production.min.js:216
mi @ react-dom.production.min.js:131
hs @ react-dom.production.min.js:220
Ml @ react-dom.production.min.js:259
t.unstable_runWithPriority @ scheduler.production.min.js:18
Va @ react-dom.production.min.js:122
Dl @ react-dom.production.min.js:252
bl @ react-dom.production.min.js:243
(anonymous) @ react-dom.production.min.js:123
t.unstable_runWithPriority @ scheduler.production.min.js:18
Va @ react-dom.production.min.js:122
$a @ react-dom.production.min.js:123
Wa @ react-dom.production.min.js:122
ml @ react-dom.production.min.js:237
wo @ react-dom.production.min.js:170
(anonymous) @ use-sync-external-store-shim.production.min.js:10
(anonymous) @ index.mjs:102
l @ index.mjs:402
(anonymous) @ index.mjs:40
f @ index.mjs:175
(anonymous) @ index.mjs:306
d @ regeneratorRuntime.js:72
(anonymous) @ regeneratorRuntime.js:55
(anonymous) @ regeneratorRuntime.js:97
r @ asyncToGenerator.js:3
s @ asyncToGenerator.js:22
Promise.then (async)
r @ asyncToGenerator.js:12
s @ asyncToGenerator.js:22
(anonymous) @ asyncToGenerator.js:27
(anonymous) @ asyncToGenerator.js:19
(anonymous) @ index.mjs:308
(anonymous) @ index.mjs:379
hs @ react-dom.production.min.js:219
Ml @ react-dom.production.min.js:259
t.unstable_runWithPriority @ scheduler.production.min.js:18
Va @ react-dom.production.min.js:122
Dl @ react-dom.production.min.js:252
bl @ react-dom.production.min.js:243
ml @ react-dom.production.min.js:237
ec @ react-dom.production.min.js:285
(anonymous) @ react-dom.production.min.js:289
Sl @ react-dom.production.min.js:244
oc @ react-dom.production.min.js:289
t.render @ react-dom.production.min.js:296
(anonymous) @ index.tsx:6
(anonymous) @ index.tsx:6
(anonymous) @ index.tsx:6
xhr.js:187     GET http://127.0.0.1:8265/api/prometheus_health 500 (Cannot connect to host localhost:9090 ssl:default [Connect call failed ('127.0.0.1', 9090)])
(anonymous) @ xhr.js:187
e.exports @ xhr.js:13
e.exports @ dispatchRequest.js:51
u.request @ Axios.js:108
r.forEach.u.<computed> @ Axios.js:129
(anonymous) @ bind.js:9
i @ requestHandlers.ts:33
(anonymous) @ utils.ts:33
d @ regeneratorRuntime.js:72
(anonymous) @ regeneratorRuntime.js:55
(anonymous) @ regeneratorRuntime.js:97
r @ asyncToGenerator.js:3
s @ asyncToGenerator.js:22
(anonymous) @ asyncToGenerator.js:27
(anonymous) @ asyncToGenerator.js:19
(anonymous) @ utils.ts:32
(anonymous) @ utils.ts:62
d @ regeneratorRuntime.js:72
(anonymous) @ regeneratorRuntime.js:55
(anonymous) @ regeneratorRuntime.js:97
r @ asyncToGenerator.js:3
l @ asyncToGenerator.js:25
Promise.then (async)
r @ asyncToGenerator.js:12
s @ asyncToGenerator.js:22
(anonymous) @ asyncToGenerator.js:27
(anonymous) @ asyncToGenerator.js:19
(anonymous) @ utils.ts:44
(anonymous) @ App.tsx:146
d @ regeneratorRuntime.js:72
(anonymous) @ regeneratorRuntime.js:55
(anonymous) @ regeneratorRuntime.js:97
r @ asyncToGenerator.js:3
s @ asyncToGenerator.js:22
(anonymous) @ asyncToGenerator.js:27
(anonymous) @ asyncToGenerator.js:19
(anonymous) @ App.tsx:139
(anonymous) @ App.tsx:157
Ul @ react-dom.production.min.js:262
t.unstable_runWithPriority @ scheduler.production.min.js:18
Va @ react-dom.production.min.js:122
kl @ react-dom.production.min.js:261
bl @ react-dom.production.min.js:243
(anonymous) @ react-dom.production.min.js:123
t.unstable_runWithPriority @ scheduler.production.min.js:18
Va @ react-dom.production.min.js:122
$a @ react-dom.production.min.js:123
Wa @ react-dom.production.min.js:122
Sl @ react-dom.production.min.js:244
oc @ react-dom.production.min.js:289
t.render @ react-dom.production.min.js:296
(anonymous) @ index.tsx:6
(anonymous) @ index.tsx:6
(anonymous) @ index.tsx:6
AutoscalerStatusCards.tsx:29 Uncaught (in promise) TypeError: Cannot read properties of undefined (reading 'split')
    at ti (AutoscalerStatusCards.tsx:29:46)
    at Ja (AutoscalerStatusCards.tsx:12:10)
    at ni (AutoscalerStatusCards.tsx:69:11)
    at oo (react-dom.production.min.js:157:137)
    at zo (react-dom.production.min.js:180:154)
    at Ws (react-dom.production.min.js:269:343)
    at Al (react-dom.production.min.js:250:347)
    at Ol (react-dom.production.min.js:250:278)
    at xl (react-dom.production.min.js:250:138)
    at bl (react-dom.production.min.js:243:163)
</details>",nightly install run ray manually cluster launcher still work console output get internal server error anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous wa anonymous anonymous anonymous read undefined reading ti ni zo al u mi anonymous wa wo anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous read undefined reading ti zo al u mi anonymous wa wo anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous get connect host default connect call anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous anonymous wa anonymous anonymous anonymous uncaught promise read undefined reading ti ni zo al,issue,negative,neutral,neutral,neutral,neutral,neutral
1745037994,@anyscalesam I tried to view some logs printed out by ray but it was empty. I double checked setting my resources as stated by previous issues opened on github but that did not solve the issue. It seems the main issue here is that the head node is not able to communicate with the worker nodes. Do you have any insight on this?,tried view printed ray empty double checked setting stated previous solve issue main issue head node able communicate worker insight,issue,negative,positive,neutral,neutral,positive,positive
1744814881,"I'd be interested in a solution that would allow for on-demand fine-tuning models or even on-demand training small models. For example:

1. User makes a train_model(params) request to Serve
2. Ray starts training, validating and tuning the model
3. User makes a predict(x) request to Serve

This would be useful when creating web applications where the user can use the UI to make small changes to the ML pipeline (selecting features, filtering training data, choosing thresholds etc.) where pre-computing each combination of metaparameters would be infeasible.",interested solution would allow even training small example user request serve ray training tuning model user predict request serve would useful web user use make small pipeline filtering training data choosing combination would infeasible,issue,positive,positive,neutral,neutral,positive,positive
1744744429,"> I believe it is the same issue as #39913. Can you try the master Ray and verify if it is the case? (https://docs.ray.io/en/master/ray-overview/installation.html#daily-releases-nightlies). We are planning to include the fix to 2.7.1 release which is planned on 10/9

This works for manually starting ray: `pip install -U ""ray[default] @ https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-3.0.0.dev0-cp39-cp39-manylinux2014_x86_64.whl""`. Cluster launcher still doesn't work (workers uninitialized).",believe issue try master ray verify case include fix release work manually starting ray pip install ray default cluster launcher still work,issue,negative,neutral,neutral,neutral,neutral,neutral
1744580240,"> To merge the PR we should add a test!

Agreed, but I would need help on the perfect fix to use for this as I am not too well versed in Ray internals. I made a reproduction repository (https://github.com/XavierGeerinck/ray-repro-cython-async) that shows this issue",merge add test agreed would need help perfect fix use well versed ray internals made reproduction repository issue,issue,positive,positive,positive,positive,positive,positive
1744572393,"> What ray processes would be using that port with my config? 

I think it could be leaked processes or other process that we are missing that needs to bind to the port.

> ok, good luck I guess in finding this error again in a place where we don't have the mitigation in place. I just checked one of our dev ray clusters and it was ""hung"" with this error. lsof showed nothing using the port identified in the log and the cluster would accept a new job now in this state. So, something ""let loose"" of the port eventually but while it was locked the cluster was unusable (unless restarted which forced a port release it seems).

Just to make sure I understand, there's no process that uses the specific port, but you still cannot start a job and the cluster is in unusable state (because workers cannot bind to the port)? 

Is there any way you can reproduce this more easily in your environment? For example, setting the port range to very small numbers (like 10~ish) and keep creating/destroying new actors? I wonder if your environment has an issue with socket closing time (like longer socket closing time than a common environment within AWS)


",ray would port think could process missing need bind port good luck guess finding error place mitigation place checked one dev ray hung error nothing port log cluster would accept new job state something let loose port eventually locked cluster unusable unless forced port release make sure understand process specific port still start job cluster unusable state bind port way reproduce easily environment example setting port range small like keep new wonder environment issue socket time like longer socket time common environment within,issue,positive,positive,neutral,neutral,positive,positive
1744537972,is there any proposal I can read to understand context better? ,proposal read understand context better,issue,negative,positive,positive,positive,positive,positive
1744537009,we will write a doc in the next version ,write doc next version,issue,negative,neutral,neutral,neutral,neutral,neutral
1744535900,not yet. I will probably do it after Wed (2.7.1 cut date),yet probably wed cut date,issue,negative,neutral,neutral,neutral,neutral,neutral
1744523948,"No, we should fix it in 2.8! It is a known issue we decided to fix it for 2.8 ",fix known issue decided fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1744512859,"@a-zhenya this issue is probably fixed in ray 2.7. Can you try that out? The reason why when you create a new driver, it doesn't pick up the IP address that was given by ray start. It is fixed in 2.7",issue probably fixed ray try reason create new driver pick address given ray start fixed,issue,negative,positive,positive,positive,positive,positive
1744506251,"Hmm feel like it is not very meaningful changes here though. We can merge it, but if the address is malformed like that, it basically means even ray start should already fail (and we should have better format check in the highest layer by e2e principle, i.e., ray start)?   ",feel like meaningful though merge address malformed like basically even ray start already fail better format check highest layer principle ray start,issue,positive,positive,positive,positive,positive,positive
1744498040,Updated a branch due to lots of premerge test failures,branch due lot test,issue,negative,negative,negative,negative,negative,negative
1744437077,"> I am hesistant to merge the PR before we knew the exact address. Is this common you have address with more than one :?

But the change in this PR actually makes things more resilient to malformed address? I think this is the right check to enforce regardless the response from the original issue though. ",merge knew exact address common address one change actually resilient malformed address think right check enforce regardless response original issue though,issue,negative,positive,positive,positive,positive,positive
1744364607,"Thanks for the review.

To summarize, we run into the following problems:

1. Function trainables and Ray Train workers save checkpoints, but don't have access to auto-filled metrics (which are populated by the trainable), so can't use e.g. `TRAINING_ITERATION` as the checkpoint ID
2. Class trainables report a result and are then instructed to save, so the checkpoint directory name they choose can't retrospectively be added to the result log

We can obviously work around and special case these things, but let's see how we can generally solve this.

The main affected API for 2) seems to be `Result.from_path`. If we trial metadata in the trial folder, or checkpoint metadata in the checkpoint folders, we can remove the reliance on `result.json`.

Tor 1), there's no reason why we can't fill the training iteration etc in the highest level.

The change for 2) is definitely out of scope for this PR. For filling training iteration etc. in the worker, I also think the scope is too large for this fix, so I'd like to propose to land this PR, then follow-up with 1), and consider solutions for 2) separately and in scope with other options (e.g. moving class trainables to be function trainables)",thanks review summarize run following function ray train save access metric trainable ca use id class report result instructed save directory name choose ca retrospectively added result log obviously work around special case let see generally solve main affected trial trial folder remove reliance tor reason ca fill training iteration highest level change definitely scope filling training iteration worker also think scope large fix like propose land consider separately scope moving class function,issue,positive,positive,positive,positive,positive,positive
1744343563,@alanwguo would love to but the latest ray version isn't working for me - https://github.com/ray-project/ray/issues/40001 so I can't even start ray.,would love latest ray version working ca even start ray,issue,positive,positive,positive,positive,positive,positive
1744219169,"@harborn 

> Is there a separate channel for discussions related to further integration/development (such as slack/discord etc?)

Could you reach out to me on Ray slack? We should set up a collaboration channel.",separate channel related could reach ray slack set collaboration channel,issue,negative,neutral,neutral,neutral,neutral,neutral
1744218123,"
> @jjyao is it mandatory that the GPU be a part of AWS ? we do have an Intel Developer Cloud system where we can provision GPUs. Regarding AWS , it is still in discussion but a faster approach would be to allocate a IDC GPU machine which can be used for CI.

@can-anyscale could you help answer this question. IIUC, our CI tests run on aws ec2 instances.",mandatory part developer cloud system provision regarding still discussion faster approach would allocate machine used could help answer question run,issue,negative,neutral,neutral,neutral,neutral,neutral
1744199507,and please fix the breaking unit tests. those are container tagging logic that are related to the default python versions. you can also ask @can-anyscale for help.,please fix breaking unit container logic related default python also ask help,issue,positive,neutral,neutral,neutral,neutral,neutral
1744192980,"@jjyao  is it mandatory that the GPU be a part of AWS ? we do have an Intel Developer Cloud system where we can provision GPUs.  Regarding AWS , it is still in discussion but a faster approach would be to allocate a IDC GPU machine which can be used for CI.  ",mandatory part developer cloud system provision regarding still discussion faster approach would allocate machine used,issue,negative,neutral,neutral,neutral,neutral,neutral
1744078451,"Taking a look
Seems like the main cause is here
https://github.com/ray-project/ray/blob/f45ef45edc55983edea8542fa338769f6d0b90b3/python/ray/_raylet.pyx#L1603
which we can't set deserialize object that has `sigterm_handler` for `signal.SIGINT` as it will be deferred within `DeferSigint` context",taking look like main cause ca set object deferred within context,issue,negative,positive,positive,positive,positive,positive
1744056326,I am hesistant to merge the PR before we knew the exact address. Is this common you have address with more than one :? ,merge knew exact address common address one,issue,negative,negative,neutral,neutral,negative,negative
1744040507,@jonathan-anyscale @jjyao any ideas here? Not sure if this could be caused by the fact that Ray Train doesn't run the training script on the main thread?,sure could fact ray train run training script main thread,issue,negative,positive,positive,positive,positive,positive
1743998456,"yeah i have been slowly bisecting this for the past few weeks as well, but it's slow going (due to the reasons you mentioned), and also got inundated with a bunch of other oncall requests. Still looking into it, thanks for bumping",yeah slowly past well slow going due also got bunch still looking thanks bumping,issue,positive,negative,negative,negative,negative,negative
1743997623,"Just FYI @scottjlee , this test has been failing more than a month. I think @bveeramani and myself previously tried to bisect but we failed (the test requires some type of number of machines that make it hard to run).",test failing month think previously tried bisect test type number make hard run,issue,negative,negative,negative,negative,negative,negative
1743994662,"After pairing up w/ @rkooo567, seems like this is actually caused by Agent cleaning up metrics after not receiving these from the actor for longer than 120s

```
562023-10-02 10:03:55,330	INFO metrics_agent.py:245 -- Metrics from a worker (fa522cdccc8158063289ea2a6b9822eff668fac36f088a6c9f3b29c1) is cleaned up due to timeout. Time since last report 123.15761423896765s
```

And the reason these are not received from the agent are b/c batch of metrics exceeds the current 4Mb gRPC frame limit:

```
15805[2023-10-02 10:02:02,139 W 1730490 1730499] metric_exporter.cc:212: [1] Export metrics to agent failed: GrpcUnknown: RPC Error message: Received message larger than max (4200110 vs. 4194304); RPC Error details: . This won't affect Ray, but you can lose metrics from the cluster.
```


So the solution from the Ray side should be:

 - If exporter fails to export the metrics due to exceeding the frame limit, we should retry with the smaller batches rather than just unsuccessfully trying same batch size",like actually agent cleaning metric actor longer metric worker due time since last report reason received agent batch metric current frame limit export metric agent error message received message error wo affect ray lose metric cluster solution ray side exporter export metric due exceeding frame limit retry smaller rather unsuccessfully trying batch size,issue,negative,negative,neutral,neutral,negative,negative
1743967325,"Actually, the PushActorTask failure is not for the actual test but the job that submits it. The test itself fails during the verification phase after launching the cluster.",actually failure actual test job test verification phase cluster,issue,negative,negative,negative,negative,negative,negative
1743963858,"can you open the developer console and let me know if there are any errors? You can go to ""More tools -> Developer Tools -> Console"" in the Chrome menu.",open developer console let know go developer console chrome menu,issue,negative,neutral,neutral,neutral,neutral,neutral
1743942165,"i think updating the example will be pretty quick, just need to understand what would be the best way to show that it succeeded.
for the progress bar issue, i'll create a new issue with a ray data-only reproducible example.",think example pretty quick need understand would best way show progress bar issue create new issue ray reproducible example,issue,positive,positive,positive,positive,positive,positive
1743941400,@architkulkarni can we just chat in person to determine priority.,chat person determine priority,issue,negative,neutral,neutral,neutral,neutral,neutral
1743938450,@rickyyx Talked with @rkooo567 that this should be for Ray 2.7.1. But do let us know if it's not happening by Wednesday🙏,ray let u know happening,issue,negative,neutral,neutral,neutral,neutral,neutral
1743928094,"cc @c21 for review and merge.
EDIT: i see there are some unit test failures. @Zandew to re-assign to c21 once PR is ready.",review merge edit see unit test ready,issue,negative,positive,positive,positive,positive,positive
1743914054,Ideally there would be more than one type of fish in the output so we can see it classifying different fish... Not sure if there's a way to guarantee that though.,ideally would one type fish output see different fish sure way guarantee though,issue,positive,positive,positive,positive,positive,positive
1743912197,"I see, I missed the fact that you could scroll through the images.  (There's an invisible scroll bar)

For `display(img)` I think the average user is just going to copy paste the code and expect it to work. So maybe we can explicitly include the IPython.display import and tell the user to use IPython or Jupyter.

The progress bar is still confusing though, because it goes from 0/200 to 1/200 and then stops (even assuming the 1/1 is from an unrelated call as you suggested). ",see fact could scroll invisible scroll bar display think average user going copy paste code expect work maybe explicitly include import tell user use progress bar still though go even assuming unrelated call,issue,negative,negative,negative,negative,negative,negative
1743907016,"The 5 ""tench"" lines come from displaying each of the 5 examples from `predictions.take_batch(5)`. On the actual [example page output](https://docs.ray.io/en/latest/data/examples/huggingface_vit_batch_prediction.html#verify-and-save-results) itself, it shows the example image then the label (which makes more sense I think):
<img width=""708"" alt=""Screenshot at Oct 02 16-16-03"" src=""https://github.com/ray-project/ray/assets/5122851/35b18e2a-2712-4e9c-8422-82f5873b01fb"">

Would the most helpful addition here be to add something like ""Successfully loaded 5 samples"" at the end? Or any other suggestion to make it obvious it succeeded (without having to rely on displaying images on the screen, which may or may not be possible depending on how the user is running the code).

I think `display` is from [IPython.display](https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html), which we need for rendering the sample image in the jupyter notebook containing the example code.",tench come actual example page output example image label sense think would helpful addition add something like successfully loaded end suggestion make obvious without rely screen may may possible depending user running code think display need rendering sample image notebook example code,issue,positive,positive,positive,positive,positive,positive
1743903629,"@c21 
One Flacky error we face we we used boto based s3fs. 
```
Traceback (most recent call last):
  File ""/machine-learning/trainer/ray/train/actor_based_trainer.py"", line 272, in launch_mlenv_trainer
    wait_for_ray_mlenv_job(ray_mlenv_job)
  File ""/machine-learning/trainer/ray/train/actor_based_trainer.py"", line 172, in wait_for_ray_mlenv_job
    ray.get(finished_proc_ref)
  File ""/usr/local/lib/python3.8/site-packages/ray/_private/auto_init_hook.py"", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.8/site-packages/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.8/site-packages/ray/_private/worker.py"", line 2493, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(SSLError): [36mray::RayMLEnvRunner.run_trainer()[39m (pid=719, ip=10.12.14.68, actor_id=2d06687add5ff296da94860c02000000, repr=<trainer.ray.train.actor_based_trainer.RayMLEnvRunner object at 0x7e849a57bf70>)
  File ""/machine-learning/trainer/ray/train/actor_based_trainer.py"", line 64, in run_trainer
    run_single_process(config_bundle, ds_iter)
  File ""/machine-learning/trainer/ppytorch/utils/launcher.py"", line 127, in run_single_process
    trainer.run()
  File ""/machine-learning/trainer/ppytorch/base/distributed.py"", line 107, in run
    super().run(base_dir)
  File ""/machine-learning/trainer/utils/ptrainer.py"", line 247, in run
    raise e
  File ""/machine-learning/trainer/utils/ptrainer.py"", line 234, in run
    solver.execute(gpus=self.gpus, run_dir=run_dir, tb_log_dir=tb_log_dir)
  File ""/machine-learning/trainer/ppytorch/mlenv/common/solver/basic_solver.py"", line 817, in execute
    for batch in self.train_dataset_loader:
  File ""/usr/local/lib/python3.8/site-packages/ray/data/iterator.py"", line 189, in iter_batches
    yield from iter_batches(
  File ""/usr/local/lib/python3.8/site-packages/ray/data/_internal/block_batching/iter_batches.py"", line 176, in iter_batches
    next_batch = next(async_batch_iter)
  File ""/usr/local/lib/python3.8/site-packages/ray/data/_internal/block_batching/util.py"", line 289, in make_async_gen
    raise next_item
  File ""/usr/local/lib/python3.8/site-packages/ray/data/_internal/block_batching/util.py"", line 266, in execute_computation
    for item in fn(thread_safe_generator):
  File ""/usr/local/lib/python3.8/site-packages/ray/data/_internal/block_batching/iter_batches.py"", line 167, in _async_iter_batches
    yield from extract_data_from_batch(batch_iter)
  File ""/usr/local/lib/python3.8/site-packages/ray/data/_internal/block_batching/util.py"", line 210, in extract_data_from_batch
    for batch in batch_iter:
  File ""/usr/local/lib/python3.8/site-packages/ray/data/_internal/block_batching/iter_batches.py"", line 306, in restore_original_order
    for batch in batch_iter:
  File ""/usr/local/lib/python3.8/site-packages/ray/data/_internal/block_batching/iter_batches.py"", line 218, in threadpool_computations_format_collate
    yield from formatted_batch_iter
  File ""/usr/local/lib/python3.8/site-packages/ray/data/_internal/block_batching/util.py"", line 179, in collate
    for batch in batch_iter:
  File ""/usr/local/lib/python3.8/site-packages/ray/data/_internal/block_batching/util.py"", line 158, in format_batches
    for batch in block_iter:
  File ""/usr/local/lib/python3.8/site-packages/ray/data/_internal/block_batching/util.py"", line 117, in blocks_to_batches
    for block in block_iter:
  File ""/usr/local/lib/python3.8/site-packages/ray/data/_internal/block_batching/util.py"", line 54, in resolve_block_refs
    for block_ref in block_ref_iter:
  File ""/usr/local/lib/python3.8/site-packages/ray/data/_internal/block_batching/iter_batches.py"", line 254, in prefetch_batches_locally
    for block_ref, metadata in block_ref_iter:
  File ""/usr/local/lib/python3.8/site-packages/ray/data/_internal/block_batching/util.py"", line 246, in __next__
    return next(self.it)
  File ""/usr/local/lib/python3.8/site-packages/ray/data/_internal/iterator/stream_split_iterator.py"", line 85, in gen_blocks
    block_ref: Optional[Tuple[ObjectRef[Block], BlockMetadata]] = ray.get(
ray.exceptions.RayTaskError(SSLError): [36mray::SplitCoordinator.get()[39m (pid=6305, ip=10.12.120.150, actor_id=3cf8ac336c1a012e46b685d202000000, repr=<ray.data._internal.iterator.stream_split_iterator.SplitCoordinator object at 0x7f5a89f3a070>)
  File ""/usr/local/lib/python3.8/site-packages/ray/data/_internal/iterator/stream_split_iterator.py"", line 210, in get
    next_bundle = self._output_iterator.get_next(output_split_idx)
  File ""/usr/local/lib/python3.8/site-packages/ray/data/_internal/execution/streaming_executor.py"", line 129, in get_next
    raise item
  File ""/usr/local/lib/python3.8/site-packages/ray/data/_internal/execution/streaming_executor.py"", line 129, in get_next
    raise item
  File ""/usr/local/lib/python3.8/site-packages/ray/data/_internal/execution/streaming_executor.py"", line 129, in get_next
    raise item
  [Previous line repeated 4 more times]
  File ""/usr/local/lib/python3.8/site-packages/ray/data/_internal/execution/streaming_executor.py"", line 187, in run
    while self._scheduling_loop_step(self._topology) and not self._shutdown:
  File ""/usr/local/lib/python3.8/site-packages/ray/data/_internal/execution/streaming_executor.py"", line 235, in _scheduling_loop_step
    process_completed_tasks(topology)
  File ""/usr/local/lib/python3.8/site-packages/ray/data/_internal/execution/streaming_executor_state.py"", line 333, in process_completed_tasks
    op.notify_work_completed(ref)
  File ""/usr/local/lib/python3.8/site-packages/ray/data/_internal/execution/operators/actor_pool_map_operator.py"", line 219, in notify_work_completed
    task.output = self._map_ref_to_ref_bundle(ref)
  File ""/usr/local/lib/python3.8/site-packages/ray/data/_internal/execution/operators/map_operator.py"", line 360, in _map_ref_to_ref_bundle
    block_metas = ray.get(all_refs[-1])
ray.exceptions.RayTaskError(SSLError): [36mray::ReadParquetBulk->MapBatches(bad_actors_filter)->MapBatches(tabularml_arrow_feature_converter)->MapBatches(BatcherActor)()[39m (pid=5704, ip=10.12.56.38)
  File ""/usr/local/lib/python3.8/site-packages/urllib3/response.py"", line 438, in _error_catcher
    yield
  File ""/usr/local/lib/python3.8/site-packages/urllib3/response.py"", line 515, in read
    data = self._fp.read() if not fp_closed else b""""
  File ""/usr/local/lib/python3.8/http/client.py"", line 472, in read
    s = self._safe_read(self.length)
  File ""/usr/local/lib/python3.8/http/client.py"", line 613, in _safe_read
    data = self.fp.read(amt)
  File ""/usr/local/lib/python3.8/socket.py"", line 669, in readinto
    return self._sock.recv_into(b)
  File ""/usr/local/lib/python3.8/ssl.py"", line 1241, in recv_into
    return self.read(nbytes, buffer)
  File ""/usr/local/lib/python3.8/ssl.py"", line 1099, in read
    return self._sslobj.read(len, buffer)
ssl.SSLError: [SSL: DECRYPTION_FAILED_OR_BAD_RECORD_MAC] decryption failed or bad record mac (_ssl.c:2635)
```
When we use pyarrow S3FileSystem we could face OSError when it talking with aws-cpp-sdk",one error face used based recent call last file line file line file line return file line wrapper return file line get raise object file line file line file line run super file line run raise file line run file line execute batch file line yield file line next file line raise file line item file line yield file line batch file line batch file line yield file line collate batch file line batch file line block file line file line file line return next file line optional block object file line get file line raise item file line raise item file line raise item previous line repeated time file line run file line topology file line ref file line ref file line file line yield file line read data else file line read file line data amt file line return file line return buffer file line read return buffer bad record mac use could face talking,issue,negative,negative,neutral,neutral,negative,negative
1743897541,"Sure, maybe the memory location is a distraction.  The main point of the issue is that the output is baffling for a first-time user reading through the tutorial.  It prints out five identical ""tench"" lines.  If it's working as intended, then the doc or the sample script should be updated to make it more obvious that it worked so the user can feel successful.

Also the `display(img)` in the tutorial doesn't work out of the box (where is `display` defined?)",sure maybe memory location distraction main point issue output baffling user reading tutorial five identical tench working intended doc sample script make obvious worked user feel successful also display tutorial work box display defined,issue,positive,positive,positive,positive,positive,positive
1743890609,"ah, actually i think the `1/1` progress bar may be coming from the `.take_batch()` call, because this adds a `Limit` operator with 1 task (which is creating the 1/1 progress bar). This is a bit confusing, so we will discuss internally how to clarify this view.

Regarding the memory location, I don't think ray is modifying the memory location here, we think it's purely related to PIL? Are you able to see otherwise when reading the images with just PIL and not through ray? @architkulkarni ",ah actually think progress bar may coming call limit operator task progress bar bit discus internally clarify view regarding memory location think ray memory location think purely related able see otherwise reading ray,issue,positive,positive,positive,positive,positive,positive
1743889416,"I'll make an issue to track this on the sphinx-doc/sphinx repo. I don't think the ask is very large here, so I might just do this if there's time in the next few weeks.",make issue track think ask large might time next,issue,negative,positive,positive,positive,positive,positive
1743883579,":point_up: @anyscalesam I'm happy to look closer at this one this week. I'll start by checking the commit history for Sept 13-14 to see what changed in the docs, and will report back with my findings.",happy look closer one week start commit history sept see report back,issue,positive,positive,positive,positive,positive,positive
1743881307,@simran-2797 What's the fixed link for the second example? When I last checked the one given above was failing linktest checks.,fixed link second example last checked one given failing,issue,negative,positive,neutral,neutral,positive,positive
1743880442,"> @bveeramani Any chance we can either a) change the behavior with an autosummary option, b) modify autosummary to do this globally, or c) add an upstream option to sphinx.ext.autosummary to configure it to not show signatures globally? It would be really nice if we don't have to keep adding `:nosignatures:` everywhere.

I looked into it and wasn't able to find an option to do this globally.

> add an upstream option to sphinx.ext.autosummary to configure it to not show signatures globally

Would be awesome if we could do this but I don't have the bandwidth personally. Hopefully, developers will continue to use `nosignatures` by convention. ",chance either change behavior option modify globally add upstream option configure show globally would really nice keep everywhere able find option globally add upstream option configure show globally would awesome could personally hopefully continue use convention,issue,positive,positive,positive,positive,positive,positive
1743877982,"We have a related PR which may end up resolving the 1/1 progress bar issue: https://github.com/ray-project/ray/pull/39828
We can revisit this PR after it is merged, to see if the progress bar issue is still present.",related may end progress bar issue revisit see progress bar issue still present,issue,positive,neutral,neutral,neutral,neutral,neutral
1743867675,"> Would the `object` class ever be listed in a list with multiple bases? Or is it automatically dropped if there is another base class to list?

Don't think so unless you explicitly did something like `Subclass(BaseClass, object)`, but that would be incredibly unusual.",would object class ever listed list multiple base automatically another base class list think unless explicitly something like subclass object would incredibly unusual,issue,negative,negative,negative,negative,negative,negative
1743865905,"> @bveeramani Does this message only appear if you explicitly subclass `object`? 

No, it shows up even if you don't explicitly subclass `object`.

> If so, we can tear out any of those instances, as it was only necessary to subclass `object` in Python 2.

Totally agree. I think we still have those in a handful of places. ",message appear explicitly subclass object even explicitly subclass object tear necessary subclass object python totally agree think still handful,issue,negative,neutral,neutral,neutral,neutral,neutral
1743864414,"> @jmakov what web browser are you using?

Tried latest Firefox and Chromium.",web browser tried latest chromium,issue,negative,positive,positive,positive,positive,positive
1743833212,"Hi @yiwei00000 , any reason to `materialize()` the dataset in the above code example? This will load the entire dataset in memory, as opposed to taking advantage of streaming execution.",hi reason materialize code example load entire memory opposed taking advantage streaming execution,issue,negative,neutral,neutral,neutral,neutral,neutral
1743829041,"Seems that this is not related to python 3.11, but the restriction comes from using Windows with Pyarrow 7+ (see [setup.py](https://github.com/ray-project/ray/blob/master/python/setup.py#L245-L246)). It looks like our internal fix for the [Arrow bug](https://github.com/apache/arrow/issues/26685), implemented in https://github.com/ray-project/ray/pull/29993, is not compatible with Windows.",related python restriction come see like internal fix arrow bug compatible,issue,negative,neutral,neutral,neutral,neutral,neutral
1743821426,@angelinalg it's for the ray team updating the wheels. The comment is not visible to users (it's like a code comment),ray team comment visible like code comment,issue,negative,neutral,neutral,neutral,neutral,neutral
1743816800,@chappidim we are going to assign it to you. Can you tell us the timeline of the fix? ,going assign tell u fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1743813049,"@bveeramani Does this message only appear if you explicitly subclass `object`? If so, we can tear out any of those instances, as it was only necessary to subclass `object` in Python 2.",message appear explicitly subclass object tear necessary subclass object python,issue,negative,neutral,neutral,neutral,neutral,neutral
1743809922,"It is not possible now (we do retry every 1 second). Right now, the best way is to do it in the application layer (try/except ray.get from your) code. ",possible retry every second right best way application layer code,issue,positive,positive,positive,positive,positive,positive
1743806379,Thanks @ujjawal-khare for your contribution! I will assign the issue to you for now -- please feel free to assign me the PR for review once ready.,thanks contribution assign issue please feel free assign review ready,issue,positive,positive,positive,positive,positive,positive
1743805869,"@KamenShah I'd vote to make the background transparent. It doesn't matter that much in this case because we'll need to redo a lot of CSS as is to support dark themes with the latest version of Sphinx, so it's up to you whether you think the effort to change it here is worth it.",vote make background transparent matter much case need redo lot support dark latest version sphinx whether think effort change worth,issue,positive,positive,positive,positive,positive,positive
1743801464,"Really like this change, thank you! :rocket: 

> The duplicate links to the references leads to janky and inconsistent behavior. For example, if you click ""Ray Data API"" in the ""Ray Data"" section of the documentation, you abruptly jump to the ""References"" section of the documentation. This doesn't happen with train.

This happens because having links in two separate places in the TOC kind of undermines the idea of a TOC tree in the first place. With Sphinx each article should be a child or parent of other articles; having them in multiple places will result in this kind of jank.",really like change thank rocket duplicate link inconsistent behavior example click ray data ray data section documentation abruptly jump section documentation happen train link two separate kind idea tree first place sphinx article child parent multiple result kind jank,issue,positive,positive,positive,positive,positive,positive
1743800360,"1. Add exit code for task/actor/node failure exceptions messages
2. Adding the actual node failure reason to node dead error message.

Scoped to 2.9",add exit code failure actual node failure reason node dead error message,issue,negative,negative,negative,negative,negative,negative
1743797915,"@KamenShah If this is part of a global style, I'd vote to move the color into a CSS variable so that people know that it's part of our theme, and so that if we change it in the future it will be easy to do so without hunting down the different rules that have this color hardcoded.",part global style vote move color variable people know part theme change future easy without hunting different color,issue,negative,positive,positive,positive,positive,positive
1743794695,"@bveeramani Any chance we can either a) change the behavior with an autosummary option, b) modify autosummary to do this globally, or c) add an upstream option to sphinx.ext.autosummary to configure it to not show signatures globally? It would be really nice if we don't have to keep adding `:nosignatures:` everywhere.",chance either change behavior option modify globally add upstream option configure show globally would really nice keep everywhere,issue,positive,positive,positive,positive,positive,positive
1743793179,Yes aiming for 2.8. I'm picking it up this week from the Serve side,yes aiming week serve side,issue,negative,neutral,neutral,neutral,neutral,neutral
1743788670,"Local mode has been deprecated since Ray 2.0. https://github.com/ray-project/ray/blob/ad1e06bdbcb901914280dcc4b7400092ccafbdc1/python/ray/_private/worker.py#L1232 We recommend you to use the Ray debugger for a better debugging experience https://docs.ray.io/en/master/ray-observability/user-guides/debug-apps/ray-debugging.html 

",local mode since ray recommend use ray better experience,issue,positive,positive,positive,positive,positive,positive
1743783976,Current status; We are waiting for REP! ,current status waiting rep,issue,negative,neutral,neutral,neutral,neutral,neutral
1743778109,"@architkulkarni Who is the audience for the doc updates? Users of Ray? If so, are they the ones that are changing the wheels, or is the Ray Team changing the wheels. I'm trying to figure out how to use active voice for the sentence, vs passive voice.",audience doc ray ray team trying figure use active voice sentence passive voice,issue,positive,negative,negative,negative,negative,negative
1743775936,I believe it is the same issue as https://github.com/ray-project/ray/issues/39913. Can you try the master Ray and verify if it is the case? (https://docs.ray.io/en/master/ray-overview/installation.html#daily-releases-nightlies). We are planning to include the fix to 2.7.1 release which is planned on 10/9 ,believe issue try master ray verify case include fix release,issue,negative,neutral,neutral,neutral,neutral,neutral
1743773655,cc @c21. This seems like a blocker for supporting python 3.11. ,like blocker supporting python,issue,positive,positive,positive,positive,positive,positive
1743770875,"CC: @krfricke , @matthewdeng , look like a decision for the ml team to make?",look like decision team make,issue,negative,neutral,neutral,neutral,neutral,neutral
1743768810,"@rickyyx not to mention manually starting ray [not working](https://github.com/ray-project/ray/issues/40001) and cluster launcher not working. Wondering how ray works at all for anybody. As someone who uses ray for more than a year, every other release breaks a core part. ",mention manually starting ray working cluster launcher working wondering ray work anybody someone ray year every release core part,issue,negative,neutral,neutral,neutral,neutral,neutral
1743765701,"Lint failed 

```


python/ray/serve/tests/test_grpc.py:530:5: F811 redefinition of unused 'sys' from line 2
--
  | python/ray/serve/tests/test_regression.py:274:5: F811 redefinition of unused 'sys' from line 3
```
",lint redefinition unused line redefinition unused line,issue,negative,neutral,neutral,neutral,neutral,neutral
1743759365,@ericl sorry somehow it is missed. I will create a P0 issue and make sure to finish it asap,sorry somehow create issue make sure finish,issue,negative,neutral,neutral,neutral,neutral,neutral
1743737659,"The only thing I'm able to see on my end from the Google Search Console is whether this is still an issue, how long it's been happening, and roughly what the cause is. If this happens to be a simple fix, then it would be great to have this video be indexable in Google Search. Otherwise, the back-up is that the YouTube video is still indexed if people search for it and the Ray docs landing page will be indexed as a search result, just without the accompanying video in preview.",thing able see end search console whether still issue long happening roughly cause simple fix would great video search otherwise video still indexed people search ray landing page indexed search result without video preview,issue,positive,positive,positive,positive,positive,positive
1743732888,"@peytondmurray 
We should remove the first example.
We can add the second example in the example gallery (and link it to uber blog)",remove first example add second example example gallery link,issue,negative,positive,positive,positive,positive,positive
1743714680,"I think this has to be a high severity level, if we think it indicates a bug. We might even raise an exception in the future if this happens.

However, I don't think today we can raise an exception, since UDFs can return large blocks in map batches and these aren't split. The best we can do is warn about this.",think high severity level think bug might even raise exception future however think today raise exception since return large map split best warn,issue,positive,positive,positive,positive,positive,positive
1743710225,"cc @gvspraveen could someone from the cluster team help take a look? I believe this is more relevant to cluster launcher as of now rather than the actual autoscaling logics since ""running everything manually works"". 
",could someone cluster team help take look believe relevant cluster launcher rather actual since running everything manually work,issue,negative,positive,positive,positive,positive,positive
1743707116,Maybe we could publish the cleaned up dataset as an AI here. ,maybe could publish ai,issue,negative,neutral,neutral,neutral,neutral,neutral
1743706562,@emmyscode can you please add a priority label and remove triage after having done so. It's assigned to @peytondmurray are y'all aligned on who will take the next step here?,please add priority label remove triage done assigned take next step,issue,negative,neutral,neutral,neutral,neutral,neutral
1743704673,@rkooo567 please add a priority and remove triage label,please add priority remove triage label,issue,negative,neutral,neutral,neutral,neutral,neutral
1743658075,"The approach suggested in [this comment](https://github.com/ray-project/ray/issues/39781#issuecomment-1736479133) seems to have a high level of complexity and could lead to alterations in existing behaviors of KubeRay, such as autoscaling, worker group, and new headless services, potentially impacting maintainability. Hence, I have the following proposal:

* GKE
  * Users should create a TPU node pool for each RayCluster, and each node pool has unique taints/labels for Pod scheduling.

* TPU
  * TPU team should provide two utility functions:
    * `get_podslice_id`: Get the unique ID of a PodSlice that this TPU Pod belongs to.
    * `get_tpu_worker_id`: Get a unique integer for each TPU Pod in the same PodSlice.

* KubeRay
  *  KubeRay does not track which TPU Pods belong to which TPU PodSlice. That is, KubeRay will not set `group_id`.
  * Users configure the TPU Pod's nodeSelector and tolerations to ensure the Pod is scheduled on a TPU PodSlice belonging to a specific TPU node pool.
  * KubeRay always respects the decisions made by Ray Autoscaler to create or delete specific Pods. It should not determine whether the Ray Pod is a TPU Pod, nor whether these four Ray Pods belong to the same TPU PodSlice. Ray Autoscaler should be the single source of the truth.

* Ray Core
  * Setup environments
    * `group_id`: Use `get_podslice_id` to set it.
    * `TPU_WORKER_ID`: Use `get_tpu_worker_id` to set it.
    * `TPU_WORKER_HOSTNAMES`: Ray Core, aware of the IP information of each Ray node and the `group_id` information, can handle `TPU_WORKER_HOSTNAMES` gracefully.
  * Scheduling
    * https://github.com/ray-project/ray/issues/39781#issuecomment-1741313233
  * Autoscaling   ",approach comment high level complexity could lead worker group new headless potentially hence following proposal create node pool node pool unique pod team provide two utility get unique id pod get unique integer pod track belong set configure pod ensure pod belonging specific node pool always made ray create delete specific determine whether ray pod pod whether four ray belong ray single source truth ray core setup use set use set ray core aware information ray node information handle gracefully,issue,positive,positive,positive,positive,positive,positive
1743620235,"What does P2 mean here?  I believe this is something high value to all OSS devs productivity since the lack of logs on non linux CI basically makes all test failures non-debuggable. Not sure what other issues are being marked  >P2, but this probably is a bit more important than what P2 usually sounds ( not gonna do in the next release IIUC)

> @rickyyx would the surfacing of this be via our CI pipeline itself (hence why it's assigned to @can-anyscale).

Yes, it will be through CI. 


",mean believe something high value productivity since lack non basically test sure marked probably bit important usually gon na next release would surfacing via pipeline hence assigned yes,issue,positive,positive,neutral,neutral,positive,positive
1743615993,@rickyyx would the surfacing of this be via our CI pipeline itself (hence why it's assigned to @can-anyscale). Setting it to P2.,would surfacing via pipeline hence assigned setting,issue,negative,neutral,neutral,neutral,neutral,neutral
1743577999,"Keeping the warning in stdout is fine, as long as we can the message clear. I don't think we should suggest users to increase the parallelism. Because this issue can only happen when (1) one single row is bigger than the target block size; 2) there is a bug in Ray Data. ",keeping warning fine long message clear think suggest increase parallelism issue happen one single row bigger target block size bug ray data,issue,negative,positive,neutral,neutral,positive,positive
1743516732,"> Since this is for dev to debugging perf, only logging to data logs makes more sense.

@raulchen Do you suggest we move both the log and warn to data log only? I think showing the warning in stdout still makes sense, since this is a pretty large issue that users should be aware of without having to look at data specific logs.",since dev logging data sense suggest move log warn data log think showing warning still sense since pretty large issue aware without look data specific,issue,negative,positive,positive,positive,positive,positive
1743495568,"Since this is for dev to debugging perf, only logging to data logs makes more sense. ",since dev logging data sense,issue,negative,neutral,neutral,neutral,neutral,neutral
1743463134,"I think it looks good. Some potential things to monitor after merge:

```
1_1_actor_calls_sync: [2627.6907826949946, 17.95559542036301] => [2310.426069343179, 38.1428781457671]
 client__put_gigabytes: [0.13195088324631216, 0.002155235235063773] => [0.09375705565294579, 0.002669758704407111]
```",think good potential monitor merge,issue,negative,positive,positive,positive,positive,positive
1743435089,"> And use `""904dbce085bc542b93fbe06d75f3b02a65d3a2b4""` in `test_get_master_wheel_url` as well

Sorry, can you use `0910639b6eba1b77b9a36b9f3350c5aa274578dd` for `test_get_master_wheel_url` instead? I just checked and this one has all the wheels uploaded.

Also, the lint job is failing in buildkite/premerge. You can run `ci/setup_hooks.sh` to install a hook that automatically runs the linter locally before pushing.
",use well sorry use instead checked one also lint job failing run install hook automatically linter locally pushing,issue,negative,negative,negative,negative,negative,negative
1743419637,"> sure thing. i noticed they both use the same edge case
> 
> ```python
>             # TODO(https://github.com/ray-project/ray/issues/31362)
>             if py_version == (3, 11) and sys_platform != ""linux"":
>                 continue
> ```
> 
> shall i remove those as well? And should they go through all the possibilities like the changes made to `test_get_wheel_filename`

Yes, that would be ideal, thanks!",sure thing use edge case python continue shall remove well go like made yes would ideal thanks,issue,positive,positive,positive,positive,positive,positive
1743381455,"> @sihanwang41 what is the status of Multi Apps in Ray Serve? The documentation still shows only the `serve.run` method with centralised YAML file which I am not sure whether it is as flexible as the old `.deploy()` capability when you have multiple requests coming in parallel.
> 
> For example: every time a user presses a button on a UI, I want to deploy a model on a shared Ray Cluster. Using the old API, I could just call `deploy()`. Now I need to retrieve the yaml file, update it, and re-run `serve.run` using that YAML file.
> 
> Now, what happens if two requests come at the same time, and both retrieve the original YAML file? Will they over-write and kill each other as they are unaware of each other at deployment time?

Hi @andreapiso , it is fully supported now! 
you can use `serve.run(xxx, name=""app1"")` and `serve.run(xxx, name=""app2"")` to deploy them separately. For YAML file, you need to include all the applications into the YAML. If apps are not in the YAML file, the app will be removed. For more information, please check: https://docs.ray.io/en/latest/serve/multi-app.html
",status ray serve documentation still method file sure whether flexible old capability multiple coming parallel example every time user button want deploy model ray cluster old could call deploy need retrieve file update file two come time retrieve original file kill unaware deployment time hi fully use deploy separately file need include file removed information please check,issue,negative,positive,positive,positive,positive,positive
1743379881,"Hey @WeichenXu123, thanks for contributing! I'll take a look at this PR sometime this week",hey thanks take look sometime week,issue,negative,positive,positive,positive,positive,positive
1743361767,"Top-level cause is network error:
```
(ses_6uxmpf59v8saqyyr9yf5g6kub6) (g-aa97dfaab13d70001) [/tmp/ray/session_2023-10-01_20-09-57_410429_48/logs/python-core-driver-01000000ffffffffffffffffffffffffffffffffffffffffffffffff_359.log] [2023-10-01 20:59:00,023 I 359 640] direct_actor_task_submitter.cc:583: PushActorTask failed because of network error, this task will be stashed away and waiting for Death info from GCS, task_id=16310a0f0a45af5c6abaadd46c8431a95ac0ecb501000000, wait_queue_size=1
```
Going to check for a potential underlying cause, otherwise we could try to increase the number of retries. Interesting that same test doesn't fail for AWS.",cause network error network error task away waiting death going check potential underlying cause otherwise could try increase number interesting test fail,issue,negative,neutral,neutral,neutral,neutral,neutral
1743227179,"`PPO` with `EnvRunner` passes the `tuned_examples` test. However it is way slower in convergence than using the Sampler API. 

Old API: https://tensorboard.dev/experiment/hbDOMIJJTRSgnADj3MiOvQ/#scalars&_smoothingWeight=0

EnvRunner: https://tensorboard.dev/experiment/0ztlZgVvS3GpUvG7DIbLnQ/#scalars&_smoothingWeight=0",test however way convergence sampler old,issue,negative,positive,neutral,neutral,positive,positive
1743179128,@zhe-thoughts just in case the notification was missed. Can you approve this if you think we can pick it🙏,case notification approve think pick,issue,negative,neutral,neutral,neutral,neutral,neutral
1743177564,This is also failing on the release branch [[jailed]gcp_cluster_launcher_full (eb165c3)](https://buildkite.com/ray-project/release-tests-branch/builds/2205#018aed7a-b27f-4950-bc37-4f02478d9510) Marking as release blocker🙏,also failing release branch marking release blocker,issue,negative,neutral,neutral,neutral,neutral,neutral
1742767959,"Just seen that [**""Python 3.11 support is experimental.""**](https://docs.ray.io/en/latest/ray-overview/installation.html) I guess the default pip package is not for python 3.11 (which comes by default with Debian, and similar distros for some time now). 
will try other distros with older python version... (python 3.10 seems supported). Fix me if I'm wrong, thanks!",seen python support experimental guess default pip package python come default similar time try older python version python fix wrong thanks,issue,negative,negative,neutral,neutral,negative,negative
1742725810,"Hi, I hit to the same problem with latest package from pip (ray version 2.7.0) and python (version 3.11.4): 
ray.serve.start(), ray.serve.run() run OK, and have visible/expected counterpart on the dashboard, but when trying to obtain prediction, the model fails with:

```
> (HTTPProxyActor pid=791) ERROR 2023-10-02 09:47:43,092 http_proxy 172.17.0.3 85adb9a7-94cf-4591-b690-8cb12b6971f9 /mydemo mydemo http_proxy.py:1355 - Passing coroutines is forbidden, use tasks explicitly.
> (HTTPProxyActor pid=791) Traceback (most recent call last):
> (HTTPProxyActor pid=791)   File ""/opt/conda/lib/python3.11/site-packages/ray/serve/_private/http_proxy.py"", line 1342, in send_request_to_replica_streaming
> (HTTPProxyActor pid=791)     status_code = await self._consume_and_send_asgi_message_generator(
> (HTTPProxyActor pid=791)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
> (HTTPProxyActor pid=791)   File ""/opt/conda/lib/python3.11/site-packages/ray/serve/_private/http_proxy.py"", line 1228, in _consume_and_send_asgi_message_generator
> (HTTPProxyActor pid=791)     obj_ref = next_obj_ref_task.result()
> (HTTPProxyActor pid=791)               ^^^^^^^^^^^^^^^^^^^^^^^^^^
> (HTTPProxyActor pid=791)   File ""python/ray/_raylet.pyx"", line 387, in _next_async
> (HTTPProxyActor pid=791)   File ""/opt/conda/lib/python3.11/asyncio/tasks.py"", line 415, in wait
> (HTTPProxyActor pid=791)     raise TypeError(""Passing coroutines is forbidden, use tasks explicitly."")
> (HTTPProxyActor pid=791) TypeError: Passing coroutines is forbidden, use tasks explicitly.
> (HTTPProxyActor pid=791) ERROR:    ASGI callable returned without starting response.
> (ServeReplica:mydemo:MyModelDeployment pid=843) INFO 2023-10-02 09:47:43,099 MyModelDeployment mydemo#MyModelDeployment#DjSeNn 85adb9a7-94cf-4591-b690-8cb12b6971f9 /mydemo mydemo replica.py:749 - __CALL__ OK 0.2ms",hi hit problem latest package pip ray version python version run counterpart dashboard trying obtain prediction model error passing forbidden use explicitly recent call last file line await file line file line file line wait raise passing forbidden use explicitly passing forbidden use explicitly error callable returned without starting response,issue,negative,positive,positive,positive,positive,positive
1742714807,"@sihanwang41 what is the status of Multi Apps in Ray Serve? The documentation still shows only the `serve.run` method with centralised YAML file which I am not sure whether it is as flexible as the old `.deploy()` capability when you have multiple requests coming in parallel. 

For example: every time a user presses a button on a UI, I want to deploy a model on a shared Ray Cluster. Using the old API, I could just call `deploy()`. Now I need to retrieve the yaml file, update it, and re-run `serve.run` using that YAML file.

Now, what happens if two requests come at the same time, and both retrieve the original YAML file? Will they over-write and kill each other as they are unaware of each other at deployment time? ",status ray serve documentation still method file sure whether flexible old capability multiple coming parallel example every time user button want deploy model ray cluster old could call deploy need retrieve file update file two come time retrieve original file kill unaware deployment time,issue,negative,positive,positive,positive,positive,positive
1742446632,"Sorry for the late review. I'm reviewing it now.

Regarding CI, we run tests on AWS so we need an aws instance with intel GPUs.",sorry late review regarding run need instance,issue,negative,negative,negative,negative,negative,negative
1742405346,"Lint failure

```


Mon Oct  2 03:10:47 UTC 2023 Flake8....
--
  | python/ray/autoscaler/_private/spark/node_provider.py:4:1: F401 'threading' imported but unused
  | python/ray/autoscaler/_private/spark/node_provider.py:5:1: F401 'time' imported but unused
  | python/ray/autoscaler/_private/spark/node_provider.py:211:89: E501 line too long (91 > 88 characters)
  | python/ray/util/spark/cluster_init.py:2:1: F401 'tempfile' imported but unused

```",lint failure mon flake unused unused line long unused,issue,negative,negative,negative,negative,negative,negative
1742232428,"Hi @jjyao @scottsun94, I noticed that this issue hasn't been resolved yet. Can I take this on?

My thought is use [is_initialized](https://sourcegraph.com/github.com/ray-project/ray@master/-/blob/python/ray/_private/worker.py?L2082) API to check if `ray.init` has been called yet, and if so, add a warning message [here](https://sourcegraph.com/github.com/ray-project/ray/-/blob/python/ray/scripts/scripts.py?L860&signin).",hi issue resolved yet take thought use check yet add warning message,issue,negative,neutral,neutral,neutral,neutral,neutral
1742223312,"@scottjlee I tried to fix this but I just realized #39960 seems to be a more generalized way to solve this issue, so probably we should wait for his approach to land.

Previously I tried to make BlocksToBatchesMapTransformFn.__call__() emit the empty block in user specified format in #39580. In the original code, it seems that when emitting the empty_block, the empty_block is not converted to `self._batch_format` but uses the format of the first block, whereas `formatted_batch_iter` gets converted to the specified format. I was trying to make this function always return blocks using the user specified format, which makes reduce() function doesn't get heterogeneous blocks as its inputs and can also solve this issue.
```
        empty_block = BlockAccessor.for_block(first).builder().build()
        # Don't hold the first block in memory, so we reset the reference.
        first = None

        # Ensure that zero-copy batch views are copied so mutating UDFs don't error.
        formatted_batch_iter = batch_blocks(
            blocks=blocks,
            stats=None,
            batch_size=self._batch_size,
            batch_format=self._batch_format,
            ensure_copy=self._ensure_copy,
        )

        first = next(formatted_batch_iter, None)
        if first is None:
            # If the input blocks are all empty, then yield an empty block with same
            # format as the input blocks.
            return [empty_block]
        else:
            return itertools.chain([first], formatted_batch_iter)
```
However, that cause a few test to fail, and I still don't quite understand why. If you got time, would you please give me some hint about where it could go wrong?",tried fix generalized way solve issue probably wait approach land previously tried make emit empty block user format original code converted format first block whereas converted format trying make function always return user format reduce function get heterogeneous also solve issue first hold first block memory reset reference first none ensure batch copied error first next none first none input empty yield empty block format input return else return first however cause test fail still quite understand got time would please give hint could go wrong,issue,negative,positive,neutral,neutral,positive,positive
1742186687,"> That makes sense. Would it be possible if I could have a go at it?

Absolutely, thanks!
",sense would possible could go absolutely thanks,issue,negative,positive,neutral,neutral,positive,positive
1742071010,"I agree, there should at least be more comprehensive error message or clear point in documentation that you can`t just plug an algorithm and it learns, as with almost all other algorithms. And that you have to modify an env.

Also please get rid of extra dots in documentation example. They cause SyntaxError
![extra dots error](https://github.com/ray-project/ray/assets/60827442/3a1a49bb-4f3d-4a55-b2fb-7a7fd27822b1)

",agree least comprehensive error message clear point documentation plug algorithm almost modify also please get rid extra documentation example cause extra error,issue,negative,negative,neutral,neutral,negative,negative
1741971891,"Generally it'd be good to write more comments, especially the high level design and how things work together. This can help future users understand and maintain the code.",generally good write especially high level design work together help future understand maintain code,issue,positive,positive,positive,positive,positive,positive
1741965467,"+1 same issue for me. It happens, when one of the worker nodes is uninitialised or is spinning up. You can see the issue is with the response got from autoscaler status, and the dashboard crashes.",issue one worker spinning see issue response got status dashboard,issue,negative,neutral,neutral,neutral,neutral,neutral
1741965203,"+1 same issue for me. Even with systems on cloud (3rd party cloud, not AWS/GCS/Azure). Opened all ports, sometimes it gets connected, some times it shows uninitialized.",issue even cloud party cloud sometimes connected time,issue,negative,neutral,neutral,neutral,neutral,neutral
1741755778,"> > Windows wheel can't be installed in Python 3.11
> 
> Hi @nrudakov - could you provide a bit more in your env setup and ray version? And what's the error message?

@rickyyx I have opened an issue here https://github.com/ray-project/ray/issues/38300",wheel ca python hi could provide bit setup ray version error message issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1741686348,That makes sense. Would it be possible if I could have a go at it? ,sense would possible could go,issue,negative,neutral,neutral,neutral,neutral,neutral
1741673886,"A unrelated note.

You mind adding the  `hacktoberfest-accepted` label to this PR once merged

Apologies, i could sworn i thought this repo was in hacktoberfest. it showed up on my search results.

If its too much of a ask. its okay!",unrelated note mind label could sworn thought search much ask,issue,negative,positive,positive,positive,positive,positive
1741672801,"sure thing. i noticed they both use the same edge case
```python
            # TODO(https://github.com/ray-project/ray/issues/31362)
            if py_version == (3, 11) and sys_platform != ""linux"":
                continue
```

shall i remove those as well?
And should they go through all the possibilities like the changes made to `test_get_wheel_filename`",sure thing use edge case python continue shall remove well go like made,issue,positive,positive,positive,positive,positive,positive
1741642503,"> Windows wheel can't be installed in Python 3.11

Hi @nrudakov - could you provide a bit more in your env setup and ray version? And what's the error message?",wheel ca python hi could provide bit setup ray version error message,issue,negative,neutral,neutral,neutral,neutral,neutral
1741594144,"Yes, and thinking about this more another scenario is if the user accidentally returns a single humongous row or something like this, which isn't an uncommon error when working with tensor data.

Even just playing around with a couple examples, I found a bug where `map_batches` doesn't seem to split blocks into the right size, and without this kind of log it would be pretty hard to identify these sort of issues.",yes thinking another scenario user accidentally single row something like uncommon error working tensor data even around couple found bug seem split right size without kind log would pretty hard identify sort,issue,negative,positive,positive,positive,positive,positive
1741582061,"It sounds like from this discussion, we want to add this as an ""insurance policy,"" i.e. users really shouldn't be running into this issue, but in case they do, we log a warning with a suggestion to increase parallelism. Since the current changes will only emit a warning if the block size exceeds the configured target size (very rare), there's little downside to including this (i.e. no excessive spam, it's printed only when truly needed), so I think this is a beneficial addition? @amogkam @raulchen 

Since we only log the warning when the block size exceeds the configured target size (which should rarely be happening), and we always log the info to the data-specific log file, I think users wouldn't be getting spammed. Later, we can add additional statistics like min/avg/max block size to the dashboard, like Amog suggested.
",like discussion want add insurance policy really running issue case log warning suggestion increase parallelism since current emit warning block size target size rare little downside excessive printed truly think beneficial addition since log warning block size target size rarely happening always log log file think would getting later add additional statistic like block size dashboard like,issue,negative,positive,neutral,neutral,positive,positive
1741565879,@rkooo567 should we downgrade this to p1 per your latest comment then?,downgrade per latest comment,issue,negative,positive,positive,positive,positive,positive
1741563002,"Yes, our open source license with Algolia allows a single index and we create it with latest. We don't have plans to upgrade this in the near future. ",yes open source license single index create latest upgrade near future,issue,positive,positive,positive,positive,positive,positive
1741554513,@justinvyu could you add to the PR description explaining the problem/fix?,could add description explaining,issue,negative,neutral,neutral,neutral,neutral,neutral
1741535495,"> Yep, that's correct, this is using the Ray Client -- I can wrap the code in a remote function, and was able to use the Ray Job CLI to submit.
> 
> Technically, is the Data/Client incompatibility related to the use of generators within Ray Data, or something else? (didn't see technical info on the architecture limitations there and am curious for other workloads I use the client for)

@spolcyn Correct, we believe this is an incompatibility with the streaming generator integration introduced to Ray Data in 2.7",yep correct ray client wrap code remote function able use ray job submit technically incompatibility related use within ray data something else see technical architecture curious use client correct believe incompatibility streaming generator integration ray data,issue,positive,positive,neutral,neutral,positive,positive
1741534772,`Dataset().groupby().map_groups()` is also not working for me neither on `2.6.3` or `2.7.0` from within the script. It does work with `ray job submit` CLI. `.map_batches()` does work without `ray job submit` on `2.6.3`. ,also working neither within script work ray job submit work without ray job submit,issue,negative,neutral,neutral,neutral,neutral,neutral
1741533360,"Yep, that's correct, this is using the Ray Client -- I can wrap the code in a remote function, and was able to use the Ray Job CLI to submit.

Technically, is the Data/Client incompatibility related to the use of generators within Ray Data, or something else? (didn't see technical info on the architecture limitations there and am curious for other workloads I use the client for)",yep correct ray client wrap code remote function able use ray job submit technically incompatibility related use within ray data something else see technical architecture curious use client,issue,positive,positive,neutral,neutral,positive,positive
1741529935,"It looks like this is being run on Ray Client? If that's the case, it is likely the the combination of Ray Client + Ray Data which is the issue. Due to architectural limitations on Ray Client, there is limited support for Ray Data on Ray Client unfortunately ([more info](https://docs.ray.io/en/latest/cluster/running-applications/job-submission/ray-client.html#when-to-use-ray-client)). 

Is there a way you can avoid Ray Client for your use case? Another potential workaround could be to wrap the data code with a remote function and call it. ",like run ray client case likely combination ray client ray data issue due architectural ray client limited support ray data ray client unfortunately way avoid ray client use case another potential could wrap data code remote function call,issue,negative,negative,negative,negative,negative,negative
1741501627,"Here's a repro script:
```
import ray

connect_to_ray()  # this function should connect to a remote ray instance

def burn_cpu(record):
    while True:
        pass

dataset = ray.data.range(20)
dataset = dataset.repartition(10)
dataset = dataset.map(fn=burn_cpu)
dataset.write_parquet(path=""local_output"")

",script import ray function connect remote ray instance record true pas,issue,negative,positive,positive,positive,positive,positive
1741406376,"But yes, I should properly look at go/flaky and jail the list of existing flaky tests first",yes properly look jail list flaky first,issue,negative,positive,neutral,neutral,positive,positive
1741405810,I'm kind of choosing the most difficult ones on purpose ;). If we can beat this then the trophy is only a matter of time.,kind choosing difficult purpose beat trophy matter time,issue,negative,positive,neutral,neutral,positive,positive
1741399644,"I see there is a TODO here on refactoring BlockAccessor.zip() to work with N other blocks: 
https://github.com/ray-project/ray/blob/88d595b2df0ecd09eb48b18a4b0769c83d9023a4/python/ray/data/_internal/execution/operators/zip_operator.py#L228-L230

Would be happy to help out on ^ in a follow up. And perhaps get some sort of [block normalization](https://github.com/ray-project/ray/pull/39960) implemented when handling mismatched blocks/Accessors.",see work would happy help follow perhaps get sort block normalization handling,issue,positive,positive,positive,positive,positive,positive
1741347855,"@alexeykudinkin yes and no, I'm working on adding a linear backoff to the proxy so if the same issue happen again it will be mitigated automatically. There's a PR attached to this https://github.com/ray-project/ray/pull/39738

This is taking longer bc we are also doing a refactoring of the proxy state and making test better at the same time. Reopen for now. Eta is next Tuesday.",yes working linear proxy issue happen automatically attached taking longer also proxy state making test better time reopen eta next,issue,positive,positive,positive,positive,positive,positive
1741344100,"@GeneDer we can close this one, provided that we figured the issue was caused by the worker shell script taking too long to start (due to misconfiguration of the DNS on the host, sudo was taking 10s to execute)",close one provided figured issue worker shell script taking long start due misconfiguration host taking execute,issue,negative,negative,neutral,neutral,negative,negative
1741333434,"Failed tests:
- serve:test_multiplex 
- serve:test_proxy_state
- serve:test_max_replicas_per_node   
- serve:test_new_handle_api  
- serve:test_new_handle_api_set_via_env_var
- serve:test_persistence 
- rllib:examples/nested_action_spaces_ppo_torch
- air:test_legacy_dataset_config  

These are all unrelated. Linkcheck failure is unrelated because this PR doesn't touch the docs.",serve serve serve serve serve serve air unrelated failure unrelated touch,issue,negative,negative,negative,negative,negative,negative
1741313233,"In order to support this, ideally we have better support for bundle grouping in placement groups (i.e., all bundles should land on a pod with the same label, regardless of what the label is).

Instead, we currently have the following solution. 

1. On each TPU pod slice, we designate one node in the pod slice to be a ""tpu-pod-slice-head"". This should be designated either with a custom resource or label (new feature, not totally baked). 
2. On each node of the TPU pod slice, a custom resource with $TPU_POD_ID should be specified.
3. So one node should look like: `ray start --address --resources={""tpu-pod-slice-head-{TPU_TYPE}"": 1, $TPU_POD_ID: 4}`, rest of nodes should look like `ray start --address --resources={$TPU_POD_ID: 1}`
4. Then, you make sure your deployment always lands on a head node (via the serve deployment API)
5. The deployment then detects the TPU_POD_ID on that head node, and determines how many TPUs are in the pod-slice (say, X). Then provisions X actors assigned to those TPU_POD_IDs.

```python
@serve.deployment(ray_actor_options={""resources"": {""tpu-pod-slice-head-TPU_TYPE"": 1}}
class LLMServer:
    def __init__(self):
        pod_id = os.environ.get(""TPU_POD_ID"")
        num_tpus_in_pod = calculate_num_tpus_in_pod()
        TPUActor = Actor.options(resources={pod_id: 1, ""tpu"": 1})
        self._actor_group = [TPUActor.remote() for _ in range(num_tpus_in_pod)]

    def generate(self, request):
        return [actor.generate(request) for actor in self._actor_group]

    def group_generate_handler(self, request):
        results = await self.generate(request)
        # join and postprocess results, e.g. beamsearch etc.
```",order support ideally better support bundle grouping placement land pod label regardless label instead currently following solution pod slice designate one node pod slice either custom resource label new feature totally baked node pod slice custom resource one node look like ray start address rest look like ray start address make sure deployment always head node via serve deployment deployment head node many say assigned python class self range generate self request return request actor self request await request join,issue,positive,positive,positive,positive,positive,positive
1741270586,@jjyao would you mind giving codeowner approval to the `gcs.proto` change?,would mind giving approval change,issue,positive,neutral,neutral,neutral,neutral,neutral
1741249974,"> I noticed that random_shuffle can fuse with the map tasks. However, I believe that shuffle and map are still separate Ray tasks with different scheduling strategies. Correct me if I'm wrong. Thanks!

That's right, we currently have a limited operator fusion rule, where we can merge MapOperator->AllToAllOperators (such as shuffle). Note that this isn't supported in the other direction yet (AllToAllOperator->MapOperator). I believe the scheduling strategy of the fused operator will be used (in the above case, fuses into a new AllToAllOperator).",fuse map however believe shuffle map still separate ray different correct wrong thanks right currently limited operator fusion rule merge shuffle note direction yet believe strategy fused operator used case new,issue,negative,positive,neutral,neutral,positive,positive
1741247387,I might add the script as a unit test later.,might add script unit test later,issue,negative,neutral,neutral,neutral,neutral,neutral
1741213621,"A few more todo items:

- [x] We are dropping `sphinx-external-toc`, so there are a bunch of orphaned pages; we need to build a new toctree with these pages included to be displayed at the landing page for the docs.
- [x] Move the CSAT widget out of a custom template. There's a [`pydata-sphinx-theme` layout guide](https://pydata-sphinx-theme.readthedocs.io/en/latest/user_guide/layout.html) that shows where different things can be placed; the HTML for this can just be put in the sphinx configuration instead.
- [x] Sort out syntax highlighting. In addition to pygments, we are also loading highlight.js in multiple places - without extra work highlight.js will not work for dark/light theme switches.",dropping bunch need build new included displayed landing page move custom template layout guide different put sphinx configuration instead sort syntax addition also loading multiple without extra work work theme,issue,negative,positive,neutral,neutral,positive,positive
1741201766,@can-anyscale please sanity check that I'm not breaking any best practices in the buildkite job definition :),please sanity check breaking best job definition,issue,positive,positive,positive,positive,positive,positive
1741158237,"please use release tags, like `rayproject/ray:2.7.0-py310-cu121` or `rayproject/ray-ml:2.5.0-py310-gpu`, without the commit prefix.

ones with commit prefix are temporary and will be cleaned up periodically. docker hub has issues with too many tags in one repo, so we cannot keep them for ever. (and docker hub recently introduced an UI issue where a deleted tag can still be shown in the Web UI..)",please use release like without commit prefix commit prefix temporary periodically docker hub many one keep ever docker hub recently issue tag still shown web,issue,negative,positive,positive,positive,positive,positive
1741046831,"I noticed that random_shuffle can fuse with the map tasks. However, I believe that shuffle and map are still separate Ray tasks with different scheduling strategies.  Correct me if I'm wrong. Thanks!",fuse map however believe shuffle map still separate ray different correct wrong thanks,issue,negative,negative,neutral,neutral,negative,negative
1741002402,Using `RAY_memory_monitor_refresh_ms=0 ray start` on every node (manually) not only the head node works so I'm closing this one.,ray start every node manually head node work one,issue,negative,neutral,neutral,neutral,neutral,neutral
1740987288,Wanted to test on `2.7.0` but for some reason the conda package is still at `2.6.3` after 2 Weeks...,test reason package still,issue,negative,neutral,neutral,neutral,neutral,neutral
1740970505,Same issue here. I'm forced to stay on Pydantic 1.x because of Ray and it's hard to work with with FastAPI,issue forced stay ray hard work,issue,negative,negative,negative,negative,negative,negative
1740931974,Chiming in that we had to backport our application to pydantic v1.10 to work with ray data. ,application work ray data,issue,negative,neutral,neutral,neutral,neutral,neutral
1740226653,Make sense. ill go to my previous changes and make a pr.,make sense ill go previous make,issue,negative,negative,negative,negative,negative,negative
1740210403,"I'm not sure. One reason to add these logs is to surface cases where block
splitting didn't work for whatever reason.

On Thu, Sep 28, 2023, 1:48 PM Scott Lee ***@***.***> wrote:

> ***@***.**** commented on this pull request.
> ------------------------------
>
> In python/ray/data/_internal/execution/streaming_executor_state.py
> <https://github.com/ray-project/ray/pull/39656#discussion_r1340651875>:
>
> > +        target_max_block_size = DataContext.get_current().target_max_block_size
> +
> +        if self.first_block_size_bytes > (
> +            BLOCK_SIZE_TO_MAX_TARGET_RATIO * target_max_block_size
> +        ):
> +            logger.get_logger().warning(
> +                f""{self.op.name} in-memory block size of ""
> +                f""{(self.first_block_size_bytes / 2**20):.2f} MB is significantly ""
> +                f""larger than the maximium target block size of ""
> +                f""{(target_max_block_size / 2**20):.2f} MB.""
> +            )
> +        else:
> +            logger.get_logger().info(
> +                f""{self.op.name} in-memory block size: ""
> +                f""{(self.first_block_size_bytes / 2**20):.2f} MB""
> +            )
>
> @ericl <https://github.com/ericl> what was the original scenario where
> this issue appeared? The block splitting should be ensuring we don't get
> block sizes larger than ``target_max_block_size`, are there operators where
> this is not being handled?
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/ray-project/ray/pull/39656#discussion_r1340651875>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAADUSVXZDLYBGJHCZGFTC3X4XPALANCNFSM6AAAAAA4XKGVOU>
> .
> You are receiving this because you were mentioned.Message ID:
> ***@***.***>
>
",sure one reason add surface block splitting work whatever reason lee wrote pull request block size significantly target block size else block size original scenario issue block splitting get block size handled reply directly view id,issue,negative,positive,positive,positive,positive,positive
1740187769,"> As I recall the wheel filename is autogenerated by the build and we don't have control over it

we do have control, but the file name is conforming with the wheel path standard. the s3 path is just our own thing.

- for the `universal`, maybe just throw an exception?
- I feel that you can just fix the `m1` part and call it a day.

because it is python standard, it does not really change much over time. so I am not so worried about docs consistency. if we already use sphinx extension to generate docs, then it is fine. if it needs a lot of extra work and need to introduce many additional dependencies and set up to make it work, then I think it is probably an overkill.

the real issue is test coverage. like if we expect that `get_wheel_filename()` function to work on m1, then we should have test that cover that use case.


also.. IMO we should list unstable `3.0.0.dev0` in docs.. instead, should have a more formal ""nightly"" build, with date as release numbers, on a consistent git commit, with )(some) test results attached with it, and have a list of urls/filenames in the release.  -- but that is a longer discussion..",recall wheel build control control file name wheel path standard path thing universal maybe throw exception feel fix part call day python standard really change much time worried consistency already use sphinx extension generate fine need lot extra work need introduce many additional set make work think probably real issue test coverage like expect function work test cover use case also list unstable dev instead formal nightly build date release consistent git commit test attached list release longer discussion,issue,negative,positive,positive,positive,positive,positive
1740128022,"> What are examples of operations other than map and read operations?

I examined the code and observed that there are two kinds of operators: Physical Operators and Logical Operators. And here is my summary of some operations:


1. For the [logic filter operation](https://sourcegraph.com/github.com/ray-project/ray/-/blob/python/ray/data/_internal/logical/operators/map_operator.py?L167), it uses the physical map operator underneath, so it use the ``SPREAD`` scheduling strategy.

2. For the split operation, it has its [physical split operator](https://sourcegraph.com/github.com/ray-project/ray/-/blob/python/ray/data/_internal/execution/operators/output_splitter.py?L19), so it uses the ``DEFAULT`` scheduling strategy.

3. For the [logic sort operation](https://sourcegraph.com/github.com/ray-project/ray/-/blob/python/ray/data/_internal/logical/operators/all_to_all_operator.py?L106), it uses the physical AllToAllOperator underneath, so it uses the ``DEFAULT`` scheduling strategy.

4. For the [logic shuffle operation](https://sourcegraph.com/github.com/ray-project/ray/-/blob/python/ray/data/_internal/logical/operators/all_to_all_operator.py?L55), it uses the physical AllToAllOperator underneath, so it uses the ``DEFAULT`` scheduling strategy.

Therefore, I added Split, Sort, and Shuffle as examples.

> let's also add a link to https://docs.ray.io/en/latest/ray-core/scheduling/index.html#scheduling-strategies, so readers can find out what theses strategies actually mean.

Done.



",map read code two physical logical summary logic filter operation physical map operator underneath use spread strategy split operation physical split operator default strategy logic sort operation physical underneath default strategy logic shuffle operation physical underneath default strategy therefore added split sort shuffle let also add link find thesis actually mean done,issue,negative,negative,neutral,neutral,negative,negative
1740121134,"@stevenhubhub can you try changing the `RAY_ADDRESS` port to `8265` instead of `6379`?  the port 6379 is used internally by Ray workers, but port 8265 is used for APIs such as the dashboard server and the Jobs API server.",try port instead port used internally ray port used dashboard server server,issue,negative,neutral,neutral,neutral,neutral,neutral
1740096037,"I don't have experience with sphinx extensions, but if it works, your approach sounds great! You can make the call on where `wheels.yaml` should live, I don't think we have any strict guidelines here.

@aslonnie any concerns about the above approach? The summary of the problem this solves is that we have code to generate the wheel URLs for use by runtime_env downloading, but today it needs to be manually kept in sync with the actual wheel URLs.  (Not sure who manages the wheel uploads. As I recall the wheel filename is autogenerated by the build and we don't have control over it)",experience sphinx work approach great make call live think strict approach summary problem code generate wheel use today need manually kept sync actual wheel sure wheel recall wheel build control,issue,positive,positive,positive,positive,positive,positive
1740095659,I could also expand the sphinx extension to render out the entire table. That way the only thing that would need updating is `wheels.yaml`,could also expand sphinx extension render entire table way thing would need,issue,negative,neutral,neutral,neutral,neutral,neutral
1740089968,"Looks like there is some docker failure now, any idea on how to fix that?",like docker failure idea fix,issue,negative,negative,negative,negative,negative,negative
1740081554,"Never mind, it turns out that pulling `detached` out of Java was pretty straightforward. I removed it in Java and the `ServeControllerAvatar`. Let's see if the tests pass.",never mind turn detached pretty straightforward removed let see pas,issue,positive,positive,positive,positive,positive,positive
1740076762,"> Java failed, very possibly related

The Java code uses `detached` throughout the codebase. It'll likely take a lot of time to pull it out of there. I added the `detached` parameter back to the `ServeControllerAvatar` and raised an error inside its constructor if the value is `False`. Let's see if this works.",possibly related code detached throughout likely take lot time pull added detached parameter back raised error inside constructor value false let see work,issue,negative,negative,negative,negative,negative,negative
1740075562,"@rkooo567 not sure if we still need this. If so, would you mind taking a look? Otherwise, feel free to close this PR.",sure still need would mind taking look otherwise feel free close,issue,positive,positive,positive,positive,positive,positive
1740071326,"@zcin @edoakes @sihanwang41 I updated this PR with a new implementation. Instead of counting bytes, the metric now counts total transmissions. The metric is tagged by namespace and state, so we know what the objects are being sent. Please take another look.",new implementation instead counting metric total metric tagged state know sent please take another look,issue,negative,positive,neutral,neutral,positive,positive
1740046272,"okay i can give it a shot.

How does this implementation sound

1. Move all existing wheel URLs in `installation.rst` to a `wheels.yaml` 
In this format
```yaml
- Linux Python 3.10 (x86_64): https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-3.0.0.dev0-cp310-cp310-manylinux2014_x86_64.whl
- Linux Python 3.9 (x86_64): https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-3.0.0.dev0-cp39-cp39-manylinux2014_x86_64.whl
- Linux Python 3.8 (x86_64): https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-3.0.0.dev0-cp38-cp38-manylinux2014_x86_64.whl
# ....
```
_Not sure where to place the `wheels.yaml` file_

2. Create a sphinx extension that will replace the keyword ``` :wheellink:`Linux Python 3.10 (x86_64)` ``` to the required hyperlink with provided label
3.  Create a method in [utils.py](https://github.com/ray-project/ray/blob/bb892a6bd0224ab3ae11859268ddef01cbb68e71/python/ray/_private/utils.py) that will read and parse the `wheels.yaml` file. It will return the data as a `dict`
4. Refactor `get_wheel_filename()` such that it will use the provided arguments to find right link and return the filename

So for any new updates. They only update a person need to do are always in  `installation.rst` for formatting and `wheels.yaml` for the actual link",give shot implementation sound move wheel format python python python sure place create sphinx extension replace python provided label create method read parse file return data use provided find right link return new update person need always actual link,issue,positive,positive,positive,positive,positive,positive
1740018016,"> That is because `sys.getsizeof` only reports the memory to which this object is accounted for not including nested object inferring to.

Thanks for the catch @sihanwang41! This is pretty surprising. I dug deeper into this, and I don't think there's a safe or straightforward way to calculate the true size of an object in Python. I'll have to change the overall implementation of this metric.

My plan is to track the number of broadcasts per namespace instead. It's not ideal, but it'll at least give us an understanding of when the `LongPollHost` is sending traffic.
",memory object object thanks catch pretty surprising dug think safe straightforward way calculate true size object python change overall implementation metric plan track number per instead ideal least give u understanding sending traffic,issue,positive,positive,positive,positive,positive,positive
1739940712,"Here's the traceback:

```
ray up python/ray/autoscaler/aws/example-minimal.yaml
hello
Cluster: aws-example-minimal

2023-09-28 10:16:30,801	INFO util.py:375 -- setting max workers for head node type to 0
2023-09-28 10:16:30,801	INFO util.py:379 -- setting max workers for ray.worker.default to 2
Traceback (most recent call last):
  File ""/Users/hongchao.deng/miniforge3/envs/ray/bin/ray"", line 8, in <module>
    sys.exit(main())
  File ""/Users/hongchao.deng/miniforge3/envs/ray/lib/python3.9/site-packages/ray/scripts/scripts.py"", line 2499, in main
    return cli()
  File ""/Users/hongchao.deng/miniforge3/envs/ray/lib/python3.9/site-packages/click/core.py"", line 1157, in __call__
    return self.main(*args, **kwargs)
  File ""/Users/hongchao.deng/miniforge3/envs/ray/lib/python3.9/site-packages/click/core.py"", line 1078, in main
    rv = self.invoke(ctx)
  File ""/Users/hongchao.deng/miniforge3/envs/ray/lib/python3.9/site-packages/click/core.py"", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/Users/hongchao.deng/miniforge3/envs/ray/lib/python3.9/site-packages/click/core.py"", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/Users/hongchao.deng/miniforge3/envs/ray/lib/python3.9/site-packages/click/core.py"", line 783, in invoke
    return __callback(*args, **kwargs)
  File ""/Users/hongchao.deng/miniforge3/envs/ray/lib/python3.9/site-packages/ray/autoscaler/_private/cli_logger.py"", line 856, in wrapper
    return f(*args, **kwargs)
  File ""/Users/hongchao.deng/miniforge3/envs/ray/lib/python3.9/site-packages/ray/scripts/scripts.py"", line 1299, in up
    create_or_update_cluster(
  File ""/Users/hongchao.deng/miniforge3/envs/ray/lib/python3.9/site-packages/ray/autoscaler/_private/commands.py"", line 311, in create_or_update_cluster
    config = _bootstrap_config(config, no_config_cache=no_config_cache)
  File ""/Users/hongchao.deng/miniforge3/envs/ray/lib/python3.9/site-packages/ray/autoscaler/_private/commands.py"", line 383, in _bootstrap_config
    provider_cls = importer(config[""provider""])
  File ""/Users/hongchao.deng/miniforge3/envs/ray/lib/python3.9/site-packages/ray/autoscaler/_private/providers.py"", line 29, in _import_aws
    from ray.autoscaler._private.aws.node_provider import AWSNodeProvider
  File ""/Users/hongchao.deng/miniforge3/envs/ray/lib/python3.9/site-packages/ray/autoscaler/_private/aws/node_provider.py"", line 9, in <module>
    import botocore
ModuleNotFoundError: No module named 'botocore'
```",ray hello cluster setting head node type setting recent call last file line module main file line main return file line return file line main file line invoke return file line invoke return file line invoke return file line wrapper return file line file line file line importer provider file line import file line module import module,issue,negative,positive,neutral,neutral,positive,positive
1739879446,@angelinalg could you stamp for doc codeowners please?,could stamp doc please,issue,negative,neutral,neutral,neutral,neutral,neutral
1739879018,"I need to revert this, sorry. This test has been failing consistently on the master branch after this is merged. ",need revert sorry test failing consistently master branch,issue,negative,negative,negative,negative,negative,negative
1739865630,"@edoakes remove old grpc is already merged, can you directly rebase and move the test_deployment_state into the file? :) ",remove old already directly rebase move file,issue,negative,positive,neutral,neutral,positive,positive
1739840810,"1. I feel dashboard integration is P1. I don't expect API changes (maybe in runtime env?).
2. AFAIK we've only ever gotten this to work in ray local mode. so there's technical complexity to get it working with a real cluster / ray worker.",feel dashboard integration expect maybe ever gotten work ray local mode technical complexity get working real cluster ray worker,issue,negative,positive,neutral,neutral,positive,positive
1739830299,"I am getting erratic code coverage reports from pytest-- it's occasionally reporting that the function that's passed to ray as a remote has not been covered. To fix this, I was hoping to initialize ray in the test code using local_mode=True as suggested above. But now I get this warning: `DeprecationWarning: local mode is an experimental feature that is no longer maintained and will be removed in the future.For debugging consider using Ray debugger.` That's not cool. How are we supposed to ensure our unit test code coverage? ",getting erratic code coverage occasionally function ray remote covered fix initialize ray test code get warning local mode experimental feature longer removed consider ray cool supposed ensure unit test code coverage,issue,negative,positive,neutral,neutral,positive,positive
1739807748,"> Looks like some of the CI tests are failing due to the new bigquery dependencies - where would be the appropriate place to add that?

I don't think we want to make BigQuery a hard dependency for Ray. Could you lazily import the BigQuery dependency like we do for PIL in `ImageDatasource`?

https://github.com/ray-project/ray/blob/ca95274e32189ef633c96b82f0237b0233dd8444/python/ray/data/datasource/image_datasource.py#L66-L66

https://github.com/ray-project/ray/blob/ca95274e32189ef633c96b82f0237b0233dd8444/python/ray/data/datasource/image_datasource.py#L72-L81",like failing due new would appropriate place add think want make hard dependency ray could lazily import dependency like,issue,negative,negative,neutral,neutral,negative,negative
1739777150,"Let me rebase and check, now that the dust has settled with the perf variations around grpc/jemalloc.",let rebase check dust settled around,issue,negative,neutral,neutral,neutral,neutral,neutral
1739741491,"~Oh looks like it's just `python/requirements.txt` not `requirements/requirements.txt`?~

 No, that one doesn't have `ray[default]` either",like one ray default either,issue,negative,neutral,neutral,neutral,neutral,neutral
1739739976,"@krfricke 

https://github.com/architkulkarni/ray/blob/1fd210222b79fca568bc341f1cd915f842f998ea/python/setup.py#L235-L237
```
# If you're adding dependencies for ray extras, please
# also update the matching section of requirements/requirements.txt
# in this directory
```
It seems the file structure has changed. I couldn't find the relevant requirements.txt file for `ray[default]`, do you know where I should add it?",ray please also update matching section directory file structure could find relevant file ray default know add,issue,negative,positive,positive,positive,positive,positive
1739732493,@hongchaodeng could you please post the exact traceback here for the botocore not found error?,could please post exact found error,issue,negative,positive,positive,positive,positive,positive
1739723968,"> > what are those rllib tune metadata file changes?
> 
> please also check those files.
> 
> they are binary files, and probably should not be changed.
> 
> also not sure also if they should be checked in
> 
> @can-anyscale , I think we should have a limited whitelist of places that require changes, not via repo-wide text replace.

Interesting point, I think in the pervious process we would also change those file https://github.com/ray-project/ray/commit/2441e73d94a860c4f0385db94c31f3f09b2c73cf. But I do agree those shouldn't impact how things are working. Will revert the change and update the docs",tune file please also check binary probably also sure also checked think limited require via text replace interesting point think pervious process would also change file agree impact working revert change update,issue,positive,positive,positive,positive,positive,positive
1739713870,"> what are those rllib tune metadata file changes?

please also check those files.

they are binary files, and probably should not be changed.

also not sure also if they should be checked in


----

@can-anyscale , I think we should have a limited whitelist of places that require changes, not via repo-wide text replace.",tune file please also check binary probably also sure also checked think limited require via text replace,issue,negative,positive,positive,positive,positive,positive
1739711930,Reviewed last week as part of weekly triage of all open issues; will investigate more to decide priority by next ~Wednesday.,last week part weekly triage open investigate decide priority next,issue,negative,neutral,neutral,neutral,neutral,neutral
1739687035,"@cadedaniel @scv119 can you say a bit more about the context here? Specifically
1. Do we need dashboard integration, or any API changes?
2. Any insight as to why this issue is ""large"" as compared to ""small"" (if it is through runtime envs)",say bit context specifically need dashboard integration insight issue large small,issue,negative,negative,neutral,neutral,negative,negative
1739679413,"> Q: do you know which address was passed to the given issue?

Asked in the original issue 

",know address given issue original issue,issue,negative,positive,positive,positive,positive,positive
1739674764,Can you also make sure the verbosity options are described in the Ray documentation somewhere?,also make sure verbosity ray documentation somewhere,issue,negative,positive,positive,positive,positive,positive
1739674229,"@hongchaodeng some tests failures look potentially related, could you please take a look?

e.g.

bazel-out/k8-opt/bin/python/ray/tests/test_cli.runfiles/com_github_ray_project_ray/python/ray/tests/test_cli.py::test_ray_status_multinode[False]
RuntimeError: result.exception=status() missing 1 required positional argument: 'verbose' result.output=
Link to test: [../root/.cache/bazel/_bazel_root/1df605deb6d24fc8068f6e25793ec703/execroot/com_github_ray_project_ray/bazel-out/k8-opt/bin/python/ray/tests/test_cli.runfiles/com_github_ray_project_ray/bazel-out/k8-opt/bin/python/ray/tests/test_cli.runfiles/com_github_ray_project_ray/python/ray/tests/test_cli.py:948](https://github.com/ray-project/ray/blob/root/.cache/bazel/_bazel_root/1df605deb6d24fc8068f6e25793ec703/execroot/com_github_ray_project_ray/bazel-out/k8-opt/bin/python/ray/tests/test_cli.runfiles/com_github_ray_project_ray/bazel-out/k8-opt/bin/python/ray/tests/test_cli.runfiles/com_github_ray_project_ray/python/ray/tests/test_cli.py#L948)",look potentially related could please take look false missing positional argument link test,issue,negative,negative,negative,negative,negative,negative
1739669909,"Amazing, thanks for the investigation. The root cause and proposed fix sounds correct to me.",amazing thanks investigation root cause fix correct,issue,positive,positive,positive,positive,positive,positive
1739661107,"> Q: do you know which address was passed to the given issue? It means after we merge this PR, the user will still see the failure right?

oh no, we do `rsplit(s, 1) which will split to max 2 tokens. ",know address given issue merge user still see failure right oh split,issue,negative,negative,neutral,neutral,negative,negative
1739654891,"> how much more work/code would it be to refactor and directly run the HTTP handlers currently defined in the agent in the head instead? We'll want to move to this shortly, so maybe we can skip this intermediate step.

@edoakes Would we still want the routes on the dashboard agent to function? I.e. are you asking if we should proxy requests from the agent to the head instead of the other way around?",much would directly run currently defined agent head instead want move shortly maybe skip intermediate step would still want dashboard agent function proxy agent head instead way around,issue,negative,positive,neutral,neutral,positive,positive
1739642982,"@shrekris-anyscale I doubt there are many users directly querying the dashboard agent (vs. using the CLI or KubeRay), so I'm good w/ immediately deprecating this path.",doubt many directly querying dashboard agent good immediately path,issue,negative,positive,positive,positive,positive,positive
1739437614,"> @michaelhly I think we want to keep `ArrowBlockAccessor` and `PandasBlockAccessor` separate, in that `ArrowBlockAccessor` only accepts `Arrow` blocks, and `PandasBlockAccessor` only accepts `pandas` DataFrames.
> 
> Instead of modifying `ArrowBlockAccessor` to accept both Arrow and pandas blocks, I think we should instead convert all the blocks to Arrow, then use `ArrowBlockAccessor.aggregate_combined_blocks()` ([more details here](https://github.com/ray-project/ray/issues/39206#issuecomment-1734320744)) - similar to the implementation in #39817.

Okay. I am closing this PR in case someone else wants to pick it up.",think want keep separate arrow instead accept arrow think instead convert arrow use similar implementation case someone else pick,issue,negative,neutral,neutral,neutral,neutral,neutral
1739348887,"@krfricke , thank you for the example! How would you recommend this be done if the `Searcher` is `BayesOptSearch`, which will raise an exception if tune.grid_search is passed? (Although the `Repeater` [strategy](https://github.com/ray-project/ray/issues/7744#issuecomment-604080073) will technically work with `BayesOptSearch` it's not appropriate for nested cross-validation because the prior would be the average performance (b/c Repeater takes the average) and would include performance on hold-out data b/c it would be incorporating results from neighboring outer-splits. This is complex so I'm happy to explain further if you wish 😄  )",thank example would recommend done searcher raise exception although repeater strategy technically work appropriate prior would average performance repeater average would include performance data would neighboring complex happy explain wish,issue,positive,positive,neutral,neutral,positive,positive
1739250003,@architkulkarni this PR is ready. Would you mind taking a look?,ready would mind taking look,issue,negative,positive,positive,positive,positive,positive
1739004279,"> BTW, I have successfully used Repeater to do K-Fold as of yesterday. So it's definitely possible.

@adivekar-utexas , 
Would you please share how you got `Repeater` working with `BasicVariantGenerator`? Thank you!",successfully used repeater yesterday definitely possible would please share got repeater working thank,issue,positive,positive,positive,positive,positive,positive
1738916063,"> This node has an IP address of 10.13.23.83, but we cannot find a local Raylet with the same address.

I am also experiencing the same error message. The error comes from raylet binary, not a py file. Can you direct me on a module that should be updated to get better flexibility?",node address find local raylet address also error message error come raylet binary file direct module get better flexibility,issue,negative,positive,positive,positive,positive,positive
1738788703,@cool-RR yea true - but imo deprecated is just another word for supported :P,yea true another word,issue,negative,positive,positive,positive,positive,positive
1738722772,"@smolboii Sure but if you add that one, you might be ignoring actually useful deprecation warnings. That's why I prefer going over them one-by-one.",sure add one might actually useful deprecation prefer going,issue,positive,positive,positive,positive,positive,positive
1738717307,"@cool-RR thanks for that :). I ended up adding ""WARNING deprecation.py:50 -- DeprecationWarning:"" as that was common to all of my deprecation warnings.",thanks ended warning common deprecation,issue,negative,negative,neutral,neutral,negative,negative
1738671598,can you make sure to run release tests that are failing to verify it is fixed before merging it? ,make sure run release failing verify fixed,issue,negative,positive,positive,positive,positive,positive
1738670394,"cc @rickyyx @vitsai  https://github.com/ray-project/ray/pull/39901

Can you guys make sure to merge ^ asap or merge this PR? I cannot check this out because I will be in a flight, but if you cannot merge the forward fix by afternoon tmrw, please merge the revert PR

",make sure merge merge check flight merge forward fix afternoon please merge revert,issue,positive,positive,positive,positive,positive,positive
1738666901,can you make sure to cherry pick this to 2.7.1?,make sure cherry pick,issue,negative,positive,positive,positive,positive,positive
1738665729,"Q: do you know which address was passed to the given issue? It means after we merge this PR, the user will still see the failure right? ",know address given issue merge user still see failure right,issue,negative,negative,neutral,neutral,negative,negative
1738599609,"> hi, have u solved the problem?

I'm sorry, we didn't end up addressing this issue. I will try to reproduce the problem later to see if I can replicate it. 
Are you currently experiencing this issue as well?",hi problem sorry end issue try reproduce problem later see replicate currently issue well,issue,negative,negative,negative,negative,negative,negative
1738424169,"You can’t install dashboards if you are using pydantic >2

(I’m using poetry)

On Tue, Sep 26, 2023 at 17:35 shrekris-anyscale ***@***.***>
wrote:

> This also breaks compatibility with the dashboard too
>
> Could you give more details? What errors are you seeing?
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/ray-project/ray/issues/39722#issuecomment-1736482804>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAFZTGC2U27DAHE75H3MFVTX4NYEPANCNFSM6AAAAAA44CIBSY>
> .
> You are receiving this because you commented.Message ID:
> ***@***.***>
>
",install poetry tue wrote also compatibility dashboard could give seeing reply directly view id,issue,negative,positive,neutral,neutral,positive,positive
1738421544,@architkulkarni @richardliaw Could you please help to merge this PR if the buildkites reports no isssues related to this PR?,could please help merge related,issue,positive,neutral,neutral,neutral,neutral,neutral
1738410117,"Lint failed 
```


python/ray/tests/test_tls_auth.py:142:5: F841 local variable 'out' is assigned to but never used
--
  | 🚨 Error: The command exited with status 123

```

Could you also update the PR description to be more descriptive.",lint local variable assigned never used error command status could also update description descriptive,issue,negative,neutral,neutral,neutral,neutral,neutral
1738303925,"After looking more into how ml and rllib are using bazel tags, I don't think the one-dimensional filtering test type will work. I'm closing this and replacing with this solution https://github.com/ray-project/ray/pull/39928",looking think filtering test type work solution,issue,negative,neutral,neutral,neutral,neutral,neutral
1738299949,"Hi @architkulkarni 

I have fully examined the code and identified the obscure and complex logic that caused this bug.

## Default behavior will always print verbose logs

If we look at the `cli_logger.verbose()` definition:

```python
    def verbose(self, msg: str, *args: Any, **kwargs: Any):
        """"""Prints a message if verbosity is not 0.

        For arguments, see `_format_msg`.
        """"""
        if self.verbosity > 0:
            self.print(msg, *args, _level_str=""VINFO"", **kwargs)
```

This relies on the `verbosity()` func. Here is its definition:

```python
    @property
    def verbosity(self):
        if self._verbosity_overriden:
            return self._verbosity
        elif not self.pretty:
            return 999
        return self._verbosity
```

So this relies on both three variables: `_verbosity_overriden`, `pretty`, and `_verbosity`.

During `__init__()`, they are set as below:

```python
        self._verbosity = 0
        self._verbosity_overriden = False
        self.pretty = False
```

So based on the default values, `verbosity()` func returns 999 by default so `verbose()` will **ALWAYS PRINT**!

## Adding `add_click_logging_options` annotation changes default behavior

When you added `add_click_logging_options` annotation to any subcommand, the behavior changes!

```python
CLICK_LOGGING_OPTIONS = [
    click.option(
        ""--log-style"",
        required=False,
        type=click.Choice(cli_logger.VALID_LOG_STYLES, case_sensitive=False),
        default=""auto"",
        help=(
            ""If 'pretty', outputs with formatting and color. If 'record', ""
            ""outputs record-style without formatting. ""
            ""'auto' defaults to 'pretty', and disables pretty logging ""
            ""if stdin is *not* a TTY.""
        ),
    ),
    click.option(
        ""--log-color"",
        required=False,
        type=click.Choice([""auto"", ""false"", ""true""], case_sensitive=False),
        default=""auto"",
        help=(""Use color logging. Auto enables color logging if stdout is a TTY.""),
    ),
    click.option(""-v"", ""--verbose"", default=None, count=True),
]


def add_click_logging_options(f: Callable) -> Callable:
    for option in reversed(CLICK_LOGGING_OPTIONS):
        f = option(f)

    @wraps(f)
    def wrapper(*args, log_style=None, log_color=None, verbose=None, **kwargs):
        cli_logger.configure(log_style, log_color, verbose)
        return f(*args, **kwargs)

    return wrapper
```

When a subcommand is registered, something like below will be run:

```python
 cli_logger.configure(""auto"", ""auto"", None)
```

This `auto` value does the magic inside `configure()` function:

```python
    def configure(self, log_style=None, color_mode=None, verbosity=None):
        """"""Configures the logger according to values.""""""
        if log_style is not None:
            self._set_log_style(log_style)

        if color_mode is not None:
            self._set_color_mode(color_mode)

        if verbosity is not None:
            self._set_verbosity(verbosity)

        self.detect_colors()

    def _set_log_style(self, x):
        """"""Configures interactivity and formatting.""""""
        self._log_style = x.lower()
        self.interactive = _isatty()

        if self._log_style == ""auto"":
            self.pretty = _isatty()
        elif self._log_style == ""record"":
            self.pretty = False
            self._set_color_mode(""false"")
        elif self._log_style == ""pretty"":
            self.pretty = True
```
Here `self._set_log_style(""auto"")` will be run, and then `self.pretty = _isatty()` will be run. `_isatty()` will return `True` on an interactive shell.

So in this time, `verbosity()` return `0` and will **NOT print anything verbose** by default unless `-v` is set.

## Conclusion and solution

I think the best way to fix this now is to add `add_click_logging_options` to all subcommands. I saw that `up` has it now. Maybe it didn't when in May. But `get_head_ip` doesn't have it and I can still reproduce the bug.
",hi fully code obscure complex logic bug default behavior always print verbose look definition python verbose self message verbosity see verbosity definition python property verbosity self return return return three pretty set python false false based default verbosity default verbose always print annotation default behavior added annotation behavior python auto color without pretty logging auto false true auto use color logging auto color logging verbose callable callable option reversed option wrapper verbose return return wrapper registered something like run python auto auto none auto value magic inside configure function python configure self logger according none none verbosity none verbosity self interactivity auto record false false pretty true auto run run return true interactive shell time verbosity return print anything verbose default unless set conclusion solution think best way fix add saw maybe may still reproduce bug,issue,positive,positive,neutral,neutral,positive,positive
1738286377,"```(activity_probe_head) Cluster resource state is too large, skipping. Size: 5392391 Bytes. Product autoscaler will stop functioning before it receives an appropriate sized state.``` is the failure reason, which the linked PR will mitigate.",cluster resource state large skipping size product stop appropriate sized failure reason linked mitigate,issue,negative,positive,positive,positive,positive,positive
1738282211,"@zcin @edoakes Should we log a deprecation warning for users querying the Serve agent now, or one minor version from now? I'm in favor of doing it now.",log deprecation warning querying serve agent one minor version favor,issue,negative,negative,neutral,neutral,negative,negative
1738237143,"What's the usecases for multiple available_node_types here? Maybe just some high-level examples would be really helpful!
",multiple maybe would really helpful,issue,negative,positive,neutral,neutral,positive,positive
1738223559,"> is there a good way to avoid regressions like this in general for frontend?

One way we could do it is by adding component tests using React or end-to-end tests using Cypress. However, the UI is continuously changing, making it hard to assert what the correct UI should be. My suggestion is to use screenshots to ensure there are no major regressions, and it's also easy to implement.


cc @alanwguo Any idea?",good way avoid like general one way could component react cypress however continuously making hard assert correct suggestion use ensure major also easy implement idea,issue,positive,positive,positive,positive,positive,positive
1738208908,Looks like some of the CI tests are failing due to the new bigquery dependencies - where would be the appropriate place to add that?,like failing due new would appropriate place add,issue,negative,positive,positive,positive,positive,positive
1738135483,"> @sihanwang41 could you please separate (7) out into its own PR so this is purely removing the existing stuff?

done",could please separate purely removing stuff done,issue,negative,positive,positive,positive,positive,positive
1738060173,That would indeed be better if you can get it working! Definitely want to avoid manual updates where possible.  I think the only requirement would be that it continues to render nicely as a table in the docs.,would indeed better get working definitely want avoid manual possible think requirement would render nicely table,issue,positive,positive,positive,positive,positive,positive
1738032099,"Sure thing. I can make those changes.

I personally think it would be better to keep a running list of links that can be used in python and turn `get_wheel_filename()` to a method that would find those names based on the provide arguments and would throw an error if that name does not exist.

That way any changes, like support of  a python version, would propagate automatically without the need of manual change. 

I am not familiar with this code base, so you might have your reasons. I just thought i would thrown in a suggestion",sure thing make personally think would better keep running list link used python turn method would find based provide would throw error name exist way like support python version would propagate automatically without need manual change familiar code base might thought would thrown suggestion,issue,positive,positive,positive,positive,positive,positive
1738026649,"I'm not entirely sure how we verify this 😅. I'm 100% sure some Serve docs require us able to import grpc and run grpc requests, but given the test are passing we are probably good here",entirely sure verify sure serve require u able import run given test passing probably good,issue,positive,positive,positive,positive,positive,positive
1738018218,"@michaelhly I think we want to keep `ArrowBlockAccessor` and `PandasBlockAccessor` separate, in that `ArrowBlockAccessor` only accepts `Arrow` blocks, and `PandasBlockAccessor` only accepts `pandas` DataFrames. 

Instead of modifying `ArrowBlockAccessor` to accept both Arrow and pandas blocks, I think we should instead convert all the blocks to Arrow, then use `ArrowBlockAccessor.aggregate_combined_blocks()` ([more details here](https://github.com/ray-project/ray/issues/39206#issuecomment-1734320744)) - similar to the implementation in https://github.com/ray-project/ray/pull/39817.",think want keep separate arrow instead accept arrow think instead convert arrow use similar implementation,issue,negative,neutral,neutral,neutral,neutral,neutral
1737951922,"@N3XT14 yeah that's right, you can directly use the difference between `cur_usage.overall.object_store_memory_str()` and `limits.object_store_memory_str()`, which will be the excess spilled to external storage.",yeah right directly use difference excess external storage,issue,negative,positive,positive,positive,positive,positive
1737921300,"Thanks @scottjlee. So the change is to just make the output more descriptive by providing the necessary information regarding the portion of the memory.

I do have certain questions about the information to be provided in the output.
Eg: 3.75 GB / 1.65 GB Object Store Memory (2.10 GB spilled to external storage)

so 2.10 which is the difference will always be ""spilled to external storage"" or should I look for where the remaining storage is assigned/present?


",thanks change make output descriptive providing necessary information regarding portion memory certain information provided output object store memory external storage difference always external storage look storage,issue,positive,positive,neutral,neutral,positive,positive
1737916557,"Thanks for the screenshots. I think I can confirm the mmap file leaks from the following observations:
- Before: 0/2GB reported object store memory, 381GB used disk.
- After: 0/2GB reported object store memory, 516GB disk used.

That's 200GB of leaked plasma files, whereas normal object store allocation would at most create 2GB of files in /tmp (maybe additional 2GB for fallback allocation).

I'm going to tag this as P0 until further verification, since this seems like a possibly serious regression in a core component.",thanks think confirm file following object store memory used disk object store memory disk used plasma whereas normal object store allocation would create maybe additional fallback allocation going tag verification since like possibly serious regression core component,issue,positive,positive,neutral,neutral,positive,positive
1737899607,Working on it. Beg of next Mon is ETA,working beg next mon eta,issue,negative,neutral,neutral,neutral,neutral,neutral
1737875768,"> You can ""mark as draft"" until it's ready for review

@architkulkarni Sure thing! Apology for any of the noise.",mark draft ready review sure thing apology noise,issue,positive,positive,positive,positive,positive,positive
1737869743,@sihanwang41 could you please separate (7) out into its own PR so this is purely removing the existing stuff?,could please separate purely removing stuff,issue,negative,positive,positive,positive,positive,positive
1737869099,"Ah gotcha! You can ""mark as draft"" until it's ready for review, so codeowners don't get notified: https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/changing-the-stage-of-a-pull-request#converting-a-pull-request-to-a-draft",ah mark draft ready review get notified,issue,negative,positive,positive,positive,positive,positive
1737865951,"@architkulkarni Thanks for the review!
I have planned to add more things to it including pictures for the demo.
I will also update this based on your feedback.
Will ping you again once it is ready",thanks review add also update based feedback ping ready,issue,positive,positive,positive,positive,positive,positive
1737842380,"1. Ah you're right, maybe just update this test then: https://github.com/architkulkarni/ray/blob/9ab37bc295e4c63fb962ae12e1018d6246689134/python/ray/tests/test_runtime_env.py#L111 
2. Yeah, that would be good. Basically just a note for any person that updates installation.rst with new filenames, that they should go to get_wheel_filename and update it there too.",ah right maybe update test yeah would good basically note person new go update,issue,positive,positive,positive,positive,positive,positive
1737840478,"@Anindyadeep Thanks for bringing this up -- this is a result of the external integration with `transformers` using outdated APIs and will require a fix in the next `transformers` release. I will keep this thread updated for when that update PR gets merged in.

For now, you can either (1) use `ray<2.7` or (2) achieve something similar by using Ray Tune directly:

```python
from ray import tune

def train_fn(config):
    ...

    training_arguments = TrainingArguments(
        num_train_epochs=config[""num_train_epochs""],
        ...
    )
    trainer = Trainer(...)
    trainer.train()

tuner = tune.Tuner(train_fn, param_space={""num_train_epochs"": tune.choice([2, 3, 4, 5])})
results = tuner.fit()
```",thanks result external integration outdated require fix next release keep thread update either use ray achieve something similar ray tune directly python ray import tune trainer trainer tuner,issue,negative,negative,neutral,neutral,negative,negative
1737823249,"2
I am not sure what to add in the comment for `installation.rst`.
Do you me to add a github link to `get_wheel_filename()` and note that it must be up to date if any new python version or  architecture is added?",sure add comment add link note must date new python version architecture added,issue,negative,positive,positive,positive,positive,positive
1737820162,"Okay, got two questions!

1
you mind elaborating on what https://github.com/ray-project/ray/blob/3735ba410ddfa63cc8516f69fa8b3997259a3f11/release/runtime_env_tests/workloads/wheel_urls.py  needs updating

I haven't spot an issue. Besides, it not checking all the URLs. it will only test those that are compatible to the architecture the code is running on. Since `get_release_wheel_url()` and `get_master_wheel_url()` will invoke `get_wheel_filename()` without specifying an architecture. So it will use ` platform.processor`.",got two mind need spot issue besides test compatible architecture code running since invoke without architecture use,issue,negative,neutral,neutral,neutral,neutral,neutral
1737791317,I'll leave it to @GeneDer to review the other parts; otherwise I can stamp this ,leave review otherwise stamp,issue,negative,neutral,neutral,neutral,neutral,neutral
1737790180,All doc-related CI build & test passed so I think we are good. Thank you for fixing this!,build test think good thank fixing,issue,positive,positive,positive,positive,positive,positive
1737786086,"Rerun the release tests that are using lightning 2.0

✅ dolly v2: https://buildkite.com/ray-project/release-tests-pr/builds/55087#018b02a5-bc24-4210-bf63-2ab4c6ed2907
✅ vicuna: https://buildkite.com/ray-project/release-tests-pr/builds/55140#018b070b-42e5-484f-a300-83256a4df5e9
",rerun release lightning dolly vicuna,issue,negative,neutral,neutral,neutral,neutral,neutral
1737707442,"> @Zandew @scottjlee btw, do you know why this is causing failures now? what changed?

It may be the additional calls to `get_object_locations` that cause the tests to timeout",know causing may additional cause,issue,negative,neutral,neutral,neutral,neutral,neutral
1737609173,"> Hi @Ox0400 , what is `ray_ds` in the reproducible above?

same like
```
ds = ray.data.read_json('xxx')
ds = ds.filter(....)
ray_ds = ds.groupby('__doc_id__').map_groups(xxx)
...
```
",hi ox reproducible like,issue,negative,neutral,neutral,neutral,neutral,neutral
1737601061,"> Hi @Ox0400 Thanks for the contribution!
> 
> Could you update the PR description to describe what's the issue you are trying to fix?您能否更新 PR 描述以描述您尝试解决的问题是什么？

@jjyao The PR is fix the raise errorwhen start cli with help option. e.g:  `python -m ray.dashboard.dashboard  --help`",hi ox thanks contribution could update description describe issue trying fix fix raise start help option python help,issue,positive,positive,positive,positive,positive,positive
1737334818,"@asoans Yes it's true, I can pass a dataclass currently to the `train_loop_config` of `TorchTrainer`, but there are two minor flaws currently:

1. `train_loop_config` is annotated (and documented) as `Optional[Dict]`, which means that type checking code will note this as an error https://github.com/ray-project/ray/blob/master/python/ray/train/torch/torch_trainer.py#L179
2. Ray's built in logging will not properly log the individual keys and values inside the dataclass:
```
Training started with configuration:
╭──────────────────────────────────────────╮
│ Training config                          │
├──────────────────────────────────────────┤
│ train_loop_config   ...sparse': True}}}} │
╰──────────────────────────────────────────╯
```

So to get dataclasses supported as a ""first class citizen"", the [underlying code to print this table](https://github.com/krfricke/ray/blob/master/python/ray/tune/experimental/output.py#L566) would need to be tweaked to support a dataclass, not just a dict.

edit: maybe this could be as easy as running the dataclass through `dataclasses.asdict` before passing it to the table printing function",yes true pas currently two minor currently optional type code note error ray built logging properly log individual inside training configuration training sparse true get first class citizen underlying code print table would need support edit maybe could easy running passing table printing function,issue,positive,positive,positive,positive,positive,positive
1737313841,"this PR is not urgent, let's work on other PRs firstly :)",urgent let work firstly,issue,negative,positive,positive,positive,positive,positive
1736799046,premerge is failing because this depends on https://github.com/ray-project/rayci/pull/111; i put this up anyway to see how the other PR logic can be used in production,failing put anyway see logic used production,issue,negative,neutral,neutral,neutral,neutral,neutral
1736751423,"Did you want to use dataclasses instead of pydantic models? You can try 

`from dataclasses import dataclass

class TrainConfig;
lr: float
layer_sizes: list[int]
loss: str

train_config = TrainConfig(lr=0.2, layer_sizes=[32, 32, 16], loss=""bce"")
 `
and then pass the dataclass into the TorchTrainer
",want use instead try import class float list loss pas,issue,negative,neutral,neutral,neutral,neutral,neutral
1736636812,"I ran the test 3 times, and it passed each time without flaking. This change seems to fix the test.",ran test time time without change fix test,issue,negative,neutral,neutral,neutral,neutral,neutral
1736630728,"+1 issue here, we have simillar issue on production, about 100 QPS here
FastAPI + serve deployment(replica num=1)
same scenario: AsyncGenerator + StreamResponse",issue issue production serve deployment replica scenario,issue,negative,neutral,neutral,neutral,neutral,neutral
1736605948,"Hmm actually not seeing any difference on master with this change, so I'll close for now.",actually seeing difference master change close,issue,negative,neutral,neutral,neutral,neutral,neutral
1736601790,"ok, good luck I guess in finding this error again in a place where we don't have the mitigation in place.  I just checked one of our dev ray clusters and it was ""hung"" with this error.  lsof showed nothing using the port identified in the log and the cluster would accept a new job now in this state.  So, something ""let loose"" of the port eventually but while it was locked the cluster was unusable (unless restarted which forced a port release it seems).
",good luck guess finding error place mitigation place checked one dev ray hung error nothing port log cluster would accept new job state something let loose port eventually locked cluster unusable unless forced port release,issue,negative,positive,positive,positive,positive,positive
1736592556,"I will try to see which process next time it happens. What ray processes would be using that port with my config? I thought it would have to be a worker as that is the only thing in the range.  It was definitely some ray process.   When this happens the ray cluster is unusable and has to be restarted.  It is dead in the water for sure, so this is not a minor issue.  We have a ""work around"" which is very much a hack that is a script that looks for this error in the logs and then does a cluster restart if found.",try see process next time ray would port thought would worker thing range definitely ray process ray cluster unusable dead water sure minor issue work around much hack script error cluster restart found,issue,negative,positive,neutral,neutral,positive,positive
1736585872,"> The serve logger
> 
> ```
> logger = logging.getLogger(""ray.serve"")
> ```
> 
> should use the default handler which emits to stderr. In order to fix it, you should reconfigure the logger to emit output to stdout (https://docs.python.org/3/howto/logging.html)

Yes, I redirected stdout and err in `python/ray/serve/_private/logging_utils.py`, reference: https://stackoverflow.com/questions/16061641/python-logging-split-between-stdout-and-stderr",serve logger logger use default handler order fix logger emit output yes err reference,issue,negative,neutral,neutral,neutral,neutral,neutral
1736585352,@scottjlee I believe this will fix a lot of interleaving block access issues :),believe fix lot block access,issue,negative,neutral,neutral,neutral,neutral,neutral
1736564423,"Tested with the following script:
```python
>>> import ray
>>> import numpy as np
>>> from ray.data.preprocessors import StandardScaler
>>> 
>>> def map_test(data):
...     print(data)
...     return data
... 
>>> ds = ray.data.from_items([
...     {""A"": x % 3, ""B"": x} for x in range(100)]).groupby(
...     ""A"").map_groups(map_test).map_batches(map_test, batch_format='pandas')
2023-09-26 22:14:54,050	INFO worker.py:1673 -- Started a local Ray instance.
>>> 
>>> ss = StandardScaler(['A', 'B'])
>>> transformed_ds = ss.fit_transform(ds).materialize()
2023-09-26 22:14:54,570	INFO dataset.py:2380 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.
2023-09-26 22:14:54,572	INFO streaming_executor.py:93 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[Sort] -> TaskPoolMapOperator[MapBatches(group_fn)->MapBatches(map_test)] -> AllToAllOperator[Aggregate] -> LimitOperator[limit=1]
2023-09-26 22:14:54,572	INFO streaming_executor.py:94 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=True, actor_locality_enabled=True, verbose_progress=False)
2023-09-26 22:14:54,572	INFO streaming_executor.py:96 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
[dataset]: Run `pip install tqdm` to enable progress reporting.
(MapBatches(group_fn)->MapBatches(map_test) pid=67139)     A   B
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 0   0   0
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 1   0   3
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 2   0   6
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 3   0   9
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 4   0  12
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 5   0  15
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 6   0  18
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 7   0  21
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 8   0  24
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 9   0  27
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 10  0  30
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 11  0  33
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 12  0  36
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 13  0  39
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 14  0  42
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 15  0  45
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 16  0  48
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 17  0  51
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 18  0  54
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 19  0  57
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 20  0  60
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 21  0  63
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 22  0  66
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 23  0  69
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 24  0  72
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 25  0  75
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 26  0  78
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 27  0  81
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 28  0  84
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 29  0  87
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 30  0  90
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 31  0  93
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 32  0  96
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 33  0  99
(MapBatches(group_fn)->MapBatches(map_test) pid=67139)     A   B
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 0   0   0
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 1   0   3
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 2   0   6
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 3   0   9
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 4   0  12
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 5   0  15
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 6   0  18
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 7   0  21
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 8   0  24
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 9   0  27
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 10  0  30
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 11  0  33
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 12  0  36
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 13  0  39
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 14  0  42
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 15  0  45
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 16  0  48
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 17  0  51
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 18  0  54
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 19  0  57
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 20  0  60
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 21  0  63
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 22  0  66
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 23  0  69
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 24  0  72
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 25  0  75
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 26  0  78
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 27  0  81
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 28  0  84
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 29  0  87
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 30  0  90
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 31  0  93
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 32  0  96
(MapBatches(group_fn)->MapBatches(map_test) pid=67139) 33  0  99
2023-09-26 22:14:55,399	INFO streaming_executor.py:93 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[Sort] -> TaskPoolMapOperator[MapBatches(group_fn)->MapBatches(map_test)->MapBatches(StandardScaler._transform_pandas)]
2023-09-26 22:14:55,399	INFO streaming_executor.py:94 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=True, actor_locality_enabled=True, verbose_progress=False)
2023-09-26 22:14:55,399	INFO streaming_executor.py:96 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
2023-09-26 22:14:55,750	WARNING plan.py:568 -- Warning: The Ray cluster currently does not have any available CPUs. The Dataset job will hang unless more CPUs are freed up. A common reason is that cluster resources are used by Actors or Tune trials; see the following link for more details: https://docs.ray.io/en/latest/data/data-internals.html#ray-data-and-tune
>>> print(list(transformed_ds.iter_rows()))
[{'A': -1.2095677687472157, 'B': -1.7148160424389378}, {'A': -1.2095677687472157, 'B': -1.6108877974426385}, {'A': -1.2095677687472157, 'B': -1.5069595524463393}, {'A': -1.2095677687472157, 'B': -1.40303130745004}, {'A': -1.2095677687472157, 'B': -1.2991030624537407}, {'A': -1.2095677687472157, 'B': -1.1951748174574415}, {'A': -1.2095677687472157, 'B': -1.0912465724611422}, {'A': -1.2095677687472157, 'B': -0.987318327464843}, {'A': -1.2095677687472157, 'B': -0.8833900824685438}, {'A': -1.2095677687472157, 'B': -0.7794618374722445}, {'A': -1.2095677687472157, 'B': -0.6755335924759452}, {'A': -1.2095677687472157, 'B': -0.571605347479646}, {'A': -1.2095677687472157, 'B': -0.4676771024833467}, {'A': -1.2095677687472157, 'B': -0.36374885748704744}, {'A': -1.2095677687472157, 'B': -0.25982061249074817}, {'A': -1.2095677687472157, 'B': -0.1558923674944489}, {'A': -1.2095677687472157, 'B': -0.051964122498149634}, {'A': -1.2095677687472157, 'B': 0.051964122498149634}, {'A': -1.2095677687472157, 'B': 0.1558923674944489}, {'A': -1.2095677687472157, 'B': 0.25982061249074817}, {'A': -1.2095677687472157, 'B': 0.36374885748704744}, {'A': -1.2095677687472157, 'B': 0.4676771024833467}, {'A': -1.2095677687472157, 'B': 0.571605347479646}, {'A': -1.2095677687472157, 'B': 0.6755335924759452}, {'A': -1.2095677687472157, 'B': 0.7794618374722445}, {'A': -1.2095677687472157, 'B': 0.8833900824685438}, {'A': -1.2095677687472157, 'B': 0.987318327464843}, {'A': -1.2095677687472157, 'B': 1.0912465724611422}, {'A': -1.2095677687472157, 'B': 1.1951748174574415}, {'A': -1.2095677687472157, 'B': 1.2991030624537407}, {'A': -1.2095677687472157, 'B': 1.40303130745004}, {'A': -1.2095677687472157, 'B': 1.5069595524463393}, {'A': -1.2095677687472157, 'B': 1.6108877974426385}, {'A': -1.2095677687472157, 'B': 1.7148160424389378}, {'A': 0.012217856249971888, 'B': -1.6801732941068381}, {'A': 0.012217856249971888, 'B': -1.5762450491105389}, {'A': 0.012217856249971888, 'B': -1.4723168041142396}, {'A': 0.012217856249971888, 'B': -1.3683885591179403}, {'A': 0.012217856249971888, 'B': -1.264460314121641}, {'A': 0.012217856249971888, 'B': -1.1605320691253418}, {'A': 0.012217856249971888, 'B': -1.0566038241290425}, {'A': 0.012217856249971888, 'B': -0.9526755791327433}, {'A': 0.012217856249971888, 'B': -0.848747334136444}, {'A': 0.012217856249971888, 'B': -0.7448190891401447}, {'A': 0.012217856249971888, 'B': -0.6408908441438455}, {'A': 0.012217856249971888, 'B': -0.5369625991475462}, {'A': 0.012217856249971888, 'B': -0.43303435415124697}, {'A': 0.012217856249971888, 'B': -0.3291061091549477}, {'A': 0.012217856249971888, 'B': -0.2251778641586484}, {'A': 0.012217856249971888, 'B': -0.12124961916234914}, {'A': 0.012217856249971888, 'B': -0.01732137416604988}, {'A': 0.012217856249971888, 'B': 0.08660687083024939}, {'A': 0.012217856249971888, 'B': 0.19053511582654867}, {'A': 0.012217856249971888, 'B': 0.2944633608228479}, {'A': 0.012217856249971888, 'B': 0.3983916058191472}, {'A': 0.012217856249971888, 'B': 0.5023198508154465}, {'A': 0.012217856249971888, 'B': 0.6062480958117458}, {'A': 0.012217856249971888, 'B': 0.710176340808045}, {'A': 0.012217856249971888, 'B': 0.8141045858043443}, {'A': 0.012217856249971888, 'B': 0.9180328308006436}, {'A': 0.012217856249971888, 'B': 1.0219610757969428}, {'A': 0.012217856249971888, 'B': 1.125889320793242}, {'A': 0.012217856249971888, 'B': 1.2298175657895414}, {'A': 0.012217856249971888, 'B': 1.3337458107858406}, {'A': 0.012217856249971888, 'B': 1.43767405578214}, {'A': 0.012217856249971888, 'B': 1.5416023007784392}, {'A': 0.012217856249971888, 'B': 1.6455305457747385}, {'A': 1.2340034812471596, 'B': -1.6455305457747385}, {'A': 1.2340034812471596, 'B': -1.5416023007784392}, {'A': 1.2340034812471596, 'B': -1.43767405578214}, {'A': 1.2340034812471596, 'B': -1.3337458107858406}, {'A': 1.2340034812471596, 'B': -1.2298175657895414}, {'A': 1.2340034812471596, 'B': -1.125889320793242}, {'A': 1.2340034812471596, 'B': -1.0219610757969428}, {'A': 1.2340034812471596, 'B': -0.9180328308006436}, {'A': 1.2340034812471596, 'B': -0.8141045858043443}, {'A': 1.2340034812471596, 'B': -0.710176340808045}, {'A': 1.2340034812471596, 'B': -0.6062480958117458}, {'A': 1.2340034812471596, 'B': -0.5023198508154465}, {'A': 1.2340034812471596, 'B': -0.3983916058191472}, {'A': 1.2340034812471596, 'B': -0.2944633608228479}, {'A': 1.2340034812471596, 'B': -0.19053511582654867}, {'A': 1.2340034812471596, 'B': -0.08660687083024939}, {'A': 1.2340034812471596, 'B': 0.01732137416604988}, {'A': 1.2340034812471596, 'B': 0.12124961916234914}, {'A': 1.2340034812471596, 'B': 0.2251778641586484}, {'A': 1.2340034812471596, 'B': 0.3291061091549477}, {'A': 1.2340034812471596, 'B': 0.43303435415124697}, {'A': 1.2340034812471596, 'B': 0.5369625991475462}, {'A': 1.2340034812471596, 'B': 0.6408908441438455}, {'A': 1.2340034812471596, 'B': 0.7448190891401447}, {'A': 1.2340034812471596, 'B': 0.848747334136444}, {'A': 1.2340034812471596, 'B': 0.9526755791327433}, {'A': 1.2340034812471596, 'B': 1.0566038241290425}, {'A': 1.2340034812471596, 'B': 1.1605320691253418}, {'A': 1.2340034812471596, 'B': 1.264460314121641}, {'A': 1.2340034812471596, 'B': 1.3683885591179403}, {'A': 1.2340034812471596, 'B': 1.4723168041142396}, {'A': 1.2340034812471596, 'B': 1.5762450491105389}, {'A': 1.2340034812471596, 'B': 1.6801732941068381}]
```",tested following script python import ray import import data print data return data range local ray instance tip use instead take show return batch format dag input sort aggregate execution tip detailed progress run true run pip install enable progress dag input sort execution tip detailed progress run true warning warning ray cluster currently available job unless freed common reason cluster used tune see following link print list,issue,positive,positive,positive,positive,positive,positive
1736564388,"> I found a similar problem. Plasma object store in raylet process doesn't release mmaped memory of objects used in a finished job.
> 
> My environment is:
> 
>     * MacBook Pro M2 Max, 64 GB RAM, 12 CPU Cores
> 
>     * MacOS 13.5.1 (22G90)
> 
>     * Python 3.11.4 arm64
> 
>     * Ray version 2.6.3
> 
> 
> My test script is:
> 
> ```
> import ray
> import pyarrow as pa
> 
> schema = pa.schema([('v', pa.binary())])
> def map(batch):
>     lines = []
>     for __ in range(len(batch['id'])):
>         lines.append(b'*' * 99)
>     return pa.Table.from_arrays([lines], schema=schema)
> 
> def test(n):
>     ray.init()
>     ray.data.DataContext.get_current().execution_options.verbose_progress = True
>     ray.data.DataContext.get_current().use_push_based_shuffle = True
>     parallelism = max(1, int(n * 100 / (128 * 1024 * 1024)))
>     ds = ray.data.range(n, parallelism=parallelism)\
>             .map_batches(map, zero_copy_batch=True)\
>             .repartition(1)
>     ds.materialize()
> 
> if __name__ == '__main__':
>     test(700000000)
> ```
> 
> run following shell command:
> 
> ```
> ray start --head
> python ./test.py
> ```
> 
> And you will find the disk usage increases, run the `vmmap PID` command and you will find a lot of mmaped files still exist in raylet process, and when you `ray stop` killing all the ray processes then the disk usage decreases.

I run the test script using Ray 2.7.0 (Python 3.11.5) in the same machine and the problem still exists.

Here are screenshots of Ray dashboard about the node.

_Before running the script:_

<img width=""1488"" alt="""" src=""https://github.com/ray-project/ray/assets/88138737/5867ae35-6f5b-4f85-b20c-6c1b32e6987d"">

<img width=""1492"" alt="""" src=""https://github.com/ray-project/ray/assets/88138737/a6e2157c-7b10-49b5-8d43-59b76cf4282f"">


_After running the script:_

<img width=""1485"" alt="""" src=""https://github.com/ray-project/ray/assets/88138737/98673ec3-b24b-476b-8edd-39498a5c390f"">

<img width=""1473"" alt="""" src=""https://github.com/ray-project/ray/assets/88138737/c810338a-4ae0-449b-a02a-649253dde670"">

There's no visible files under '/tmp' because mmapped 'plasmaXXXXXX' files are unlinked right after its creation.

Here's the output of `vmmap PID` after running the script:

[leak_vmmap.txt](https://github.com/ray-project/ray/files/12733597/leak_vmmap.txt)
",found similar problem plasma object store raylet process release memory used finished job environment pro ram python arm ray version test script import ray import pa schema map batch range batch return test true true parallelism map test run following shell command ray start head python find disk usage run command find lot still exist raylet process ray stop killing ray disk usage run test script ray python machine problem still ray dashboard node running script running script visible unlinked right creation output running script,issue,negative,positive,positive,positive,positive,positive
1736559803,"> Looks good to me, as long as the unit tests are added in the next PR.

I have added the unit test in this PR in the latest commit, yesterday I didn't have time for that. Now that this PR had some issue and wasn't merged. I have plenty of time today so I paid the tech debt.
",good long unit added next added unit test latest commit yesterday time issue plenty time today tech debt,issue,negative,positive,positive,positive,positive,positive
1736545111,cc @can-anyscale would it be possible for us to add `trn1.32xl` instances to use in our CI tests?,would possible u add use,issue,negative,neutral,neutral,neutral,neutral,neutral
1736542994,"> So, the issue happens if the dashboard (that lives in the same proc as GCS) fails to health check the GCS for 40 times (10 seconds timeout each).
> 
> So somehow your dashboard process couldn't communicate with gcs_server process for a while. You said this happens after ""idle"", not when you have a heavy workload?

Yes, it usually happens when I go to check the next morning.
I will try using the environment variables， thanks for your help",issue dashboard health check time somehow dashboard process could communicate process said idle heavy yes usually go check next morning try environment thanks help,issue,positive,negative,neutral,neutral,negative,negative
1736525671,"I am also new to Ray. I think this would be okay, but you have to check with someone who has better context on what the contracts are for different block types. Thanks for working on this!",also new ray think would check someone better context different block thanks working,issue,negative,positive,positive,positive,positive,positive
1736510874,"> @chappidim One concern is that these features have not yet been tested. Is it possible for you to provide AWS Neuron instances and test these code? (Our current CI is running on general `g4dn` and `m5` instances.)

I did manual testing for all_reduce function mentioned in PR overview. I'm happy to run a training script of your choice on trn1.32xl instance/s (limit=2) if needed. Let me know.",one concern yet tested possible provide neuron test code current running general manual testing function overview happy run training script choice let know,issue,positive,positive,positive,positive,positive,positive
1736484256,"> Lint failed: https://buildkite.com/ray-project/premerge/builds/6504#018ad097-37b8-460b-b2c0-9fae441269bb/185-440
> 
> You can run setup_hooks.sh to install a pre-push hook that will automatically run lint and prevent this issue

Thank you! This bothers me for some time.",lint run install hook automatically run lint prevent issue thank time,issue,negative,neutral,neutral,neutral,neutral,neutral
1736482804,"> This also breaks compatibility with the dashboard too

Could you give more details? What errors are you seeing?",also compatibility dashboard could give seeing,issue,negative,neutral,neutral,neutral,neutral,neutral
1736479133,"On the Kuberay side, I propose we make each ""worker group"" map to each TPU pod slice. Specifically we'll need an extension of the ""worker group"" concept (call it a ""TPU worker group"").

The high level idea is as follows:
1. Each TPU worker group corresponds to exactly one TPU pod slice. 
2. The size of each TPU worker group is statically known. This can usually be derived from the TPU topology (e.g. ""2x2x4"" means 4 TPU hosts in one slice, etc).
3. A TPU worker group cannot be resized once created. It can only be created (scale from 0 to N) or deleted (scale from N to 0).
4. For multi-slice autoscaling, this means we can only scale up or down by whole pod slices at a time. For the Kuberay operator this means the creation or deletion of TPU worker groups, instead of specific replicas.

On the Kuberay operator side [1][2], we need to perform a few additional steps when reconciling a worker pod: 
* Pods in a TPU worker group need to be scheduled with affinity. Most likely this will require using a topology key derived from the GKE node pool name. This ensures that all workers in a TPU worker group are scheduled on the same slice.
* The Ray process on each Worker needs a “group_id” marker that identifies workers on the same slice. This marker should also be derived from the GKE node pool name. This is exactly the same as the group id mentioned above. This ensures Ray can schedule actors/tasks on the same slice.
* In addition, TPU worker groups can perform additional initialization by adding the following environment variables:
         * TPU_WORKER_ID: Set to the index of the pod in the worker group. Note that this only works if we are creating new worker groups as a whole - we should never add replicas to existing TPU worker groups. Since we map each TPU worker group to a pod slice of known shape, the precise number of workers for each TPU worker group should be statically known.
         * TPU_WORKER_HOSTNAMES: This is a concatenation of all the statically known worker host names. Again, since we know the number of workers statically, this can be deterministically generated.
         * Correspondingly, each TPU worker group needs its own headless service to ensure that the subdomains can be created.

On the Ray autoscaler side [3], we need the Kuberay Node Provider to support the following:
* When the application makes an additional resource request, the node provider identifies the corresponding TPU worker group that matches with the request.
* Instead of adding replicas to the same group, the node provider patches the Ray Cluster resource with a new worker group. The new worker group will share the same topology key based on GKE node pool name.
* When scaling down, the node provider needs to know that an entire worker group should be terminated instead of one worker node at a time.

[1] - https://github.com/ray-project/kuberay/blob/master/ray-operator/controllers/ray/common/pod.go
[2] - https://github.com/ray-project/kuberay/blob/master/ray-operator/controllers/ray/raycluster_controller.go
[3] - https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/_private/kuberay/node_provider.py

@kevin85421 What do you think?
",side propose make worker group map pod slice specifically need extension worker group concept call worker group high level idea worker group exactly one pod slice size worker group statically known usually derived topology one slice worker group scale scale scale whole pod time operator creation deletion worker instead specific operator side need perform additional reconciling worker pod worker group need affinity likely require topology key derived node pool name worker group slice ray process worker need marker slice marker also derived node pool name exactly group id ray schedule slice addition worker perform additional following environment set index pod worker group note work new worker whole never add worker since map worker group pod slice known shape precise number worker group statically known concatenation statically known worker host since know number statically correspondingly worker group need headless service ensure ray side need node provider support following application additional resource request node provider corresponding worker group request instead group node provider ray cluster resource new worker group new worker group share topology key based node pool name scaling node provider need know entire worker group instead one worker node time think,issue,positive,positive,neutral,neutral,positive,positive
1736448963,"Looks like the address is unexpected: 

https://github.com/ray-project/ray/blob/releases/2.6.3/python/ray/_raylet.pyx#L2597C24-L2597C24

Are there "":"" in the address? 

Either way, this code looks wrong. ",like address unexpected address either way code wrong,issue,negative,negative,negative,negative,negative,negative
1736446427,"@amogkam @raulchen - discussed with @ericl, ideally we would want the user to look at the warning and realize they should try adjusting parallelism in order to reduce block size. we can add a paragraph in the [Performance Tips](https://docs.ray.io/en/latest/data/performance-tips.html#tuning-read-parallelism) page describing this -- i think we discuss it in the [""Tuning read parallelism""](https://docs.ray.io/en/latest/data/performance-tips.html#tuning-read-parallelism) section, but there's no clear course of action.

does that make sense from user's perspective?",ideally would want user look warning realize try parallelism order reduce block size add paragraph performance page think discus tuning read parallelism section clear course action make sense user perspective,issue,negative,positive,positive,positive,positive,positive
1736443069,Merging to help understand transient failures like this: https://github.com/ray-project/ray/issues/39883,help understand transient like,issue,positive,neutral,neutral,neutral,neutral,neutral
1736442679,"> I agree, retry sounds good in the short term.
> 
> If ""ray stop"" fails to shut down all nodes, I still think it makes more sense for `ray down` to return a nonzero error code and also print a warning (maybe suggesting to rerun `ray down`), rather than to just return zero and print the warning. I feel like if `ray down` succeeds the user should have confidence that the cluster was shut down.
> 
> I'm not sure why SIGSEGV is being raised and I wonder if that's a bug.

I think we could follow up with retry + warning logging? ",agree retry good short term ray stop shut still think sense ray return nonzero error code also print warning maybe suggesting rerun ray rather return zero print warning feel like ray user confidence cluster shut sure raised wonder bug think could follow retry warning logging,issue,negative,positive,positive,positive,positive,positive
1736413674,"@keerthanvasist forgive my noobness, but what if all the nicknames are typed as follow:
```
{'name': 'Luna', 'age': 4, 'nicknames': array(['Looney', 'Loona'], dtype='<U6')}
{'name': 'Rory', 'age': 14, 'nicknames': array(['Rorey'], dtype='<U5')}
{'name': 'Scout', 'age': 9, 'nicknames': array(['Scoot'], dtype='<U5')}
```",forgive follow array array array,issue,negative,neutral,neutral,neutral,neutral,neutral
1736362646,"Ideally, Pydantic either (1) wouldn't mutate the original dataclass or (2) would keep a copy of the original dataclass that we could use to create a custom serializer. Unfortunately, it looks like Pydantic (1) mutates the original dataclass and (2) doesn't keep a copy of the original dataclass anywhere.

One possible, hacky workaround: the Pydantic dataclass still has the same fields as the original dataclass. Serve could reconstruct a copy of the original dataclass by creating a new dataclass with the same fields upon deserialization. However, if the user defines a custom constructor, that would be lost.",ideally either would mutate original would keep copy original could use create custom unfortunately like original keep copy original anywhere one possible hacky still original serve could reconstruct copy original new upon however user custom constructor would lost,issue,positive,positive,positive,positive,positive,positive
1736361468,"confimred it is fixed

```
                logger.error(
                    ""Error should be written to 'dashboard.log' or ""
                    ""'dashboard.err'. We are printing the last ""
                    f""{lines_to_read} lines for you. See ""
                    ""'https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure' ""  # noqa
                    ""to find where the log file is.""
                )
```",fixed error written printing last see find log file,issue,negative,positive,neutral,neutral,positive,positive
1736360582,"Duplicate as https://github.com/ray-project/ray/issues/39866. 

```
    server = aiogrpc.server(options=((""grpc.so_reuseport"", 0),))

    grpc_port = ray._private.tls_utils.add_port_to_grpc_server(
        server, f""{grpc_ip}:{grpc_port}""
    )
```
Regarding the server port allocation, we have the next line that allocates the correct port",duplicate server server regarding server port allocation next line correct port,issue,negative,neutral,neutral,neutral,neutral,neutral
1736359739,"This turns out to be harder than expected. The issue is that Pydantic 1.10.x converts vanilla dataclasses into Pydantic dataclasses when they're used as type hints in a Pydantic model. During this process, Pydantic also replaces plain Python methods with cythonized methods, which can't be serialized with `cloudpickle` due to https://github.com/cloudpipe/cloudpickle/issues/408. When Ray attempts to serialize the dataclass or Pydantic model with cloudpickle, it fails with the following error:

```console
% python repro.py 
Traceback (most recent call last):
  File ""repro.py"", line 32, in <module>
    serialized_object = pickle.dumps(BackendConfig())
  File ""/Users/shrekris/Desktop/ray/python/ray/cloudpickle/cloudpickle_fast.py"", line 88, in dumps
    cp.dump(obj)
  File ""/Users/shrekris/Desktop/ray/python/ray/cloudpickle/cloudpickle_fast.py"", line 733, in dump
    return Pickler.dump(self, obj)
AttributeError: Can't pickle local object '__create_fn__.<locals>.__init__'
```

The `__init__` method has been cythonized (from [objgraph](https://objgraph.readthedocs.io/en/stable/)):

<img width=""372"" alt=""Screen Shot 2023-09-26 at 2 59 18 PM"" src=""https://github.com/ray-project/ray/assets/92341594/d36c0a5f-b358-4015-95c1-9aefcefa2bbf"">
",turn harder issue vanilla used type model process also plain python ca due ray serialize model following error console python recent call last file line module file line file line dump return self ca pickle local object method screen shot,issue,negative,negative,neutral,neutral,negative,negative
1736357756,This seems bad. cc @rickyyx have you seen a similar issue before? ,bad seen similar issue,issue,negative,negative,negative,negative,negative,negative
1736357703,"And here's a repro script:

```python
# File name: repro.py

import ray
# import pickle
import ray.cloudpickle as pickle

from pydantic import BaseModel
from dataclasses import dataclass
from typing import Any, Dict, List, Optional

# START_RAY = True
START_RAY = False

if START_RAY:
    ray.init(ignore_reinit_error=True)

@dataclass
class BackendMetadata:
    is_blocking: bool = True
    autoscaling_config: Optional[Dict[str, Any]] = None

class BackendConfig(BaseModel):
    internal_metadata: BackendMetadata = BackendMetadata()

if START_RAY:
    ray.get(ray.put(BackendConfig()))

    @ray.remote
    def consume(f):
        pass

    ray.get(consume.remote(BackendConfig()))
else:
    serialized_object = pickle.dumps(BackendConfig())
    deserialized_object = pickle.loads(serialized_object)
```",script python file name import ray import pickle import pickle import import import list optional true false class bool true optional none class consume pas else,issue,positive,positive,neutral,neutral,positive,positive
1736357022,"> Btw, we should follow up with this. Adding the exit code itself is not very useful (we should integrate to prod or Ray dashboard)

Adding it to the dashboard is tracked here https://github.com/ray-project/ray/issues/38142",follow exit code useful integrate prod ray dashboard dashboard tracked,issue,negative,positive,positive,positive,positive,positive
1736356848,"Here's a debugging script that I'm using to experiment with the failing test:

```python
# import pickle
import ray.cloudpickle as pickle

from copy import deepcopy
from pydantic import BaseModel
from pydantic.dataclasses import is_builtin_dataclass
from dataclasses import dataclass
from typing import Any, Dict, Optional

@dataclass
class BackendMetadata:
    is_blocking: bool = True
    autoscaling_config: Optional[Dict[str, Any]] = None

bm = deepcopy(BackendMetadata)

# breakpoint()

class BackendConfig(BaseModel):
    internal_metadata: BackendMetadata = BackendMetadata()

my = BackendConfig()

def ser():
    print(pickle.dumps(BackendMetadata))

def cmp():
    print(bm is BackendMetadata)

def pd_dc():
    print(is_builtin_dataclass(BackendMetadata))

breakpoint()
culprit = type(my.__dict__[""internal_metadata""]).__dict__[""__init__""]

print(""Finished"")

# Hypothesis: adding BackendMetadata to the BaseModel makes pydantic switch
# the dataclass's __init__ method with a cyfunction __init__ method that
# can't be serialized by cloudpickle (but can be serialized with pickle?).
```",script experiment failing test python import pickle import pickle copy import import import import import optional class bool true optional none class ser print print print culprit type print finished hypothesis switch method method ca pickle,issue,negative,positive,positive,positive,positive,positive
1736346992,"From a ray start command, it looks like all the necessary ports for 2.6 (runtime-env-agent-port is introduced in 2.7, but it is not in 2.6) are correctly specified. 

Can you try seeing which process uses the port when this happens next time? I don't think we can further debug without a repro or the additional information. The error itself also should be harmless (I think it just fails to start a new process, and I believe the followup worker startup should work)",ray start command like necessary correctly try seeing process port next time think without additional information error also harmless think start new process believe worker work,issue,positive,positive,neutral,neutral,positive,positive
1736333459,"I am not exactly sure why there's such issue. It can happen due to network error (which is not common because both gcs_server and dashboard runs in the same host). 2. The GCS is overloaded and responds slowly (which is also uncommon because you said it happens at idle time). 

you can alternatively modify this behavior by setting env var when you run ray start --head

GCS_CHECK_ALIVE_MAX_COUNT_OF_RPC_ERROR=40 by default
GCS_CHECK_ALIVE_RPC_TIMEOUT=10 (seconds) by default

",exactly sure issue happen due network error common dashboard host slowly also uncommon said idle time alternatively modify behavior setting run ray start head default default,issue,negative,positive,positive,positive,positive,positive
1736330386,"So, the issue happens if the dashboard (that lives in the same proc as GCS) fails to health check the GCS for 40 times (10 seconds timeout each). 

So somehow your dashboard process couldn't communicate with gcs_server process for a while. You said this happens after ""idle"", not when you have a heavy workload? ",issue dashboard health check time somehow dashboard process could communicate process said idle heavy,issue,negative,negative,negative,negative,negative,negative
1736324261,This won't work. Let's discuss the design together and make another PR,wo work let discus design together make another,issue,negative,neutral,neutral,neutral,neutral,neutral
1736322369,"There are some internal grpc APIs that can shutdown raylet, but we don't really recommend you to use it (since it is unstable)",internal shutdown raylet really recommend use since unstable,issue,negative,positive,neutral,neutral,positive,positive
1736322128,I think the best way you can do is to submit a script that calls ray stop,think best way submit script ray stop,issue,negative,positive,positive,positive,positive,positive
1736320949,"it is pretty easy for the basic case, but you should make this work with `get_current_placement_group()`, which involves in a little bit of cpp changes",pretty easy basic case make work little bit,issue,positive,positive,positive,positive,positive,positive
1736320575,I think strategy only is actually good enough for now,think strategy actually good enough,issue,negative,positive,positive,positive,positive,positive
1736320277,"So placement_group.py creates an object PlacementGroup. it currently only contains the ID, but we'd like to include bundle info and strategy ",object currently id like include bundle strategy,issue,negative,neutral,neutral,neutral,neutral,neutral
1736318873,"Btw, we should follow up with this. Adding the exit code itself is not very useful (we should integrate to prod or Ray dashboard)",follow exit code useful integrate prod ray dashboard,issue,negative,positive,positive,positive,positive,positive
1736310740,"The serve logger 
```
logger = logging.getLogger(""ray.serve"")
```

should use the default handler which emits to stderr. In order to fix it, you should reconfigure the logger to emit output to stdout (https://docs.python.org/3/howto/logging.html)",serve logger logger use default handler order fix logger emit output,issue,negative,neutral,neutral,neutral,neutral,neutral
1736299231,"Didn't see anything exciting happening there, only `monitor.log` has some entries:
<details>

```
2023-09-22 21:37:07,546 INFO monitor.py:699 -- Starting monitor using ray installation: /home/jernej_m/mambaforge-pypy3/envs/test_ray/lib/python3.10/site-packages/ray/__init__.py
2023-09-22 21:37:07,546 INFO monitor.py:700 -- Ray version: 2.6.3
2023-09-22 21:37:07,546 INFO monitor.py:701 -- Ray commit: {{RAY_COMMIT_SHA}}
2023-09-22 21:37:07,546 INFO monitor.py:702 -- Monitor started with command: ['/home/jernej_m/mambaforge-pypy3/envs/test_ray/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.py', '--logs-dir=/tmp/ray/session_2023-09-22_21-37-05_827384_110848/logs', '--logging-rotate-bytes=536870912', '--logging-rotate-backup-count=5', '--gcs-address=192.168.0.101:6379', '--autoscaling-config=~/ray_bootstrap_config.yaml', '--monitor-ip=192.168.0.101']
2023-09-22 21:37:07,552 INFO monitor.py:167 -- session_name: session_2023-09-22_21-37-05_827384_110848
2023-09-22 21:37:07,554 INFO monitor.py:199 -- Starting autoscaler metrics server on port 44217
2023-09-22 21:37:07,556 INFO monitor.py:224 -- Monitor: Started
2023-09-22 21:37:07,571 INFO node_provider.py:53 -- ClusterState: Loaded cluster state: []
2023-09-22 21:37:07,572 INFO node_provider.py:114 -- ClusterState: Writing cluster state: ['192.168.0.106', '192.168.0.107', '192.168.0.108', '192.168.0.110', '192.168.0.101']
2023-09-22 21:37:07,572 INFO autoscaler.py:274 -- disable_node_updaters:False
2023-09-22 21:37:07,572 INFO autoscaler.py:282 -- disable_launch_config_check:False
2023-09-22 21:37:07,572 INFO autoscaler.py:294 -- foreground_node_launch:False
2023-09-22 21:37:07,572 INFO autoscaler.py:304 -- worker_liveness_check:True
2023-09-22 21:37:07,572 INFO autoscaler.py:312 -- worker_rpc_drain:True
2023-09-22 21:37:07,573 INFO autoscaler.py:362 -- StandardAutoscaler: {'cluster_name': 'test', 'auth': {'ssh_user': 'jernej_m', 'ssh_private_key': '~/ray_bootstrap_key.pem'}, 'upscaling_speed': 1.0, 'idle_timeout_minutes': 5, 'docker': {}, 'initialization_commands': [], 'setup_commands': ['source ~/mambaforge-pypy3/etc/profile.d/conda.sh && mamba env update -f /mnt/ray/mount/env.yaml -n test_ray --prune'], 'head_setup_commands': ['source ~/mambaforge-pypy3/etc/profile.d/conda.sh && mamba env update -f /mnt/ray/mount/env.yaml -n test_ray --prune'], 'worker_setup_commands': ['source ~/mambaforge-pypy3/etc/profile.d/c>
2023-09-22 21:37:07,574 INFO monitor.py:394 -- Autoscaler has not yet received load metrics. Waiting.
2023-09-22 21:37:12,588 INFO autoscaler.py:141 -- The autoscaler took 0.0 seconds to fetch the list of non-terminated nodes.
2023-09-22 21:37:12,588 INFO load_metrics.py:161 -- LoadMetrics: Removed ip: 192.168.0.108.
2023-09-22 21:37:12,588 INFO load_metrics.py:164 -- LoadMetrics: Removed 1 stale ip mappings: {'192.168.0.108'} not in {'192.168.0.101'}
2023-09-22 21:37:12,589 INFO autoscaler.py:421 --
======== Autoscaler status: 2023-09-22 21:37:12.589294 ========
Node status
---------------------------------------------------------------
Healthy:
 1 local.cluster.node
Pending:
 (no pending nodes)
Recent failures:
 (no failures)

Resources
---------------------------------------------------------------
Usage:
 0.0/32.0 CPU
 0.0/2.0 GPU
 0B/77.60GiB memory
 0B/37.25GiB object_store_memory

Demands:
 (no resource demands)
2023-09-22 21:37:12,590 INFO autoscaler.py:1368 -- StandardAutoscaler: Queue 4 new nodes for launch
2023-09-22 21:37:12,590 INFO autoscaler.py:464 -- The autoscaler took 0.003 seconds to complete the update iteration.
2023-09-22 21:37:12,591 INFO node_launcher.py:174 -- NodeLauncher0: Got 4 nodes to launch.
2023-09-22 21:37:12,592 INFO monitor.py:424 -- :event_summary:Resized to 56 CPUs, 4 GPUs.
2023-09-22 21:37:12,594 INFO node_provider.py:114 -- ClusterState: Writing cluster state: ['192.168.0.106', '192.168.0.107', '192.168.0.108', '192.168.0.110', '192.168.0.101']
2023-09-22 21:37:12,594 INFO node_provider.py:114 -- ClusterState: Writing cluster state: ['192.168.0.106', '192.168.0.107', '192.168.0.108', '192.168.0.110', '192.168.0.101']
2023-09-22 21:37:12,595 INFO node_provider.py:114 -- ClusterState: Writing cluster state: ['192.168.0.106', '192.168.0.107', '192.168.0.108', '192.168.0.110', '192.168.0.101']
2023-09-22 21:37:12,596 INFO node_provider.py:114 -- ClusterState: Writing cluster state: ['192.168.0.106', '192.168.0.107', '192.168.0.108', '192.168.0.110', '192.168.0.101']
2023-09-22 21:37:12,596 INFO node_launcher.py:174 -- NodeLauncher0: Launching 4 nodes, type local.cluster.node.
2023-09-22 21:37:17,608 INFO autoscaler.py:141 -- The autoscaler took 0.001 seconds to fetch the list of non-terminated nodes.
2023-09-22 21:37:17,609 INFO autoscaler.py:421 --
======== Autoscaler status: 2023-09-22 21:37:17.609649 ========
Node status
---------------------------------------------------------------
Healthy:
 2 local.cluster.node
Pending:
 192.168.0.106: local.cluster.node, uninitialized
 192.168.0.107: local.cluster.node, uninitialized
 192.168.0.110: local.cluster.node, uninitialized
Recent failures:
 (no failures)

Resources
---------------------------------------------------------------
Usage:
 0.0/56.0 CPU
 0.0/4.0 GPU
 0B/98.01GiB memory
 0B/46.00GiB object_store_memory

Demands:
 (no resource demands)
2023-09-22 21:37:17,619 INFO autoscaler.py:1316 -- Creating new (spawn_updater) updater thread for node 192.168.0.106.
2023-09-22 21:37:17,620 INFO autoscaler.py:1316 -- Creating new (spawn_updater) updater thread for node 192.168.0.107.
2023-09-22 21:37:17,620 INFO autoscaler.py:1316 -- Creating new (spawn_updater) updater thread for node 192.168.0.108.
2023-09-22 21:37:17,620 INFO autoscaler.py:1316 -- Creating new (spawn_updater) updater thread for node 192.168.0.110.
```

</details>
Running everything manually works. Would be nice to have a working cluster launcher for on prem clusters.",see anything exciting happening starting monitor ray installation ray version ray commit monitor command starting metric server port monitor loaded cluster state writing cluster state false false false true true mamba update prune mamba update prune yet received load metric waiting took fetch list removed removed stale status node status healthy pending pending recent usage memory resource queue new launch took complete update iteration got launch writing cluster state writing cluster state writing cluster state writing cluster state type took fetch list status node status healthy pending recent usage memory resource new thread node new thread node new thread node new thread node running everything manually work would nice working cluster launcher,issue,positive,positive,neutral,neutral,positive,positive
1736282465,"Hi @DrinkingMilktea, thanks for reporting this. At a high level, what are you trying to do?  `run_on_cluster` is marked as `@DeveloperAPI` (i.e. not a public API), so I wonder if there's another way of accomplishing what you're trying to do",hi thanks high level trying marked public wonder another way trying,issue,negative,positive,positive,positive,positive,positive
1736279289,a p0 issue opened here: https://github.com/ray-project/ray/issues/39866. We can close this one I believe,issue close one believe,issue,negative,neutral,neutral,neutral,neutral,neutral
1736250454,@xwjiang2010 @kouroshHakha is this something we can target as part of next ray release?,something target part next ray release,issue,negative,neutral,neutral,neutral,neutral,neutral
1736247249,Ray Train doesn't support ParameterServerStrategy out of the box. For TensorFlow you can use the TensorflowTrainer with MultiWorkerMirroredStrategy as described [here](https://docs.ray.io/en/releases-2.7.0/train/distributed-tensorflow-keras.html).,ray train support box use,issue,negative,neutral,neutral,neutral,neutral,neutral
1736236419,@c21 this reads like an offline batch inference item can you review here and see if this is something we want to target for ray 2.8?,like batch inference item review see something want target ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1736235500,I would say it's not. I will take a look at the CR to try and appreciate the engineering constraints though. ,would say take look try appreciate engineering though,issue,negative,neutral,neutral,neutral,neutral,neutral
1736234339,this doesn't really fit anywhere; ill fix this as part of ray 2.8 release,really fit anywhere ill fix part ray release,issue,negative,negative,neutral,neutral,negative,negative
1736232739,"@Amier2 can you please advise here?

UPDATE: 11/8 as no response; please re-open if this issue is still occuring on latest ray (28 at time of this message).",please advise update response please issue still latest ray time message,issue,positive,positive,positive,positive,positive,positive
1736232523,@mattip can we close since 35472 was merged and released as party of ray27? @Adjei-Mensah can you verify with latest ray issue is resolved?,close since party ray verify latest ray issue resolved,issue,positive,positive,positive,positive,positive,positive
1736229583,Think this should be Core; @rkooo567 what's your best take on priority for this item?,think core best take priority item,issue,positive,positive,positive,positive,positive,positive
1736226196,@matthewdeng can you take a look and set priority for this one?,take look set priority one,issue,negative,neutral,neutral,neutral,neutral,neutral
1736222279,observability related item; but this feels like some issue with the head node where the dashbaord is hosted... @akshay-anyscale @xieus can you advise on next step here?,observability related item like issue head node advise next step,issue,negative,neutral,neutral,neutral,neutral,neutral
1736219146,@scottsun94 @alanwguo where should this go to? Exp team the right landing spot?,go team right landing spot,issue,negative,positive,positive,positive,positive,positive
1736201929,"It is hard to catch it as in our current environment. I am testing for this error in the logs and auto restarting the ray cluster if I find it.  When I did a netstat while diagnosing a little it was a ray process that held onto the port, don't recall which for sure. Sorry.",hard catch current environment testing error auto ray cluster find little ray process onto port recall sure sorry,issue,negative,negative,neutral,neutral,negative,negative
1736153915,"can you confirm that the files can be read directly with the `pyarrow` API? e.g. [ParquetDataset](https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html):

```
pq.ParquetDataset(paths, filesystem)
```

we want to see if there is an issue with reading (1) single file, and (2) multiple files at once. we are trying to possibly rule out the issue to the metadata fetching as you mentioned above, and to see if the issue is present for one or multiple files.

another thing we noticed is that in the code from [this comment](https://github.com/ray-project/ray/issues/39548#issuecomment-1714549157), the `read_parquet(...)` is not being materialized, so this doesn't actually read in the full data (only does some metadata fetching to do some file size estimates). could you try materializing with `.materialize()` to see if the data can be read in properly one file/multiple files at a time?",confirm read directly want see issue reading single file multiple trying possibly rule issue fetching see issue present one multiple another thing code comment actually read full data fetching file size could try see data read properly one time,issue,negative,positive,neutral,neutral,positive,positive
1736145193,"> Even for running the cluster launcher release tests on master, we'll need to add in a loop of ""sleep""s waiting for the wheels to be available on AWS (the building and uploading can take 1-2 hours), because now the cluster launcher tests will depend on these wheels. 

This is something that coudl be possible with buildkite right?  cc @can-anyscale ",even running cluster launcher release master need add loop sleep waiting available building take cluster launcher depend something possible right,issue,negative,positive,positive,positive,positive,positive
1736105127,Please let me know if you have any creative solutions for the testing issue that I may have overlooked!,please let know creative testing issue may,issue,positive,positive,positive,positive,positive,positive
1736070578,"Thanks - yeah, manually adding the route works. Our use case is that, there are already some wsgi/asgi applications that needs to be mounted (like fastapi, flask, and gradio), and there isn't possibility to manually write `@app.get`. As a result, we will basically need the app to mount another app (which is the one I defined in the __init__ function called `subapp`).",thanks yeah manually route work use case already need mounted like flask possibility manually write result basically need mount another one defined function,issue,positive,positive,positive,positive,positive,positive
1736049382,"> Yup fair enough. I was thinking we could keep it purely internal and only have the shuffle vs non-shuffle block configs, to keep it simpler for the user level.

Sounds good, let me try it and see. I do think the right way to expose it at the user level might be through a `repartition` call instead of setting it on each logical op, so it would make sense to do it.",fair enough thinking could keep purely internal shuffle block keep simpler user level good let try see think right way expose user level might repartition call instead setting logical would make sense,issue,negative,positive,positive,positive,positive,positive
1736032479,"2.7 should have patches that mitigate this - but this is essentially this https://github.com/ray-project/ray/issues/38189 as well. 

Current plan is to fix in 2.8. ",mitigate essentially well current plan fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1736027420,"I reran `disconnects.py` three times, and it passed each time without flaking. This change seems to fix the test.",three time time without change fix test,issue,negative,neutral,neutral,neutral,neutral,neutral
1736001753,"Lint failed: https://buildkite.com/ray-project/premerge/builds/6504#018ad097-37b8-460b-b2c0-9fae441269bb/185-440

You can run setup_hooks.sh to install a pre-push hook that will automatically run lint and prevent this issue",lint run install hook automatically run lint prevent issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1735999443,Per the above - we are now closing as we believe Ray 2.5+ has GC operating correctly. @merrysailor please re-open if you still observe this issue.,per believe ray operating correctly please still observe issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1735985691,cc @rickyyx can you try and repro and see if this is fixed since ray27 release in the v2 oss autoscaler?,try see fixed since ray release,issue,negative,positive,neutral,neutral,positive,positive
1735982276,@akshay-anyscale remaining work is on Serve side; can you please triage and prioritize for next Ray release?,work serve side please triage next ray release,issue,negative,neutral,neutral,neutral,neutral,neutral
1735978073,"@jjyao The users can do something like `ScalingConfig(num_workers=8, resources_per_worker={""neuron_cores"": 1})` with the current API. But as we incorporate more accelerator, the `use_gpu` flag would be a bit confusing. 

Possibly we can update the apis as `ScalingConfig(accelerator=""gpu/neuron/tpu"")`",something like current incorporate accelerator flag would bit possibly update,issue,negative,neutral,neutral,neutral,neutral,neutral
1735976416,Do you know if it's disk usage or object store memory itself that's leaked? It would be useful to post the graphs of physical disk as well as the logical object store memory usage from the Ray dashboard here,know disk usage object store memory would useful post physical disk well logical object store memory usage ray dashboard,issue,positive,positive,positive,positive,positive,positive
1735974441,"Absolutely! Let me know if you need any pointers

On Tue, Sep 26, 2023 at 10:22 AM Leption ***@***.***> wrote:

> Hello!
> Can i work on this?
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/ray-project/ray/issues/39842#issuecomment-1735971490>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ABJU5RX2UZ25BOHOCIHZ2ALX4MFMBANCNFSM6AAAAAA5G2EWNU>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>
",absolutely let know need tue wrote hello work reply directly view id,issue,negative,positive,positive,positive,positive,positive
1735970704,"@jjyao right now this can be used by setting `ScalingConfig.resources_per_worker` and ignoring the `use_gpu` flag. In the future we may be able to further extend the API to be more friendly for different accelerator types. 

See https://github.com/ray-project/ray/pull/39130 for more info",right used setting flag future may able extend friendly different accelerator see,issue,positive,positive,positive,positive,positive,positive
1735968103,At least investigate and root cause this. cc @jjyao @rkooo567 ,least investigate root cause,issue,negative,negative,negative,negative,negative,negative
1735966213,Fixing in v2 is P0 to explicitly scope the problem here.,fixing explicitly scope problem,issue,negative,neutral,neutral,neutral,neutral,neutral
1735962197,"So I tried to manage to replicate the issue with an example environment StatelessCartPole. When I am running the following script

```python
from ray.rllib.examples.env.stateless_cartpole import StatelessCartPole
from ray.rllib.algorithms.ppo import PPO

model_dict={""use_attention"": True,
                ""max_seq_len"": 10,
                ""attention_num_transformer_units"": 1,
                ""attention_dim"": 32,
                ""attention_memory_inference"": 10,
                ""attention_memory_training"": 10,
                ""attention_num_heads"": 1,
                ""attention_head_dim"": 32,
                ""attention_position_wise_mlp_dim"": 32,
            }

nn_config= {
        # config to pass to env class
        # ""env_config"": env_config,
        #neural network config
        ""lr"": 0.003,
        ""model"": model_dict,
        ""gamma"": 0.95,
        ""train_batch_size"":20_000,
        ""num_rollout_worker"":1,
        ""training"": {""_enable_learner_api"": False},
        ""rl_module"": {'_enable_rl_module_api':False},
    }

nn_kwargs = {""env"": StatelessCartPole,
            ""config"": nn_config
                    }

a = PPO(**nn_kwargs)

print(a)
```

I am getting the following error message

```
023-09-26 13:01:32,694 ERROR actor_manager.py:500 -- Ray error, taking actor 1 out of service. The actor died because of an error raised in its creation task, ray::RolloutWorker.__init__() (pid=49587, ip=127.0.0.1, actor_id=aa728f30adf8893bd810948a01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x103033390>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/evaluation/rollout_worker.py"", line 525, in __init__
    self._update_policy_map(policy_dict=self.policy_dict)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/evaluation/rollout_worker.py"", line 1727, in _update_policy_map
    self._build_policy_map(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/evaluation/rollout_worker.py"", line 1838, in _build_policy_map
    new_policy = create_policy_for_framework(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/utils/policy.py"", line 142, in create_policy_for_framework
    return policy_class(observation_space, action_space, merged_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo_torch_policy.py"", line 49, in __init__
    TorchPolicyV2.__init__(
  File ""/...../lib/python3.11/site-packages/ray/rllib/policy/torch_policy_v2.py"", line 92, in __init__
    model = self.make_rl_module()
            ^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/policy/policy.py"", line 424, in make_rl_module
    marl_module = marl_spec.build()
                  ^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/marl_module.py"", line 462, in build
    module = self.marl_module_class(module_config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 315, in new_init
    previous_init(self, *args, **kwargs)
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/marl_module.py"", line 58, in __init__
    super().__init__(config or MultiAgentRLModuleConfig())
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 307, in __init__
    self.setup()
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/marl_module.py"", line 65, in setup
    self._rl_modules[module_id] = module_spec.build()
                                  ^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 104, in build
    module = self.module_class(module_config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 315, in new_init
    previous_init(self, *args, **kwargs)
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 315, in new_init
    previous_init(self, *args, **kwargs)
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/torch/torch_rl_module.py"", line 82, in __init__
    RLModule.__init__(self, *args, **kwargs)
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 307, in __init__
    self.setup()
  File ""/...../lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo_rl_module.py"", line 20, in setup
    catalog = self.config.get_catalog()
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 189, in get_catalog
    return self.catalog_class(
           ^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo_catalog.py"", line 69, in __init__
    super().__init__(
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/models/catalog.py"", line 111, in __init__
    self._determine_components_hook()
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/models/catalog.py"", line 131, in _determine_components_hook
    self._encoder_config = self._get_encoder_config(
                           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/models/catalog.py"", line 283, in _get_encoder_config
    raise NotImplementedError
NotImplementedError
2023-09-26 13:01:32,697 ERROR actor_manager.py:500 -- Ray error, taking actor 2 out of service. The actor died because of an error raised in its creation task, ray::RolloutWorker.__init__() (pid=49588, ip=127.0.0.1, actor_id=68806610df392e6e5d041b2e01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x106e76010>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/evaluation/rollout_worker.py"", line 525, in __init__
    self._update_policy_map(policy_dict=self.policy_dict)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/evaluation/rollout_worker.py"", line 1727, in _update_policy_map
    self._build_policy_map(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/evaluation/rollout_worker.py"", line 1838, in _build_policy_map
    new_policy = create_policy_for_framework(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/utils/policy.py"", line 142, in create_policy_for_framework
    return policy_class(observation_space, action_space, merged_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo_torch_policy.py"", line 49, in __init__
    TorchPolicyV2.__init__(
  File ""/...../lib/python3.11/site-packages/ray/rllib/policy/torch_policy_v2.py"", line 92, in __init__
    model = self.make_rl_module()
            ^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/policy/policy.py"", line 424, in make_rl_module
    marl_module = marl_spec.build()
                  ^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/marl_module.py"", line 462, in build
    module = self.marl_module_class(module_config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 315, in new_init
    previous_init(self, *args, **kwargs)
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/marl_module.py"", line 58, in __init__
    super().__init__(config or MultiAgentRLModuleConfig())
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 307, in __init__
    self.setup()
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/marl_module.py"", line 65, in setup
    self._rl_modules[module_id] = module_spec.build()
                                  ^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 104, in build
    module = self.module_class(module_config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 315, in new_init
    previous_init(self, *args, **kwargs)
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 315, in new_init
    previous_init(self, *args, **kwargs)
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/torch/torch_rl_module.py"", line 82, in __init__
    RLModule.__init__(self, *args, **kwargs)
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 307, in __init__
    self.setup()
  File ""/...../lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo_rl_module.py"", line 20, in setup
    catalog = self.config.get_catalog()
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 189, in get_catalog
    return self.catalog_class(
           ^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo_catalog.py"", line 69, in __init__
    super().__init__(
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/models/catalog.py"", line 111, in __init__
    self._determine_components_hook()
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/models/catalog.py"", line 131, in _determine_components_hook
    self._encoder_config = self._get_encoder_config(
                           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/models/catalog.py"", line 283, in _get_encoder_config
    raise NotImplementedError
NotImplementedError
Traceback (most recent call last):
  File ""/...../lib/python3.11/site-packages/ray/rllib/evaluation/worker_set.py"", line 157, in __init__
    self._setup(
  File ""/...../lib/python3.11/site-packages/ray/rllib/evaluation/worker_set.py"", line 227, in _setup
    self.add_workers(
  File ""/...../lib/python3.11/site-packages/ray/rllib/evaluation/worker_set.py"", line 593, in add_workers
    raise result.get()
  File ""/...../lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py"", line 481, in __fetch_result
    result = ray.get(r)
             ^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/_private/auto_init_hook.py"", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/_private/worker.py"", line 2526, in get
    raise value
ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, ray::RolloutWorker.__init__() (pid=49587, ip=127.0.0.1, actor_id=aa728f30adf8893bd810948a01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x103033390>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/evaluation/rollout_worker.py"", line 525, in __init__
    self._update_policy_map(policy_dict=self.policy_dict)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/evaluation/rollout_worker.py"", line 1727, in _update_policy_map
    self._build_policy_map(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/evaluation/rollout_worker.py"", line 1838, in _build_policy_map
    new_policy = create_policy_for_framework(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/utils/policy.py"", line 142, in create_policy_for_framework
    return policy_class(observation_space, action_space, merged_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo_torch_policy.py"", line 49, in __init__
    TorchPolicyV2.__init__(
  File ""/...../lib/python3.11/site-packages/ray/rllib/policy/torch_policy_v2.py"", line 92, in __init__
    model = self.make_rl_module()
            ^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/policy/policy.py"", line 424, in make_rl_module
    marl_module = marl_spec.build()
                  ^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/marl_module.py"", line 462, in build
    module = self.marl_module_class(module_config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 315, in new_init
    previous_init(self, *args, **kwargs)
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/marl_module.py"", line 58, in __init__
    super().__init__(config or MultiAgentRLModuleConfig())
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 307, in __init__
    self.setup()
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/marl_module.py"", line 65, in setup
    self._rl_modules[module_id] = module_spec.build()
                                  ^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 104, in build
    module = self.module_class(module_config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 315, in new_init
    previous_init(self, *args, **kwargs)
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 315, in new_init
    previous_init(self, *args, **kwargs)
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/torch/torch_rl_module.py"", line 82, in __init__
    RLModule.__init__(self, *args, **kwargs)
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 307, in __init__
    self.setup()
  File ""/...../lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo_rl_module.py"", line 20, in setup
    catalog = self.config.get_catalog()
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 189, in get_catalog
    return self.catalog_class(
           ^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo_catalog.py"", line 69, in __init__
    super().__init__(
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/models/catalog.py"", line 111, in __init__
    self._determine_components_hook()
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/models/catalog.py"", line 131, in _determine_components_hook
    self._encoder_config = self._get_encoder_config(
                           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/core/models/catalog.py"", line 283, in _get_encoder_config
    raise NotImplementedError
NotImplementedError
 
During handling of the above exception, another exception occurred:
 
Traceback (most recent call last):
  File ""/Users/paula/Desktop/Projects/RL Practice/RLLIB_Practice4/stateless_cartpole_attention.py"", line 40, in <module>
    a = PPO(**nn_kwargs)
        ^^^^^^^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py"", line 517, in __init__
    super().__init__(
  File ""/...../lib/python3.11/site-packages/ray/tune/trainable/trainable.py"", line 169, in __init__
    self.setup(copy.deepcopy(self.config))
  File ""/...../lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py"", line 639, in setup
    self.workers = WorkerSet(
                   ^^^^^^^^^^
  File ""/...../lib/python3.11/site-packages/ray/rllib/evaluation/worker_set.py"", line 179, in __init__
    raise e.args[0].args[2]
NotImplementedError
(RolloutWorker pid=49588) 2023-09-26 13:01:32,674       WARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
(RolloutWorker pid=49588) Exception raised in creation task: The actor died because of an error raised in its creation task, ray::RolloutWorker.__init__() (pid=49588, ip=127.0.0.1, actor_id=68806610df392e6e5d041b2e01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x106e76010>)
(RolloutWorker pid=49588)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(RolloutWorker pid=49588)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(RolloutWorker pid=49588)   File ""/...../lib/python3.11/site-packages/ray/rllib/evaluation/rollout_worker.py"", line 525, in __init__
(RolloutWorker pid=49588)     self._update_policy_map(policy_dict=self.policy_dict)
(RolloutWorker pid=49588)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(RolloutWorker pid=49588)   File ""/...../lib/python3.11/site-packages/ray/rllib/evaluation/rollout_worker.py"", line 1727, in _update_policy_map
(RolloutWorker pid=49588)     self._build_policy_map(
(RolloutWorker pid=49588)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(RolloutWorker pid=49588)   File ""/...../lib/python3.11/site-packages/ray/rllib/evaluation/rollout_worker.py"", line 1838, in _build_policy_map
(RolloutWorker pid=49588)     new_policy = create_policy_for_framework(
(RolloutWorker pid=49588)                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(RolloutWorker pid=49588)   File ""/...../lib/python3.11/site-packages/ray/rllib/utils/policy.py"", line 142, in create_policy_for_framework
(RolloutWorker pid=49588)     return policy_class(observation_space, action_space, merged_config)
(RolloutWorker pid=49588)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(RolloutWorker pid=49588)   File ""/...../lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo_torch_policy.py"", line 49, in __init__
(RolloutWorker pid=49588)     TorchPolicyV2.__init__(
(RolloutWorker pid=49588)   File ""/...../lib/python3.11/site-packages/ray/rllib/policy/torch_policy_v2.py"", line 92, in __init__
(RolloutWorker pid=49588)     model = self.make_rl_module()
(RolloutWorker pid=49588)             ^^^^^^^^^^^^^^^^^^^^^
(RolloutWorker pid=49588)   File ""/...../lib/python3.11/site-packages/ray/rllib/policy/policy.py"", line 424, in make_rl_module
(RolloutWorker pid=49588)     marl_module = marl_spec.build()
(RolloutWorker pid=49588)                   ^^^^^^^^^^^^^^^^^
(RolloutWorker pid=49588)   File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/marl_module.py"", line 462, in build
(RolloutWorker pid=49588)     module = self.marl_module_class(module_config)
(RolloutWorker pid=49588)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(RolloutWorker pid=49588)   File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 315, in new_init
(RolloutWorker pid=49588)     previous_init(self, *args, **kwargs)
(RolloutWorker pid=49588)   File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/marl_module.py"", line 58, in __init__
(RolloutWorker pid=49588)     super().__init__(config or MultiAgentRLModuleConfig())
(RolloutWorker pid=49588)   File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 307, in __init__
(RolloutWorker pid=49588)     self.setup()
(RolloutWorker pid=49588)   File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/marl_module.py"", line 65, in setup
(RolloutWorker pid=49588)     self._rl_modules[module_id] = module_spec.build()
(RolloutWorker pid=49588)                                   ^^^^^^^^^^^^^^^^^^^
(RolloutWorker pid=49588)   File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 104, in build
(RolloutWorker pid=49588)     module = self.module_class(module_config)
(RolloutWorker pid=49588)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(RolloutWorker pid=49588)   File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 315, in new_init
(RolloutWorker pid=49588)     previous_init(self, *args, **kwargs)
(RolloutWorker pid=49588)   File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 315, in new_init
(RolloutWorker pid=49588)     previous_init(self, *args, **kwargs)
(RolloutWorker pid=49588)   File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/torch/torch_rl_module.py"", line 82, in __init__
(RolloutWorker pid=49588)     RLModule.__init__(self, *args, **kwargs)
(RolloutWorker pid=49588)   File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 307, in __init__
(RolloutWorker pid=49588)     self.setup()
(RolloutWorker pid=49588)   File ""/...../lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo_rl_module.py"", line 20, in setup
(RolloutWorker pid=49588)     catalog = self.config.get_catalog()
(RolloutWorker pid=49588)               ^^^^^^^^^^^^^^^^^^^^^^^^^
(RolloutWorker pid=49588)   File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 189, in get_catalog
(RolloutWorker pid=49588)     return self.catalog_class(
(RolloutWorker pid=49588)            ^^^^^^^^^^^^^^^^^^^
(RolloutWorker pid=49588)   File ""/...../lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo_catalog.py"", line 69, in __init__
(RolloutWorker pid=49588)     super().__init__(
(RolloutWorker pid=49588)   File ""/...../lib/python3.11/site-packages/ray/rllib/core/models/catalog.py"", line 111, in __init__
(RolloutWorker pid=49588)     self._determine_components_hook()
(RolloutWorker pid=49588)   File ""/...../lib/python3.11/site-packages/ray/rllib/core/models/catalog.py"", line 131, in _determine_components_hook
(RolloutWorker pid=49588)     self._encoder_config = self._get_encoder_config(
(RolloutWorker pid=49588)                            ^^^^^^^^^^^^^^^^^^^^^^^^
(RolloutWorker pid=49588)   File ""/...../lib/python3.11/site-packages/ray/rllib/core/models/catalog.py"", line 283, in _get_encoder_config
(RolloutWorker pid=49588)     raise NotImplementedError
(RolloutWorker pid=49588) NotImplementedError
(RolloutWorker pid=49587) 2023-09-26 13:01:32,673       WARNING env.py:162 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.
(pid=49587) DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
(RolloutWorker pid=49587) 2023-09-26 13:01:32,674       WARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
(RolloutWorker pid=49587) Exception raised in creation task: The actor died because of an error raised in its creation task, ray::RolloutWorker.__init__() (pid=49587, ip=127.0.0.1, actor_id=aa728f30adf8893bd810948a01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x103033390>)
(RolloutWorker pid=49587)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(RolloutWorker pid=49587)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)
(RolloutWorker pid=49587)   File ""/...../lib/python3.11/site-packages/ray/rllib/core/models/catalog.py"", line 111, in __init__ [repeated 9x across cluster]
(RolloutWorker pid=49587)     self._update_policy_map(policy_dict=self.policy_dict)
(RolloutWorker pid=49587)   File ""/...../lib/python3.11/site-packages/ray/rllib/evaluation/rollout_worker.py"", line 1727, in _update_policy_map
(RolloutWorker pid=49587)     self._build_policy_map(
(RolloutWorker pid=49587)   File ""/...../lib/python3.11/site-packages/ray/rllib/evaluation/rollout_worker.py"", line 1838, in _build_policy_map
(RolloutWorker pid=49587)     new_policy = create_policy_for_framework(
(RolloutWorker pid=49587)                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(RolloutWorker pid=49587)   File ""/...../lib/python3.11/site-packages/ray/rllib/utils/policy.py"", line 142, in create_policy_for_framework
(RolloutWorker pid=49587)     return policy_class(observation_space, action_space, merged_config)
(RolloutWorker pid=49587)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(RolloutWorker pid=49587)     TorchPolicyV2.__init__(
(RolloutWorker pid=49587)     model = self.make_rl_module()
(RolloutWorker pid=49587)             ^^^^^^^^^^^^^^^^^^^^^
(RolloutWorker pid=49587)   File ""/...../lib/python3.11/site-packages/ray/rllib/policy/policy.py"", line 424, in make_rl_module
(RolloutWorker pid=49587)     marl_module = marl_spec.build()
(RolloutWorker pid=49587)                   ^^^^^^^^^^^^^^^^^
(RolloutWorker pid=49587)   File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 104, in build [repeated 2x across cluster]
(RolloutWorker pid=49587)     module = self.marl_module_class(module_config)
(RolloutWorker pid=49587)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(RolloutWorker pid=49587)   File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 315, in new_init [repeated 3x across cluster]
(RolloutWorker pid=49587)     previous_init(self, *args, **kwargs) [repeated 3x across cluster]
(RolloutWorker pid=49587)     super().__init__(config or MultiAgentRLModuleConfig())
(RolloutWorker pid=49587)     self.setup() [repeated 2x across cluster]
(RolloutWorker pid=49587)   File ""/...../lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo_rl_module.py"", line 20, in setup [repeated 2x across cluster]
(RolloutWorker pid=49587)     self._rl_modules[module_id] = module_spec.build()
(RolloutWorker pid=49587)            ^^^^^^^^^^^^^^^^^^^ [repeated 2x across cluster]
(RolloutWorker pid=49587)     module = self.module_class(module_config)
(RolloutWorker pid=49587)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(RolloutWorker pid=49587)     RLModule.__init__(self, *args, **kwargs)
(RolloutWorker pid=49587)     catalog = self.config.get_catalog()
(RolloutWorker pid=49587)                            ^^^^^^^^^^^^^^^^^^^^^^^^^ [repeated 2x across cluster]
(RolloutWorker pid=49587)   File ""/...../lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py"", line 189, in get_catalog
(RolloutWorker pid=49587)     return self.catalog_class(
(RolloutWorker pid=49587)     super().__init__(
(RolloutWorker pid=49587)     self._determine_components_hook()
(RolloutWorker pid=49587)   File ""/...../lib/python3.11/site-packages/ray/rllib/core/models/catalog.py"", line 131, in _determine_components_hook
(RolloutWorker pid=49587)     self._encoder_config = self._get_encoder_config(
(RolloutWorker pid=49587)   File ""/...../lib/python3.11/site-packages/ray/rllib/core/models/catalog.py"", line 283, in _get_encoder_config
(RolloutWorker pid=49587)     raise NotImplementedError
(RolloutWorker pid=49587) NotImplementedError

```

Is it related to the issue you mentioned? and how can i resolve it? Thanks",tried manage replicate issue example environment running following script python import import true pas class neural network model gamma training false false print getting following error message error ray error taking actor service actor error raised creation task ray object file line file line file line file line return file line file line model file line file line build module file line self file line super file line file line setup file line build module file line self file line self file line self file line file line setup file line return file line super file line file line file line raise error ray error taking actor service actor error raised creation task ray object file line file line file line file line return file line file line model file line file line build module file line self file line super file line file line setup file line build module file line self file line self file line self file line file line setup file line return file line super file line file line file line raise recent call last file line file line file line raise file line result file line return file line wrapper return file line get raise value actor error raised creation task ray object file line file line file line file line return file line file line model file line file line build module file line self file line super file line file line setup file line build module file line self file line self file line self file line file line setup file line return file line super file line file line file line raise handling exception another exception recent call last file line module file line super file line file line setup file line raise warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done exception raised creation task actor error raised creation task ray object file line file line file line file line return file line file line model file line file line build module file line self file line super file line file line setup file line build module file line self file line self file line self file line file line setup file line return file line super file line file line file line raise warning attribute horizon default infinity environment reset raise error future warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done exception raised creation task actor error raised creation task ray object repeated across cluster ray default set disable log deduplication see file line repeated across cluster file line file line file line return model file line file line build repeated across cluster module file line repeated across cluster self repeated across cluster super repeated across cluster file line setup repeated across cluster repeated across cluster module self repeated across cluster file line return super file line file line raise related issue resolve thanks,issue,positive,positive,positive,positive,positive,positive
1735955293,@chappidim One concern is that these features have not yet been tested. Is it possible for you to provide AWS Neuron instances and test these code? (Our current CI is running on general `g4dn` and `m5` instances.),one concern yet tested possible provide neuron test code current running general,issue,negative,positive,neutral,neutral,positive,positive
1735819082,"@matthewdeng besides this PR, what are the other changes needed to support different accelerators? For example, E.g. Currently `ScalingConfig` has a `use_gpu` flag, do we need to change that?",besides support different example currently flag need change,issue,negative,neutral,neutral,neutral,neutral,neutral
1735770575,Too much to revert. Create a new branch.,much revert create new branch,issue,negative,positive,positive,positive,positive,positive
1735635728,"> Q: can we hide batch from the API and just do internally? Maybe we can set a parameter to actor pool or submit API instead?

Good idea. Let me take some time to design it.",hide batch internally maybe set parameter actor pool submit instead good idea let take time design,issue,negative,positive,positive,positive,positive,positive
1735244993,"@c21 

The delta reading logic has some issue in this PR,
we have to suspend this task but waiting for delta-rs-kernel package readiness,

but I have another PR relating to Ray data reader for databricks users, could you take a look ?
https://github.com/ray-project/ray/pull/39852",delta reading logic issue suspend task waiting package readiness another ray data reader could take look,issue,negative,neutral,neutral,neutral,neutral,neutral
1735240309,"@anyscalesam I don't think that answers this question, it's still not clear how the ray autoscaler decides which node type to start.",think question still clear ray node type start,issue,negative,positive,positive,positive,positive,positive
1735239483,"Not really, I'm looking for help launching spot instances reliably.",really looking help spot reliably,issue,negative,positive,positive,positive,positive,positive
1735105039,"I think the error indicates an out-of-memory problem when calling CreateFileMapping ([error code 1455](https://learn.microsoft.com/en-us/windows/win32/debug/system-error-codes--1300-1699-) indicates ""ERROR_COMMITMENT_LIMIT: The paging file is too small for this operation to complete""). Is there a reason you use Windows in a cloud cluster? In general linux is to be preferred since it is better tested and usually costs less. 

Your monitoring seems to show a memory spike around 5.5 hours, and goes over 100% a bit later. Maybe that is connected to the out-of-memory error?

You start your report with ""For the past few days..."". Was it working differently before that?",think error problem calling error code file small operation complete reason use cloud cluster general preferred since better tested usually le show memory spike around go bit later maybe connected error start report past day working differently,issue,negative,negative,neutral,neutral,negative,negative
1735062047,"> The new API seems a bit complicated for someone unfamiliar with vSphere, but this might be unavoidable. I think the following suggestions might mitigate this:
> 
> * Are there references in vSphere docs for the new terms ""library item"", ""resource pool"", ""datastore"" etc? It would be good to link to these.
> * I think adding some example config snippets for `frozen_vm` in the docs would go a long way! The ones in the PR description might be a good starting point.
> * There are a lot of constraints of the form ""X must be specified, or if Y is specified Z must also be specified"". Can we make sure these constraints are validated and can we add unit tests to make sure they fail fast with user friendly errors?
> 
> Other than this, looks good! Just minor comments.
> 
> The PR is a bit large, in the future it would be great to submit a series of smaller PRs.

1. Reference added. The initial thought is that: the people who want to run Ray on vSphere should already have some basic knowledge of vSphere. This is a sellable product, it is a rare case that a person buy it but doesn't learn about it. 😄 
2. Example snipptes added.
3. We will add this in the next PR, basically my idea is to add a validator function to check the node config at the early stage, covering all the combinations. Then add a UT covers all the cases for the validator function.

The large PR is because we didn't know that you have a code freeze and cherry pick process. We did this change in our internal repo with 8 small MRs, we intentionally made them on-hold to wait your 2.7.0 tag. But actually we shouldn't have worried about your 2.7.0 release because the commits will not be cherry-picked if we made consensus on this in our Slack channel.

From now on we will only raise small PRs, and we will pin the ones we want you to help cherry-pick in our Slack channel.

",new bit complicated someone unfamiliar might unavoidable think following might mitigate new library item resource pool would good link think example would go long way description might good starting point lot form must must also make sure add unit make sure fail fast user friendly good minor bit large future would great submit series smaller reference added initial thought people want run ray already basic knowledge sellable product rare case person buy learn example added add next basically idea add function check node early stage covering add ut function large know code freeze cherry pick process change internal small intentionally made wait tag actually worried release made consensus slack channel raise small pin want help slack channel,issue,positive,positive,positive,positive,positive,positive
1734986035,"> 1. Is it a regression?
> 2. When is the plan to migrate to V2?

1. Not a regression, has been around in v1.
2. Will be fixed in v2. ",regression plan migrate regression around fixed,issue,negative,positive,neutral,neutral,positive,positive
1734912374,"Our usage pattern is to have heavy load for several hours, followed by idle time of several hours to one day, and this cycle repeats. Could this be related to the issue? The problem always occurs after the idle period.",usage pattern heavy load several idle time several one day cycle could related issue problem always idle period,issue,negative,negative,neutral,neutral,negative,negative
1734890280,"> Based on the logs, it seems like it couldn't communicate with GCS. This commonly happens when the GCS is terminated. Can you check the log of gcs_server.out around the time this issue happened?

dashboard.log:
```
2023-09-26 13:39:51,521 ERROR head.py:187 -- Failed to check gcs health, client timed out.
2023-09-26 13:40:07,524 ERROR head.py:187 -- Failed to check gcs health, client timed out.
2023-09-26 13:40:23,527 ERROR head.py:187 -- Failed to check gcs health, client timed out.
2023-09-26 13:40:39,531 ERROR head.py:187 -- Failed to check gcs health, client timed out.
2023-09-26 13:40:39,532 ERROR head.py:198 -- Dashboard exiting because it received too many GCS RPC errors count: 41, threshold is 40.
```
gcs_server.out:
```

[2023-09-26 13:38:13,844 I 3276 3276] (gcs_server) gcs_server.cc:873: GcsTaskManager Event stats:


Global stats: 0 total (0 active)
Queueing time: mean = -nan s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
Execution time:  mean = -nan s, total = 0.000 s
Event stats:


[2023-09-26 13:39:13,844 I 3276 3276] (gcs_server) gcs_server.cc:255: GcsNodeManager:
- RegisterNode request count: 1
- DrainNode request count: 0
- GetAllNodeInfo request count: 333204
- GetInternalConfig request count: 1

GcsActorManager:
- RegisterActor request count: 0
- CreateActor request count: 0
- GetActorInfo request count: 0
- GetNamedActorInfo request count: 0
- GetAllActorInfo request count: 1
- KillActor request count: 0
- ListNamedActors request count: 0
- Registered actors count: 0
- Destroyed actors count: 0
- Named actors count: 0
- Unresolved actors count: 0
- Pending actors count: 0
- Created actors count: 0
- owners_: 0
- actor_to_register_callbacks_: 0
- actor_to_create_callbacks_: 0
- sorted_destroyed_actor_list_: 0

GcsResourceManager:
- GetResources request count: 0
- GetAllAvailableResources request count304312
- ReportResourceUsage request count: 0
- GetAllResourceUsage request count: 14438

GcsPlacementGroupManager:
- CreatePlacementGroup request count: 0
- RemovePlacementGroup request count: 0
- GetPlacementGroup request count: 0
- GetAllPlacementGroup request count: 0
- WaitPlacementGroupUntilReady request count: 0
- GetNamedPlacementGroup request count: 0
- Scheduling pending placement group count: 0
- Registered placement groups count: 0
- Named placement group count: 0
- Pending placement groups count: 0
- Infeasible placement groups count: 0

GcsPublisher {}

[runtime env manager] ID to URIs table:
[runtime env manager] URIs reference table:

GcsTaskManager:
-Total num task events reported: 0
-Total num status task events dropped: 0
-Total num profile events dropped: 0
-Total num bytes of task event stored: 0MiB
-Current num of task events stored: 0
-Total num of actor creation tasks: 0
-Total num of actor tasks: 0
-Total num of normal tasks: 0
-Total num of driver tasks: 0


[2023-09-26 13:39:13,844 I 3276 3276] (gcs_server) gcs_server.cc:872: Event stats:


Global stats: 1488739 total (4 active)
Queueing time: mean = 624.283 us, max = 38.023 s, min = -0.001 s, total = 929.395 s
Execution time:  mean = 139.046 us, total = 207.004 s
Event stats:
        NodeInfoGcsService.grpc_server.GetAllNodeInfo - 333204 total (0 active), CPU time: mean = 53.961 us, total = 17.980 s
        NodeResourceInfoGcsService.grpc_server.GetAllAvailableResources - 304312 total (0 active), CPU time: mean = 85.579 us, total = 26.043 s
        GcsInMemoryStore.Put - 187959 total (0 active), CPU time: mean = 38.312 us, total = 7.201 s
        InternalKVGcsService.grpc_server.InternalKVPut - 187955 total (0 active), CPU time: mean = 34.849 us, total = 6.550 s
        InternalKVGcsService.grpc_client.InternalKVPut - 86838 total (0 active), CPU time: mean = 16.821 us, total = 1.461 s
        RayletLoadPulled - 72353 total (1 active), CPU time: mean = 209.512 us, total = 15.159 s
        NodeManagerService.grpc_client.GetResourceLoad - 72350 total (0 active), CPU time: mean = 38.213 us, total = 2.765 s
        GcsInMemoryStore.Get - 71892 total (0 active), CPU time: mean = 38.854 us, total = 2.793 s
        InternalKVGcsService.grpc_server.InternalKVGet - 71890 total (0 active), CPU time: mean = 48.089 us, total = 3.457 s
        NodeInfoGcsService.grpc_server.CheckAlive - 28837 total (0 active), CPU time: mean = 47.886 us, total = 1.381 s
        UNKNOWN - 24128 total (1 active), CPU time: mean = 11.393 us, total = 274.895 ms
        HealthCheck - 24118 total (0 active), CPU time: mean = 8.396 us, total = 202.492 ms
        NodeResourceInfoGcsService.grpc_server.GetAllResourceUsage - 14438 total (0 active), CPU time: mean = 105.394 us, total = 1.522 s
        GCSServer.deadline_timer.debug_state_dump - 7237 total (1 active), CPU time: mean = 16.502 ms, total = 119.422 s
        GCSServer.deadline_timer.debug_state_event_stats_print - 1208 total (1 active, 1 running), CPU time: mean = 655.797 us, total = 792.202 ms
        GcsInMemoryStore.GetAll - 8 total (0 active), CPU time: mean = 21.456 us, total = 171.646 us
        PeriodicalRunner.RunFnPeriodically - 4 total (0 active), CPU time: mean = 85.590 us, total = 342.360 us
        InternalKVGcsService.grpc_server.InternalKVDel - 1 total (0 active), CPU time: mean = 52.823 us, total = 52.823 us
        GcsResourceManager::Update - 1 total (0 active), CPU time: mean = 101.194 us, total = 101.194 us
        NodeInfoGcsService.grpc_server.RegisterNode - 1 total (0 active), CPU time: mean = 186.007 us, total = 186.007 us
        JobInfoGcsService.grpc_server.GetAllJobInfo - 1 total (0 active), CPU time: mean = 88.961 us, total = 88.961 us
        NodeInfoGcsService.grpc_server.GetInternalConfig - 1 total (0 active), CPU time: mean = 34.975 us, total = 34.975 us
        GcsInMemoryStore.Delete - 1 total (0 active), CPU time: mean = 28.294 us, total = 28.294 us
        ActorInfoGcsService.grpc_server.GetAllActorInfo - 1 total (0 active), CPU time: mean = 25.894 us, total = 25.894 us
        GcsHealthCheckManager::AddNode - 1 total (0 active), CPU time: mean = 16.142 us, total = 16.142 us
```
and there is nothing in gcs_server.err.
The gcs_server process looks fine:
```
root@f5ff1ecac469:/tmp/ray/session_latest/logs# ps axu|grep gcs_server
root      3276  1.4  0.4 2661512 150788 ?      Sl   Sep25  18:34 /root/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2023-09-25_17-31-09_169504_3228/logs --config_list=eyJvYmplY3Rfc3BpbGxpbmdfY29uZmlnIjogIntcInR5cGVcIjogXCJmaWxlc3lzdGVtXCIsIFwicGFyYW1zXCI6IHtcImRpcmVjdG9yeV9wYXRoXCI6IFwiL3RtcC9yYXkvc2Vzc2lvbl8yMDIzLTA5LTI1XzE3LTMxLTA5XzE2OTUwNF8zMjI4XCJ9fSIsICJpc19leHRlcm5hbF9zdG9yYWdlX3R5cGVfZnMiOiB0cnVlfQ== --gcs_server_port=9270 --metrics-agent-port=45215 --node-ip-address=172.17.0.3 --session-name=session_2023-09-25_17-31-09_169504_3228

root@f5ff1ecac469:/tmp/ray/session_latest/logs# telnet 172.17.0.3 9270
Trying 172.17.0.3...
Connected to 172.17.0.3.
Escape character is '^]'.
Connection closed by foreign host.
root@f5ff1ecac469:/tmp/ray/session_latest/logs# telnet 172.17.0.3 45215
Trying 172.17.0.3...
Connected to 172.17.0.3.
Escape character is '^]'.
```",based like could communicate commonly check log around time issue error check health client timed error check health client timed error check health client timed error check health client timed error dashboard received many count threshold event global total active time mean min total execution time mean total event request count request count request count request count request count request count request count request count request count request count request count registered count count count unresolved count pending count count request count request count request count request count request count request count request count request count request count request count pending placement group count registered placement count placement group count pending placement count infeasible placement count manager id table manager reference table task status task profile task event mib task actor creation actor normal driver event global total active time mean u min total execution time mean u total event total active time mean u total total active time mean u total total active time mean u total total active time mean u total total active time mean u total total active time mean u total total active time mean u total total active time mean u total total active time mean u total total active time mean u total unknown total active time mean u total total active time mean u total total active time mean u total total active time mean total total active running time mean u total total active time mean u total u total active time mean u total u total active time mean u total u total active time mean u total u total active time mean u total u total active time mean u total u total active time mean u total u total active time mean u total u total active time mean u total u total active time mean u total u nothing process fine root root root trying connected escape character connection closed foreign host root trying connected escape character,issue,positive,negative,negative,negative,negative,negative
1734862319,"@alanwguo @chaowanggg
If possible, we should include it in 2.7.1. A really bad UI bug.

",possible include really bad bug,issue,negative,negative,negative,negative,negative,negative
1734843016,"> @JingChen23 Looks like there are some potentially relevant test failures: https://buildkite.com/ray-project/premerge/builds/6411

Thanks Archit, this is bacause I forgot to checkout the change on the UT file from our internal repo.",like potentially relevant test thanks forgot change ut file internal,issue,positive,positive,positive,positive,positive,positive
1734652001,@simonsays1980 thanks for reporting this; have you been able to workaround the issue? venv Ray support is something we're aware of. conda envs atypically plays nicer.,thanks able issue ray support something aware atypically,issue,positive,positive,positive,positive,positive,positive
1734649494,"@mjrlee we support something similar on Anyscale Platform. See [here](https://docs.anyscale.com/configure/compute-configs/alternative-instances). Would that be what you're looking for?

cc @richardliaw ",support something similar platform see would looking,issue,negative,neutral,neutral,neutral,neutral,neutral
1734648177,"@AwesomeLemon Yes, Ray Train/Tune will require cloud storage or NFS in 2.7+ for multi-node training.

One detail is that this is only strictly enforced (e.g., we will raise an error) if you try to [report a checkpoint](https://docs.ray.io/en/releases-2.7.0/train/user-guides/checkpoints.html) without [setting up persistent storage](https://docs.ray.io/en/releases-2.7.0/train/user-guides/persistent-storage.html).",yes ray require cloud storage training one detail strictly enforced raise error try report without setting persistent storage,issue,negative,neutral,neutral,neutral,neutral,neutral
1734645147,Thanks for raising this issue; lemme take the action item to decide which team can take ownership of this at Anyscale to investigate further with you. cc @richardliaw ,thanks raising issue take action item decide team take ownership investigate,issue,negative,positive,positive,positive,positive,positive
1734643703,Reviewed; needs further investigation. Will review further on assignee to investigate further later this week.,need investigation review assignee investigate later week,issue,negative,neutral,neutral,neutral,neutral,neutral
1734642062,"Reviewed further today at weekly Ray Core triage meeting. 

@ali-naqvi please let us know if you have any further questions but per @xieus would it be alright for you to raise an REP and than we can discuss ownership/scope from there a starting point.",today weekly ray core triage meeting please let u know per would alright raise rep discus starting point,issue,positive,neutral,neutral,neutral,neutral,neutral
1734631871,"hey @zshareef - following up on the comment from @matthewdeng above we recently reworked our Train API layer as part of the Ray 2.7 release (which is now out!)

Can you please follow [the aforementioned example ](https://docs.ray.io/en/master/train/examples/pytorch/convert_existing_pytorch_code_to_ray_train.html)from Matt and see if you're able to get it to run successfully?",hey following comment recently reworked train layer part ray release please follow example see able get run successfully,issue,positive,positive,positive,positive,positive,positive
1734630317,"It's array of float32s ([...]) since it's embedding values. There are no nulls anywhere. If embedding value was null then it's already replaced with len([0,0,0...,0]) = 1024 in earlier steps.

Speaking of null values in nested columns, I would have preferred to keep it that way or either []. But I ran into other issues, so replaced it with len([0,0,0...,0]) = 1024 in earlier steps. However, it would be nice if Ray Data can gracefully handle nulls in nested columns and it can be expanded at runtime in data loader. This will help reduce disk space greatly, at least in my use-case.",array since anywhere value null already speaking null would preferred keep way either ran however would nice ray data gracefully handle expanded data loader help reduce disk space greatly least,issue,positive,positive,positive,positive,positive,positive
1734626286,"> remaining 300 are nested columns of size 1024

can you clarify what this nested column structure looks like? does this mean in each record, this column contains an array of `float32`s?

can this nested column be null, or its contents be null?",size clarify column structure like mean record column array float column null content null,issue,negative,negative,negative,negative,negative,negative
1734621454,"@scottjlee Yes, all files have same underlying schema/data types. As mentioned above, I can't share data but generating following dataset, with random float32 values, should help reproduce the issue. Please let me know, if you need more clarification.

### Full dataset: 
- 550,000 records
- Each record has 6902 columns: Out of these 6602 columns have float32 and remaining 300 are nested columns of size 1024 (embedding values of float32).
- 1,113 files
- 307 GB",yes underlying ca share data generating following random float help reproduce issue please let know need clarification full record float size float,issue,positive,negative,neutral,neutral,negative,negative
1734617190,"@meprem looping back on this, do you have some synthetic data you can share with us which is similar?

do each of the files have the same underlying schema/data types? from looking at the stack trace, that seems to be my first guess",looping back synthetic data share u similar underlying looking stack trace first guess,issue,negative,positive,neutral,neutral,positive,positive
1734615757,"@jdixosnd This is odd. I just ran the code again and seeing the expected response. Also we have CI to ensure these code are already tested and are not throwing error. Based on the log `PredictDeployment#iakJjL replica.py:438 - Request failed due to TypeError:` and `packages/ray/serve/_private/replica.py"", line 420, in invoke_single` I can almost guarenteed this is not on Ray 2.7.0 per [source code](https://github.com/ray-project/ray/blob/releases/2.7.0/python/ray/serve/_private/replica.py#L420).

Can you try running `ray --version` command in a terminal and `python -c ""import ray; print(ray.__version__)""` to double check you are on Ray 2.7.0? And possible run `pip install ""ray[serve]"" -U` to ensure Ray and Ray serve are up to date? ",odd ran code seeing response also ensure code already tested throwing error based log request due line almost ray per source code try running ray version command terminal python import ray print double check ray possible run pip install ray serve ensure ray ray serve date,issue,negative,negative,neutral,neutral,negative,negative
1734600080,Ray Serve quickstart is still broken. Can someone fix the docs/explain what you're supposed to do to get Ray Serve working? Thanks!,ray serve still broken someone fix supposed get ray serve working thanks,issue,negative,negative,neutral,neutral,negative,negative
1734584455,Would love to see this happen!,would love see happen,issue,positive,positive,positive,positive,positive,positive
1734573650,"@edoakes Good idea, I've changed the test from running 30 iterations to your suggested approach.",good idea test running approach,issue,negative,positive,positive,positive,positive,positive
1734529985,Close since python grpcio no longer included. ,close since python longer included,issue,negative,neutral,neutral,neutral,neutral,neutral
1734521382,Hey @jmakov - will you be able to get any `monitor.*` logs generated? That would be helpful to debug. ,hey able get monitor would helpful,issue,negative,positive,positive,positive,positive,positive
1734517068,"Sorry yes, I missed part of the instructions on the page. Will close this issue now.",sorry yes part page close issue,issue,negative,negative,negative,negative,negative,negative
1734510052,Looks like the dashboard agent died from the logs. can you give us the log from `dashboard_agent.log` when this happens? ,like dashboard agent give u log,issue,negative,neutral,neutral,neutral,neutral,neutral
1734508863,"cc @rickyyx can you follow up with the investigation? 

```
    type: local
    head_ip: 192.168.0.101
    # You may need to supply a public ip for the head node if you need
    # to run `ray up` from outside of the Ray cluster's network
    # (e.g. the cluster is in an AWS VPC and you're starting ray from your laptop)
    # This is useful when debugging the local node provider with cloud VMs.
    # external_head_ip: YOUR_HEAD_PUBLIC_IP
    worker_ips:
      - 192.168.0.106
      - 192.168.0.107
      - 192.168.0.108
      - 192.168.0.110
```

Can you tell us what this exactly for? ",follow investigation type local may need supply public head node need run ray outside ray cluster network cluster starting ray useful local node provider cloud tell u exactly,issue,negative,positive,neutral,neutral,positive,positive
1734508119,cuda121 support already added in ray 2.7,support already added ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1734506779,"cc @rickyyx we added P1.5 for now, but feel free to reassign the priority ",added feel free reassign priority,issue,positive,positive,positive,positive,positive,positive
1734504744,"```
raylet, ip=192.168.1.100) [2023-08-28 03:39:18,893 C 4034351 4034351] grpc_server.cc:119: Check failed: server_ Failed to start the grpc server. The specified port is 18321. This means that Ray’s core components will not be able to function correctly. If the server startup error message is Address already in use, it indicates the server fails to start because the port is already used by other processes (such as --node-manager-port, --object-manager-port, --gcs-server-port, and ports between --min-worker-port, --max-worker-port). Try running sudo lsof -i :18321 to check if there are other processes listening to the port.\
```

Is it possible for you to ssh and run lsof on this port when you see this error? ",raylet check start server port ray core able function correctly server error message address already use server start port already used try running check listening possible run port see error,issue,negative,positive,positive,positive,positive,positive
1734499865,"I think we can support it just for CPU (but it makes no sense for GPU). Can you tell us a little more detail about the use case? Also, feel free to contribute it! ",think support sense tell u little detail use case also feel free contribute,issue,positive,positive,positive,positive,positive,positive
1734493610,Are you still seeing this issue? Or did you end up finding the problem? ,still seeing issue end finding problem,issue,negative,neutral,neutral,neutral,neutral,neutral
1734484785,"We fixed various issues relevant to process leak. We will try repro with ^ script, and close the issue if it is not happening again. ",fixed various relevant process leak try script close issue happening,issue,negative,positive,positive,positive,positive,positive
1734477276,"<img width=""1604"" alt=""Screen Shot 2023-09-22 at 10 53 39 AM"" src=""https://github.com/ray-project/ray/assets/18510752/b5f9e5eb-f47d-42e5-b42c-9064ae64dc0f"">

Note: This test has been flakier again after this PR. cc @rynewang @jjyao ",screen shot note test,issue,negative,neutral,neutral,neutral,neutral,neutral
1734470094,"Based on the logs, it seems like it couldn't communicate with GCS. This commonly happens when the GCS is terminated. Can you check the log of gcs_server.out around the time this issue happened? ",based like could communicate commonly check log around time issue,issue,negative,negative,negative,negative,negative,negative
1734466314,I think it is the data package (not core). cc @c21 can you take a look? ,think data package core take look,issue,negative,neutral,neutral,neutral,neutral,neutral
1734406231,"@zcin instead of having a set number of, can we iteratively schedule a small batch up to a maximum number of times until we see all 3 replicas? If you make it a large number, say 1000, then the odds of this flaking should be approximately zero while maintaining very fast average/median runtime.",instead set number iteratively schedule small batch maximum number time see make large number say odds approximately zero fast,issue,negative,negative,neutral,neutral,negative,negative
1734404473,"@shrekris-anyscale I think that'd be testing a _slightly_ different path because in that case we're verifying the behavior when replicas are occupied. As written, the test is verifying that we load balance even when replicas are free to take more requests.",think testing different path case behavior written test load balance even free take,issue,positive,positive,positive,positive,positive,positive
1734362565,"@michaelhly thanks for your first contribution! i left a few suggested changes, let me know if you have any questions or followups and i'd be happy to discuss",thanks first contribution left let know happy discus,issue,positive,positive,positive,positive,positive,positive
1734320744,"The Ray Data team has also seen several other issues related to this one and have discussed how to best tackle all of them.

As you suggested above, the issue is that the blocks are of different types in the `reduce()` method of `aggregate_task_spec.py`. Before we call `aggregate_combined_blocks()` [here](https://github.com/ray-project/ray/blob/master/python/ray/data/_internal/planner/exchange/aggregate_task_spec.py#L70-L72), if we convert all the input blocks to Arrow first, this should ensure all blocks are Arrow before being aggregated.

@PRESIDENT810 if you have further questions on this, please let me know here and I can provide further guidance",ray data team also seen several related one best tackle issue different reduce method call convert input arrow first ensure arrow president please let know provide guidance,issue,positive,positive,positive,positive,positive,positive
1734313265,"> The error still persists in version 2.7.0, and the commit in #39580 has resolved it. I kindly request for someone to merge it into the master branch.

No, that commit is broken (causing some failure in other tests). I will try to fix it when I have some time to investigate into that... (Or can someone provide some hints about which part I should look into? I'm also just a beginner of ray and not very familiar with it)",error still version commit resolved kindly request someone merge master branch commit broken causing failure try fix time investigate someone provide part look also beginner ray familiar,issue,negative,positive,neutral,neutral,positive,positive
1734311526,"There's an issue with `_get_or_create_router` that needs to be addressed before merging this: the logic to construct the router is now conditional on the class, but we create the router and then pass it in before updating the class. Need to think of a solution here.",issue need logic construct router conditional class create router pas class need think solution,issue,positive,neutral,neutral,neutral,neutral,neutral
1734257668,"Hi @N3XT14 , thanks for your interest. The main code path involved is in `_report_current_usage` in [streaming_executor.py](https://github.com/ray-project/ray/blob/master/python/ray/data/_internal/execution/streaming_executor.py#L300). We can update `resources_status` here accordingly, and update all related unit tests which check this text.

You can take a look at [this page](https://docs.ray.io/en/latest/ray-contribute/getting-involved.html) for best practices contributing to Ray. Thanks again for your contributions and let me know if you have questions!

(Please feel free to assign the PR to me once it is ready for review.)",hi thanks interest main code path involved update accordingly update related unit check text take look page best ray thanks let know please feel free assign ready review,issue,positive,positive,positive,positive,positive,positive
1734185776,@sihanwang41 do I need some special setup to run the client? Can you help reproducing the issue together ?,need special setup run client help issue together,issue,positive,positive,positive,positive,positive,positive
1734180619,"> Edit: Actually I see what you mean here. I think if we want to eventually expose the max block size at the top-level, we probably will still want to set it as an attribute on the logical op.

Yup fair enough. I was thinking we could keep it purely internal and only have the shuffle vs non-shuffle block configs, to keep it simpler for the user level.",edit actually see mean think want eventually expose block size probably still want set attribute logical fair enough thinking could keep purely internal shuffle block keep simpler user level,issue,negative,positive,positive,positive,positive,positive
1734178783,This turned out to be a red herring--- the actual issue was to_tf() was creating a separate dataset job to get the dataset schema. cc @c21 ,turned red herring actual issue separate job get schema,issue,negative,neutral,neutral,neutral,neutral,neutral
1734150939,"> Perhaps it would be cleaner to only have the attribute at the physical op level, instead of in both the logical and physical classes? The assignment can then be done during planning as a rule.

Hmm actually can you say more about how to do that? From the current code, it seemed like we'd have to set the block size at the logical stage, but maybe I'm missing something here...

Edit: Actually I see what you mean here. I think if we want to eventually expose the max block size at the top-level, we probably will still want to set it as an attribute on the logical op.",perhaps would cleaner attribute physical level instead logical physical class assignment done rule actually say current code like set block size logical stage maybe missing something edit actually see mean think want eventually expose block size probably still want set attribute logical,issue,negative,positive,neutral,neutral,positive,positive
1734140318,"@JingChen23 Looks like there are some potentially relevant test failures: https://buildkite.com/ray-project/premerge/builds/6411
",like potentially relevant test,issue,negative,positive,positive,positive,positive,positive
1734098801,"Running the added handle throughput benchmark w/ the change:
```
DeploymentHandle throughput (num_replicas=10, batch_size=100): 2219.45 +- 20.17 requests/s
```

vs. baseline (master):
```
DeploymentHandle throughput (num_replicas=10, batch_size=100): 2306.67 +- 36.98 requests/s
```

~4% decrease (expected due to minor additional overhead of `concurrent.futures.Future`).",running added handle throughput change throughput master throughput decrease due minor additional overhead,issue,negative,negative,neutral,neutral,negative,negative
1733964875,"yes, likely still an issue, and the TODO is moved into ci/build/build-manylinux-wheel.sh file ;)",yes likely still issue file,issue,negative,neutral,neutral,neutral,neutral,neutral
1733501849,@bveeramani would you mind taking a look at this patch? Let me know if you have any comments.,would mind taking look patch let know,issue,negative,neutral,neutral,neutral,neutral,neutral
1733367466,"The documented snippet does not work. The server fails the assert statement, which requires request type as ''http""

> (HTTPProxyActor pid=38382)   File ""/anaconda/envs/azureml_py38/lib/python3.8/site-packages/ray/serve/_private/http_proxy.py"", line 279, in __call__
> (HTTPProxyActor pid=38382)     assert scope[""type""] == ""http""",snippet work server assert statement request type file line assert scope type,issue,negative,neutral,neutral,neutral,neutral,neutral
1733308374,"> @aslonnie @can-anyscale is this issue still relevant now?

probably still relevant, we never really audit the wheel, and the `TODO` comment was removed by @can-anyscale (which probably should be kept):

https://github.com/ray-project/ray/pull/38415/files#diff-38e85fefc7a419849845aad70765dc1f50055cd84a65f175374cf634ef6543b1L122

which might explains the various segfaults.. as we build the wheels in manylinux, but run them in ubuntu..",issue still relevant probably still relevant never really audit wheel comment removed probably kept might various build run,issue,negative,positive,positive,positive,positive,positive
1732991407,Thanks for the interest! @scottjlee for code pointer.,thanks interest code pointer,issue,positive,positive,positive,positive,positive,positive
1732969727,"Hi, 
I am not using ray up to start the cluster. The client/worker node, where raylet is suppose to start, runs the `ray start` without `--head` to start the ray startup processes. Since ray head instance runs before the workers, they can then connect to it using the address and port passed. 

I am doing the above in SLURM jobscripts by the way. 

At the moment my shutdown process is: 


on head node:
```
# Shutdown workers before the head node
touch $PWD/shutdown.txt
sleep 20
echo "" Stopping ray on Head node: $(/bin/hostname)""
ray stop
rm $PWD/shutdown.txt
```

On each worker node:

```
# worker shutdown strategy
if [ -f ""shutdown.txt"" ] ; then
  echo "" Stopping ray on Node: $(/bin/hostname)""
  ray stop
else
  while [ ! -f ""shutdown.txt"" ]; 
   do
     sleep 20
   done   
fi
```


",hi ray start cluster node raylet suppose start ray start without head start ray since ray head instance connect address port way moment shutdown process head node shutdown head node touch sleep echo stopping ray head node ray stop worker node worker shutdown strategy echo stopping ray node ray stop else sleep done fi,issue,negative,neutral,neutral,neutral,neutral,neutral
1732964523,"Currently there is no way to do so. How do you start the Ray cluster? If you are using `ray up`, there is a `ray down` cli to shutdown the cluster.",currently way start ray cluster ray ray shutdown cluster,issue,negative,neutral,neutral,neutral,neutral,neutral
1732929719,"@dongreenberg,

When a task blocks on `ray.get()`, it's CPU resource will be freed and will be reclaimed when `ray.get()` returns. This is the reason why Ray can run the following code

```
ray.init(num_cpus=1)

@ray.remtoe(num_cpus=1)
def child():
   return ""hello""
 
@ray.remote(num_cpus=1)
def parent():
  return ray.get(child.remote()) // Temporarily release the cpu resource so that child task can acquire it and run.
```",task resource freed reason ray run following code child return hello parent return temporarily release resource child task acquire run,issue,negative,neutral,neutral,neutral,neutral,neutral
1732855013,It seems won't affect the ray workers，but it's annoying and I have to restart ray and django to fix it,wo affect ray annoying restart ray fix,issue,negative,negative,negative,negative,negative,negative
1732834863,"The error still persists in version 2.7.0, and the commit in #39580 has resolved it. I kindly request for someone to merge it into the master branch.",error still version commit resolved kindly request someone merge master branch,issue,positive,positive,positive,positive,positive,positive
1732674847,This issue shows up intermittently. The only workaround I have is to restart the experiment. ,issue intermittently restart experiment,issue,negative,neutral,neutral,neutral,neutral,neutral
1732630621,I think `--port` option might not make sense since there is no clear way to identify each `ray start --address` instance by it's port. There should be a way to tell apart between ray instances running on the same machine. ,think port option might make sense since clear way identify ray start address instance port way tell apart ray running machine,issue,negative,positive,positive,positive,positive,positive
1732590966,"I would love to work on this. Would be great if you could guide me I understand the context of it though.

Thanks",would love work would great could guide understand context though thanks,issue,positive,positive,positive,positive,positive,positive
1732587574,"Hey. I can take this issue. But I do have one question. According to my understanding, the need here is to mention this configuration inside the other link as well.

Thanks. Let me know if I am wrong and what needs to be done",hey take issue one question according understanding need mention configuration inside link well thanks let know wrong need done,issue,negative,negative,negative,negative,negative,negative
1732585840,"Would love to take this issue. It would also be great if you could point me in some direction. Also, I don't think I have access to the slack thread

Thanks",would love take issue would also great could point direction also think access slack thread thanks,issue,positive,positive,positive,positive,positive,positive
1732555412,"The `AttributeError: 'NoneType' object has no attribute 'items'` was from `return self._get_obs() if not terminated else None`.
I’ve made the change to self._get_obs() if not terminated else self.observation_space.sample(). However, I’m still encountering the error message: `Cannot create PPOConfig from the given config_dict! Property stdout_file is not supported.`
@ArturNiederfahrenhorst I would be thankful if you have time to take a look at it.",object attribute return else none made change else however still error message create given property would thankful time take look,issue,negative,neutral,neutral,neutral,neutral,neutral
1732512645,"Hi @edoakes :wave:

It looks like this (or related) PR is introducing breaking changes (that are not behind the new handle api feature flag) - or at least CI starts failing when we bump to ray 2.7.

Could you have a look at the diff I propose [here](https://github.com/kserve/kserve/pull/3075#issuecomment-1722767156) to account for the breaking changes? Is it intended?

edit: nevermind, think it was introduced in 2.5 already:)",hi wave like related breaking behind new handle feature flag least failing bump ray could look propose account breaking intended edit think already,issue,positive,negative,negative,negative,negative,negative
1732477430,It seems that the ask is to clearly state how autoscaler determines which node types to start and the underlying priority.,ask clearly state node start underlying priority,issue,negative,positive,positive,positive,positive,positive
1732477295,@edoakes What do you think about the change to add a `--port` option to `ray stop`? I'm happy to work with someone to help close this issue if they are assigned to that change.,think change add port option ray stop happy work someone help close issue assigned change,issue,positive,positive,positive,positive,positive,positive
1732426591,"@ali-naqvi We are glad to hear that LinkedIn is building the new serving infra based on Ray. Enhancing Serve HA is indeed an important topic to discuss and thanks to you and your team for expressing interest in contributing to build more diversified 
HA solution. 

As a natural next step, we encourage your team to raise a Ray Enhancement Proposal (REP) on https://github.com/ray-project/enhancements. Meanwhile, if you have more detailed prod requirements you would like to discuss, please feel free to ping us on #ray-core channel of Ray OSS Slack. Thanks.

cc: @zhe-thoughts @anyscalesam @scv119 ",glad hear building new serving infra based ray serve ha indeed important topic discus thanks team interest build diversified ha solution natural next step encourage team raise ray enhancement proposal rep meanwhile detailed prod would like discus please feel free ping u channel ray slack thanks,issue,positive,positive,positive,positive,positive,positive
1732379262,"> Is there any plan to merge this?

The first time I put it up, it didn't get a review. Let me merge in latest changes, fix merge conflicts, and try again though.",plan merge first time put get review let merge latest fix merge try though,issue,negative,positive,positive,positive,positive,positive
1732358806,"Is there any update on this issue? it seems trivially important to machines in which several ray instances have to co-exist for one reason or another, i.e. multi-tenant systems",update issue trivially important several ray one reason another,issue,negative,positive,positive,positive,positive,positive
1732195306,"It should be possible to adjust the exceptions that would be retried using
the retry_exceptions arg, which can take a list of exception classes to
retry.

Though I'm not sure if this issue still applies
https://github.com/ray-project/ray/issues/30456

On Fri, Sep 22, 2023, 6:45 PM Jiun-Yu Lee ***@***.***> wrote:

> I confirmed that os._exit(1) will trigger retry. And I agree with you
> that by default application error for actor task should not be retried
> because it is stateful. In the reproduce script though I intentionally put
> in this to the map_batches by still did not retry
> ray_remote_args = {
> ""max_restarts"": 3,
> ""max_task_retries"": 3,
> }
> I wonder if it is still good to leave an option to users so that if they
> think the actor is safe to retry they can retry. Also for e.g. let’s say a
> read task is fused with an downstream actor based transform. We want to
> retry on the flacky IO error, but since read & transform are now performed
> in the same _MapWorker actor we are not able to retry it.
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/ray-project/ray/issues/39800#issuecomment-1732174855>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAADUSXDUZEBUPYF72QHKBDX3Y5KBANCNFSM6AAAAAA5CHPXOU>
> .
> You are receiving this because you commented.Message ID:
> ***@***.***>
>
",possible adjust would take list exception class retry though sure issue still lee wrote confirmed trigger retry agree default application error actor task stateful reproduce script though intentionally put still retry wonder still good leave option think actor safe retry retry also let say read task fused downstream actor based transform want retry io error since read transform actor able retry reply directly view id,issue,positive,positive,positive,positive,positive,positive
1732189438,"In the `map` function, is the `fn_constructor_args` helpful? In the example use case above, you would be able to pass args to `ModelPredictor` using this parameter.

https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map.html",map function helpful example use case would able pas parameter,issue,negative,positive,positive,positive,positive,positive
1732174855,"Thanks @ericl ! I confirmed that `os._exit(1)` will trigger retry. And I agree with you that by default application error for actor task should not be retried because it is stateful. In the reproduce script though I intentionally put in this to the map_batches by still did not retry
ray_remote_args = {
    ""max_restarts"": 3,
    ""max_task_retries"": 3,
}
I wonder if it is still good to leave an option to users so that if they think the actor is safe to retry they can retry. Also for e.g. let’s say a read task is fused with an downstream actor based transform. We want to retry on the flacky IO error, but since read & transform are now performed in the same _MapWorker actor we are not able to retry it.",thanks confirmed trigger retry agree default application error actor task stateful reproduce script though intentionally put still retry wonder still good leave option think actor safe retry retry also let say read task fused downstream actor based transform want retry io error since read transform actor able retry,issue,positive,positive,positive,positive,positive,positive
1732168286,"@sven1977 @ArturNiederfahrenhorst I just installed the lib and tested the latest script shared in this issue and I sill have the error.

## Steps to replicate
1) `conda create --name new-ray python=3.8`
2) `conda activate new-ray`
3) `pip install -U ray[rllib] torch`
4) `python script.py` where the script is attached below

## Script.py
```
import argparse
from gymnasium.spaces import Dict, Tuple, Box, Discrete, MultiDiscrete
import os

import ray
from ray import air, tune
from ray.tune.registry import register_env
from ray.rllib.utils.test_utils import check_learning_achieved
from ray.tune.registry import get_trainable_cls


import gymnasium as gym
from gymnasium.spaces import Box, Dict, Discrete, Tuple
import numpy as np
import tree  # pip install dm_tree

from ray.rllib.utils.spaces.space_utils import flatten_space


class CustomEnv(gym.Env):
    """"""Custom env with multi discrete action space

    """"""

    def __init__(self, config):
        self.observation_space = Box(-1.0, 1.0, (2,))
        self.action_space = MultiDiscrete([2,2])
        self.flattened_action_space = flatten_space(self.action_space)
        self.episode_len = 100

    def reset(self, *, seed=None, options=None):
        self.steps = 0
        return self._next_obs(), {}

    def step(self, action):
        self.steps += 1
        action = tree.flatten(action)
        reward = 0.0
        for a, o, space in zip(
            action, self.current_obs_flattened, self.flattened_action_space
        ):
            # Box: -abs(diff).
            if isinstance(space, gym.spaces.Box):
                reward -= np.sum(np.abs(a - o))
            # Discrete: +1.0 if exact match.
            if isinstance(space, gym.spaces.Discrete):
                reward += 1.0 if a == o else 0.0
        done = truncated = self.steps >= self.episode_len
        return self._next_obs(), reward, done, truncated, {}

    def _next_obs(self):
        self.current_obs = self.observation_space.sample()
        self.current_obs_flattened = tree.flatten(self.current_obs)
        return self.current_obs


parser = argparse.ArgumentParser()
parser.add_argument(
    ""--run"", type=str, default=""PPO"", help=""The RLlib-registered algorithm to use.""
)
parser.add_argument(
    ""--framework"",
    choices=[""tf"", ""tf2"", ""torch""],
    default=""torch"",
    help=""The DL framework specifier."",
)
parser.add_argument(""--num-cpus"", type=int, default=0)
parser.add_argument(
    ""--as-test"",
    action=""store_true"",
    help=""Whether this script should be run as a test: --stop-reward must ""
    ""be achieved within --stop-timesteps AND --stop-iters."",
)
parser.add_argument(
    ""--local-mode"",
    action=""store_true"",
    help=""Init Ray in local mode for easier debugging."",
)
parser.add_argument(
    ""--stop-iters"", type=int, default=100, help=""Number of iterations to train.""
)
parser.add_argument(
    ""--stop-timesteps"", type=int, default=100000, help=""Number of timesteps to train.""
)
parser.add_argument(
    ""--stop-reward"", type=float, default=0.0, help=""Reward at which we stop training.""
)

if __name__ == ""__main__"":
    args = parser.parse_args()
    ray.init(num_cpus=args.num_cpus or None, local_mode=args.local_mode)
    register_env(
        ""CustomEnv"", lambda c: CustomEnv(c)
    )

    config = (
        get_trainable_cls(args.run)
        .get_default_config()
        .environment(""CustomEnv"")
        .framework(args.framework)
        .rollouts(num_rollout_workers=0, num_envs_per_worker=20)
        # No history in Env (bandit problem).
        .training(gamma=0.0, lr=0.0005)
        # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.
        .resources(num_gpus=int(os.environ.get(""RLLIB_NUM_GPUS"", ""0"")))
    )

    if args.run == ""PPO"":
        config.training(
            # We don't want high entropy in this Env.
            entropy_coeff=0.00005,
            num_sgd_iter=4,
            vf_loss_coeff=0.01,
        )

    stop = {
        ""training_iteration"": args.stop_iters,
        ""episode_reward_mean"": args.stop_reward,
        ""timesteps_total"": args.stop_timesteps,
    }

    results = tune.Tuner(
        args.run, param_space=config, run_config=air.RunConfig(stop=stop, verbose=1)
    ).fit()

    if args.as_test:
        check_learning_achieved(results, args.stop_reward)

    ray.shutdown()
```

## Relevant Error Log
```
Traceback (most recent call last):
  File ""/home/alexpalms/miniconda3/envs/new-ray/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py"", line 110, in resolve_future
    result = ray.get(future)
  File ""/home/alexpalms/miniconda3/envs/new-ray/lib/python3.8/site-packages/ray/_private/auto_init_hook.py"", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File ""/home/alexpalms/miniconda3/envs/new-ray/lib/python3.8/site-packages/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
  File ""/home/alexpalms/miniconda3/envs/new-ray/lib/python3.8/site-packages/ray/_private/worker.py"", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(AttributeError): ray::PPO.train() (pid=702896, ip=192.168.0.38, actor_id=4759bace2b689d18ce33dc4101000000, repr=PPO)
  File ""/home/alexpalms/miniconda3/envs/new-ray/lib/python3.8/site-packages/ray/tune/trainable/trainable.py"", line 400, in train
    raise skipped from exception_cause(skipped)
  File ""/home/alexpalms/miniconda3/envs/new-ray/lib/python3.8/site-packages/ray/tune/trainable/trainable.py"", line 397, in train
    result = self.step()
  File ""/home/alexpalms/miniconda3/envs/new-ray/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py"", line 853, in step
    results, train_iter_ctx = self._run_one_training_iteration()
  File ""/home/alexpalms/miniconda3/envs/new-ray/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py"", line 2838, in _run_one_training_iteration
    results = self.training_step()
  File ""/home/alexpalms/miniconda3/envs/new-ray/lib/python3.8/site-packages/ray/rllib/algorithms/ppo/ppo.py"", line 448, in training_step
    train_results = self.learner_group.update(
  File ""/home/alexpalms/miniconda3/envs/new-ray/lib/python3.8/site-packages/ray/rllib/core/learner/learner_group.py"", line 184, in update
    self._learner.update(
  File ""/home/alexpalms/miniconda3/envs/new-ray/lib/python3.8/site-packages/ray/rllib/core/learner/learner.py"", line 1304, in update
    ) = self._update(nested_tensor_minibatch)
  File ""/home/alexpalms/miniconda3/envs/new-ray/lib/python3.8/site-packages/ray/rllib/core/learner/torch/torch_learner.py"", line 365, in _update
    return self._possibly_compiled_update(batch)
  File ""/home/alexpalms/miniconda3/envs/new-ray/lib/python3.8/site-packages/ray/rllib/core/learner/torch/torch_learner.py"", line 123, in _uncompiled_update
    loss_per_module = self.compute_loss(fwd_out=fwd_out, batch=batch)
  File ""/home/alexpalms/miniconda3/envs/new-ray/lib/python3.8/site-packages/ray/rllib/core/learner/learner.py"", line 1024, in compute_loss
    loss = self.compute_loss_for_module(
  File ""/home/alexpalms/miniconda3/envs/new-ray/lib/python3.8/site-packages/ray/rllib/algorithms/ppo/torch/ppo_torch_learner.py"", line 87, in compute_loss_for_module
    action_kl = prev_action_dist.kl(curr_action_dist)
  File ""/home/alexpalms/miniconda3/envs/new-ray/lib/python3.8/site-packages/ray/rllib/models/torch/torch_distributions.py"", line 327, in kl
    for cat, oth_cat in zip(self._cats, other.cats)
AttributeError: '<class 'ray.rllib.models.torch.torch_distributions' object has no attribute 'cats'
```
## Full Error Log
(attached)
[FullErrorLog.log](https://github.com/ray-project/ray/files/12705302/FullErrorLog.log)
",tested latest script issue sill error replicate create name activate pip install ray torch python script attached import import box discrete import o import ray ray import air tune import import import import gymnasium gym import box discrete import import tree pip install import class custom discrete action space self box reset self return step self action action action reward space zip action box space reward discrete exact match space reward else done truncated return reward done truncated self return parser run algorithm use framework torch torch framework specifier whether script run test must within ray local mode easier number train number train reward stop training none lambda history bandit problem use set want high entropy stop relevant error log recent call last file line result future file line return file line wrapper return file line get raise ray file line train raise file line train result file line step file line file line file line update file line update file line return batch file line file line loss file line file line cat zip class object attribute full error log attached,issue,positive,positive,positive,positive,positive,positive
1732128573,One more question before we can merge. See comment on `config.use_gae` for `bc.py`,one question merge see comment,issue,negative,neutral,neutral,neutral,neutral,neutral
1732123725,The nested action spaces test is currently flakey. Due to a harder criterion being used since some other PR was merged a while back. This should be a separate PR as it has nothing to do with the changes herein.,action test currently due harder criterion used since back separate nothing herein,issue,negative,negative,neutral,neutral,negative,negative
1732106113,"> It looks a little weird that it's not labeled ""Exit code: 42"". If it's just a plain number, it might be confused for the last line of the user script's output (could be bad if they print out a list of numbers and intend to use the last number as their calculation result).

This is not how it looks like now. I just want to show that we have a way to show the `message` field of the job. And the exit code will be logged there.

> Ideally it would appear in the GUI somewhere near Status: FAILED.

I think that the message button is good enough for now. We can add it if needed in the future. We can keep it open to track this.",little weird exit code plain number might confused last line user script output could bad print list intend use last number calculation result like want show way show message field job exit code logged ideally would appear somewhere near status think message button good enough add future keep open track,issue,negative,negative,neutral,neutral,negative,negative
1732096122,Apologies for the delay. Aiming to get this in soon.,delay aiming get soon,issue,negative,neutral,neutral,neutral,neutral,neutral
1732082177,"Potential fix with https://github.com/ray-project/ray/pull/39805, closing to run the test (seems that re-opening this issue when it was at jailed state immediately puts it back into jailed state, so it is not included in the nightly release test run).",potential fix run test issue state immediately back state included nightly release test run,issue,negative,neutral,neutral,neutral,neutral,neutral
1732072872,"For those who stumbles on this issue and unsure where the documentation is at, like me, [here](https://docs.ray.io/en/latest/tune/api/doc/ray.tune.Trainable.html) is the documentation.",issue unsure documentation like documentation,issue,negative,neutral,neutral,neutral,neutral,neutral
1731899162,"Hi, this is not an accurate number even with measuring the non-wire size. Check
```
import sys
from typing import Any, NamedTuple
from dataclasses import dataclass

class DeploymentID(NamedTuple):
    name: str
    app: str

@dataclass
class UpdatedObject:
    object_snapshot: Any
    # The identifier for the object's version. There is not sequential relation
    # among different object's snapshot_ids.
    snapshot_id: int

a = UpdatedObject(1, {DeploymentID(""1"", ""2""): 1})
print(sys.getsizeof(a))
b = UpdatedObject(1, {""1"": 1})
print(sys.getsizeof(b))
```

It will print the same size. That is because `sys.getsizeof` only reports the memory to which this object is accounted for not including nested object inferring to. 

Two directions to go:
1. Push this metric into the core side. (if we can get the metrics per task request payload)
2. Write our own check size function instead of using `sys.getsizeof`.",hi accurate number even measuring size check import import import class name class identifier object version sequential relation among different object print print print size memory object object two go push metric core side get metric per task request write check size function instead,issue,negative,positive,positive,positive,positive,positive
1731858016,"@sudhirn-anyscale I see, that will be added by https://github.com/ray-project/ray/pull/39675 which will be in Ray 2.8.  The exit code will appear in the `JobInfo` field returned by the CLI `ray job info` and the SDK [`get_job_info`](https://docs.ray.io/en/latest/cluster/running-applications/job-submission/doc/ray.job_submission.JobSubmissionClient.get_job_info.html)

",see added ray exit code appear field returned ray job,issue,negative,neutral,neutral,neutral,neutral,neutral
1731852704,"@architkulkarni - Ideally customer would like to make SDK call on a job and see a return code in one of the status fields. IT does not have to be displayed on dashboard. 

They would like to avoid searching logs for a error code because return code in logs could match to anything. ",ideally customer would like make call job see return code one status displayed dashboard would like avoid searching error code return code could match anything,issue,negative,positive,positive,positive,positive,positive
1731837837,"I see, I think that's fine as a minimal way to get the exit code. A few thoughts:
 
- It looks a little weird that it's not labeled ""Exit code: 42"". If it's just a plain number, it might be confused for the last line of the user script's output (could be bad if they print out a list of numbers and intend to use the last number as their calculation result).  
- Ideally it would appear in the GUI somewhere near Status: FAILED.

@sudhirn-anyscale is the status quo enough for the users you're dealing with?",see think fine minimal way get exit code little weird exit code plain number might confused last line user script output could bad print list intend use last number calculation result ideally would appear somewhere near status status quo enough dealing,issue,negative,negative,neutral,neutral,negative,negative
1731825465,"> Actually looks like the context is saved as an attribute of the dataset: https://github.com/ray-project/ray/pull/36030/files#diff-71f73f8b71d468f63f8b3be7282ec12b657a2a3f86324eb740d369e874d74df4R126.

@c21 did we ever come to a consensus on whether we wanted to set this through the context?

> would it be safer to use that? how would this work with multiple datasets?

With multiple datasets, I think they will still be using the same `DataContext` since this is set at the node level, and not an individual dataset level right?",actually like context saved attribute ever come consensus whether set context would use would work multiple multiple think still since set node level individual level right,issue,positive,positive,neutral,neutral,positive,positive
1731809200,"When I changed `http_proxies` -> `proxies` in schema.py, I forgot to modify the description: https://github.com/ray-project/ray/blob/master/python/ray/serve/schema.py#L902. Could you include a fix in this PR @GeneDer?",forgot modify description could include fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1731778533,"<img width=""484"" alt=""Screenshot 2023-09-22 at 10 19 52 AM"" src=""https://github.com/ray-project/ray/assets/7005244/00a8f70b-8b11-480c-b30d-51a5fc599871"">

Right now, the idle state is reflected in dashboard here. I wasn't sure about adding it to that part of the cluster tab because it displays information at a per-worker granularity, whereas we have idle information at a per-node granularity.",right idle state reflected dashboard sure part cluster tab information granularity whereas idle information granularity,issue,negative,positive,positive,positive,positive,positive
1731752342,"Also - i guess the PR doesn't update the dashboard view yet? Or it's automatically handled with changes in the PR?

i.e. the active status: 

<img width=""1607"" alt=""image"" src=""https://github.com/ray-project/ray/assets/11676094/91c54d15-a997-4da8-b10a-47e124bc40a2"">
",also guess update dashboard view yet automatically handled active status image,issue,negative,negative,negative,negative,negative,negative
1731740798,"![image](https://github.com/ray-project/ray/assets/9677264/5367570c-4e68-4d4b-8001-4f1d80448a28)


Actually, we already allow users to view the job message if it failed. The return code is logged both in logs and message according to https://github.com/ray-project/ray/pull/37273. I think dashboard part is already there and we can close it. We don't have to separately show the status code.
cc: @architkulkarni to confirm.",image actually already allow view job message return code logged message according think dashboard part already close separately show status code confirm,issue,negative,neutral,neutral,neutral,neutral,neutral
1731579076,Also the nested action spaces test is failing. Shouldn't that be fixed before this PR is merged? ,also action test failing fixed,issue,negative,positive,neutral,neutral,positive,positive
1731378053,"I would appreciate it if you could refrain from closing this issue, as there is no option to reopen it. I will close it myself once I have resolved the problem.",would appreciate could refrain issue option reopen close resolved problem,issue,negative,neutral,neutral,neutral,neutral,neutral
1731234432,"I am facing the same issue. I can't upgrade to Python 3.11 on windows because of this issue: https://github.com/ray-project/ray/issues/39727
And when I try to create the msgpack checkpoint I get the following error:

> can not serialize 'Dict' object

It would also be great if we could split the algo restore from the saving in convert_to_msgpack_checkpoint() because the restore from Algorithm does not work for me with a custom env and policies.

![image](https://github.com/ray-project/ray/assets/1881640/9fe4fffb-1f17-4b62-8f52-e190dff4b6bb)

Installed Version:
msgpack                       1.0.4
msgpack-numpy                 0.4.8

",facing issue ca upgrade python issue try create get following error serialize object would also great could split restore saving restore algorithm work custom image version,issue,positive,positive,positive,positive,positive,positive
1731227962,"This blocks me as well. The issue seems to be only related to windows. On Linux I can install ray 2.7 in python 3.11.
![image](https://github.com/ray-project/ray/assets/1881640/68e0bb7f-f732-4483-bc40-5d297ad9e788)
",well issue related install ray python image,issue,negative,neutral,neutral,neutral,neutral,neutral
1730749214,"Thanks for your prompt response! Your suggestion solved the problem. 

With a separate issue, I have a custom environment (which is really complicated and has dependencies on multiple other packages) that I am struggling to execute with RLLIB LSTM and attention networks, although it runs successfully with normal feedforward networks. 

I had posted an issue [here](https://discuss.ray.io/t/issue-with-lstm-ppo-mask-dimension-mismatch-with-custom-environment/12194) but I was wondering if there is a private forum to discuss this rather than post it here as an issue. Thanks",thanks prompt response suggestion problem separate issue custom environment really complicated multiple struggling execute attention although successfully normal posted issue wondering private forum discus rather post issue thanks,issue,positive,positive,neutral,neutral,positive,positive
1730666176,"Looks insanely good! Thinking about why this is so much simpler:
* A bunch of the complexity of the current ray client comes from trying to pretend that everything is being run from a *driver* colocated on the node/a drop in replacement for driver code, but its not really clear why this is a requirement if the user just wants code executed on their cluster (would they really care if everything is rooted at an actor instead) from a different machine
    * An enormous amount of complexity is from trying to mimic colocated driver performance (i.e. the extra layer of indirection on object references, and basically everything other than chunking that goes over streaming-streaming RPCs)
    * A bit shaky on the history details, but I feel like a lot of the complexity from @client_mode_hook is there to try to mimic auto-init for drivers
* A lot of pain comes from using gRPC as the protocol between laptop and head node
   * From a dev side, one thing that comes to mind is overhead that comes from deserializing large messages in the proxy server to figure out which driver to send requests to, which seems way more tractable over http directly. Also because python grpc package drags us deeply into dependency hell 
   * From a user side, exposing the ray client port 10001 for the cluster + setting up tls is a pain, and duplicates work if they've already set up http(s) for ray dashboard/job submission. If we can integrate this directly into dashboard it gets rid of all that extra work

A quick hack to get more of the ray APIs working, I suspect a good chunk can just by wrapping them in your implementation of remote and then fetching the result (basically swap them out with this workaround): https://github.com/ray-project/ray/issues/36833#issuecomment-1610466979",insanely good thinking much simpler bunch complexity current ray client come trying pretend everything run driver drop replacement driver code really clear requirement user code executed cluster would really care everything rooted actor instead different machine enormous amount complexity trying mimic driver performance extra layer indirection object basically everything go bit shaky history feel like lot complexity try mimic lot pain come protocol head node dev side one thing come mind overhead come large proxy server figure driver send way tractable directly also python package u deeply dependency hell user side ray client port cluster setting pain work already set ray submission integrate directly dashboard rid extra work quick hack get ray working suspect good chunk wrapping implementation remote fetching result basically swap,issue,negative,positive,positive,positive,positive,positive
1730561093,"Use `os.environ.get` for some buildkite envs, except for BUILDKITE_COMMIT which doesn't really make sense with any default empty value",use except really make sense default empty value,issue,negative,positive,neutral,neutral,positive,positive
1730549363,Absolutely! There seems to be quite a number of trim down we can do in ray ci across many things.,absolutely quite number trim ray across many,issue,negative,positive,positive,positive,positive,positive
1730543692,"This is resolved by https://github.com/ray-project/ray/pull/38362 when we change the default location to `EveryNode`. Previously the client would start on `HeadOnly` then later changed to `EveryNode`, thus printing the duplicated logs. Now that `EveryNode` is the default everywhere, this config differ log is no longer printed. 

Besides that, the client is not actually started multiple times, the code would reuse the existing global client, no this issue is no-op now.",resolved change default location previously client would start later thus printing default everywhere differ log longer printed besides client actually multiple time code would reuse global client issue,issue,negative,negative,neutral,neutral,negative,negative
1730543328,"Yes, @ArturNiederfahrenhorst , could you please provide a reference/few lines of code on how to use that preprocessor (to avoid flattening)? Thank you for your help!

Ok nevermind, got it:
Pass the following to your trainer config:
`""_disable_preprocessor_api"": True` (similar to how you pass `""num_workers"": 3`, etc.)",yes could please provide code use avoid flattening thank help got pas following trainer true similar pas,issue,positive,positive,positive,positive,positive,positive
1730466376,"@Yangqing I believe this is due to the `@ingress` decorator on `app` that freezes and pickles it. You can see how this is implemented: https://github.com/ray-project/ray/blob/master/python/ray/serve/api.py#L194

Can you elaborate a bit more about the motivation for setting up a separate `subapp` in your deployment? If you just do the following, then you will get your expected behavior 
```from ray import serve
from fastapi import FastAPI
import requests

app = FastAPI()

@serve.deployment(route_prefix=""/"")
@serve.ingress(app)
class FastAPIWrapper:
    @app.get(""/sub"")
    def f(self):
        return ""Hello from the sub!""

serve.run(FastAPIWrapper.bind())
resp = requests.get(""http://localhost:8000/sub"")
print(resp.text)  # ""Hello from the sub!""

```",believe due ingres decorator see elaborate bit motivation setting separate deployment following get behavior ray import serve import import class self return hello sub resp print hello sub,issue,negative,positive,positive,positive,positive,positive
1730443617,I think this is because RuntimeError is treated as an application error rather than a system fault. Does the same thing happen if you os._exit(1) or some other forced shutdown of the process instead to mimic a system fault?,think application error rather system fault thing happen forced shutdown process instead mimic system fault,issue,negative,negative,negative,negative,negative,negative
1730357773,"Passing now https://anyscale-ray--39629.com.readthedocs.build/en/39629/ \o/, as well as the doc build in premerge, CC: @matthewdeng 

Look like readthedocs wasn't happy with building horovod in its build environment so I pulled it out of requirements-doc and that does the job.",passing well doc build look like happy building build environment job,issue,positive,positive,positive,positive,positive,positive
1730356776,"@rkooo567 @jjyao @rickyyx @scv119 I just updated this again yesterday. Since 2.7.0 is already out, the metrics won't be changing anymore. Can some of you approve this PR and merge it? Thanks! ",yesterday since already metric wo approve merge thanks,issue,negative,positive,positive,positive,positive,positive
1730342191,"I have a work around, setting the `Tuner(reuse_actors=False)` ensures the trials work. ",work around setting tuner work,issue,negative,neutral,neutral,neutral,neutral,neutral
1730335306,"> > Then it will be similar with current approach right? When an attribute of core_worker not found, it will also just raise an exception.
> 
> Yeah it is .But we don't need to write `if not hasattr(self, ""core_worker""):` everywhere?

I guess we could always don't check the existence of the attribute and let it raise exception. ",similar current approach right attribute found also raise exception yeah need write self everywhere guess could always check existence attribute let raise exception,issue,negative,positive,neutral,neutral,positive,positive
1730302927,@rajendra-avesha Can you try this out and let us know if this solves your issue? https://docs.ray.io/en/latest/ray-core/runtime_env_auth.html,try let u know issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1730294760,"Ah good catch, investigating!",ah good catch investigating,issue,negative,positive,positive,positive,positive,positive
1730287257,@samanvithms2002 Thanks for reporting it. I think this is already https://github.com/ray-project/ray/pull/37809 Please try again with Ray 2.7.0 or nightly to see it taking affect,thanks think already please try ray nightly see taking affect,issue,positive,positive,positive,positive,positive,positive
1730269053,Looks like there's an issue on the server side for `https://lightning.ai/docs/pytorch/stable/objects.inv`. I'll push a dummy commit to retrigger CI.,like issue server side push dummy commit,issue,positive,neutral,neutral,neutral,neutral,neutral
1730264481,"I see, I think we should simply deprecate these APIs altogether. Actions should be computed on individual Policies (old stack) or RLModules (new stack) only.
These APIs are too leaky and not well defined (multi-agent vs preprocessor vs ...).

For now, as a quick workaround, could you simply use:
```
algo.compute_single_action(obs)
```

instead?

In case you need a certain policy (in a multi-agent case, you can also do):
```
algo.compute_single_action(obs, policy_id=[xyz])
```",see think simply deprecate altogether individual old stack new stack leaky well defined quick could simply use instead case need certain policy case also,issue,negative,positive,positive,positive,positive,positive
1730257397,What's the qps a single router actor is handling? looks like 1000 * 10-20 == 10-20k?,single router actor handling like,issue,negative,negative,neutral,neutral,negative,negative
1730248923,"@matthewdeng , @krfricke , @ericl can I get a stamp for this PR to move the doc build to ray civ2. Thankkks",get stamp move doc build ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1730238805,"I see, it's because - for some reason - the new API stack (mostly PPO's RLModule) behaves differently for `forward_exploration()` (returns vf preds b/c we probably assumed this method would always be used for collecting training data) and `forward_inference()` (b/c we probably assumed this method would NOT be used for collecting training data).
However, since either way, the Policy.postprocess_trajectory is called (which requires vf_preds), this fails in the explore=False case.
Solution for now that PPO is still using Policy classes:
* Make `forward_inference` return the same structure as `forward_exploration`
* Add TODO to clean up the postprocessing step (we probably shouldn't do postprocessing on the sampling side at all and instead compute the value function outputs on the learner side, when we intend to do an actual update).",see reason new stack mostly differently probably assumed method would always used training data probably assumed method would used training data however since either way case solution still policy class make return structure add clean step probably sampling side instead compute value function learner side intend actual update,issue,positive,positive,positive,positive,positive,positive
1730172781,"Actually looks like the context is saved as an attribute of the dataset: https://github.com/ray-project/ray/pull/36030/files#diff-71f73f8b71d468f63f8b3be7282ec12b657a2a3f86324eb740d369e874d74df4R126.

would it be safer to use that? how would this work with multiple datasets?",actually like context saved attribute would use would work multiple,issue,positive,neutral,neutral,neutral,neutral,neutral
1730152077,"I was under the impression that by default it was supposed to propagate across the cluster, so this does make sense to me.

Propagating it by default and allowing workers to override the context (in the train loop) sounds good to me.",impression default supposed propagate across cluster make sense default override context train loop good,issue,positive,positive,positive,positive,positive,positive
1730137642,"Seems that previously #29192 we only passed DataContext to Ray Train Trainable, this PR aims to pass it further into Ray Train Workers. 

I am actually ok with this change. @matthewdeng what do you think?",previously ray train trainable pas ray train actually change think,issue,negative,negative,neutral,neutral,negative,negative
1730069912,Thank you for your understanding during the past 3 weeks. Ray team is back to normal support mode now.,thank understanding past ray team back normal support mode,issue,positive,negative,neutral,neutral,negative,negative
1729996477,"For what it's worth, albeit with an entirely different call stack, I'm seeing a similar error message: ""Windows fatal exception: access violation"". 

This is with an application using sockets under asyncio, with the IOCP Proactor on Windows 10. The Python version is 3.11.5 installed via Chocolatey

When using a selector event loop on Windows, the segfault does not occur as such.

```python
import asyncio as aio
import sys
loop = aio.SelectorEventLoop() if sys.platform == ""win32"" else aio.get_event_loop_policy().get_event_loop()
```

Towards reproducing the error: There's an example using [HTTPX](https://www.python-httpx.org/async/) to run a single HTTP request  [moved to [gist](https://gist.github.com/spchamp/f6d6f37a8212918cfa66896a0e79c082)]

With the example, the Windows access violation might  not occur until the end of  `loop.run_until_complete()`. 

Using a selector event loop, the segfault does not occur.

HTH, apologies if it's too far off topic, moreover with the different call stack in the example.
",worth albeit entirely different call stack seeing similar error message fatal exception access violation application proactor python version via selector event loop occur python import import loop win else towards error example run single request gist example access violation might occur end selector event loop occur far topic moreover different call stack example,issue,negative,positive,positive,positive,positive,positive
1729992571,"I think your installation is broken. Can you try the following?
* In your conda or other env, do `pip uninstall -y ray`
* Erase the `ray` directory (should it still exist after above uninstall).
* Clone a fresh ray source from github into some directory.
* `pip install -U [latest ray wheel for your platform]`
* In the cloned git dir, run: `cd ray; python python/ray/setup-dev.py` and enter `Y` only for RLlib, then CTRL+C terminate the script

Try your steps from above again.",think installation broken try following pip ray erase ray directory still exist clone fresh ray source directory pip install latest ray wheel platform git run ray python enter terminate script try,issue,negative,positive,neutral,neutral,positive,positive
1729985538,"Closing this issue for now. Feel free to open a new one should you continue to have problems running the MAML algo.

Note, though, that this algo (and several others) will very soon move into a new `rllib_contrib` repository (outside of RLlib) and we will reduce support for any of these algos that will be moved in there due to our ongoing effort to reduce our maintenance load.",issue feel free open new one continue running note though several soon move new repository outside reduce support due ongoing effort reduce maintenance load,issue,positive,positive,neutral,neutral,positive,positive
1729981810,"Awesome! Actually, that is a good question :)
MAML is the only algorithm left that still uses the old ""execution plan"" API. You can take a look at `rllib.algorithms.maml.maml.py` and check the code in there.
From what I understand:
* MAML uses `batch_mode=complete_episodes` so each training iteration runs some exact number of finished episodes.
* Before each such ""rollout"", the env's task is set via the above mentioned extra environment API. See https://github.com/ray-project/ray/blob/master/rllib/algorithms/maml/maml.py#L217",awesome actually good question algorithm left still old execution plan take look check code understand training iteration exact number finished task set via extra environment see,issue,positive,positive,positive,positive,positive,positive
1729968050,"PR got merged into master. Closing this issue as well.
Related issue: https://github.com/ray-project/ray/issues/39453",got master issue well related issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1729961756,"Hey @grizzlybearg , thanks for raising this issue. Could you try to boil down your reproduction script to a manageable/debuggable size? Then we might be able to better assist. Possible questions a debugger would have would be:
* Does this error occur on a local (laptop) setup?
* Does it happen for a simpler built-in algo, like PPO?
* With a simpler setup: no eval workers, only one or zero remote workers (`num_workers=0`), etc..
* Without Ray Tune or at least with a simpler setup (no checkpoints, no pbt, hyperparam tuning, etc)",hey thanks raising issue could try boil reproduction script size might able better assist possible would would error occur local setup happen simpler like simpler setup one zero remote without ray tune least simpler setup tuning,issue,positive,positive,positive,positive,positive,positive
1729931379,"@JingChen23 Just letting you know you can mark the PR as draft until it's ready for review! https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/changing-the-stage-of-a-pull-request#converting-a-pull-request-to-a-draft

",know mark draft ready review,issue,negative,positive,positive,positive,positive,positive
1729929881,"Looking good, let me know once this is ready for review",looking good let know ready review,issue,positive,positive,positive,positive,positive,positive
1729865089,"> @architkulkarni the `_memory` keyword argument is hidden here
> 
> https://github.com/ray-project/ray/blob/ae0dbe6342f25f7a2a0185620a658b534c03fe8e/python/ray/_private/worker.py#L1316
> 
> should we make it public?

Good catch! You're saying according to https://docs.ray.io/en/latest/ray-core/scheduling/resources.html#specifying-node-resources `memory` is a ""native"" resource, and the doc implies that it should be an argument for `ray.init()`, but that's not the case.

It would be great if you could create a separate Github issue for this! Also, you can mark your PR as ""ready for review"" once it's no longer work in progress.",argument hidden make public good catch saying according memory native resource doc argument case would great could create separate issue also mark ready review longer work progress,issue,positive,positive,positive,positive,positive,positive
1729766832,"Sorry, this is still not done. Pushing for the reviewers to give this approval ...",sorry still done pushing give approval,issue,negative,negative,negative,negative,negative,negative
1729738435,"Hey @MatFl , yes, if you change python versions, your pickle checkpoints become unusable.
You will have to convert them to msgpack checkpoints first. Can you post the error you are getting when using the `pickle -> msgpack` conversion utilities?",hey yes change python pickle become unusable convert first post error getting pickle conversion,issue,negative,positive,positive,positive,positive,positive
1729373478,"I've managed to get K-fold cross validation working with `TorchTrainer` and the `BasicVariantGenerator`. The approach I took involves using the `constant_grid_search=True` parameter of `BasicVariantGenerator`.

```python
...

  search_space = {
      'kfold': tune.grid_search([1, 2, 3, 4, 5]), # search over k-fold with 5 folds
      # ... other hyper parameters
  }

  tuner = tune.Tuner(
      trainer, # instance of TorchTrainer
      param_space={'train_loop_config':  search_space},
      tune_config = tune.TuneConfig(
          scheduler=scheduler,
          search_alg=BasicVariantGenerator(
              constant_grid_search=True, # Required for kfolds
          ),
      )
  )

...
```

In the `train_loop_per_worker` function of `trainer` I handle the actual cross-validation logic. But this at least ensures that the hyperparameters remain constant over each fold and that each fold can be executed in parallel. Not sure of how much help this is to anyone else, as it is fairly implementation-specific.",get cross validation working approach took parameter python search hyper tuner trainer instance function trainer handle actual logic least remain constant fold fold executed parallel sure much help anyone else fairly,issue,positive,positive,positive,positive,positive,positive
1729294415,"

> hi @harborn, thanks again for submitting this PR to support intel accelerators in Ray. Given multiple teams are trying to add support of different accelerators to Ray. We'd like to come up with a design first to see how we should support them in a unified way [holistic design](https://github.com/ray-project/ray/issues/38504) in Ray 2.8, which will be started after ray summit (9/20).
> 
> In the mean time, can you share a more information how Intel GPU/Accelerator can be used with other libraries like pytorch? Thanks

@scv119 @cadedaniel Could you share a tentative timeline regarding review of the PR and also regarding further discussion regarding CI (Intel GPU) if that is in the plan ? 
Intel GPU (max/flex series) rely on intel extension for pytorch (IPEX) for all torch functionalities (c10/aten/dist etc). For out of the box support IPEX is mandatorily required  for  any Intel GPU device functionality. For distributed computing, such as scaling up/out of multi card training/finetuning/inference , we need Oneccl BIndings for Pytorch (oneCCL ) + IPEX . OneCCL is a collective communications library similar to NCCL (but interacts with Level 0 for Intel).  
Is there a separate channel for discussions related to further integration/development (such as slack/discord etc?).",hi thanks support ray given multiple trying add support different ray like come design first see support unified way holistic design ray ray summit mean time share information used like thanks could share tentative regarding review also regarding discussion regarding plan series rely extension torch box support mandatorily device functionality distributed scaling card need collective library similar level separate channel related,issue,positive,positive,neutral,neutral,positive,positive
1729201640,"> Hmm, something is wrong with the env registration lambda. I think somewhere you provide a env creator function that has no input arguments.
> 
> Maybe here?
> 
> ```
> train_env = lambda: RankingEnv,
> ```
> 
> However RLlib always passes in the `config.env_config` dict when it calls the registered env creator, which is why you are getting this error.
> 
> Changing your code to the following should help:
> 
> ```
> train_env = lambda env_config: RankingEnv,
> ```

Thank you for your prompt reply. I have made the changes as per your suggestions:
```ruby
drl_agent = DRLlibv2(
    trainable=""PPO"",
    train_env = lambda env_config: RankingEnv,
    run_name = ""PPO_TRAIN"",
    local_dir = ""/content/PPO_TRAIN"",
    params = train_config.to_dict(),
    num_samples = 1,#Number of samples of hyperparameters config to run
    training_iterations=5,
    checkpoint_freq=5,
    # scheduler_=scheduler_,
    search_alg=search_alg,
    metric = ""episode_reward_mean"",
    mode = ""max""
    # callbacks=[wandb_callback]
)
```
However, I am still encountering the same warnings and experiencing failures. My primary goal is to pass my custom environment class named RankingEnv to the DRLlibv2 class so that I can run it with Ray Tune.
In the train_tune_model function of DRLlibv2, I register my environment using register_env and then pass its name to tune.Tuner:
```ruby
register_env(self.params['env'], lambda env_config: self.train_env(env_config))
```
I also attempted to register it like this, but it did not resolve the issue:
```ruby
register_env(self.params['env'], self.train_env)
```
I would appreciate any further guidance or insights you can provide to help me resolve this issue. Thank you.",something wrong registration lambda think somewhere provide creator function input maybe lambda however always registered creator getting error code following help lambda thank prompt reply made per ruby lambda number run metric mode however still primary goal pas custom environment class class run ray tune function register environment pas name ruby lambda also register like resolve issue ruby would appreciate guidance provide help resolve issue thank,issue,positive,negative,neutral,neutral,negative,negative
1728947443,"All good, just thought of putting it up somewhere in case someone else ran into the same issue as I did. ",good thought somewhere case someone else ran issue,issue,negative,positive,positive,positive,positive,positive
1728602301,">  Then it will be similar with current approach right? When an attribute of core_worker not found, it will also just raise an exception.

Yeah it is .But we don't need to write `if not hasattr(self, ""core_worker""):` everywhere? ",similar current approach right attribute found also raise exception yeah need write self everywhere,issue,negative,positive,neutral,neutral,positive,positive
1728602015,"Sure, I will create a group chat on ray slack",sure create group chat ray slack,issue,positive,positive,positive,positive,positive,positive
1728600544,"Ray cluster fates share with the head node, so it is expected ",ray cluster share head node,issue,negative,neutral,neutral,neutral,neutral,neutral
1728599933,Closing in favor of https://github.com/ray-project/ray/issues/39781 because this thread has a lot of redundant information,favor thread lot redundant information,issue,negative,negative,negative,negative,negative,negative
1728599042,"Met with @richardliaw at Ray Summit regarding multi host serving, summarizing what we discussed. Also Richard asked me to open this as a separate issue and tag @jjyao and @scv119 for feedback and/or help identify any gaps!

# Problem Statement

Our primary use case is multi host serving, e.g. when a replica in our deployment is a TPU pod slice. This pod slice consists of multiple ray worker nodes that have TPU VMs and are provisioned as an atomic unit. 

Primary use case is production-grade serving of large models that do not fit within a single host TPU VM (estimated >65B parameters from [here](https://cloud.google.com/blog/products/compute/how-cloud-tpu-v5e-accelerates-large-scale-ai-inference)).

Note that while this primarily targets TPU pod slices, this would also affect other accelerator types (like Gaudi and GPU VM pods connected by higher bandwidth network) that requires a notion of grouping ray workers together in a deployment. 

## Visualization

To demonstrate this concept, here is a snippet of code of what a sample deployment could look like using today's Ray APIs:

```
@serve.deployment(num_replicas=1, route_prefix=""/"")
class APIIngress:
    def __init__(self, handle):
        self._handle = handle

    async def __call__(self, request):
        return await self._handle.generate.remote(request)


@serve.deployment(autoscaling_config={
    ""min_replicas"": 1,
    ""max_replicas"": 256,
  })
class LLMServer:
    # E.g. represents a v5e-16 and we would want up to 256 replicas of this unit
    def __init__(self):
        # v5e-16 is composed of 4-chip VMs...
        num_hosts = 4
        self._shards = [LLMShard(...) for _ in range(num_hosts)]

    async def generate(self, request):
        response_shards = ray.get([s.generate(request) for s in self._shards])
        # join and postprocess results, e.g. beamsearch etc.
        return processed_result


@ray.remote(resources={""TPU"": 4})
class LLMShard:
    # E.g. represents a 4-chip v5e VM host
    def __init__(self):
        import jax
        self._model_shard = # load checkpoint...

    def generate(self, request):
        return self._model_shard.generate(request)
```

As an image:

![ray-multihost-serving-deployment drawio](https://github.com/ray-project/ray/assets/9057208/628f663b-c28e-4f7b-b6ac-1f40f86bb5ba)

## Issues
Autoscaling code snippet above will not work because of two problems:
1) How does Ray know ""how"" to autoscale?
  - autoscaling currently operates based on resource requests, and would not understand e.g. that we want to scale a unit of v5e-16.
2) How does Ray know where to place LLMShards?
  - e.g. when an LLMServer creates shards, they should land on the same TPU pod slice, but Ray currently cannot tell that unique TPU VMs are or are not part of the same TPU pod slice

And a minor usability concern (3): if our `LLMShard` consisted of multiple functionalities past generate, we might have to duplicate function definitions for each function call. 

# Proposal
A better version would be something like this (same as above, skipping the APIIngress):

## Serve API
```
@serve.deployment(tpu=""tpu-v5e-16"") # or tpu=(""v5e"", ""4x4"") using the accelerator config definition
class LLMServer:
    def __init__(self):
        # e.g. per-shard init logic
        self._model_shard = ... # checkpoint load

    def generate(self, request):
        self._model_shard.generate(request)

    @serve.group_handler
    def group_generate_handler(self, request):
        results = await self.generate(request)
        # join and postprocess results, e.g. beamsearch etc.
```
e.g. we introduce:
1) deployment of a TPU type in which we can specify the exact topology we want a deployment replica to represent
2) a `group_handler` decorator - similar to `@serve.batch` but instead this allows us to define our joining and postprocessing strategy within the same deployment. This will help mitigate issue (3)


## Ray Core - Placement Groups
Serve API could lower the `tpu=""tpu-v5e-16""` portion to a placement group. We would need to modify the placement group to accept something like this:
```
bundles = [{""TPU_V5E"": 4}] * 4
placement_group(bundles, grouped=True)
```
E.g. the bundles will consist of 4 Ray worker nodes each with 4 v5e chips, and the placement group specifies that they should be ""grouped.""

This way, the autoscaler can deduce from this placement group spec that this should correspond to a TPU pod of 4 v5e-4s, resolving problem (1).

This leaves issue (2), e.g. how do we ensure that the shards reach the right ray workers? While the placement group will hint to Ray Core that we have to take special consideration about how Raylets schedule this task or actor, we still need Ray to know about the relationship between TPU VM hosts if they are part of the same TPU pod slice. We have to manage this at the resource provisioning level.

## Ray Cluster Launcher
For both the VM cluster launcher and KubeRay, we would need to modify the `ray start` command so we can specify the grouping relationship. For instance, that could look like this:
```
ray start --resources={""TPU"": 4} --group_id=$TPU_POD_SLICE_ID
```

Fortunately, TPU VM pod slices are created with a single unique ID naming the pod slice, and we should also be able to specify a unique identifier in the ray start command in KubeRay/GKE. 
",met ray summit regarding host serving also open separate issue tag feedback help identify problem statement primary use case host serving replica deployment pod slice pod slice multiple ray worker atomic unit primary use case serving large fit within single host note primarily pod would also affect accelerator like connected higher network notion grouping ray together deployment visualization demonstrate concept snippet code sample deployment could look like today ray class self handle handle self request return await request class would want unit self composed range generate self request request join return class host self import load generate self request return request image code snippet work two ray know currently based resource would understand want scale unit ray know place land pod slice ray currently tell unique part pod slice minor usability concern multiple past generate might duplicate function function call proposal better version would something like skipping serve accelerator definition class self logic load generate self request request self request await request join introduce deployment type specify exact topology want deployment replica represent decorator similar instead u define joining strategy within deployment help mitigate issue ray core placement serve could lower portion placement group would need modify placement group accept something like consist ray worker chip placement group grouped way deduce placement group spec correspond pod problem leaf issue ensure reach right ray placement group hint ray core take special consideration schedule task actor still need ray know relationship part pod slice manage resource level ray cluster launcher cluster launcher would need modify ray start command specify grouping relationship instance could look like ray start fortunately pod single unique id naming pod slice also able specify unique identifier ray start command,issue,positive,positive,positive,positive,positive,positive
1728590970,"@architkulkarni the `_memory` keyword argument is hidden here 
https://github.com/ray-project/ray/blob/ae0dbe6342f25f7a2a0185620a658b534c03fe8e/python/ray/_private/worker.py#L1316

should we make it public?",argument hidden make public,issue,negative,negative,neutral,neutral,negative,negative
1728567439,"Ah good call, repeating compiling ray is properly not avoidable since it's the common denominator on all of these builds  and we have flipped it to the last step in civ2 to be able to cache other stuffs which take longer to run.

That said, we can properly cache even the ray build itself so that these repeating compilation are no-op.

Would be in another PR though, this issue already exists before this PR.",ah good call ray properly avoidable since common denominator last step able cache take longer run said properly cache even ray build compilation would another though issue already,issue,negative,positive,positive,positive,positive,positive
1728515686,"I, sorry for the late reply. The problem was indeed with the shape of my observation space in the __init__ method. A bit of a struggle to find and not really clearly explained in the docs but it works now ! Thank you for the help ! ",sorry late reply problem indeed shape observation space method bit struggle find really clearly work thank help,issue,negative,negative,negative,negative,negative,negative
1728482097,"Just run this one last time, let me know if there is anything else we need for this one🙏
```
(ray) gene@geneanyscale2023 release_logs % ./compare_perf_metrics 2.6.3 2.7.0 | sort -nr -k2
REGRESSION 701.34%: dashboard_p50_latency_ms (LATENCY) regresses from 13.316 to 106.707 (701.34%) in 2.7.0/benchmarks/many_actors.json
REGRESSION 491.14%: dashboard_p95_latency_ms (LATENCY) regresses from 1641.087 to 9701.149 (491.14%) in 2.7.0/benchmarks/many_actors.json
REGRESSION 144.91%: dashboard_p99_latency_ms (LATENCY) regresses from 3961.132 to 9701.149 (144.91%) in 2.7.0/benchmarks/many_actors.json
REGRESSION 90.89%: dashboard_p99_latency_ms (LATENCY) regresses from 7303.73 to 13941.91 (90.89%) in 2.7.0/benchmarks/many_tasks.json
REGRESSION 18.84%: dashboard_p95_latency_ms (LATENCY) regresses from 5662.863 to 6729.761 (18.84%) in 2.7.0/benchmarks/many_tasks.json
REGRESSION 14.74%: single_client_tasks_and_get_batch (THROUGHPUT) regresses from 10.988766549840475 to 9.369535279594958 (14.74%) in 2.7.0/microbenchmark.json
REGRESSION 11.78%: actors_per_second (THROUGHPUT) regresses from 848.4535001176805 to 748.5322140167257 (11.78%) in 2.7.0/benchmarks/many_actors.json
REGRESSION 10.77%: 1_1_actor_calls_sync (THROUGHPUT) regresses from 2527.884786760515 to 2255.614293958201 (10.77%) in 2.7.0/microbenchmark.json
REGRESSION 8.41%: 1_1_async_actor_calls_sync (THROUGHPUT) regresses from 1519.8915131779668 to 1392.1278469787085 (8.41%) in 2.7.0/microbenchmark.json
REGRESSION 8.02%: 1_n_actor_calls_async (THROUGHPUT) regresses from 11016.813113385808 to 10133.72696574923 (8.02%) in 2.7.0/microbenchmark.json
REGRESSION 7.92%: 1_1_async_actor_calls_with_args_async (THROUGHPUT) regresses from 2071.4152029622965 to 1907.3804159989331 (7.92%) in 2.7.0/microbenchmark.json
REGRESSION 7.50%: 1_1_actor_calls_async (THROUGHPUT) regresses from 8232.577421386566 to 7615.355914488919 (7.50%) in 2.7.0/microbenchmark.json
REGRESSION 6.60%: avg_pg_create_time_ms (LATENCY) regresses from 0.871186292793918 to 0.9287227387393717 (6.60%) in 2.7.0/stress_tests/stress_test_placement_group.json
REGRESSION 5.80%: 1_1_actor_calls_concurrent (THROUGHPUT) regresses from 5038.298026762205 to 4745.83263563276 (5.80%) in 2.7.0/microbenchmark.json
REGRESSION 5.63%: n_n_actor_calls_async (THROUGHPUT) regresses from 32688.224759117064 to 30847.92669705198 (5.63%) in 2.7.0/microbenchmark.json
REGRESSION 5.03%: 1_n_async_actor_calls_async (THROUGHPUT) regresses from 9607.6917020388 to 9124.377528222414 (5.03%) in 2.7.0/microbenchmark.json
REGRESSION 4.79%: 107374182400_large_object_time (LATENCY) regresses from 33.47957800099999 to 35.08243522800001 (4.79%) in 2.7.0/scalability/single_node.json
REGRESSION 4.16%: avg_pg_remove_time_ms (LATENCY) regresses from 0.7933515765763139 to 0.8263823783760937 (4.16%) in 2.7.0/stress_tests/stress_test_placement_group.json
REGRESSION 2.92%: placement_group_create/removal (THROUGHPUT) regresses from 982.6908282286798 to 954.0380362861301 (2.92%) in 2.7.0/microbenchmark.json
REGRESSION 2.07%: single_client_put_gigabytes (THROUGHPUT) regresses from 18.410571624942172 to 18.029502579731037 (2.07%) in 2.7.0/microbenchmark.json
REGRESSION 1.99%: stage_0_time (LATENCY) regresses from 9.37568211555481 to 9.562453985214233 (1.99%) in 2.7.0/stress_tests/stress_test_many_tasks.json
REGRESSION 1.83%: single_client_tasks_async (THROUGHPUT) regresses from 10940.075908384719 to 10739.407361558973 (1.83%) in 2.7.0/microbenchmark.json
REGRESSION 1.78%: stage_3_time (LATENCY) regresses from 2753.045879125595 to 2802.1650245189667 (1.78%) in 2.7.0/stress_tests/stress_test_many_tasks.json
REGRESSION 1.74%: stage_1_avg_iteration_time (LATENCY) regresses from 22.907386016845702 to 23.305240750312805 (1.74%) in 2.7.0/stress_tests/stress_test_many_tasks.json
REGRESSION 1.59%: multi_client_tasks_async (THROUGHPUT) regresses from 28883.241079598323 to 28423.644858766176 (1.59%) in 2.7.0/microbenchmark.json
REGRESSION 1.30%: single_client_tasks_sync (THROUGHPUT) regresses from 1329.0736290293253 to 1311.812164358857 (1.30%) in 2.7.0/microbenchmark.json
REGRESSION 1.19%: multi_client_put_calls_Plasma_Store (THROUGHPUT) regresses from 12887.724106896032 to 12734.699835552969 (1.19%) in 2.7.0/microbenchmark.json
REGRESSION 1.16%: 1000000_queued_time (LATENCY) regresses from 179.73244193300002 to 181.82263824499995 (1.16%) in 2.7.0/scalability/single_node.json
REGRESSION 0.92%: 10000_args_time (LATENCY) regresses from 16.737809073999998 to 16.89121779300001 (0.92%) in 2.7.0/scalability/single_node.json
REGRESSION 0.82%: client__put_gigabytes (THROUGHPUT) regresses from 0.13393152379385806 to 0.13283428838343245 (0.82%) in 2.7.0/microbenchmark.json
REGRESSION 0.72%: n_n_async_actor_calls_async (THROUGHPUT) regresses from 25874.5792020397 to 25688.484755543966 (0.72%) in 2.7.0/microbenchmark.json
REGRESSION 0.09%: dashboard_p50_latency_ms (LATENCY) regresses from 5.529 to 5.534 (0.09%) in 2.7.0/benchmarks/many_tasks.json
2.7.0 does not have benchmarks/many_pgs.json
```",run one last time let know anything else need one ray gene sort regression latency regression latency regression latency regression latency regression latency regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression latency regression throughput regression throughput regression throughput regression latency regression latency regression throughput regression throughput regression latency regression throughput regression latency regression latency regression throughput regression throughput regression throughput regression latency regression latency regression throughput regression throughput regression latency,issue,negative,neutral,neutral,neutral,neutral,neutral
1728466227,FYI @raulchen i asked @Zandew to take this issue -- let us know if you have any other thoughts ,take issue let u know,issue,negative,neutral,neutral,neutral,neutral,neutral
1728422046,"Looks like a potential issue with the `gpustat` dependency?

```
The last 20 lines of /tmp/ray/session_2023-09-20_16-22-22_034480_2554/logs/dashboard.log (it contains the error message from the dashboard):
    head_cls_list = dashboard_utils.get_all_modules(DashboardHeadModule)
  File ""/ray/python/ray/dashboard/utils.py"", line 121, in get_all_modules
    importlib.import_module(name)
  File ""/opt/miniconda/lib/python3.8/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 975, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 843, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/ray/python/ray/dashboard/modules/reporter/reporter_agent.py"", line 52, in <module>
    import gpustat.core as gpustat
  File ""/opt/miniconda/lib/python3.8/site-packages/gpustat/__init__.py"", line 7, in <module>
    from ._version import version as __version__
  File ""/opt/miniconda/lib/python3.8/site-packages
    __version__ : str = version : str = '1.1.1'
```",like potential issue dependency last error message dashboard file line name file line return name level package level file frozen line file frozen line file frozen line file frozen line file frozen line file frozen line file line module import file line module import version file version,issue,negative,neutral,neutral,neutral,neutral,neutral
1728418494,"Thanks for the detailed report, we'll look to find a solution for this for Ray 2.8.",thanks detailed report look find solution ray,issue,positive,positive,positive,positive,positive,positive
1728412320,"Hey @AvisP , can you try the same but with the following slight config changes?
```
config._enable_rl_module_api = False
config._enable_learner_api = False
```

The reason is that this example only works on the ""old API stack"" and PPO uses the new stack already by default (that's why you should switch it off via the above config changes).
We are currently moving all examples also to the new stack, but bear with us as this might take a while.",hey try following slight false false reason example work old stack new stack already default switch via currently moving also new stack bear u might take,issue,negative,negative,neutral,neutral,negative,negative
1728411505,"Looking into this, I can't even run the tests locally on `master`:

```
ray/python/ray $ pytest
Test session starts (platform: linux, Python 3.10.8, pytest 7.0.1, pytest-sugar 0.9.5)
rootdir: /home/pdmurray/Desktop/workspace/ray, configfile: pytest.ini
plugins: shutil-1.7.0, docker-tools-3.1.3, sugar-0.9.5, lazy-fixture-0.6.3, asyncio-0.16.0, typeguard-2.13.3, virtualenv-1.7.0, timeout-2.1.0, rerunfailures-10.2, anyio-3.6.2, forked-1.4.0
timeout: 180.0s
timeout method: signal
timeout func_only: False
collecting ... 
――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――― ERROR collecting test session ――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――
/home/pdmurray/.pyenv/versions/3.10.8/lib/python3.10/importlib/__init__.py:126: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
<frozen importlib._bootstrap>:1050: in _gcd_import
    ???
<frozen importlib._bootstrap>:1027: in _find_and_load
    ???
<frozen importlib._bootstrap>:1006: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:688: in _load_unlocked
    ???
/home/pdmurray/.pyenv/versions/3.10.8/envs/ray/lib/python3.10/site-packages/_pytest/assertion/rewrite.py:171: in exec_module
    exec(co, module.__dict__)
util/collective/tests/conftest.py:6: in <module>
    from ray.util.collective.collective_group.nccl_collective_group import (
util/collective/collective_group/nccl_collective_group.py:6: in <module>
    import cupy
E   ModuleNotFoundError: No module named 'cupy'
2023-09-20 13:52:33,637	WARNING collective.py:20 -- NCCL seems unavailable. Please install Cupy following the guide at: https://docs.cupy.dev/en/stable/install.html.

====================================================================================================================================================================================================== short test summary info =======================================================================================================================================================================================================
FAILED ../.. - ModuleNotFoundError: No module named 'cupy'
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

Results (5.80s):
```",looking ca even run locally master test session platform python method signal false error test session return name level package level frozen frozen frozen frozen module import module import module warning unavailable please install following guide short test summary module interrupted error collection,issue,negative,negative,negative,negative,negative,negative
1728406473,"Tried a naive implementation, and it might be non-trivial to implement this. Generators aren't serializable, so you can't straightforwardly yield data from a read task. Might be easier to just implement a custom datasource.

```python
from typing import TYPE_CHECKING, Iterable, Iterator, List

import pyarrow as pa

from ray.data.block import BlockMetadata
from ray.data.datasource.datasource import Datasource, Reader, ReadTask


class ArrowDatasource(Datasource):
    def create_reader(self, tables: Iterable[pa.Table]) -> Reader:
        return _ArrowDatasourceReader(tables)


class _ArrowDatasourceReader(Reader):
    def __init__(self, tables: Iterable[pa.Table]) -> None:
        self._tables = tables

    def get_read_tasks(self, parallelism: int) -> List[ReadTask]:
        def read_fn() -> Iterator[pa.Table]:
            yield from self._tables

        metadata = BlockMetadata(
            num_rows=None,
            size_bytes=None,
            schema=None,
            input_files=None,
            exec_stats=None,
        )
        read_task = ReadTask(read_fn, metadata)
        return [read_task]

```

```
TypeError: Could not serialize the argument {'tables': <generator object table_generator at 0x13c723d80>} for a task or actor ray.data.read_api._get_reader:
```",tried naive implementation might implement ca straightforwardly yield data read task might easier implement custom python import iterable list import pa import import reader class self table iterable reader return table class reader self table iterable none table self parallelism list yield return could serialize argument generator object task actor,issue,negative,positive,neutral,neutral,positive,positive
1728401940,I'm closing this issue for now. Feel free to re-open should you still have problems with your example after fixing your custom env creator function.,issue feel free still example fixing custom creator function,issue,positive,positive,positive,positive,positive,positive
1728379594,Does this https://docs.ray.io/en/latest/ray-observability/user-guides/configure-logging.html#structured-logging meet the requirement? Can you ping the user who gave the feedback? @Wendi-anyscale ,meet requirement ping user gave feedback,issue,negative,neutral,neutral,neutral,neutral,neutral
1728374885,@rkooo567 what do you mean by default logger? Whose default logger? The libraries used under the hood?,mean default logger whose default logger used hood,issue,negative,negative,negative,negative,negative,negative
1728283550,"Hi @Ox0400 Thanks for the contribution!

Could you update the PR description to describe what's the issue you are trying to fix?",hi ox thanks contribution could update description describe issue trying fix,issue,negative,positive,positive,positive,positive,positive
1728162382,let me remove these from review queue and focus on civ2 migration first ;),let remove review queue focus migration first,issue,negative,positive,positive,positive,positive,positive
1728143847,"Got you, this workflow still seems a bit troublesome to do, but better than no being able to do this at all I guess.

I can follow up on this in the future to make it possible to run this locally, which probably requires everyone to move their local dev to cloud, or running things in docker.",got still bit troublesome better able guess follow future make possible run locally probably everyone move local dev cloud running docker,issue,negative,positive,positive,positive,positive,positive
1728140825,"Hi, I just [saw](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html) that `ray.data.Dataset` map batches only accepts numpy arrays and pandas DataFrames -- is there any progress in allowing users to bypass these primitives entirely and just use arrow-only primitives to interface with Ray's data plane?",hi saw map progress bypass entirely use interface ray data plane,issue,negative,neutral,neutral,neutral,neutral,neutral
1728137070,Added more clarification in the issue under the update and tried opening a PR that attempts to resolve it (more investigation / help needed for this PR),added clarification issue update tried opening resolve investigation help,issue,positive,neutral,neutral,neutral,neutral,neutral
1728113974,"""buildkite/premerge"" is required to pass, I've restarted a failed premerge test that looks unrelated. Please let me know when premerge is passing and I'll merge the PR!",pas test unrelated please let know passing merge,issue,negative,neutral,neutral,neutral,neutral,neutral
1728100326,"Events is what we need, but scrapping (pull) is not efficient and building this scrapping infra across 100s of ray clusters is a major challenge.
Also, a fundamental limitation with that approach is that, you cannot scrap for the events after the ray clusters is gone. If we are recommending folks to use ray clusters as ephemeral units, getting the timing right for the scrapping will never be perfect.

We need a mechanism to persist the event as it occurs, and make the events more durable than the ray cluster itself. Writing to a Kafka topic is one example. But more importantly building a way to allow folks to write their own listeners is a central piece and more general solution.",need scrapping pull efficient building scrapping infra across ray major challenge also fundamental limitation approach scrap ray gone use ray ephemeral getting timing right scrapping never perfect need mechanism persist event make durable ray cluster writing topic one example importantly building way allow write central piece general solution,issue,positive,positive,neutral,neutral,positive,positive
1728091517,"This is more related to persistent dashboard:  exporting the info from GCS that powers that dashboard.

There has been related discussion: @alanwguo @gvspraveen ",related persistent dashboard dashboard related discussion,issue,negative,neutral,neutral,neutral,neutral,neutral
1728088103,Will job events be enough? Users can scrape the events and trigger downstream notification/alert systems?,job enough scrape trigger downstream,issue,negative,neutral,neutral,neutral,neutral,neutral
1728010075,"Hmm, something is wrong with the env registration lambda. I think somewhere you provide a env creator function that has no input arguments.

Maybe here?
```
train_env = lambda: RankingEnv,
```
However RLlib always passes in the `config.env_config` dict when it calls the registered env creator, which is why you are getting this error.

Changing your code to the following should help:
```
train_env = lambda env_config: RankingEnv,
```",something wrong registration lambda think somewhere provide creator function input maybe lambda however always registered creator getting error code following help lambda,issue,negative,negative,negative,negative,negative,negative
1727848408,"While researching a little bit more, it might boil down to this line: 

https://github.com/ray-project/ray/blob/df77af0b984a1d10c8bf2409d1f58ff20b065d27/python/ray/_private/async_compat.py#L30

In Cython apparently it is required to check for a flag to ensure it is a coroutine (ref: https://github.com/cython/cython/pull/4902 & https://github.com/cython/cython/blob/master/tests/run/test_coroutines_pep492.pyx#L889 applying this check and https://github.com/cython/cython/issues/2273 discussing it)

Is this something that can be added? E.g., the below should fix it. Tested on my own project and this correctly identifies an async marked function, while `inspect` or `asyncio` failed on this (Python 3.8.18):

```python
return bool(
    fn.__code__.co_flags & CO_COROUTINE or getattr(fn, ""_is_coroutine"", False)
)
```  ",little bit might boil line apparently check flag ensure ref check something added fix tested project correctly marked function inspect python python return bool false,issue,negative,negative,negative,negative,negative,negative
1727685391,"I can successfully run the following code：
`from gymnasium.wrappers import TimeLimit`
`import ray`
`from ray import air`
`from ray import tune`
`from rllib_maml.maml import MAML, MAMLConfig`
`from ray.rllib.examples.env.cartpole_mass import CartPoleMassEnv`
`from ray.tune.registry import register_env`

`if __name__ == ""__main__"":`
`    ray.init()`
`    register_env(`
`        ""cartpole"",`
`        lambda env_cfg: TimeLimit(CartPoleMassEnv(), max_episode_steps=200),`
`    )`

`    rollout_fragment_length = 32`

`    config = (`
`        MAMLConfig()`
`        .rollouts(`
`            num_rollout_workers=1, rollout_fragment_length=rollout_fragment_length`
`        )`
`        .framework(""torch"")`
`        .environment(""cartpole"", clip_actions=False)`
`        .training(`
`            inner_adaptation_steps=1,`
`            maml_optimizer_steps=5,`
`            gamma=0.99,`
`            lambda_=1.0,`
`            lr=0.001,`
`            vf_loss_coeff=0.5,`
`            inner_lr=0.03,`
`            use_meta_env=False,`
`            clip_param=0.3,`
`            kl_target=0.01,`
`            kl_coeff=0.001,`
`            model=dict(fcnet_hiddens=[64, 64]),`
`            train_batch_size=rollout_fragment_length,`
`        )`
`    )`

`    num_iterations = 100`

`    tuner = tune.Tuner(`
`        MAML,`
`        param_space=config.to_dict(),`
`        run_config=air.RunConfig(`
`            stop={""training_iteration"": num_iterations},`
`            failure_config=air.FailureConfig(fail_fast=""raise""),`
`        ),`
`    )`
`    results = tuner.fit()`

Can you tell me at which step the MAML algorithm interacts with the environment and updates parameters？I tried to break the point but couldn't find any code fragments that used the MAML algorithm to calculate loss or interacted with the CartPole environment to obtain observation and action",successfully run following import import ray ray import air ray import tune import import import lambda torch tuner raise tell step algorithm environment tried break point could find code used algorithm calculate loss environment obtain observation action,issue,negative,positive,positive,positive,positive,positive
1727649864,"In other words, the ""normal"" gymnasium environment won't work with this particular algo, you always need these extra versions of envs to support MAML. Take a look at the `CartPoleMassEnv` env to see what extra methods you have to implement in your envs to make them work with MAML.",normal gymnasium environment wo work particular always need extra support take look see extra implement make work,issue,negative,positive,neutral,neutral,positive,positive
1727647430,"I think I know what this is. MAML requires all environments to support an additional API: `sample_tasks`.
Could you try changing the repro script to the following?
```
from gymnasium.wrappers import TimeLimit

from ray.rllib.algorithms.maml import MAMLConfig
from ray.rllib.examples.env.cartpole_mass import CartPoleMassEnv
from ray.tune.registry import register_env

register_env(
            ""cartpole"",
            lambda env_cfg: TimeLimit(CartPoleMassEnv(), max_episode_steps=200),
        )


config = MAMLConfig().training(use_gae=False).environment(""cartpole"")
algo = config.build()
algo.train()
```",think know support additional could try script following import import import import lambda,issue,negative,neutral,neutral,neutral,neutral,neutral
1726930415,"@serena-ruan 
Ray maintainer will start review the PR soon, could you update the PR description to list detail changes in the PR (e.g., the difference with normal mode in handling temp dir, background spark job, shutdown hook, auto shutdown timout, etc) ? (for ray maintainer easier review)",ray maintainer start review soon could update description list detail difference normal mode handling temp background spark job shutdown hook auto shutdown ray maintainer easier review,issue,positive,positive,positive,positive,positive,positive
1726721046,"> I'm running on a local install.
> 
> I checked again on the logs. They are located in (with different logs for various datetimes, but they're all the same in content): 'C:\Users_________\AppData\Local\Temp\ray\session_2023-08-23_12-47-44_328823_34212\logs...'
> 
> All the log files are empty apart from **gcs_server.out**. Here's the content:
> 
> \AppData\Local\Temp\ray\session_2023-08-23_12-47-44_328823_34212\logs\gcs_server.out
> I also ran the step-by-step you've provided;
> 
> ```
> # install python3.9 from the windows app store
> >python -m venv d:\temp\cpython_39
> >d:\temp\cpython_39\Scripts\activate
> >pip install ray[default]
> >python -c ""import ray; ray.init()""
> ```
> 
> The install ran without issues, but the last line returned the error **OSError: [Errno 0] AssignProcessToJobObject() failed**.

Windows User Solution:
Stop installing the python from the windows app store
Start mentally preparing for refactoring codes for the newer and faster 3.11+ Python Version
1. Install Python 3.9-3.11 via the official site
2. Then on VSC or your other IDEs, select interpreter to be the Python from the official installation rather than the windows app store version = Otherwise, there is going to be an issue where """"""OSError: [Errno 0] AssignProcessToJobObject() failed"""""" even if you use venv
3. Using the Python from official site as the interpreter, open terminal
4. python -m venv venv
5. copy paste on terminal of the activate.bat relative path: venv\Scripts\activate.bat 
6. install the ray library
7. now you should be able to run the ray codes without a problem.

import ray
import pandas as pd
import time

start_time = time.time()

# Read CSV file into a DataFrame
icd_csv = ray.data.read_csv('datasets/ICD_code/DXCCSR_v2023-1.csv')

print(icd_csv.schema())

"""""" Result:
2023-09-19 17:40:28,250 INFO worker.py:1642 -- Started a local Ray instance.
2023-09-19 17:40:33,805 INFO read_api.py:406 -- To satisfy the requested parallelism of 64, each read task output is split into 64 smaller blocks.
Column                                  Type
------                                  ----
'ICD-10-CM CODE'                        string
'ICD-10-CM CODE DESCRIPTION'            string
'Default CCSR CATEGORY IP'              string
'Default CCSR CATEGORY DESCRIPTION IP'  string
'Default CCSR CATEGORY OP'              string
'Default CCSR CATEGORY DESCRIPTION OP'  string
'CCSR CATEGORY 1'                       string
'CCSR CATEGORY 1 DESCRIPTION'           string
'CCSR CATEGORY 2'                       string
'CCSR CATEGORY 2 DESCRIPTION'           string
'CCSR CATEGORY 3'                       string
'CCSR CATEGORY 3 DESCRIPTION'           string
'CCSR CATEGORY 4'                       string
'CCSR CATEGORY 4 DESCRIPTION'           string
'CCSR CATEGORY 5'                       string
'CCSR CATEGORY 5 DESCRIPTION'           string
'CCSR CATEGORY 6'                       string
'CCSR CATEGORY 6 DESCRIPTION'           string
""""""",running local install checked different various content log empty apart content also ran provided install python store python pip install ray default python import ray install ran without last line returned error user solution stop python store start mentally faster python version install python via official site ides select interpreter python official installation rather store version otherwise going issue even use python official site interpreter open terminal python copy paste terminal relative path install ray library able run ray without problem import ray import import time read file print result local ray instance satisfy parallelism read task output split smaller column type code string code description string category string category description string category string category description string category string category description string category string category description string category string category description string category string category description string category string category description string category string category description string,issue,negative,positive,neutral,neutral,positive,positive
1726475309,"Thanks for reporting the issue, we debugged the issue and confirmed that the root cause is that only the `DataContext`'s `ExecutionOptions` are being passed to the train workers. We are discussing internally about how to set the full `DataContext` for the streaming executor.",thanks issue issue confirmed root cause train internally set full streaming executor,issue,negative,positive,positive,positive,positive,positive
1726321847,"Hi, was this bug fixed? Encountered the same `checkpoint_00000` file instead of a safe tensors file when running the following example: https://docs.ray.io/en/latest/train/examples/deepspeed/gptj_deepspeed_fine_tuning.html ",hi bug fixed file instead safe file running following example,issue,negative,positive,positive,positive,positive,positive
1726243798,"Turns out this is in a custom runtime_env plugin & related to use-case specific custom machine infra, so closing the issue.",turn custom related specific custom machine infra issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1726237156,"```
(base) ray@ip-10-0-110-178:~$ time host ip-10-0-110-178
;; connection timed out; no servers could be reached


real    0m10.007s
user    0m0.012s
sys     0m0.001s
```",base ray time host connection timed could real user,issue,negative,negative,negative,negative,negative,negative
1726232405,"The time appears to be spent in a network call that's timing out:
```
poll([{fd=4, events=POLLIN}], 1, 5000)  = 0 (Timeout) <5.001618>
poll([{fd=4, events=POLLOUT}], 1, 0)    = 1 ([{fd=4, revents=POLLOUT}]) <0.000022>
sendmmsg(4, [{msg_hdr={msg_name=NULL, msg_namelen=0, msg_iov=[{iov_base=""d9\1 \0\1\0\0\0\0\0\1\17ip-10-0-110-178\1s\10a""..., iov_len=59}], msg_iovlen=1, msg_controllen=0, msg_flags=0}, msg_len=59}, {msg_hdr={msg_name=NULL, msg_namelen=0, msg_iov=[{iov_base=""\266?\1 \0\1\0\0\0\0\0\1\17ip-10-0-110-178\1s\10a""..., iov_len=59}], msg_iovlen=1, msg_controllen=0, msg_flags=0}, msg_len=59}], 2, MSG_NOSIGNAL) = 2 <0.000049>
poll([{fd=4, events=POLLIN}], 1, 5000)  = 0 (Timeout) <5.004609>
```",time spent network call timing poll poll poll,issue,negative,negative,neutral,neutral,negative,negative
1726198349,"Turns out the time is spent getting sudo privilege:

```bash
(base) ray@ip-10-0-110-178:~$ time echo $$ | sudo tee /sys/fs/cgroup/workers/cgroup.procs
10181

real    0m10.020s
user    0m0.004s
sys     0m0.006s
```

```bash
root@ip-10-0-110-178:/home/ray# time echo $$ | tee /sys/fs/cgroup/workers/cgroup.procs
59070

real    0m0.001s
user    0m0.002s
sys     0m0.000s
```",turn time spent getting privilege bash base ray time echo tee real user bash root time echo tee real user,issue,negative,negative,negative,negative,negative,negative
1726109789,"Removing the `cgroup` operation reduces the time:
```bash
cd /tmp/ray/session_2023-09-19_09-51-50_955056_5319/runtime_resources/working_dir_files/_ray_pkg_b97aa028940d7b33ddb2f2b92e0800b4 && echo $$ && /home/ray/anaconda3/bin/python -c 'print(""hi"")'
```

```shell
(base) ray@ip-10-0-110-178:~$ time bash test.sh 
32704
hi

real    0m0.033s
user    0m0.031s
sys     0m0.003s
```",removing operation time bash echo hi shell base ray time bash hi real user,issue,negative,negative,negative,negative,negative,negative
1726101977,"Looks like running the worker startup command is taking 10+ seconds. I copied the worker startup command from the runtime env agent and ran it manually:
```bash
export PYTHONPATH=""/tmp/ray/session_2023-09-19_09-51-50_955056_5319/runtime_resources/working_dir_files/_ray_pkg_b97aa028940d7b33ddb2f2b92e0800b4""
cd /tmp/ray/session_2023-09-19_09-51-50_955056_5319/runtime_resources/working_dir_files/_ray_pkg_b97aa028940d7b33ddb2f2b92e0800b4 && echo $$ | sudo tee /sys/fs/cgroup/workers/cgroup.procs && /home/ray/anaconda3/bin/python -c 'print(""hi"")'
```

```shell
(base) ray@ip-10-0-110-178:~$ time bash test.sh 
24207
hi

real    0m10.076s
user    0m0.035s
sys     0m0.006s
```",like running worker command taking copied worker command agent ran manually bash export echo tee hi shell base ray time bash hi real user,issue,negative,negative,negative,negative,negative,negative
1726059403,Interesting. That's a start. I'm attempting to get another cluster up and running where it's happening,interesting start get another cluster running happening,issue,negative,positive,positive,positive,positive,positive
1725472754,"> It's because the default logger uses stderr. Please make sure to initialize the logger with stdout as an emitter. I will close the issue for now. Please reopen the issue if my claim is wrong!

I had the same problem with ray 2.3, but I can't figure out why.
",default logger please make sure initialize logger emitter close issue please reopen issue claim wrong problem ray ca figure,issue,negative,neutral,neutral,neutral,neutral,neutral
1725288875,"```
python /usr/local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --help
...
  --logging-format LOGGING_FORMAT
                        The logging format. default=""%(asctime)s\t%(levelname)s %(filename)s:%(lineno)s -- %(message)s""
...
```",python help logging format message,issue,negative,neutral,neutral,neutral,neutral,neutral
1725221481,"```
python /usr/local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --help
Traceback (most recent call last):
  File ""/usr/local/lib/python3.10/site-packages/ray/dashboard/dashboard.py"", line 197, in <module>
    args = parser.parse_args()
  File ""/usr/local/lib/python3.10/argparse.py"", line 1833, in parse_args
    args, argv = self.parse_known_args(args, namespace)
  File ""/usr/local/lib/python3.10/argparse.py"", line 1866, in parse_known_args
    namespace, args = self._parse_known_args(args, namespace)
  File ""/usr/local/lib/python3.10/argparse.py"", line 2079, in _parse_known_args
    start_index = consume_optional(start_index)
  File ""/usr/local/lib/python3.10/argparse.py"", line 2019, in consume_optional
    take_action(action, args, option_string)
  File ""/usr/local/lib/python3.10/argparse.py"", line 1943, in take_action
    action(self, namespace, argument_values, option_string)
  File ""/usr/local/lib/python3.10/argparse.py"", line 1106, in __call__
    parser.print_help()
  File ""/usr/local/lib/python3.10/argparse.py"", line 2567, in print_help
    self._print_message(self.format_help(), file)
  File ""/usr/local/lib/python3.10/argparse.py"", line 2551, in format_help
    return formatter.format_help()
  File ""/usr/local/lib/python3.10/argparse.py"", line 283, in format_help
    help = self._root_section.format_help()
  File ""/usr/local/lib/python3.10/argparse.py"", line 214, in format_help
    item_help = join([func(*args) for func, args in self.items])
  File ""/usr/local/lib/python3.10/argparse.py"", line 214, in <listcomp>
    item_help = join([func(*args) for func, args in self.items])
  File ""/usr/local/lib/python3.10/argparse.py"", line 214, in format_help
    item_help = join([func(*args) for func, args in self.items])
  File ""/usr/local/lib/python3.10/argparse.py"", line 214, in <listcomp>
    item_help = join([func(*args) for func, args in self.items])
  File ""/usr/local/lib/python3.10/argparse.py"", line 540, in _format_action
    help_text = self._expand_help(action)
  File ""/usr/local/lib/python3.10/argparse.py"", line 637, in _expand_help
    return self._get_help_string(action) % params
KeyError: 'asctime'
```",python help recent call last file line module file line file line file line file line action file line action self file line file line file file line return file line help file line join file line join file line join file line join file line action file line return action,issue,positive,positive,neutral,neutral,positive,positive
1724808965,"@rkooo567 Sure. Since classmethods are not bound to a specific instance of the class, we can execute classmethods as normal ray tasks using submit_task. The first requirement for classmethods is that the first argument of the function is the class that the method belongs to. We can do this inside ray by accessing the underlying function using the __func__ magic method. Then, when executing, we can insert the respective ActorClass as the first argument into list_args to be executed. To enable this, we need to add serialization and deserialization helpers for the ActorClass. The second requirement for classmethods is that they can mutate class attributes. To support them, we keep a centralized copy of the class attributes in the kv store. Each ActorClass has a respective class_id and when we create new copies of the ActorClass through cloudpickle, we pass along the same class_id. Hence, all copies of the ActorClass have the same class_id and they can access and manipulate the contents of the class attributes in the kv store though custom __get_attribute__ and __setattr__ methods using the class_id.

![classmethodss](https://github.com/ray-project/ray/assets/88644869/a164ea3e-f08e-4b77-b8be-f123f877a4e1)

This is what the API would look like based on the implementation. The screenshot is from a pytest case that passes. ",sure since bound specific instance class execute normal ray first requirement first argument function class method inside ray underlying function magic method insert respective first argument executed enable need add serialization second requirement mutate class support keep copy class store respective create new pas along hence access manipulate content class store though custom would look like based implementation case,issue,positive,positive,positive,positive,positive,positive
1724720637,"digged a little bit more, we noticed that it takes 10 seconds for worker process to start, which is a bit uncommon.

raylet log:
```
[2023-09-18 16:22:55,501 I 8486 8486] (raylet) worker_pool.cc:499: Started worker process with pid 450102, the token is 1101
```
worker log

```
[2023-09-18 16:23:06,348 I 450102 450102] core_worker_process.cc:107: Constructing CoreWorkerProcess. pid: 450102
[2023-09-18 16:23:06,350 I 450102 450102] io_service_pool.cc:35: IOServicePool is running with 1 io_service.
```",little bit worker process start bit uncommon raylet log raylet worker process token worker log running,issue,negative,positive,positive,positive,positive,positive
1724712576,5s was due to Serve controller kill it failing the ready check. Raising the timeout to 60s we saw this finished for like 12 seconds or something,due serve controller kill failing ready check raising saw finished like something,issue,negative,positive,neutral,neutral,positive,positive
1724707211,"our impression is that the actor scheduling failed to finish in 5 seconds, where no workers received this creation request.",impression actor finish received creation request,issue,negative,neutral,neutral,neutral,neutral,neutral
1724706464,"```
session_2023-09-18_09-20-42_745954_5373/head-10.0.14.234-i-0072fb155bd6c33dd/gcs_server.out
85892:[2023-09-18 16:22:55,495 I 8034 8034] (gcs_server) gcs_actor_manager.cc:253: Registering actor, job id = 2e000000, actor id = 743aa83ad8b2905cead424a92e000000                                                          85893:[2023-09-18 16:22:55,496 I 8034 8034] (gcs_server) gcs_actor_manager.cc:259: Registered actor, job id = 2e000000, actor id = 743aa83ad8b2905cead424a92e000000
85894:[2023-09-18 16:22:55,496 I 8034 8034] (gcs_server) gcs_actor_manager.cc:278: Creating actor, job id = 2e000000, actor id = 743aa83ad8b2905cead424a92e000000                                                             85895:[2023-09-18 16:22:55,496 I 8034 8034] (gcs_server) gcs_actor_scheduler.cc:312: Start leasing worker from node e8fd59575f3f0a7311e91b3a4ced7e20830528c5e4362b5bb2e3b6d9 for actor 743aa83ad8b2905cead424a92e000000, job i
d = 2e000000                                                                                                                                                                                                                  85896:[2023-09-18 16:23:00,596 I 8034 8034] (gcs_server) gcs_actor_manager.cc:807: Destroying actor, actor id = 743aa83ad8b2905cead424a92e000000, job id = 2e000000
85898:[2023-09-18 16:23:00,596 I 8034 8034] (gcs_server) gcs_actor_manager.cc:294: Finished creating actor, job id = 2e000000, act
```",actor job id actor id registered actor job id actor id actor job id actor id start leasing worker node actor job actor actor id job id finished actor job id act,issue,negative,neutral,neutral,neutral,neutral,neutral
1724670183,"The one thing special to note about this actor is that we use `NodeAffinitySchedulingStrategy` with `soft=False` to pin it to a specific node (it has zero resource requirements otherwise). If anything changed in this scheduling path, that'd be the first place to look.",one thing special note actor use pin specific node zero resource otherwise anything path first place look,issue,negative,positive,positive,positive,positive,positive
1724669352,"cc @xieus

This is unexpected because the resources for the actor are already available and it happens when the cluster has no load, so scheduling the actor should be very fast (<1s). We can mitigate it in serve by increasing the timeout but that may mask other issues down the line.",unexpected actor already available cluster load actor fast mitigate serve increasing may mask line,issue,negative,positive,positive,positive,positive,positive
1724177878,"> > > @yutsai84 sorry, I'm thinking if we could land this example using the new API. wdyt? Landing the example using the old API sort of defeat the purpose and may cause more confusion. If you prefer, we can bring this example to user study session and use it as the subject.
> > 
> > 
> > @xwjiang2010. That sounds great. Would you have rough timeline of the new API release (ray 2.7)? I will follow closely here too. Look forward to trying out the new API!
> 
> Branch cut in a week and roughly another 2 weeks to go through the release process. Maybe let's revisit this PR in about 3 weeks?

@xwjiang2010 , I just saw the release is cut for ray 2.7. Would you want to revisit this?",sorry thinking could land example new landing example old sort defeat purpose may cause confusion prefer bring example user study session use subject great would rough new release ray follow closely look forward trying new branch cut week roughly another go release process maybe let revisit saw release cut ray would want revisit,issue,negative,positive,neutral,neutral,positive,positive
1723788694,"We also see an occasion of the restarting loop happening on the service using Ray 2.6.3 [service-embedding](https://console.anyscale.com/o/demos/services/service2_4f2qwfzfynftmh87795rj1wjw8) in the demo org. This suggest the error is not caused by the uvloop.

<img width=""1564"" alt=""image"" src=""https://github.com/ray-project/ray/assets/7553988/08f0e15e-518f-4126-b6dc-68573e5811d9"">


  ",also see occasion loop happening service ray suggest error image,issue,negative,neutral,neutral,neutral,neutral,neutral
1723783853,Unlikely to be the case of the proxy restarting issue. Closing,unlikely case proxy issue,issue,negative,negative,negative,negative,negative,negative
1723484301,"ping @krfricke 

I am not sure whether the tests are related to documentation",ping sure whether related documentation,issue,negative,positive,positive,positive,positive,positive
1722995637,"> > Looks you are using ray client, where the performance is bottlenecked by network bandwidth. Can you try run ray jobs instead?
> 
> You mean that I should put the code in the head worker?
> 
> I find a phenomenon that when the PS only runs in the head worker, we can get the result.
> 
> However, small data can run fluently.

It works. But I think  there must be something wrong with using ray client.",ray client performance network try run ray instead mean put code head worker find phenomenon head worker get result however small data run fluently work think must something wrong ray client,issue,negative,negative,negative,negative,negative,negative
1722826909,"Hello, I am having the same error on MacOS. I installed RLlib through conda but any import of Ray (core / tune / rllib) is producing this ImportError.  
My version of MacOS is Ventura 13.5.2 (22G91).

Here are the versions used :
```
ray-core                  2.3.0           py310h6d0c2b6_1  
ray-default               2.3.0           py310hecd8cb5_1  
ray-rllib                 2.3.0           py310hecd8cb5_1  
ray-tune                  2.3.0           py310hecd8cb5_1  
```
",hello error import ray core tune version used,issue,negative,neutral,neutral,neutral,neutral,neutral
1722821385,"We are currently also struggeling with this, because parts of our code already use `pydantic >=2.0`.",currently also code already use,issue,negative,neutral,neutral,neutral,neutral,neutral
1722787352,"> Looks you are using ray client, where the performance is bottlenecked by network bandwidth. Can you try run ray jobs instead?

You mean that I should put the code in the head worker?

I find a phenomenon that when the PS only runs in the head worker, we can get the result. 

However, small data can run fluently.

",ray client performance network try run ray instead mean put code head worker find phenomenon head worker get result however small data run fluently,issue,negative,negative,negative,negative,negative,negative
1722778178,Hey @Yangqing thanks a bunch for reporting! We'll take a close look.,hey thanks bunch take close look,issue,negative,positive,positive,positive,positive,positive
1722770147,docs build and book-documentation i think need to pass.,build think need pas,issue,negative,neutral,neutral,neutral,neutral,neutral
1722769508,"`ERROR: Cannot install grpcio==1.54.2 because these package versions have conflicting dependencies.`
",error install package conflicting,issue,negative,neutral,neutral,neutral,neutral,neutral
1722748500,"> @YQ-Wang 2.7 will be released very soon (within days). Are you able to use that?
Yes, that would be great. Thanks!
",soon within day able use yes would great thanks,issue,positive,positive,positive,positive,positive,positive
1722717599,"> <img alt=""image"" width=""901"" src=""https://user-images.githubusercontent.com/44538064/265942821-c8049f81-c242-4d92-a079-ce50b54c3ac4.png"">
> In the example you provided, after ds get materialized, its underlying blocks are mixed with both pyarrow.Table and pandas.DataFrame, and the function above only uses the first item do decide which BlockAccessor it should use, which is pyarrow.Table. Therefore when it handles a pandas.DataFrame an error is triggered.
> 
> I think from the APIs here, mixing pyarrow.Table and pandas.DataFrame in List[Block] should legal. Probably this API shouldn't assume all Blocks are the same format. However if you want to get around this quickly, I think you can just drop `batch_format='pandas'` so all blocks will be pyarrow.Table and everything will be fine. You can always convert the dataset to pandas using `ds.to_pandas()`.

Thanks, it helps a lot.",image example provided get underlying mixed function first item decide use therefore error triggered think list block legal probably assume format however want get around quickly think drop everything fine always convert thanks lot,issue,negative,positive,positive,positive,positive,positive
1722673746,"the multiprocessing issue can be solved setting `sys.executable = ""path/to/python""` in your main script. Although this kinda defeats the purpose of using pyinstaller in the first place...",issue setting main script although purpose first place,issue,negative,positive,positive,positive,positive,positive
1722575744,"Looks you are using ray client, where the performance is bottlenecked by network bandwidth. Can you try run ray jobs instead?",ray client performance network try run ray instead,issue,negative,neutral,neutral,neutral,neutral,neutral
1722503658,"feels like the two containers may be sharing some of the ray resources and interfering w/ each other...is there a way to start ray in each container such that it is independent and local to that container only?  at the moment im simply using it as a multi-processing replacement for python multiprocessing library. seeing alot of these errors in som of the ray logs:

```
[2023-09-17 11:21:52,199 I 41 347] raylet_client.cc:364: Error reporting task backlog information: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details:
[2023-09-17 11:21:53,199 I 41 347] raylet_client.cc:364: Error reporting task backlog information: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details:
[2023-09-17 11:21:54,190 E 41 347] gcs_rpc_client.h:547: Failed to connect to GCS within 60 seconds. GCS may have been killed. It's either GCS is terminated by `ray stop` or is killed unexpectedly. If it is killed unexpectedly, see the log file gcs_server.out. https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure. The program will terminate.
```
",like two may ray interfering way start ray container independent local container moment simply replacement python library seeing ray error task backlog information error message connect error error task backlog information error message connect error connect within may either ray stop unexpectedly unexpectedly see log file program terminate,issue,negative,positive,neutral,neutral,positive,positive
1722490266,"im seeing this issue as well...does anyone know if a rollback to previous version solves this?
i am using debian 12 (bookworm).

what's interesting is i have to containers that are exactly the same image, just different tags and name and both running on the same server.  the first instance that i brought up works fine.  the 2nd instance exhibits the issue.

UPDATE: upon testing this fruther - it seems it can happen in either container.  these two containers are on the same server but serving as different environments, one represents qa and the other prod.  if i kick the process off at the same time, then i'll see the broken pipes in both:

```
  File ""/apps/data/publish/publisher.py"", line 23, in write_data
    log_ref = ray.put(self.log) if async_write else None
              ^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/ray/_private/auto_init_hook.py"", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/ray/_private/worker.py"", line 2597, in put
    object_ref = worker.put_object(value, owner_address=serialize_owner_address)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/ray/_private/worker.py"", line 704, in put_object
    self.core_worker.put_serialized_object_and_increment_local_ref(
  File ""python/ray/_raylet.pyx"", line 2939, in ray._raylet.CoreWorker.put_serialized_object_and_increment_local_ref
  File ""python/ray/_raylet.pyx"", line 2831, in ray._raylet.CoreWorker._create_put_buffer
  File ""python/ray/_raylet.pyx"", line 412, in ray._raylet.check_status
```

im was using latest ray 2.6.3. i rolled back to 2.6.1 and still see the issue.",seeing issue well anyone know rollback previous version bookworm interesting exactly image different name running server first instance brought work fine instance issue update upon testing happen either container two server serving different one prod kick process time see broken file line else none file line return file line wrapper return file line put value file line file line file line file line latest ray rolled back still see issue,issue,positive,positive,positive,positive,positive,positive
1722445843,"+1, I have exactly the same use case. really want a combined image.",exactly use case really want combined image,issue,negative,positive,positive,positive,positive,positive
1722348692,"> I think the property could just raise an exception instead?

Then it will be similar with current approach right? When an attribute of `core_worker` not found, it will also just raise an exception. ",think property could raise exception instead similar current approach right attribute found also raise exception,issue,negative,positive,neutral,neutral,positive,positive
1722331508,This is awesome! Is there a plan to integrate this within Ray directly?,awesome plan integrate within ray directly,issue,positive,positive,positive,positive,positive,positive
1722253471,@YQ-Wang 2.7 will be released very soon (within days). Are you able to use that? ,soon within day able use,issue,negative,positive,positive,positive,positive,positive
1722122246,"Found the answer, targeting for release 2.8, sorry for the bothering.",found answer release sorry,issue,negative,negative,negative,negative,negative,negative
1722122131,"Note that after discussion today this was not deemed a priority at the moment, but this is for tracking the issue for some point in the future.",note discussion today priority moment issue point future,issue,negative,neutral,neutral,neutral,neutral,neutral
1722121601,"> I don't think I'm the right person to get serve working with pydantic 2.0, @akshay-anyscale , can we have someone working on serve pick up the remaining work here?

Sorry for the ping, any plan to support pydantic 2.0 on ray serve?",think right person get serve working someone working serve pick work sorry ping plan support ray serve,issue,negative,negative,negative,negative,negative,negative
1722074290,"> Do you know why it passed before this pr?

It failed before as well to all commits after I added the unit test.
https://buildkite.com/ray-project/oss-ci-build-pr/builds/36093#018a8f91-5347-4e2e-a88a-e70fa9137746/3736-3919",know well added unit test,issue,negative,neutral,neutral,neutral,neutral,neutral
1722025380,Linkcheck failure unrelated (this PR doesn't modify any links),failure unrelated modify link,issue,negative,negative,negative,negative,negative,negative
1721987540,"Resolved the `test_tls_auth` but `test_ray_init:test_ray_init_sigterm_handler` failed on window build. Seems like window doesn't handle SIGTERM well as described here
https://stackoverflow.com/questions/35772001/how-to-handle-a-signal-sigint-on-a-windows-os-machine
Thus, will skip the test when it run on window",resolved window build like window handle well thus skip test run window,issue,positive,neutral,neutral,neutral,neutral,neutral
1721984459,"> what are 600 and 100-200 here? RPS or tokens per second?

Update the ticket, it is the printed info from the code. We record the time of per-token throughput.",per second update ticket printed code record time throughput,issue,negative,neutral,neutral,neutral,neutral,neutral
1721977393,"Note: We can also use this new codepath to implement the ""SplitBlocks"" op and eventually streaming repartition. This codepath should be better for SplitBlocks because it will handle different-sized blocks more evenly.",note also use new implement eventually streaming repartition better handle evenly,issue,negative,positive,positive,positive,positive,positive
1721977220,"For me it was a mistake in multi-agent environment step function. I resolved it, here is a working code below,

```
    def step(self, action_dict):
        assert (
            len(self.terminateds) != self.env.num_players
        )  # as found in ray.rllib.examples.env.multi_agent

        obs, reward, terminated, truncated, info = (
            {self.i: self.observation_space.sample()},
            0.0,
            False,
            False,
            {},
        )
        for i, action in action_dict.items():
            obs, reward, terminated, truncated, info = self.env.step(action)

        reward_dict = {self.i: reward}
        terminated = {self.i: terminated}
        if terminated[self.i]:
            self.terminateds.add(self.i)
        truncated = {self.i: truncated}
        if truncated[self.i]:
            self.truncateds.add(self.i)

        # Iter next player's perspective
        self.i = (self.i + 1) % self.env.num_players
        obs_dict = {self.i: obs}
        info_dict = {self.i: info}

        terminated[""__all__""] = len(self.terminateds) == self.env.num_players - 1
        truncated[""__all__""] = len(self.truncateds) == self.env.num_players - 1
        return obs_dict, reward_dict, terminated, truncated, info_dict
```",mistake environment step function resolved working code step self assert found reward truncated false false action reward truncated action reward truncated truncated truncated iter next player perspective truncated return truncated,issue,positive,negative,negative,negative,negative,negative
1721939944,"I'm facing the same issue as @marrrcin @jednymslowem described. I've Used the [Getting Started Guide](https://docs.ray.io/en/latest/cluster/kubernetes/getting-started.html#kuberay-quickstart) to set up ray cluster on KiND. Did the required port-forwarding, and set `RAY_ADDRESS` env variable to point to the right url (I'm able to access the dashboard) and I get the same error:
```
2023-09-16 01:16:49,657	INFO worker.py:1313 -- Using address 127.0.0.1:8265 set in the environment variable RAY_ADDRESS
2023-09-16 01:16:49,658	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 127.0.0.1:8265...
2023-09-16 01:16:54,857	ERROR utils.py:1395 -- Failed to connect to GCS. Please check `gcs_server.out` for more details.
2023-09-16 01:16:54,857	WARNING utils.py:1401 -- Unable to connect to GCS (ray head) at 127.0.0.1:8265. Check that (1) Ray with matching version started successfully at the specified address, (2) this node can reach the specified address, and (3) there is no firewall setting preventing access.
```

I would very appreciate any help, currently can't figure this out.",facing issue used getting guide set ray cluster kind set variable point right able access dashboard get error address set environment variable ray cluster address error connect please check warning unable connect ray head check ray matching version successfully address node reach address setting access would appreciate help currently ca figure,issue,positive,positive,positive,positive,positive,positive
1721923401,Added `--pull`. Change the buildkite base to forge to save 5 minutes of pulling these corebuild/databuild/servebuild images.,added pull change base forge save,issue,negative,negative,negative,negative,negative,negative
1721919561,"If remove the router deployment, it can boost the 3-4x throughput (from locust data)",remove router deployment boost throughput locust data,issue,negative,neutral,neutral,neutral,neutral,neutral
1721903826,"> if it passes 5 times in a row, it lgtm

Tried and failed several times (Either Flaky or Failed all) so on-hold for now",time row tried several time either flaky,issue,negative,neutral,neutral,neutral,neutral,neutral
1721891680,locust: 100 users at the same time and then stop the benchmarking to trigger the disconnect. actor crashed.,locust time stop trigger disconnect actor,issue,negative,neutral,neutral,neutral,neutral,neutral
1721850821,"> I wonder if folks can just compile this locally? Or is it because they don't have a linux machine?

Yes, it's mostly to streamline the compilation process. `pip-compile` can also be affected by locally installed packages/caching, so running it on a clean environment provides a common way to get a valid constraints file.",wonder compile locally machine yes mostly streamline compilation process also affected locally running clean environment common way get valid file,issue,positive,positive,positive,positive,positive,positive
1721707332,"Oh, we need to close the issue, otherwise the tests won't run nightly. I'll close this one then. The bot will reopen the issue if the test fails tonight.",oh need close issue otherwise wo run nightly close one bot reopen issue test tonight,issue,negative,neutral,neutral,neutral,neutral,neutral
1721683959,"Should be fixed by https://github.com/ray-project/ray/pull/39276, which was merged yesterday. Will monitor to see if the issue persists",fixed yesterday monitor see issue,issue,negative,positive,neutral,neutral,positive,positive
1721680080,@PhysicsACE can you create a simple proposal here? I think we need a bit of design (and clear specification of how the API looks like),create simple proposal think need bit design clear specification like,issue,positive,positive,neutral,neutral,positive,positive
1721678983,"Ideally we want to store logs generated from a driver too, but not sure how easy it is",ideally want store driver sure easy,issue,positive,positive,positive,positive,positive,positive
1721677781,Q: can we hide batch from the API and just do internally? Maybe we can set a parameter to actor pool or submit API instead? ,hide batch internally maybe set parameter actor pool submit instead,issue,negative,neutral,neutral,neutral,neutral,neutral
1721675561,Conclusion: We will make a long term fix by 2.8,conclusion make long term fix,issue,negative,negative,neutral,neutral,negative,negative
1721666732,I think the property could just raise an exception instead?,think property could raise exception instead,issue,negative,neutral,neutral,neutral,neutral,neutral
1721662509,"> LGTM. I think we should make self.core_worker to be a property instead in the longer term...

I tried in this PR actually - i think it's not a huge improvement, instead of calling `if hasattr(workrer, ""core_worker"")` we will do `if worker.core_worker`",think make property instead longer term tried actually think huge improvement instead calling,issue,positive,positive,positive,positive,positive,positive
1721621105,"> Hi @architkulkarni , kindly let us know if there is anything else we should add or amend. We'd like to include this guide in the 2.7 release 😄

Unfortunately the cherry-picks for 2.7 ended last Friday. However, you can always redirect users to the master branch docs https://docs.ray.io/en/master/cluster/vms/user-guides/launching-clusters/index.html#launching-vm-clusters (should be updated automatically in a few hours or a day)",hi kindly let u know anything else add amend like include guide release unfortunately ended last however always redirect master branch automatically day,issue,positive,positive,neutral,neutral,positive,positive
1721613717,"Failed tests: 
- learning_tests_stateless_cartpole_r2d2 unrelated
- rllib:examples/nested_action_spaces_ppo_torch unrelated
- Linkcheck error https://raw.githubusercontent.com/ray-project/ray/master/python/ray/autoscaler/vsphere/example-vsan-file-service.yaml will be fixed when this PR is merged
- Serve release tests and HA tests unrelated
- kuberay/test_autoscaling_e2e unrelated
- test_redis_tls unrelated",unrelated unrelated error fixed serve release ha unrelated unrelated unrelated,issue,negative,positive,neutral,neutral,positive,positive
1721590648,I wonder if folks can just compile this locally? Or is it because they don't have a linux machine?,wonder compile locally machine,issue,negative,neutral,neutral,neutral,neutral,neutral
1721561464,"Sound good, latest master run: https://buildkite.com/ray-project/release-tests-branch/builds/2181",sound good latest master run,issue,negative,positive,positive,positive,positive,positive
1721552815,"This issue fixed with this PR! https://github.com/ray-project/ray/pull/39654
MBMPO works, I tested it!",issue fixed work tested,issue,negative,positive,neutral,neutral,positive,positive
1721531817,I checked that the doctest failure is present on the base revision as well. The remaining failures are not related.,checked failure present base revision well related,issue,negative,negative,negative,negative,negative,negative
1721501677,"I am running the tests without this pick but under another branch name here:

https://buildkite.com/ray-project/oss-ci-build-branch/builds/6134

and we will see :)",running without pick another branch name see,issue,negative,neutral,neutral,neutral,neutral,neutral
1721498883,"it's probably because the base images of the branch is stale: https://buildkite.com/ray-project/oss-ci-build-branch/builds/6133#018a9955-5647-4a34-8cf8-1f5f491d2ab9/2079

if you change the branch name (just for that test), or just delete the images like 029272617770.dkr.ecr.us-west-2.amazonaws.com/ci_base_images:oss-ci-base_gpu_latest_releases_2.7.0 , the error might go away.",probably base branch stale change branch name test delete like error might go away,issue,negative,negative,negative,negative,negative,negative
1721493435,"Yea, I ran this dozens of times on the release branch in all the recent builds https://buildkite.com/ray-project/oss-ci-build-branch/builds/6122 https://buildkite.com/ray-project/oss-ci-build-branch/builds/6126 https://buildkite.com/ray-project/oss-ci-build-branch/builds/6128 and never see it passing so decided to patch it. Agreed it's not required on master. Will not pick to master at this point unless we start seeing similar issues 👍",yea ran time release branch recent never see passing decided patch agreed master pick master point unless start seeing similar,issue,negative,neutral,neutral,neutral,neutral,neutral
1721483595,"you can probably just rerun the entire build job and get it passing. I think the base image will be rebuilt on every run.

it is also okay to merge it into master. though I do no think this change is really required.",probably rerun entire build job get passing think base image rebuilt every run also merge master though think change really,issue,negative,negative,negative,negative,negative,negative
1721409239,"@aslonnie This is running fine on master, but let me know if you want me to pick this onto master as well.",running fine master let know want pick onto master well,issue,negative,positive,positive,positive,positive,positive
1721322230,"I have a similar issue, where we have two pieces of our dataset for a recommender:
1. interactions, where we have a user id and an item id
2. metadata, where we have various columns of metadata for each user and item

ideally these could be lazily joined whenever a batch is requested, that way we don't need to needlessly duplicate the metadata across every row in advance. is something like that possible?",similar issue two recommender user id item id various user item ideally could lazily whenever batch way need needlessly duplicate across every row advance something like possible,issue,positive,positive,neutral,neutral,positive,positive
1721047015,"In our lab (in a public research institute), there are multiple servers that don't have any shared storage. Do I understand correctly that such a setup is no longer supported?",lab public research institute multiple storage understand correctly setup longer,issue,negative,neutral,neutral,neutral,neutral,neutral
1720874367,"Jovany-wang and I reproduced it together and found that it didn't occur on CentOS,
 but we were able to reproduce it on Ubuntu and Mac. 
We may need your help to take a look at this issue.  @jjyao @rkooo567 ",together found occur able reproduce mac may need help take look issue,issue,negative,positive,positive,positive,positive,positive
1720868532,"I'm using SLURM nodes. Each Node had 8 GPUs and 64 CPUs available to it ; I had 2 Nodes. The GPUs are RTX 2080 Ti.

<details>
  <summary>Output from the slurm error Log</summary>

``` bash
[2023-09-15 00:42:21,354 I 2007 2007] global_state_accessor.cc:368: This node has an IP address of 10.5.166.180, but we cannot find a local Raylet with the same address. This can happen when you connect to the Ray cluster with a different IP address or when connecting to a container.
2023-09-15 00:42:33,629 INFO worker.py:1465 -- Connecting to existing Ray cluster at address: 10.5.166.177:6379...
DEBUG:filelock:Attempting to acquire lock 140380820045936 on /tmp/ray/session_2023-09-15_00-41-50_138801_2959/node_ip_address.json.lock
DEBUG:filelock:Lock 140380820045936 acquired on /tmp/ray/session_2023-09-15_00-41-50_138801_2959/node_ip_address.json.lock
DEBUG:filelock:Lock 140380820046272 acquired on /tmp/ray/session_2023-09-15_00-41-50_138801_2959/ports_by_node.json.lock
DEBUG:filelock:Attempting to release lock 140380820046272 on /tmp/ray/session_2023-09-15_00-41-50_138801_2959/ports_by_node.json.lock
DEBUG:filelock:Lock 140380820046272 released on /tmp/ray/session_2023-09-15_00-41-50_138801_2959/ports_by_node.json.lock
2023-09-15 00:42:33,640 INFO worker.py:1640 -- Connected to Ray cluster. View the dashboard at http://127.0.0.1:8265 
2023-09-15 00:42:39,067 INFO tuner_internal.py:466 -- A `RunConfig` was passed to both the `Tuner` and the `TorchTrainer`. The run config passed to the `Tuner` is the one that will be used.
2023-09-15 00:42:39,149 INFO tune.py:654 -- [output] This will use the new output engine with verbosity 2. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
DEBUG:fsspec.local:open file: /XXXXXXXXXXX/ray_results/10DataLocSentiLex/tuner.pkl
DEBUG:fsspec.local:open file: /XXXXXXXXXXX/ray_results/10DataLocSentiLex/basic-variant-state-2023-09-15_00-42-39.json
DEBUG:fsspec.local:open file: /XXXXXXXXXXX/ray_results/10DataLocSentiLex/experiment_state-2023-09-15_00-42-39.json
(TorchTrainer pid=7841) Starting distributed worker processes: ['8006 (10.5.166.177)', '8007 (10.5.166.177)', '8008 (10.5.166.177)', '8009 (10.5.166.177)', '8010 (10.5.166.177)', '8011 (10.5.166.177)', '8012 (10.5.166.177)', '8013 (10.5.166.177)']
(RayTrainWorker pid=8006) Setting up process group for: env:// [rank=0, world_size=8]
(TorchTrainer pid=2376, ip=10.5.166.180) Starting distributed worker processes: ['2501 (10.5.166.180)', '2502 (10.5.166.180)', '2503 (10.5.166.180)', '2504 (10.5.166.180)', '2505 (10.5.166.180)', '2506 (10.5.166.180)', '2507 (10.5.166.180)', '2508 (10.5.166.180)']
(RayTrainWorker pid=2504, ip=10.5.166.180) [rank: 3] Global seed set to 1234
(RayTrainWorker pid=2501, ip=10.5.166.180) Setting up process group for: env:// [rank=0, world_size=8]
(RayTrainWorker pid=2504, ip=10.5.166.180) LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
DEBUG:fsspec.local:open file: /XXXXXXXXXXX/ray_results/10DataLocSentiLex/TorchTrainer_02133_00000_Bys2a/Tokenised.lock
DEBUG:fsspec.local:open file: /XXXXXXXXXXX/ray_results/10DataLocSentiLex/TorchTrainer_02133_00000_Bys2a/params.pkl
DEBUG:fsspec.local:open file: /XXXXXXXXXXX/ray_results/10DataLocSentiLex/TorchTrainer_02133_00000_Bys2a/params.json
DEBUG:fsspec.local:open file: /XXXXXXXXXXX/ray_results/10DataLocSentiLex/TorchTrainer_02133_00000_Bys2a/events.out.tfevents.1694731368.dlcgpu17
DEBUG:fsspec.local:open file: /XXXXXXXXXXX/ray_results/10DataLocSentiLex/TorchTrainer_02133_00000_Bys2a/ray_results_log/torch_trainer_logs/csv_torch_trainer_logs/metrics.csv
DEBUG:fsspec.local:open file: /XXXXXXXXXXX/ray_results/10DataLocSentiLex/TorchTrainer_02133_00000_Bys2a/ray_results_log/torch_trainer_logs/csv_torch_trainer_logs/checkpoints/epoch=2-step=672.ckpt
DEBUG:fsspec.local:open file: /XXXXXXXXXXX/ray_results/10DataLocSentiLex/TorchTrainer_02133_00000_Bys2a/progress.csv
(RayTrainWorker pid=9232) Setting up process group for: env:// [rank=0, world_size=8]

(RayTrainWorker pid=9237) LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7] [repeated 7x across cluster]
(RayTrainWorker pid=9232) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/work/dlclarge1/XXXXXXXXXXX/ray_cluster_test/ray_results_result/10DataLocSentiLex/TorchTrainer_02133_00002_gBsbc/checkpoint_000000) [repeated 7x across cluster]
(TorchTrainer pid=10349) Starting distributed worker processes: ['10477 (10.5.166.177)', '10478 (10.5.166.177)', '10479 (10.5.166.177)', '10480 (10.5.166.177)', '10481 (10.5.166.177)', '10482 (10.5.166.177)', '10483 (10.5.166.177)', '10484 (10.5.166.177)']
(RayTrainWorker pid=10477) Setting up process group for: env:// [rank=0, world_size=8]
Map:   0%|          | 0/3974 [00:00<?, ? examples/s]
Map: 100%|██████████| 3974/3974 [00:00<00:00, 34273.36 examples/s]
Map: 100%|██████████| 3974/3974 [00:00<00:00, 42666.13 examples/s]
Map:  25%|██▌       | 1000/3974 [00:00<00:00, 3427.19 examples/s]
(RayTrainWorker pid=10482) [rank: 5] Global seed set to 1234 [repeated 7x across cluster]
2023-09-15 00:49:31,019 ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_02133_00003_AQA9Q
Traceback (most recent call last):
  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py"", line 110, in resolve_future
    result = ray.get(future)
  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/ray/_private/auto_init_hook.py"", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/ray/_private/worker.py"", line 2554, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(DeferredCudaCallError): ray::_Inner.train() (pid=10349, ip=10.5.166.177, actor_id=72bb75c5cad7e1c46b5c31d201000000, repr=TorchTrainer)
  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/ray/tune/trainable/trainable.py"", line 400, in train
    raise skipped from exception_cause(skipped)
  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/ray/train/_internal/utils.py"", line 54, in check_for_failure
    ray.get(object_ref)
ray.exceptions.RayTaskError(DeferredCudaCallError): ray::_RayTrainWorker__execute.get_next() (pid=10481, ip=10.5.166.177, actor_id=051b2da48eb0dbfbdbc6f0a701000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7ef05023b460>)
  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/torch/cuda/__init__.py"", line 145, in _check_capability
    capability = get_device_capability(d)
  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/torch/cuda/__init__.py"", line 381, in get_device_capability
    prop = get_device_properties(device)
  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/torch/cuda/__init__.py"", line 399, in get_device_properties
    return _get_device_properties(device)  # type: ignore[name-defined]
RuntimeError: CUDA error: unknown error
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

ray::_RayTrainWorker__execute.get_next() (pid=10481, ip=10.5.166.177, actor_id=051b2da48eb0dbfbdbc6f0a701000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7ef05023b460>)
  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/ray/train/_internal/worker_group.py"", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/ray/train/_internal/utils.py"", line 129, in discard_return_wrapper
    train_func(*args, **kwargs)
    prop = get_device_properties(device)
  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/torch/cuda/__init__.py"", line 395, in get_device_properties
    _lazy_init()  # will define _get_device_properties
  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/torch/cuda/__init__.py"", line 264, in _lazy_init
    raise DeferredCudaCallError(msg) from e
torch.cuda.DeferredCudaCallError: CUDA call failed lazily at initialization with error: CUDA error: unknown error
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


CUDA call was originally invoked at:

['  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/ray/_private/workers/default_worker.py"", line 278, in <module>\n    worker.main_loop()\n', '  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/ray/_private/worker.py"", line 783, in main_loop\n    self.core_worker.run_task_loop()\n', '  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/ray/_private/worker.py"", line 729, in deserialize_objects\n    return context.deserialize_objects(data_metadata_pairs, object_refs)\n', '  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/ray/_private/serialization.py"", line 404, in deserialize_objects\n    obj = self._deserialize_object(data, metadata, object_ref)\n', '  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/ray/_private/serialization.py"", line 270, in _deserialize_object\n    return self._deserialize_msgpack_data(data, metadata_fields)\n', '  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/ray/_private/serialization.py"", line 225, in _deserialize_msgpack_data\n    python_objects = self._deserialize_pickle5_data(pickle5_data)\n', '  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/ray/_private/serialization.py"", line 215, in _deserialize_pickle5_data\n    obj = pickle.loads(in_band)\n', '  File ""<frozen importlib._bootstrap>"", line 1027, in _find_and_load\n', '  File ""<frozen importlib._bootstrap>"", line 992, in _find_and_load_unlocked\n', '  File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed\n', '  File ""<frozen importlib._bootstrap>"", line 1027, in _find_and_load\n', '  File ""<frozen importlib._bootstrap>"", line 1006, in _find_and_load_unlocked\n', '  File ""<frozen importlib._bootstrap>"", line 688, in _load_unlocked\n', '  File ""<frozen importlib._bootstrap_external>"", line 883, in exec_module\n', '  File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed\n', '  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/ray/train/torch/__init__.py"", line 3, in <module>\n    import torch  # noqa: F401\n', '  File ""<frozen importlib._bootstrap>"", line 1027, in _find_and_load\n', '  File ""<frozen importlib._bootstrap>"", line 1006, in _find_and_load_unlocked\n', '  File ""<frozen importlib._bootstrap>"", line 688, in _load_unlocked\n', '  File ""<frozen importlib._bootstrap_external>"", line 883, in exec_module\n', '  File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed\n', '  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/torch/__init__.py"", line 1146, in <module>\n    _C._initExtension(manager_path())\n', '  File ""<frozen importlib._bootstrap>"", line 1027, in _find_and_load\n', '  File ""<frozen importlib._bootstrap>"", line 1006, in _find_and_load_unlocked\n', '  File ""<frozen importlib._bootstrap>"", line 688, in _load_unlocked\n', '  File ""<frozen importlib._bootstrap_external>"", line 883, in exec_module\n', '  File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed\n', '  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/torch/cuda/__init__.py"", line 197, in <module>\n    _lazy_call(_check_capability)\n', '  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/torch/cuda/__init__.py"", line 195, in _lazy_call\n    _queued_calls.append((callable, traceback.format_stack()))\n']
(TorchTrainer pid=11391) Starting distributed worker processes: ['11499 (10.5.166.177)', '11500 (10.5.166.177)', '11501 (10.5.166.177)', '11502 (10.5.166.177)', '11503 (10.5.166.177)', '11504 (10.5.166.177)', '11505 (10.5.166.177)', '11506 (10.5.166.177)']

2023-09-15 00:49:58,967 ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_02133_00004_TyQxl
Traceback (most recent call last):
  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/pytorch_lightning/accelerators/cuda.py"", line 44, in setup_device
    _check_cuda_matmul_precision(device)
  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/lightning_fabric/accelerators/cuda.py"", line 349, in _check_cuda_matmul_precision
    major, _ = torch.cuda.get_device_capability(device)
  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/torch/cuda/__init__.py"", line 381, in get_device_capability
    prop = get_device_properties(device)
  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/torch/cuda/__init__.py"", line 398, in get_device_properties
    raise AssertionError(""Invalid device id"")
AssertionError: Invalid device id
(TorchTrainer pid=12403) Starting distributed worker processes: ['12536 (10.5.166.177)', '12537 (10.5.166.177)', '12538 (10.5.166.177)', '12539 (10.5.166.177)', '12540 (10.5.166.177)', '12541 (10.5.166.177)', '12542 (10.5.166.177)', '12543 (10.5.166.177)']
(RayTrainWorker pid=11500) [rank: 1] Global seed set to 42 [repeated 4x across cluster]
(RayTrainWorker pid=12536) Setting up process group for: env:// [rank=0, world_size=8]
(RayTrainWorker pid=12542) /XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
(RayTrainWorker pid=12542)   warnings.warn(""Can't initialize NVML"")
(RayTrainWorker pid=12542) [rank: 6] Global seed set to 1234
(RayTrainWorker pid=12541) /XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML [repeated 7x across cluster]
(RayTrainWorker pid=12541)   warnings.warn(""Can't initialize NVML"") [repeated 7x across cluster]
(RayTrainWorker pid=12536) GPU available: True (cuda), used: True
(RayTrainWorker pid=12536) TPU available: False, using: 0 TPU cores
(RayTrainWorker pid=12536) IPU available: False, using: 0 IPUs
(RayTrainWorker pid=12536) HPU available: False, using: 0 HPUs
2023-09-15 00:50:27,045 ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_02133_00005_WqX4q
Traceback (most recent call last):
  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py"", line 110, in resolve_future
    result = ray.get(future)
  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/ray/_private/auto_init_hook.py"", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/ray/_private/worker.py"", line 2554, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(AssertionError): ray::_Inner.train() (pid=12403, ip=10.5.166.177, actor_id=8973239945bd93cc9cfd39cb01000000, repr=TorchTrainer)
  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/ray/tune/trainable/trainable.py"", line 400, in train
    raise skipped from exception_cause(skipped)
  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/ray/train/_internal/utils.py"", line 54, in check_for_failure
    ray.get(object_ref)
    prop = get_device_properties(device)
  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/torch/cuda/__init__.py"", line 398, in get_device_properties
    raise AssertionError(""Invalid device id"")
AssertionError: Invalid device id
(TorchTrainer pid=13442) Starting distributed worker processes: ['13554 (10.5.166.177)', '13555 (10.5.166.177)', '13556 (10.5.166.177)', '13557 (10.5.166.177)', '13558 (10.5.166.177)', '13559 (10.5.166.177)', '13560 (10.5.166.177)', '13561 (10.5.166.177)']

2023-09-15 00:50:53,563 ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_02133_00006_ctZ2i
Traceback (most recent call last):
  File ""/XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py"", line 110, in resolve_future
    result = ray.get(future)

(TorchTrainer pid=14458) Starting distributed worker processes: ['14588 (10.5.166.177)', '14589 (10.5.166.177)', '14590 (10.5.166.177)', '14591 (10.5.166.177)', '14592 (10.5.166.177)', '14593 (10.5.166.177)', '14594 (10.5.166.177)', '14595 (10.5.166.177)']
(RayTrainWorker pid=13559) Some weights of BertForSequenceClassification were not initialized from the model checkpoint at deepset/bert-base-german-cased-oldvocab and are newly initialized: ['classifier.bias', 'classifier.weight'] [repeated 2x across cluster]
(RayTrainWorker pid=13559) You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. [repeated 7x across cluster]
(RayTrainWorker pid=13555) Some weights of BertForSequenceClassification were not initialized from the model checkpoint at deepset/bert-base-german-cased-oldvocab and are newly initialized: ['classifier.weight', 'classifier.bias'] [repeated 4x across cluster]
(RayTrainWorker pid=13561) [rank: 7] Global seed set to 1234 [repeated 6x across cluster]
(RayTrainWorker pid=14588) Setting up process group for: env:// [rank=0, world_size=8]
(RayTrainWorker pid=14588) /XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
(RayTrainWorker pid=14588)   warnings.warn(""Can't initialize NVML"")
Map:   0%|          | 0/3974 [00:00<?, ? examples/s]
(RayTrainWorker pid=14594) /XXXXXXXXXXX/XXXXXXXXXXX/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML [repeated 7x across cluster]
(RayTrainWorker pid=14594)   warnings.warn(""Can't initialize NVML"") [repeated 7x across cluster]

```
</details>",node available ti summary output error log bash node address find local raylet address happen connect ray cluster different address container ray cluster address acquire lock lock acquired lock acquired release lock lock connected ray cluster view dashboard tuner run tuner one used output use new output engine verbosity disable new output use legacy output engine set environment variable information please see open file open file open file starting distributed worker setting process group starting distributed worker rank global seed set setting process group open file open file open file open file open file open file open file setting process group repeated across cluster successfully repeated across cluster starting distributed worker setting process group map map map map rank global seed set repeated across cluster error trial task trial recent call last file line result future file line return file line wrapper return file line get raise ray file line train raise file line ray object file line capability file line prop device file line return device type ignore error unknown error compile enable exception direct cause following exception ray object file line raise file line prop device file line define file line raise call lazily error error unknown error compile enable call originally file line module file line file line return file line data file line return data file line file line file frozen line file frozen line file frozen line file frozen line file frozen line file frozen line file frozen line file frozen line file line module import torch file frozen line file frozen line file frozen line file frozen line file frozen line file line module file frozen line file frozen line file frozen line file frozen line file frozen line file line module file line callable starting distributed worker error trial task trial recent call last file line device file line major device file line prop device file line raise invalid device id invalid device id starting distributed worker rank global seed set repeated across cluster setting process group ca initialize ca initialize rank global seed set ca initialize repeated across cluster ca initialize repeated across cluster available true used true available false available false available false error trial task trial recent call last file line result future file line return file line wrapper return file line get raise ray file line train raise file line prop device file line raise invalid device id invalid device id starting distributed worker error trial task trial recent call last file line result future starting distributed worker model newly repeated across cluster probably train model task able use inference repeated across cluster model newly repeated across cluster rank global seed set repeated across cluster setting process group ca initialize ca initialize map ca initialize repeated across cluster ca initialize repeated across cluster,issue,negative,negative,neutral,neutral,negative,negative
1720750297,"I previously claimed that I reproduced it, but actually I didn't. I tried two versions later and still couldn't replicate your issue.
@jovany-wang  
I would like to meet you in person to reproduce it offline.",previously actually tried two later still could replicate issue would like meet person reproduce,issue,negative,neutral,neutral,neutral,neutral,neutral
1720386109,"One Ray on spark unit test failed:
https://buildkite.com/ray-project/oss-ci-build-pr/builds/36156#018a92ba-9d35-4f3d-a23f-e34d5efa67f8",one ray spark unit test,issue,negative,neutral,neutral,neutral,neutral,neutral
1720381098,"> In the mean time, can you share a more information how Intel GPU/Accelerator can be used with other libraries like pytorch? Thanks

@scv119 check here
https://github.com/ray-project/ray/pull/36493#issuecomment-1661828459",mean time share information used like thanks check,issue,positive,negative,neutral,neutral,negative,negative
1720366122,"Hi @architkulkarni , kindly let us know if there is anything else we should add or amend. We'd like to include this guide in the 2.7 release 😄 ",hi kindly let u know anything else add amend like include guide release,issue,positive,positive,positive,positive,positive,positive
1720358757,"> hi @harborn, thanks again for submitting this PR to support intel accelerators in Ray. Given multiple teams are trying to add support of different accelerators to Ray. We'd like to come up with a design first to see how we should support them in a unified way [holistic design](https://github.com/ray-project/ray/issues/38504) in Ray 2.8, which will be started after ray summit (9/20).
> 
> In the mean time, can you share a more information how Intel GPU/Accelerator can be used with other libraries like pytorch? Thanks

pytorch intel extension: [https://github.com/intel/intel-extension-for-pytorch](url)
Collective Communications Library by Intel oneAPI: [https://github.com/intel/torch-ccl](url)",hi thanks support ray given multiple trying add support different ray like come design first see support unified way holistic design ray ray summit mean time share information used like thanks extension collective library,issue,positive,positive,neutral,neutral,positive,positive
1720349037,All children tasks look closed - @matthewdeng can we close this master task?,look closed close master task,issue,negative,negative,neutral,neutral,negative,negative
1720347706,You will need to create a ticket in https://github.com/grpc/grpc/issues with a minimal reproduction (i.e. one that does not involve Ray),need create ticket minimal reproduction one involve ray,issue,negative,negative,neutral,neutral,negative,negative
1720341687,"@pcmoritz This was just a sideline work from https://anyscaleteam.slack.com/archives/C01DVKB5SHE/p1694558756536899?thread_ts=1694556456.013449&cid=C01DVKB5SHE That we are testing if we can find a grpc version that doesn't have performance regression while have a complete memory leak fixed. The regression is simply looking at the `multi_client_tasks_async` number from the release test [microbenchmark.aws](https://buildkite.com/ray-project/release-tests-pr/builds/53172#018a8d85-ef8e-41ec-bdc0-35dee6a95c66/364-420) I think none of those 1.51.x-1.58.0 are candidates for us. Let me know if you already have a ticket tracking grpc upgrades, I can add some notes there. Or let me know if you want me to create a ticket talking about it. Also note those are running against the release branch not to master, but probably won't make much differences anyways.  ",sideline work testing find version performance regression complete memory leak fixed regression simply looking number release test think none u let know already ticket add let know want create ticket talking also note running release branch master probably wo make much anyways,issue,negative,positive,neutral,neutral,positive,positive
1720336496,Did you create an upstream ticket with a repro of the regression? That's very important to do -- at some point we WILL need to upgrade (e.g. due to security reasons) so if the regression is fixed soon that will make our lives much easier :),create upstream ticket regression important point need upgrade due security regression fixed soon make much easier,issue,positive,positive,positive,positive,positive,positive
1720324436,"The dataset used here doesn't support streaming due to the `.tar` archive format:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/opt/anaconda3/envs/ray-build/lib/python3.9/site-packages/datasets/load.py"", line 2146, in load_dataset
    return builder_instance.as_streaming_dataset(split=split)
  File ""/opt/anaconda3/envs/ray-build/lib/python3.9/site-packages/datasets/builder.py"", line 1329, in as_streaming_dataset
    splits_generators = {sg.name: sg for sg in self._split_generators(dl_manager)}
  File ""/Users/sjl/.cache/huggingface/modules/datasets_modules/datasets/lj_speech/6e03b5308ab68705d902e2bbeaff59feb283307770fafb8d6704e604b6d179aa/lj_speech.py"", line 90, in _split_generators
    root_path = dl_manager.download_and_extract(_DL_URL)
  File ""/opt/anaconda3/envs/ray-build/lib/python3.9/site-packages/datasets/download/streaming_download_manager.py"", line 1063, in download_and_extract
    return self.extract(self.download(url_or_urls))
  File ""/opt/anaconda3/envs/ray-build/lib/python3.9/site-packages/datasets/download/streaming_download_manager.py"", line 1015, in extract
    urlpaths = map_nested(self._extract, url_or_urls, map_tuple=True)
  File ""/opt/anaconda3/envs/ray-build/lib/python3.9/site-packages/datasets/utils/py_utils.py"", line 456, in map_nested
    return function(data_struct)
  File ""/opt/anaconda3/envs/ray-build/lib/python3.9/site-packages/datasets/download/streaming_download_manager.py"", line 1025, in _extract
    raise NotImplementedError(
NotImplementedError: Extraction protocol for TAR archives like 'https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2' is not implemented in streaming mode. Please use `dl_manager.iter_archive` instead.
```

This means that `from_huggingface` will fall back to the non-streaming implementation, which calls `from_arrow` on the HF Dataset's underlying Arrow table. As a result, the Ray Dataset has only one block corresponding to this Arrow table.",used support streaming due archive format recent call last file line module file line return file line file line file line return file line extract file line return function file line raise extraction protocol tar like streaming mode please use instead fall back implementation underlying arrow table result ray one block corresponding arrow table,issue,positive,negative,neutral,neutral,negative,negative
1720312978,"Regarding naming: I tried my best, so if you guys have suggestions on names I'll change it, otherwise we can always do it as a follow-up. For implementation unification: right now `SetIdleResource` is private, so I added the public one specifically for non-resource artifacts. But we can also unify and make public, don't have an opinion here either.",regarding naming tried best change otherwise always implementation unification right private added public one specifically also unify make public opinion either,issue,positive,positive,positive,positive,positive,positive
1720289349,Going to rebase and rerun release tests/microbenchmarks,going rebase rerun release,issue,negative,neutral,neutral,neutral,neutral,neutral
1720234558,"Is this on one machine with 8 GPUs, or 8 machines with 1 GPU (or something else)?

Which kind of GPUs are you using?",one machine something else kind,issue,positive,positive,positive,positive,positive,positive
1720119528,"It looks like https://github.com/ray-project/ray/pull/29192 attempts to fix a similar issue and adds a unit test related to this issue. But in https://github.com/ray-project/ray/pull/38640, we remove this test as we deprecate the `preprocessor` arg.",like fix similar issue unit test related issue remove test deprecate,issue,negative,neutral,neutral,neutral,neutral,neutral
1720106013,"It looks like this issue also occurs without the use of preprocessors. Based off the reproducible given above, I further narrowed it down into a runnable example:

```
import ray
from ray.data.context import DataContext
from ray.data.preprocessors import BatchMapper, StandardScaler
from ray.train.torch import TorchTrainer
from ray.air.config import ScalingConfig


ctx = DataContext.get_current()
ctx.enable_tensor_extension_casting = False

def train_loop_per_worker():
    # enable_tensor_extension_casting works here but not outside train_loop_per_worker
    # ctx = DataContext.get_current()
    # ctx.enable_tensor_extension_casting = False
    ctx_worker = DataContext.get_current()
    assert ctx_worker.enable_tensor_extension_casting is False

trainer = TorchTrainer(
    train_loop_per_worker=train_loop_per_worker,
    datasets={""train"": ray.data.range(10)},
    scaling_config=ScalingConfig(num_workers=1),
)
result = trainer.fit()
```",like issue also without use based reproducible given runnable example import ray import import import import false work outside false assert false trainer train result,issue,negative,negative,negative,negative,negative,negative
1720077407,This is due to grpc upgrade. We will handle it differently ,due upgrade handle differently,issue,negative,negative,neutral,neutral,negative,negative
1720044604,"I agree, retry sounds good in the short term.

If ""ray stop"" fails to shut down all nodes, I still think it makes more sense for `ray down` to return a nonzero error code and also print a warning (maybe suggesting to rerun `ray down`), rather than to just return zero and print the warning.  I feel like if `ray down` succeeds the user should have confidence that the cluster was shut down.

I'm not sure why SIGSEGV is being raised and I wonder if that's a bug.  ",agree retry good short term ray stop shut still think sense ray return nonzero error code also print warning maybe suggesting rerun ray rather return zero print warning feel like ray user confidence cluster shut sure raised wonder bug,issue,positive,positive,positive,positive,positive,positive
1720030477,I think the fix is in already. Let's manually run that test?,think fix already let manually run test,issue,negative,neutral,neutral,neutral,neutral,neutral
1719929325,"So it seems if a command exits non zero on the remote node (e.g. when ray stop fails to gracefully shutdown things) - we will fail the command, leading to SIGSEGV as shown here : https://buildkite.com/ray-project/release-tests-pr/builds/53226#018a8fbe-572c-4d61-b5f7-f0bdde801690
```
py:360 -- \x1b[33mIf you experience issues with the cloud provider, try re-running the command with \x1b[1m--no-config-cache\x1b[22m\x1b[26m.\x1b[39m\n2023-09-13 11:28:42,463\tINFO commands.py:440 -- Destroying cluster. \x1b[4mConfirm [y/N]:\x1b[24m y \x1b[2m[automatic, due to --yes]\x1b[22m\n2023-09-13 11:28:43,316\tINFO command_runner.py:204 -- \x1b[37mFetched IP\x1b[39m: \x1b[1m35.227.137.116\x1b[22m\n2023-09-13 11:28:43,316\tVINFO command_runner.py:371 -- Running `\x1b[1mdocker exec -it  ray_container /bin/bash -c \'bash --login -c -i \'""\'""\'source ~/.bashrc; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (ray stop)\'""\'""\'\' \x1b[22m\x1b[26m`\n1/1 stopped.\r1/4 stopped.\r2/4 stopped.\r3/4 stopped.\r4/4 stopped.\r\x1b[33mStopped only 5 out of 6 Ray processes within the grace period 16 seconds. Set `\x1b[1m-v\x1b[22m\x1b[33m` to see more details. Remaining processes [psutil.Process(pid=192, name=\'gcs_server\', status=\'terminated\', started=\'11:25:00\')] will be forcefully terminated.\x1b[39m\r\n\x1b[33mYou can also use `\x1b[1m--force\x1b[22m\x1b[33m` to forcefully terminate processes or set higher `--grace-period` to wait longer time for proper termination.\x1b[39m\r\n\x1b[0m'
Traceback (most recent call last):
  File ""launch_and_verify_cluster.py"", line 336, in <module>
    run_ray_commands(cluster_config, retries, no_config_cache, num_expected_nodes)
  File ""launch_and_verify_cluster.py"", line 251, in run_ray_commands
    cleanup_cluster(cluster_config)
  File ""launch_and_verify_cluster.py"", line 180, in cleanup_cluster
    raise e
  File ""launch_and_verify_cluster.py"", line 173, in cleanup_cluster
    subprocess.run(
  File ""/home/ray/anaconda3/lib/python3.8/subprocess.py"", line 516, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['ray', 'down', '-v', '-y', '/tmp/tmpao_rgdsy.yaml']' died with <Signals.SIGSEGV: 11>.
```


Retry usually works. Thoughts on how to handle this? @architkulkarni 

I think in the long run, we should not fail this bad if the `ray stop` exits non-zero on the remote cluster, but prints warnings or retries. ",command non zero remote node ray stop gracefully shutdown fail command leading shown experience cloud provider try command cluster automatic due yes running login export ray stop ray within grace period set see forcefully also use forcefully terminate set higher wait longer time proper recent call last file line module file line file line raise file line file line run raise command retry usually work handle think long run fail bad ray stop remote cluster,issue,negative,negative,negative,negative,negative,negative
1719875980,"Sorry, this is still not done yet - the impact is minimal, I believe it's a stacktrace when a core worker was shutdown forcefully so it wasn't prioritized. But we should fix it ASAP I agree - i just put up a quick patch here: https://github.com/ray-project/ray/pull/39663

Or if you are seeing this showing up in other unexpected cases?",sorry still done yet impact minimal believe core worker shutdown forcefully fix agree put quick patch seeing showing unexpected,issue,negative,negative,neutral,neutral,negative,negative
1719838257,Core nightly release test: https://buildkite.com/ray-project/release-tests-pr/builds/53291,core nightly release test,issue,negative,neutral,neutral,neutral,neutral,neutral
1719637883,"Yep, doing some explicit load shedding makes sense as well.

> Splitting I/O handling b/w multiple loops is actually tricky -- since we don't control where callbacks are called (ie await's are continued), it would be tricky to separate it on per component basis.

Not sure what you mean by this -- you can't await coroutines across threads/loops. The proposal is to run all of the handle/router code on its own asyncio loop and use `run_coroutine_threadsafe` when running the assignment task.",yep explicit load shedding sense well splitting handling multiple actually tricky since control ie await continued would tricky separate per component basis sure mean ca await across proposal run code loop use running assignment task,issue,positive,positive,neutral,neutral,positive,positive
1719348223,"@LorrinWWW I have found the cause of the problem, I have not started the runtime of ray cluster server，just type input these commands like this:
```shell
# start the ray runtime
ray start --head --port port_number

# add the node to this ray cluster
ray start --address='10.104.8.83:port_number'

# the setting of below host is different from 10.104.8.83 above-mentioned
python -m vllm.entrypoints.api_server \
        --host=$host \
        --port=$port \
        --model=$model \
        --tokenizer=$tokenizer  \
        --tensor-parallel-size=$tensor_parallel_size

# terminate the ray runtime when not in use
ray stop
```",found cause problem ray cluster type input like shell start ray ray start head port add node ray cluster ray start setting host different python host port model terminate ray use ray stop,issue,negative,neutral,neutral,neutral,neutral,neutral
1719313320,"@edcxan, thanks for opening this issue. This is a good one :)
The broader take here should be, ioo:

- Move PPO from RolloutWorker/Policy API to the new EnvRunner (replaces RolloutWorker) + MARLModule (replaces PolicyMap) APIs. See DreamerV3's (albeit single-agent only) EnvRunner under `algorithms.dreamerv3.utils.env_runner.py`
- Make the new EnvRunner:
** Use gymnasium.vector as the environment API (get rid of RLlib's own quirky Env APIs; again, use DreamerV3 as example)
** Use Connectors directly in the EnvRunner (<- this could be a phase II).
** Use DreamerV3's Episode class to store data temporarily (this makes data easily accessible by EnvRunner for compute action calls: forward_exploration/inference())
** Pass data from ongoing Episode through Connectors and into RLModules for action computation.
** The user might configure a custom function that allows them to extract the ""correct"" data from the Episode given some timestep. This way, we can solve (and get rid of) the conundrum of the TrajectoryViewAPI via a simpler yet more powerful functional API. For example, should the user know that her model requires the last 10 rewards besides the observation, she can write a custom function to extract those data from the ongoing Episode object (and use 0-padding or any other solution for episode-edge cases). (<- this could be phase II)
** The same happens on the way back to the env: EnvRunner will use the EnvConnector to pass the computed action back to the environment.
** Maybe: Should the module return something from its get_internal_state() method, the EnvRunner might automatically handle RNN-state passing into the module's forward methods as well as storing the most recent state for the next call. Again, see DreamerV3's EnvRunner for a working example of such behavior. (<- this could be phase II; phase I w/o LSTM support)",thanks opening issue good one take move new see albeit make new use environment get rid quirky use example use directly could phase use episode class store data temporarily data easily accessible compute action pas data ongoing episode action computation user might configure custom function extract correct data episode given way solve get rid conundrum via simpler yet powerful functional example user know model last besides observation write custom function extract data ongoing episode object use solution could phase way back use pas action back environment maybe module return something method might automatically handle passing module forward well recent state next call see working example behavior could phase phase support,issue,positive,positive,positive,positive,positive,positive
1719280386,This is still there in 2.6. Is this gonna be solved soon?,still gon na soon,issue,negative,neutral,neutral,neutral,neutral,neutral
1718978371,"A small addition - the crash occurs when executing the algo.train() line, config.build completes without errors.",small addition crash line without,issue,negative,negative,negative,negative,negative,negative
1718783433,"Nice! I [tried](https://github.com/ray-project/ray/pull/39448) this last week as well but did not follow up, thanks for creating an issue to track this. 😄 ",nice tried last week well follow thanks issue track,issue,positive,positive,positive,positive,positive,positive
1718775631,"hi @harborn,
thanks again for submitting this PR to support intel accelerators in Ray. Given multiple teams are trying to add support of different accelerators to Ray. We'd like to come up with a design first to see how we should support them in a unified way [holistic design](https://github.com/ray-project/ray/issues/38504) in Ray 2.8, which will be started after ray summit (9/20).

In the mean time, can you share a more information how Intel GPU/Accelerator can be used with other libraries like pytorch? Thanks",hi thanks support ray given multiple trying add support different ray like come design first see support unified way holistic design ray ray summit mean time share information used like thanks,issue,positive,positive,neutral,neutral,positive,positive
1718765593,"@architkulkarni @kevin85421 
Thanks for the review! I checked it via the link and the doc looks good now.",thanks review checked via link doc good,issue,positive,positive,positive,positive,positive,positive
1718735322,We took another pass this week and this should be done from the Train/Tune perspective for 2.7. ,took another pas week done perspective,issue,negative,neutral,neutral,neutral,neutral,neutral
1718676747,"So we still need to handle (rather than reject) ray syncer COMMAND messages. These are the messages from NodeManager  sending potential resources deadlocks:

https://github.com/ray-project/ray/blob/c7b352eb6b149668ceb2cd8f78dd9715337cd9e2/src/ray/raylet/node_manager.cc#L2764-L2769


We could have had `RESOURCE_VIEW` carrying this bit of information from resource reports (i.e. here in [LocalResourceManager](https://github.com/ray-project/ray/blob/2522aa47783b4c045290a09d16d8140be97ca687/src/ray/raylet/scheduling/local_resource_manager.cc#L279-L281) ) . However, this message is stopped being sent when there's no available resources updates, which happens when resources deadlock happens. Therefore, it's not easy to piggyback the bit of info in RESOURCE_VIEW message. 


Thus, this PR still accepts both COMMANDS and RESOURCE_VIEW messages, and do the conditional updates depending on where the messages are from. ",still need handle rather reject ray command sending potential could carrying bit information resource however message stopped sent available deadlock therefore easy bit message thus still conditional depending,issue,negative,positive,positive,positive,positive,positive
1718644886,"Sound good! Once the fix is in, you can simply close this issue to unjail the test

Alternatively, you can run this test on that PR that fixes it to confirm first",sound good fix simply close issue test alternatively run test confirm first,issue,negative,positive,positive,positive,positive,positive
1718490172,"> Introduce a backoff policy for this timeout. This could be shared with the existing backoff for selecting replicas or could be a separate backoff policy only for the case where these timeouts occur.

There are a few ways we can do load-shedding in this case, but i think more importantly we actually should start doing the _shedding_ part, meaning that after we exhausted N retries, Proxy should just fail the request and give way to the other ones (as opposed to retrying indefinitely)

> Always run the router in a separate thread with its own asyncio loop. This will help in the case where the asyncio loop is clogged, but we may still see the issue under heavy CPU contention. This would also remove the difference between the ""sync"" and ""async"" handles case.

Ideally, all I/O alas request/response handling should happen on a single evloop, anything else have to be offloaded. 

Splitting I/O handling b/w multiple loops is actually tricky -- since we don't control where callbacks are called (ie await's are continued), it would be tricky to separate it on per component basis.",introduce policy could could separate policy case occur way case think importantly actually start part meaning exhausted proxy fail request give way opposed indefinitely always run router separate thread loop help case loop may still see issue heavy contention would also remove difference sync case ideally ala handling happen single anything else splitting handling multiple actually tricky since control ie await continued would tricky separate per component basis,issue,negative,negative,neutral,neutral,negative,negative
1718451498,merge conflict + update the description ,merge conflict update description,issue,negative,neutral,neutral,neutral,neutral,neutral
1718441254,"Added e2e integration tests, which failed consistently on 2.7 but pass here with PR. ",added integration consistently pas,issue,negative,positive,positive,positive,positive,positive
1718410980,"To me, this is a pretty critical issue. We need a solid foundation, conceptually and functionally, for such a foundational feature.

@akshay-anyscale @architkulkarni 
cc: @rkooo567 @rickyyx @richardliaw ",pretty critical issue need solid foundation conceptually functionally foundational feature,issue,negative,positive,neutral,neutral,positive,positive
1718406787,not needed anymore. fixed in the master by setting longer min backoff time,fixed master setting longer min time,issue,negative,positive,neutral,neutral,positive,positive
1718342406,I am trying to add a quick test for this. ,trying add quick test,issue,negative,positive,positive,positive,positive,positive
1718296251,"Empirically, the router seemed to be able to proxy only about 30 streaming requests/responses before maxing out its CPU. This is much lower than expected; we should be able to handle at least a few hundred concurrent requests (similar to max throughput of a single HTTP proxy).",router able proxy streaming much lower able handle least hundred concurrent similar throughput single proxy,issue,negative,positive,positive,positive,positive,positive
1718196550,"For now, let's go without LSTM. Adding support for offline recurrent networks needs a little bit of design decision making and will impact other algorithms, too.",let go without support recurrent need little bit design decision making impact,issue,negative,negative,negative,negative,negative,negative
1718187564,"```
multi_client_put_gigabytes: noise
avg_pg_remove_time_ms: not perf senstivie
1_n_async_actor_calls_async: Regression
107374182400_large_object_time: regression
1_n_actor_calls_async: noise
1_1_async_actor_calls_sync: regression
n_n_actor_calls_async: regression
actors_per_second: jiajun's result is within noise range, gene's result seems regression
1_1_actor_calls_sync: noise
1_1_actor_calls_concurrent: noise
single_client_tasks_async: noise
single_client_tasks_and_get_batch: noise
1_1_actor_calls_async: minor regression
```

So generally actor related perf seems to be regressed
```
1_1_async_actor_calls_sync: regression
n_n_actor_calls_async: regression
1_n_async_actor_calls_async: Regression
107374182400_large_object_time: regression
many_actors
```",noise regression regression noise regression regression result within noise range gene result regression noise noise noise noise minor regression generally actor related regression regression regression regression,issue,negative,positive,neutral,neutral,positive,positive
1718151167,"Result from @jjyao 
```
REGRESSION 435.58%: dashboard_p95_latency_ms (LATENCY) regresses from 1641.087 to 8789.363 (435.58%) in 2.7.0/benchmarks/many_actors.json
REGRESSION 230.98%: dashboard_p50_latency_ms (LATENCY) regresses from 13.316 to 44.073 (230.98%) in 2.7.0/benchmarks/many_actors.json
REGRESSION 121.89%: dashboard_p99_latency_ms (LATENCY) regresses from 3961.132 to 8789.363 (121.89%) in 2.7.0/benchmarks/many_actors.json
REGRESSION 40.50%: dashboard_p99_latency_ms (LATENCY) regresses from 7303.73 to 10261.603 (40.50%) in 2.7.0/benchmarks/many_tasks.json
REGRESSION 21.00%: dashboard_p95_latency_ms (LATENCY) regresses from 5662.863 to 6852.209 (21.00%) in 2.7.0/benchmarks/many_tasks.json
REGRESSION 18.81%: multi_client_put_gigabytes (THROUGHPUT) regresses from 38.350117983138134 to 31.135548542527147 (18.81%) in 2.7.0/microbenchmark.json
REGRESSION 16.64%: avg_pg_remove_time_ms (LATENCY) regresses from 0.7933515765763139 to 0.9253939984977227 (16.64%) in 2.7.0/stress_tests/stress_test_placement_group.json
REGRESSION 15.08%: dashboard_p50_latency_ms (LATENCY) regresses from 2.606 to 2.999 (15.08%) in 2.7.0/benchmarks/many_pgs.json
REGRESSION 14.16%: dashboard_p99_latency_ms (LATENCY) regresses from 150.641 to 171.97 (14.16%) in 2.7.0/benchmarks/many_pgs.json
REGRESSION 9.75%: 1_n_async_actor_calls_async (THROUGHPUT) regresses from 9607.6917020388 to 8670.987449817852 (9.75%) in 2.7.0/microbenchmark.json
REGRESSION 8.74%: 107374182400_large_object_time (LATENCY) regresses from 33.47957800099999 to 36.40618179300009 (8.74%) in 2.7.0/scalability/single_node.json
REGRESSION 8.72%: 1_n_actor_calls_async (THROUGHPUT) regresses from 11016.813113385808 to 10055.705225850837 (8.72%) in 2.7.0/microbenchmark.json
REGRESSION 7.97%: 1_1_async_actor_calls_sync (THROUGHPUT) regresses from 1519.8915131779668 to 1398.6806497488678 (7.97%) in 2.7.0/microbenchmark.json
REGRESSION 7.65%: n_n_actor_calls_async (THROUGHPUT) regresses from 32688.224759117064 to 30188.85393239531 (7.65%) in 2.7.0/microbenchmark.json
REGRESSION 7.52%: client__tasks_and_get_batch (THROUGHPUT) regresses from 0.9848332918374623 to 0.9107994380482458 (7.52%) in 2.7.0/microbenchmark.json
REGRESSION 7.17%: actors_per_second (THROUGHPUT) regresses from 848.4535001176805 to 787.6058231060754 (7.17%) in 2.7.0/benchmarks/many_actors.json
REGRESSION 6.57%: 1_1_actor_calls_sync (THROUGHPUT) regresses from 2527.884786760515 to 2361.7595574443153 (6.57%) in 2.7.0/microbenchmark.json
REGRESSION 6.32%: 1_1_actor_calls_concurrent (THROUGHPUT) regresses from 5038.298026762205 to 4719.981319913378 (6.32%) in 2.7.0/microbenchmark.json
REGRESSION 6.23%: single_client_tasks_async (THROUGHPUT) regresses from 10940.075908384719 to 10258.366848638743 (6.23%) in 2.7.0/microbenchmark.json
REGRESSION 6.08%: single_client_tasks_and_get_batch (THROUGHPUT) regresses from 10.988766549840475 to 10.32109253003945 (6.08%) in 2.7.0/microbenchmark.json
REGRESSION 5.73%: 1_1_actor_calls_async (THROUGHPUT) regresses from 8232.577421386566 to 7761.144908286035 (5.73%) in 2.7.0/microbenchmark.json
REGRESSION 4.95%: n_n_async_actor_calls_async (THROUGHPUT) regresses from 25874.5792020397 to 24594.09376839356 (4.95%) in 2.7.0/microbenchmark.json
REGRESSION 4.84%: single_client_tasks_sync (THROUGHPUT) regresses from 1329.0736290293253 to 1264.8020726657614 (4.84%) in 2.7.0/microbenchmark.json
REGRESSION 4.45%: time_to_broadcast_1073741824_bytes_to_50_nodes (LATENCY) regresses from 75.45187580800001 to 78.80815782399998 (4.45%) in 2.7.0/scalability/object_store.json
REGRESSION 3.56%: dashboard_p50_latency_ms (LATENCY) regresses from 5.529 to 5.726 (3.56%) in 2.7.0/benchmarks/many_tasks.json
REGRESSION 3.30%: avg_pg_create_time_ms (LATENCY) regresses from 0.871186292793918 to 0.8999087462482861 (3.30%) in 2.7.0/stress_tests/stress_test_placement_group.json
REGRESSION 2.43%: 10000_args_time (LATENCY) regresses from 16.737809073999998 to 17.145098938000046 (2.43%) in 2.7.0/scalability/single_node.json
REGRESSION 1.49%: client__1_1_actor_calls_concurrent (THROUGHPUT) regresses from 967.4680417367294 to 953.0283995915531 (1.49%) in 2.7.0/microbenchmark.json
REGRESSION 1.37%: client__1_1_actor_calls_sync (THROUGHPUT) regresses from 508.5752348554388 to 501.6082229033316 (1.37%) in 2.7.0/microbenchmark.json
REGRESSION 1.24%: multi_client_put_calls_Plasma_Store (THROUGHPUT) regresses from 12887.724106896032 to 12728.227663400154 (1.24%) in 2.7.0/microbenchmark.json
REGRESSION 0.76%: 3000_returns_time (LATENCY) regresses from 5.809550247000004 to 5.853592727000091 (0.76%) in 2.7.0/scalability/single_node.json
REGRESSION 0.72%: stage_1_avg_iteration_time (LATENCY) regresses from 22.907386016845702 to 23.07194697856903 (0.72%) in 2.7.0/stress_tests/stress_test_many_tasks.json
REGRESSION 0.04%: stage_3_time (LATENCY) regresses from 2753.045879125595 to 2754.156331062317 (0.04%) in 2.7.0/stress_tests/stress_test_many_tasks.json
```",result regression latency regression latency regression latency regression latency regression latency regression throughput regression latency regression latency regression latency regression throughput regression latency regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression latency regression latency regression latency regression latency regression throughput regression throughput regression throughput regression latency regression latency regression latency,issue,negative,neutral,neutral,neutral,neutral,neutral
1718093018,"I agree that the non-gpu learning test is not a good place, but instead, there should be another place where we execute this test. After all, if we can't prove learning on it, it should not be referred to as a fine-tuned example",agree learning test good place instead another place execute test ca prove learning example,issue,positive,positive,positive,positive,positive,positive
1718090816,"@sven1977 I was the one who added the test. The reason we stumbled across this issue was that we don't test this code today - the issue was raised by a user who read the docs. In fact, all of our fine-tuned examples listed under `doc/source/rllib/rllib-algorithms.rst` are not tested.
Imho, they should be release tests given that RLlib's most used Algorithm is PPO by a good margin and that we refer to these as examples of it in our documentation.",one added test reason across issue test code today issue raised user read fact listed tested release given used algorithm good margin refer documentation,issue,negative,positive,positive,positive,positive,positive
1718087059,Thanks! Is there a timeline for next release that we should be aware of?,thanks next release aware,issue,negative,positive,positive,positive,positive,positive
1718079260,"as far as the connectors, we hypothesize that you save/serialize the list of connectors in a certain order, and then when the checkpoint is unserialized, those connectors keep the same order, but then the obs space for the Dict space changes key order. so the connectors try to put obs in the wrong spot leading to the above error",far hypothesize list certain order keep order space space key order try put wrong spot leading error,issue,negative,negative,neutral,neutral,negative,negative
1718064717,"script leading to error (pre PR fix obviously):
```python
from ray.rllib.algorithms.ppo import PPOConfig
from ray.rllib.policy import Policy
from ray.rllib.utils.policy import local_policy_inference
import gymnasium

from gymnasium.spaces import Dict, Box, Discrete
from collections import OrderedDict
import os
import tempfile

class OrderedObsEnv(gymnasium.Env):

    def __init__(self, config):
        self.action_space = Box(-1.0, 1.0, (2,))
        self.observation_space = Dict(
            OrderedDict(
                {
                    ""b_obs"": Discrete(n=1),
                    ""a_obs"": Box(0.0, 1.0, ),
                }
            ),
        )
        self.max_episode_length = 20

    def reset(self, seed=None, options=None):
        self.steps = 0
        return self.observation_space.sample(), {}

    def step(self, action):
        reward = 0.0
        done = truncated = self.steps >=  self.max_episode_length
        return self.observation_space.sample(), reward, done, truncated, {}


def create_checkpoint(tmpdir):

    config = (
        PPOConfig()
        .environment(OrderedObsEnv)
    )
    algo = config.build()
    algo.save(checkpoint_dir=tmpdir)

def run(checkpoint_path, policy_id):

    policy = Policy.from_checkpoint(
        checkpoint=checkpoint_path,
        policy_ids=[policy_id]
    )

    env = OrderedObsEnv({})
    obs, _ = env.reset()
    terminated = truncated = False
    step = 0

    while not terminated and not truncated:
        step +=1
        policy_outputs = local_policy_inference(
            policy,
            ""env1"",
            ""agent_0"",
            obs,
            explore=False
        )
        action, _ ,_ = policy_outputs[0]
        print(f'actions: {action}')

if __name__ == ""__main__"":
    with tempfile.TemporaryDirectory() as tmpdir:
        policy_id = ""default_policy""
        create_checkpoint(tmpdir)
        policy_path = os.path.join(
            tmpdir,
            ""checkpoint_000000"",
            ""policies"",
            policy_id)

        run(policy_path, policy_id)
```

leads to error message
```
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/opt/conda/lib/python3.10/runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""/home/user/.vscode-server/extensions/ms-python.python-2023.14.0/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py"", line 39, in <module>
    cli.main()
  File ""/home/user/.vscode-server/extensions/ms-python.python-2023.14.0/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 430, in main
    run()
  File ""/home/user/.vscode-server/extensions/ms-python.python-2023.14.0/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 284, in run_file
    runpy.run_path(target, run_name=""__main__"")
  File ""/home/user/.vscode-server/extensions/ms-python.python-2023.14.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 321, in run_path
    return _run_module_code(code, init_globals, run_name,
  File ""/home/user/.vscode-server/extensions/ms-python.python-2023.14.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 135, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File ""/home/user/.vscode-server/extensions/ms-python.python-2023.14.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 124, in _run_code
    exec(code, run_globals)
  File ""/opt/project/scripts/ordered_fail.py"", line 78, in <module>
    run(policy_path, policy_id)
  File ""/opt/project/scripts/ordered_fail.py"", line 58, in run
    policy_outputs = local_policy_inference(
  File ""/opt/conda/lib/python3.10/site-packages/ray/rllib/utils/policy.py"", line 252, in local_policy_inference
    ac_outputs: List[AgentConnectorsOutput] = policy.agent_connectors(acd_list)
  File ""/opt/conda/lib/python3.10/site-packages/ray/rllib/connectors/agent/pipeline.py"", line 41, in __call__
    ret = c(ret)
  File ""/opt/conda/lib/python3.10/site-packages/ray/rllib/connectors/connector.py"", line 254, in __call__
    return [self.transform(d) for d in acd_list]
  File ""/opt/conda/lib/python3.10/site-packages/ray/rllib/connectors/connector.py"", line 254, in <listcomp>
    return [self.transform(d) for d in acd_list]
  File ""/opt/conda/lib/python3.10/site-packages/ray/rllib/connectors/agent/obs_preproc.py"", line 58, in transform
    d[SampleBatch.NEXT_OBS] = self._preprocessor.transform(
  File ""/opt/conda/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py"", line 319, in transform
    self.write(observation, array, 0)
  File ""/opt/conda/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py"", line 331, in write
    p.write(o, array, offset)
  File ""/opt/conda/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py"", line 195, in write
    array[offset : offset + self.size] = self.transform(observation)
  File ""/opt/conda/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py"", line 190, in transform
    self.check_shape(observation)
  File ""/opt/conda/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py"", line 74, in check_shape
    raise ValueError(
ValueError: Observation ([0.89433795] dtype=None) outside given space (Discrete(1))!
```",script leading error fix obviously python import import policy import import gymnasium import box discrete import import o import class self box discrete box reset self return step self action reward done truncated return reward done truncated run policy truncated false step truncated step policy action print action run error message recent call last file line return code none file line code file line module file line main run file line target file line return code file line code file line code file line module run file line run file line list file line ret ret file line return file line return file line transform file line transform observation array file line write array offset file line write array offset offset observation file line transform observation file line raise observation outside given space discrete,issue,negative,positive,neutral,neutral,positive,positive
1718055267,Confirmed it's a bug. we should fix it in the next release.,confirmed bug fix next release,issue,negative,positive,positive,positive,positive,positive
1718047331,"```
(ray) gene@geneanycale2023 release_logs % ./compare_perf_metrics 2.6.3 2.7.0 | sort -nr -k2
REGRESSION 680.41%: dashboard_p50_latency_ms (LATENCY) regresses from 13.316 to 103.92 (680.41%) in 2.7.0/benchmarks/many_actors.json
REGRESSION 517.09%: dashboard_p95_latency_ms (LATENCY) regresses from 1641.087 to 10126.955 (517.09%) in 2.7.0/benchmarks/many_actors.json
REGRESSION 155.66%: dashboard_p99_latency_ms (LATENCY) regresses from 3961.132 to 10126.955 (155.66%) in 2.7.0/benchmarks/many_actors.json
REGRESSION 32.71%: dashboard_p95_latency_ms (LATENCY) regresses from 5662.863 to 7515.171 (32.71%) in 2.7.0/benchmarks/many_tasks.json
REGRESSION 32.13%: dashboard_p99_latency_ms (LATENCY) regresses from 7303.73 to 9650.509 (32.13%) in 2.7.0/benchmarks/many_tasks.json
REGRESSION 22.06%: multi_client_put_gigabytes (THROUGHPUT) regresses from 38.350117983138134 to 29.888601798958504 (22.06%) in 2.7.0/microbenchmark.json
REGRESSION 19.10%: time_to_broadcast_1073741824_bytes_to_50_nodes (LATENCY) regresses from 75.45187580800001 to 89.864582848 (19.10%) in 2.7.0/scalability/object_store.json
REGRESSION 18.36%: actors_per_second (THROUGHPUT) regresses from 848.4535001176805 to 692.654567404582 (18.36%) in 2.7.0/benchmarks/many_actors.json
REGRESSION 14.92%: stage_0_time (LATENCY) regresses from 9.37568211555481 to 10.77467966079712 (14.92%) in 2.7.0/stress_tests/stress_test_many_tasks.json
REGRESSION 11.20%: n_n_actor_calls_async (THROUGHPUT) regresses from 32688.224759117064 to 29027.616979649276 (11.20%) in 2.7.0/microbenchmark.json
REGRESSION 10.62%: 1_n_actor_calls_async (THROUGHPUT) regresses from 11016.813113385808 to 9846.33856976487 (10.62%) in 2.7.0/microbenchmark.json
REGRESSION 10.49%: single_client_tasks_async (THROUGHPUT) regresses from 10940.075908384719 to 9792.802505790358 (10.49%) in 2.7.0/microbenchmark.json
REGRESSION 9.75%: 1_1_actor_calls_sync (THROUGHPUT) regresses from 2527.884786760515 to 2281.3341043774667 (9.75%) in 2.7.0/microbenchmark.json
REGRESSION 9.37%: dashboard_p50_latency_ms (LATENCY) regresses from 5.529 to 6.047 (9.37%) in 2.7.0/benchmarks/many_tasks.json
REGRESSION 9.25%: 1_1_async_actor_calls_sync (THROUGHPUT) regresses from 1519.8915131779668 to 1379.2888062868067 (9.25%) in 2.7.0/microbenchmark.json
REGRESSION 8.41%: 1_n_async_actor_calls_async (THROUGHPUT) regresses from 9607.6917020388 to 8799.332862899606 (8.41%) in 2.7.0/microbenchmark.json
REGRESSION 7.83%: stage_3_time (LATENCY) regresses from 2753.045879125595 to 2968.5303938388824 (7.83%) in 2.7.0/stress_tests/stress_test_many_tasks.json
REGRESSION 7.65%: avg_pg_create_time_ms (LATENCY) regresses from 0.871186292793918 to 0.93787518919019 (7.65%) in 2.7.0/stress_tests/stress_test_placement_group.json
REGRESSION 7.19%: single_client_tasks_and_get_batch (THROUGHPUT) regresses from 10.988766549840475 to 10.198204154394244 (7.19%) in 2.7.0/microbenchmark.json
REGRESSION 6.89%: avg_pg_remove_time_ms (LATENCY) regresses from 0.7933515765763139 to 0.8479905540542819 (6.89%) in 2.7.0/stress_tests/stress_test_placement_group.json
REGRESSION 5.73%: n_n_async_actor_calls_async (THROUGHPUT) regresses from 25874.5792020397 to 24391.40712750794 (5.73%) in 2.7.0/microbenchmark.json
REGRESSION 5.21%: single_client_tasks_sync (THROUGHPUT) regresses from 1329.0736290293253 to 1259.8947657007097 (5.21%) in 2.7.0/microbenchmark.json
REGRESSION 4.64%: 10000_args_time (LATENCY) regresses from 16.737809073999998 to 17.514890280000003 (4.64%) in 2.7.0/scalability/single_node.json
REGRESSION 2.88%: 1_1_actor_calls_concurrent (THROUGHPUT) regresses from 5038.298026762205 to 4893.00631457457 (2.88%) in 2.7.0/microbenchmark.json
REGRESSION 2.51%: multi_client_put_calls_Plasma_Store (THROUGHPUT) regresses from 12887.724106896032 to 12563.742075254824 (2.51%) in 2.7.0/microbenchmark.json
REGRESSION 2.38%: stage_1_avg_iteration_time (LATENCY) regresses from 22.907386016845702 to 23.45233299732208 (2.38%) in 2.7.0/stress_tests/stress_test_many_tasks.json
REGRESSION 2.24%: single_client_get_object_containing_10k_refs (THROUGHPUT) regresses from 13.564180981457046 to 13.25978280552952 (2.24%) in 2.7.0/microbenchmark.json
REGRESSION 2.05%: client__put_gigabytes (THROUGHPUT) regresses from 0.13393152379385806 to 0.1311833215458485 (2.05%) in 2.7.0/microbenchmark.json
REGRESSION 1.92%: 1000000_queued_time (LATENCY) regresses from 179.73244193300002 to 183.17572256600002 (1.92%) in 2.7.0/scalability/single_node.json
REGRESSION 1.90%: 107374182400_large_object_time (LATENCY) regresses from 33.47957800099999 to 34.117361394 (1.90%) in 2.7.0/scalability/single_node.json
REGRESSION 1.84%: 1_1_actor_calls_async (THROUGHPUT) regresses from 8232.577421386566 to 8081.205383010151 (1.84%) in 2.7.0/microbenchmark.json
REGRESSION 1.80%: client__put_calls (THROUGHPUT) regresses from 838.973365004957 to 823.8658610431164 (1.80%) in 2.7.0/microbenchmark.json
REGRESSION 1.62%: stage_2_avg_iteration_time (LATENCY) regresses from 62.84401955604553 to 63.864473247528075 (1.62%) in 2.7.0/stress_tests/stress_test_many_tasks.json
REGRESSION 0.46%: client__tasks_and_get_batch (THROUGHPUT) regresses from 0.9848332918374623 to 0.9802608247694551 (0.46%) in 2.7.0/microbenchmark.json
REGRESSION 0.35%: 1_1_async_actor_calls_with_args_async (THROUGHPUT) regresses from 2071.4152029622965 to 2064.1280128247445 (0.35%) in 2.7.0/microbenchmark.json
REGRESSION 0.22%: client__get_calls (THROUGHPUT) regresses from 1157.7462987111426 to 1155.1669478708916 (0.22%) in 2.7.0/microbenchmark.json
2.7.0 does not have benchmarks/many_pgs.json
```",ray gene sort regression latency regression latency regression latency regression latency regression latency regression throughput regression latency regression throughput regression latency regression throughput regression throughput regression throughput regression throughput regression latency regression throughput regression throughput regression latency regression latency regression throughput regression latency regression throughput regression throughput regression latency regression throughput regression throughput regression latency regression throughput regression throughput regression latency regression latency regression throughput regression throughput regression latency regression throughput regression throughput regression throughput,issue,negative,neutral,neutral,neutral,neutral,neutral
1717982290,"> The last round of changes looks good! One question is that now we're allowing a user to set a `working_dir` to a remote .whl file. Even though it technically makes sense because a whl file happens to be a zip of a directory, I think it might be confusing for users, so it would be better to just disallow it. What do you think? We can always add it later if users ask for it.
> 
> Also, please update the documentation in this PR to reflect the change. It's on this page https://docs.ray.io/en/latest/ray-core/handling-dependencies.html

Okk make sense, I didn't check before that local whl is not supported for `working_dir` as well. So I agree with disallowing it as user can just fill `py_modules` field for .whl file",last round good one question user set remote file even though technically sense file zip directory think might would better disallow think always add later ask also please update documentation reflect change page make sense check local well agree user fill field file,issue,positive,positive,positive,positive,positive,positive
1717963570,"Looks like one or more tests may need to be updated, e.g.: https://buildkite.com/ray-project/oss-ci-build-pr/builds/36008#018a8c11-1593-4e10-9d3d-11694b724e24/3606-4413",like one may need,issue,negative,neutral,neutral,neutral,neutral,neutral
1717934501,"Could we revisit this issue? I think after we fix the protojson and increase the message size limit. It could pass.
@can-anyscale ",could revisit issue think fix increase message size limit could pas,issue,negative,neutral,neutral,neutral,neutral,neutral
1717900708,"the new thing seems to be failing though:
https://buildkite.com/ray-project/premerge/builds/5869#018a8ed3-4161-491e-9d4d-3c811270c3fe",new thing failing though,issue,negative,positive,positive,positive,positive,positive
1717733616,"> Remaining issues:
> 
> * Simplify code [Add global mode for ray on spark in databricks #39313 (comment)](https://github.com/ray-project/ray/pull/39313#discussion_r1323849093) ， and we don't need code in [Add global mode for ray on spark in databricks #39313 (comment)](https://github.com/ray-project/ray/pull/39313#discussion_r1323857429)
> * When global ray cluster processes crash / being killed unexpectedly, we need to clean the temp dir ""/local_disk0/tmp/ray"" dir when next global ray cluster launching, so for simplifying code, for global mode, we can only clean temp data at the start of `start_ray_node` process.
> * Shall we add a reentrant lock around `try_clean_temp_dir_at_exit` function ? To keep thread-safe because `try_clean_temp_dir_at_exit` might be triggered concurrently when `sigterm signal` and `parent died` event happen simultaneously

Checked in this notebook: https://e2-dogfood.staging.cloud.databricks.com/?o=6051921418418893#notebook/108794448140774/command/4387202915058455
No problem for global mode. temp directories are cleaned up correctly for both driver and workers. logs are collected correctly.

EDIT: another finding is that the temp folder is not cleaned up for non-global mode -- 'ray' folder created on driver not deleted (ray-xx folder is deleted); 'ray-xx' folder on workers not deleted (only 'spill' dir in side), 'ray' folder on workers also not deleted.

EDIT: Verified non-global mode works fine https://e2-dogfood.staging.cloud.databricks.com/?o=6051921418418893#notebook/4387202915058456/command/4387202915058478",simplify code add global mode ray spark comment need code add global mode ray spark comment global ray cluster crash unexpectedly need clean temp next global ray cluster code global mode clean temp data start process shall add lock around function keep might triggered concurrently signal parent event happen simultaneously checked notebook problem global mode temp correctly driver collected correctly edit another finding temp folder mode folder driver folder folder side folder also edit mode work fine,issue,positive,positive,positive,positive,positive,positive
1717718974,"@jjyao Can you check the performance regression on this? [microbenchmark.aws](https://buildkite.com/ray-project/release-tests-pr/builds/53172#018a8d85-ef8e-41ec-bdc0-35dee6a95c66)

@shrekris-anyscale Can you check the memory leak on this?",check performance regression check memory leak,issue,negative,neutral,neutral,neutral,neutral,neutral
1717698083,"There is one related LinkCheck failure:

(cluster/vms/user-guides/launching-clusters/vsphere: line   98) broken    https://raw.githubusercontent.com/ray-project/ray/master/python/ray/autoscaler/vsphere/example-vsan-file-service.yaml - 404 Client Error: Not Found for url: https://raw.githubusercontent.com/ray-project/ray/master/python/ray/autoscaler/vsphere/example-vsan-file-service.yaml

It is referencing a YAML file `example-vsan-file-service.yaml` that is newly added in this PR.",one related failure line broken client error found file newly added,issue,negative,negative,negative,negative,negative,negative
1717689045,"Awesome, thanks. Could you close this when it’s been merged? Will be on the look out for the ray release, exciting to see the work in that PR allowing for more flexibility with action spaces and such as well.",awesome thanks could close look ray release exciting see work flexibility action well,issue,positive,positive,positive,positive,positive,positive
1717586083,"Hey @simonsays1980 , thanks for opening this issue. This is a good one :)
The broader take here should be, imo:
* Move PPO from RolloutWorker/Policy API to the new EnvRunner (replaces RolloutWorker) + MARLModule (replaces PolicyMap) APIs. See DreamerV3's (albeit single-agent only) EnvRunner under `algorithms.dreamerv3.utils.env_runner.py`
* Make the new EnvRunner:
** Use gymnasium.vector as the environment API (get rid of RLlib's own quirky Env APIs; again, use DreamerV3 as example)
** Use Connectors directly in the EnvRunner (<- this could be a phase II).
** Use DreamerV3's Episode class to store data temporarily (this makes data easily accessible by EnvRunner for compute action calls: `forward_exploration/inference()`)
** Pass data from ongoing Episode through Connectors and into RLModules for action computation.
** The user might configure a custom function that allows them to extract the ""correct"" data from the Episode given some timestep. This way, we can solve (and get rid of) the conundrum of the TrajectoryViewAPI via a simpler yet more powerful functional API. For example, should the user know that her model requires the last 10 rewards besides the observation, she can write a custom function to extract those data from the ongoing Episode object (and use 0-padding or any other solution for episode-edge cases). (<- this could be phase II)
** The same happens on the way back to the env: EnvRunner will use the EnvConnector to pass the computed action back to the environment.
** Maybe: Should the module return something from its `get_internal_state()` method, the EnvRunner might automatically handle RNN-state passing into the module's forward methods as well as storing the most recent state for the next call. Again, see DreamerV3's EnvRunner for a working example of such behavior. (<- this could be phase II; phase I w/o LSTM support)
",hey thanks opening issue good one take move new see albeit make new use environment get rid quirky use example use directly could phase use episode class store data temporarily data easily accessible compute action pas data ongoing episode action computation user might configure custom function extract correct data episode given way solve get rid conundrum via simpler yet powerful functional example user know model last besides observation write custom function extract data ongoing episode object use solution could phase way back use pas action back environment maybe module return something method might automatically handle passing module forward well recent state next call see working example behavior could phase phase support,issue,positive,positive,positive,positive,positive,positive
1717415673,2.8 will be released a few weeks after the Ray Summit (which is next week). So roughly beginning to mid October.,ray summit next week roughly beginning mid,issue,negative,negative,neutral,neutral,negative,negative
1717414198,"With the above PR, this minimal example is confirmed working. After merging the PR, we can close this issue, then:
```
import ray
import supersuit as ss
from ray import tune
from ray.rllib.algorithms.ppo import PPOConfig
from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv
from ray.rllib.models import ModelCatalog
from ray.rllib.models.torch.torch_modelv2 import TorchModelV2
from ray.tune.registry import register_env
from torch import nn

from pettingzoo.butterfly import pistonball_v6


class CNNModelV2(TorchModelV2, nn.Module):
    def __init__(self, obs_space, act_space, num_outputs, *args, **kwargs):
        TorchModelV2.__init__(self, obs_space, act_space, num_outputs, *args, **kwargs)
        nn.Module.__init__(self)
        self.model = nn.Sequential(
            nn.Conv2d(3, 32, [8, 8], stride=(4, 4)),
            nn.ReLU(),
            nn.Conv2d(32, 64, [4, 4], stride=(2, 2)),
            nn.ReLU(),
            nn.Conv2d(64, 64, [3, 3], stride=(1, 1)),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(3136, 512),
            nn.ReLU(),
        )
        self.policy_fn = nn.Linear(512, num_outputs)
        self.value_fn = nn.Linear(512, 1)

    def forward(self, input_dict, state, seq_lens):
        model_out = self.model(input_dict[""obs""].permute(0, 3, 1, 2))
        self._value_out = self.value_fn(model_out)
        return self.policy_fn(model_out), state

    def value_function(self):
        return self._value_out.flatten()


def env_creator(args):
    env = pistonball_v6.parallel_env(
        n_pistons=20,
        time_penalty=-0.1,
        continuous=True,
        random_drop=True,
        random_rotate=True,
        ball_mass=0.75,
        ball_friction=0.3,
        ball_elasticity=1.5,
        max_cycles=125,
    )
    env = ss.color_reduction_v0(env, mode=""B"")
    env = ss.dtype_v0(env, ""float32"")
    env = ss.resize_v1(env, x_size=84, y_size=84)
    env = ss.normalize_obs_v0(env, env_min=0, env_max=1)
    env = ss.frame_stack_v1(env, 3)
    return env


if __name__ == ""__main__"":
    ray.init()

    env_name = ""pistonball_v6""

    register_env(env_name, lambda config: ParallelPettingZooEnv(env_creator(config)))
    ModelCatalog.register_custom_model(""CNNModelV2"", CNNModelV2)

    config = (
        PPOConfig()
        .training(
            train_batch_size=512,
            sgd_minibatch_size=256,
            num_sgd_iter=2,
        )
        .environment(env=env_name, clip_actions=True)
    )

    tune.run(
        ""PPO"",
        config=config,
    )
```",minimal example confirmed working close issue import ray import ray import tune import import import import import torch import import class self self self forward self state return state self return float return lambda,issue,negative,positive,positive,positive,positive,positive
1717400487,"This is being addressed by this (currently in-review) PR:
https://github.com/ray-project/ray/pull/39459

Will be merged very soon and be part of Ray 2.8.",currently soon part ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1717265739,"Adding argc and argv to the init method makes that warning go away, but the output is still the same: 

ray::internal::RayFunctionNotFound:  Executable function not found, the function name compute",method warning go away output still ray executable function found function name compute,issue,negative,neutral,neutral,neutral,neutral,neutral
1717197632,"PR in review: https://github.com/ray-project/ray/pull/39628

Thanks for filing this issue @kuza55 ! :)",review thanks filing issue,issue,negative,positive,positive,positive,positive,positive
1717174857,"The implementation to use Intel GPU on Ray had been refactored many times, so previous comments are outdated.
The latest implementation can be seamlessly used Intel GPU as other Nvidia GPU.
No other concepts are introduced in this PR. Nvidia GPU is GPU, Intel GPU is also GPU.
The difference is detection method:
- Nvidia GPU need CUDA, with environment variable `CUDA_VISIBLE_DEVICES`
- Intel GPU need oneAPI, with environment variable `ONEAPI_DEVICE_SELECTOR`

This PR mainly changes is to support different GPUs detection and usage method. 

@cadedaniel @scv119 @jjyao @abhilash1910 ",implementation use ray many time previous outdated latest implementation seamlessly used also difference detection method need environment variable need environment variable mainly support different detection usage method,issue,negative,positive,neutral,neutral,positive,positive
1717133743,"Understood. Any idea on what the timeline for Ray 2.8 will be? For me, this issue is quite important.

I have a project called [SustainGym](https://github.com/chrisyeh96/sustaingym) where I've made 5 RL environments related to energy / sustainability applications. 3 of the 5 environments are multi-agent RL environments following the PettingZoo API.

Currently, I include RLLib as a dependency of SustainGym, because I have special versions of my environments to fit the RLLib `MultiAgentEnv` API. In theory, SustainGym should not depend on RLLib, because a RL environment package should be agnostic to the RL algorithm. Once this pull request gets merged, I can remove the RLLib dependency and instead direct SustainGym users to simply use the RLLib wrapper around the SustainGym PettingZoo env if they want to use RLLib algorithms.",understood idea ray issue quite important project made related energy following currently include dependency special fit theory depend environment package agnostic algorithm pull request remove dependency instead direct simply use wrapper around want use,issue,positive,positive,positive,positive,positive,positive
1717119148,"@ArturNiederfahrenhorst Looking at the failed tests I think that the test is running on the wrong cluster: it requests a GPU (in the YAML) but there is none. 

Shall we add `""gpu""` to the `tags` of the Pong test?",looking think test running wrong cluster none shall add pong test,issue,negative,negative,negative,negative,negative,negative
1717091841,"Hey @chrisyeh96 , yes, it'll be part of Ray 2.8, though, and won't make it into the upcoming 2.7 anymore.",hey yes part ray though wo make upcoming,issue,negative,neutral,neutral,neutral,neutral,neutral
1717085978,"1. main.cpp follow modify:
The reason of ""No code search path found yet""  is not pass `argc` and `argv` to ray.init.
```
int main(int argc, char **argv) {
  // Start ray cluster and ray runtime.
  ray::RayConfig config;
  ray::Init(config, argc, argv);
```
3. specify the ""--ray_code_search_path""
The reason of ""Executable function not found"" is remote.so not be found by ray and don't dynamic linked to worker",follow modify reason code search path found yet pas main char start ray cluster ray ray ray specify reason executable function found found ray dynamic linked worker,issue,negative,positive,neutral,neutral,positive,positive
1717081191,"> I understand why the example.so needs the definition but why cannot the
main executable just refer to the .so file?

Good question, theoretically it is also possible. 
As long as you can compile and run without any issues, it should be fine.
The reason why the ""example"" in the code is written like this is probably for convenience.",understand need definition main executable refer file good question theoretically also possible long compile run without fine reason example code written like probably convenience,issue,positive,positive,positive,positive,positive,positive
1717048868,"Hi Larry,

I understand why the example.so needs the definition but why cannot the
main executable just refer to the .so file?
Regards,

Wolfgang



On Wed, Sep 13, 2023 at 5:05 AM Larry ***@***.***> wrote:

> But why does this function definition need to be part of the main
> executable as well? Why the duplication?
>
> @wolfgangmeyerle <https://github.com/wolfgangmeyerle>
> As long as you need to use your functions, the process needs to include
> the definition of these function.
> So, in general, both ""example"" and ""example.so"" need to include the method
> definition.
> ""example"" belongs to the driver process. And ""example.so"" is used for
> remote method invocation. The worker process will dynamically link
> ""example.so"" and execute your own methods. Therefore, both are required.
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/ray-project/ray/issues/39252#issuecomment-1716872293>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/A7O7FGTRK4C7WQRTF6DNAETX2EPGJANCNFSM6AAAAAA4LFB5M4>
> .
> You are receiving this because you were mentioned.Message ID:
> ***@***.***>
>

",hi larry understand need definition main executable refer file wed larry wrote function definition need part main executable well duplication long need use process need include definition function general example need include method definition example driver process used remote method invocation worker process dynamically link execute therefore reply directly view id,issue,positive,positive,neutral,neutral,positive,positive
1717039190,"The error message is ""Executable function not found"" when calling the remote function. I copy below an example of decoupling the remote functions.

CMakeLists.txt
```
cmake_minimum_required(VERSION 3.0)
project(RayTest LANGUAGES CXX)

set(CMAKE_CXX_STANDARD 17)
set(CMAKE_CXX_STANDARD_REQUIRED ON)
add_definitions(-D_GLIBCXX_USE_CXX11_ABI=0)

include_directories(/cluster/MOMA348/ray_issue/include)

find_library(RAY_API
    NAMES ray_api
    HINTS /cluster/MOMA348/ray_issue/lib
)

if(NOT RAY_API)
    message(FATAL_ERROR ""Ray API not found"")
endif()

add_library( REMOTE_LIB
    remote.h
    remote.cpp
)       
target_link_libraries(REMOTE_LIB ${RAY_API})

add_executable(RayTest main.cpp)
target_link_libraries(RayTest REMOTE_LIB)
``` 
remote.h
```c++
int compute(int i);
``` 
remote.cpp
```c++
#include ""remote.h""

int compute(int i){
    return i;
}
``` 
main.cpp
```c++
#include <ray/api.h>
#include <vector>

#include ""remote.h""

RAY_REMOTE(compute)

int main(int argc, char** argv){
    ray::RayConfig config;
    config.address = """";

    ray::Init(config);
    std::vector<ray::ObjectRef<int>> results;
    for (int i = 0; i < 10; i++){
        results.push_back(ray::Task(compute).Remote());
    }

    for (auto& res : ray::Get(results)){
        std::cout << *res < std::endl;
    }

    ray::Shutdown();
    return 0;
}
``` 
I have also tried calling RAY_REMOTE(compute) in my RemoteLib library but that doesn't seem to work either. I suppose I am misunderstanding how the remote functions are used.

Sadly, currently, we cannot used bazel in my project and we are stucked with Cmake. Another issue is that with this decoupled case, even when I specify the ""--ray_code_search_path"" it says ""No code search path found yet"".",error message executable function found calling remote function copy example remote version project set set message ray found compute include compute return include include vector include compute main char ray ray ray ray compute auto ray ray return also tried calling compute library seem work either suppose misunderstanding remote used sadly currently used project another issue case even specify code search path found yet,issue,negative,negative,neutral,neutral,negative,negative
1716984433,"@simonsays1980 Has spent quite a bit of time dealing with the old exploration API and is currently ramping up on contributing to the new RLModules/Learner stack. Currently, nobody is working on. migrating the exploration API but I've now put it on our todo list 🙂 ",spent quite bit time dealing old exploration currently ramping new stack currently nobody working exploration put list,issue,negative,positive,neutral,neutral,positive,positive
1716928080,"Closing, seeing it succeeding on the release branch [train_multinode_persistence.aws](https://buildkite.com/ray-project/release-tests-branch/builds/2167#018a8c31-2fa0-4120-a720-3e5c248945c7)",seeing succeeding release branch,issue,negative,neutral,neutral,neutral,neutral,neutral
1716877177,"> 1. Why so file is this needed? 

As long as you need to use your functions, the process needs to include the definition of these function.
""example.so"" is used for remote method invocation. The worker process will dynamically link ""example.so"" and execute your own methods. 

> 2. I have also tried to create a so with just the remote functions but this also fails. 

What is the error message?  Do you set `--ray_code_search_path` ?

>  I am building the ray cpp example with cmake.

I recommend using Bazel for building and referring to the configuration in our Bazel project.",file long need use process need include definition function used remote method invocation worker process dynamically link execute also tried create remote also error message set building ray example recommend building configuration project,issue,positive,negative,neutral,neutral,negative,negative
1716872293,"> But why does this function definition need to be part of the main executable as well? Why the duplication?

@wolfgangmeyerle 
As long as you need to use your functions, the process needs to include the definition of these function. 
So, in general, both ""example"" and ""example.so"" need to include the method definition. 
""example"" belongs to the driver process.   And  ""example.so"" is used for remote method invocation. The worker process will dynamically link ""example.so"" and execute your own methods. Therefore, both are required.
",function definition need part main executable well duplication long need use process need include definition function general example need include method definition example driver process used remote method invocation worker process dynamically link execute therefore,issue,positive,positive,neutral,neutral,positive,positive
1716829206,"Remaining issues:

* Simplify code https://github.com/ray-project/ray/pull/39313#discussion_r1323849093 ， and we don't need code in https://github.com/ray-project/ray/pull/39313#discussion_r1323857429
* When global ray cluster processes crash / being killed unexpectedly, we need to clean the temp dir ""/local_disk0/tmp/ray"" dir when next global ray cluster launching, so for simplifying code, for global mode, we can only clean temp data at the start of `start_ray_node` process.
* Shall we add a reentrant lock around `try_clean_temp_dir_at_exit` function ? To keep thread-safe because `try_clean_temp_dir_at_exit` might be triggered concurrently when `sigterm signal` and `parent died` event happen simultaneously",simplify code need code global ray cluster crash unexpectedly need clean temp next global ray cluster code global mode clean temp data start process shall add lock around function keep might triggered concurrently signal parent event happen simultaneously,issue,negative,positive,positive,positive,positive,positive
1716793379,Thanks @sven1977! Looking forward to seeing this change merged. Do you know if it will be included in the next release of RLLib?,thanks looking forward seeing change know included next release,issue,negative,positive,neutral,neutral,positive,positive
1716561419,"Yup I think that makes sense. Btw, for the tracking I think we should also make sure to include the memory usage for the consumer/output in the progress bar, otherwise it's pretty hard to see where the memory is going.",think sense think also make sure include memory usage progress bar otherwise pretty hard see memory going,issue,positive,positive,positive,positive,positive,positive
1716546228,"Ah I see.

Actually, I'll change the priority of this one to p1 since we can assume trainers are usually synchronized, but we should really fix the memory accounting issue/include consumers in backpressure calculation separately.",ah see actually change priority one since assume usually synchronized really fix memory accounting calculation separately,issue,negative,negative,neutral,neutral,negative,negative
1716462681,"@hongchaodeng Thanks for the fix! You can check the output of this step once it's done building and let me know if it looks okay, and I'll merge it

<img width=""890"" alt=""Screenshot 2023-09-12 at 2 19 34 PM"" src=""https://github.com/ray-project/ray/assets/5459654/876f7abd-00a9-4349-bbe5-bdd0eb844741"">
",thanks fix check output step done building let know merge,issue,negative,positive,positive,positive,positive,positive
1716437925,"```
def env_creator(env_config):
        env = gym.make(f""ALE/{config.env}-v5"", frameskip=1, render_mode='rgb_array')
        env = AtariPreprocessing(env, noop_max=0, scale_obs=True)
        env = TransformReward(env, lambda x: np.clip(x, -1, 1))
        env = FrameStack(env, 4)
        return env
```

As you noted, this is different from what RLlib does internally. 
RLlib has a range of fine-tuned settings under the `tuned_examples` folder.
What happens if you use one of these (`ray/rllib/tuned_examples/ppo/atari-ppo.yaml`) and apply it to your problem, how are the results then?",lambda return noted different internally range folder use one apply problem,issue,negative,neutral,neutral,neutral,neutral,neutral
1716413620,"@can-anyscale , could you take this PR over and test if this works (and fix anything that does not work?) thanks!",could take test work fix anything work thanks,issue,negative,positive,positive,positive,positive,positive
1716398082,"I see the problem. Can you expand on the problem with the connectors?
Ultimately, it's desirable not to depend on some implicit ordering of sub-elements.
For example, if we flatten a space somewhere and concatenate all elements and later want to unflatten, this is one of the intricacies that can complicate things.

Do you also have a reproduction script that leads to an error?
We could use that or a broken down version of that as a higher-level test when fixing this.",see problem expand problem ultimately desirable depend implicit example flatten space somewhere concatenate later want one complicate also reproduction script error could use broken version test fixing,issue,negative,negative,negative,negative,negative,negative
1716392433,"Actually @stephanie-wang I don't think this is the same issue as #39600 --- for the imbalance issue reported there, the imbalance is in `OpState.output_queue`, not the data prefetched to the consumer.

I think this issue is more about tracking the consumer prefetch buffer block sizes (which afaik aren't imbalanced as they are usually filled to their max size always).",actually think issue imbalance issue imbalance data consumer think issue consumer buffer block size usually filled size always,issue,negative,positive,positive,positive,positive,positive
1716253759,"In practice, this may not be a concern for training use cases, since data-parallel workers will often synchronize on every batch anyway.",practice may concern training use since often synchronize every batch anyway,issue,negative,neutral,neutral,neutral,neutral,neutral
1716154749,"> > PR looks good, but I tried it out and don't see the bytes spilled when calling `print(Dataset.stats())`. We can merge this first if you want to add that in a separate PR after also getting stats from individual blocks, though.
> 
> Just added the spill stats to `Dataset.stats()` and updated the tests 👍

Thanks! Just added a suggestion to make the output more understandable. Will merge once this is addressed.",good tried see calling print merge first want add separate also getting individual though added spill thanks added suggestion make output understandable merge,issue,positive,positive,positive,positive,positive,positive
1716134248,"I think for tuning batch size, you can define train_dataloader and test_dataloader in your `LightningModule`, and pass `batch_size` as an initialization arguments in `LightningConfigBuilder().module(...)`.

By the way, for Ray 2.7, we are deprecating LightningTrainer, and support running lightning code with TorchTrainer (use your custom function) to provide more flexibility. In this case, you can pass any parameters including batch_size by the `TorchTrainer(train_loop_config=)` and define a search space over it. Please see  https://docs.ray.io/en/master/train/getting-started-pytorch-lightning.html for more details.
",think tuning batch size define pas way ray support running lightning code use custom function provide flexibility case pas define search space please see,issue,positive,neutral,neutral,neutral,neutral,neutral
1716127513,"Hi @jimthompson5802 , can you post another PR with the change only? 

(To pass the DCO check, you can use `git commit -m ""XXX"" -s`)",hi post another change pas check use git commit,issue,negative,neutral,neutral,neutral,neutral,neutral
1716004901,I'll ping internally to get a review. Apologies for the delay.,ping internally get review delay,issue,negative,neutral,neutral,neutral,neutral,neutral
1715928093,@wuxibin89 this issue is specifically about AWS g5 instance types. Feel free to open a new issue to discuss your problem (make sure to include NCCL debug logs!),issue specifically instance feel free open new issue discus problem make sure include,issue,negative,positive,positive,positive,positive,positive
1715743471,"@cadedaniel Same problem on 2 nodes with 8 GPU on each nodes, here is my NCCL environments:
```
NCCL_DEBUG: ""INFO""
NCCL_SOCKET_IFNAME: ""eth0""
NCCL_IB_HCA: ""^=mlx5_0""
NCCL_IB_GID_INDEX: ""3""
NCCL_IB_DISABLE: ""0""
NCCL_IB_TIMEOUT: ""25""
NCCL_IB_RETRY_CNT: ""7""
```
1. If 2 actors on same node, `collective.allreduce` succeed.
```python
@ray.remote(num_gpus=4)
class Worker:
    ...

# imperative
num_workers = 2 # each worker need 4 gpus, so 2 workers on same node
workers = []
init_rets = []
for i in range(num_workers):
    w = Worker.remote()
    workers.append(w)
    init_rets.append(w.setup.remote(num_workers, i))
_ = ray.get(init_rets)
results = ray.get([w.compute.remote() for w in workers])
print(results)
```
2. If 2 actors on different nodes, collective.allreduce` fail.
```python
@ray.remote(num_gpus=8)
class Worker:
    ...

# imperative
num_workers = 2 # each worker need 8 gpus, so 2 workers on different nodes
workers = []
init_rets = []
for i in range(num_workers):
    w = Worker.remote()
    workers.append(w)
    init_rets.append(w.setup.remote(num_workers, i))
_ = ray.get(init_rets)
results = ray.get([w.compute.remote() for w in workers])
print(results)
```
```
Traceback (most recent call last):
  File ""/opt/tiger/ray/session_2023-08-18_11-56-40_988471_1/runtime_resources/working_dir_files/_ray_pkg_c57d02c4f8b6cd93/ray_job.py"", line 40, in <module>
    results = ray.get([w.compute.remote() for w in workers])
  File ""/usr/local/lib/python3.9/dist-packages/ray/_private/client_mode_hook.py"", line 105, in wrapper
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.9/dist-packages/ray/_private/worker.py"", line 2413, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(NcclError): ray::Worker.compute() (pid=10926, ip=[fdbd:dc03:9:389::40], repr=<ray_job.Worker object at 0x7faffc0fd5e0>)
  File ""/opt/tiger/ray/session_2023-08-18_11-56-40_988471_1/runtime_resources/working_dir_files/_ray_pkg_c57d02c4f8b6cd93/ray_job.py"", line 23, in compute
    collective.allreduce(buffer, ""default"")
  File ""/usr/local/lib/python3.9/dist-packages/ray/util/collective/collective.py"", line 273, in allreduce
    g.allreduce([tensor], opts)
  File ""/usr/local/lib/python3.9/dist-packages/ray/util/collective/collective_group/nccl_collective_group.py"", line 197, in allreduce
    self._collective(tensors, tensors, collective_fn)
  File ""/usr/local/lib/python3.9/dist-packages/ray/util/collective/collective_group/nccl_collective_group.py"", line 604, in _collective
    comms = self._get_nccl_collective_communicator(key, devices)
  File ""/usr/local/lib/python3.9/dist-packages/ray/util/collective/collective_group/nccl_collective_group.py"", line 451, in _get_nccl_collective_communicator
    nccl_util.groupEnd()
  File ""cupy_backends/cuda/libs/nccl.pyx"", line 210, in cupy_backends.cuda.libs.nccl.groupEnd
  File ""cupy_backends/cuda/libs/nccl.pyx"", line 243, in cupy_backends.cuda.libs.nccl.groupEnd
  File ""cupy_backends/cuda/libs/nccl.pyx"", line 129, in cupy_backends.cuda.libs.nccl.check_status
cupy_backends.cuda.libs.nccl.NcclError: NCCL_ERROR_INTERNAL_ERROR: internal error
```",problem node succeed python class worker imperative worker need node range print different fail python class worker imperative worker need different range print recent call last file line module file line wrapper return file line get raise ray object file line compute buffer default file line tensor file line file line key file line file line file line file line internal error,issue,negative,negative,neutral,neutral,negative,negative
1715264137,"Should this refer to ""get protobuf from the default (anaconda) channel rather than the conda-forge channel"", or is the issue connected to the anaconda version resolver vs. the one in miniconda?",refer get default anaconda channel rather channel issue connected anaconda version resolver one,issue,negative,neutral,neutral,neutral,neutral,neutral
1715169784,"> Is upgrading bazel on the ray roadmap?

Respectfully, it must be. It's not reasonable to keep a dependency pinned forever. And if that's not enough of an argument, then bazel 6 specificaly would give some potentially very large advantages on the packaging side due to [bzlmod](https://bazel.build/external/migration), which should ideally unblock things like osx-support in conda-forge, improving the grpc-situation, etc. etc.",ray respectfully must reasonable keep dependency pinned forever enough argument would give potentially large side due ideally unblock like improving,issue,positive,positive,positive,positive,positive,positive
1714937299,"The test was passing on Tuesday  master last week https://buildkite.com/ray-project/release-tests-branch/builds/2130#018a69de-a215-40ce-ad5a-45eee239fc9d, something jut broke it recently",test passing master last week something jut broke recently,issue,negative,neutral,neutral,neutral,neutral,neutral
1714915489,"but, I'm currently still stuck on trying to get the concatenation function working correctly with different node sizes per batch. ",currently still stuck trying get concatenation function working correctly different node size per batch,issue,negative,neutral,neutral,neutral,neutral,neutral
1714914921,"Also, from what I have currently looked at. I changed `space_utils.py` to add this clause to get_dummy_batch_for_space function:

```
    elif isinstance(space, Graph):
        def get_nodes(num_nodes):
            return np.concatenate([get_dummy_batch_for_space(space.node_space, 1, fill_value) for _ in range(num_nodes)])
        def get_edges(num_edges):
            return np.concatenate([get_dummy_batch_for_space(space.edge_space, 1, fill_value) for _ in range(num_edges)])
        
        
        def get_edge_links(nodes, edges, batch):
            if fill_value == ""random"":
                return space.np_random.integers(
                    low=0, high=nodes[batch].shape[0], size=(edges[batch].shape[0], 2), dtype=space.dtype
                )
            return np.full(
                shape=(edges[batch].shape[0], 2), fill_value=fill_value, dtype=space.dtype
            )
            
        nodes = [get_nodes(np.random.randint(low=5, high=15)) for _ in range(batch_size)]
        edges = [get_edges(np.random.randint(low=3, high=10)) for _ in range(batch_size)]
        edge_links = [get_edge_links(nodes, edges, i) for i in range(batch_size)]

        return (nodes, edges, edge_links)
```

",also currently add clause function space graph return range return range batch random return batch batch return batch range range range return,issue,negative,negative,negative,negative,negative,negative
1714901602,"So, I haven't tested it (I just heavily simplified my use-case), but here's a full reproduction script:

Firstly the environment:

```
import gymnasium as gym
from gymnasium.spaces import Space, Box, Discrete, Graph, GraphInstance
import numpy as np

class TestEnv(gym.Env):    
    @property
    def observation_space(self) -> Space:
        # Graph Space
        return gym.spaces.Dict({
            'graph': Graph(
                        node_space=Box(low=-1, high=np.inf, shape=(5,), dtype=np.int32),
                        edge_space=Discrete(1),
            )
        })
           
    @property
    def action_space(self) -> Space:
        # Discrete Action Space
        return Discrete(5)

    def reset(self, *, seed=None, option=None):
        # Sample will return Dict with GraphInstance for Space
        return self.observation_space.sample(), {}
    
    def step(self, action):
        # DUMMY Actions
        return self.observation_space.sample(), 0, False, False, {}
```

Secondly, a model:

```
import gymnasium as gym
from ray.rllib.utils.typing import ModelConfigDict
import torch
from ray.rllib.models.torch.torch_modelv2 import TorchModelV2
from torch_geometric.data import Data
from torch_geometric.loader import DataLoader

from torch_geometric.nn import GCNConv
from torch_geometric.nn import Sequential as GNNSequential
from torch.nn import Sequential, Module, Linear


class GraphModel(TorchModelV2, Module):
    
    def __init__(self, obs_space: gym.spaces.Space, action_space: gym.spaces.Space, num_outputs: int, model_config: ModelConfigDict, name: str):
        super().__init__(obs_space, action_space, num_outputs, model_config, name)
        self.gnn = GNNSequential('x, edge_index', [(GCNConv(5, 5), 'x, edge_index -> x')])
        
        # Policy Head
        self._policy_head = Sequential(
            Linear(in_features=5, out_features=num_outputs)
        )
        
        self._value_head = Sequential(
            Linear(in_features=5, out_features=1)
        )
        
        
    def forward(self, input_dict, state, seq_len):
        
        batches = len(input_dict['obs']['graph'][0])
        
        # NOTE: Model assumes that each batch can have different number of nodes and they are stored as a list of 2-d tensors (where the first list is the batch, second is node number, third is feature)
        node_features = input_dict['obs']['graph'][0]
        adj_matrix = input_dict['obs']['graph'][2]
        
        # Get the number of nodes in each batch
        batch_node_size = [len(node_features[i]) for i in range(batches)]
        max_nodes = max(batch_node_size)
        
        # Pad the Node Features dynamically to batch with largest input size
        for batch in range(batches):
            to_append = torch.zeros(max_nodes - node_features[batch].shape[0], node_features[batch].shape[1])
            node_features[batch] = torch.cat([node_features[batch], to_append], dim=0)
        node_features = torch.stack(node_features, dim=1).type(torch.int).reshape(batches, max_nodes, -1)
        
        
        # Minibatch using PyTorch Geometric DataLoader
        data_list = []
        for batch in range(batches):
            node_batch_features = node_features[batch, :, :]
            adj = adj_matrix[batch].type(torch.long).permute(1, 0)
            data_list.append(Data(x=node_batch_features, edge_index=adj))
        data = DataLoader(data_list, batch_size=batches, shuffle=False)
        
        # Do GNN Inference
        for batch in data:
            output_gnn = self.gnn(batch.x, batch.edge_index).view(batches, max_nodes, -1)
        
        # Extract First Node to be output feature
        embedding = torch.empty(batches, 5)
        for batch in range(batches):
            embedding[batch] = output_gnn[batch, 0, :]
            
        # Value Head
        self.value = self._value_head(embedding)
        
        # Policy Head
        logits = self._policy_head(embedding)
        
        return logits, state
    
    def value_function(self):
        return torch.reshape(self.value, [-1])
```",tested heavily simplified full reproduction script firstly environment import gymnasium gym import space box discrete graph import class property self space graph space return graph property self space discrete action space return discrete reset self sample return space return step self action dummy return false false secondly model import gymnasium gym import import torch import import data import import import sequential import sequential module linear class module self name super name policy head sequential linear sequential linear forward self state note model batch different number list first list batch second node number third feature get number batch range pad node dynamically batch input size batch range batch batch batch batch geometric batch range batch batch data data inference batch data extract first node output feature batch range batch batch value head policy head return state self return,issue,positive,positive,neutral,neutral,positive,positive
1714893045,"Yep those are flaky on the release branch as well https://buildkite.com/ray-project/oss-ci-build-branch/builds/6072#018a82b5-d60e-4043-acf4-ad1267c46042/3700-4674 But just retry it few more times, we can eventually get a passing run",yep flaky release branch well retry time eventually get passing run,issue,positive,neutral,neutral,neutral,neutral,neutral
1714873630,"@woshiyyya @krfricke   Not sure but I may have made a mistake in trying address the DCO check.  I followed the instructions provided.  It appears I may have accidentally signed more than just my commit.

I'm thinking it might make sense to just close this PR and either start again or since you guys are refactoring the notebooks and the changes I proposed  are simple documentation one you can incorporate in your update.

Anyway let me know what I should do.",sure may made mistake trying address check provided may accidentally commit thinking might make sense close either start since simple documentation one incorporate update anyway let know,issue,negative,positive,positive,positive,positive,positive
1714807791,"Reminder:
 - merge this fix https://github.com/ray-project/ray/pull/39443 and test
 - Test this on databricks cluster with multiple nodes (at least one worker node)
 - Filed a databricks PR to set default ""RAY_TMPDIR"" environment variable",reminder merge fix test test cluster multiple least one worker node set default environment variable,issue,negative,negative,negative,negative,negative,negative
1714806480,"That pip list is passed through during ray init [(https://docs.ray.io/en/latest/ray-core/handling-dependencies.html)](https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#preparing-an-environment-using-the-ray-cluster-launcher). Does the test perhaps require those dependencies outside of ray init?

Also did the test pass previously?",pip list ray test perhaps require outside ray also test pas previously,issue,negative,negative,neutral,neutral,negative,negative
1714775427,cc @rynewang to cherry pick. Also can you create P0 issue to recover this,cherry pick also create issue recover,issue,negative,neutral,neutral,neutral,neutral,neutral
1714725902,"@rkooo567 @jjyao Not sure who has permission to get this merged. Can we get this merged and raise a cherry pick PR?
",sure permission get get raise cherry pick,issue,negative,positive,positive,positive,positive,positive
1714695976,"1. can we add a test (there must be a way to access sigterm handler? or you can set the sigterm handler -> import ray -> make sure the original sigterm handler is triggerd) 
2. We should update the doc (and specifies this behavior)",add test must way access handler set handler import ray make sure original handler update doc behavior,issue,positive,positive,positive,positive,positive,positive
1714681443,Will move it to development section of the doc,move development section doc,issue,negative,neutral,neutral,neutral,neutral,neutral
1714661473,"> PR looks good, but I tried it out and don't see the bytes spilled when calling `print(Dataset.stats())`. We can merge this first if you want to add that in a separate PR after also getting stats from individual blocks, though.

Just added the spill stats to `Dataset.stats()` and updated the tests  👍",good tried see calling print merge first want add separate also getting individual though added spill,issue,negative,positive,positive,positive,positive,positive
1714560941,"I can't share the production data. But here are some more details:
- 550,000 records
- Each record has 6902 columns: Out of these 6602 columns have float32 and remaining 300 are nested columns of size 1024 (embedding values of float32).
- 1,113 files
- 307 GB

Please let me know, if you need anything else. Thank you.",ca share production data record float size float please let know need anything else thank,issue,positive,neutral,neutral,neutral,neutral,neutral
1714549157,"When I read file one-by-one with ray it seems fine. 
```
%%time
chunks_of_1 = (train_files[i:i+1] for i in range(0,len(train_files),1))
for chunk_files in chunks_of_1:
    #try:
    ray_train_dataset = ray.data.read_parquet(chunk_files)
    #except:
    #print(f'An exception occurred while processing file: {chunk_files}')
```

## Successful:
```
CPU times: user 39.5 s, sys: 4.2 s, total: 43.7 s
Wall time: 35min 7s
```

But when I try two files at a time from same directory. It's failing with same error: 

I have some columns with nested data, do you think metadata may be the issue?

```
%%time
chunks_of_2 = (train_files[i:i+2] for i in range(0,len(train_files),2))
for chunk_files in chunks_of_2:
    #try:
    ray_train_dataset = ray.data.read_parquet(chunk_files)
    break
    #except:
    #    print(f'An exception occurred while processing file: {chunk_files}')
```

## Error:
```
(_get_read_tasks pid=43663) /home/ec2-user/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/data/datasource/parquet_datasource.py:241: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Use the '.fragments' attribute instead
(_get_read_tasks pid=43663)   pq_ds.pieces, **prefetch_remote_args
(_get_read_tasks pid=43663) /home/ec2-user/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/data/datasource/parquet_datasource.py:344: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Use the '.fragments' attribute instead
(_get_read_tasks pid=43663)   num_files = len(self._pq_ds.pieces)
(_get_read_tasks pid=43663) /home/ec2-user/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/data/datasource/parquet_datasource.py:357: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Use the '.fragments' attribute instead
(_get_read_tasks pid=43663)   self._pq_ds.pieces[idx]
                                                                                                                                                                                                                                                
---------------------------------------------------------------------------
RayTaskError(ArrowNotImplementedError)    Traceback (most recent call last)
File <timed exec>:4

File ~/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/data/read_api.py:588, in read_parquet(paths, filesystem, columns, parallelism, ray_remote_args, tensor_column_schema, meta_provider, **arrow_parquet_args)
    516 """"""Create an Arrow dataset from parquet files.
    517 
    518 Examples:
   (...)
    582     Dataset producing Arrow records read from the specified paths.
    583 """"""
    584 arrow_parquet_args = _resolve_parquet_args(
    585     tensor_column_schema,
    586     **arrow_parquet_args,
    587 )
--> 588 return read_datasource(
    589     ParquetDatasource(),
    590     parallelism=parallelism,
    591     paths=paths,
    592     filesystem=filesystem,
    593     columns=columns,
    594     ray_remote_args=ray_remote_args,
    595     meta_provider=meta_provider,
    596     **arrow_parquet_args,
    597 )

File ~/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/_private/auto_init_hook.py:24, in wrap_auto_init.<locals>.auto_init_wrapper(*args, **kwargs)
     21 @wraps(fn)
     22 def auto_init_wrapper(*args, **kwargs):
     23     auto_init_ray()
---> 24     return fn(*args, **kwargs)

File ~/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/data/read_api.py:344, in read_datasource(datasource, parallelism, ray_remote_args, **read_args)
    331     scheduling_strategy = NodeAffinitySchedulingStrategy(
    332         ray.get_runtime_context().get_node_id(),
    333         soft=False,
    334     )
    335     get_read_tasks = cached_remote_fn(
    336         _get_read_tasks, retry_exceptions=False, num_cpus=0
    337     ).options(scheduling_strategy=scheduling_strategy)
    339     (
    340         requested_parallelism,
    341         min_safe_parallelism,
    342         inmemory_size,
    343         read_tasks,
--> 344     ) = ray.get(
    345         get_read_tasks.remote(
    346             datasource,
    347             ctx,
    348             cur_pg,
    349             parallelism,
    350             local_uri,
    351             _wrap_arrow_serialization_workaround(read_args),
    352         )
    353     )
    355 # Compute the number of blocks the read will return. If the number of blocks is
    356 # expected to be less than the requested parallelism, boost the number of blocks
    357 # by adding an additional split into `k` pieces to each read task.
    358 if read_tasks:

File ~/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/_private/auto_init_hook.py:24, in wrap_auto_init.<locals>.auto_init_wrapper(*args, **kwargs)
     21 @wraps(fn)
     22 def auto_init_wrapper(*args, **kwargs):
     23     auto_init_ray()
---> 24     return fn(*args, **kwargs)

File ~/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/_private/client_mode_hook.py:103, in client_mode_hook.<locals>.wrapper(*args, **kwargs)
    101     if func.__name__ != ""init"" or is_client_mode_enabled_by_default:
    102         return getattr(ray, func.__name__)(*args, **kwargs)
--> 103 return func(*args, **kwargs)

File ~/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/_private/worker.py:2493, in get(object_refs, timeout)
   2491     worker.core_worker.dump_object_store_memory_usage()
   2492 if isinstance(value, RayTaskError):
-> 2493     raise value.as_instanceof_cause()
   2494 else:
   2495     raise value

RayTaskError(ArrowNotImplementedError): ray::_get_read_tasks() (pid=43663, ip=172.31.28.148)
  File ""/home/ec2-user/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/data/read_api.py"", line 1928, in _get_read_tasks
    reader = ds.create_reader(**kwargs)
  File ""/home/ec2-user/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/data/datasource/parquet_datasource.py"", line 170, in create_reader
    return _ParquetDatasourceReader(**kwargs)
  File ""/home/ec2-user/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/data/datasource/parquet_datasource.py"", line 254, in __init__
    self._encoding_ratio = self._estimate_files_encoding_ratio()
  File ""/home/ec2-user/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/data/datasource/parquet_datasource.py"", line 378, in _estimate_files_encoding_ratio
    sample_ratios = sample_bar.fetch_until_complete(futures)
  File ""/home/ec2-user/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/data/_internal/progress_bar.py"", line 93, in fetch_until_complete
    for ref, result in zip(done, ray.get(done)):
ray.exceptions.RayTaskError(ArrowNotImplementedError): ray::_sample_piece() (pid=43685, ip=172.31.28.148)
  File ""/home/ec2-user/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/data/datasource/parquet_datasource.py"", line 492, in _sample_piece
    batch = next(batches)
  File ""pyarrow/_dataset.pyx"", line 3603, in _iterator
  File ""pyarrow/_dataset.pyx"", line 3221, in pyarrow._dataset.TaggedRecordBatchIterator.__next__
  File ""pyarrow/error.pxi"", line 144, in pyarrow.lib.pyarrow_internal_check_status
  File ""pyarrow/error.pxi"", line 121, in pyarrow.lib.check_status
pyarrow.lib.ArrowNotImplementedError: Unsupported cast from double to null using function cast_null
```",read file ray fine time range try except print exception file successful time user total wall time min try two time directory failing error data think may issue time range try break except print exception file error attribute removed future version use attribute instead attribute removed future version use attribute instead attribute removed future version use attribute instead recent call last file timed file parallelism create arrow parquet arrow read return file return file parallelism parallelism compute number read return number le parallelism boost number additional split read task file return file return ray return file get value raise else raise value ray file line reader file line return file line file line file line ref result zip done done ray file line batch next file line file line file line file line unsupported cast double null function,issue,positive,positive,positive,positive,positive,positive
1714545517,"Thanks @sven1977 for the changes! I've suggested 2 minor changes in my code review.

There's also one other place that should be updated. The docstring for `PettingZooEnv` currently still says that the wrapper only supports environments with homogeneous action / observation spaces. With this pull request, this comment should be removed. See below:

https://github.com/ray-project/ray/blob/5a4389be43a3d432d4c91cfbeb14a8cd62a26453/rllib/env/wrappers/pettingzoo_env.py#L23-L27

Thanks for your work on this!",thanks minor code review also one place currently still wrapper homogeneous action observation pull request comment removed see thanks work,issue,positive,positive,neutral,neutral,positive,positive
1714530606,"merged, but some of the k8s chaos tests are failing?

https://buildkite.com/ray-project/oss-ci-build-pr/builds/35832#018a84be-42b6-4393-9a71-c9d6b633fde4/7582-7618 

not sure if I made a mistake or not..",chaos failing sure made mistake,issue,negative,positive,positive,positive,positive,positive
1714501471,Rerun passing after manual fix: https://buildkite.com/ray-project/release-tests-branch/builds/2149#018a85c7-609a-4e5c-ab4a-af4c2ab095d2,rerun passing manual fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1714485231,"I think this (and other GCP tests) been failing on master as well since last Thursday https://buildkite.com/ray-project/release-tests-branch/builds/2140, also tried numerously times over the weekend and this morning and never sew a success 😅",think failing master well since last also tried numerously time weekend morning never sew success,issue,negative,positive,neutral,neutral,positive,positive
1714423180,"Hmm the error message is unfortunately not very informative but I think it may be due to not being able to read one of the files properly, could be that the data was corrupted somehow during creation. Is it possible for you to provide a copy of the data or some synthetic copy? Otherwise, you could try reading each file manually with pyarrow and check that you are able to do so without error.",error message unfortunately informative think may due able read one properly could data corrupted somehow creation possible provide copy data synthetic copy otherwise could try reading file manually check able without error,issue,negative,positive,neutral,neutral,positive,positive
1714403689,"Thank you for quick reply. I get the same error. 

```
(base) [ec2-user@ip- train-dataset]$ ls | wc -l
1113
(base) [ec2-user@ip-train-dataset]$

ray_train_dataset = ray.data.read_parquet(train_files)
```

## Error:
```
(_get_read_tasks pid=44471) /home/ec2-user/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/data/datasource/parquet_datasource.py:241: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Use the '.fragments' attribute instead
(_get_read_tasks pid=44471)   pq_ds.pieces, **prefetch_remote_args                                                                                                                                                                 
(_get_read_tasks pid=44471) /home/ec2-user/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/data/datasource/parquet_datasource.py:344: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Use the '.fragments' attribute instead
(_get_read_tasks pid=44471)   num_files = len(self._pq_ds.pieces)
(_get_read_tasks pid=44471) /home/ec2-user/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/data/datasource/parquet_datasource.py:357: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Use the '.fragments' attribute instead
(_get_read_tasks pid=44471)   self._pq_ds.pieces[idx]
(_get_read_tasks pid=44471) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::_sample_piece() (pid=45516, ip=172.31.28.148)                                                                          
(_get_read_tasks pid=44471)   File ""/home/ec2-user/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/data/datasource/parquet_datasource.py"", line 492, in _sample_piece
(_get_read_tasks pid=44471)     batch = next(batches)
(_get_read_tasks pid=44471)   File ""pyarrow/_dataset.pyx"", line 3603, in _iterator
(_get_read_tasks pid=44471)   File ""pyarrow/_dataset.pyx"", line 3221, in pyarrow._dataset.TaggedRecordBatchIterator.__next__
(_get_read_tasks pid=44471)   File ""pyarrow/error.pxi"", line 144, in pyarrow.lib.pyarrow_internal_check_status
(_get_read_tasks pid=44471)   File ""pyarrow/error.pxi"", line 121, in pyarrow.lib.check_status
(_get_read_tasks pid=44471) pyarrow.lib.ArrowNotImplementedError: Unsupported cast from double to null using function cast_null
(_get_read_tasks pid=44471) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::_sample_piece() (pid=44448, ip=172.31.28.148)
(_get_read_tasks pid=44471)   File ""/home/ec2-user/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/data/datasource/parquet_datasource.py"", line 492, in _sample_piece
(_get_read_tasks pid=44471)     batch = next(batches)
(_get_read_tasks pid=44471)   File ""pyarrow/_dataset.pyx"", line 3603, in _iterator
(_get_read_tasks pid=44471)   File ""pyarrow/_dataset.pyx"", line 3221, in pyarrow._dataset.TaggedRecordBatchIterator.__next__
(_get_read_tasks pid=44471)   File ""pyarrow/error.pxi"", line 144, in pyarrow.lib.pyarrow_internal_check_status
(_get_read_tasks pid=44471)   File ""pyarrow/error.pxi"", line 121, in pyarrow.lib.check_status
(_get_read_tasks pid=44471) pyarrow.lib.ArrowNotImplementedError: Unsupported cast from double to null using function cast_null
(_get_read_tasks pid=44471) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::_sample_piece() (pid=50639, ip=172.31.28.148)
(_get_read_tasks pid=44471)   File ""/home/ec2-user/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/data/datasource/parquet_datasource.py"", line 492, in _sample_piece
(_get_read_tasks pid=44471)     batch = next(batches)
(_get_read_tasks pid=44471)   File ""pyarrow/_dataset.pyx"", line 3603, in _iterator
(_get_read_tasks pid=44471)   File ""pyarrow/_dataset.pyx"", line 3221, in pyarrow._dataset.TaggedRecordBatchIterator.__next__
(_get_read_tasks pid=44471)   File ""pyarrow/error.pxi"", line 144, in pyarrow.lib.pyarrow_internal_check_status
(_get_read_tasks pid=44471)   File ""pyarrow/error.pxi"", line 121, in pyarrow.lib.check_status
(_get_read_tasks pid=44471) pyarrow.lib.ArrowNotImplementedError: Unsupported cast from double to null using function cast_null
(_get_read_tasks pid=44471) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::_sample_piece() (pid=50643, ip=172.31.28.148)
(_get_read_tasks pid=44471)   File ""/home/ec2-user/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/data/datasource/parquet_datasource.py"", line 492, in _sample_piece
(_get_read_tasks pid=44471)     batch = next(batches)
(_get_read_tasks pid=44471)   File ""pyarrow/_dataset.pyx"", line 3603, in _iterator
(_get_read_tasks pid=44471)   File ""pyarrow/_dataset.pyx"", line 3221, in pyarrow._dataset.TaggedRecordBatchIterator.__next__
(_get_read_tasks pid=44471)   File ""pyarrow/error.pxi"", line 144, in pyarrow.lib.pyarrow_internal_check_status
(_get_read_tasks pid=44471)   File ""pyarrow/error.pxi"", line 121, in pyarrow.lib.check_status
(_get_read_tasks pid=44471) pyarrow.lib.ArrowNotImplementedError: Unsupported cast from double to null using function cast_null
(_get_read_tasks pid=44471) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::_sample_piece() (pid=50401, ip=172.31.28.148)
(_get_read_tasks pid=44471)   File ""/home/ec2-user/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/data/datasource/parquet_datasource.py"", line 492, in _sample_piece
(_get_read_tasks pid=44471)     batch = next(batches)
(_get_read_tasks pid=44471)   File ""pyarrow/_dataset.pyx"", line 3603, in _iterator
(_get_read_tasks pid=44471)   File ""pyarrow/_dataset.pyx"", line 3221, in pyarrow._dataset.TaggedRecordBatchIterator.__next__
(_get_read_tasks pid=44471)   File ""pyarrow/error.pxi"", line 144, in pyarrow.lib.pyarrow_internal_check_status
(_get_read_tasks pid=44471)   File ""pyarrow/error.pxi"", line 121, in pyarrow.lib.check_status
(_get_read_tasks pid=44471) pyarrow.lib.ArrowNotImplementedError: Unsupported cast from double to null using function cast_null
---------------------------------------------------------------------------
RayTaskError(ArrowNotImplementedError)    Traceback (most recent call last)
File <timed exec>:1

File ~/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/data/read_api.py:588, in read_parquet(paths, filesystem, columns, parallelism, ray_remote_args, tensor_column_schema, meta_provider, **arrow_parquet_args)
    516 """"""Create an Arrow dataset from parquet files.
    517 
    518 Examples:
   (...)
    582     Dataset producing Arrow records read from the specified paths.
    583 """"""
    584 arrow_parquet_args = _resolve_parquet_args(
    585     tensor_column_schema,
    586     **arrow_parquet_args,
    587 )
--> 588 return read_datasource(
    589     ParquetDatasource(),
    590     parallelism=parallelism,
    591     paths=paths,
    592     filesystem=filesystem,
    593     columns=columns,
    594     ray_remote_args=ray_remote_args,
    595     meta_provider=meta_provider,
    596     **arrow_parquet_args,
    597 )

File ~/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/_private/auto_init_hook.py:24, in wrap_auto_init.<locals>.auto_init_wrapper(*args, **kwargs)
     21 @wraps(fn)
     22 def auto_init_wrapper(*args, **kwargs):
     23     auto_init_ray()
---> 24     return fn(*args, **kwargs)

File ~/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/data/read_api.py:344, in read_datasource(datasource, parallelism, ray_remote_args, **read_args)
    331     scheduling_strategy = NodeAffinitySchedulingStrategy(
    332         ray.get_runtime_context().get_node_id(),
    333         soft=False,
    334     )
    335     get_read_tasks = cached_remote_fn(
    336         _get_read_tasks, retry_exceptions=False, num_cpus=0
    337     ).options(scheduling_strategy=scheduling_strategy)
    339     (
    340         requested_parallelism,
    341         min_safe_parallelism,
    342         inmemory_size,
    343         read_tasks,
--> 344     ) = ray.get(
    345         get_read_tasks.remote(
    346             datasource,
    347             ctx,
    348             cur_pg,
    349             parallelism,
    350             local_uri,
    351             _wrap_arrow_serialization_workaround(read_args),
    352         )
    353     )
    355 # Compute the number of blocks the read will return. If the number of blocks is
    356 # expected to be less than the requested parallelism, boost the number of blocks
    357 # by adding an additional split into `k` pieces to each read task.
    358 if read_tasks:

File ~/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/_private/auto_init_hook.py:24, in wrap_auto_init.<locals>.auto_init_wrapper(*args, **kwargs)
     21 @wraps(fn)
     22 def auto_init_wrapper(*args, **kwargs):
     23     auto_init_ray()
---> 24     return fn(*args, **kwargs)

File ~/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/_private/client_mode_hook.py:103, in client_mode_hook.<locals>.wrapper(*args, **kwargs)
    101     if func.__name__ != ""init"" or is_client_mode_enabled_by_default:
    102         return getattr(ray, func.__name__)(*args, **kwargs)
--> 103 return func(*args, **kwargs)

File ~/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/_private/worker.py:2493, in get(object_refs, timeout)
   2491     worker.core_worker.dump_object_store_memory_usage()
   2492 if isinstance(value, RayTaskError):
-> 2493     raise value.as_instanceof_cause()
   2494 else:
   2495     raise value

RayTaskError(ArrowNotImplementedError): ray::_get_read_tasks() (pid=44471, ip=172.31.28.148)
  File ""/home/ec2-user/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/data/read_api.py"", line 1928, in _get_read_tasks
    reader = ds.create_reader(**kwargs)
  File ""/home/ec2-user/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/data/datasource/parquet_datasource.py"", line 170, in create_reader
    return _ParquetDatasourceReader(**kwargs)
  File ""/home/ec2-user/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/data/datasource/parquet_datasource.py"", line 254, in __init__
    self._encoding_ratio = self._estimate_files_encoding_ratio()
  File ""/home/ec2-user/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/data/datasource/parquet_datasource.py"", line 378, in _estimate_files_encoding_ratio
    sample_ratios = sample_bar.fetch_until_complete(futures)
  File ""/home/ec2-user/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/data/_internal/progress_bar.py"", line 93, in fetch_until_complete
    for ref, result in zip(done, ray.get(done)):
ray.exceptions.RayTaskError(ArrowNotImplementedError): ray::_sample_piece() (pid=44406, ip=172.31.28.148)
  File ""/home/ec2-user/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/data/datasource/parquet_datasource.py"", line 492, in _sample_piece
    batch = next(batches)
  File ""pyarrow/_dataset.pyx"", line 3603, in _iterator
  File ""pyarrow/_dataset.pyx"", line 3221, in pyarrow._dataset.TaggedRecordBatchIterator.__next__
  File ""pyarrow/error.pxi"", line 144, in pyarrow.lib.pyarrow_internal_check_status
  File ""pyarrow/error.pxi"", line 121, in pyarrow.lib.check_status
pyarrow.lib.ArrowNotImplementedError: Unsupported cast from double to null using function cast_null
(_get_read_tasks pid=44471) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::_sample_piece() (pid=45571, ip=172.31.28.148)
(_get_read_tasks pid=44471)   File ""/home/ec2-user/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/data/datasource/parquet_datasource.py"", line 492, in _sample_piece
(_get_read_tasks pid=44471)     batch = next(batches)
(_get_read_tasks pid=44471)   File ""pyarrow/_dataset.pyx"", line 3603, in _iterator
(_get_read_tasks pid=44471)   File ""pyarrow/_dataset.pyx"", line 3221, in pyarrow._dataset.TaggedRecordBatchIterator.__next__
(_get_read_tasks pid=44471)   File ""pyarrow/error.pxi"", line 144, in pyarrow.lib.pyarrow_internal_check_status
(_get_read_tasks pid=44471)   File ""pyarrow/error.pxi"", line 121, in pyarrow.lib.check_status
(_get_read_tasks pid=44471) pyarrow.lib.ArrowNotImplementedError: Unsupported cast from double to null using function cast_null
(_get_read_tasks pid=44471) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::_sample_piece() (pid=50716, ip=172.31.28.148)
(_get_read_tasks pid=44471)   File ""/home/ec2-user/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/data/datasource/parquet_datasource.py"", line 492, in _sample_piece
(_get_read_tasks pid=44471)     batch = next(batches)
(_get_read_tasks pid=44471)   File ""pyarrow/_dataset.pyx"", line 3603, in _iterator
(_get_read_tasks pid=44471)   File ""pyarrow/_dataset.pyx"", line 3221, in pyarrow._dataset.TaggedRecordBatchIterator.__next__
(_get_read_tasks pid=44471)   File ""pyarrow/error.pxi"", line 144, in pyarrow.lib.pyarrow_internal_check_status
(_get_read_tasks pid=44471)   File ""pyarrow/error.pxi"", line 121, in pyarrow.lib.check_status
(_get_read_tasks pid=44471) pyarrow.lib.ArrowNotImplementedError: Unsupported cast from double to null using function cast_null
(_get_read_tasks pid=44471) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::_sample_piece() (pid=50401, ip=172.31.28.148)
(_get_read_tasks pid=44471)   File ""/home/ec2-user/miniconda3/envs/pytorch_ploomber/lib/python3.8/site-packages/ray/data/datasource/parquet_datasource.py"", line 492, in _sample_piece
(_get_read_tasks pid=44471)     batch = next(batches)
(_get_read_tasks pid=44471)   File ""pyarrow/_dataset.pyx"", line 3603, in _iterator
(_get_read_tasks pid=44471)   File ""pyarrow/_dataset.pyx"", line 3221, in pyarrow._dataset.TaggedRecordBatchIterator.__next__
(_get_read_tasks pid=44471)   File ""pyarrow/error.pxi"", line 144, in pyarrow.lib.pyarrow_internal_check_status
(_get_read_tasks pid=44471)   File ""pyarrow/error.pxi"", line 121, in pyarrow.lib.check_status
(_get_read_tasks pid=44471) pyarrow.lib.ArrowNotImplementedError: Unsupported cast from double to null using function cast_null
```",thank quick reply get error base base error attribute removed future version use attribute instead attribute removed future version use attribute instead attribute removed future version use attribute instead unhandled error suppress ray file line batch next file line file line file line file line unsupported cast double null function unhandled error suppress ray file line batch next file line file line file line file line unsupported cast double null function unhandled error suppress ray file line batch next file line file line file line file line unsupported cast double null function unhandled error suppress ray file line batch next file line file line file line file line unsupported cast double null function unhandled error suppress ray file line batch next file line file line file line file line unsupported cast double null function recent call last file timed file parallelism create arrow parquet arrow read return file return file parallelism parallelism compute number read return number le parallelism boost number additional split read task file return file return ray return file get value raise else raise value ray file line reader file line return file line file line file line ref result zip done done ray file line batch next file line file line file line file line unsupported cast double null function unhandled error suppress ray file line batch next file line file line file line file line unsupported cast double null function unhandled error suppress ray file line batch next file line file line file line file line unsupported cast double null function unhandled error suppress ray file line batch next file line file line file line file line unsupported cast double null function,issue,negative,negative,neutral,neutral,negative,negative
1714386020,"> @shrekris-anyscale can we mark these metrics as some kind of developer API btw? They are pretty likely to change as we evolve the routing code

Yeah, good idea. I added a footnote marking them as `developer metrics`.",mark metric kind developer pretty likely change evolve routing code yeah good idea added footnote marking developer metric,issue,positive,positive,positive,positive,positive,positive
1714385810,"I think you may be bumping into a performance bug related to picking too low of a parallelism value. Ideally, it'd be best if you can use the default parameters. What happens if you do not specify any additional arguments to `read_parquet`?

```python
ray_train_dataset = ray.data.read_parquet(train_files)
```",think may bumping performance bug related low parallelism value ideally best use default specify additional python,issue,positive,positive,positive,positive,positive,positive
1714372214,"Looks like it's not just actor based workload has regression? 

```
REGRESSION 20.08%: single_client_tasks_async (THROUGHPUT) regresses from 10940.075908384719 to 8743.097657759507 (20.08%) in 2.7.0/microbenchmark.json
REGRESSION 18.99%: multi_client_tasks_async (THROUGHPUT) regresses from 28883.241079598323 to 23399.550953458893 (18.99%) in 2.7.0/microbenchmark.json
```

these are tasks only i guess. ",like actor based regression regression throughput regression throughput guess,issue,negative,neutral,neutral,neutral,neutral,neutral
1714359488,"on a failing test, `pip freeze | grep torch` gives you nothing.

on a succeeding test, 

```
(base)  ray@ip-10-0-18-61:~/default$ pip freeze | grep torch
pytorch-lightning==2.0.6
torch==2.0.1
torchaudio==2.0.2
torchdata==0.6.1
torchmetrics==1.0.1
torchtext==0.15.2
torchvision==0.15.2
```
On that note, I think [the feature request](https://github.com/ray-project/ray/issues/37955) will be really helpful for this kind of issues.",failing test pip freeze torch nothing succeeding test base ray pip freeze torch note think feature request really helpful kind,issue,negative,negative,neutral,neutral,negative,negative
1714357777,"Could you update the PR description?

Also, I'm thinking we should actually update the benchmarks to not have to use a GPU, since we're currently not even testing this part. Can we switch to an m5 instance type with the same number of CPUs? There should be a way to override the `num_gpus` that is set on each node.",could update description also thinking actually update use since currently even testing part switch instance type number way override set node,issue,negative,neutral,neutral,neutral,neutral,neutral
1714357630,"I put up a https://github.com/ray-project/ray/pull/39551 to change the pip install requirement syntax a bit. But I don't think that's causing the issue.
",put change pip install requirement syntax bit think causing issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1714356769,@rkooo567 I believe the lint error was resolved? Mind taking another poke for Arvind?,believe lint error resolved mind taking another poke,issue,negative,neutral,neutral,neutral,neutral,neutral
1714342837,"PR looks good, but I tried it out and don't see the bytes spilled when calling `print(Dataset.stats())`. We can merge this first if you want to add that in a separate PR after also getting stats from individual blocks, though.",good tried see calling print merge first want add separate also getting individual though,issue,negative,positive,positive,positive,positive,positive
1714326977,@rynewang lets also skip in the release branch as well. ,also skip release branch well,issue,negative,neutral,neutral,neutral,neutral,neutral
1714306304,"SlateQ, by default, must have `_disable_preprocessor_api=True`, but by using this generic AlgorithmConfig object, this setting was set to False.",default must generic object setting set false,issue,negative,negative,negative,negative,negative,negative
1714305393,"Ah, figured it out. The algo itself is sound. It's the example script that's buggy:

```
config = AlgorithmConfig()...
```

Should be:
```
config = (
    get_trainable_cls(args.run)
   .get_default_config()
   ....
)
```",ah figured sound example script buggy,issue,negative,positive,positive,positive,positive,positive
1714297323,"Hey @jovany-wang , could you set `config.exploration(_disable_preprocessor_api=True)`? This should solve it.
I'm not sure, why this was not caught by our tests, but this example script is currently only tested vs tf2, not torch. We will provide a fix.",hey could set solve sure caught example script currently tested torch provide fix,issue,negative,positive,positive,positive,positive,positive
1714288014,"@rkooo567 just updated those logs post gRPC upgrade on the release branch. Here are the comparisons 
```
(ray) gene@geneanycale2023 release_logs % ./compare_perf_metrics 2.6.3 2.7.0 | sort -nr -k2
REGRESSION 688.55%: dashboard_p50_latency_ms (LATENCY) regresses from 13.316 to 105.003 (688.55%) in 2.7.0/benchmarks/many_actors.json
REGRESSION 428.42%: dashboard_p95_latency_ms (LATENCY) regresses from 1641.087 to 8671.912 (428.42%) in 2.7.0/benchmarks/many_actors.json
REGRESSION 118.93%: dashboard_p99_latency_ms (LATENCY) regresses from 3961.132 to 8671.912 (118.93%) in 2.7.0/benchmarks/many_actors.json
REGRESSION 58.68%: dashboard_p99_latency_ms (LATENCY) regresses from 7303.73 to 11589.701 (58.68%) in 2.7.0/benchmarks/many_tasks.json
REGRESSION 41.19%: dashboard_p95_latency_ms (LATENCY) regresses from 5662.863 to 7995.135 (41.19%) in 2.7.0/benchmarks/many_tasks.json
REGRESSION 21.83%: avg_pg_remove_time_ms (LATENCY) regresses from 0.7933515765763139 to 0.966530055554946 (21.83%) in 2.7.0/stress_tests/stress_test_placement_group.json
REGRESSION 21.08%: single_client_tasks_and_get_batch (THROUGHPUT) regresses from 10.988766549840475 to 8.672365911748972 (21.08%) in 2.7.0/microbenchmark.json
REGRESSION 20.49%: multi_client_put_gigabytes (THROUGHPUT) regresses from 38.350117983138134 to 30.49063355658837 (20.49%) in 2.7.0/microbenchmark.json
REGRESSION 20.08%: single_client_tasks_async (THROUGHPUT) regresses from 10940.075908384719 to 8743.097657759507 (20.08%) in 2.7.0/microbenchmark.json
REGRESSION 18.99%: multi_client_tasks_async (THROUGHPUT) regresses from 28883.241079598323 to 23399.550953458893 (18.99%) in 2.7.0/microbenchmark.json
REGRESSION 18.79%: 1_n_actor_calls_async (THROUGHPUT) regresses from 11016.813113385808 to 8946.497160928911 (18.79%) in 2.7.0/microbenchmark.json
REGRESSION 18.62%: 1_1_actor_calls_sync (THROUGHPUT) regresses from 2527.884786760515 to 2057.0784242572117 (18.62%) in 2.7.0/microbenchmark.json
REGRESSION 18.31%: actors_per_second (THROUGHPUT) regresses from 848.4535001176805 to 693.0998499747534 (18.31%) in 2.7.0/benchmarks/many_actors.json
REGRESSION 17.04%: 1_n_async_actor_calls_async (THROUGHPUT) regresses from 9607.6917020388 to 7970.793594306987 (17.04%) in 2.7.0/microbenchmark.json
REGRESSION 16.71%: n_n_actor_calls_async (THROUGHPUT) regresses from 32688.224759117064 to 27225.758970328756 (16.71%) in 2.7.0/microbenchmark.json
REGRESSION 15.98%: single_client_tasks_sync (THROUGHPUT) regresses from 1329.0736290293253 to 1116.7226049709902 (15.98%) in 2.7.0/microbenchmark.json
REGRESSION 13.68%: 1_1_async_actor_calls_with_args_async (THROUGHPUT) regresses from 2071.4152029622965 to 1787.972766438858 (13.68%) in 2.7.0/microbenchmark.json
REGRESSION 12.74%: 1_1_actor_calls_async (THROUGHPUT) regresses from 8232.577421386566 to 7183.918279875392 (12.74%) in 2.7.0/microbenchmark.json
REGRESSION 12.27%: 1_1_async_actor_calls_sync (THROUGHPUT) regresses from 1519.8915131779668 to 1333.343529173284 (12.27%) in 2.7.0/microbenchmark.json
REGRESSION 11.10%: time_to_broadcast_1073741824_bytes_to_50_nodes (LATENCY) regresses from 75.45187580800001 to 83.825163508 (11.10%) in 2.7.0/scalability/object_store.json
REGRESSION 10.39%: n_n_async_actor_calls_async (THROUGHPUT) regresses from 25874.5792020397 to 23186.654951322602 (10.39%) in 2.7.0/microbenchmark.json
REGRESSION 9.92%: avg_pg_create_time_ms (LATENCY) regresses from 0.871186292793918 to 0.9576469174181217 (9.92%) in 2.7.0/stress_tests/stress_test_placement_group.json
REGRESSION 9.38%: 1_1_actor_calls_concurrent (THROUGHPUT) regresses from 5038.298026762205 to 4565.772212164153 (9.38%) in 2.7.0/microbenchmark.json
REGRESSION 7.33%: placement_group_create/removal (THROUGHPUT) regresses from 982.6908282286798 to 910.6188577003039 (7.33%) in 2.7.0/microbenchmark.json
REGRESSION 6.58%: stage_3_time (LATENCY) regresses from 2753.045879125595 to 2934.2411539554596 (6.58%) in 2.7.0/stress_tests/stress_test_many_tasks.json
REGRESSION 5.22%: client__put_gigabytes (THROUGHPUT) regresses from 0.13393152379385806 to 0.12693556139531936 (5.22%) in 2.7.0/microbenchmark.json
REGRESSION 4.75%: multi_client_put_calls_Plasma_Store (THROUGHPUT) regresses from 12887.724106896032 to 12275.386001839906 (4.75%) in 2.7.0/microbenchmark.json
REGRESSION 4.20%: single_client_get_object_containing_10k_refs (THROUGHPUT) regresses from 13.564180981457046 to 12.994713320420606 (4.20%) in 2.7.0/microbenchmark.json
REGRESSION 3.95%: stage_1_avg_iteration_time (LATENCY) regresses from 22.907386016845702 to 23.81243762969971 (3.95%) in 2.7.0/stress_tests/stress_test_many_tasks.json
REGRESSION 3.78%: 1_1_async_actor_calls_async (THROUGHPUT) regresses from 2683.097200627191 to 2581.8090833719934 (3.78%) in 2.7.0/microbenchmark.json
REGRESSION 2.01%: single_client_put_calls_Plasma_Store (THROUGHPUT) regresses from 5759.206110210831 to 5643.2886813637 (2.01%) in 2.7.0/microbenchmark.json
REGRESSION 1.92%: client__tasks_and_get_batch (THROUGHPUT) regresses from 0.9848332918374623 to 0.9659649294333619 (1.92%) in 2.7.0/microbenchmark.json
REGRESSION 1.89%: 1000000_queued_time (LATENCY) regresses from 179.73244193300002 to 183.129132563 (1.89%) in 2.7.0/scalability/single_node.json
REGRESSION 1.59%: dashboard_p50_latency_ms (LATENCY) regresses from 5.529 to 5.617 (1.59%) in 2.7.0/benchmarks/many_tasks.json
REGRESSION 1.40%: stage_0_time (LATENCY) regresses from 9.37568211555481 to 9.507137775421143 (1.40%) in 2.7.0/stress_tests/stress_test_many_tasks.json
REGRESSION 1.33%: 10000_args_time (LATENCY) regresses from 16.737809073999998 to 16.961238414000007 (1.33%) in 2.7.0/scalability/single_node.json
REGRESSION 0.73%: 3000_returns_time (LATENCY) regresses from 5.809550247000004 to 5.851692576000005 (0.73%) in 2.7.0/scalability/single_node.json
REGRESSION 0.53%: n_n_actor_calls_with_arg_async (THROUGHPUT) regresses from 2958.8505427111486 to 2943.051987461239 (0.53%) in 2.7.0/microbenchmark.json
2.7.0 does not have benchmarks/many_pgs.json
```",post upgrade release branch ray gene sort regression latency regression latency regression latency regression latency regression latency regression latency regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression latency regression throughput regression latency regression throughput regression throughput regression latency regression throughput regression throughput regression throughput regression latency regression throughput regression throughput regression throughput regression latency regression latency regression latency regression latency regression latency regression throughput,issue,negative,neutral,neutral,neutral,neutral,neutral
1714285941,"@rynewang Not sure if you are working on a fix. Just want to check in, if not, we should probably just add a `@pytest.mark.skip` annotation so at least it will be green for the release",sure working fix want check probably add annotation least green release,issue,negative,neutral,neutral,neutral,neutral,neutral
1714283067,Annotations done and picked into ray27rc. Target for 27 is done.,done picked target done,issue,negative,neutral,neutral,neutral,neutral,neutral
1714270660,"@sven1977 super cool, thanks for the quick fix! Will this be included in the 2.7 release? Do you already have a release date for it?",super cool thanks quick fix included release already release date,issue,positive,positive,positive,positive,positive,positive
1714256177,"seems like dependencies are missing on top of cu121. @can-anyscale do you know why that's the case?
Is the `pip` list respected in `release_tests.yaml`?",like missing top know case pip list,issue,negative,positive,positive,positive,positive,positive
1714254542,"This seemed to be running fine in the release branch [lightgbm_tune_4x16.aws](https://buildkite.com/ray-project/release-tests-branch/builds/2149#018a7e5c-95ec-47f0-8afd-ea08e81d8e33) Will drop the release blocker tag for now

",running fine release branch drop release blocker tag,issue,negative,positive,positive,positive,positive,positive
1714242109,"> Well, currently dockerhub shows 404 to any rayproject repo...

we are looking at it. seems like a UI only issue. images are still pull-able.",well currently looking like issue still,issue,positive,neutral,neutral,neutral,neutral,neutral
1714241634,"Job: https://console.anyscale-staging.com/o/anyscale-internal/jobs/prodjob_jzkjv161r1e1lmzyawz31a2vtv
Related log:
```
    def test_trainer(storage_path_storage_filesystem_label, tmp_path, monkeypatch):
        """"""Tests that a data parallel trainer can save and restore checkpoints to
        various storage types properly. Also records checkpoint save/restore timing.
    
        Here's the rundown of what this test does:
        1. Passes in a `custom_save_fn` and `custom_restore_fn` to the trainer to
           record how long the operations take, as well as save a large checkpoint.
           See `create_checkpoint` for details on the checkpoint contents.
        2. Configures the training loop to fail 3 times.
        3. Runs the trainer, which will fail 2 times and recover via FailureConfig.
           This first run will exit on the 3rd failure.
        4. Manually restores the trainer, which will restore from the 3rd failure and
           run to completion.
        5. Downloads the results from the storage path and asserts that the contents
           are all correct. See `ray.train.test_new_persistence` for the expected filetree.
        6. Tests a new run with `resume_from_checkpoint`.
        """"""
        monkeypatch.setenv(""RAY_AIR_LOCAL_CACHE_DIR"", str(tmp_path / ""ray_results""))
    
        ray.init(runtime_env={""working_dir"": "".""}, ignore_reinit_error=True)
    
        storage_path, storage_filesystem, label = storage_path_storage_filesystem_label
        checkpoint_config = train.CheckpointConfig(
            num_to_keep=TestConstants.NUM_ITERATIONS // 2
        )
        exp_name = ""test_trainer""
    
        # NOTE: We use fsspec directly for cleaning up the cloud folders and
        # downloading for inspection, since the pyarrow default implementation
        # doesn't delete/download files properly from GCS.
        fsspec_fs, storage_fs_path = (
            fsspec.core.url_to_fs(
                os.environ[""ANYSCALE_ARTIFACT_STORAGE""] + ""/test-persistence""
            )
            if ""cloud"" in label
            else fsspec.core.url_to_fs(storage_path)
        )
        experiment_fs_path = os.path.join(storage_fs_path, exp_name)
        if fsspec_fs.exists(experiment_fs_path):
            print(""\nDeleting results from a previous run...\n"")
            fsspec_fs.rm(experiment_fs_path, recursive=True)
>       assert not fsspec_fs.exists(experiment_fs_path)
E       AssertionError: assert not True
E        +  where True = <bound method S3FileSystem._exists of <s3fs.core.S3FileSystem object at 0x7fd6895ec700>>('anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/org_7c1Kalm9WcX2bNIjW53GUT/cld_kvedZWag2qA8i5BjxUevf5i7/artifact_storage/test-persistence/test_trainer')
E        +    where <bound method S3FileSystem._exists of <s3fs.core.S3FileSystem object at 0x7fd6895ec700>> = <s3fs.core.S3FileSystem object at 0x7fd6895ec700>.exists

test_persistence.py:201: AssertionError
```",job related log data parallel trainer save restore various storage properly also timing test trainer record long take well save large see content training loop fail time trainer fail time recover via first run exit failure manually trainer restore failure run completion storage path content correct see new run label note use directly cleaning cloud inspection since default implementation properly cloud label else print previous run assert assert true true bound method object bound method object object,issue,negative,negative,neutral,neutral,negative,negative
1714237583,"Job: https://console.anyscale-staging.com/o/anyscale-internal/jobs/prodjob_qjs1mgwqbpyji6itfhmc94nuds
Related log:
```
Cleaning up cluster...
2023-09-11 09:37:54,042	INFO util.py:374 -- setting max workers for head node type to 0
2023-09-11 09:37:54,042	INFO util.py:378 -- setting max workers for ray_worker_small to 2
2023-09-11 09:37:54,224	INFO commands.py:385 -- Checking GCP environment settings
2023-09-11 09:37:55,673	INFO config.py:514 -- _configure_key_pair: Creating new key pair ray-autoscaler_gcp_us-west1_anyscale-bridge-cd812d38_ubuntu_0
Traceback (most recent call last):
  File ""/home/ray/anaconda3/bin/ray"", line 8, in <module>
    sys.exit(main())
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py"", line 2490, in main
    return cli()
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py"", line 1157, in __call__
    return self.main(*args, **kwargs)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py"", line 1078, in main
    rv = self.invoke(ctx)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py"", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py"", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py"", line 783, in invoke
    return __callback(*args, **kwargs)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py"", line 856, in wrapper
    return f(*args, **kwargs)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py"", line 1329, in down
    teardown_cluster(
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/autoscaler/_private/commands.py"", line 438, in teardown_cluster
    config = _bootstrap_config(config)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/autoscaler/_private/commands.py"", line 413, in _bootstrap_config
    resolved_config = provider_cls.bootstrap_config(config)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/autoscaler/_private/gcp/node_provider.py"", line 244, in bootstrap_config
    return bootstrap_gcp(cluster_config)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/autoscaler/_private/gcp/config.py"", line 365, in bootstrap_gcp
    config = _configure_key_pair(config, compute)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/autoscaler/_private/gcp/config.py"", line 519, in _configure_key_pair
    _create_project_ssh_key_pair(project, public_key, ssh_user, compute)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/autoscaler/_private/gcp/config.py"", line 777, in _create_project_ssh_key_pair
    compute.projects()
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/googleapiclient/_helpers.py"", line 130, in positional_wrapper
    return wrapped(*args, **kwargs)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/googleapiclient/http.py"", line 851, in execute
    raise HttpError(resp, content, uri=self.uri)
googleapiclient.errors.HttpError: <HttpError 413 when requesting https://compute.googleapis.com/compute/v1/projects/anyscale-bridge-cd812d38/setCommonInstanceMetadata?alt=json returned ""Value for field 'metadata.items[0].value' is too large: maximum size 262144 character(s); actual size 262257."">
Traceback (most recent call last):
  File ""launch_and_verify_cluster.py"", line 323, in <module>
    run_ray_commands(cluster_config, retries, no_config_cache, num_expected_nodes)
  File ""launch_and_verify_cluster.py"", line 187, in run_ray_commands
    cleanup_cluster(cluster_config)
  File ""launch_and_verify_cluster.py"", line 172, in cleanup_cluster
    subprocess.run([""ray"", ""down"", ""-v"", ""-y"", str(cluster_config)], check=True)
  File ""/home/ray/anaconda3/lib/python3.8/subprocess.py"", line 516, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['ray', 'down', '-v', '-y', '/tmp/tmpc9x6ymqj.yaml']' returned non-zero exit status 1.
Subprocess return code: 1
[INFO 2023-09-11 09:37:56,591] anyscale_job_wrapper.py: 190  Process 2798 exited with return code 1.
[INFO 2023-09-11 09:37:56,591] anyscale_job_wrapper.py: 292  Finished with return code 1. Time taken: 5.750278438999999
[WARNING 2023-09-11 09:37:56,591] anyscale_job_wrapper.py: 68  Couldn't upload to cloud storage: '/tmp/release_test_out.json' does not exist.
```",job related log cleaning cluster setting head node type setting environment new key pair recent call last file line module main file line main return file line return file line main file line invoke return file line invoke return file line invoke return file line wrapper return file line file line file line file line return file line compute file line project compute file line file line return wrapped file line execute raise resp content returned value field large maximum size character actual size recent call last file line module file line file line ray file line run raise command returned exit status return code process return code finished return code time taken warning could cloud storage exist,issue,negative,positive,neutral,neutral,positive,positive
1714234211,"Job: https://console.anyscale-staging.com/o/anyscale-internal/jobs/prodjob_fzxhu688y7asrxwf4gj2g2cqrx
Related log:
```
Cleaning up cluster...
2023-09-10 02:23:19,067	INFO util.py:374 -- setting max workers for head node type to 0
2023-09-10 02:23:19,225	INFO commands.py:385 -- Checking GCP environment settings
2023-09-10 02:23:20,861	INFO config.py:514 -- _configure_key_pair: Creating new key pair ray-autoscaler_gcp_us-west1_anyscale-bridge-cd812d38_ubuntu_0
Traceback (most recent call last):
  File ""/home/ray/anaconda3/bin/ray"", line 8, in <module>
    sys.exit(main())
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py"", line 2490, in main
    return cli()
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py"", line 1157, in __call__
    return self.main(*args, **kwargs)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py"", line 1078, in main
    rv = self.invoke(ctx)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py"", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py"", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py"", line 783, in invoke
    return __callback(*args, **kwargs)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py"", line 856, in wrapper
    return f(*args, **kwargs)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py"", line 1329, in down
    teardown_cluster(
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/autoscaler/_private/commands.py"", line 438, in teardown_cluster
    config = _bootstrap_config(config)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/autoscaler/_private/commands.py"", line 413, in _bootstrap_config
    resolved_config = provider_cls.bootstrap_config(config)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/autoscaler/_private/gcp/node_provider.py"", line 244, in bootstrap_config
    return bootstrap_gcp(cluster_config)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/autoscaler/_private/gcp/config.py"", line 365, in bootstrap_gcp
    config = _configure_key_pair(config, compute)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/autoscaler/_private/gcp/config.py"", line 519, in _configure_key_pair
    _create_project_ssh_key_pair(project, public_key, ssh_user, compute)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/autoscaler/_private/gcp/config.py"", line 777, in _create_project_ssh_key_pair
    compute.projects()
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/googleapiclient/_helpers.py"", line 130, in positional_wrapper
    return wrapped(*args, **kwargs)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/googleapiclient/http.py"", line 851, in execute
    raise HttpError(resp, content, uri=self.uri)
googleapiclient.errors.HttpError: <HttpError 413 when requesting https://compute.googleapis.com/compute/v1/projects/anyscale-bridge-cd812d38/setCommonInstanceMetadata?alt=json returned ""Value for field 'metadata.items[0].value' is too large: maximum size 262144 character(s); actual size 262257."">
Traceback (most recent call last):
  File ""launch_and_verify_cluster.py"", line 323, in <module>
    run_ray_commands(cluster_config, retries, no_config_cache, num_expected_nodes)
  File ""launch_and_verify_cluster.py"", line 187, in run_ray_commands
    cleanup_cluster(cluster_config)
  File ""launch_and_verify_cluster.py"", line 172, in cleanup_cluster
    subprocess.run([""ray"", ""down"", ""-v"", ""-y"", str(cluster_config)], check=True)
  File ""/home/ray/anaconda3/lib/python3.8/subprocess.py"", line 516, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['ray', 'down', '-v', '-y', '/tmp/tmp5x5wtc34.yaml']' returned non-zero exit status 1.
Subprocess return code: 1
[INFO 2023-09-10 02:23:21,709] anyscale_job_wrapper.py: 190  Process 2784 exited with return code 1.
[INFO 2023-09-10 02:23:21,709] anyscale_job_wrapper.py: 292  Finished with return code 1. Time taken: 5.490707444999998
[WARNING 2023-09-10 02:23:21,709] anyscale_job_wrapper.py: 68  Couldn't upload to cloud storage: '/tmp/release_test_out.json' does not exist.
```",job related log cleaning cluster setting head node type environment new key pair recent call last file line module main file line main return file line return file line main file line invoke return file line invoke return file line invoke return file line wrapper return file line file line file line file line return file line compute file line project compute file line file line return wrapped file line execute raise resp content returned value field large maximum size character actual size recent call last file line module file line file line ray file line run raise command returned exit status return code process return code finished return code time taken warning could cloud storage exist,issue,negative,positive,neutral,neutral,positive,positive
1714220513,"> Have you discussed the solution with Jiajun btw?

Yes, but just a high level by moving the code inside the driver, but seems like there's error in other test that didn't call `ray.init` so I'll check that today",solution yes high level moving code inside driver like error test call check today,issue,positive,positive,positive,positive,positive,positive
1714213032,"Related log: https://buildkite.com/ray-project/oss-ci-build-branch/builds/6072#018a7fbf-97ed-497a-9487-a9fd303144de/4272-4379
```


[Errno 2] No such file or directory: '::test_owner_assign_inner_object.txt'
--
  | FAILED
  |  
  | ================================== FAILURES ===================================
  | _______________________ test_owner_assign_inner_object ________________________
  |  
  | shutdown_only = None
  |  
  | def test_owner_assign_inner_object(shutdown_only):
  |  
  | ray.init()
  |  
  | @ray.remote
  | class Owner:
  | def warmup(self):
  | pass
  |  
  | @ray.remote
  | def get_borrowed_object():
  | ref = ray.put((""test_borrowed""))
  | return [ref]
  |  
  | owner = Owner.remote()
  | ray.get(owner.warmup.remote())
  |  
  | class OutObject:
  | def __init__(self, owned_inner_ref, borrowed_inner_ref):
  | self.owned_inner_ref = owned_inner_ref
  | self.borrowed_inner_ref = borrowed_inner_ref
  |  
  | owned_inner_ref = ray.put(""test_owned"")
  |  
  | borrowed_inner_ref = ray.get(get_borrowed_object.remote())[0]
  | out_ref = ray.put(OutObject(owned_inner_ref, borrowed_inner_ref), _owner=owner)
  |  
  | # wait enough time to delete data when the reference count is lower
  | # than expected
  | del owned_inner_ref, borrowed_inner_ref
  | time.sleep(10)
  |  
  | assert ray.get(ray.get(out_ref).owned_inner_ref) == ""test_owned""
  | >       assert ray.get(ray.get(out_ref).borrowed_inner_ref) == ""test_borrowed""
  |  
  | \\?\C:\Users\ContainerAdministrator\AppData\Local\Temp\Bazel.runfiles_af6igswj\runfiles\com_github_ray_project_ray\python\ray\tests\test_object_assign_owner.py:191:
  | _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
  | c:\install\ray\python\ray\_private\auto_init_hook.py:24: in auto_init_wrapper
  | return fn(*args, **kwargs)
  | c:\install\ray\python\ray\_private\auto_init_hook.py:24: in auto_init_wrapper
  | return fn(*args, **kwargs)
  | c:\install\ray\python\ray\_private\client_mode_hook.py:102: in wrapper
  | return getattr(ray, func.__name__)(*args, **kwargs)
  | c:\install\ray\python\ray\util\client\api.py:42: in get
  | return self.worker.get(vals, timeout=timeout)
  | c:\install\ray\python\ray\util\client\worker.py:434: in get
  | res = self._get(to_get, op_timeout)
  | _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
  |  
  | self = <ray.util.client.worker.Worker object at 0x0000021A6DBA1D00>
  | ref = [ClientObjectRef(000c6522fa08b41fffffffffffffffffffffffff0100000002e1f505)]
  | timeout = 2.0
  |  
  | def _get(self, ref: List[ClientObjectRef], timeout: float):
  | req = ray_client_pb2.GetRequest(ids=[r.id for r in ref], timeout=timeout)
  | data = bytearray()
  | try:
  | resp = self._get_object_iterator(req, metadata=self.metadata)
  | for chunk in resp:
  | if not chunk.valid:
  | try:
  | err = cloudpickle.loads(chunk.error)
  | except (pickle.UnpicklingError, TypeError):
  | logger.exception(""Failed to deserialize {}"".format(chunk.error))
  | raise
  | >                   raise err
  | E                   ValueError: ClientObjectRef b'\x00\x0ce""\xfa\x08\xb4\x1f\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\x01\x00\x00\x00\x02\xe1\xf5\x05' is not found for client 341456133f5d44e9bfe6254632095c0d
  |  
  | c:\install\ray\python\ray\util\client\worker.py:462: ValueError
  | ============================== warnings summary ===============================
  | ::test_owner_assign_bug
  | C:\Miniconda3\lib\site-packages\_pytest\threadexception.py:73: PytestUnhandledThreadExceptionWarning: Exception in thread ray_print_logs
  |  
  | Traceback (most recent call last):
  | File ""C:\Miniconda3\lib\threading.py"", line 932, in _bootstrap_inner
  | self.run()
  | File ""C:\Miniconda3\lib\threading.py"", line 870, in run
  | self._target(*self._args, **self._kwargs)
  | File ""c:\install\ray\python\ray\_private\worker.py"", line 819, in print_logs
  | global_worker_stdstream_dispatcher.emit(data)
  | File ""c:\install\ray\python\ray\_private\ray_logging.py"", line 181, in emit
  | handle(data)
  | File ""c:\install\ray\python\ray\_private\worker.py"", line 1796, in print_to_stdstream
  | print_worker_logs(batch, sink)
  | File ""c:\install\ray\python\ray\_private\worker.py"", line 1954, in print_worker_logs
  | for line in lines:
  | File ""c:\install\ray\python\ray\_private\worker.py"", line 1832, in filter_autoscaler_events
  | if is_autoscaler_v2():
  | File ""c:\install\ray\python\ray\autoscaler\v2\utils.py"", line 547, in is_autoscaler_v2
  | raise Exception(
  | Exception: GCS address could not be resolved (e.g. ray.init() not called)
  |  
  | warnings.warn(pytest.PytestUnhandledThreadExceptionWarning(msg))
  |  
  | -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
  | =========================== short test summary info ===========================
  | FAILED ::test_owner_assign_inner_object - ValueError: ClientObjectRef b'\x00\...
  | ============= 1 failed, 1 passed, 6 skipped, 1 warning in 29.71s ==============
  | (16:45:48) FAIL: //python/ray/tests:test_object_assign_owner_client_mode (see C:/tmp/4lhdprva/execroot/com_github_ray_project_ray/bazel-out/x64_windows-opt/testlogs/python/ray/tests/test_object_assign_owner_client_mode/test_attempts/attempt_1.log)


```",related log file directory none class owner self pas ref return ref owner class self wait enough time delete data reference count lower assert assert return return wrapper return ray get return get self object ref self ref list float ref data try resp chunk resp try err except raise raise err found client summary exception thread recent call last file line file line run file line data file line emit handle data file line batch sink file line line file line file line raise exception exception address could resolved short test summary warning fail see,issue,negative,negative,neutral,neutral,negative,negative
1714201745,"Related log: https://buildkite.com/ray-project/oss-ci-build-branch/builds/6072#018a81ce-0bb3-4cf5-8639-9584a483585c/1629-2504
```


Training completed after 1 iterations at 2023-09-11 02:13:20. Total running time: 5s
--
  |  
  | Observed metrics: {'world_size': 2, 'world_rank': 0, 'local_rank': 0, 'data_shard':    item
  | 0     1
  | 1     4, 'timestamp': 1694398398, 'done': True, 'training_iteration': 1, 'trial_id': 'c379b_00000', 'date': '2023-09-11_02-13-19', 'time_this_iter_s': 3.0804243087768555, 'time_total_s': 3.0804243087768555, 'pid': 22653, 'hostname': '5ea91f2a4317', 'node_ip': '172.16.16.3', 'config': {}, 'time_since_restore': 3.0804243087768555, 'iterations_since_restore': 1, 'checkpoint_dir_name': None, 'experiment_tag': '0'}
  | Traceback (most recent call last):
  | File ""/opt/miniconda/lib/python3.8/site-packages/pandas/core/indexes/base.py"", line 3802, in get_loc
  | return self._engine.get_loc(casted_key)
  | File ""pandas/_libs/index.pyx"", line 138, in pandas._libs.index.IndexEngine.get_loc
  | File ""pandas/_libs/index.pyx"", line 165, in pandas._libs.index.IndexEngine.get_loc
  | File ""pandas/_libs/hashtable_class_helper.pxi"", line 5745, in pandas._libs.hashtable.PyObjectHashTable.get_item
  | File ""pandas/_libs/hashtable_class_helper.pxi"", line 5753, in pandas._libs.hashtable.PyObjectHashTable.get_item
  | KeyError: 'loss'
  |  
  | The above exception was the direct cause of the following exception:
  |  
  | Traceback (most recent call last):
  | File ""/root/.cache/bazel/_bazel_root/5fe90af4e7d1ed9fcf52f59e39e126f5/execroot/com_github_ray_project_ray/bazel-out/k8-opt/bin/doc/source/train/doc_code/key_concepts.runfiles/com_github_ray_project_ray/doc/source/train/doc_code/key_concepts.py"", line 132, in <module>
  | print(""Minimum loss"", min(df[""loss""]))
  | File ""/opt/miniconda/lib/python3.8/site-packages/pandas/core/frame.py"", line 3807, in __getitem__
  | indexer = self.columns.get_loc(key)
  | File ""/opt/miniconda/lib/python3.8/site-packages/pandas/core/indexes/base.py"", line 3804, in get_loc
  | raise KeyError(key) from err
  | KeyError: 'loss'


```",related log training total running time metric item true none recent call last file line return file line file line file line file line exception direct cause following exception recent call last file line module print minimum loss min loss file line indexer key file line raise key err,issue,negative,positive,neutral,neutral,positive,positive
1714091920,Hi @WeichenXu123 this is related to Ray on Spark,hi related ray spark,issue,negative,neutral,neutral,neutral,neutral,neutral
1714042058,"Done!
Best,
Ian
Security @ Anyscale


On Mon, Sep 11, 2023 at 10:25 AM Lonnie Liu ***@***.***>
wrote:

> ***@***.**** approved this pull request.
>
> update PR title and description?
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/ray-project/ray/pull/38524#pullrequestreview-1620145198>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AFC5KQUZF5VMKFEJZICATI3XZ4NNJANCNFSM6AAAAAA3TEJEDM>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>
",done best security mon wrote pull request update title description reply directly view id,issue,positive,positive,positive,positive,positive,positive
1713942636,"Changing the Databricks run time to 
13.3 LTS ML (includes Apache Spark 3.4.1, GPU, Scala 2.12) and issue is resolved 

perviously was using 

12.2 LTS ML (includes Apache Spark 3.2.1, GPU, Scala 2.12)",run time apache spark scala issue resolved perviously apache spark scala,issue,positive,neutral,neutral,neutral,neutral,neutral
1713748137,"Actually, this might have been fixed by this PR here:
https://github.com/ray-project/ray/pull/39464

But let me check ...",actually might fixed let check,issue,negative,positive,neutral,neutral,positive,positive
1713732018,"Sorry for deprioritizing this issue! But we are very close to moving A3C (and some other algos) into a new ""RLlib contrib"" repo, so support for this algorithm will be very limited.

https://github.com/ray-project/rllib-contrib",sorry issue close moving new support algorithm limited,issue,negative,negative,negative,negative,negative,negative
1713725110,"Hey @zzzrpagliari , thanks for filing this issue, however, the error doesn't tell us that much of what went wrong on your end.
Could you provide more details on your environment, OS, and on how you installed ray on your machine?",hey thanks filing issue however error tell u much went wrong end could provide environment o ray machine,issue,negative,negative,neutral,neutral,negative,negative
1713715561,"Hey @chrisyeh96 , thanks for the hint. Changed this to be even more flexible now for both parallel and non-parallel env wrappers.",hey thanks hint even flexible parallel,issue,positive,positive,neutral,neutral,positive,positive
1713617012,"```
[2m[36m(_RemoteRayLightGBMActor pid=273, ip=10.0.42.93)[0m [failure_signal_handler.cc : 332] RAW: Signal 11 raised at PC=0x7fe3eee63087 while already in AbslFailureSignalHandler()
[2m[36m(_RemoteRayLightGBMActor pid=273, ip=10.0.42.93)[0m PC: @     0x7fe3eee63087  (unknown)  LightGBM::DenseBin<>::ConstructHistogram()
[2m[36m(_RemoteRayLightGBMActor pid=273, ip=10.0.42.93)[0m [2023-09-10 19:51:30,025 E 273 329] logging.cc:361: *** SIGSEGV received at time=1694400690 on cpu 0 ***
[2m[36m(_RemoteRayLightGBMActor pid=273, ip=10.0.42.93)[0m [2023-09-10 19:51:30,025 E 273 329] logging.cc:361: PC: @     0x7fe3eee63087  (unknown)  LightGBM::DenseBin<>::ConstructHistogram()
[2m[36m(_RemoteRayLightGBMActor pid=273, ip=10.0.42.93)[0m Fatal Python error: Segmentation fault
[2m[36m(_RemoteRayLightGBMActor pid=273, ip=10.0.42.93)[0m 
[2m[36m(_RemoteRayLightGBMActor pid=273, ip=10.0.42.93)[0m     @     0x7fe54b19a420  (unknown)  (unknown)
[2m[36m(_RemoteRayLightGBMActor pid=273, ip=10.0.42.93)[0m [2023-09-10 19:51:30,025 E 273 329] logging.cc:361:     @     0x7fe54b19a420  (unknown)  (unknown)
2023-09-10 19:51:30,217	WARNING worker.py:2065 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff60b9400997b3ef8ef294453103000000 Worker ID: 914ca28c4fd10c0d10cf2092deeeb7e28b48e4853eda180998b8f970 Node ID: 4e5f826dad3dd88ae6082e29107cecaecf1b8660da38a04cc5334ad2 Worker IP address: 10.0.42.93 Worker port: 10004 Worker PID: 273 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2m[36m(_RemoteRayLightGBMActor pid=157, ip=10.0.59.254)[0m [LightGBM] [Fatal] Socket recv error, Connection reset by peer (code: 104)
2023-09-10 19:51:30,326	INFO elastic.py:176 -- Actor status: 31 alive, 1 dead (32 total)
```

Seems like the same error.

@iycheng can you double check if jemalloc is not used in the worker one more time? Also if it is true, it is probably not true that the error is caused by jemalloc. I suspect in this case, this is a bug from lightgbm? Is it possible if we are using the same version? (or run an old comit with the same version that's used in that particular test)",raw signal raised already unknown received unknown fatal python error segmentation fault unknown unknown unknown unknown warning worker task unexpected system error problem check dead worker id worker id node id worker address worker port worker worker exit type worker exit detail worker unexpectedly connection error code end file potential root process killer due high memory usage ray stop force worker unexpectedly due unexpected fatal socket error connection reset peer code actor status alive dead total like error double check used worker one time also true probably true error suspect case bug possible version run old version used particular test,issue,negative,positive,neutral,neutral,positive,positive
1713605779,"Can you update the PR description? Also, test_prometheus_physical_stats_record failure seesm related",update description also failure related,issue,negative,negative,negative,negative,negative,negative
1713288182,Can confirm that this is still an issue in Ray 2.6.3.,confirm still issue ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1713045324,Thanks a lot! I didn't notice this guide before.,thanks lot notice guide,issue,negative,positive,positive,positive,positive,positive
1712843297,"@aslonnie: addressed all comments, except for the usage of wanda, I cannot figure that self on my own, is there an example I can use (run_wanda.sh ??), thankks",except usage figure self example use,issue,negative,neutral,neutral,neutral,neutral,neutral
1712585472,Good test! Maybe we can merge this after the matrix testing fix as a new workload?,good test maybe merge matrix testing fix new,issue,negative,positive,positive,positive,positive,positive
1712487860,Can you merge the latest master? Also lint failure + left of comments,merge latest master also lint failure left,issue,negative,positive,neutral,neutral,positive,positive
1712487427,@rynewang we should probably cherry pick the test fix (since we merged the gRPC PR to 2.7 branch),probably cherry pick test fix since branch,issue,negative,neutral,neutral,neutral,neutral,neutral
1712437773,"Hey @a4rcvv, thanks for raising this! Indeed, we need to test further how the new stack behaves on these more complex multi-agent scenarios. However, this script does work on master and will work properly on Ray 2.7.

Some output from running this on my laptop in the master branch for fw=torch.
```
Trial PPO_open_spiel_env_bbed7_00000 finished iteration 2 at 2023-09-09 09:02:12. Total running time: 29s
╭────────────────────────────────────────────────────────────╮
│ Trial PPO_open_spiel_env_bbed7_00000 result                │
├────────────────────────────────────────────────────────────┤
│ episodes_total                                         366 │
│ num_env_steps_sampled                                 8000 │
│ num_env_steps_trained                                    0 │
│ sampler_results/episode_len_mean                   21.2781 │
│ sampler_results/episode_reward_mean             -0.0925134 │
╰────────────────────────────────────────────────────────────╯

```

One thing, though, it does require you to install `pip install open_spiel`.",hey thanks raising indeed need test new stack complex however script work master work properly ray output running master branch trial finished iteration total running time trial result one thing though require install pip install,issue,negative,positive,neutral,neutral,positive,positive
1712432421,"You are right, there is no good pre-check right now, even in AlgorithmConfig for some settings. Yeah, these probably fell through the cracks.
We'll have to go through the class again and make sure that e.g. these binary settings (one string or another) are checked properly (or yes, solved with enums).",right good right even yeah probably fell go class make sure binary one string another checked properly yes,issue,positive,positive,positive,positive,positive,positive
1712431003,"Makes sense! Thanks for your response and the additional information. For completeness and to make this easier for us to debug, could you complete your reproduction script? Maybe with a very simple model that should be able to handle the graph input.",sense thanks response additional information completeness make easier u could complete reproduction script maybe simple model able handle graph input,issue,positive,positive,positive,positive,positive,positive
1712416894,Note: I will merge this next Mon,note merge next mon,issue,negative,neutral,neutral,neutral,neutral,neutral
1712415952,"Decided not to mark it as a release blocker.
After analyzing logs, it seems to be very niche edge case, and the bug Ruiyang’s PR will handle was just a side effect of other issue.
We will try handling the other issue instead (we will just fail fast when this happens), and will find a proper fix for 2.8 ",decided mark release blocker niche edge case bug handle side effect issue try handling issue instead fail fast find proper fix,issue,negative,negative,neutral,neutral,negative,negative
1712409363,Going to mark this as resolved as the final step of publishing the versions for 2.7 is tracked in the release process.,going mark resolved final step tracked release process,issue,negative,neutral,neutral,neutral,neutral,neutral
1712408838,looks like this has been picked successfully by @GeneDer  - @matthewdeng  can we close this out?,like picked successfully close,issue,positive,positive,positive,positive,positive,positive
1712408181,"@edoakes @shrekris-anyscale OK let's leave it to 2.8 then. Even though the PR is about adding metrics, the changes are not that simple / risk-free

Given the amount of boarderline PRs we should need either a 2.7.1 or a quicker 2.8. Thanks for the understanding",let leave even though metric simple given amount need either thanks understanding,issue,negative,positive,neutral,neutral,positive,positive
1712407490,@sven1977 we are past pick deadline for 2.7. Can we leave this to 2.8?,past pick deadline leave,issue,negative,negative,negative,negative,negative,negative
1712401050,Hmm looks like spark on ray somehow failed (unexpected). Investigating..,like spark ray somehow unexpected investigating,issue,positive,positive,neutral,neutral,positive,positive
1712385353,"@zhe-thoughts we needed these metrics to debug https://github.com/ray-project/ray/issues/39418, so if a user encounters another similar issue we may be unable to diagnose it.

On its own it does not fix any release-blocking issue, so understood if it's too late.",metric user another similar issue may unable diagnose fix issue understood late,issue,negative,negative,negative,negative,negative,negative
1712368402,@edoakes or @akshay-anyscale could you confirm that this is a release blocker? What's the consequence of leaving it to 2.8 or 2.7.1? We are at very late stage for non-docs picks,could confirm release blocker consequence leaving late stage,issue,negative,negative,negative,negative,negative,negative
1712368285,@zhe-thoughts I think you accidentally clicked on comment instead of approve :)  ,think accidentally comment instead approve,issue,negative,neutral,neutral,neutral,neutral,neutral
1712360521,@rynewang can you provide your wheel to user asap to verify if it fixes their issue? ,provide wheel user verify issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1712360445,We discovered this issue from real user. We should fix this for 2.7,discovered issue real user fix,issue,negative,positive,positive,positive,positive,positive
1712359575,"We decided to accept the performance regression from this test (it is also not worse than ray 2.2) for other benefits due to grpc upgrade, but if I can discover the root cause next week, we can consider picking",decided accept performance regression test also worse ray due upgrade discover root cause next week consider,issue,negative,negative,negative,negative,negative,negative
1712278486,"There are a few instances of passive voice that I didn't comment on. I'm assuming that is industry vernacular. But if it isn't, tI recommend you use the active voice consistently.",passive voice comment assuming industry vernacular ti recommend use active voice consistently,issue,positive,positive,neutral,neutral,positive,positive
1712265811,"@raulchen - lint is failing

```
--- a/python/ray/data/datasource/file_based_datasource.py
--
  | +++ b/python/ray/data/datasource/file_based_datasource.py
  | @@ -596,7 +596,6 @@ class _FileBasedDatasourceReader(Reader):
  | yield data
  |  
  | def create_read_task_fn(read_paths, num_threads):
  | -
  | def read_task_fn():
  | nonlocal num_threads, read_paths
  | print(f""read_task_fn: {read_paths}"")
  | @@ -637,7 +636,9 @@ class _FileBasedDatasourceReader(Reader):
  | file_sizes=file_sizes,
  | )
  |  
  | -            read_task_fn = create_read_task_fn(read_paths, self._delegate._NUM_THREADS_PER_TASK)
  | +            read_task_fn = create_read_task_fn(
  | +                read_paths, self._delegate._NUM_THREADS_PER_TASK
  | +            )
  |  
  | read_task = ReadTask(read_task_fn, meta)


```",lint failing class reader yield data nonlocal print class reader meta,issue,negative,neutral,neutral,neutral,neutral,neutral
1712254816,"Also, how risky is this change without a unit test? ",also risky change without unit test,issue,negative,neutral,neutral,neutral,neutral,neutral
1712221642,@edoakes can we get this merged in so I can raise the cherry pick PR🙏,get raise cherry pick,issue,negative,neutral,neutral,neutral,neutral,neutral
1712198754,"This works

```
  warn(msg)
CUDA SETUP: CUDA runtime path found: /home/ubuntu/mambaforge/envs/fsdp/lib/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 118
CUDA SETUP: Loading binary /home/ubuntu/mambaforge/envs/fsdp/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...
2023-09-08 20:39:12,695 INFO worker.py:1649 -- Started a local Ray instance.
2023-09-08 20:39:26,413 INFO tune.py:656 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2023-09-08 20:39:26,418 INFO tensorboardx.py:178 -- pip install ""ray[tune]"" to see TensorBoard files.
2023-09-08 20:39:26,418 WARNING callback.py:137 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`

View detailed results here: /home/ubuntu/ray_results/TorchTrainer_2023-09-08_20-39-26
(TrainTrainable pid=286063) [2023-09-08 20:39:30,456] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
(TrainTrainable pid=286063)
(TrainTrainable pid=286063) ===================================BUG REPORT===================================
(TrainTrainable pid=286063) Welcome to bitsandbytes. For bug reports, please run
(TrainTrainable pid=286063)
(TrainTrainable pid=286063) python -m bitsandbytes
(TrainTrainable pid=286063)
(TrainTrainable pid=286063)  and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
(TrainTrainable pid=286063) ================================================================================
(TrainTrainable pid=286063) bin /home/ubuntu/mambaforge/envs/fsdp/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so
(TrainTrainable pid=286063) /home/ubuntu/mambaforge/envs/fsdp/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32
(TrainTrainable pid=286063) CUDA SETUP: Loading binary /home/ubuntu/mambaforge/envs/fsdp/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...
(TrainTrainable pid=286063) /home/ubuntu/mambaforge/envs/fsdp/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
(TrainTrainable pid=286063)   warn(""The installed version of bitsandbytes was compiled without GPU support. ""

Training started with configuration:
╭──────────────────────────────────────────────────────────────────────────────────────────╮
│ Training config                                                                          │
├──────────────────────────────────────────────────────────────────────────────────────────┤
│ train_loop_config/deepspeed_config/bf16/enabled                                    False │
│ train_loop_config/deepspeed_config/fp16/enabled                                     True │
│ train_loop_config/deepspeed_config/gradient_accumulation_steps                         1 │
│ train_loop_config/deepspeed_config/gradient_clipping                                True │
│ train_loop_config/deepspeed_config/optimizer/params/lr                             2e-05 │
│ train_loop_config/deepspeed_config/optimizer/type                                  AdamW │
│ train_loop_config/deepspeed_config/scheduler/params/warmup_num_steps                 100 │
│ train_loop_config/deepspeed_config/scheduler/type                               WarmupLR │
│ train_loop_config/deepspeed_config/steps_per_print                                    10 │
│ train_loop_config/deepspeed_config/train_micro_batch_size_per_gpu                     16 │
│ train_loop_config/deepspeed_config/wall_clock_breakdown                            False │
│ train_loop_config/deepspeed_config/zero_optimization/offload_optimizer/device       none │
│ train_loop_config/deepspeed_config/zero_optimization/offload_param/device           none │
│ train_loop_config/deepspeed_config/zero_optimization/stage                             3 │
│ train_loop_config/eval_batch_size                                                     32 │
│ train_loop_config/num_epochs                                                           3 │
│ train_loop_config/seed                                                                42 │
│ train_loop_config/train_batch_size                                                    16 │
╰──────────────────────────────────────────────────────────────────────────────────────────╯
(TorchTrainer pid=286063) Starting distributed worker processes: ['286289 (104.171.202.138)', '286290 (104.171.202.138)']
(RayTrainWorker pid=286289) Setting up process group for: env:// [rank=0, world_size=2]
```

Woohooo",work warn setup path found setup highest compute capability among setup version setup loading binary local ray instance output use new output engine verbosity disable new output use legacy output engine set environment variable information please see pip install ray tune see warning logger either one please make sure latest version pip install view detailed setting auto detect welcome bug please run python submit information together error trace bin undefined symbol setup loading binary version without support multiplication quantization unavailable warn version without training configuration training false true true false none none starting distributed worker setting process group,issue,positive,positive,positive,positive,positive,positive
1712183518,"Thanks @sven1977 for responding so quickly to #39453!

I have a suggestion: can we make RLLib's `PettingZooEnv` and `ParallelPettingZooEnv` wrappers support PettingZoo environments where agents have different `action_spaces` and `observation_spaces`? This would actually simplify the RLLib wrapper code quite significantly.

For example, here's what the `__init__()` method for `ParallelPettingZooEnv` would look like:

```python
class ParallelPettingZooEnv(MultiAgentEnv):
    def __init__(self, env):
        super().__init__()
        self.par_env = env
        self.par_env.reset()
        self._agent_ids = set(self.par_env.agents)

        self.observation_space = gym.spaces.Dict(self.par_env.observation_spaces)
        self.action_space = gym.spaces.Dict(self.par_env.action_spaces)
```",thanks quickly suggestion make support different would actually simplify wrapper code quite significantly example method would look like python class self super set,issue,positive,positive,positive,positive,positive,positive
1712172129,"I have coordinated with @zcin offline. Although we did not find the root cause, it appears that the issue has been resolved in the nightly build. I plan to keep this issue open until we observe stable performance from KubeRay's CI following the transition to version 2.7.0.",although find root cause issue resolved nightly build plan keep issue open observe stable performance following transition version,issue,negative,neutral,neutral,neutral,neutral,neutral
1712165759,"Could you try using our [2.7 rc candidate](https://pypi.org/project/ray/2.7.0rc0/) instead of 2.6.3
",could try candidate instead,issue,negative,neutral,neutral,neutral,neutral,neutral
1712152286,"@vitsai  could you compare the metrics there? These are perf tests, so metrics are important.",could compare metric metric important,issue,negative,positive,positive,positive,positive,positive
1712111128,"@edoakes This change is ready to merge. [The test failures](https://buildkite.com/ray-project/oss-ci-build-pr/builds/35656#018a7599-11df-4bee-b573-4fef0ae1c76e) are unrelated:

<img width=""1125"" alt=""Screen Shot 2023-09-08 at 12 07 39 PM"" src=""https://github.com/ray-project/ray/assets/92341594/fd43b15b-8eee-4137-9321-55b0dadf21ea"">

<img width=""1344"" alt=""Screen Shot 2023-09-08 at 12 08 18 PM"" src=""https://github.com/ray-project/ray/assets/92341594/776bdfa7-62fa-404e-8da4-014a637018a4"">
",change ready merge test unrelated screen shot screen shot,issue,negative,positive,positive,positive,positive,positive
1712093303,@krfricke This seems to be failing consistently on [test_websockets](https://buildkite.com/ray-project/premerge/builds/5449#018a75f3-a304-4e7d-8c0d-49d1a2999d65/6-403) with SIGTERM. Can you take a look?,failing consistently take look,issue,negative,positive,positive,positive,positive,positive
1712062596,"> IF it is the infra issue, I think we should reduce the num pgs until infra issue is fixed, not jailing the test

Agreed. Let's reduce (number of pg * bundle per gp) to O(100) and re-enable this test.
",infra issue think reduce infra issue fixed test agreed let reduce number bundle per test,issue,negative,positive,neutral,neutral,positive,positive
1712046861,"@GeneDer ah thanks for checking, let me make a 1 line fix for the documentation since it was fixed in another PR that will not be cherry-picked.",ah thanks let make line fix documentation since fixed another,issue,negative,positive,positive,positive,positive,positive
1712028557,"> @iycheng actually, I don't thnk I understand how this disables jemalloc only in workers. can you tell me some details?

Worker is started by raylet and raylet has LD_PRELOAD setup, so worker will have this too. Remove it will remove it from the  worker.",actually understand tell worker raylet raylet setup worker remove remove worker,issue,negative,neutral,neutral,neutral,neutral,neutral
1712001271,I tested this on RTD and the fixed doc links seem to be working. Thank you!,tested fixed doc link seem working thank,issue,negative,positive,neutral,neutral,positive,positive
1711990093,"```
REGRESSION 44.12%: client__put_gigabytes (THROUGHPUT) regresses from 0.13393152379385806 to 0.07484254417628397 (44.12%) in 2.7.0/microbenchmark.json
REGRESSION 14.90%: placement_group_create/removal (THROUGHPUT) regresses from 982.6908282286798 to 836.2520755126423 (14.90%) in 2.7.0/microbenchmark.json
REGRESSION 9.61%: single_client_tasks_sync (THROUGHPUT) regresses from 1329.0736290293253 to 1201.3784261554574 (9.61%) in 2.7.0/microbenchmark.json
REGRESSION 9.42%: 1_1_async_actor_calls_sync (THROUGHPUT) regresses from 1519.8915131779668 to 1376.696749130666 (9.42%) in 2.7.0/microbenchmark.json
REGRESSION 9.23%: single_client_tasks_and_get_batch (THROUGHPUT) regresses from 10.988766549840475 to 9.974994834952925 (9.23%) in 2.7.0/microbenchmark.json
REGRESSION 8.79%: 1_1_actor_calls_sync (THROUGHPUT) regresses from 2527.884786760515 to 2305.695886255063 (8.79%) in 2.7.0/microbenchmark.json
REGRESSION 7.52%: 1_1_actor_calls_concurrent (THROUGHPUT) regresses from 5038.298026762205 to 4659.35946688914 (7.52%) in 2.7.0/microbenchmark.json
REGRESSION 6.67%: single_client_put_calls_Plasma_Store (THROUGHPUT) regresses from 5759.206110210831 to 5375.213700448151 (6.67%) in 2.7.0/microbenchmark.json
REGRESSION 6.29%: single_client_tasks_async (THROUGHPUT) regresses from 10940.075908384719 to 10252.473015950294 (6.29%) in 2.7.0/microbenchmark.json
REGRESSION 6.07%: single_client_put_gigabytes (THROUGHPUT) regresses from 18.410571624942172 to 17.293085559802083 (6.07%) in 2.7.0/microbenchmark.json
REGRESSION 4.83%: n_n_actor_calls_async (THROUGHPUT) regresses from 32688.224759117064 to 31109.264908112025 (4.83%) in 2.7.0/microbenchmark.json
REGRESSION 3.98%: n_n_actor_calls_with_arg_async (THROUGHPUT) regresses from 2958.8505427111486 to 2841.0947976038783 (3.98%) in 2.7.0/microbenchmark.json
REGRESSION 3.77%: 1_n_async_actor_calls_async (THROUGHPUT) regresses from 9607.6917020388 to 9245.044511461581 (3.77%) in 2.7.0/microbenchmark.json
REGRESSION 3.11%: 1_1_async_actor_calls_with_args_async (THROUGHPUT) regresses from 2071.4152029622965 to 2007.0173686510257 (3.11%) in 2.7.0/microbenchmark.json
REGRESSION 2.16%: multi_client_put_calls_Plasma_Store (THROUGHPUT) regresses from 12887.724106896032 to 12609.86695150846 (2.16%) in 2.7.0/microbenchmark.json
REGRESSION 1.68%: 1_1_async_actor_calls_async (THROUGHPUT) regresses from 2683.097200627191 to 2638.057476754568 (1.68%) in 2.7.0/microbenchmark.json
REGRESSION 1.68%: 1_n_actor_calls_async (THROUGHPUT) regresses from 11016.813113385808 to 10832.013864699971 (1.68%) in 2.7.0/microbenchmark.json
```",regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput,issue,negative,neutral,neutral,neutral,neutral,neutral
1711985727,"IF it is the infra issue, I think we should reduce the num pgs until infra issue is fixed, not jailing the test ",infra issue think reduce infra issue fixed test,issue,negative,positive,neutral,neutral,positive,positive
1711984851,what's the root cause of this? I am surprised this test has been jailed without me noticing,root cause test without,issue,negative,neutral,neutral,neutral,neutral,neutral
1711978174,@krfricke and @matthewdeng could you explain and confirm why this is a release blocker?,could explain confirm release blocker,issue,negative,neutral,neutral,neutral,neutral,neutral
1711955937,actually I am hesistant to merge this PR until we resolve the grpc upgrade issue (since it can also somehow affect things). Maybe let's wait until we resolve it so that we can understand the impact of this PR isolated? ,actually merge resolve upgrade issue since also somehow affect maybe let wait resolve understand impact isolated,issue,negative,neutral,neutral,neutral,neutral,neutral
1711954121,"Sounds good! You can ""mark as draft"" in that case https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/changing-the-stage-of-a-pull-request#converting-a-pull-request-to-a-draft",good mark draft case,issue,negative,positive,positive,positive,positive,positive
1711947462,"Hey @sven1977 , thank you for the fast response.

Just a small clarification regarding `AlgorithmConfig`. Here is the example of code which has created the problem (btw. algorithm was PPO):
```
config: AlgorithmConfig = get_trainable_cls(cfg.algorithm).get_default_config()
...
config.evaluation( 
        evaluation_interval=1,
        evaluation_duration=1,
        evaluation_duration_unit='episode',
 )
```
While the error in this particular example might have leaked through some `AlgorithmConfig`'s checks, I was pretty confident that the my bug was hid by `Algorithm`'s constructions ( example: [algorithm.py#L1295-L1303](https://github.com/ray-project/ray/blob/master/rllib/algorithms/algorithm.py#L1295-L1303)  as linked above, but this is not the only instance) like:

```
if unit == ""episodes"":
    ...
else:
    ... # assumes: unit == ""timesteps""
```",hey thank fast response small clarification regarding example code problem algorithm error particular example might pretty confident bug algorithm example linked instance like unit else unit,issue,positive,positive,positive,positive,positive,positive
1711940253,Let's run and compare the perf asap. We already saw one regression. ,let run compare already saw one regression,issue,negative,neutral,neutral,neutral,neutral,neutral
1711940005,"> @jonathan-anyscale is this ready for review or still WIP?

still WIP, doing some refactor instead of this method and trying to test it right now",ready review still still instead method trying test right,issue,negative,positive,positive,positive,positive,positive
1711938719,Jiajun is correct. This came from last night with the code from the day before. I can rerun the test and collect the logs again after the gRPC PR is picked into the release branch ,correct came last night code day rerun test collect picked release branch,issue,negative,neutral,neutral,neutral,neutral,neutral
1711926651,"Ah yes this is for Ray 2.7, you can try out these new APIs with https://pypi.org/project/ray/2.7.0rc0/ ",ah yes ray try new,issue,negative,positive,positive,positive,positive,positive
1711925673,I'm going to close this one and include it in my list of picks :),going close one include list,issue,negative,neutral,neutral,neutral,neutral,neutral
1711904321,Thanks for the contribution! Looks like this was fixed by https://github.com/ray-project/ray/pull/38629.,thanks contribution like fixed,issue,positive,positive,positive,positive,positive,positive
1711884955,"Hi @matthewdeng 
Thank you for the reply and the reference.
Unfortunately, this example is also out-date.
For example, in this code you have
`from ray.train import ScalingConfig`
Although now `ScalingConfig` is not the part of `ray.train`, rather it is part of `ray.air.config`.

After adjusting these things, I am getting the error:

AttributeError: module 'ray.train' has no attribute 'get_context'",hi thank reply reference unfortunately example also example code import although part rather part getting error module attribute,issue,negative,negative,negative,negative,negative,negative
1711876218,Let me spend some time on this today.,let spend time today,issue,negative,neutral,neutral,neutral,neutral,neutral
1711863346,"Hey @tbukic , thanks for raising this issue! :)
As of Ray 2.0, you should use RLlib's `AlgorithmConfig` classes, which do all sorts of type checking on the config attributes as well as the configured values early on, but also deeper within the code.
Here is a link to the docs where the usage of these configs is explained in detail:
https://docs.ray.io/en/latest/rllib/rllib-training.html#configuring-rllib-algorithms
",hey thanks raising issue ray use class type well early also within code link usage detail,issue,positive,positive,positive,positive,positive,positive
1711862191,"[The test failures](https://buildkite.com/ray-project/premerge/builds/5365) are unrelated:

<img width=""1160"" alt=""Screen Shot 2023-09-08 at 8 37 32 AM"" src=""https://github.com/ray-project/ray/assets/92341594/ffecc8d3-5b44-4010-8485-22c092063322"">

<img width=""1340"" alt=""Screen Shot 2023-09-08 at 8 37 56 AM"" src=""https://github.com/ray-project/ray/assets/92341594/1ab56a9f-efa0-4115-9b41-6a85914ed0b1"">


",test unrelated screen shot screen shot,issue,negative,neutral,neutral,neutral,neutral,neutral
1711858220,"@rkooo567 Yes, both `lightgbm_train_moderate` and `lightgbm_tune_4x16` are failing consistently on the release branch
- https://buildkite.com/ray-project/release-tests-branch/builds/2135#018a714b-fca5-4be4-9c90-1adb46170acd
- https://buildkite.com/ray-project/release-tests-branch/builds/2135#018a7164-9de3-4747-bf76-e17696dfed11
- https://buildkite.com/ray-project/release-tests-branch/builds/2135#018a7196-b348-4915-a8bc-7992b60a2c21
- https://buildkite.com/ray-project/release-tests-branch/builds/2135#018a714b-fcb0-43d0-9e32-4568a6c5e5d1
- https://buildkite.com/ray-project/release-tests-branch/builds/2135#018a7164-a63d-4e6d-8ef2-a1a0118ad36e
- https://buildkite.com/ray-project/release-tests-branch/builds/2135#018a7196-bce9-43b7-8f96-d9d825c100a1

I think as long as we are able to see success runs each for both then it's good to go :) ",yes failing consistently release branch think long able see success good go,issue,positive,positive,positive,positive,positive,positive
1711831068,@jjyao Let us know when this is ready,let u know ready,issue,negative,positive,positive,positive,positive,positive
1711830815,"https://buildkite.com/ray-project/release-tests-pr/builds/52730#018a747a-c579-482c-8082-3c90eb54337a

passes, 

cc @krfricke @can-anyscale did it fail consistently in the release branch>? How many times should we run to verify it is fixed? ",fail consistently release branch many time run verify fixed,issue,negative,positive,neutral,neutral,positive,positive
1711826863,"Pick of https://github.com/ray-project/ray/pull/39158

@krfricke I will also log it to the sheet. Let me know if this is ready to be merged",pick also log sheet let know ready,issue,negative,positive,positive,positive,positive,positive
1711727390,Doesn't resolve the problem - it's more likely related to #38897,resolve problem likely related,issue,negative,neutral,neutral,neutral,neutral,neutral
1711706911,PR in review. Thanks for raising this issue (and providing the solution!) @Phirefly9 :),review thanks raising issue providing solution,issue,positive,positive,positive,positive,positive,positive
1711669097,"Ah, I see, we need to simply change `rollout_fragment_length=auto`. That should fix it.

The problem is that the batch size must be composable of the number of envs in total (num_rollout_workers x num_envs_per_worker) and the rollout_fragment_length

```
ValueError: Your desired train_batch_size (5000) or a value 10% off of that
cannot be achieved with your other settings (num_rollout_workers=32;
num_envs_per_worker=5; rollout_fragment_length=20)! Try setting rollout_fragment_length to 'auto' OR 31.
```

cc: @simonsays1980 :)",ah see need simply change fix problem batch size must number total desired value try setting,issue,negative,neutral,neutral,neutral,neutral,neutral
1711664094,"Can we simply match the tuned_example file with the yaml file under `ray/release/rllib_tests/learning_tests/yaml/ppo`? That one uses Atari as well and works as it's a release test.

Something must be different between these two that breaks things.",simply match file file one well work release test something must different two,issue,negative,neutral,neutral,neutral,neutral,neutral
1711660445,PR in review. Thanks for raising this issue @chrisyeh96 ! :),review thanks raising issue,issue,negative,positive,positive,positive,positive,positive
1711655572,We'll publish the new versions after #39452 is merged and the docker images have been built. This is part of the release process.,publish new docker built part release process,issue,negative,positive,positive,positive,positive,positive
1711624361,"Note that this is only a problem currently for PPO in combination with LSTM/RNNs.
DreamerV3 is not affected by this as it doesn't use the old Policy APIs anymore.
All other algos are not yet on the new stack by default (however, they would show the same problem once moved).",note problem currently combination affected use old policy yet new stack default however would show problem,issue,negative,positive,neutral,neutral,positive,positive
1711592928,"This is the culprit here (policy/eager_tf_policy_v2.py), which we don't do on the old stack. A single run through this code takes about 0.008sec on my laptop, which is (just by itself) already 5-6 times longer than the entire old-stack forward pass.
```
    input_dict = self.maybe_add_time_dimension(input_dict, seq_lens=seq_lens)
```",culprit old stack single run code already time longer entire forward pas,issue,negative,positive,neutral,neutral,positive,positive
1711585444,"Still an issue:

```
tf2 new stack

Trial status: 1 TERMINATED
Current time: 2023-09-08 14:20:55. Total running time: 2min 5s
Logical resource usage: 0/16 CPUs, 0/0 GPUs
╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                          status         iter     total time (s)      ts     reward     episode_reward_max     episode_reward_min     episode_len_mean     episodes_this_iter │
├─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ PPO_StatelessCartPole_dddcf_00000   TERMINATED       20            124.705   10240      24.67                     92                      9                24.67                     17 │
╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯


tf2 old stack:
Trial status: 1 TERMINATED
Current time: 2023-09-08 14:22:11. Total running time: 34s
Logical resource usage: 0/16 CPUs, 0/0 GPUs
╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                          status         iter     total time (s)      ts     reward     episode_reward_max     episode_reward_min     episode_len_mean     episodes_this_iter │
├─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ PPO_StatelessCartPole_41a07_00000   TERMINATED       20            32.4985   10240      33.38                     93                     11                33.38                     14 │
╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

```",still issue new stack trial status current time total running time min logical resource usage trial name status iter total time reward old stack trial status current time total running time logical resource usage trial name status iter total time reward,issue,positive,positive,neutral,neutral,positive,positive
1711453300,"Okay. I tested myself. The reason why it didn't happen was because we have a ""check parent"" task and do `asyncio.gather(checK_parent)` that's basically infinite running before calling self.server.wait_for_termination.

In windows, we don't run check_parent, so it goes into wait_for_termination immediately, which fails it. 

I think this means we should add minimal test in windows, but I feel like that's too high overhead. So the current solution seems okay to me. ",tested reason happen check parent task basically infinite running calling run go immediately think add minimal test feel like high overhead current solution,issue,positive,positive,neutral,neutral,positive,positive
1711227914,same issue here. Dont know is wrong,issue dont know wrong,issue,negative,negative,negative,negative,negative,negative
1711159862,"Hi Larry,

thank you for the clarification. In the meantime we made some progress
regarding ray and why it crashes. I was able to find out how to get around
it but I do not understand why the workaround is required. As I'm working
with CMake I took the original cpp example and created a cmake file for it.
I must have missed the fact that in addition to the executable you create
also a library out of the code in the executable and link to it. If I do
this I'm able to get it running but I do not see a reason why executable
and library are basically containing the same code for the example to work.

For sure ray needs to know when it receives a message from a client which
function needs to be executed. I also understand that this function is
searched then in the path containing .so files in the ray_code_search_path.
I assume your accessing these shared object files then opening up the so's
via dlopen looking for the function definitions containing the macro of
RAY_REMOTE(myfunctionName). But why does this function definition need to
be part of the main executable as well? Why the duplication?
Can you explain a little bit what is going on behind the curtains so that
it makes sense to me?

Regards,

Wolfgang

On Thu, Sep 7, 2023 at 1:31 PM Larry ***@***.***> wrote:

>
>    1. If the issue is caused by CXX11_ABI inconsistency, you don't need
>    to worry about which specific error is being reported. Due to ABI
>    inconsistency, many basic functions like malloc and std::string() will
>    crash.
>    2. The reason why local_mode=true does not have any issues is that
>    local_mode does not invoke the interfaces of libray_api.so.
>    3. There are significant differences between C++ and Python/Java, and
>    it can be more challenging to develop and debug in C++. However, its
>    performance is the best.
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/ray-project/ray/issues/39252#issuecomment-1709985898>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/A7O7FGSVDU2TL2X6DHTRJPDXZGV5PANCNFSM6AAAAAA4LFB5M4>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>

",hi larry thank clarification made progress regarding ray able find get around understand working took original example file must fact addition executable create also library code executable link able get running see reason executable library basically code example work sure ray need know message client function need executed also understand function path assume object opening via looking function macro function definition need part main executable well duplication explain little bit going behind sense larry wrote issue inconsistency need worry specific error due inconsistency many basic like crash reason invoke significant develop however performance best reply directly view id,issue,positive,positive,positive,positive,positive,positive
1711122180,"I think this looks good. Can you verify if it fixes the issue (maybe manually verify it)? 

Also curious why minimal tests couldn't detect this issue. Maybe we are downloading grpcio inside minimal tests? Can you double check? (it doesn't block the PR)",think good verify issue maybe manually verify also curious minimal could detect issue maybe inside minimal double check block,issue,negative,positive,neutral,neutral,positive,positive
1711120015,"```
def test_temp_dir_with_node_ip_address(ray_start_cluster, short_tmp_path):
    cluster = ray_start_cluster
    cluster.add_node(temp_dir=short_tmp_path)
    ray.init(address=cluster.address)
    assert short_tmp_path == ray._private.worker._global_node.get_temp_dir_path()
```

Surprisingly, it seems like this never worked before (the tmp dir is wrong from the driver). It is revealed now because we now rely on this directory to initialize the worker",cluster assert surprisingly like never worked wrong driver revealed rely directory initialize worker,issue,positive,positive,neutral,neutral,positive,positive
1711104261,Seems like it fails with the same error as endpoint. cc @rynewang I will merge this after you merge that PR,like error merge merge,issue,negative,neutral,neutral,neutral,neutral,neutral
1711102887,"Btw, the current PR is kind of a point fix. It seems like this issue can happen if `get_node_ip_address` without initializing ray (I only found this happens from log monitor and autoscaler, but it is not maintainable). I verified most of time, it is used when ray is already initialized, and it returns the correct node ip from `ray._private.worker._global_node.node_ip_address`, except autoscaler & log monitor., 

If we want this API to obtain IP from the session dir, it requires us to pass session dir (which doesn't seem like a good idea given it is a public API). I think we need more holistic solution here (I will create a doc after 2.7 release) instead of relying on fragile implementation now. ",current kind point fix like issue happen without ray found log monitor maintainable time used ray already correct node except log want obtain session u pas session seem like good idea given public think need holistic solution create doc release instead fragile implementation,issue,positive,positive,positive,positive,positive,positive
1711100021,"@darthhexx and @shayanhoshyari , can you share your use case (if it isn't distributed training)?",share use case distributed training,issue,negative,neutral,neutral,neutral,neutral,neutral
1711078881,"the test did run, but in 3 parallel dockers, and only one has tests.

@can-anyscale ..",test run parallel one,issue,negative,neutral,neutral,neutral,neutral,neutral
1711044554,Hi @aslonnie and @can-anyscale  looks like the test is not being run. here is my pr triggering the test: https://buildkite.com/ray-project/premerge/builds/5248#018a71c9-6550-4f3e-bd16-12bbe7132bfc,hi like test run test,issue,negative,neutral,neutral,neutral,neutral,neutral
1711024932,"latest comparisons 
```
(ray) gene@geneanycale2023 release_logs % ./compare_perf_metrics 2.6.3 2.7.0 | sort -nr -k2
REGRESSION 458.76%: dashboard_p50_latency_ms (LATENCY) regresses from 13.316 to 74.405 (458.76%) in 2.7.0/benchmarks/many_actors.json
REGRESSION 446.95%: dashboard_p95_latency_ms (LATENCY) regresses from 1641.087 to 8975.972 (446.95%) in 2.7.0/benchmarks/many_actors.json
REGRESSION 126.60%: dashboard_p99_latency_ms (LATENCY) regresses from 3961.132 to 8975.972 (126.60%) in 2.7.0/benchmarks/many_actors.json
REGRESSION 92.76%: dashboard_p99_latency_ms (LATENCY) regresses from 7303.73 to 14078.349 (92.76%) in 2.7.0/benchmarks/many_tasks.json
REGRESSION 43.42%: dashboard_p95_latency_ms (LATENCY) regresses from 5662.863 to 8121.815 (43.42%) in 2.7.0/benchmarks/many_tasks.json
REGRESSION 26.23%: dashboard_p95_latency_ms (LATENCY) regresses from 58.601 to 73.97 (26.23%) in 2.7.0/benchmarks/many_nodes.json
REGRESSION 19.57%: dashboard_p50_latency_ms (LATENCY) regresses from 5.529 to 6.611 (19.57%) in 2.7.0/benchmarks/many_tasks.json
REGRESSION 15.09%: client__put_gigabytes (THROUGHPUT) regresses from 0.13393152379385806 to 0.11371929384736562 (15.09%) in 2.7.0/microbenchmark.json
REGRESSION 9.61%: 1_1_async_actor_calls_with_args_async (THROUGHPUT) regresses from 2071.4152029622965 to 1872.4258363763167 (9.61%) in 2.7.0/microbenchmark.json
REGRESSION 7.20%: 1000000_queued_time (LATENCY) regresses from 179.73244193300002 to 192.66913281499998 (7.20%) in 2.7.0/scalability/single_node.json
REGRESSION 5.84%: actors_per_second (THROUGHPUT) regresses from 848.4535001176805 to 798.8964980032177 (5.84%) in 2.7.0/benchmarks/many_actors.json
REGRESSION 4.58%: client__get_calls (THROUGHPUT) regresses from 1157.7462987111426 to 1104.7018525960716 (4.58%) in 2.7.0/microbenchmark.json
REGRESSION 3.49%: client__1_1_actor_calls_sync (THROUGHPUT) regresses from 508.5752348554388 to 490.8207787588942 (3.49%) in 2.7.0/microbenchmark.json
REGRESSION 3.06%: 1_1_async_actor_calls_sync (THROUGHPUT) regresses from 1519.8915131779668 to 1473.3189286996085 (3.06%) in 2.7.0/microbenchmark.json
REGRESSION 2.37%: client__1_1_actor_calls_async (THROUGHPUT) regresses from 1007.9864291252518 to 984.0493755334898 (2.37%) in 2.7.0/microbenchmark.json
REGRESSION 2.09%: 107374182400_large_object_time (LATENCY) regresses from 33.47957800099999 to 34.178862154 (2.09%) in 2.7.0/scalability/single_node.json
REGRESSION 1.67%: single_client_get_object_containing_10k_refs (THROUGHPUT) regresses from 13.564180981457046 to 13.337175847420632 (1.67%) in 2.7.0/microbenchmark.json
REGRESSION 1.67%: 3000_returns_time (LATENCY) regresses from 5.809550247000004 to 5.9067989380000085 (1.67%) in 2.7.0/scalability/single_node.json
REGRESSION 1.38%: multi_client_put_calls_Plasma_Store (THROUGHPUT) regresses from 12887.724106896032 to 12709.676416730535 (1.38%) in 2.7.0/microbenchmark.json
REGRESSION 0.99%: 10000_args_time (LATENCY) regresses from 16.737809073999998 to 16.903846098000002 (0.99%) in 2.7.0/scalability/single_node.json
REGRESSION 0.78%: placement_group_create/removal (THROUGHPUT) regresses from 982.6908282286798 to 974.981422456155 (0.78%) in 2.7.0/microbenchmark.json
REGRESSION 0.73%: 1_1_actor_calls_sync (THROUGHPUT) regresses from 2527.884786760515 to 2509.3730848129885 (0.73%) in 2.7.0/microbenchmark.json
REGRESSION 0.69%: multi_client_put_gigabytes (THROUGHPUT) regresses from 38.350117983138134 to 38.087386420244215 (0.69%) in 2.7.0/microbenchmark.json
REGRESSION 0.47%: 1_1_actor_calls_async (THROUGHPUT) regresses from 8232.577421386566 to 8194.146466814414 (0.47%) in 2.7.0/microbenchmark.json
REGRESSION 0.14%: client__tasks_and_get_batch (THROUGHPUT) regresses from 0.9848332918374623 to 0.9834082023841304 (0.14%) in 2.7.0/microbenchmark.json
2.7.0 does not have benchmarks/many_pgs.json
(ray) gene@geneanycale2023 release_logs %
```",latest ray gene sort regression latency regression latency regression latency regression latency regression latency regression latency regression latency regression throughput regression throughput regression latency regression throughput regression throughput regression throughput regression throughput regression throughput regression latency regression throughput regression latency regression throughput regression latency regression throughput regression throughput regression throughput regression throughput regression throughput ray gene,issue,negative,positive,positive,positive,positive,positive
1711015638,"Thanks @architkulkarni ! 

> For now the example yamls are tested manually, right? 

Yes they are.

> In the future we can add them to automatic release tests that are run periodically in CI.

Yes we are building some in-house automation currently as they require on-prem vSphere environments. Will explore how to integrate such tests into the GitHub repo.",thanks example tested manually right yes future add automatic release run periodically yes building currently require explore integrate,issue,positive,positive,positive,positive,positive,positive
1711013704,@rkooo567 Seems like all the performance blockers are resolved at this point. Let me know if you want me to rerun the script and update the result or if you think this is good to merge!,like performance resolved point let know want rerun script update result think good merge,issue,positive,positive,positive,positive,positive,positive
1711003579,"@richardliaw do you want to combine this into your PR #39400?

I like the change in `index.md` in Sam's PR and the change in `installation.rst` in Richard's PR 😄 ",want combine like change sam change,issue,negative,neutral,neutral,neutral,neutral,neutral
1710967295,(probably because I retried at the time of failure in the last comment),probably time failure last comment,issue,negative,negative,negative,negative,negative,negative
1710948828,"You machine has a different environment from microbenchmark, so we shouldn't rely on that result. Let me rerun tests",machine different environment rely result let rerun,issue,negative,neutral,neutral,neutral,neutral,neutral
1710945172,Decided not to pick because it is not the blocker for the user. It will be included in ray 2.8,decided pick blocker user included ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1710942226,"Tests look good, seems like microbenchmark ran into some infra failure so running it again now. On my machine, I did see the throughput go back up to ~13k/s from ~11k/s on the latest commit, so personally don't think we have to wait.",look good like ran infra failure running machine see throughput go back latest commit personally think wait,issue,positive,positive,positive,positive,positive,positive
1710914152,"@rkooo567 Would you help taking a look ?

This should be a critical bug,
because in many cases we have to set custom ""--temp-dir"" directory (e.g. for using scalable storage device), but the bug blocks setting custom ""--temp-dir"" directory.",would help taking look critical bug many set custom directory scalable storage device bug setting custom directory,issue,negative,positive,positive,positive,positive,positive
1710912612,"Hey @pranith7 , sorry for the delay here. The Ray team is planning on working on this specific issue soon, but please take a look at the [GH Issues page](https://github.com/ray-project/ray/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22) for a list of other good first issues to tackle.",hey sorry delay ray team working specific issue soon please take look page list good first tackle,issue,negative,positive,positive,positive,positive,positive
1710908422,All chaos test passing. Will merge it once we verify performance is not significantly affected from core release tests,chaos test passing merge verify performance significantly affected core release,issue,negative,positive,positive,positive,positive,positive
1710901506,"I switched the `actor_name` tag with `actor_id` in both metrics. Getting the `actor_name` requires calling Ray State API, and that was raising obscure errors inside `test_grpc` and `test_client`.

E.g.: on [`test_grpc_proxy_on_draining_nodes`](https://buildkite.com/ray-project/premerge/builds/5202):

```
>       raise RuntimeError(message)
E       RuntimeError: The condition wasn't met before the timeout expired. Last exception: Traceback (most recent call last):
E         File ""/rayci/python/ray/_private/test_utils.py"", line 553, in wait_for_condition
E           if condition_predictor(**kwargs):
E         File ""/rayci/python/ray/serve/tests/utils.py"", line 153, in ping_grpc_list_applications
E           response, call = stub.ListApplications.with_call(request=request)
E         File ""/opt/miniconda/lib/python3.8/site-packages/grpc/_channel.py"", line 1043, in with_call
E           return _end_unary_response_blocking(state, call, True, None)
E         File ""/opt/miniconda/lib/python3.8/site-packages/grpc/_channel.py"", line 910, in _end_unary_response_blocking
E           raise _InactiveRpcError(state)  # pytype: disable=not-instantiable
E       grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
E       	status = StatusCode.UNAVAILABLE
E       	details = ""Socket closed""
E       	debug_error_string = ""UNKNOWN:Error received from peer  {created_time:""2023-09-07T20:22:34.980939551+00:00"", grpc_status:14, grpc_message:""Socket closed""}""
E       >
```

For the sake of time, I switched the tags, and stopped calling the Ray State API.",switched tag metric getting calling ray state raising obscure inside raise message condition met last exception recent call last file line file line response call file line return state call true none file line raise state status socket closed unknown error received peer socket closed sake time switched stopped calling ray state,issue,negative,positive,neutral,neutral,positive,positive
1710900665,"@ArturNiederfahrenhorst note that I customized the snippet for an even more relevant case:
```
import argparse
from gymnasium.spaces import Dict, Tuple, Box, Discrete, MultiDiscrete
import os

import ray
from ray import air, tune
from ray.tune.registry import register_env
from ray.rllib.utils.test_utils import check_learning_achieved
from ray.tune.registry import get_trainable_cls


import gymnasium as gym
from gymnasium.spaces import Box, Dict, Discrete, Tuple
import numpy as np
import tree  # pip install dm_tree

from ray.rllib.utils.spaces.space_utils import flatten_space


class CustomEnv(gym.Env):
    """"""Custom env with multi discrete action space

    """"""

    def __init__(self, config):
        self.observation_space = Box(-1.0, 1.0, (2,))
        self.action_space = MultiDiscrete([2,2])
        self.flattened_action_space = flatten_space(self.action_space)
        self.episode_len = 100

    def reset(self, *, seed=None, options=None):
        self.steps = 0
        return self._next_obs(), {}

    def step(self, action):
        self.steps += 1
        action = tree.flatten(action)
        reward = 0.0
        for a, o, space in zip(
            action, self.current_obs_flattened, self.flattened_action_space
        ):
            # Box: -abs(diff).
            if isinstance(space, gym.spaces.Box):
                reward -= np.sum(np.abs(a - o))
            # Discrete: +1.0 if exact match.
            if isinstance(space, gym.spaces.Discrete):
                reward += 1.0 if a == o else 0.0
        done = truncated = self.steps >= self.episode_len
        return self._next_obs(), reward, done, truncated, {}

    def _next_obs(self):
        self.current_obs = self.observation_space.sample()
        self.current_obs_flattened = tree.flatten(self.current_obs)
        return self.current_obs


parser = argparse.ArgumentParser()
parser.add_argument(
    ""--run"", type=str, default=""PPO"", help=""The RLlib-registered algorithm to use.""
)
parser.add_argument(
    ""--framework"",
    choices=[""tf"", ""tf2"", ""torch""],
    default=""torch"",
    help=""The DL framework specifier."",
)
parser.add_argument(""--num-cpus"", type=int, default=0)
parser.add_argument(
    ""--as-test"",
    action=""store_true"",
    help=""Whether this script should be run as a test: --stop-reward must ""
    ""be achieved within --stop-timesteps AND --stop-iters."",
)
parser.add_argument(
    ""--local-mode"",
    action=""store_true"",
    help=""Init Ray in local mode for easier debugging."",
)
parser.add_argument(
    ""--stop-iters"", type=int, default=100, help=""Number of iterations to train.""
)
parser.add_argument(
    ""--stop-timesteps"", type=int, default=100000, help=""Number of timesteps to train.""
)
parser.add_argument(
    ""--stop-reward"", type=float, default=0.0, help=""Reward at which we stop training.""
)

if __name__ == ""__main__"":
    args = parser.parse_args()
    ray.init(num_cpus=args.num_cpus or None, local_mode=args.local_mode)
    register_env(
        ""CustomEnv"", lambda c: CustomEnv(c)
    )

    config = (
        get_trainable_cls(args.run)
        .get_default_config()
        .environment(""CustomEnv"")
        .framework(args.framework)
        .rollouts(num_rollout_workers=0, num_envs_per_worker=20)
        # No history in Env (bandit problem).
        .training(gamma=0.0, lr=0.0005)
        # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.
        .resources(num_gpus=int(os.environ.get(""RLLIB_NUM_GPUS"", ""0"")))
    )

    if args.run == ""PPO"":
        config.training(
            # We don't want high entropy in this Env.
            entropy_coeff=0.00005,
            num_sgd_iter=4,
            vf_loss_coeff=0.01,
        )

    stop = {
        ""training_iteration"": args.stop_iters,
        ""episode_reward_mean"": args.stop_reward,
        ""timesteps_total"": args.stop_timesteps,
    }

    results = tune.Tuner(
        args.run, param_space=config, run_config=air.RunConfig(stop=stop, verbose=1)
    ).fit()

    if args.as_test:
        check_learning_achieved(results, args.stop_reward)

    ray.shutdown()
```

And this reproduces what I have been experiencing with my custom env, so the error `AttributeError: '<class 'ray.rllib.models.torch.torch_distributions' object has no attribute 'cats'`",note snippet even relevant case import import box discrete import o import ray ray import air tune import import import import gymnasium gym import box discrete import import tree pip install import class custom discrete action space self box reset self return step self action action action reward space zip action box space reward discrete exact match space reward else done truncated return reward done truncated self return parser run algorithm use framework torch torch framework specifier whether script run test must within ray local mode easier number train number train reward stop training none lambda history bandit problem use set want high entropy stop custom error class object attribute,issue,positive,positive,positive,positive,positive,positive
1710855494,"FYI the limit of 50 was set empirically by observing the metrics during the test. Aside from the overloaded ""weakest link"" proxy, the others hovered around 10-30 and basically never exceeded 50.

With this change, the slowest proxy was stable at 50 (but had higher latencies than the others).",limit set observing metric test aside link proxy around basically never change proxy stable higher,issue,negative,positive,positive,positive,positive,positive
1710850933,"Sorry, I got confused. `air_example_dolly_v2_lightning_fsdp_finetuning` is fine. `workspace_template_finetuning_llms_with_deepspeed_llama_2_7b` is the failing one ",sorry got confused fine failing one,issue,negative,negative,negative,negative,negative,negative
1710850361,"@GeneDer It seems to be an infra issue, where the cluster failed to start due to gpu instance shortage. Can we retry it later?

```
[INFO 2023-09-07 21:29:08,433] anyscale_job_manager.py: 216  ... job not yet running ...(1621 seconds, 178 seconds to job timeout) ...
--
  | [INFO 2023-09-07 21:29:57,895] anyscale_job_manager.py: 216  ... job not yet running ...(1671 seconds, 128 seconds to job timeout) ...
  | [INFO 2023-09-07 21:30:08,193] anyscale_job_manager.py: 216  ... job not yet running ...(1681 seconds, 118 seconds to job timeout) ...
  | [INFO 2023-09-07 21:30:41,634] anyscale_job_manager.py: 216  ... job not yet running ...(1715 seconds, 84 seconds to job timeout) ...
  | [INFO 2023-09-07 21:31:07,889] anyscale_job_manager.py: 216  ... job not yet running ...(1741 seconds, 58 seconds to job timeout) ...
  | [INFO 2023-09-07 21:31:52,113] anyscale_job_manager.py: 216  ... job not yet running ...(1785 seconds, 14 seconds to job timeout) ...
  | [INFO 2023-09-07 21:32:15,080] anyscale_job_manager.py: 155  Terminating job prodjob_76rakxteapdvilcxp1sk5ni8mr...
  | [INFO 2023-09-07 21:32:36,469] anyscale_job_manager.py: 158  Job prodjob_76rakxteapdvilcxp1sk5ni8mr terminated!
  | [ERROR 2023-09-07 21:32:36,469] glue.py: 471  Cluster did not start within 1800 seconds.
```

And yes it's another release test: `workspace_template_finetuning_llms_with_deepspeed_llama_2_7b`.
",infra issue cluster start due instance shortage retry later job yet running job job yet running job job yet running job job yet running job job yet running job job yet running job job job error cluster start within yes another release test,issue,negative,negative,neutral,neutral,negative,negative
1710848510,"Sorry, wrong issue. Still seeing air_example_dolly_v2_lightning_fsdp_finetuning failing after cherry pick https://buildkite.com/ray-project/release-tests-branch/builds/2136#018a7196-a7db-4e20-851b-bab2c859c6d9",sorry wrong issue still seeing failing cherry pick,issue,negative,negative,negative,negative,negative,negative
1710844511,"Cherry pick PR merged https://github.com/ray-project/ray/pull/39368 and seeing the test passing https://buildkite.com/ray-project/release-tests-branch/builds/2137#018a716d-1953-42ce-86a0-264c8dfe41e8

Feel free to reopen and drop release blocker tag if we are seeing issue on master again",cherry pick seeing test passing feel free reopen drop release blocker tag seeing issue master,issue,negative,positive,positive,positive,positive,positive
1710842747,Cherry pick PR merged https://github.com/ray-project/ray/pull/39368 and seeing the test passing https://buildkite.com/ray-project/oss-ci-build-branch/builds/5959#018a70c5-dcc3-4dea-af9c-c9b3025b1577,cherry pick seeing test passing,issue,negative,neutral,neutral,neutral,neutral,neutral
1710829450," `single_client_tasks_and_get_batch = [12.225312658308761, 0.38310197760588016]` for [microbenchmark release test](https://buildkite.com/ray-project/release-tests-pr/builds/52552#018a7168-8a87-45c8-8193-aa9f833d433b) after addressing comments (does not include latest commit, which only changes 1 line of test logic)",release test include latest commit line test logic,issue,negative,positive,positive,positive,positive,positive
1710821690,"After discussing with @angelinalg offline, I've separated the autoscaling guide into `Ray Serve Autoscaling` and `Advanced Ray Serve Autoscaling` because a good chunk of the content is too overwhelming for beginners who only want to try out autoscaling.

Ray Serve Autoscaling:
- Manual Scaling
- Autoscaling Basic Configuration
- Basic example
- Ray Serve Autoscaler vs Ray Autoscaler

Advanced Ray Serve Autoscaling
- Autoscaling config parameters
- Model composition example
- Troubleshooting guide",guide ray serve advanced ray serve good chunk content overwhelming want try ray serve manual scaling basic configuration basic example ray serve ray advanced ray serve model composition example guide,issue,positive,positive,positive,positive,positive,positive
1710818939,can you run core release tests and report the perf difference here? ,run core release report difference,issue,negative,neutral,neutral,neutral,neutral,neutral
1710756066,Oh @krfricke I think it's because you didn't pull the latest changes of the branch before committing this. The parent of https://github.com/ray-project/ray/commit/3fdfc126ba701f5aba45e753e1deb8d17dabd64e is https://github.com/ray-project/ray/commit/b0f638a0dc4443445a75617c24ddf6613166a3af (2 weeks ago).,oh think pull latest branch parent ago,issue,negative,positive,positive,positive,positive,positive
1710747002,"Yes this is needed for the new Train APIs in 2.7, but it's not clear to me why these errors are coming up in the CI tests. The new persistence mode should be enabled... @krfricke can you investigate?
",yes new train clear coming new persistence mode investigate,issue,positive,positive,positive,positive,positive,positive
1710689524,"I just copied this doc from the KubeRay repo, and did not address Vale's writing recommendations. I will do it after the Ray Summit.",copied doc address vale writing ray summit,issue,negative,neutral,neutral,neutral,neutral,neutral
1710688787,cc @angelinalg would you mind reviewing or approving this PR so that we can merge it before the cherry-pick deadline? Thanks!,would mind merge deadline thanks,issue,negative,positive,positive,positive,positive,positive
1710652425,"@raulchen @c21 what does this test do? It seems to be sending infeasible gang resource requests:
```
(autoscaler +29s) [autoscaler] Current infeasible gang resource requests: {""requests"":[{""resourcesBundle"":{""CPU"":1}, ""placementConstraints"":[{""antiAffinity"":{""labelName"":""_PG_7df29998785b117ddc8ce708213502000000""}}]}, {""resourcesBundle"":{""CPU"":1}, ""placementConstraints"":[{""antiAffinity"":{""labelName"":""_PG_7df29998785b117ddc8ce708213502000000""}}]}, {""resourcesBundle"":{""CPU"":1}, ""placementConstraints"":[{""antiAffinity"":{""labelName"":""_PG_7df29998785b117ddc8ce708213502000000""}}]}, {""resourcesBundle"":{""CPU"":1}, ""placementConstraints"":[{""antiAffinity"":{""labelName"":""_PG_7df29998785b117ddc8ce708213502000000""}}]}, {""resourcesBundle"":{""CPU"":1}, ""placementConstraints"":[{""antiAffinity"":{""labelName"":""_PG_7df29998785b117ddc8ce708213502000000""}}]}, {""resourcesBundle"":{""CPU"":1}, ""placementConstraints"":[{""antiAffinity"":{""labelName"":""_PG_7df29998785b117ddc8ce708213502000000""}}]}, {""resourcesBundle"":{""CPU"":1}, ""placementConstraints"":[{""antiAffinity"":{""labelName"":""_PG_7df29998785b117ddc8ce708213502000000""}}]}, {""resourcesBundle"":{""CPU"":1}, ""placementConstraints"":[{""antiAffinity"":{""labelName"":""_PG_7df29998785b117ddc8ce708213502000000""}}]}, {""resourcesBundle"":{""CPU"":1}, ""placementConstraints"":[{""antiAffinity"":{""labelName"":""_PG_7df29998785b117ddc8ce708213502000000""}}]}, {""resourcesBundle"":{""CPU"":1}, ""placementConstraints"":[{""antiAffinity"":{""labelName"":""_PG_7df29998785b117ddc8ce708213502000000""}}]}, {""resourcesBundle"":{""CPU"":1}, ""placementConstraints"":[{""antiAffinity"":{""labelName"":""_PG_7df29998785b117ddc8ce708213502000000""}}]}, {""resourcesBundle"":{""CPU"":1}, ""placementConstraints"":[{""antiAffinity"":{""labelName"":""_PG_7df29998785b117ddc8ce708213502000000""}}]}, {""resourcesBundle"":{""CPU"":1}, ""placementConstraints"":[{""antiAffinity"":{""labelName"":""_PG_7df29998785b117ddc8ce708213502000000""}}]}, {""resourcesBundle"":{""CPU"":1}, ""placementConstraints"":[{""antiAffinity"":{""labelName"":""_PG_7df29998785b117ddc8ce708213502000000""}}]}, {""resourcesBundle"":{""CPU"":1}, ""placementConstraints"":[{""antiAffinity"":{""labelName"":""_PG_7df29998785b117ddc8ce708213502000000""}}]}, {""resourcesBundle"":{""CPU"":1}, ""placementConstraints"":[{""antiAffinity"":{""labelName"":""_PG_7df29998785b117ddc8ce708213502000000""}}]}], ""details"":""7df29998785b117ddc8ce708213502000000:STRICT_SPREAD|PENDING""}
```

The compute config has 4 nodes max, but the gang request is asking for 16 x 1CPU strict spread.",test sending infeasible gang resource current infeasible gang resource compute gang request strict spread,issue,negative,neutral,neutral,neutral,neutral,neutral
1710628575,"RLlib test failures unrelated
Linkcheck failure: 

(ray-contribute/profiling: line  119) broken    http://goog-perftools.sourceforge.net/doc/cpu_profiler.html - 403 Client Error: Forbidden for url: http://goog-perftools.sourceforge.net/doc/cpu_profiler.html
--
  | (ray-contribute/profiling: line  104) broken    http://goog-perftools.sourceforge.net/doc/pprof-test-big.gif - 403 Client Error: Forbidden for url: http://goog-perftools.sourceforge.net/doc/pprof-test-big.gif

unrelated

test_redis_tls , test_client_builder unrelated
test_websockets unrelated



",test unrelated failure line broken client error forbidden line broken client error forbidden unrelated unrelated unrelated,issue,negative,negative,negative,negative,negative,negative
1710606249,Close this PR. I will open a cherry-pick PR after https://github.com/ray-project/ray/pull/39393 is merged into the master branch.,close open master branch,issue,negative,neutral,neutral,neutral,neutral,neutral
1710594459,@kevin85421 @architkulkarni Let's merge into master first and cherry pick,let merge master first cherry pick,issue,negative,positive,positive,positive,positive,positive
1710564349,@kevin85421 btw both linkcheck and doctest are failing. Let me update branch to see if those are fixed ,failing let update branch see fixed,issue,negative,positive,neutral,neutral,positive,positive
1710561513,"> thx, will fix it after

do you have a pr/link/something that we can track the progress of the fixing?",fix track progress fixing,issue,negative,neutral,neutral,neutral,neutral,neutral
1710560146,"> Actually @kevin85421 @architkulkarni could you help explain why this was not landed on master first?

Nothing special here. I just want to get the top priority thing (merge into the release branch) done first. ",actually could help explain landed master first nothing special want get top priority thing merge release branch done first,issue,negative,positive,positive,positive,positive,positive
1710553822,"retry works, ping to merge. @edoakes 
",retry work ping merge,issue,negative,neutral,neutral,neutral,neutral,neutral
1710548371,Actually @kevin85421 @architkulkarni could you help explain why this was not landed on master first?,actually could help explain landed master first,issue,negative,positive,positive,positive,positive,positive
1710546120,"Yes, we are. It should easily reach 100 reward on cartpole.",yes easily reach reward,issue,positive,positive,positive,positive,positive,positive
1710539290,I did not address the Vale writing recommendations in this PR. I will review all the recommendations from Vale for all KubeRay docs after the Ray Summit.,address vale writing review vale ray summit,issue,negative,neutral,neutral,neutral,neutral,neutral
1710535822,"> @kevin85421 Is this also a PR raised against the release branch or if this is a cherry pick?

@GeneDer Yes, this PR is for the release branch. I will cherry-pick this commit to the master branch after it is merged into the release branch.",also raised release branch cherry pick yes release branch commit master branch release branch,issue,positive,neutral,neutral,neutral,neutral,neutral
1710534486,This PR mainly introduces actionable error messages when users migrate their existing training scripts from Ray 2.6 to Ray 2.7.,mainly actionable error migrate training ray ray,issue,negative,positive,positive,positive,positive,positive
1710532268,@krfricke @matthewdeng could you explain / confirm why this is a release blocker? Thanks,could explain confirm release blocker thanks,issue,negative,positive,positive,positive,positive,positive
1710530390,@matthewdeng is the new persistence mode disabled on 2.7?,new persistence mode disabled,issue,negative,negative,neutral,neutral,negative,negative
1710525823,"Hi @sven1977,

So, I tried disabling the preprocessor API, but still didn't work.

As I have been investigating further, it seems RLLIB doesn't support the [graph](https://gymnasium.farama.org/api/spaces/composite/#graph) space. Specifically, there seems to be four problems that I have found (1) the space_utils get_dummy_batch_for_space function, (2) the serialization function, (3) concatenating sample batches with different node sizes, and (4) compressing the batch space doesn't work.

I created a fix for (1) and (2), and can work on making a pull request for those. But, I'm still unsure of how to tackle (3) and (4).

I'm unsure if this should be classified as a feature request or a bug report now. I think generally supporting this graph space will be extremely helpful for those (like me) using GNNs and not wanting to create an assumption within their environments a max graph nodes element. ",hi tried still work investigating support graph space specifically four found function serialization function sample different node size batch space work fix work making pull request still unsure tackle unsure classified feature request bug report think generally supporting graph space extremely helpful like wanting create assumption within graph element,issue,positive,positive,neutral,neutral,positive,positive
1710524196,"Open PR > target... 2.8 instead? This would affect cherry pick and make it super difficult. It's also a niche corner case that isn't critical to our users.

Open PR doesn't just add DeveloerAPI label; if that's just the scope we can do that. 

However the Open PR also changes the API always takes in and accepts Training results.

If it's just annotations this can be super easy to do and cherry pick for 2.7.

",open target instead would affect cherry pick make super difficult also niche corner case critical open add label scope however open also always training super easy cherry pick,issue,positive,positive,neutral,neutral,positive,positive
1710518973,Reviewed today - on track to close remaining 3 sub GH Issues by RC deadline tomorrow.,today track close sub deadline tomorrow,issue,negative,neutral,neutral,neutral,neutral,neutral
1710518448,Reviewed - leaving it open @matthewdeng to sanity check.,leaving open sanity check,issue,negative,neutral,neutral,neutral,neutral,neutral
1710517697,Keep this open until docs merge as part of RC tomorrow @justinvyu ,keep open merge part tomorrow,issue,negative,neutral,neutral,neutral,neutral,neutral
1710515381,@matthewdeng we have to cherry pick > fix is already in a PR. Once picked let's run test again and it should pass.,cherry pick fix already picked let run test pas,issue,negative,neutral,neutral,neutral,neutral,neutral
1710514579,"Reviewed today in Train standup with @krfricke  > believe next step here leads to Core troubleshooting further. @xieus @iycheng as FYI cc @matthewdeng 

If disagree please Slack and we'll sort this out live.",today train believe next step core disagree please slack sort live,issue,negative,positive,neutral,neutral,positive,positive
1710446327,@matthewdeng Do you know what is the most updated version of this example? Maybe we could point the user to that.,know version example maybe could point user,issue,negative,neutral,neutral,neutral,neutral,neutral
1710443530,Hey @Abhyudhey - unfortunately ray right now doesn't provide such mechanism. ,hey unfortunately ray right provide mechanism,issue,negative,negative,negative,negative,negative,negative
1710442713,Closing this issue. README and docs have been updated on master as well as (upcoming) Ray 2.7.,issue master well upcoming ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1710434472,@kevin85421 Is this also a PR raised against the release branch or if this is a cherry pick? ,also raised release branch cherry pick,issue,negative,neutral,neutral,neutral,neutral,neutral
1710364408,"@rkooo567 
I am using ray v2.6.3. Does ray currently have any mechanism to purge `/tmp/ray/session_latest/logs/old` dead worker files ?
In my case the directory size keeps increasing, I can obviously write some cronjob to purge the directory periodically, but I was wondering whether Ray already provides a way to purge this dir ?

",ray ray currently mechanism purge dead worker case directory size increasing obviously write purge directory periodically wondering whether ray already way purge,issue,negative,negative,neutral,neutral,negative,negative
1710331293,"Hey @dkupsh , thanks for filing this issue. Could you try one thing and disable the preprocessor API via your config?

```
config.experimental(_disable_preprocessor_api=True)
```

if you are still working with config dicts:
```
config[""_disable_preprocessor_api""] = True
```",hey thanks filing issue could try one thing disable via still working true,issue,positive,positive,positive,positive,positive,positive
1710303922,Failing in release branch as well. Mark as release blocker https://buildkite.com/ray-project/release-tests-branch/builds/2131#018a6fff-e1d2-41f8-95bf-398b1e44b3e3,failing release branch well mark release blocker,issue,negative,neutral,neutral,neutral,neutral,neutral
1710137966,This might face owner died error if schema_object_ref is created on a worker which is OOM killed,might face owner error worker,issue,negative,neutral,neutral,neutral,neutral,neutral
1710072825,"Using polars IPC reader writer would be best as a serializer. 

We will add that in polars internally, but it isn't there yet.",reader writer would best add internally yet,issue,positive,positive,positive,positive,positive,positive
1710044782,"Hi lads @rkooo567, @edoakes ,

Can you please have a look at this? Much appreciated!",hi please look much,issue,negative,positive,positive,positive,positive,positive
1709985898,"1. If the issue is caused by `CXX11_ABI` inconsistency, you don't need to worry about which specific error is being reported. Due to ABI inconsistency, many basic functions like malloc and std::string() will crash. 
2. The reason why local_mode=true does not have any issues is that local_mode does not invoke the interfaces of libray_api.so.
3. There are significant differences between C++ and `Python/Java`, and it can be more challenging to develop and debug in C++. However, its performance is the best.",issue inconsistency need worry specific error due inconsistency many basic like crash reason invoke significant develop however performance best,issue,negative,positive,positive,positive,positive,positive
1709984872,"Potentially we need to re-add a pyarrow 6 test, depending on if we want to continue to test support for it.",potentially need test depending want continue test support,issue,negative,neutral,neutral,neutral,neutral,neutral
1709984298,"I've updated the PR with a new compiled requirements file. So far it looks good, we'll just have to see if all tests are passing.",new file far good see passing,issue,negative,positive,positive,positive,positive,positive
1709888525,"Ah, I see.

How are the functions populated in c++ to the server?

I see currently not where the issue is that ray crashes.
All I do is doing a simple calculation. I do a RAY_REMOTE(myfunction) to
make it known to the server.
If I set config.local_mode to true everything is running fine but as soon
as I set this variable to false it crashes when I try to receive the
results via the ray::Get(results)

As far as I know there is no mechanism in c++ to run code remotely on
another machine as object serializations can just contain member variables
but not code like in an interpreter language like java or python. How is it
implemented? Do I need to take special consideration (i.e. spawn the server
in cpp with the macro that makes the function known to the server) or is it
sufficient to do this in the connecting client only?

Regards,

Wolfgang


On Thu, Sep 7, 2023 at 10:48 AM Larry ***@***.***> wrote:

> The reason：
> Since the C++ Worker runs by dynamically linking library_api.so, the
> CXX11_ABI option used by the user needs to be consistent with Ray. #18273
> <https://github.com/ray-project/ray/pull/18273>
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/ray-project/ray/issues/39252#issuecomment-1709739577>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/A7O7FGULHBWVLWQS7FMFE2TXZGC7BANCNFSM6AAAAAA4LFB5M4>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>",ah see server see currently issue ray simple calculation make known server set true everything running fine soon set variable false try receive via ray far know mechanism run code remotely another machine object contain member code like interpreter language like python need take special consideration spawn server macro function known server sufficient client larry wrote since worker dynamically linking option used user need consistent ray reply directly view id,issue,positive,positive,positive,positive,positive,positive
1709859454,"""allocate multiple containers to the same IP"" this would be a really important feaure to us. We deployed multiple envs in same machine, like pre and prod. It cost me 2 days to find out this  is a ray bug...
",allocate multiple would really important u multiple machine like prod cost day find ray bug,issue,positive,positive,positive,positive,positive,positive
1709787078,"Hi, I will get back to this issue soon. We are a little bit overoccupied lately due to ray 2.7 release. ",hi get back issue soon little bit lately due ray release,issue,negative,negative,negative,negative,negative,negative
1709763578,"> I think these fixes make sense. I'm not so familiar with our hyperband implementation so would need to spend more time on this for a full review.
> 
> If tests are passing, happy to get this in now (if aiming for 2.7)

Tests seem to be passing fine - and yes, let's pick for 2.7 as it fixes behavior relating to the new storage path",think make sense familiar implementation would need spend time full review passing happy get aiming seem passing fine yes let pick behavior new storage path,issue,positive,positive,positive,positive,positive,positive
1709752719,"Running microbenchmark here https://buildkite.com/ray-project/release-tests-pr/builds/52417.

@vitsai can you let me know the number after this PR from this test after it is completed? ",running let know number test,issue,negative,neutral,neutral,neutral,neutral,neutral
1709739577,"The reason：
Since the C++ Worker runs by dynamically linking `library_api.so`, the `CXX11_ABI` option used by the user needs to be consistent with Ray.   https://github.com/ray-project/ray/pull/18273   

",since worker dynamically linking option used user need consistent ray,issue,negative,positive,positive,positive,positive,positive
1709719663,"> LGTM, approved.
> 
> Just out of curiosity, why a metaclass instead of defining getattr directly?

Because it's a `@classmethod` - `getattr` only works on instances, but since we use `from_bytes` etc as a constructor, the object doesn't exist, yet, and getattr on the derived class is not invoked.",curiosity instead directly work since use constructor object exist yet derived class,issue,negative,positive,neutral,neutral,positive,positive
1709716000,"cc @rynewang 

After debugging test failure, I realized this PR means it can take up to 30 seconds to detect if the actor is restarted (it's because after network failure, we cannot reliably detect if the actor is DEAD or RESTARTED, and we rely on this 1 second timeout to fail fast). I think it is not acceptable. I will close the PR for now. 

I think we should fix this issue as a part of https://github.com/ray-project/ray/issues/29656.

There are 3 cases where actor network call fails;
- Pure network issue -> raises a correct error message saying actor task is failed due to network issue
- Actor is killed -> raises a correct error message why actor is dead
- Actor is killed & actor is restarted -> if retry > 0, we retry. Otherwise, raises a correct error that actor task is failed due to network issue

One idea for the protocol (assuming actor state notification is correct). 

- Whenever we send an actor task, we should track the ""actor_task_restart_cnt"" (the # of restarts the actor ).
- If cached actor state is already dead (DEAD is a terminal state), we raise ActorDiedError
- If actor task failed, we check the cached actor_task_restart_cnt from previous notification.
    - if actor_task_restart_cnt == cached_restart_cnt, we wait for the notification (either ALIVE, when actor is restarted, or DEAD, when an actor is dead, should come).
        - If ALIVE notification comes in, it means actor is restarted. Mark the task as network failure and retry if retry_cnt > 0. 
        - If DEAD notification comes in, it means the actor is dead. Raises an actor died error.
        - If actor_task_restart_cnt == cached_restart_cnt and none of notification comes, it means it is a pure network failure (or notification is not delivered). We wait for X seconds for the notification, and if it doesn't come, we can mark the task as network failure, or we can just infinitely wait (if we assume the notification has to be delivered eventually)
    - If actor_task_restart_cnt < cached_restart_cnt, it means the actor is already restarted. Mark the task as network failure and retry if retry_cnt >0.
",test failure take detect actor network failure reliably detect actor dead rely second fail fast think acceptable close think fix issue part actor network call pure network issue correct error message saying actor task due network issue actor correct error message actor dead actor actor retry retry otherwise correct error actor task due network issue one idea protocol assuming actor state notification correct whenever send actor task track actor actor state already dead dead terminal state raise actor task check previous notification wait notification either alive actor dead actor dead come alive notification come actor mark task network failure retry dead notification come actor dead actor error none notification come pure network failure notification wait notification come mark task network failure infinitely wait assume notification eventually actor already mark task network failure retry,issue,negative,negative,negative,negative,negative,negative
1709629871,"This function `get_default_temp_dir` we should also update code:
```
class DefaultDatabricksRayOnSparkStartHook(RayOnSparkStartHook):
    def get_default_temp_dir(self):
        return os.environ.get(""RAY_TMPDIR"", _DATABRICKS_DEFAULT_TMP_DIR)
   ...
```",function also update code class self return,issue,negative,neutral,neutral,neutral,neutral,neutral
1709575763,"@jjyao I think the core nightly test failure is fixed by https://github.com/ray-project/ray/commit/313a9230955816319858db9b6261a46c1167fb93. You can double check it by downloading a log file from gcs_server.out and check if the check failure is from the autoscaler manager

Can you also post the performance difference from the master here just in case to see if there's a big change? ",think core nightly test failure fixed double check log file check check failure manager also post performance difference master case see big change,issue,negative,negative,negative,negative,negative,negative
1709487554,"@richardliaw quick update on this:

1. I am not planning to remove dead code and tests for 2.7.
2. There is some minor cleanup in flight, e.g. for cleaning up some additional AIR/Train API references
3. I am planning to do another pass over for references to AIR (in particular in the Train docs)

Could you start doing a broader pass over all the documentation, to see if there is anything else? ",quick update remove dead code minor cleanup flight cleaning additional another pas air particular train could start pas documentation see anything else,issue,negative,positive,neutral,neutral,positive,positive
1709458524,"Hi Larry,

a colleague of mine just found out the issue. It seems that compiling the
example without the compile option ""-Dp_GLIBCXX_USECXX11_ABI=0"" is causing
the issue.
Do you have an idea what's the reason for this?

Now that I don't get the std::bad_alloc anymore I
m getting a SIG_SEV in the api.h file line 202

It seems that the line
result = ray::internal::GetRayRuntime()->Get(ids) is crashing with a SIG_SEV

After running gdb I could spot from the provided backtraces that the function crashes in
ray::internal::NativeObjectStore::GetRaw(std::vector<ray::ObjectID, std::allocator<ray::ObjectID> const&, int)



Regards,

Wolfgang


On Thu, Sep 7, 2023 at 4:34 AM Larry ***@***.***> wrote:

>
>    1. Can you give me stack trace while crash by using gdb? eg: gdb exmple
>    2. How do you compile exmple.cc?
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/ray-project/ray/issues/39252#issuecomment-1709386502>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/A7O7FGUKCTJLGJDQXH4SWILXZEXBNANCNFSM6AAAAAA4LFB5M4>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
",hi larry colleague mine found issue example without compile option causing issue idea reason get getting file line line result ray get running could spot provided function ray ray ray larry wrote give stack trace crash compile reply directly view id,issue,negative,positive,neutral,neutral,positive,positive
1709448951,"I thought partial function can be an workaround for this like I used for `ray.util.iter.from_iterators`, but `map` treats function parameter as Task.

Sample code for `ray.util.iter.from_iterators`
```python
from functools import partial

import ray
import webdataset as wds


def wds_generator(paths):
    dataset = wds.WebDataset(paths)
    for sample in dataset:
        yield sample

def ray_generator(partitioned_wds_paths):
    generator_funcs = [partial(wds_generator, paths) for paths in partitioned_wds_paths]
    return ray.util.iter.from_iterators(generator_funcs).gather_async()
```

Possible workaround if map takes function as class constructor
```python
pipe.map(partial(ModelPredictor, model_path), num_gpus=1, compute=ray.data.ActorPoolStrategy(min_size=1, max_size=2))
```",thought partial function like used map function parameter task sample code python import partial import ray import sample yield sample partial return possible map function class constructor python partial,issue,negative,negative,neutral,neutral,negative,negative
1709442885,"Update: After some testing, I found out that the problem lies with the custom env function. After changing the env from a custom function to simply using the ones declared in Ray's registry with Rays preprocessing, the PPO training results seems to be ok. Though, I am not sure why this is so. Perhaps there is a bug with passing a custom env fn.",update testing found problem custom function custom function simply declared ray registry training though sure perhaps bug passing custom,issue,negative,positive,positive,positive,positive,positive
1709425502,Do you guys see the issue in the latest version? (ray 2.6),see issue latest version ray,issue,negative,positive,positive,positive,positive,positive
1709400695,"Hello, I have also encountered a similar issue where memory is constantly growing, but my raylet does not output a lot of logs. But I'm sorry I didn't understand what you modified to solve this problem. Can you explain it in detail? Thank you very much.",hello also similar issue memory constantly growing raylet output lot sorry understand solve problem explain detail thank much,issue,negative,negative,neutral,neutral,negative,negative
1709386502,"1.  Can you give me stack trace while crash by using gdb?  eg:  `gdb exmple`
2. How do you compile exmple.cc?  ",give stack trace crash compile,issue,negative,neutral,neutral,neutral,neutral,neutral
1709359752,Would it be easier to just fix forward here?,would easier fix forward,issue,negative,neutral,neutral,neutral,neutral,neutral
1709335525,"Also CC: @brucez-anyscale , @hanming-lu since the stack trace comes from auto-scaler

```
(autoscaler +31m25s) Cluster is terminating (reason: production job/service complete).
[2023-09-06 03:40:42,465 E 3744 3872] core_worker.cc:589: :info_message: Attempting to recover 4 lost objects by resubmitting their tasks. To disable object reconstruction, set @ray.remote(max_retries=0).
*** SIGTERM received at time=1693996843 on cpu 14 ***
PC: @     0x7fe09318a7d1  (unknown)  pthread_cond_timedwait@@GLIBC_2.3.2
    @     0x7fe09318f420  (unknown)  (unknown)
    @ ... and at least 1 more frames
```",also since stack trace come cluster reason production complete recover lost disable object reconstruction set received unknown unknown unknown least,issue,negative,negative,neutral,neutral,negative,negative
1709322023,The previous version of this PR was messed up due to rebase. Sorry about that. It should be clean and ready for review now.,previous version due rebase sorry clean ready review,issue,positive,negative,neutral,neutral,negative,negative
1709315010,"> Just some nits. Thanks for all of this great docs work.
> 
> In case you're wondering about my suggestions to replace `via`. Using `via` is discouraged in technical writing because it is often ambiguous and sometime too formal sounding. I didn't make suggestions for some occurrences of `via` because I wasn't sure what the better word woudl be. Some options are: through, with, using, in. Hope that helps.

Thanks for the added context this is helpful :)",thanks great work case wondering replace via via technical writing often ambiguous sometime formal sounding make via sure better word hope thanks added context helpful,issue,positive,positive,positive,positive,positive,positive
1709314266,"@sihanwang41 docs build and premerge failed. Neither look related, rebasing to retry CI.",build neither look related retry,issue,negative,neutral,neutral,neutral,neutral,neutral
1709247834,I'll revert this PR and open a new one with the fix.,revert open new one fix,issue,negative,positive,neutral,neutral,positive,positive
1709244367,"This broke the documentation build - @allenwang28 @architkulkarni could you take a look?

```
/home/docs/checkouts/readthedocs.org/user_builds/anyscale-ray/checkouts/38669/python/ray/runtime_context.py:docstring of ray.runtime_context.RuntimeContext.get_resource_ids:5: WARNING: Unexpected indentation.
/home/docs/checkouts/readthedocs.org/user_builds/anyscale-ray/checkouts/38669/python/ray/runtime_context.py:docstring of ray.runtime_context.RuntimeContext.get_resource_ids:3: WARNING: Inline interpreted text or phrase reference start-string without end-string.
```",broke documentation build could take look warning unexpected indentation warning text phrase reference without,issue,negative,positive,neutral,neutral,positive,positive
1709236728,"one more place that I found about picking ml-or-not that is based on the old ""cpu-or-not"" logic.",one place found based old logic,issue,negative,positive,neutral,neutral,positive,positive
1709203856,"Failed tests:
- :kubernetes: ❤️‍🩹 🏃 chaos network delay many job submissions unrelated
- doc build runtime_context.py errors unrelated
- test_client_builder   unrelated
- py38 build failure unrelated",chaos network delay many job unrelated doc build unrelated unrelated build failure unrelated,issue,negative,positive,neutral,neutral,positive,positive
1709188155,"@rkooo567 I think this is unfortunate :( 

Let's clean this part with the highest priority after the summit @jjyao ",think unfortunate let clean part highest priority summit,issue,negative,negative,neutral,neutral,negative,negative
1709186423,"yes, please. Just add a `@pytest.mark.skip` should be good",yes please add good,issue,positive,positive,positive,positive,positive,positive
1709185428,"we are planning to fix it for 2.8. If you want to take it out for 2.7 branch, we can do that",fix want take branch,issue,negative,neutral,neutral,neutral,neutral,neutral
1709180464,"<img width=""826"" alt=""Screenshot 2023-09-07 at 6 55 15 AM"" src=""https://github.com/ray-project/ray/assets/18510752/f59c65ec-387c-4a18-a19f-78aeded62e94"">

the new result seems to be within noise range. Closing it",new result within noise range,issue,negative,positive,positive,positive,positive,positive
1709179568,"<img width=""613"" alt=""Screenshot 2023-09-07 at 6 54 14 AM"" src=""https://github.com/ray-project/ray/assets/18510752/3e699002-217a-477c-b40f-5036d853204d"">

Test also seems to be pretty noisy. Closing it",test also pretty noisy,issue,negative,positive,positive,positive,positive,positive
1709179284,"The thing is it failed 3 times straight in the release branch 😅 If there is no bug and we are not planning to fix it, can we take it out for now?",thing time straight release branch bug fix take,issue,negative,positive,positive,positive,positive,positive
1709177853,"I think we don't need to handle this. This is a new test, and the flakiness is due to how test is written not the actual bug. We can fix it in ray 2.8",think need handle new test flakiness due test written actual bug fix ray,issue,negative,positive,neutral,neutral,positive,positive
1709169851,Test failures unrelated or flaky in master. ,test unrelated flaky master,issue,negative,neutral,neutral,neutral,neutral,neutral
1709169478,"would prefer to avoid purely reverting https://github.com/ray-project/ray/pull/38323 as it's a bugfix we prioritized due to user requests

is there another way to solve the above issue without unconditionally injecting the parameter? one possibility would be to inject the parameter lazily upon first invocation. then we can guarantee it happens post-`ray.init`",would prefer avoid purely due user another way solve issue without unconditionally parameter one possibility would inject parameter lazily upon first invocation guarantee,issue,negative,negative,neutral,neutral,negative,negative
1709165576,"cc @gvspraveen @architkulkarni @angelinalg 

Due to the urgency of the release, I opened a PR to the release branch directly. I will cherry-pick the PR to the master branch once it is merged into the release branch.",due urgency release release branch directly master branch release branch,issue,negative,negative,neutral,neutral,negative,negative
1709130053,Adding the concurrency itself lgtm. However it's kind of hidden behavior that we overwrite `--maximum_startup_concurrency` with env var `RAY_maximum_startup_concurrency`. Maybe more comnents on the `src/ray/raylet/main.cc` flag definition?,concurrency however kind hidden behavior overwrite maybe flag definition,issue,positive,positive,positive,positive,positive,positive
1709122487,"Heads up, I think this PR broke the ""broken links"" CI job, fixing with https://github.com/ray-project/ray/pull/39344",think broke broken link job fixing,issue,negative,negative,negative,negative,negative,negative
1709119565,"Specifically, this regression is explained by the bit of extra overhead in unconditionally executing 
```
    setattr(
        function,
        ""__signature__"",
        _add_param_to_signature(
            function,
            inspect.Parameter(
                ""_ray_trace_ctx"", inspect.Parameter.KEYWORD_ONLY, default=None
            ),
        ),
    )
```
during `_inject_tracing_into_function(` when defining a remote function, which had only been executed upon `_is_tracing_enabled()` before this PR. 

Unfortunately, `_is_tracing_enabled()` is not accurate until init is called as it must retrieve the value from the KV store through GCS.",specifically regression bit extra overhead unconditionally function function remote function executed upon unfortunately accurate must retrieve value store,issue,negative,positive,positive,positive,positive,positive
1709030870,"Bisected to: https://github.com/ray-project/ray/pull/38323

Matches attached dashboard for this metric as well. 
<img width=""1231"" alt=""Screenshot 2023-09-06 at 10 36 22 AM"" src=""https://github.com/ray-project/ray/assets/7005244/47ab09c4-1237-4af6-883e-ccabd4304d33"">
",attached dashboard metric well,issue,negative,neutral,neutral,neutral,neutral,neutral
1708988908,"```
2023-09-06 12:41:20,073 INFO agent.py:153 -- Loaded 0 modules.
2023-09-06 12:41:20,075 ERROR agent.py:434 -- Agent is working abnormally. It will exit immediately.
Traceback (most recent call last):
  File ""C:\Users\chenshen\miniconda3\envs\2.7.0rc0-3.7-env\lib\site-packages\ray\dashboard\agent.py"", line 432, in <module>
    loop.run_until_complete(agent.run())
  File ""C:\Users\chenshen\miniconda3\envs\2.7.0rc0-3.7-env\lib\asyncio\base_events.py"", line 587, in run_until_complete
    return future.result()
  File ""C:\Users\chenshen\miniconda3\envs\2.7.0rc0-3.7-env\lib\site-packages\ray\dashboard\agent.py"", line 221, in run
    await self.server.wait_for_termination()
AttributeError: 'NoneType' object has no attribute 'wait_for_termination'
```",loaded error agent working abnormally exit immediately recent call last file line module file line return file line run await object attribute,issue,negative,neutral,neutral,neutral,neutral,neutral
1708902247,"IIRC, we previously discussed and decided not to do this, since it is against most of the ML/data community conventions, e.g.,

```
In [6]: torch.tensor(3)
Out[6]: tensor(3)

In [7]: np.ndarray(3)
Out[7]: array([2.5235972e-316, 4.9406565e-324, 1.3833838e-322])

In [16]: spark.range(3)
Out[16]: DataFrame[id: bigint]
```",previously decided since community tensor array id,issue,negative,negative,negative,negative,negative,negative
1708820157,"Looks like just noise. Latest release test [buildkite](https://buildkite.com/ray-project/release-tests-branch/builds/2076#018a34f1-b050-4289-9527-aa3a69cfc6bc):

```
{""success"":1,""stage_0_time"":10.090721845626831,""stage_1_time"":218.923992395401,""stage_1_avg_iteration_time"":21.892388343811035,""stage_1_max_iteration_time"":22.956207275390625,""stage_1_min_iteration_time"":21.046199083328247,""stage_2_time"":271.77341651916504,""stage_2_avg_iteration_time"":54.35451073646546,""stage_2_max_iteration_time"":55.05571150779724,""stage_2_min_iteration_time"":52.3507604598999,""stage_3_creation_time"":2.6196391582489014,""stage_3_time"":2660.021636247635,""stage_4_spread"":0.7157805181881539,""perf_metrics"":[{""perf_metric_name"":""stage_0_time"",""perf_metric_value"":10.090721845626831,""perf_metric_type"":""LATENCY""},{""perf_metric_name"":""stage_1_avg_iteration_time"",""perf_metric_value"":21.892388343811035,""perf_metric_type"":""LATENCY""},{""perf_metric_name"":""stage_2_avg_iteration_time"",""perf_metric_value"":54.35451073646546,""perf_metric_type"":""LATENCY""},{""perf_metric_name"":""stage_3_creation_time"",""perf_metric_value"":2.6196391582489014,""perf_metric_type"":""LATENCY""},{""perf_metric_name"":""stage_3_time"",""perf_metric_value"":2660.021636247635,""perf_metric_type"":""LATENCY""},{""perf_metric_name"":""stage_4_spread"",""perf_metric_value"":0.7157805181881539,""perf_metric_type"":""LATENCY""}]}
```


Notice `stage_0_time` = 10.090721845626831 which is not as good, but it is a mere 7% regression so not very significant.",like noise latest release test success latency latency latency latency latency latency notice good mere regression significant,issue,positive,positive,positive,positive,positive,positive
1708817753,"Looks like just noise. Latest release test [buildkite](https://buildkite.com/ray-project/release-tests-branch/builds/2076#018a34f1-b050-4289-9527-aa3a69cfc6bc):

```
{""success"":1,""stage_0_time"":10.090721845626831,""stage_1_time"":218.923992395401,""stage_1_avg_iteration_time"":21.892388343811035,""stage_1_max_iteration_time"":22.956207275390625,""stage_1_min_iteration_time"":21.046199083328247,""stage_2_time"":271.77341651916504,""stage_2_avg_iteration_time"":54.35451073646546,""stage_2_max_iteration_time"":55.05571150779724,""stage_2_min_iteration_time"":52.3507604598999,""stage_3_creation_time"":2.6196391582489014,""stage_3_time"":2660.021636247635,""stage_4_spread"":0.7157805181881539,""perf_metrics"":[{""perf_metric_name"":""stage_0_time"",""perf_metric_value"":10.090721845626831,""perf_metric_type"":""LATENCY""},{""perf_metric_name"":""stage_1_avg_iteration_time"",""perf_metric_value"":21.892388343811035,""perf_metric_type"":""LATENCY""},{""perf_metric_name"":""stage_2_avg_iteration_time"",""perf_metric_value"":54.35451073646546,""perf_metric_type"":""LATENCY""},{""perf_metric_name"":""stage_3_creation_time"",""perf_metric_value"":2.6196391582489014,""perf_metric_type"":""LATENCY""},{""perf_metric_name"":""stage_3_time"",""perf_metric_value"":2660.021636247635,""perf_metric_type"":""LATENCY""},{""perf_metric_name"":""stage_4_spread"",""perf_metric_value"":0.7157805181881539,""perf_metric_type"":""LATENCY""}]}
```


Notice `stage_3_creation_time` = 2.6196391582489014 which is good.",like noise latest release test success latency latency latency latency latency latency notice good,issue,positive,positive,positive,positive,positive,positive
1708814835,"@kevin85421 Fixed, sorry for the unnecessary back and forth",fixed sorry unnecessary back forth,issue,negative,negative,negative,negative,negative,negative
1708809435,"Passes locally now after the fix.

```
(ray-py38) archit@archit-C02D27V8MD6N jobs_tests % python workloads/jobs_basic.py --working-dir ""https://github.com/anyscale/job-services-cuj-examples/archive/refs/heads/main.zip"" 
status: PENDING
status: PENDING
status: PENDING
status: PENDING
status: PENDING
status: PENDING
status: PENDING
status: PENDING
status: PENDING
status: PENDING
status: PENDING
status: RUNNING
status: RUNNING
status: RUNNING
status: RUNNING
status: RUNNING
status: RUNNING
status: RUNNING
status: RUNNING
status: RUNNING
status: RUNNING
status: SUCCEEDED
Status message:  Job finished successfully.
PASSED
```",locally fix python status pending status pending status pending status pending status pending status pending status pending status pending status pending status pending status pending status running status running status running status running status running status running status running status running status running status running status status message job finished successfully,issue,negative,positive,positive,positive,positive,positive
1708808062,Oh sorry I must have messed up my merge let me take a look,oh sorry must merge let take look,issue,negative,negative,negative,negative,negative,negative
1708803987,"Test run: https://buildkite.com/ray-project/release-tests-pr/builds/52156#018a689a-d80d-4943-98ce-8630157e721c

| Metric Name              | PR Value  | Master Value  |
|--------------------------|----------|---------------|
| actors_per_second        | 927.895  | 906.765       |
| dashboard_p50_latency_ms | 12.033   | 26.096        |
| dashboard_p95_latency_ms | 2181.354 | 8930.232      |
| dashboard_p99_latency_ms | 8320.816 | 8930.232      |
",test run metric name value master value,issue,positive,neutral,neutral,neutral,neutral,neutral
1708803202,Yeah it's a typo. I'll put up a fix there. ,yeah typo put fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1708801424,"That must be the issue:
```
Trial training_function_ce2e7_00001 errored after 0 iterations at 2023-09-06 10:21:32. Total running time: 4s
Error file: /Users/archit/ray_results/training_function_2023-09-06_10-21-24/training_function_ce2e7_00001_1_alpha=0.0100,beta=3_2023-09-06_10-21-27/error.txt
2023-09-06 10:21:32,304 ERROR tune_controller.py:1507 -- Trial task failed for trial training_function_ce2e7_00000
Traceback (most recent call last):
  File ""/Users/archit/ray/python/ray/air/execution/_internal/event_manager.py"", line 110, in resolve_future
    result = ray.get(future)
  File ""/Users/archit/ray/python/ray/_private/auto_init_hook.py"", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File ""/Users/archit/ray/python/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
  File ""/Users/archit/ray/python/ray/_private/worker.py"", line 2554, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): ray::ImplicitFunc.train() (pid=97546, ip=127.0.0.1, actor_id=65079a35898b6d9f1e8db18702000000, repr=training_function)
  File ""/Users/archit/ray/python/ray/tune/trainable/trainable.py"", line 394, in train
    raise skipped from exception_cause(skipped)
  File ""/Users/archit/ray/python/ray/air/_internal/util.py"", line 91, in run
    self._ret = self._target(*self._args, **self._kwargs)
  File ""/Users/archit/ray/python/ray/tune/trainable/function_trainable.py"", line 326, in <lambda>
    training_func=lambda: self._trainable_func(
  File ""/Users/archit/ray/python/ray/tune/trainable/function_trainable.py"", line 801, in _trainable_func
    output = fn()
  File ""run_simple_tune_job.py"", line 18, in training_function
    train.report(mean_loss=intermediate_score)
  File ""/Users/archit/ray/python/ray/train/_internal/session.py"", line 770, in wrapper
    return fn(*args, **kwargs)
TypeError: report() got an unexpected keyword argument 'mean_loss'
```",must issue trial total running time error file error trial task trial recent call last file line result future file line return file line wrapper return file line get raise ray file line train raise file line run file line lambda file line output file line file line wrapper return report got unexpected argument,issue,negative,positive,neutral,neutral,positive,positive
1708794735,"@justinvyu would you be able to look into this?  I'm able to reproduce it locally:

```
ray start --head
```

```
(ray-py38) archit@archit-C02D27V8MD6N jobs_tests % python workloads/jobs_basic.py --working-dir ""https://github.com/anyscale/job-services-cuj-examples/archive/refs/heads/main.zip"" 
status: PENDING
status: PENDING
status: PENDING
status: PENDING
status: PENDING
status: PENDING
status: PENDING
status: PENDING
status: PENDING
status: PENDING
status: RUNNING
status: RUNNING
status: RUNNING
status: RUNNING
status: RUNNING
status: RUNNING
status: RUNNING
status: RUNNING
status: RUNNING
status: RUNNING
status: RUNNING
status: FAILED
Status message:  Job entrypoint command failed with exit code 1, last available logs (truncated to 20,000 chars):
(ImplicitFunc pid=93615)   storage_local_path=/Users/archit/ray_results [repeated 2x across cluster]
(ImplicitFunc pid=93615)   storage_filesystem=<pyarrow._fs.LocalFileSystem object at 0x7fc43b7bd570> [repeated 2x across cluster]
(ImplicitFunc pid=93615)   storage_fs_path=/Users/archit/ray_results [repeated 2x across cluster]
(ImplicitFunc pid=93615)   current_checkpoint_index=0 [repeated 6x across cluster]
(ImplicitFunc pid=93615) > [repeated 2x across cluster]
(ImplicitFunc pid=93615) /Users/archit/ray/python/ray/train/_internal/syncer.py:96: UserWarning: `SyncConfig(upload_dir)` is a deprecated configuration and will be ignored. Please remove it from your `SyncConfig`, as this will raise an error in a future version of Ray. [repeated 2x across cluster]
(ImplicitFunc pid=93615) Please specify `train.RunConfig(storage_path)` instead. [repeated 2x across cluster]
(ImplicitFunc pid=93615)   warnings.warn( [repeated 4x across cluster]
(ImplicitFunc pid=93615) /Users/archit/ray/python/ray/train/_internal/syncer.py:96: UserWarning: `SyncConfig(syncer)` is a deprecated configuration and will be ignored. Please remove it from your `SyncConfig`, as this will raise an error in a future version of Ray. [repeated 2x across cluster]
(ImplicitFunc pid=93615) Please implement custom syncing logic with a custom `pyarrow.fs.FileSystem` instead, and pass it into `train.RunConfig(storage_filesystem)`. [repeated 2x across cluster]

Traceback (most recent call last):
  File ""workloads/jobs_basic.py"", line 73, in <module>
    assert status == JobStatus.SUCCEEDED
AssertionError
(ray-py38) archit@archit-C02D27V8MD6N jobs_tests % python workloads/jobs_basic.py --working-dir ""workloads""
2023-09-06 10:15:10,186 INFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_0d7ce4205ca83a9e.zip.
2023-09-06 10:15:10,186 INFO packaging.py:518 -- Creating a file package for local directory 'workloads'.
status: PENDING
status: PENDING
status: RUNNING
status: RUNNING
status: RUNNING
status: RUNNING
status: RUNNING
status: RUNNING
status: RUNNING
status: RUNNING
status: RUNNING
status: SUCCEEDED
Status message:  Job finished successfully.
PASSED
```",would able look able reproduce locally ray start head python status pending status pending status pending status pending status pending status pending status pending status pending status pending status pending status running status running status running status running status running status running status running status running status running status running status running status status message job command exit code last available truncated repeated across cluster object repeated across cluster repeated across cluster repeated across cluster repeated across cluster configuration please remove raise error future version ray repeated across cluster please specify instead repeated across cluster repeated across cluster configuration please remove raise error future version ray repeated across cluster please implement custom logic custom instead pas repeated across cluster recent call last file line module assert status python package file package local directory status pending status pending status running status running status running status running status running status running status running status running status running status status message job finished successfully,issue,positive,positive,positive,positive,positive,positive
1708789008,"verify with early commits it still show same results with latest infra 
ef2a0402a3a147cb127d7d15fd48b9411db25c64 (Queued task time: 190.6656566590002 (1000000 tasks)). 
not a regression on ray core side",verify early still show latest infra task time regression ray core side,issue,negative,positive,positive,positive,positive,positive
1708769447,`get_worker_id()` is already added to RuntimeContext so we just need to add `get_actor_name()`.,already added need add,issue,negative,neutral,neutral,neutral,neutral,neutral
1708755023,"Failed tests:

- tests:test_task_events 
- tests:test_client_library_integration
- serve:test_air_integrations  
- serve:tutorial_rllib  
Linkcheck failure: 
```
(serve/getting_started: line  260) broken    https://www.starlette.io/ - HTTPSConnectionPool(host='www.starlette.io', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f7dccd65670>: Failed to establish a new connection: [Errno -2] Name or service not known'))
--
  | (serve/http-guide: line   30) broken    https://www.starlette.io/requests/ - HTTPSConnectionPool(host='www.starlette.io', port=443): Max retries exceeded with url: /requests/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f7dccd65190>: Failed to establish a new connection: [Errno -2] Name or service not known'))
  | (serve/http-guide: line  101) broken    https://www.starlette.io/responses/#streamingresponse - HTTPSConnectionPool(host='www.starlette.io', port=443): Max retries exceeded with url: /responses/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f7dccd65670>: Failed to establish a new connection: [Errno -2] Name or service not known'))
  | (serve/http-guide: line   30) broken    https://www.starlette.io/responses/ - HTTPSConnectionPool(host='www.starlette.io', port=443): Max retries exceeded with url: /responses/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f7dc9652640>: Failed to establish a new connection: [Errno -2] Name or service not known'))
```



All unrelated",serve serve failure line broken object establish new connection name service known line broken object establish new connection name service known line broken object establish new connection name service known line broken object establish new connection name service known unrelated,issue,negative,negative,negative,negative,negative,negative
1708752679,@akshay-anyscale yea i'll remove that in my other PR that's updating that file entirely,yea remove file entirely,issue,negative,neutral,neutral,neutral,neutral,neutral
1708749695,"@kevin85421 ah thanks I forgot to save that file, could you review it again?",ah thanks forgot save file could review,issue,positive,positive,positive,positive,positive,positive
1708665334,"@mattip 

Sorry for missing this one. Currently the team is busy with Ray summit, we will review after that.",sorry missing one currently team busy ray summit review,issue,negative,negative,negative,negative,negative,negative
1708628785,@edoakes just in case you missed this. It's ready to merge. All related tests are passing,case ready merge related passing,issue,negative,positive,neutral,neutral,positive,positive
1708614791,"Ah yes this is fixed in master, I will make a cherry-pick PR for this.",ah yes fixed master make,issue,negative,positive,neutral,neutral,positive,positive
1708573607,@rynewang Please pass to new oncall or find a new owner if you are no longer oncall,please pas new find new owner longer,issue,negative,positive,positive,positive,positive,positive
1708563228,@matthewdeng I know your team is still working to fix it. But just create this issue to track it,know team still working fix create issue track,issue,negative,neutral,neutral,neutral,neutral,neutral
1708547326,"@iycheng do you think the libjemalloc inclusion could have to do with this?

https://console.anyscale-staging.com/o/anyscale-internal/jobs/prodjob_npevmtbb57nlc9yjgrsdq3adh1

```
(_RemoteRayLightGBMActor pid=233, ip=10.0.57.106) *** SIGSEGV received at time=1693992747 on cpu 1 ***
(_RemoteRayLightGBMActor pid=233, ip=10.0.57.106) PC: @     0x7fb669cf1087  (unknown)  LightGBM::DenseBin<>::ConstructHistogram()
(_RemoteRayLightGBMActor pid=233, ip=10.0.57.106) [2023-09-06 02:32:27,352 E 233 282] logging.cc:361: *** SIGSEGV received at time=1693992747 on cpu 1 ***
(_RemoteRayLightGBMActor pid=233, ip=10.0.57.106) [2023-09-06 02:32:27,352 E 233 282] logging.cc:361: PC: @     0x7fb669cf1087  (unknown)  LightGBM::DenseBin<>::ConstructHistogram()
(_RemoteRayLightGBMActor pid=233, ip=10.0.57.106) Fatal Python error: Segmentation fault
(_RemoteRayLightGBMActor pid=233, ip=10.0.57.106) [failure_signal_handler.cc : 329] RAW: Signal 11 raised at PC=0x7fb669cf1038 while already in AbslFailureSignalHandler() [repeated 3x across cluster]
(_RemoteRayLightGBMActor pid=233, ip=10.0.57.106) 
(_RemoteRayLightGBMActor pid=233, ip=10.0.57.106)     @     0x7fb7c2ebf420  (unknown)  (unknown)
(_RemoteRayLightGBMActor pid=233, ip=10.0.57.106) [2023-09-06 02:32:27,353 E 233 282] logging.cc:361:     @     0x7fb7c2ebf420  (unknown)  (unknown)
(_RemoteRayLightGBMActor pid=236, ip=10.0.63.212)     @     0x7f50ceef9420  (unknown)  (unknown)
(_RemoteRayLightGBMActor pid=236, ip=10.0.63.212) [2023-09-06 02:32:27,352 E 236 281] logging.cc:361:     @     0x7f50ceef9420  (unknown)  (unknown)
(_RemoteRayLightGBMActor pid=236, ip=10.0.63.212) 
(_RemoteRayLightGBMActor pid=236, ip=10.0.63.212) Stack (most recent call first):
(_RemoteRayLightGBMActor pid=236, ip=10.0.63.212)   File ""/home/ray/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py"", line 3557 in update
(_RemoteRayLightGBMActor pid=236, ip=10.0.63.212)   File ""/home/ray/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py"", line 266 in train
(_RemoteRayLightGBMActor pid=236, ip=10.0.63.212)   File ""/home/ray/anaconda3/lib/python3.8/site-packages/lightgbm/sklearn.py"", line 842 in fit
(_RemoteRayLightGBMActor pid=236, ip=10.0.63.212)   File ""/home/ray/anaconda3/lib/python3.8/site-packages/lightgbm_ray/main.py"", line 465 in _train
(_RemoteRayLightGBMActor pid=236, ip=10.0.63.212)   File ""/home/ray/anaconda3/lib/python3.8/threading.py"", line 870 in run
(_RemoteRayLightGBMActor pid=236, ip=10.0.63.212)   File ""/home/ray/anaconda3/lib/python3.8/threading.py"", line 932 in _bootstrap_inner
(_RemoteRayLightGBMActor pid=236, ip=10.0.63.212)   File ""/home/ray/anaconda3/lib/python3.8/threading.py"", line 890 in _bootstrap
```

",think inclusion could received unknown received unknown fatal python error segmentation fault raw signal raised already repeated across cluster unknown unknown unknown unknown unknown unknown unknown unknown stack recent call first file line update file line train file line fit file line file line run file line file line,issue,negative,negative,neutral,neutral,negative,negative
1708459193,Look like it's still broken in the latest run :(: https://buildkite.com/ray-project/release-tests-branch/builds/2129#018a69d0-7cd5-4db8-9674-3ae60d8d2b84,look like still broken latest run,issue,negative,positive,neutral,neutral,positive,positive
1708414641,"@scottjlee can i have your stamp on this PR again - i merged https://github.com/ray-project/ray/pull/39042 into this PR so I can land the whole stack and now it requires approval from code owner again, thanks",stamp land whole stack approval code owner thanks,issue,positive,positive,positive,positive,positive,positive
1708201669,this is much easier to review. thanks so much for the splitting.,much easier review thanks much splitting,issue,positive,positive,positive,positive,positive,positive
1708002584,@rkooo567 sounds good! the above is actually a bot that acts under my profile pic.,good actually bot profile pic,issue,negative,positive,positive,positive,positive,positive
1708000185,"@can-anyscale this is a real issue, and we should fix this. The test is just flaky when ""PA is enabled"" because of new code we added.  ",real issue fix test flaky pa new code added,issue,negative,positive,positive,positive,positive,positive
1707990273,"So far I see 2 possible solutions: 

- copy methods from `Episode` (V1) to `EpisodeV2` such as `(set_)last_observation_for` and `(set_)last_row_obs_for` and update `EnvRunnerV2` to call them. This could be done while looping over `env_obs` in https://github.com/ray-project/ray/blob/ca0e04994edcbbced2a2b18215b7ebf8d47c7bce/rllib/evaluation/env_runner_v2.py#L533 with something like: 
```python
# Collect raw and filtered observations
episode._set_last_raw_obs(agent_id, obs)
filtered_obs = _get_or_raise(self._worker.filters, policy_id)(obs)
episode._set_last_observation(agent_id, filtered_obs)
```
- use `_agent_collectors` from `EpisodeV2`, with something like `episode._agent_collectors[agent_id].buffers[SampleBatch.OBS][-1][-1]` to get last processed observation. Not sure how to get raw obs though.

@ArturNiederfahrenhorst let me know your thoughts, I'd be happy to work on a PR.",far see possible copy episode update call could done looping something like python collect raw use something like get last observation sure get raw though let know happy work,issue,positive,positive,positive,positive,positive,positive
1707987076,cc @jjyao @scv119 can you give me a final go and close the issue? ,give final go close issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1707985085,"```
[2023-09-04 01:49:03,500 C 208 208] (gcs_server) gcs_autoscaler_state_manager.cc:140:  Check failed: pg_data.state() == rpc::PlacementGroupTableData::PENDING || pg_data.state() == rpc::PlacementGroupTableData::RESCHEDULING Placement group load should only include pending/rescheduling PGs. 
*** StackTrace Information ***
/home/ray/anaconda3/lib/python3.8/site-packages/ray/core/src/ray/gcs/gcs_server(+0xa4fb5a) [0x560ced270b5a] ray::operator<<()
/home/ray/anaconda3/lib/python3.8/site-packages/ray/core/src/ray/gcs/gcs_server(+0xa51632) [0x560ced272632] ray::SpdLogMessage::Flush()
/home/ray/anaconda3/lib/python3.8/site-packages/ray/core/src/ray/gcs/gcs_server(+0xa51947) [0x560ced272947] ray::RayLog::~RayLog()
/home/ray/anaconda3/lib/python3.8/site-packages/ray/core/src/ray/gcs/gcs_server(+0x39b52b) [0x560cecbbc52b] ray::gcs::GcsAutoscalerStateManager::GetPendingGangResourceRequests()
/home/ray/anaconda3/lib/python3.8/site-packages/ray/core/src/ray/gcs/gcs_server(+0x39c866) [0x560cecbbd866] ray::gcs::GcsAutoscalerStateManager::MakeClusterResourceStateInternal()
/home/ray/anaconda3/lib/python3.8/site-packages/ray/core/src/ray/gcs/gcs_server(+0x39c93c) [0x560cecbbd93c] ray::gcs::GcsAutoscalerStateManager::HandleGetClusterResourceState()
/home/ray/anaconda3/lib/python3.8/site-packages/ray/core/src/ray/gcs/gcs_server(+0x285864) [0x560cecaa6864] ray::rpc::ServerCallImpl<>::HandleRequestImpl()
/home/ray/anaconda3/lib/python3.8/site-packages/ray/core/src/ray/gcs/gcs_server(+0x5f8366) [0x560cece19366] EventTracker::RecordExecution()
/home/ray/anaconda3/lib/python3.8/site-packages/ray/core/src/ray/gcs/gcs_server(+0x5ea1ce) [0x560cece0b1ce] std::_Function_handler<>::_M_invoke()
/home/ray/anaconda3/lib/python3.8/site-packages/ray/core/src/ray/gcs/gcs_server(+0x5ea726) [0x560cece0b726] boost::asio::detail::completion_handler<>::do_complete()
/home/ray/anaconda3/lib/python3.8/site-packages/ray/core/src/ray/gcs/gcs_server(+0xb50cfb) [0x560ced371cfb] boost::asio::detail::scheduler::do_run_one()
/home/ray/anaconda3/lib/python3.8/site-packages/ray/core/src/ray/gcs/gcs_server(+0xb52de9) [0x560ced373de9] boost::asio::detail::scheduler::run()
/home/ray/anaconda3/lib/python3.8/site-packages/ray/core/src/ray/gcs/gcs_server(+0xb532a2) [0x560ced3742a2] boost::asio::io_context::run()
/home/ray/anaconda3/lib/python3.8/site-packages/ray/core/src/ray/gcs/gcs_server(+0x183099) [0x560cec9a4099] main
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf3) [0x7f79fe85e083] __libc_start_main
/home/ray/anaconda3/lib/python3.8/site-packages/ray/core/src/ray/gcs/gcs_server(+0x1cfc67) [0x560cec9f0c67]
```

Root cause seems like it is because the REMOVED placement group could be reported as a load. ",check placement group load include information ray ray ray ray ray ray ray boost boost boost boost main root cause like removed placement group could load,issue,positive,positive,positive,positive,positive,positive
1707977230,I have the same error and passing framework to _validate (as done in the PR by @simonsays1980) fixes the issue!,error passing framework done issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1707894276,"<img width=""901"" alt=""image"" src=""https://github.com/ray-project/ray/assets/44538064/c8049f81-c242-4d92-a079-ce50b54c3ac4"">

In the example you provided, after ds get materialized, its underlying blocks are mixed with both pyarrow.Table and pandas.DataFrame, and the function above only uses the first item do decide which BlockAccessor it should use, which is pyarrow.Table. Therefore when it handles a pandas.DataFrame an error is triggered.

I think from the APIs here, mixing pyarrow.Table and pandas.DataFrame in List[Block] should legal. Probably this API shouldn't assume all Blocks are the same format. However if you want to get around this quickly, I think you can just drop `batch_format='pandas'` so all blocks will be pyarrow.Table and everything will be fine. You can always convert the dataset to pandas using `ds.to_pandas()`.",image example provided get underlying mixed function first item decide use therefore error triggered think list block legal probably assume format however want get around quickly think drop everything fine always convert,issue,negative,positive,positive,positive,positive,positive
1707886922,"I dig a little into this. I think it's because in this file `ray/data/_internal/planner/exchange/aggregate_task_spec.py`:
```
@staticmethod
    def reduce(
        key: Optional[str],
        aggs: List[AggregateFn],
        *mapper_outputs: List[Block],
        partial_reduce: bool = False,
    ) -> Tuple[Block, BlockMetadata]:
        return BlockAccessor.for_block(mapper_outputs[0]).aggregate_combined_blocks(
            list(mapper_outputs), key, aggs, finalize=not partial_reduce
        )
```
It uses the first item in mapper_outputs to get a BlockAccessor for either pandas or pyarrow tables, but in your case, mapper_outputs contains both `pandas.DataFrame` and `pyarraw.Table`, and trying to use ArrowBlockAccessor on a pandas.DataFrame caused this error, which is obvious because in your error logs it's clearly using methods in `arrow_block.py`.",dig little think file reduce key optional list list block bool false block return list key first item get either table case trying use error obvious error clearly,issue,negative,negative,neutral,neutral,negative,negative
1707746962,"@aslonnie I split this into two PRs:
- https://github.com/ray-project/ray/pull/39299 that does renaming
- https://github.com/ray-project/ray/pull/39300 that turn container into objects

I keep all the test files at the top level directory so the changes are smaller

You can safely ignore this PR",split two turn container keep test top level directory smaller safely ignore,issue,negative,positive,positive,positive,positive,positive
1707617615,"Hello @scottjlee, could you please provide me with some additional information? I'm new to contributing and would like to get started on this.",hello could please provide additional information new would like get,issue,positive,positive,positive,positive,positive,positive
1707561123,This test is passing on release branch so it should not be a release blocker https://buildkite.com/ray-project/release-tests-branch/builds/2128#018a6766-3847-4170-bcff-c1d3f2835677?,test passing release branch release blocker,issue,negative,neutral,neutral,neutral,neutral,neutral
1707413036,hey @architkulkarni looks like approvals are done; can you take a look at tests and merge if tests-ok?,hey like done take look merge,issue,negative,neutral,neutral,neutral,neutral,neutral
1707328821,"Thanks, the update looks great!

> Hey @jjyao - I understand, thanks for the heads up. @richardliaw is actually announcing this feature today so I am a bit worried about that timeline of post 2.7. I'm wondering if we can push this version through in time for the 2.7 release? But I can definitely help out with the refactor after the design if you need an extra set of hands!

I'm not familiar with the announcement unfortunately, @richardliaw can you confirm whether or not this PR should be included in Ray 2.7?",thanks update great hey understand thanks actually feature today bit worried post wondering push version time release definitely help design need extra set familiar announcement unfortunately confirm whether included ray,issue,positive,positive,positive,positive,positive,positive
1707322012,Blocked on https://github.com/ray-project/ray/pull/39210 because we need grpc to be upgraded to >= 1.49.0 to have  https://github.com/grpc/grpc/pull/30567. Once our #39210 is merged I will merge to this PR and CI should work.,blocked need merge work,issue,negative,neutral,neutral,neutral,neutral,neutral
1707258311,"> how important is `/nodes?view=summary` ?

AFAIK, this is what frontend queries for the Cluster overview page. ",important cluster overview page,issue,negative,positive,positive,positive,positive,positive
1707211677,"gce passing: https://buildkite.com/ray-project/release-tests-pr/builds/52088#018a66a7-b881-4a10-b7ed-6eff9e344c0a
sanity check aws passing: https://buildkite.com/ray-project/release-tests-pr/builds/51959#018a53ec-deaf-43b8-ae83-631730009569

Time out is increased since the test specifically tested auto scaling behavior which takes time. Last run takes 636.582953317 s.",passing sanity check passing time since test specifically tested auto scaling behavior time last run,issue,negative,neutral,neutral,neutral,neutral,neutral
1707169742,"oh - i think we are now 
1. fetching logical resources as part of node's summary. https://github.com/ray-project/ray/commit/4ca8b235ffce901c2b41e3fdba642f444836f8e6#diff-fef1d004a19e775c3387880dbf89709d33990509484863185274892f0e4bae2a

2. And https://github.com/ray-project/ray/commit/329b409b54c7dcb2801a102f74702e61be2d8872 that adds V2 PA supports

We should proabbaly run it on v1 to see if any of these 2 PRs are the reasons. 

Also, is this a release blocker? We never tracked this dashboard perf thing in any of previous release. I am not sure how reliable the signals are. ",oh think fetching logical part node summary pa run see also release blocker never tracked dashboard thing previous release sure reliable,issue,negative,positive,positive,positive,positive,positive
1707114106,"Task fails because the Head Node has enough memory whereas Workers do not. We need the Rank 0 Worker and the Trainer should co-locate on the same Node.

Can we just change the test to specify the memory to what we need and than it should automatically be placed onto the right Node.

@woshiyyya to take the action item on specifying this so the Worker and Trainer do get co-located on the Head Node properly. For Ray28 we'll need to figure out a better UX for implicit Worker/Trainer co-location (especially with the rank0 worker(s)) AI on @matthewdeng ",task head node enough memory whereas need rank worker trainer node change test specify memory need automatically onto right node take action item worker trainer get head node properly ray need figure better implicit especially rank worker ai,issue,negative,negative,negative,negative,negative,negative
1707064615,"A more recent run on the 2.7 branch seems to be better: https://github.com/ray-project/ray/pull/39025#issuecomment-1706745333
```
REGRESSION 5.76%: single_client_tasks_async (THROUGHPUT) regresses from 10940.075908384719 to 10309.812853287176 (5.76%) in 2.7.0/microbenchmark.json

```
",recent run branch better regression throughput,issue,negative,positive,positive,positive,positive,positive
1707061145,"This has been hitting [11k -> 9.5k] on the master - more of a variance than regression IMO. 

![image](https://github.com/ray-project/ray/assets/11676094/c930bcb4-f2d4-4133-b699-7c1a57c5c214)

https://b534fd88.us1a.app.preset.io/superset/explore/p/Woy7xr59kq1/

cc @GeneDer @rkooo567 ",master variance regression image,issue,negative,neutral,neutral,neutral,neutral,neutral
1707040276,I guess we should reopen the issue until we see it pass on master and on the release branch.  cc @can-anyscale ,guess reopen issue see pas master release branch,issue,negative,neutral,neutral,neutral,neutral,neutral
1707035451,"The ""remote"" release test pulls the scripts from the head of `main` for that repo",remote release test head main,issue,negative,positive,neutral,neutral,positive,positive
1707034434,Great! Do you mind making the necessary change? https://github.com/anyscale/job-services-cuj-examples/,great mind making necessary change,issue,positive,positive,positive,positive,positive,positive
1707027226,"Ohh, I see, thanks for looking into this so quickly! Yup, we also need to make the same changes to this uploaded `zip`, otherwise it will error.",see thanks looking quickly also need make zip otherwise error,issue,negative,positive,positive,positive,positive,positive
1707019548,"Ray Jobs is owned by platform team but I can help out.  There's two independent things here, but I'm not sure which is relevant.

- `pip: ""ray[tune]""` will run `pip install ray[tune]`, so it will pull the latest release if tune isn't already installed, but I don't think it will downgrade tune if it's already installed.  Not sure which case is happening here.

- The difference betwen `jobs_basic_local_working_dir` and `jobs_basic_remote_working_dir` is that in the local test, `run_simple_tune_job.py` comes from Ray master, and in the remote test, `run_simple_tune_job.py` comes from some fixed working_dir zip that was uploaded a long time ago.  Since your PR changes that file on Ray master, that's probably related.  Do the changes in this file https://github.com/ray-project/ray/pull/38478/files#diff-008ef2cc6049b2ebe824de22d78421b63a931bf1b1163de9f8be4fa0eb6c9d6a give you any hints?

I can't read your link, it says ""The page /o/anyscale-internal/jobs/prodjob_gh1bqtz52yvfbjlfhi32jtujcn could not be found.""

I'm going to just try removing `ray[tune]` from the runtime env.  ",ray platform team help two independent sure relevant pip ray tune run pip install ray tune pull latest release tune already think downgrade tune already sure case happening difference local test come ray master remote test come fixed zip long time ago since file ray master probably related file give ca read link page could found going try removing ray tune,issue,positive,positive,positive,positive,positive,positive
1707003730,"After trying it out a bit more, I think `FailureConfig` is enough for my use case for now. Sorry about the confusion - closing the issue!",trying bit think enough use case sorry confusion issue,issue,negative,negative,negative,negative,negative,negative
1706997061,"@architkulkarni Are you the release test owner here?

The blamed commit shouldn't be raising an error here -- it deprecates `tune.report` usage but this release test has already been migrated to the new API.

I see a runtime env here:

```python
        runtime_env={
            ""pip"": [""ray[tune]""],
            ""working_dir"": args.working_dir,
        },
```

Does this overwrite master's version of Ray with the latest release? I'm not so sure this is the problem though, since the `jobs_basic_local_working_dir` release test is working fine. Any ideas what may be happening here?

https://console.anyscale-staging.com/o/anyscale-internal/jobs/prodjob_gh1bqtz52yvfbjlfhi32jtujcn",release test owner blamed commit raising error usage release test already new see python pip ray tune overwrite master version ray latest release sure problem though since release test working fine may happening,issue,negative,positive,positive,positive,positive,positive
1706968093,@aslonnie can you help merge this if you are cool with the change? 🙏,help merge cool change,issue,positive,positive,positive,positive,positive,positive
1706960993,"@anyscalesam Yea, this is kinda complicated. The release test for serve is fix, but the CI is still failing due to fetching logs for long running jobs takes too long and Buildkite just kills agent in this case and fail the test. I have another PR trying to fix that https://github.com/ray-project/ray/pull/39146 Hopefully Lonnie will be able to approve it and get this closed ",yea complicated release test serve fix still failing due fetching long running long agent case fail test another trying fix hopefully able approve get closed,issue,negative,negative,negative,negative,negative,negative
1706958981,"Yeah I think this is working as intended:

```python
import asyncio
import time

import ray


@ray.remote
class Sleeper:
    async def sleep(self):
        async def print_and_sleep():
            try:
                print(""Inside print_and_sleep"")
                await asyncio.sleep(3)
                print(""print_and_sleep done"")
            except asyncio.CancelledError:
                print(""print_and_sleep cancelled"")

        try:
            print(""Sleeper received request!"")
            await asyncio.shield(print_and_sleep())
            print(""Sleeper finished sleeping!"")
            return ""hi""
        except asyncio.CancelledError:
            print(""Sleeper cancelled"")

sleeper = Sleeper.remote()
obj_ref = sleeper.sleep.remote()
time.sleep(1)
ray.cancel(obj_ref)

time.sleep(5)
```

```
(ray) eoakes@Edwards-MacBook-Pro-2 serve % python core.py
2023-09-05 11:42:42,252 INFO worker.py:1640 -- Started a local Ray instance. View the dashboard at http://127.0.0.1:8265
(Sleeper pid=71751) Sleeper received request!
(Sleeper pid=71751) Inside print_and_sleep
(Sleeper pid=71751) Sleeper cancelled
(Sleeper pid=71751) print_and_sleep done
```",yeah think working intended python import import time import ray class sleeper sleep self try print inside await print done except print try print sleeper received request await print sleeper finished sleeping return hi except print sleeper sleeper ray serve python local ray instance view dashboard sleeper sleeper received request sleeper inside sleeper sleeper sleeper done,issue,negative,neutral,neutral,neutral,neutral,neutral
1706958310,"Wait, sorry, this is still expected as per the asyncio docs... `sleep` should get cancelled but `print_and_sleep` should not.",wait sorry still per sleep get,issue,negative,negative,negative,negative,negative,negative
1706957911,"@shrekris-anyscale looks to be a core issue:
```python
import asyncio
import time

import ray


@ray.remote
class Sleeper:
    async def sleep(self):
        async def print_and_sleep():
            print(""Inside print_and_sleep"")
            await asyncio.sleep(3)
            print(""print_and_sleep done"")

        try:
            print(""Sleeper received request!"")
            await asyncio.shield(print_and_sleep())
            print(""Sleeper finished sleeping!"")
            return ""hi""
        except asyncio.CancelledError:
            print(""Sleeper was cancelled -- this should not happen!"")

sleeper = Sleeper.remote()
obj_ref = sleeper.sleep.remote()
time.sleep(1)
ray.cancel(obj_ref)

assert ray.get(obj_ref) == ""hi""
```

```
2023-09-05 11:39:40,893 INFO worker.py:1640 -- Started a local Ray instance. View the dashboard at http://127.0.0.1:8265
(Sleeper pid=71360) Sleeper received request!
(Sleeper pid=71360) Inside print_and_sleep
(Sleeper pid=71360) Sleeper was cancelled -- this should not happen!
Traceback (most recent call last):
  File ""core.py"", line 28, in <module>
    assert ray.get(obj_ref) == ""hi""
  File ""/Users/eoakes/code/ray/python/ray/_private/auto_init_hook.py"", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File ""/Users/eoakes/code/ray/python/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
  File ""/Users/eoakes/code/ray/python/ray/_private/worker.py"", line 2554, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TaskCancelledError): ray::Sleeper.sleep() (pid=71360, ip=127.0.0.1, actor_id=5ba53e8bb158abafd4eb465601000000, repr=<core.Sleeper object at 0x105142880>)
    raise CancelledError()
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

ray::Sleeper.sleep() (pid=71360, ip=127.0.0.1, actor_id=5ba53e8bb158abafd4eb465601000000, repr=<core.Sleeper object at 0x105142880>)
ray.exceptions.TaskCancelledError: Task: TaskID(16310a0f0a45af5c5ba53e8bb158abafd4eb465601000000) was cancelled.
```",core issue python import import time import ray class sleeper sleep self print inside await print done try print sleeper received request await print sleeper finished sleeping return hi except print sleeper happen sleeper assert hi local ray instance view dashboard sleeper sleeper received request sleeper inside sleeper sleeper happen recent call last file line module assert hi file line return file line wrapper return file line get raise ray object raise handling exception another exception ray object task,issue,negative,neutral,neutral,neutral,neutral,neutral
1706957545,@GeneDer can we close this or downgrade to P1 for the CI task? Since the actual task is succeeding now?,close downgrade task since actual task succeeding,issue,negative,neutral,neutral,neutral,neutral,neutral
1706942851,"Not sure about the planning for the dashboard part, perhaps @alanwguo knows.",sure dashboard part perhaps,issue,negative,positive,positive,positive,positive,positive
1706941427,Thanks @architkulkarni . Do you which release is planned for enhancement (Ray dashboard display of error code),thanks release enhancement ray dashboard display error code,issue,negative,positive,positive,positive,positive,positive
1706938976,"For the purposes of that thread it's resolved (make the exit code appear somewhere in the logs). It's resolved in the Ray nightly and in Ray 2.7.

But we'll leave this issue open to track the enhancement which is to show it in the dashboard.",thread resolved make exit code appear somewhere resolved ray nightly ray leave issue open track enhancement show dashboard,issue,negative,neutral,neutral,neutral,neutral,neutral
1706901929,"in addition to `last_info_for` there seems other useful methods disappeared in `EpisodeV2` like `last_observation_for` and `last_raw_obs_for`.

If you modify `rllib/examples/custom_metrics_and_callbacks.py` to train on PPO, you end up with exceptions on missing methods. Sample code provided below:

```python
""""""Example of using RLlib's debug callbacks.

Here we use callbacks to track the average CartPole pole angle magnitude as a
custom metric.

We then use `keep_per_episode_custom_metrics` to keep the per-episode values
of our custom metrics and do our own summarization of them.
""""""

import argparse
import os
from typing import Dict

import gymnasium as gym
import numpy as np
import ray
from ray import air, tune
from ray.rllib.algorithms.callbacks import DefaultCallbacks
from ray.rllib.algorithms.ppo import PPOConfig
from ray.rllib.env import BaseEnv
from ray.rllib.evaluation import Episode, RolloutWorker
from ray.rllib.policy import Policy

parser = argparse.ArgumentParser()
parser.add_argument(
    ""--framework"",
    choices=[""tf"", ""tf2"", ""torch""],
    default=""torch"",
    help=""The DL framework specifier."",
)
parser.add_argument(""--stop-iters"", type=int, default=2000)


# Create a custom CartPole environment that maintains an estimate of velocity
class CustomCartPole(gym.Env):
    def __init__(self, config):
        self.env = gym.make(""CartPole-v1"")
        self.observation_space = self.env.observation_space
        self.action_space = self.env.action_space
        self._pole_angle_vel = 0.0
        self.last_angle = 0.0

    def reset(self, *, seed=None, options=None):
        self._pole_angle_vel = 0.0
        obs, info = self.env.reset()
        self.last_angle = obs[2]
        return obs, info

    def step(self, action):
        obs, rew, term, trunc, info = self.env.step(action)
        angle = obs[2]
        self._pole_angle_vel = (
            0.5 * (angle - self.last_angle) + 0.5 * self._pole_angle_vel
        )
        info[""pole_angle_vel""] = self._pole_angle_vel
        return obs, rew, term, trunc, info


class MyCallbacks(DefaultCallbacks):

    def on_episode_step(
        self,
        *,
        worker: RolloutWorker,
        base_env: BaseEnv,
        policies: Dict[str, Policy],
        episode: Episode,
        env_index: int,
        **kwargs
    ):
        # Make sure this episode is ongoing.
        assert episode.length > 0, (
            ""ERROR: `on_episode_step()` callback should not be called right ""
            ""after env reset!""
        )
        pole_angle = abs(episode.last_observation_for()[2])
        raw_angle = abs(episode.last_raw_obs_for()[2])
        assert pole_angle == raw_angle
        episode.user_data[""pole_angles""].append(pole_angle)

        # Sometimes our pole is moving fast. We can look at the latest velocity
        # estimate from our environment and log high velocities.
        if np.abs(episode.last_info_for()[""pole_angle_vel""]) > 0.25:
            print(""This is a fast pole!"")


if __name__ == ""__main__"":
    args = parser.parse_args()

    config = (
        PPOConfig()
        .environment(CustomCartPole)
        .framework(args.framework)
        .callbacks(MyCallbacks)
        .resources(num_gpus=int(os.environ.get(""RLLIB_NUM_GPUS"", ""0"")))
        .rollouts(enable_connectors=True)
        .reporting(keep_per_episode_custom_metrics=True)
    )

    ray.init(local_mode=True)
    tuner = tune.Tuner(
        ""PPO"",
        run_config=air.RunConfig(
            stop={
                ""training_iteration"": args.stop_iters,
            },
        ),
        param_space=config,
    )
    # there is only one trial involved.
    result = tuner.fit().get_best_result()

    # Verify episode-related custom metrics are there.
    custom_metrics = result.metrics[""custom_metrics""]
    print(custom_metrics)
    assert ""pole_angle_mean"" in custom_metrics
    assert ""pole_angle_var"" in custom_metrics
```

Is there a workaround for this?",addition useful like modify train end missing sample code provided python example use track average pole angle magnitude custom metric use keep custom metric summarization import import o import import gymnasium gym import import ray ray import air tune import import import import episode import policy parser framework torch torch framework specifier create custom environment estimate velocity class self reset self return step self action term action angle angle return term class self worker policy episode episode make sure episode ongoing assert error right reset assert sometimes pole moving fast look latest velocity estimate environment log high print fast pole tuner one trial involved result verify custom metric print assert assert,issue,positive,positive,positive,positive,positive,positive
1706789868,"Just to come back, a related issue was discussed in #33976, which is now closed. But that one only handled ctrl+c on the main node, as far as I understand?  Will this issue here resolve the case when there is a crash on a worker node for whatever reason, which holds GPU memory on that worker? Or is that already solved somewhere else? This is a problem I keep having on my setup.",come back related issue closed one handled main node far understand issue resolve case crash worker node whatever reason memory worker already somewhere else problem keep setup,issue,negative,positive,neutral,neutral,positive,positive
1706751367,"@rkooo567 Make sure to fix DCO so I can merge it. Also, I'm not sure if failed premerge tests are related to this change. I rerun those tests, but in case if they are related, we should also fix them before I can merge :) ",make sure fix merge also sure related change rerun case related also fix merge,issue,positive,positive,positive,positive,positive,positive
1706745333,"@rkooo567 The release tests are rerun almost every night when there are new cherry picks. Just updated the PR to the latest run. This is the results
```
(ray) gene@geneanycale2023 release_logs % ./compare_perf_metrics 2.6.3 2.7.0 | sort -nr -k2
REGRESSION 17124.78%: dashboard_p50_latency_ms (LATENCY) regresses from 5.529 to 952.358 (17124.78%) in 2.7.0/benchmarks/many_tasks.json
REGRESSION 501.60%: dashboard_p95_latency_ms (LATENCY) regresses from 1641.087 to 9872.813 (501.60%) in 2.7.0/benchmarks/many_actors.json
REGRESSION 253.18%: dashboard_p50_latency_ms (LATENCY) regresses from 13.316 to 47.029 (253.18%) in 2.7.0/benchmarks/many_actors.json
REGRESSION 149.24%: dashboard_p99_latency_ms (LATENCY) regresses from 3961.132 to 9872.813 (149.24%) in 2.7.0/benchmarks/many_actors.json
REGRESSION 86.50%: dashboard_p95_latency_ms (LATENCY) regresses from 5662.863 to 10561.25 (86.50%) in 2.7.0/benchmarks/many_tasks.json
REGRESSION 57.12%: dashboard_p99_latency_ms (LATENCY) regresses from 7303.73 to 11475.966 (57.12%) in 2.7.0/benchmarks/many_tasks.json
REGRESSION 25.49%: multi_client_put_gigabytes (THROUGHPUT) regresses from 38.350117983138134 to 28.575525187974343 (25.49%) in 2.7.0/microbenchmark.json
REGRESSION 17.80%: single_client_tasks_and_get_batch (THROUGHPUT) regresses from 10.988766549840475 to 9.032974959236881 (17.80%) in 2.7.0/microbenchmark.json
REGRESSION 14.61%: single_client_get_object_containing_10k_refs (THROUGHPUT) regresses from 13.564180981457046 to 11.582495121556633 (14.61%) in 2.7.0/microbenchmark.json
REGRESSION 12.48%: 1_1_actor_calls_sync (THROUGHPUT) regresses from 2527.884786760515 to 2212.2970905634065 (12.48%) in 2.7.0/microbenchmark.json
REGRESSION 11.94%: 1_1_async_actor_calls_with_args_async (THROUGHPUT) regresses from 2071.4152029622965 to 1824.1292885711082 (11.94%) in 2.7.0/microbenchmark.json
REGRESSION 10.65%: 1_1_actor_calls_concurrent (THROUGHPUT) regresses from 5038.298026762205 to 4501.674745639853 (10.65%) in 2.7.0/microbenchmark.json
REGRESSION 9.44%: 1_n_actor_calls_async (THROUGHPUT) regresses from 11016.813113385808 to 9977.241133747568 (9.44%) in 2.7.0/microbenchmark.json
REGRESSION 7.04%: 1_1_actor_calls_async (THROUGHPUT) regresses from 8232.577421386566 to 7653.188945060761 (7.04%) in 2.7.0/microbenchmark.json
REGRESSION 6.94%: 1000000_queued_time (LATENCY) regresses from 179.73244193300002 to 192.20176690999998 (6.94%) in 2.7.0/scalability/single_node.json
REGRESSION 6.50%: dashboard_p95_latency_ms (LATENCY) regresses from 58.601 to 62.41 (6.50%) in 2.7.0/benchmarks/many_nodes.json
REGRESSION 6.07%: n_n_actor_calls_async (THROUGHPUT) regresses from 32688.224759117064 to 30702.557512448617 (6.07%) in 2.7.0/microbenchmark.json
REGRESSION 6.01%: time_to_broadcast_1073741824_bytes_to_50_nodes (LATENCY) regresses from 75.45187580800001 to 79.98399561600002 (6.01%) in 2.7.0/scalability/object_store.json
REGRESSION 5.76%: single_client_tasks_async (THROUGHPUT) regresses from 10940.075908384719 to 10309.812853287176 (5.76%) in 2.7.0/microbenchmark.json
REGRESSION 4.31%: multi_client_put_calls_Plasma_Store (THROUGHPUT) regresses from 12887.724106896032 to 12332.38477965388 (4.31%) in 2.7.0/microbenchmark.json
REGRESSION 4.04%: actors_per_second (THROUGHPUT) regresses from 848.4535001176805 to 814.1863770480064 (4.04%) in 2.7.0/benchmarks/many_actors.json
REGRESSION 3.90%: 1_n_async_actor_calls_async (THROUGHPUT) regresses from 9607.6917020388 to 9232.961669737171 (3.90%) in 2.7.0/microbenchmark.json
REGRESSION 3.11%: 1_1_async_actor_calls_sync (THROUGHPUT) regresses from 1519.8915131779668 to 1472.6370514139858 (3.11%) in 2.7.0/microbenchmark.json
REGRESSION 2.53%: 1_1_async_actor_calls_async (THROUGHPUT) regresses from 2683.097200627191 to 2615.219485440995 (2.53%) in 2.7.0/microbenchmark.json
REGRESSION 1.36%: single_client_put_calls_Plasma_Store (THROUGHPUT) regresses from 5759.206110210831 to 5680.905101515121 (1.36%) in 2.7.0/microbenchmark.json
REGRESSION 1.33%: placement_group_create/removal (THROUGHPUT) regresses from 982.6908282286798 to 969.6173278174784 (1.33%) in 2.7.0/microbenchmark.json
REGRESSION 0.69%: client__put_gigabytes (THROUGHPUT) regresses from 0.13393152379385806 to 0.1330015012769415 (0.69%) in 2.7.0/microbenchmark.json
REGRESSION 0.66%: multi_client_tasks_async (THROUGHPUT) regresses from 28883.241079598323 to 28693.647655418066 (0.66%) in 2.7.0/microbenchmark.json
REGRESSION 0.65%: single_client_tasks_sync (THROUGHPUT) regresses from 1329.0736290293253 to 1320.40816436792 (0.65%) in 2.7.0/microbenchmark.json
REGRESSION 0.39%: stage_0_time (LATENCY) regresses from 9.37568211555481 to 9.41209888458252 (0.39%) in 2.7.0/stress_tests/stress_test_many_tasks.json
REGRESSION 0.26%: client__get_calls (THROUGHPUT) regresses from 1157.7462987111426 to 1154.6885607873562 (0.26%) in 2.7.0/microbenchmark.json
2.7.0 does not have benchmarks/many_pgs.json

```",release rerun almost every night new cherry latest run ray gene sort regression latency regression latency regression latency regression latency regression latency regression latency regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression latency regression latency regression throughput regression latency regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression latency regression throughput,issue,negative,positive,positive,positive,positive,positive
1706719722,"Here is a list of perf blockers. cc @GeneDer if you ever run tests again, please post the result here. This will help us understanding test result better.

- https://github.com/ray-project/ray/issues/39259
- https://github.com/ray-project/ray/issues/39218
- https://github.com/ray-project/ray/issues/39216
- https://github.com/ray-project/ray/issues/39215
- https://github.com/ray-project/ray/issues/39214
- https://github.com/ray-project/ray/issues/39213
- https://github.com/ray-project/ray/issues/39024

Note that this doesn't necessarily mean it needs fixes. Some of them may be noise. ",list ever run please post result help u understanding test result better note necessarily mean need may noise,issue,positive,positive,neutral,neutral,positive,positive
1706714338,"@GeneDer do you have a plan to run nightly test one more time? It'd be great if we can get more data for vague cases.


- single_client_tasks_and_get_batch. Seems noise, but it seems to regress recently, and we should avoid cherry picking regression PR: https://github.com/ray-project/ray/issues/39259
- 1_1_actor_calls_async: Seems to be noise. 
<img width=""763"" alt=""Screenshot 2023-09-05 at 11 12 39 PM"" src=""https://github.com/ray-project/ray/assets/18510752/01e9470e-5bec-43af-9e6c-751111e35b61"">
- 1_1_async_actor_calls_sync. Seems to be noise
<img width=""762"" alt=""Screenshot 2023-09-05 at 11 14 41 PM"" src=""https://github.com/ray-project/ray/assets/18510752/6b34774c-9cdd-42f6-ac61-c2a2f66d2925"">
- 1_n_actor_calls_async seems to be noise 
<img width=""799"" alt=""Screenshot 2023-09-05 at 11 15 27 PM"" src=""https://github.com/ray-project/ray/assets/18510752/c1d5c2f4-4ada-4c61-a35d-7c1ef9f6906f"">
- 1_1_async_actor_calls_async seems to be noise
<img width=""796"" alt=""Screenshot 2023-09-05 at 11 16 03 PM"" src=""https://github.com/ray-project/ray/assets/18510752/754c23f3-5f5b-45ad-a24d-70891bc1e18a"">
- single_client_tasks_sync seems to be noise
<img width=""772"" alt=""Screenshot 2023-09-05 at 11 16 40 PM"" src=""https://github.com/ray-project/ray/assets/18510752/cb09e2d2-bd5b-4b0f-8b63-214ee31d6d8a"">
- 1_1_async_actor_calls_with_args_async seems to be noise
<img width=""780"" alt=""Screenshot 2023-09-05 at 11 17 19 PM"" src=""https://github.com/ray-project/ray/assets/18510752/97365125-acbe-46ae-a279-082793f85d79"">

Client tests are not tracked by us

Everything else is < 3% regression, so we won't investigate it. 

cc @jjyao @scv119 Please double check ^


",plan run nightly test one time great get data vague noise regress recently avoid cherry regression noise noise noise noise noise noise client tracked u everything else regression wo investigate please double check,issue,positive,positive,neutral,neutral,positive,positive
1706509115,"test_advanced_9.py succeeded! I think other weird failures are unrelated, but let me try merging the latest master in case",think weird unrelated let try latest master case,issue,negative,neutral,neutral,neutral,neutral,neutral
1706477170,"@krfricke, @woshiyyya  I tried your suggestions as well as what is suggested in the linked tutorial I added. 
The tutorial link did not work out as I wanted but your advice worked! Thank you :)

A further doubt though, (should I raise this as a separate issue?):
In the example [here](https://docs.ray.io/en/latest/tune/examples/tune-pytorch-lightning.html#configuring-the-search-space), choosing batch size is mentioned but in the config space declared, it is not varied:
```python
config={
        ""layer_1_size"": tune.choice([32, 64, 128]),
        ""layer_2_size"": tune.choice([64, 128, 256]),
        ""lr"": tune.loguniform(1e-4, 1e-1),
    }
```

Is it possible to change the batch size of the DataModule by maybe adding it to `searchable_lightning_config` as:
```python
searchable_lightning_config = (
    LightningConfigBuilder()
    .module(config={
        ""layer_1_size"": tune.choice([32, 64, 128]),
        ""layer_2_size"": tune.choice([64, 128, 256]),
        ""lr"": tune.loguniform(1e-4, 1e-1),
    }).fit_params(datamodule=dm, config= {""batch_size"": tune.choice([32, 64, 128])} )
    .build()
)
``` 
`fit_params` feeds into the `Trainer.fit()` of Lightning, so I do not think it would work, unless I use the `fit_start` hook of the Trainer to redeclare the Datamodule with my given batch size. Is there a better solution?

Or should I use the [Vanilla Pytorch Lightning with Tune](https://docs.ray.io/en/latest/tune/examples/tune-vanilla-pytorch-lightning.html#tune-vanilla-pytorch-lightning-ref) example instead if I want to add batch size as a Hyperparamter?

Thanks again!
 ",tried well linked tutorial added tutorial link work advice worked thank doubt though raise separate issue example choosing batch size space declared varied python possible change batch size maybe python lightning think would work unless use hook trainer redeclare given batch size better solution use vanilla lightning tune example instead want add batch size thanks,issue,positive,positive,positive,positive,positive,positive
1706402558,Failing tests are not related to the PR. Tested that profiling still works locally.,failing related tested still work locally,issue,negative,neutral,neutral,neutral,neutral,neutral
1706376610,"I noticed that there is no implementation of `load_state()` in `RandomRLModule` (https://github.com/ray-project/ray/blob/master/rllib/examples/rl_module/random_rl_module.py).

Adding an implementation like below solves the problem in my environment.

```py
class RandomRLModule(RLModule):
  ...
  # add load_state()
  def load_state(self, dir) -> None:
        pass
```",implementation implementation like problem environment class add self none pas,issue,negative,neutral,neutral,neutral,neutral,neutral
1706284783,"I observed similar behaviors when trying to visit the failed deployment( killed due to OOM ) through HTTP. The request will hang. However, I can't use `request_timeout_s` because sometimes model inference takes very long.",similar trying visit deployment due request however ca use sometimes model inference long,issue,negative,negative,neutral,neutral,negative,negative
1706177918,"Sorry @larrylian this was the emergency PR in case we cannot get https://github.com/ray-project/ray/pull/39194 in. I think https://github.com/ray-project/ray/pull/39194 will work out, so let me close the issue",sorry emergency case get think work let close issue,issue,negative,negative,negative,negative,negative,negative
1706162087,@ArturNiederfahrenhorst and @kouroshHakha Could you guys help review this minor change and then merge it?,could help review minor change merge,issue,negative,negative,neutral,neutral,negative,negative
1706161805,"hmm @rynewang @jjyao Q: Instead of manually setting this, should we set the ""cap"" instead? The current approach is hard to be applied generically. 
",instead manually setting set cap instead current approach hard applied generically,issue,negative,negative,negative,negative,negative,negative
1706139491,Let's defer this after 2.7 (and update after we picked all remaining fixes),let defer update picked,issue,negative,neutral,neutral,neutral,neutral,neutral
1706084062,"Unfortunately, I also encountered this issue with version 2.4.0, but my cache does not seem to be particularly large. It seems that the memory has been increasing since we started submitting jobs to the ray-cluster（kuberay）, until it reaches around 50%. Even if no jobs are running, the memory usage is still high.
```
sys/fs/cgroup/memory# cat memory.stat
cache 7702360064
rss 4729700352
rss_huge 2789212160
shmem 6167658496
mapped_file 6168662016
dirty 8192
writeback 0
swap 0
pgpgin 178125506
pgpgout 177528593
pgfault 195961384
pgmajfault 954
inactive_anon 5990920192
active_anon 4934561792
inactive_file 1026723840
active_file 507977728
unevictable 0
hierarchical_memory_limit 24000000000
hierarchical_memsw_limit 24000000000
total_cache 7702360064
total_rss 4729700352
total_rss_huge 2789212160
total_shmem 6167658496
total_mapped_file 6168662016
total_dirty 8192
total_writeback 0
total_swap 0
total_pgpgin 178125506
total_pgpgout 177528593
total_pgfault 195961384
total_pgmajfault 954
total_inactive_anon 5990920192
total_active_anon 4934561792
total_inactive_file 1026723840
total_active_file 507977728
total_unevictable 0
```
",unfortunately also issue version cache seem particularly large memory increasing since around even running memory usage still high cat cache dirty swap,issue,negative,negative,negative,negative,negative,negative
1706069310,@vitsai NOTE: I added a commit to fix test_advanced_9,note added commit fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1705933583,"I found a similar problem. Plasma object store in raylet process doesn't release mmaped memory of objects used in a finished job.

My environment is:
- MacBook Pro M2 Max, 64 GB RAM, 12 CPU Cores
- MacOS 13.5.1 (22G90)
- Python 3.11.4 arm64
- Ray version 2.6.3 

My test script is:
```
import ray
import pyarrow as pa

schema = pa.schema([('v', pa.binary())])
def map(batch):
    lines = []
    for __ in range(len(batch['id'])):
        lines.append(b'*' * 99)
    return pa.Table.from_arrays([lines], schema=schema)

def test(n):
    ray.init()
    ray.data.DataContext.get_current().execution_options.verbose_progress = True
    ray.data.DataContext.get_current().use_push_based_shuffle = True
    parallelism = max(1, int(n * 100 / (128 * 1024 * 1024)))
    ds = ray.data.range(n, parallelism=parallelism)\
            .map_batches(map, zero_copy_batch=True)\
            .repartition(1)
    ds.materialize()

if __name__ == '__main__':
    test(700000000)
```

run following shell command:

```
ray start --head
python ./test.py
```

And you will find the disk usage increases, run the `vmmap PID` command and you will find a lot of mmaped files still exist in raylet process, and when you `ray stop` killing all the ray processes then the disk usage decreases.",found similar problem plasma object store raylet process release memory used finished job environment pro ram python arm ray version test script import ray import pa schema map batch range batch return test true true parallelism map test run following shell command ray start head python find disk usage run command find lot still exist raylet process ray stop killing ray disk usage,issue,negative,positive,positive,positive,positive,positive
1705874339,"Can we tell customer this is resolved ? Also, I am referring original [slack thread here](https://anyscaleteam.slack.com/archives/C03GND24AA0/p1690453506816719). ",tell customer resolved also original slack thread,issue,negative,positive,positive,positive,positive,positive
1705863502,"Is this reproduction script your complete scenario? 
Without time.sleep, the driver will exit, and the model actor will be destroyed. 
Additionally, metric reporting has a delay, so it is possible that the metrics may not be reported.",reproduction script complete scenario without driver exit model actor additionally metric delay possible metric may,issue,negative,positive,neutral,neutral,positive,positive
1705850338,"> What's the reason for having features parity in `ray.state.actors`

@rickyyx 
1. We both have many jobs in a ray cluster, so a job's manager role want to get this job's actors.  Use `ray.state.actors` to fetching all actors in the cluster can lead to performance degradation.
2. We only want to get alive actors.

If you also think this requirement is acceptable, I can handle completing this issue.",reason parity many ray cluster job manager role want get job use fetching cluster lead performance degradation want get alive also think requirement acceptable handle issue,issue,negative,positive,positive,positive,positive,positive
1705809445,"Both options sound good :D, though I hope the bazel part can be simpler. And yes, let's talk about it in a meeting.",sound good though hope part simpler yes let talk meeting,issue,positive,positive,positive,positive,positive,positive
1705799187,"@ArturNiederfahrenhorst It's a critical issue to us, could you prioritize it?",critical issue u could,issue,negative,neutral,neutral,neutral,neutral,neutral
1705507965,"We've been able to work around this. But should issues arise, I'll update here
",able work around arise update,issue,negative,positive,positive,positive,positive,positive
1705491822,"Blame commit doesn't make sense so properly the test is flaky. Also fail on release branch, please cherry pick the fix. Thankks",blame commit make sense properly test flaky also fail release branch please cherry pick fix,issue,negative,negative,negative,negative,negative,negative
1705490636,"Look like it is also broken in release branch, please cherry pick, thankks",look like also broken release branch please cherry pick,issue,negative,negative,negative,negative,negative,negative
1705279829,"The top solution on [stack overflow](https://stackoverflow.com/a/72492737/9470078) solved this issue for me:
> ## Limit the number of CPUs
> Ray will launch as many worker processes as your execution node has CPUs (or CPU cores). If that's more than you reserved, slurm will start killing processes.
>
>You can limit the number of worker processes as such:
>
> ```
> import ray
> ray.init(ignore_reinit_error=True, num_cpus=4)
> print(""success"")
> ```

(though I don't think it solves the OP's problem as they have already set `num_cups`)",top solution stack overflow issue limit number ray launch many worker execution node reserved start killing limit number worker import ray print success though think problem already set,issue,negative,positive,positive,positive,positive,positive
1705221347,"Ok, confirmed that this is good to pick now!
Thanks everyone!
cc: @zhe-thoughts ",confirmed good pick thanks everyone,issue,positive,positive,positive,positive,positive,positive
1704802739,@shrekris-anyscale  Thanks for your help! I'll adopt the workaround and wait for your work~,thanks help adopt wait,issue,positive,positive,positive,positive,positive,positive
1704362420,"A proper fix is out for review (https://github.com/ray-project/ray/pull/38772). As the current impact scope is small (only when there are a few thousands of runtime envs) and the risk of the fix (multiplexing TCP connections for 1K+ HTTP requests) is not low, it is expected to take some time to get a proper review.",proper fix review current impact scope small risk fix low take time get proper review,issue,negative,negative,neutral,neutral,negative,negative
1704289669,"> hello! has there been any further discussion or progress on this issue? currently, the PBT scheduler doesn’t play nicely with the mlflow callback because hyperparameter values are changed during a trial and that raises an exception. Wondering if a possible solution may be to treat each perturbation as a new trial and log it separately? thanks!

Any update on this?",hello discussion progress issue currently play nicely trial exception wondering possible solution may treat perturbation new trial log separately thanks update,issue,positive,positive,positive,positive,positive,positive
1703938894,"When creating the reproduction script, I noticed that the behaviour only occurs with `""use_lstm"": True` in the model config. 
The `actions` tensor in the SampleBatch used in the `update(...)` function in `rllib/core/learner/learner.py` is of shape `(200,20,2)`. When `""use_lstm"": False`, the shape of the tensor is `(4000,2)`.

20 seems to be the `max_seq_len` property of the lstm model.",reproduction script behaviour true model tensor used update function shape false shape tensor property model,issue,negative,negative,neutral,neutral,negative,negative
1703860495,"test_advanced_9: you may need to do ci_repro and debug it. Seems like a weird failure
placement_group.py: can you make it large test?
test_tempfile: seems like a simple test issue that happens because we are sharing the same session dir now",may need like weird failure make large test like simple test issue session,issue,negative,negative,negative,negative,negative,negative
1703857499,"I was eventually able to get past this issue by using `machineType: n1-standard-2`, `sourceImage: projects/deeplearning-platform-release/global/images/family/common-cu113`, and `acceleratorType: nvidia-tesla-t4`.  Here is the full `ray_head_gpu` node type spec I used to modify `example-gpu-docker.yaml`:
```
ray_head_gpu:
    # The resources provided by this node type.
    resources: {""CPU"": 2, ""GPU"": 1}
    # Provider-specific config for the head node, e.g. instance type. By default
    # Ray will auto-configure unspecified fields such as subnets and ssh-keys.
    # For more documentation on available fields, see:
    # https://cloud.google.com/compute/docs/reference/rest/v1/instances/insert
    node_config:
        machineType: n1-standard-2
        disks:
          - boot: true
            autoDelete: true
            type: PERSISTENT
            initializeParams:
              diskSizeGb: 50
              # See https://cloud.google.com/compute/docs/images for more images
              sourceImage: projects/deeplearning-platform-release/global/images/family/common-cu113
        # Make sure to set scheduling->onHostMaintenance to TERMINATE when GPUs are present
        guestAccelerators:
          - acceleratorType: nvidia-tesla-t4
            acceleratorCount: 1
        metadata:
          items:
            - key: install-nvidia-driver
              value: ""True""
        scheduling:
          - onHostMaintenance: TERMINATE
```

 I also made similar changes to `ray_worker_gpu`.",eventually able get past issue full node type spec used modify provided node type head node instance type default ray unspecified documentation available see boot true true type persistent see make sure set terminate present key value true terminate also made similar,issue,positive,positive,positive,positive,positive,positive
1703837558,"That worked and ray got installed. Thanks! when I was doing a `pip install pyarrow` then it works but not with the version specified for `ray[rllib]` Although this issue should be fixed as I had to install a lot of packages manually when not using `pip install ""ray[rllib]""`",worked ray got thanks pip install work version ray although issue fixed install lot manually pip install ray,issue,negative,positive,positive,positive,positive,positive
1703729374,Thanks a lot for raising this issue!,thanks lot raising issue,issue,negative,positive,positive,positive,positive,positive
1703728896,"I can reproduce.
@kouroshHakha @sven1977 @avnishn This is one of our most basic examples and it should 100% be working.",reproduce one basic working,issue,negative,neutral,neutral,neutral,neutral,neutral
1703723237,"Hi @edcxan,
these are all valid points. Getting the RLModules / Learner stack is high priority for the RLlib team.
Can you put up a PR with the items? Ideally, if you have a short script that creates errors _without_ your changes but does not create errors _with_ your changes, that would be great and accelerate the process.
Do you think that would be possible?

Thanks for raising this issue in any case!
",hi valid getting learner stack high priority team put ideally short script create would great accelerate process think would possible thanks raising issue case,issue,positive,positive,positive,positive,positive,positive
1703722330,What happens if you just install  `pip intall ray==2.6.3`? pyarrow is a Ray Core dependency and this not have anything to do with rllib.,install pip ray core dependency anything,issue,negative,neutral,neutral,neutral,neutral,neutral
1703703862,Reminder: You need to fix https://github.com/ray-project/ray/pull/39194#issuecomment-1703662605 to pass gcs_ha_e2e_2.py,reminder need fix pas,issue,negative,neutral,neutral,neutral,neutral,neutral
1703691936,Finally got a success run https://buildkite.com/ray-project/release-tests-pr/builds/51930#018a52b1-7c5b-4357-831a-4dbf55770f6e ,finally got success run,issue,positive,positive,positive,positive,positive,positive
1703662605,"Awesome. Some build faliures + (gcs_ha_e2e_2.py failure is same as what I told you. Set shorter time for

```
RAY_CONFIG(int64_t, raylet_client_num_connect_attempts, 10)
RAY_CONFIG(int64_t, raylet_client_connect_timeout_milliseconds, 1000)
```

in both worker/head containers inside `conftest_docker.py` via `RAY_raylet_client_num_connect_attempts=10` and `RAY_raylet_client_connect_timeout_milliseconds=100`


",awesome build failure told set shorter time inside via,issue,negative,positive,positive,positive,positive,positive
1703658487,"Note: all the core tests are failing

```




ImportError: cannot import name 'builder' from 'google.protobuf.internal' (/opt/miniconda/lib/python3.8/site-packages/google/protobuf/internal/__init__.py)
 

<br class=""Apple-interchange-newline"">
```",note core failing import name,issue,negative,neutral,neutral,neutral,neutral,neutral
1703648462,"Verified that services restarts the head node smoothly and retains the same session dir with this change:

https://console.anyscale-staging.com/o/anyscale-internal/clusters/ses_59t6hd9eevn5jpirwqe5q3faq6?user=usr_x9zm6779nv4qcf5ljs3n11anwq&command-history-section=head_start_up_log",head node smoothly session change,issue,negative,positive,positive,positive,positive,positive
1703638561,"1. lint failure
2. can you add unit tests for redis output? 

After it is addressed, I will approve the PR",lint failure add unit output approve,issue,negative,negative,negative,negative,negative,negative
1703610618,"1. windows build failure
2. Remove config test (it is the same anyway)
3. Set RAY_BACKEND_LOG_LEVEL=warning in the beg of Redis client  and unset before it returns. 
4. Add session dir changed unit test and skip when redis is enabled
5. Fail if session name is overwritten
",build failure remove test anyway set beg client unset add session unit test skip fail session name,issue,negative,negative,negative,negative,negative,negative
1703537855,"Changed test_advanced_9 because a failure to connect to Redis is a fatal, which now happens before GCS starts.

Regarding the extra logs: they should be once-per-init, and the log levels are pre-existing in RedisContext, so I don't think we need to change them.",failure connect fatal regarding extra log think need change,issue,negative,negative,negative,negative,negative,negative
1703530151,"Removing either of the `asyncio.shield()` calls don't seem to help either.

If we remove the outer one, the scary-looking logs go away, but the `sleeper` gets canceled:

```python
# File name: repro.py

import asyncio
from ray import serve
from ray.serve.handle import RayServeHandle


@serve.deployment
class Forwarder:
    def __init__(self, sleeper_handle: RayServeHandle):
        self.sleeper_handle = sleeper_handle

    async def __call__(self):
        try:
            print(""Forwarder received request!"")
            await (await asyncio.shield(self.sleeper_handle.remote()))

        except asyncio.CancelledError:
            print(""Forwarder's request was cancelled!"")


@serve.deployment
async def sleeper():
    try:
        print(""Sleeper received request!"")
        await asyncio.sleep(3)
        print(""Sleeper deployment finished sleeping!"")
    except asyncio.CancelledError:
        print(""Sleeper was cancelled."")


app = Forwarder.bind(sleeper.bind())
```

```
2023-09-01 16:50:42,710	SUCC scripts.py:519 -- Deployed Serve app successfully.
(ServeReplica:default:sleeper pid=38636) Sleeper received request!
(ServeReplica:default:Forwarder pid=38637) Forwarder received request!
(HTTPProxyActor pid=38629) INFO 2023-09-01 16:50:55,430 http_proxy 10.103.209.171 0c2c0846-e8ff-4970-a544-90d7b269f8ea / default http_proxy.py:1215 - Client for request 0c2c0846-e8ff-4970-a544-90d7b269f8ea disconnected during execution, cancelling request.
(ServeReplica:default:sleeper pid=38636) INFO 2023-09-01 16:50:55,434 sleeper default#sleeper#kwaprY 0c2c0846-e8ff-4970-a544-90d7b269f8ea / default replica.py:749 - __CALL__ OK 612.0ms
(ServeReplica:default:sleeper pid=38636) Sleeper was cancelled.
(ServeReplica:default:Forwarder pid=38637) Forwarder's request was cancelled!
(ServeReplica:default:Forwarder pid=38637) INFO 2023-09-01 16:50:55,433 Forwarder default#Forwarder#FYoENn 0c2c0846-e8ff-4970-a544-90d7b269f8ea / default replica.py:749 - __CALL__ OK 626.2ms
```

And if we remove the inner one, the `sleeper` gets canceled, and we see the scary-looking logs:

```python
# File name: repro.py

import asyncio
from ray import serve
from ray.serve.handle import RayServeHandle


@serve.deployment
class Forwarder:
    def __init__(self, sleeper_handle: RayServeHandle):
        self.sleeper_handle = sleeper_handle

    async def __call__(self):
        try:
            print(""Forwarder received request!"")
            await asyncio.shield(await self.sleeper_handle.remote())

        except asyncio.CancelledError:
            print(""Forwarder's request was cancelled!"")


@serve.deployment
async def sleeper():
    try:
        print(""Sleeper received request!"")
        await asyncio.sleep(3)
        print(""Sleeper deployment finished sleeping!"")
    except asyncio.CancelledError:
        print(""Sleeper was cancelled."")


app = Forwarder.bind(sleeper.bind())
```

```
2023-09-01 16:51:43,705	SUCC scripts.py:519 -- Deployed Serve app successfully.
(ServeReplica:default:Forwarder pid=38923) Forwarder received request!
(ServeReplica:default:sleeper pid=38922) Sleeper received request!
(HTTPProxyActor pid=38913) INFO 2023-09-01 16:51:46,012 http_proxy 10.103.209.171 224a08dc-50ab-408f-bb4b-39cf8ed9c97d / default http_proxy.py:1215 - Client for request 224a08dc-50ab-408f-bb4b-39cf8ed9c97d disconnected during execution, cancelling request.
(ServeReplica:default:Forwarder pid=38923) Forwarder's request was cancelled!
(ServeReplica:default:Forwarder pid=38923) INFO 2023-09-01 16:51:46,014 Forwarder default#Forwarder#iohgyd 224a08dc-50ab-408f-bb4b-39cf8ed9c97d / default replica.py:749 - __CALL__ OK 532.0ms
(ServeReplica:default:sleeper pid=38922) Sleeper was cancelled.
(ServeReplica:default:sleeper pid=38922) INFO 2023-09-01 16:51:46,015 sleeper default#sleeper#uLxpNQ 224a08dc-50ab-408f-bb4b-39cf8ed9c97d / default replica.py:749 - __CALL__ OK 516.8ms
(ServeReplica:default:Forwarder pid=38923) Task exception was never retrieved
(ServeReplica:default:Forwarder pid=38923) future: <Task finished name='Task-17' coro=<_wrap_awaitable() done, defined at /Users/shrekris/miniforge3/envs/ae/lib/python3.8/asyncio/tasks.py:688> exception=RayTaskError(TaskCancelledError)(TaskCancelledError(TaskID(33aa54f9afa77becc15688d94cbcf495a8b3959901000000)))>
(ServeReplica:default:Forwarder pid=38923) Traceback (most recent call last):
(ServeReplica:default:Forwarder pid=38923)   File ""/Users/shrekris/miniforge3/envs/ae/lib/python3.8/asyncio/tasks.py"", line 695, in _wrap_awaitable
(ServeReplica:default:Forwarder pid=38923)     return (yield from awaitable.__await__())
(ServeReplica:default:Forwarder pid=38923) ray.exceptions.RayTaskError(TaskCancelledError): ray::ServeReplica:default:sleeper.handle_request() (pid=38922, ip=10.103.209.171, actor_id=c15688d94cbcf495a8b3959901000000, repr=<ray.serve._private.replica.ServeReplica:default:sleeper object at 0x1175e5d00>)
(ServeReplica:default:Forwarder pid=38923)     raise CancelledError()
(ServeReplica:default:Forwarder pid=38923) concurrent.futures._base.CancelledError
(ServeReplica:default:Forwarder pid=38923) 
(ServeReplica:default:Forwarder pid=38923) During handling of the above exception, another exception occurred:
(ServeReplica:default:Forwarder pid=38923) 
(ServeReplica:default:Forwarder pid=38923) ray::ServeReplica:default:sleeper.handle_request() (pid=38922, ip=10.103.209.171, actor_id=c15688d94cbcf495a8b3959901000000, repr=<ray.serve._private.replica.ServeReplica:default:sleeper object at 0x1175e5d00>)
(ServeReplica:default:Forwarder pid=38923) ray.exceptions.TaskCancelledError: Task: TaskID(33aa54f9afa77becc15688d94cbcf495a8b3959901000000) was cancelled.
```",removing either seem help either remove outer one go away sleeper python file name import ray import serve import class forwarder self self try print forwarder received request await await except print forwarder request sleeper try print sleeper received request await print sleeper deployment finished sleeping except print sleeper serve successfully default sleeper sleeper received request default forwarder forwarder received request default client request disconnected execution request default sleeper sleeper default sleeper default default sleeper sleeper default forwarder forwarder request default forwarder forwarder default forwarder default remove inner one sleeper see python file name import ray import serve import class forwarder self self try print forwarder received request await await except print forwarder request sleeper try print sleeper received request await print sleeper deployment finished sleeping except print sleeper serve successfully default forwarder forwarder received request default sleeper sleeper received request default client request disconnected execution request default forwarder forwarder request default forwarder forwarder default forwarder default default sleeper sleeper default sleeper sleeper default sleeper default default forwarder task exception never default forwarder future task finished done defined default forwarder recent call last default forwarder file line default forwarder return yield default forwarder ray default default sleeper object default forwarder raise default forwarder default forwarder default forwarder handling exception another exception default forwarder default forwarder ray default default sleeper object default forwarder task,issue,positive,positive,positive,positive,positive,positive
1703528529,"However, when I pull the `asyncio.shield()` call into the `Forwarder`, I get different exceptions:

```python
# File name: repro.py

import asyncio
from ray import serve
from ray.serve.handle import RayServeHandle


@serve.deployment
class Forwarder:
    def __init__(self, sleeper_handle: RayServeHandle):
        self.sleeper_handle = sleeper_handle

    async def __call__(self):
        try:
            print(""Forwarder received request!"")
            await asyncio.shield(await asyncio.shield(self.sleeper_handle.remote()))

        except asyncio.CancelledError:
            print(""Forwarder's request was cancelled!"")


@serve.deployment
async def sleeper():
    try:
        print(""Sleeper received request!"")
        await asyncio.sleep(3)
        print(""Sleeper deployment finished sleeping!"")
    except asyncio.CancelledError:
        print(""Sleeper was cancelled."")


app = Forwarder.bind(sleeper.bind())
```

```
2023-09-01 16:53:13,129	SUCC scripts.py:519 -- Deployed Serve app successfully.
(ServeReplica:default:sleeper pid=39309) Sleeper received request!
(ServeReplica:default:Forwarder pid=39310) Forwarder received request!
(HTTPProxyActor pid=39307) INFO 2023-09-01 16:53:17,797 http_proxy 10.103.209.171 7902f768-2ea9-45df-b130-9ca88d630e39 / default http_proxy.py:1215 - Client for request 7902f768-2ea9-45df-b130-9ca88d630e39 disconnected during execution, cancelling request.
(ServeReplica:default:sleeper pid=39309) Sleeper was cancelled.
(ServeReplica:default:sleeper pid=39309) INFO 2023-09-01 16:53:17,803 sleeper default#sleeper#xdfIrG 7902f768-2ea9-45df-b130-9ca88d630e39 / default replica.py:749 - __CALL__ OK 606.9ms
(ServeReplica:default:Forwarder pid=39310) INFO 2023-09-01 16:53:17,802 Forwarder default#Forwarder#SbcLZA 7902f768-2ea9-45df-b130-9ca88d630e39 / default replica.py:749 - __CALL__ OK 621.0ms
(ServeReplica:default:Forwarder pid=39310) Task exception was never retrieved
(ServeReplica:default:Forwarder pid=39310) future: <Task finished name='Task-17' coro=<_wrap_awaitable() done, defined at /Users/shrekris/miniforge3/envs/ae/lib/python3.8/asyncio/tasks.py:688> exception=RayTaskError(TaskCancelledError)(TaskCancelledError(TaskID(15c61fefcb25a2c389cc44fa4d648fc6fcb5a7e401000000)))>
(ServeReplica:default:Forwarder pid=39310) Traceback (most recent call last):
(ServeReplica:default:Forwarder pid=39310)   File ""/Users/shrekris/miniforge3/envs/ae/lib/python3.8/asyncio/tasks.py"", line 695, in _wrap_awaitable
(ServeReplica:default:Forwarder pid=39310)     return (yield from awaitable.__await__())
(ServeReplica:default:Forwarder pid=39310) ray.exceptions.RayTaskError(TaskCancelledError): ray::ServeReplica:default:sleeper.handle_request() (pid=39309, ip=10.103.209.171, actor_id=89cc44fa4d648fc6fcb5a7e401000000, repr=<ray.serve._private.replica.ServeReplica:default:sleeper object at 0x10faa2ca0>)
(ServeReplica:default:Forwarder pid=39310)     raise CancelledError()
(ServeReplica:default:Forwarder pid=39310) concurrent.futures._base.CancelledError
(ServeReplica:default:Forwarder pid=39310) 
(ServeReplica:default:Forwarder pid=39310) During handling of the above exception, another exception occurred:
(ServeReplica:default:Forwarder pid=39310) 
(ServeReplica:default:Forwarder pid=39310) ray::ServeReplica:default:sleeper.handle_request() (pid=39309, ip=10.103.209.171, actor_id=89cc44fa4d648fc6fcb5a7e401000000, repr=<ray.serve._private.replica.ServeReplica:default:sleeper object at 0x10faa2ca0>)
(ServeReplica:default:Forwarder pid=39310) ray.exceptions.TaskCancelledError: Task: TaskID(15c61fefcb25a2c389cc44fa4d648fc6fcb5a7e401000000) was cancelled.
(ServeReplica:default:Forwarder pid=39310) Forwarder's request was cancelled!
```

I expect to see the `Forwarder` get cancelled (which happens), and the `sleeper` to finish executing (which doesn't happen). Instead, the `sleeper` is cancelled. The scary-looking logs are unexpected too.",however pull call forwarder get different python file name import ray import serve import class forwarder self self try print forwarder received request await await except print forwarder request sleeper try print sleeper received request await print sleeper deployment finished sleeping except print sleeper serve successfully default sleeper sleeper received request default forwarder forwarder received request default client request disconnected execution request default sleeper sleeper default sleeper sleeper default sleeper default default forwarder forwarder default forwarder default default forwarder task exception never default forwarder future task finished done defined default forwarder recent call last default forwarder file line default forwarder return yield default forwarder ray default default sleeper object default forwarder raise default forwarder default forwarder default forwarder handling exception another exception default forwarder default forwarder ray default default sleeper object default forwarder task default forwarder forwarder request expect see forwarder get sleeper finish happen instead sleeper unexpected,issue,positive,positive,positive,positive,positive,positive
1703525176,"@edoakes found in [the asyncio docs](https://docs.python.org/3/library/asyncio-task.html#shielding-from-cancellation) that the caller of `asyncio.shield()` gets canceled, but the callee still executes. That is indeed what I see after modifying `print_and_sleep` a bit:

```python
...
@serve.deployment
async def sleeper():

    async def print_and_sleep():
        print(""Inside print_and_sleep"")
        await asyncio.sleep(3)
        print(""Finished sleeping in print_and_sleep"")
...
```

```
2023-09-01 16:39:51,591	SUCC scripts.py:519 -- Deployed Serve app successfully.
(ServeReplica:default:Forwarder pid=35945) Forwarder received request!
(ServeReplica:default:sleeper pid=35944) Sleeper received request!
(ServeReplica:default:sleeper pid=35944) Inside print_and_sleep
(HTTPProxyActor pid=35942) INFO 2023-09-01 16:39:55,853 http_proxy 10.103.209.171 ae40b189-5bdf-4763-b9e7-edc329cd5ae5 / default http_proxy.py:1215 - Client for request ae40b189-5bdf-4763-b9e7-edc329cd5ae5 disconnected during execution, cancelling request.
(ServeReplica:default:Forwarder pid=35945) Forwarder's request was cancelled!
(ServeReplica:default:sleeper pid=35944) INFO 2023-09-01 16:39:55,855 sleeper default#sleeper#hxSgDe ae40b189-5bdf-4763-b9e7-edc329cd5ae5 / default replica.py:749 - __CALL__ OK 488.8ms
(ServeReplica:default:sleeper pid=35944) Sleeper was cancelled -- this should not happen!
(ServeReplica:default:Forwarder pid=35945) INFO 2023-09-01 16:39:55,855 Forwarder default#Forwarder#WVCROE ae40b189-5bdf-4763-b9e7-edc329cd5ae5 / default replica.py:749 - __CALL__ OK 503.9ms
(ServeReplica:default:sleeper pid=35944) Finished sleeping in print_and_sleep
```",found caller still indeed see bit python sleeper print inside await print finished sleeping serve successfully default forwarder forwarder received request default sleeper sleeper received request default sleeper inside default client request disconnected execution request default forwarder forwarder request default sleeper sleeper default sleeper default default sleeper sleeper happen default forwarder forwarder default forwarder default default sleeper finished sleeping,issue,positive,positive,positive,positive,positive,positive
1703409230,As discussed yesterday let's remove the Deploy Many Models sub-paging and move Deploy multiple applications and Deploy multiplexed models to the top level,yesterday let remove deploy many move deploy multiple deploy top level,issue,negative,positive,positive,positive,positive,positive
1703400343,This change is blocked on https://github.com/ray-project/ray/issues/39228. The doc tests will fail until that's resolved.,change blocked doc fail resolved,issue,negative,negative,negative,negative,negative,negative
1703357893,"> By the way, just a heads up that `_raylet.pyx` wraps the kv methods in the `_auto_reconnect` decorator (also defined in _raylet.pyx), which will call `_connect` for a set number of retries -- is the plan to keep the behavior?

I'd really like to remove it but I fear it's too big a change. To mitigate in Connect() I check if we had already connected, and if so we long-wait for the channel to be ready.",way decorator also defined call set number plan keep behavior really like remove fear big change mitigate connect check already connected channel ready,issue,negative,positive,positive,positive,positive,positive
1703356874,"By the way, just a heads up that `_raylet.pyx` wraps the kv methods in the `_auto_reconnect` decorator (also defined in _raylet.pyx), which will call `_connect` for a set number of retries -- is the plan to keep the behavior?",way decorator also defined call set number plan keep behavior,issue,negative,neutral,neutral,neutral,neutral,neutral
1703294152,"Confirmed with @iycheng that this indeed works when adding the corresponding env variables

RAY_REDIS_CA_CERT=/etc/ssl/certs/ca-certificates.crt RAY_REDIS_ADDRESS=rediss://xyz",confirmed indeed work corresponding,issue,negative,positive,positive,positive,positive,positive
1703272938,"@edoakes @sihanwang41 Just FYI I addressed the comments. PTAL when you have a sec, would be nice if we can get this cherry-pick today🙏",sec would nice get today,issue,negative,positive,positive,positive,positive,positive
1703272826,"@allenyin55 performed the test, let me try to perform what you suggested.
",test let try perform,issue,negative,neutral,neutral,neutral,neutral,neutral
1703271136,@KamenShah you need to use the cert the memorydb is using. You can't blindly generate the certs and use that.,need use ca blindly generate use,issue,negative,negative,negative,negative,negative,negative
1703236840,I see this issue is closed but did not see a PR attached to this issue. Does ray dashboard server support TLS connection?,see issue closed see attached issue ray dashboard server support connection,issue,negative,negative,neutral,neutral,negative,negative
1703147140,@jjyao can you post screenshots of the before and after memory usage for the serve workload?,post memory usage serve,issue,negative,neutral,neutral,neutral,neutral,neutral
1703121994,@KamenShah were you able to successfully connect to this memorydb instance from the cluster using `redis-cli`? That'd be my first step in verifying that it's a ray issue.,able successfully connect instance cluster first step ray issue,issue,negative,positive,positive,positive,positive,positive
1703089475,@edoakes please add the release blocker label,please add release blocker label,issue,negative,neutral,neutral,neutral,neutral,neutral
1703080124,"> @kevin85421 to resolve DCO and merge conflicts

Done. Thanks!",resolve merge done thanks,issue,positive,positive,positive,positive,positive,positive
1703065372,"```
(ray) gene@geneanycale2023 release_logs % ./compare_perf_metrics 2.6.3 2.7.0 | sort -nr -k2
[blocker] REGRESSION 417.04%: dashboard_p95_latency_ms (LATENCY) regresses from 1641.087 to 8485.128 (417.04%) in 2.7.0/benchmarks/many_actors.json
[blocker] REGRESSION 123.56%: dashboard_p50_latency_ms (LATENCY) regresses from 13.316 to 29.769 (123.56%) in 2.7.0/benchmarks/many_actors.json
[blocker] REGRESSION 114.21%: dashboard_p99_latency_ms (LATENCY) regresses from 3961.132 to 8485.128 (114.21%) in 2.7.0/benchmarks/many_actors.json
[blocker] REGRESSION 63.62%: dashboard_p99_latency_ms (LATENCY) regresses from 7303.73 to 11950.181 (63.62%) in 2.7.0/benchmarks/many_tasks.json
[blocker] REGRESSION 37.71%: dashboard_p95_latency_ms (LATENCY) regresses from 5662.863 to 7798.254 (37.71%) in 2.7.0/benchmarks/many_tasks.json
[blocker] REGRESSION 31.50%: dashboard_p95_latency_ms (LATENCY) regresses from 58.601 to 77.059 (31.50%) in 2.7.0/benchmarks/many_nodes.json
[blocker] REGRESSION 31.20%: stage_0_time (LATENCY) regresses from 9.37568211555481 to 12.301149845123291 (31.20%) in 2.7.0/stress_tests/stress_test_many_tasks.json
[blocker] REGRESSION 19.02%: stage_3_creation_time (LATENCY) regresses from 3.414729595184326 to 4.064252614974976 (19.02%) in 2.7.0/stress_tests/stress_test_many_tasks.json
[blocker] REGRESSION 9.59%: single_client_tasks_async (THROUGHPUT) regresses from 10940.075908384719 to 9890.90145401096 (9.59%) in 2.7.0/microbenchmark.json
[blocker] REGRESSION 8.65%: avg_iteration_time (LATENCY) regresses from 1.7737579917907715 to 1.9272290825843812 (8.65%) in 2.7.0/stress_tests/stress_test_dead_actors.json
[blocker] REGRESSION 7.43%: 1000000_queued_time (LATENCY) regresses from 179.73244193300002 to 193.08359617699998 (7.43%) in 2.7.0/scalability/single_node.json
[noise] REGRESSION 7.27%: 1_1_actor_calls_concurrent (THROUGHPUT) regresses from 5038.298026762205 to 4672.038646803514 (7.27%) in 2.7.0/microbenchmark.json

... still in progress
REGRESSION 7.26%: single_client_tasks_and_get_batch (THROUGHPUT) regresses from 10.988766549840475 to 10.19074851337956 (7.26%) in 2.7.0/microbenchmark.json
REGRESSION 6.45%: 1_1_actor_calls_async (THROUGHPUT) regresses from 8232.577421386566 to 7701.295087357076 (6.45%) in 2.7.0/microbenchmark.json
REGRESSION 5.52%: 1_1_async_actor_calls_sync (THROUGHPUT) regresses from 1519.8915131779668 to 1435.945513303161 (5.52%) in 2.7.0/microbenchmark.json
REGRESSION 5.03%: 1_n_actor_calls_async (THROUGHPUT) regresses from 11016.813113385808 to 10462.712344965346 (5.03%) in 2.7.0/microbenchmark.json
REGRESSION 4.80%: 1_1_async_actor_calls_async (THROUGHPUT) regresses from 2683.097200627191 to 2554.3704032142223 (4.80%) in 2.7.0/microbenchmark.json
REGRESSION 4.72%: single_client_tasks_sync (THROUGHPUT) regresses from 1329.0736290293253 to 1266.3279725566122 (4.72%) in 2.7.0/microbenchmark.json
REGRESSION 4.61%: 1_1_async_actor_calls_with_args_async (THROUGHPUT) regresses from 2071.4152029622965 to 1975.8795067561405 (4.61%) in 2.7.0/microbenchmark.json
REGRESSION 3.13%: client__1_1_actor_calls_concurrent (THROUGHPUT) regresses from 967.4680417367294 to 937.2194076338153 (3.13%) in 2.7.0/microbenchmark.json
REGRESSION 3.01%: client__1_1_actor_calls_async (THROUGHPUT) regresses from 1007.9864291252518 to 977.6396355829638 (3.01%) in 2.7.0/microbenchmark.json
REGRESSION 2.40%: n_n_actor_calls_async (THROUGHPUT) regresses from 32688.224759117064 to 31903.667675785688 (2.40%) in 2.7.0/microbenchmark.json
REGRESSION 2.40%: avg_pg_remove_time_ms (LATENCY) regresses from 0.7933515765763139 to 0.8123740045036456 (2.40%) in 2.7.0/stress_tests/stress_test_placement_group.json
REGRESSION 2.11%: 1_1_actor_calls_sync (THROUGHPUT) regresses from 2527.884786760515 to 2474.5841190880965 (2.11%) in 2.7.0/microbenchmark.json
REGRESSION 1.99%: stage_4_spread (LATENCY) regresses from 0.7514885036633645 to 0.7664489428514454 (1.99%) in 2.7.0/stress_tests/stress_test_many_tasks.json
REGRESSION 1.95%: multi_client_put_calls_Plasma_Store (THROUGHPUT) regresses from 12887.724106896032 to 12636.344204001407 (1.95%) in 2.7.0/microbenchmark.json
REGRESSION 1.24%: multi_client_put_gigabytes (THROUGHPUT) regresses from 38.350117983138134 to 37.87616565828975 (1.24%) in 2.7.0/microbenchmark.json
REGRESSION 0.99%: 1_n_async_actor_calls_async (THROUGHPUT) regresses from 9607.6917020388 to 9512.864150737129 (0.99%) in 2.7.0/microbenchmark.json
REGRESSION 0.54%: 107374182400_large_object_time (LATENCY) regresses from 33.47957800099999 to 33.658706489999986 (0.54%) in 2.7.0/scalability/single_node.json
2.7.0 does not have benchmarks/many_pgs.json
```",ray gene sort blocker regression latency blocker regression latency blocker regression latency blocker regression latency blocker regression latency blocker regression latency blocker regression latency blocker regression latency blocker regression throughput blocker regression latency blocker regression latency noise regression throughput still progress regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression throughput regression latency regression throughput regression latency regression throughput regression throughput regression throughput regression latency,issue,negative,neutral,neutral,neutral,neutral,neutral
1703050741,"It's somehow already fixed, and this PR will also fix https://github.com/ray-project/ray/pull/39204. 

Since it is working already, I won't cherry pick ^",somehow already fixed also fix since working already wo cherry pick,issue,negative,positive,neutral,neutral,positive,positive
1702989672,"mark

Any ideas about this problem? I tried and ray.serve with vllm do double the GPU consumption.",mark problem tried double consumption,issue,negative,neutral,neutral,neutral,neutral,neutral
1702767018,"Also, this seems very loud>? 

```
[2023-09-01 22:38:12,558 I 53255 1167620] redis_context.cc:478: Resolve Redis address to 127.0.0.1
[2023-09-01 22:38:12,558 I 53255 1167620] redis_context.cc:364: Attempting to connect to address 127.0.0.1:49159.
[2023-09-01 22:38:12,558 I 53255 1167620] redis_context.cc:364: Attempting to connect to address 127.0.0.1:49159.
[2023-09-01 22:38:12,559 I 53255 1167620] redis_context.cc:532: Redis cluster leader is 127.0.0.1:49159
[2023-09-01 22:38:12,559 I 53255 1167620] redis_context.cc:478: Resolve Redis address to 127.0.0.1
[2023-09-01 22:38:12,559 I 53255 1167620] redis_context.cc:364: Attempting to connect to address 127.0.0.1:49159.
[2023-09-01 22:38:12,559 I 53255 1167620] redis_context.cc:364: Attempting to connect to address 127.0.0.1:49159.
[2023-09-01 22:38:12,559 I 53255 1167620] redis_context.cc:532: Redis cluster leader is 127.0.0.1:49159
[2023-09-01 22:38:12,560 E 53255 1167620] _raylet.cpp:865: Failed to get session_name
```

(maybe let's set the log level to warning when we instantiate the config)",also loud resolve address connect address connect address cluster leader resolve address connect address connect address cluster leader get maybe let set log level warning,issue,negative,positive,neutral,neutral,positive,positive
1702762278,"Okay, the test result seems pretty promising (looks like test_advanced_9.py is not that tricky to fix). I just started mac test and release test `k8s_serve_ha_test`. Can you sync with @edoakes to test this change asap with services? ",test result pretty promising like tricky fix mac test release test sync test change,issue,positive,positive,positive,positive,positive,positive
1702622297,"Looking forward to the next release as I could not get it to work with `storage_path=""./""` either",looking forward next release could get work either,issue,negative,neutral,neutral,neutral,neutral,neutral
1702557742,"@gjoliver As you are fluent in the connectors, what could be the best solution here? ",fluent could best solution,issue,positive,positive,positive,positive,positive,positive
1702504339,"@imperio-wxm I followed your way and the code to reproduce. After checking, I found that /opt/python/files was the root permission on my side, so nodeB could not see the code search path. I change the code search path and the code works.  Please check whether the code search path over there is a permission problem, and then run the code to try.",way code reproduce found root permission side could see code search path change code search path code work please check whether code search path permission problem run code try,issue,negative,neutral,neutral,neutral,neutral,neutral
1702378105,"Hmm Victoria's fix is merged, and the fact that it fixes several issues make me feel like we should also recreate the channel whenever we retry to connect. I think the slow tests are highly likely slow due to the same reason (channel state transition is slow due to exponential backoff)",fix fact several make feel like also recreate channel whenever retry connect think slow highly likely slow due reason channel state transition slow due exponential,issue,negative,negative,negative,negative,negative,negative
1702268443,"I think this also fixes https://github.com/ray-project/ray/issues/39111 

I ran the test 100 times on this PR, and it now only failed 4 out of 100 times: https://buildkite.com/ray-project/oss-ci-build-pr/builds/34672#018a4def-7346-4291-8285-34cbaa818016
<img width=""882"" alt=""image"" src=""https://github.com/ray-project/ray/assets/11676094/b3c464a4-1d2f-4c74-b661-49c199563346"">
",think also ran test time time image,issue,negative,neutral,neutral,neutral,neutral,neutral
1702250278,"@Darkdragon84  @PRESIDENT810 
I Recently, I have been proposing a new API for batch submission of actor tasks. I believe that this can perfectly meet your requirements and significantly improve performance.

[Core] ray.util.ActorPool supports batch submission of remote actor tasks https://github.com/ray-project/ray/issues/39196",president recently new batch submission actor believe perfectly meet significantly improve performance core batch submission remote actor,issue,positive,positive,positive,positive,positive,positive
1702219306,"> Do we already have an unit test?

I did manually test. UI dashboard is hard to write a unit test, but I think manually test should be enough",already unit test manually test dashboard hard write unit test think manually test enough,issue,negative,negative,negative,negative,negative,negative
1702212292,"Passes
```


[INFO 2023-09-01 06:01:41,857] anyscale_job_runner.py: 145  Output: {'collected_metrics': True, 'last_prepare_time_taken': 6.347128261999956, 'prepare_return_codes': [0], 'return_code': 0, 'total_time_taken': 360.87290291500005, 'uploaded_artifact': False, 'uploaded_metrics': True, 'uploaded_results': True, 'workload_time_taken': 307.565308664}
--
  |  

<br class=""Apple-interchange-newline"">
```",output true false true true,issue,positive,positive,positive,positive,positive,positive
1702200238,The PR is already in the release branch. Closing now. Feel free to reopen and drop release-blocker tag if there are more work to do ,already release branch feel free reopen drop tag work,issue,negative,positive,positive,positive,positive,positive
1702012351,"It is, but not at the cost of the workload melting down or using way too many resources for ingest.

For ingest, you just want to saturate the learner thread, anything excess is just causing stability issues or interfering with other workloads in the cluster.",cost melting way many ingest ingest want saturate learner thread anything excess causing stability interfering cluster,issue,negative,positive,positive,positive,positive,positive
1701963784,"I see. May I ask why you prefer a custom exploration class over overriding the RLModule's `_forward_exploration` method? From my standpoint, it's very similar. In the custom exploration class, you code up your custom behaviour. In the _forward_exploration, you also code up your behaviour. Only that in the custom exploration class you are writing a class, in the `_forward_exploratoin`, you are overriding a method (that is meant to be overriden - we don't consider it internal).

Here is why I, personally, like overriding `_forward_exploration`: It gives you direct access over all inputs and lets you do the model forward pass and sampling of actions however you like. It's the most explicit control you could have over defining exploration behavior. With the old config style - stuff like `{""type"": some_class}` - how do you know how exploration object and model relate? How do you debug your custom exploration class if RLlib hides the whole complexity of its Models? Many users are unaware of how exactly their models ""look"".

The goal of many of our recent modifications is to let RLlib do less ""shady"" stuff in the background that users are unaware off and empower them to make explicit decisions about what's happening.",see may ask prefer custom exploration class method standpoint similar custom exploration class code custom behaviour also code behaviour custom exploration class writing class method meant consider internal personally like direct access model forward pas sampling however like explicit control could exploration behavior old style stuff like type know exploration object model relate custom exploration class whole complexity many unaware exactly look goal many recent let le shady stuff background unaware empower make explicit happening,issue,positive,positive,positive,positive,positive,positive
1701936634,"Looks like this and #38431 are caused by:
- runtime env agent fails to start due to port in use -> raylet fails -> test times out because a node that we think is there isn't, or test fails because head node exits
- node times out because processes are slower to startup on osx -> test times out "" ""

Probably the root cause is breaking PR is https://github.com/ray-project/ray/pull/37585 but the flakiness is likely improved by #39092. Will follow up to improve this test further after 2.7.",like agent start due port use raylet test time node think test head node node time test time probably root cause breaking flakiness likely follow improve test,issue,positive,negative,neutral,neutral,negative,negative
1701924654,"> Looks good to me!
> 
> To be clear, how does the user trigger these debug logs to show up? Is it via `--verbose`?

Thanks Archit for the approval.

The verbose flag will not trigger the debug log. What I did is to modify the log level to ""debug"" in this file: 
/ray/python/ray/_private/ray_constants.py

I ran setup-dev.py, so this file's log level change will take effect during ray up.

For the autoscaler, actually I added dir /ray/python/ray/_private/ into file mount to replace the code in the head node, and it shows the debug level log.

That is a little hacky, but I see that in the [doc](https://docs.ray.io/en/latest/ray-observability/user-guides/configure-logging.html), it tells people to change the log level in python code... ",good clear user trigger show via verbose thanks approval verbose flag trigger log modify log level file ran file log level change take effect ray actually added file mount replace code head node level log little hacky see doc people change log level python code,issue,positive,positive,positive,positive,positive,positive
1701924227,"After looking into this further, the issue is in how bootstrap's ScrollSpy behaves. Some people have reported that [modifying a CSS rule seems to fix](https://github.com/twbs/bootstrap/issues/36431#issuecomment-1397391047) it, but when I load our docs I don't see any conflicting CSS rule here :/. I tried setting the appropriate rule anyway, but didn't see any change. This is supposed to get fixed in a later version of `sphinx-book-theme` anyway, so I think it might just be best to focus on upgrading our frontend build system entirely.",looking issue bootstrap people rule fix load see conflicting rule tried setting appropriate rule anyway see change supposed get fixed later version anyway think might best focus build system entirely,issue,negative,positive,positive,positive,positive,positive
1701922667,"High level design doc: https://docs.google.com/document/d/1sNqy4zyPq6TgzS9nnaQHXVRWumtfQW5tIShVokuLvyY/edit

The PR looks like a lot of changes, but for the most part, I just moved code around. Github gets a bit confused on identify changes due to file renaming.",high level design doc like lot part code around bit confused identify due file,issue,negative,negative,negative,negative,negative,negative
1701919605,"`ray.data.read_csv(""s3://anonymous@air-example-data/breast_cancer.csv"", ray_remote_args={""num_cpus"": 0})` the `ray_remote_args` parameter is supposed to only set the read tasks. So the behavior is expected.
But we indeed don't have an API for setting the resources of `split` tasks. ",parameter supposed set read behavior indeed setting split,issue,negative,neutral,neutral,neutral,neutral,neutral
1701892604,"Totally, let me write into a doc",totally let write doc,issue,negative,neutral,neutral,neutral,neutral,neutral
1701878737,@scottjlee this PR will bring sharding to runtime and reduce your time-to-signal wait time from 1 hour -> 30 minutes ;); also need your stamp/review on the data part,bring reduce wait time hour also need data part,issue,negative,neutral,neutral,neutral,neutral,neutral
1701869276,Thanks @matthewdeng for the careful review! Updated the example accordingly.,thanks careful review example accordingly,issue,positive,positive,neutral,neutral,positive,positive
1701834881,@vitsai let's remove it from CI test and add it to microbenchmark. Let's do it in a follow up PR,let remove test add let follow,issue,negative,neutral,neutral,neutral,neutral,neutral
1701816496,"I checked the examples in [Ray Train Examples](https://docs.ray.io/en/master/train/examples.html). Seems that they  already handled metrics aggregation with 3rd party library (lightning, transformers, accelerate), or does not have evaluation sets. I'll close it now.",checked ray train already handled metric aggregation party library lightning accelerate evaluation close,issue,negative,neutral,neutral,neutral,neutral,neutral
1701762397,"BTW, it seems like https://github.com/ray-project/ray/issues/31860 is resolved on master, so this issue may also no longer be a problem. @ArturNiederfahrenhorst @simonsays1980 ",like resolved master issue may also longer problem,issue,negative,neutral,neutral,neutral,neutral,neutral
1701761981,"Looks like the reproducible example no longer has the error, so closing the issue. Please feel free to reopen if it is not resolved.
```
>>> data = [{""dict_c"": {""a"": np.array([[1,2,3],[4,5,6]])}, ""int_c"": 1}]
>>> df = ray.data.from_items(data).to_pandas()
>>> df
                          dict_c  int_c
0  {'a': [[1, 2, 3], [4, 5, 6]]}      1
```",like reproducible example longer error issue please feel free reopen resolved data data,issue,positive,positive,positive,positive,positive,positive
1701712858,@GeneDer would be great if we could make these release tests at some point :),would great could make release point,issue,positive,positive,positive,positive,positive,positive
1701684824,"Thanks for the review!

> Do you want to add this example to the Examples Gallery? Instructions are at go/example-gallery.

@angelinalg I'm not sure how to decide. Should all examples be in the gallery or is there some criteria? Also, would you tag this with code-example or tutorial?",thanks review want add example gallery sure decide gallery criterion also would tag tutorial,issue,positive,positive,positive,positive,positive,positive
1701671733,"> Currently attempting the exact same thing. Is it possible to restate the complete correct implementation of your centralized crititc with multiple agent? Maybe adapting the example to support mulitple agents right of the batch would ease this for many users.

hello 
Has anyone implemented it?",currently exact thing possible restate complete correct implementation multiple agent maybe example support right batch would ease many hello anyone,issue,positive,positive,positive,positive,positive,positive
1701551688,Do you want to add this example to the Examples Gallery? Instructions are at go/example-gallery.,want add example gallery,issue,negative,neutral,neutral,neutral,neutral,neutral
1701530043,"2023, version 2.6.3 - still the same on AWS.  (+many, many other issues - is anybody working on `ray up` on AWS???)

```yaml
# use: ray up -y ray-cluster-config.yaml to start it

cluster_name: test
max_workers: 16

setup_commands:
  # unattended upgrades is a mess; also problems with APT, ""uninstall"" manually
  - ""[ -f ~/.ok ] || (while ! sudo rm /usr/bin/unattended-upgrade; do sleep 1; done)""
  - ""[ -f ~/.ok ] || (sudo killall -9 unattended-upgrade || true)""  # just in case
  - touch ~/.ok
  # installed wrong version!
  - pip install ""ray[default]==2.6.3""

provider:
  type: aws
  region: us-east-1
  cache_stopped_nodes: False

available_node_types:
  ray.head.default:
    node_config:
      InstanceType: m5.large
  ray.worker.default:
    max_workers: 16
    node_config:
      InstanceType: m5.large
      InstanceMarketOptions:
        MarketType: spot
```

",version still many anybody working ray use ray start test unattended mess also apt manually sleep done true case touch wrong version pip install ray default provider type region false spot,issue,negative,positive,neutral,neutral,positive,positive
1701526659,`test_telemetry` passed 3 times in a row on Windows. This change seems to deflake the test. @edoakes this is ready to merge.,time row change test ready merge,issue,negative,positive,positive,positive,positive,positive
1701482142,">I do not want to push to other places on pipelines (for each commit).

Agree. I mostly used the default ray images comes with Raycluster yaml.
I updated the issue title and lowered the Severity level because I am not getting the rate limit right now.",want push commit agree mostly used default ray come issue title severity level getting rate limit right,issue,positive,positive,positive,positive,positive,positive
1701476977,"Yep, still debugging the issue for this and thus marked as draft. I think the serve job is not causing the issue. There might be something in how we log/ fetching the logs. Also seeing similar issue in some other release tests",yep still issue thus marked draft think serve job causing issue might something fetching also seeing similar issue release,issue,negative,positive,neutral,neutral,positive,positive
1701473141,"@GeneDer is this your attempt at debugging or do you actually intend to run the test for a shorter duration? The whole point of the test is to be ""long running,"" so let's avoid shortening it",attempt actually intend run test shorter duration whole point test long running let avoid shortening,issue,negative,positive,neutral,neutral,positive,positive
1701437421,"Possibly related to 
```
Traceback (most recent call last):
  File ""/tmp/ray/session_2023-08-31_00-17-02_421842_48/runtime_resources/working_dir_files/s3_ray-release-automation-results_working_dirs_dask_on_ray_100gb_sort_aws_py311_ktqgonnkui__anyscale_pkg_5f68f09bdaf9a6a5fc2b76fecaa096f7/dask_on_ray/dask_on_ray_sort.py"", line 225, in <module>
    output = trial(
             ^^^^^^
  File ""/tmp/ray/session_2023-08-31_00-17-02_421842_48/runtime_resources/working_dir_files/s3_ray-release-automation-results_working_dirs_dask_on_ray_100gb_sort_aws_py311_ktqgonnkui__anyscale_pkg_5f68f09bdaf9a6a5fc2b76fecaa096f7/dask_on_ray/dask_on_ray_sort.py"", line 146, in trial
    df.set_index(""a"", shuffle=""tasks"", max_branch=float(""inf"")).head(
  File ""/home/ray/anaconda3/lib/python3.11/site-packages/ray/util/dask/scheduler.py"", line 203, in ray_dask_get
    result = ray_get_unpack(object_refs, progress_bar_actor=pb_actor)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ray/anaconda3/lib/python3.11/site-packages/ray/util/dask/scheduler.py"", line 504, in ray_get_unpack
    computed_result = get_result(object_refs)
                      ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ray/anaconda3/lib/python3.11/site-packages/ray/util/dask/scheduler.py"", line 491, in get_result
    return ray.get(object_refs)
           ^^^^^^^^^^^^^^^^^^^^
  File ""/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py"", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py"", line 2554, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError: ray::dask:('head-800-10-sort_index-da104cf65035e80d66311e1c8dcee6b3', 0)() (pid=636, ip=10.0.62.176)
  At least one of the input arguments for this task could not be computed:
ray.exceptions.RayTaskError: ray::dask:('head-partial-10-sort_index-da104cf65035e80d66311e1c8dcee6b3', 178)() (pid=640, ip=10.0.62.176)
  At least one of the input arguments for this task could not be computed:
ray.exceptions.RayTaskError: ray::dask:('sort_index-da104cf65035e80d66311e1c8dcee6b3', 178)() (pid=640, ip=10.0.62.176)
  At least one of the input arguments for this task could not be computed:
ray.exceptions.RayTaskError: ray::dask:('simple-shuffle-9e54694d6ba4e4daf2314c538fa4226e', 178)() (pid=640, ip=10.0.62.176)
  At least one of the input arguments for this task could not be computed:
ray.exceptions.ObjectFetchTimedOutError: Failed to retrieve object 2e14316f415b27ebffffffffffffffffffffffff02000000b3000000. To see information about where this ObjectRef was created in Python, set the environment variable RAY_record_ref_creation_sites=1 during `ray start` and `ray.init()`.
```

https://console.anyscale-staging.com/o/anyscale-internal/jobs/prodjob_bix4iwh3f4gkts5dd57wigymt4",possibly related recent call last file line module output trial file line trial file line result file line file line return file line return file line wrapper return file line get raise ray least one input task could ray least one input task could ray least one input task could ray least one input task could retrieve object see information python set environment variable ray start,issue,negative,negative,negative,negative,negative,negative
1701409732,"Checked the release test metrics, it's consistent in past several days, closing the issue.",checked release test metric consistent past several day issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1701403892,"Windows tests, doctest, and k8s chaos tests unrelated. This PR only touches the vSphere node provider",chaos unrelated node provider,issue,negative,neutral,neutral,neutral,neutral,neutral
1701402080,I think this is a duplicate of https://github.com/ray-project/ray/issues/33318 - see that issue for more information.,think duplicate see issue information,issue,negative,neutral,neutral,neutral,neutral,neutral
1701391172,"The value is pretty platform dependent; 6s -> 2s was on some AWS machine, but on Mac it is 10.7s -> 6.7s. Seems like a test here would just be flaky and also not add much value since it would have to test the ceiling of all the start times. I guess we could choose one specific platform + machine to always run the test on?",value pretty platform dependent machine mac like test would flaky also add much value since would test ceiling start time guess could choose one specific platform machine always run test,issue,positive,positive,positive,positive,positive,positive
1701369582,"Thanks for the detailed response @JalinWang!

> That's why the code changes can't cause actors to reload! So what's the recommended workflow for developing?

As a workaround for now, you can add an unused env var to the `runtime_env` and update its value each time you deploy. That should trigger a code version update. E.g.:

`runtime_env = {""env_vars"": {""FAKE_ENV_VAR"": ""v1""}}`

and you could change `v1` to `v2` and so forth whenever you redeploy.

I think there are some improvements we can make to this flow, but currently the team is busy getting ready for Ray Summit (see #39159 for context). We can make longer-term improvements once the team gets some more time. Apologies for any inconvenience! Please don't hesitate to follow up in this issue (or file new ones) in the meantime if you run into any more problems.",thanks detailed response code ca cause reload add unused update value time deploy trigger code version update could change forth whenever redeploy think make flow currently team busy getting ready ray summit see context make team time inconvenience please hesitate follow issue file new run,issue,positive,positive,positive,positive,positive,positive
1701353752,"Yea, my intention is just to notify users that there might be typos in the function paths they passed. Technically proxies shouldn't fail due to this. And if all the functions are unable to be imported, I would expect the proxy actor can still start HTTP proxy without gRPC proxy running. But can totally just raise the error and stop right there. Will patch it after other higher priority items!",yea intention notify might function technically fail due unable would expect proxy actor still start proxy without proxy running totally raise error stop right patch higher priority,issue,negative,negative,neutral,neutral,negative,negative
1701335137,"@raulchen Yes, I tried convert raw streaming bytes to pyarrow table objects. It consumed 2x memory of object storage with comparison of using raw bytes, and this case was not acceptable for me. ",yes tried convert raw streaming table memory object storage comparison raw case acceptable,issue,positive,negative,negative,negative,negative,negative
1701320569,"This was modified due to Alexey's comment https://github.com/ray-project/ray/pull/37310#discussion_r1302304968 But yea, I totally agree it makes things easier to work with if dictionary is allowed. Will patch it after other high priority items",due comment yea totally agree easier work dictionary patch high priority,issue,positive,positive,neutral,neutral,positive,positive
1701287766,"Hi @krfricke  Thank you for the detailed explanation. I also used this[ tutorial](https://github.com/ray-project/ray-educational-materials/blob/main/NLP_workloads/Text_generation/LLM_finetuning_and_batch_inference.ipynb) and it is as you said.  I am  mixing up the concepts of actors, workers, and nodes.

I have a few doubts:
1.  If i use a config:
```scaling_config = ScalingConfig(
        num_workers=1, use_gpu=use_gpu, resources_per_worker={""CPU"": 2, ""GPU"": 8}
    )
```
Does this only one LightningTrainer will run with access to 8 GPUs?  Would it still train in a distributed fashion?

2. You said with config 
```
scaling_config = ScalingConfig(
        num_workers=2, use_gpu=use_gpu, resources_per_worker={""CPU"": 1, ""GPU"": 1}
    )
```
Each trial has 2 workers, each using 1 GPU, 1 CPU. Given 4 GPUS on the SLURM cluster, 2 Trials would run.

What config can I use so 2 trials run in parallel, each with 2 GPUs per worker? I couldn't find the flag to set number of trials (Do I add the ConcurrencyLimiter?) 

3.  How does the Ray cluster system know if I want to allocate a job to an entire SLURM node? Does it matter to the setup if it is running on the GPUs of the same node or if the GPUs during a particular trial are split across nodes? This part is a bit of a black box for me right now. I tried reading the documentation on the Ray Actors but are they created per node or just processes that are allocated the given number of GPU/CPU?

Thank you!
",hi thank detailed explanation also used tutorial said use one run access would still train distributed fashion said trial given cluster would run use run parallel per worker could find flag set number add ray cluster system know want allocate job entire node matter setup running node particular trial split across part bit black box right tried reading documentation ray per node given number thank,issue,positive,positive,positive,positive,positive,positive
1701247797,I will make regression release blockers by today,make regression release today,issue,negative,neutral,neutral,neutral,neutral,neutral
1701235055,All tests also passed on the serve cancellation PR with this change merged in: https://buildkite.com/ray-project/oss-ci-build-pr/builds/34426,also serve cancellation change,issue,negative,neutral,neutral,neutral,neutral,neutral
1701228370,"Same tracebacks are happening on the release branch, so I'm not concerned about them: https://buildkite.com/ray-project/release-tests-branch/builds/2103#018a44b2-6415-495b-8823-7824101415f8

Note that this test also forcibly kills replicas constantly, so seeing some tracebacks is expected.",happening release branch concerned note test also forcibly constantly seeing,issue,negative,neutral,neutral,neutral,neutral,neutral
1701224312,"@rkooo567 there are a lot of failures that have tracebacks like this:
```
28338 ^[[2m^[[36m(pid=149561)^[[0m Traceback (most recent call last):
28339 ^[[2m^[[36m(pid=149561)^[[0m   File ""python/ray/_raylet.pyx"", line 1997, in ray._raylet.task_execution_handler
28340 ^[[2m^[[36m(pid=149561)^[[0m   File ""python/ray/_raylet.pyx"", line 1853, in ray._raylet.execute_task_with_cancellation_handler
28341 ^[[2m^[[36m(pid=149561)^[[0m   File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/function_manager.py"", line 574, in load_actor_class
28342 ^[[2m^[[36m(pid=149561)^[[0m     actor_class = self._load_actor_class_from_gcs(
28343 ^[[2m^[[36m(pid=149561)^[[0m   File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/function_manager.py"", line 677, in _load_actor_class_from_gcs
28344 ^[[2m^[[36m(pid=149561)^[[0m     actor_class = pickle.loads(pickled_class)
28345 ^[[2m^[[36m(pid=149561)^[[0m   File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/serve/__init__.py"", line 4, in <module>
28346 ^[[2m^[[36m(pid=149561)^[[0m     from ray.serve.api import (
28347 ^[[2m^[[36m(pid=149561)^[[0m   File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/serve/api.py"", line 15, in <module>
28348 ^[[2m^[[36m(pid=149561)^[[0m     from ray.serve.built_application import BuiltApplication
28349 ^[[2m^[[36m(pid=149561)^[[0m   File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/serve/built_application.py"", line 7, in <module>
28350 ^[[2m^[[36m(pid=149561)^[[0m     from ray.serve.deployment import Deployment
28351 ^[[2m^[[36m(pid=149561)^[[0m   File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/serve/deployment.py"", line 24, in <module>
28352 ^[[2m^[[36m(pid=149561)^[[0m     from ray.serve.context import get_global_client
28353 ^[[2m^[[36m(pid=149561)^[[0m   File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/serve/context.py"", line 12, in <module>
28354 ^[[2m^[[36m(pid=149561)^[[0m     from ray.serve._private.client import ServeControllerClient
28355 ^[[2m^[[36m(pid=149561)^[[0m   File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/serve/_private/client.py"", line 27, in <module>
28356 ^[[2m^[[36m(pid=149561)^[[0m     from ray.serve._private.deploy_utils import get_deploy_args
28357 ^[[2m^[[36m(pid=149561)^[[0m   File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/serve/_private/deploy_utils.py"", line 8, in <module>
28358 ^[[2m^[[36m(pid=149561)^[[0m     from ray.serve.schema import ServeApplicationSchema
28359 ^[[2m^[[36m(pid=149561)^[[0m   File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/serve/schema.py"", line 856, in <module>
28360 ^[[2m^[[36m(pid=149561)^[[0m     class ServeInstanceDetails(BaseModel, extra=Extra.forbid):
28361 ^[[2m^[[36m(pid=149561)^[[0m   File ""pydantic/main.py"", line 197, in pydantic.main.ModelMetaclass.__new__
28362 ^[[2m^[[36m(pid=149561)^[[0m   File ""pydantic/fields.py"", line 497, in pydantic.fields.ModelField.infer
28363 ^[[2m^[[36m(pid=149561)^[[0m   File ""pydantic/fields.py"", line 460, in pydantic.fields.ModelField._get_field_info
28364 ^[[2m^[[36m(pid=149561)^[[0m   File ""pydantic/typing.py"", line 120, in pydantic.typing.get_origin
28365 ^[[2m^[[36m(pid=149561)^[[0m   File ""/home/ray/anaconda3/lib/python3.8/typing.py"", line 1271, in get_origin
28366 ^[[2m^[[36m(pid=149561)^[[0m     def get_origin(tp):
28367 ^[[2m^[[36m(pid=149561)^[[0m   File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py"", line 778, in sigterm_handler
28368 ^[[2m^[[36m(pid=149561)^[[0m     sys.exit(1)
28369 ^[[2m^[[36m(pid=149561)^[[0m SystemExit: 1
28370 ^[[2m^[[36m(pid=149561)^[[0m AttributeError: 'Worker' object has no attribute 'core_worker'
```

Looks sort of suspicious (some others had this raised in `terminate_asyncio_thread` which looks related). Let me check other recent runs to see if this is new.",lot like recent call last file line file line file line file line file line module import file line module import file line module import deployment file line module import file line module import file line module import file line module import file line module class file line file line file line file line file line file line object attribute sort suspicious raised related let check recent see new,issue,negative,positive,neutral,neutral,positive,positive
1701207331,Is there any progress on this issue? I am running into the same bug. ,progress issue running bug,issue,negative,neutral,neutral,neutral,neutral,neutral
1701149538,"@rkooo567 I ran the linter to fix the linting issues, so that should be good. I also ran some commands to fix a commit that did not have a sign-off however I think I may have messed up and picked up a bunch of commits from main as well. Any suggestions on how to fix this would be appreciated!",ran linter fix good also ran fix commit however think may picked bunch main well fix would,issue,positive,positive,positive,positive,positive,positive
1700868848,"@mvanness354 I've investigated this further and posted a fix in #39157, can you maybe take a look?",posted fix maybe take look,issue,negative,neutral,neutral,neutral,neutral,neutral
1700835975,"> storage_path=""./"" works for me and I can perform tuning without saving any model checkpoints. However, if I do need to save checkpoints in the future this will not work. Hopefully this is fully fixed in 2.7 release.
> 
> > Thanks for raising this and following up. This is indeed a bug, and it should be fixed here: #38319
> > The fix will be included in Ray 2.7.
> > As a workaround, you can set the `storage_path` to a relative local directory, which will then not trigger the buggy code path.
> > ```
> > from ray import air, tune
> > 
> > tuner = tune.Tuner(
> >     train_fn,
> >     run_config=air.RunConfig(storage_path=""./"")
> > )
> > tuner.fit()
> > ```

Hey @harryseely , have you tried this with PBT?? I'm unable to use `storage_path=""./""` as it fails all the time.",work perform tuning without saving model however need save future work hopefully fully fixed release thanks raising following indeed bug fixed fix included ray set relative local directory trigger buggy code path ray import air tune tuner hey tried unable use time,issue,positive,negative,neutral,neutral,negative,negative
1700835475,"Hi! Sorry for the late reply. I changed the prints to logs to see the process and thread ids. I also added logging in the actor method `A.add` to see if the task is actually executed (which it is as printing the future suggests). Indeed it seems that the callback is running in a different thread in the main process
```
2023-08-31 13:01:13,606	INFO worker.py:1621 -- Started a local Ray instance.
2023-08-31 13:01:14,984|pid=70664|tid=139890904958016|main|INFO: queue size before fetching result: 1
2023-08-31 13:01:14,984|pid=70664|tid=139890904958016|main|INFO: submitted job
2023-08-31 13:01:14,986|pid=70664|tid=139890163443264|main|INFO: adding Actor(A, 03f116f29b484715b18a454b01000000) to queue for future <Future at 0x7f3add0339d0 state=finished returned int>
(A pid=71054) 2023-08-31 13:01:14,985|pid=71054|tid=139827396707392|A|INFO: adding 3 to 2
(A pid=71054) 2023-08-31 13:01:14,985|pid=71054|tid=139827396707392|A|INFO: returning 5
2023-08-31 13:01:16,985|pid=70664|tid=139890904958016|main|INFO: fetching result for <Future at 0x7f3add0339d0 state=finished returned int>
2023-08-31 13:01:16,985|pid=70664|tid=139890904958016|main|INFO: fetched result: 5
```
you can see that basically all logs from the main process (70664) come from the main thread (139890904958016), only the log from within the callback comes from a different thread 139890163443264. If that's the issue, how can we remedy that? 
Is it possible to wrap a thread-safe queue implementation in `ray.util.Queue`?",hi sorry late reply see process thread also added logging actor method see task actually executed printing future indeed running different thread main process local ray instance queue size fetching result job actor queue future future returned fetching result future returned fetched result see basically main process come main thread log within come different thread issue remedy possible wrap queue implementation,issue,negative,negative,neutral,neutral,negative,negative
1700740314,I'm also experiencing the same issue when trying to adopt aspects of the `example-gpu-docker.yaml` cluster config.,also issue trying adopt cluster,issue,negative,neutral,neutral,neutral,neutral,neutral
1700610118,"@pcmoritz can you check the new changes? https://github.com/ray-project/ray/pull/38776/commits/8b17f8f254019698b99dd0de5348ef42ee8847a4

I will merge once tests pass! (and if it looks good to you)",check new merge pas good,issue,negative,positive,positive,positive,positive,positive
1700524136,"> Oh interesting, are you using this workflow for development or production? Generally in production, we recommend folks use a `runtime_env` or Docker container with all the Serve app's code to avoid issues when upgrading code.

Partial production :thinking: An internal project and currently under development stage. That's why we need to frequently update codes and redeploy them. `runtime_env` requires a packing scheme which frustrates me a little. When the project is stable, I may try it. As for Docker, our slurm cluster has disabled containers for certain reasons :(

Offtopic: `runtime_env` is another issue I haven't filed yet. This config REQUIRES pointing to a `zip` file. However, 1). we already have NFS for sync and do not want to pack the project folder (includes some large checkpoints) over and over again; 2). there are many folders scattered over with different structures and each one needs a packing script; 3). Ray Air already supports NFS scheme https://github.com/ray-project/ray/issues/37177. So I'm wondering if it's okay to support NFS path in `runtime_env`. 

> And did you notice this issue started in Ray 2.6.1, or did you also see it in previous versions?

I didn't try other versions. If needed, I may spare some time to test next week.

UPDATE:
Oh I find the under-the-hood logic [here](https://github.com/ray-project/ray/pull/34430)
>The config's import_path and runtime_env tells us the code version for the deployments

That's why the code changes can't cause actors to reload! So what's the recommended workflow for developing? Pure packing script with `runtime_env` pointing to a fixed zip path seems not to work. Use commit hashing? But how to update the deployment config.yaml file efficiently :thinking: 
",oh interesting development production generally production recommend use docker container serve code avoid code partial production thinking internal project currently development stage need frequently update redeploy scheme little project stable may try docker cluster disabled certain another issue yet pointing zip file however already sync want pack project folder large many scattered different one need script ray air already scheme wondering support path notice issue ray also see previous try may spare time test next week update oh find logic u code version code ca cause reload pure script pointing fixed zip path work use commit update deployment file efficiently thinking,issue,positive,positive,neutral,neutral,positive,positive
1700426205,"Yea, release branch is also failing. Would be great to mark it unstable and cherry pick the PR if it's no longer a release blocker🙏",yea release branch also failing would great mark unstable cherry pick longer release blocker,issue,negative,positive,positive,positive,positive,positive
1700425484,@rynewang can you make sure to cherry pick this to 2.7 branch? ,make sure cherry pick branch,issue,negative,positive,positive,positive,positive,positive
1700425320,@krfricke just a reminder to create a cherry-pick PR to get it into the release branch as well :) ,reminder create get release branch well,issue,negative,neutral,neutral,neutral,neutral,neutral
1700396839,"SGTM. I guess we can also track the completed rpcs with status to understand the network failures

```
Why no error counts?
Error counts can be computed on your metrics backend by totalling the different per-status values.
```

",guess also track status understand network error error metric different,issue,negative,neutral,neutral,neutral,neutral,neutral
1700392684,Test result so far seems pretty promising,test result far pretty promising,issue,positive,positive,positive,positive,positive,positive
1700361600,"@aslonnie: totally, i'll merge them together. Also this PR depends on this PR https://github.com/ray-project/ray/pull/38737 that needs some review ;))

This PR needs approval from a few code owners, I guess mostly from data team, CC: @c21 or @amogkam , thankkks ",totally merge together also need review need approval code guess mostly data team,issue,negative,positive,positive,positive,positive,positive
1700359335,"Hi @krfricke, thanks for providing these results. I was able to install from nightly using your link and verify that checkpointing does work currently. However, there are 2 errors that I see in the nightly version:

1) The checkpointing works correctly if `checkpoint_frequency` is set in the trainer, but not if it is set in the tuner. This is not a problem as a long as it is clear that `checkpoint_frequency` should always be set in the trainer instead of the tuner, which is not the case right now. Perhaps this could be clearer, via the documentation and/or by raising a warning or error.

2) When installing from nightly, I actually encountered an error during tuning. Specifically, when trying to resume a trial, I get the error below. I think the issue is that [MODEL_FILENAME](https://github.com/ray-project/ray/blob/edba68c3e7cf255d1d6479329f305adb7fa4c3ed/python/ray/train/xgboost/xgboost_checkpoint.py#L18C34-L18C34) in `XGBoostCheckpoint` is set to `model.json`, but [MODEL_KEY](https://github.com/ray-project/ray/blob/2522aa47783b4c045290a09d16d8140be97ca687/python/ray/air/constants.py#L5) is set to `model` (no `.json`). I was able to fix the issue by simply changing `MODEL_FILENAME` to `model` instead of `model.json`. 
```
xgboost.core.XGBoostError: [04:38:17] ../src/common/io.cc:102: Opening /root/ray_results/breast_cancer/XGBoostTrainer_0a5be_00001_1_eta=0.2134,max_depth=6_2023-08-31_04-37-06/checkpoint_000000/model.json failed: No such file or directory
```",hi thanks providing able install nightly link verify work currently however see nightly version work correctly set trainer set tuner problem long clear always set trainer instead tuner case right perhaps could clearer via documentation raising warning nightly actually error tuning specifically trying resume trial get error think issue set set model able fix issue simply model instead opening file directory,issue,negative,positive,positive,positive,positive,positive
1700357037,"Synced offline with @rkooo567 and @iycheng. This approach is not viable due to the complexity required to break the circular dependency of the following: 

1. create/fetch session name
3. create session dir tmp dir and log dirs and stderr stdout files using session name, 
3. start GCS (requires log_dir, stdout file, stderr file)

We would bring in a lot of edge cases and custom logic for it.",approach viable due complexity break circular dependency following session name create session log session name start file file would bring lot edge custom logic,issue,negative,negative,neutral,neutral,negative,negative
1700330333,"This is also effectively a breaking API change, which we should discuss more broadly before changing.",also effectively breaking change discus broadly,issue,negative,positive,positive,positive,positive,positive
1700329917,"Hey guys, I think we have to revert this. Setting 25% doesn't make sense for many use cases, in general for ingest you want to use a small fraction of the memory avail on the destination node, since the goal is not to maximize throughput but to minimize overhead. This is very difficult from the general use case.",hey think revert setting make sense many use general ingest want use small fraction memory avail destination node since goal maximize throughput minimize overhead difficult general use case,issue,negative,negative,neutral,neutral,negative,negative
1700300635,"we can copy the ray images to places in addition to docker hub on ray release. @kevin85421 I think you can create an account and do the mirroring yourself, or add steps into the ray release process, and let the release manager do it.

but I do not want to push to other places on pipelines (for each commit). we are even thinking about stop publishing the per-commit build.

also, I am just saying that using quay.io is unlikely going to be a reliable solution to the pull rate limit. so maybe rephrase this issue or open a new one if the request is just to add a mirror.",copy ray addition docker hub ray release think create account add ray release process let release manager want push commit even thinking stop build also saying unlikely going reliable solution pull rate limit maybe rephrase issue open new one request add mirror,issue,positive,negative,negative,negative,negative,negative
1700293815,"> As I understand it, the issue asks Ray CI to push images not only to DockerHub but also to other image registries. This way, they can pull Ray's official images from a registry without rate limitations. Is my understanding correct, @tedhtchang?

Correct. I was reviewing a KubeRay PR but I failed to pull Kuberay operator and failed to create RayCluster on KinD on a VM. For Kuberay image, I could specify the alternative quay.io repo. For Ray image, there is no alternative I can specify. It would be great if you could publish or mirror Ray image to an alternative repo as part of the build process.
For prod/dev, we are required to use our custom Ray image on Quay.io but I cannot use our custom image for opensource Kuberay development.

",understand issue ray push also image way pull ray official registry without rate understanding correct correct pull operator create kind image could specify alternative ray image alternative specify would great could publish mirror ray image alternative part build process use custom ray image use custom image development,issue,positive,positive,positive,positive,positive,positive
1700285565,Rerunning it with core https://buildkite.com/ray-project/release-tests-pr/builds/51571. I hope the wheel is correct this time...,core hope wheel correct time,issue,negative,neutral,neutral,neutral,neutral,neutral
1700280406,@edoakes can you take a look at failures from https://buildkite.com/ray-project/release-tests-pr/builds/51486#018a485f-94f3-4afa-96a5-3f3ea7cab789? (from logs its failures are pretty mysterious),take look pretty mysterious,issue,negative,positive,positive,positive,positive,positive
1700259132,"and quay.io seems to have rate limit too, just not very explicit about it:

> only rate limits in the most severe circumstances to maintain service levels (e.g. tens of requests per second from the same IP address).

https://access.redhat.com/articles/5531191

10/s can pretty easily got hit when a ray cluster is behind a nat gateway.",rate limit explicit rate severe maintain service per second address pretty easily got hit ray cluster behind nat gateway,issue,negative,positive,neutral,neutral,positive,positive
1700247632,"mostly all registries have rate limits.. bandwidth is not free and DoS issues are real..

- ecr rate limit: https://docs.aws.amazon.com/AmazonECR/latest/userguide/service-quotas.html
- gar rate limit: https://cloud.google.com/artifact-registry/quotas

not sure about quay.io , if it does not have rate limit, it should.

also docker hub is not for production use in general. ray-project/ray images on docker hub is also not for production use; it is for distribution only.

kuberay should not be encoding ray-project/ray in the k8s manifest yaml file. that is just asking for failures when user trying to scale.

self-hosting a registry backed by s3 or filesystem is also pretty simple: https://docs.docker.com/registry/deploying/",mostly rate free do real rate limit gar rate limit sure rate limit also docker hub production use general docker hub also production use distribution manifest file user trying scale registry backed also pretty simple,issue,positive,positive,positive,positive,positive,positive
1700166326,"I think the fix is there, just need to wait for nightly to catch up",think fix need wait nightly catch,issue,negative,neutral,neutral,neutral,neutral,neutral
1700128447,"@angelinalg Comments have been addressed, please take another look! Thanks!",please take another look thanks,issue,positive,positive,positive,positive,positive,positive
1700115029,"storage_path=""./"" works for me and I can perform tuning without saving any model checkpoints. However, if I do need to save checkpoints in the future this will not work. Hopefully this is fully fixed in 2.7 release.

> Thanks for raising this and following up. This is indeed a bug, and it should be fixed here: #38319
> 
> The fix will be included in Ray 2.7.
> 
> As a workaround, you can set the `storage_path` to a relative local directory, which will then not trigger the buggy code path.
> 
> ```
> from ray import air, tune
> 
> tuner = tune.Tuner(
>     train_fn,
>     run_config=air.RunConfig(storage_path=""./"")
> )
> tuner.fit()
> ```",work perform tuning without saving model however need save future work hopefully fully fixed release thanks raising following indeed bug fixed fix included ray set relative local directory trigger buggy code path ray import air tune tuner,issue,positive,positive,neutral,neutral,positive,positive
1700068894,"As I understand it, the issue asks Ray CI to push images not only to DockerHub but also to other image registries. This way, they can pull Ray's official images from a registry without rate limitations. Is my understanding correct, @tedhtchang?",understand issue ray push also image way pull ray official registry without rate understanding correct,issue,negative,neutral,neutral,neutral,neutral,neutral
1700061702,"> can we make the grpc stats off by default (add a system config to enable it)?

config added.

> Also can you give me a list of grpc stats? I wonder if we should filter some of them since it will be a lot.

https://github.com/census-instrumentation/opencensus-specs/blob/master/stats/gRPC.md

> Lastly, can you pick which stats we should track from the multi cloud setup?

`grpc.io/client/roundtrip_latency` and `grpc.io/server/server_latency` is my special interest",make default add system enable added also give list wonder filter since lot lastly pick track cloud setup special interest,issue,positive,positive,positive,positive,positive,positive
1700034169,"I think they are trying to pull, not push.

maybe login first with a docker hub token?

or mirror the image to somewhere yourself and modify the yaml file?

not much I can do here really.",think trying pull push maybe login first docker hub token mirror image somewhere modify file much really,issue,negative,positive,positive,positive,positive,positive
1700001906,"> me and @can-anyscale are currently not responsible for kuberay's CI.

This isn't related to KubeRay CI. @tedhtchang requests to push ""Ray images"" to both DockerHub and other image registries without rate limitations, such as Quay.",currently responsible related push ray image without rate quay,issue,negative,positive,neutral,neutral,positive,positive
1699990128,"- me and @can-anyscale are currently not responsible for kuberay's CI.
- CI / production probably should not be using docker hub, especially when launching a cluster or something
- most container registries have rate limiting for unauthenticated calls.

I am also not sure how is this CI related.",currently responsible production probably docker hub especially cluster something container rate limiting unauthenticated also sure related,issue,positive,positive,positive,positive,positive,positive
1699988142,I'll merge this together with https://github.com/ray-project/ray/pull/39044 so that there are no straggler failing tests on civ1,merge together straggler failing,issue,negative,neutral,neutral,neutral,neutral,neutral
1699983475,"previous release tests are here; https://buildkite.com/ray-project/release-tests-pr/builds/51486

(note: I didn't finish some of long running tests to run core tests)",previous release note finish long running run core,issue,negative,negative,negative,negative,negative,negative
1699974589,"can we make the grpc stats off by default (add a system config to enable it)?

Also can you give me a list of grpc stats? I wonder if we should filter some of them since it will be a lot. 

Lastly, can you pick which stats we should track from the multi cloud setup? ",make default add system enable also give list wonder filter since lot lastly pick track cloud setup,issue,negative,neutral,neutral,neutral,neutral,neutral
1699972905,Test test seems pretty promising,test test pretty promising,issue,positive,positive,positive,positive,positive,positive
1699957752,"Friendly bump :)

Is this a change the team is interested in? If not I can close the MR. Alternatively I can open an issue to discuss the goal, before jumping to an implementation.",friendly bump change team interested close alternatively open issue discus goal implementation,issue,positive,positive,positive,positive,positive,positive
1699917156,I will also start review after current busy period is done! ,also start review current busy period done,issue,negative,positive,neutral,neutral,positive,positive
1699908193,I will reopen the issue with the core root cause,reopen issue core root cause,issue,negative,neutral,neutral,neutral,neutral,neutral
1699893607,"After syncing with @zcin, I moved the ""Inspect an application with `serve config` and `serve status`"" section to the monitoring guide since it seemed to be a better fit there.",inspect application serve serve status section guide since better fit,issue,positive,positive,positive,positive,positive,positive
1699797369,"> This looks fine to me, but there are some timedout tests on CI https://buildkite.com/ray-project/oss-ci-build-pr/builds/34036#018a3d3e-9037-4edf-94b9-6cc3e03969b9 and I cannot tell if they are relevant or not

It's probably transient. Retry passes.",fine tell relevant probably transient retry,issue,negative,positive,positive,positive,positive,positive
1699764301,"Ah, okay that makes sense. Is there any recommendation on how to proceed from here @jjyao? I've tried del'ing the matrices within eigenproblem but that doesn't help either. The only fix currently is to just chunk up the parameter space but that would quickly become unwieldy as the space becomes larger.",ah sense recommendation proceed tried matrix within help either fix currently chunk parameter space would quickly become unwieldy space becomes,issue,negative,positive,positive,positive,positive,positive
1699756000,"I'll see if I can find time to put that together, currently just using a higher tier on our computing cluster that grants more memory as a workaround. For some context, here's what I asked on the ray slack:

> I’m currently experiencing a memory leak using APPO with the tf2 backend and tune (ray 2.5.1 and 2.6.3). To ensure it’s not my gym environment leaking memory on reset, I’ve added base_env.vector_env.restart_at(0) in the on_episode_end callback, which causes the environment to be deleted and recreated (making training slower relative to the wall clock, but memory leaks at roughly the same rate per step). Running out of memory on a cluster with 3.8gb/worker after 1.5M-5M steps.

Here are the params passed to APPO:
```
  clip_param: 0.2
  entropy_coeff: 0.0
  gamma: 0.999
  grad_clip: 0.5
  lambda_: 0.95
  learner_queue_timeout: 900
  lr: 0.0003
  minibatch_size: 250
  model:
    fcnet_hiddens:
    - 2048
    - 2048
    vf_share_layers: false
  num_sgd_iter: 10
  train_batch_size: 10000
  use_gae: true
  vtrace: false
```

The only other modification I'm making is advantage normalization with a callback:
```
    def on_postprocess_trajectory(...):
        if ""advantages"" in postprocessed_batch:
            adv = postprocessed_batch[""advantages""]
            postprocessed_batch[""advantages""] = (adv - np.mean(adv)) / (
                np.std(adv) + 1e-8
            )
```

",see find time put together currently higher tier cluster memory context ray slack currently memory leak tune ray ensure gym environment memory reset added environment making training relative wall clock memory roughly rate per step running memory cluster gamma model false true false modification making advantage normalization,issue,positive,negative,neutral,neutral,negative,negative
1699733929,@ArturNiederfahrenhorst the training is currently being performed with a CLI call using the cartpole-ppo.yaml config file provided in the tuned_examples directory.  Your comment seems to suggest that I should change over to the Python API to configure and run training.  I should then reproduce the original issue using Python API to drive training with no changes to the config.  Then I would generate results with your proposed config change of `config..experimental(_disable_preprocessor_api=True)` to see if anything changed.  Is this correct?,training currently call file provided directory comment suggest change python configure run training reproduce original issue python drive training would generate change experimental see anything correct,issue,negative,positive,positive,positive,positive,positive
1699709048,"Can you check what happens if you turn of preprocessing for the training? (`config..experimental(_disable_preprocessor_api=True)`)

Since you do...
```
env_name = ""CartPole-v1""
env = gym.make(env_name)
```
The env that you create is not wrapped in anything. It would be helpful if you could play around with this and see if it changes anything.",check turn training experimental since create wrapped anything would helpful could play around see anything,issue,positive,positive,neutral,neutral,positive,positive
1699693767,@rmalone1097 @Mark2000 are you saying you are experiencing memory leaks in general? Can you craft reproduction script that we can copy/paste and run?,mark saying memory general craft reproduction script run,issue,negative,positive,neutral,neutral,positive,positive
1699693195,Thanks @ArturNiederfahrenhorst .  Let me know how I can help.,thanks let know help,issue,positive,positive,positive,positive,positive,positive
1699691237,"Yes, there is a high interest. But we are under high load at the moment. 
We will get to this issue as soon as our prioritization permits it.",yes high interest high load moment get issue soon,issue,positive,positive,positive,positive,positive,positive
1699672404,"Yes, I will add tests when the change is finalized. To ensure consistent state of the session keys with Redis, a few more things have to change (currently in the process of completing these):

1. temp dir, storage, and cached port initialization will move to after the initialization of GCS, but before the other head processes start
2. GCS server port will never be retrieved from cache (though it doesn't look like it ever was going through the retrieval logic anyway), and will be set before starting GCS, and cached after starting GCS but before retrieving/setting the other cached ports.
3. stats initialization in GCS server itself will probably be pushed to later; in particular, the session name and dir keys must be retrieved from Redis before they can be used for initialization. (basically need to check the passed-in value with the persisted one before using) there is also the less clean option of updating post-initialization which could work for session name but not so much for log dir.
4. gcs_table_storage to add some KV functionality (eventually the two should be merged anyway) to facilitate retrieving the keys during Init rather than during Start (so that stats initialization does not have to be pushed down very far).

Most of these have to do with session_name, session_dir, and temp_dir being used both before and after the initial point of persisting to the KV store; now that point has become a sync point with existing values in the KV store, so anything using those values has to be reordered to after",yes add change ensure consistent state session change currently process temp storage port move head start server port never cache though look like ever going retrieval logic anyway set starting starting server probably later particular session name must used basically need check value one also le clean option could work session name much log add functionality eventually two anyway facilitate rather start far used initial point persisting store point become sync point store anything,issue,positive,positive,positive,positive,positive,positive
1699573100,"This is no longer a release blocker since the user requested this is okay to have it in 2.8 

cc @rkooo567 ",longer release blocker since user,issue,negative,neutral,neutral,neutral,neutral,neutral
1699570148,"And did you notice this issue started in Ray 2.6.1, or did you also see it in previous versions?",notice issue ray also see previous,issue,negative,negative,negative,negative,negative,negative
1699568590,"Oh interesting, are you using this workflow for development or production? Generally in production, we recommend folks use a `runtime_env` or Docker container with all the Serve app's code to avoid issues when upgrading code.",oh interesting development production generally production recommend use docker container serve code avoid code,issue,negative,positive,positive,positive,positive,positive
1699565328,"I have a possibly related issue: Simply initializing a dataclass inside of a ray trainable yields a dataclass with empty `fields()` (but otherwise the right attributes).
```
@dataclass
class MainConfig:
    seed: int = 42
```
Inside the ray tune:
```
(train_tune pid=230870) (Pdb) fields(MainConfig())
(train_tune pid=230870) ()
```

So in my case the dataclass breaks before even being serialized. This could also explain the serialization not working.",possibly related issue simply inside ray trainable empty otherwise right class seed inside ray tune case even could also explain serialization working,issue,negative,positive,neutral,neutral,positive,positive
1699564901,"test is passing https://buildkite.com/ray-project/release-tests-pr/builds/51444#018a44d9-27b6-4af8-8b8d-9b362e711fd5.

Going to make `None` the default",test passing going make none default,issue,negative,neutral,neutral,neutral,neutral,neutral
1699548755,"also, if you don't mind, I changed the tittle to make it easier for us to track this issue.",also mind tittle make easier u track issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1699545674,"@max-509 got it. I think the issue is because the support for raw bytes is broken. As a workaround, do you think you can manually convert the bytes data to PyArrow table objects?",got think issue support raw broken think manually convert data table,issue,negative,negative,negative,negative,negative,negative
1699538895,Great. Thanks for taking care of this issue. @rynewang pls link the PR to this issue when ready. ,great thanks taking care issue link issue ready,issue,positive,positive,positive,positive,positive,positive
1699536797,"After a discussion with @maxpumperla I'm marking this as ready for review. We still are doing ad-hoc mocking of compiled ray modules, which still display as `<MagicMock ...>` in the docs, but all of the modules that have been moved into `autodoc_mock_imports` have improved the docs already.

Future effort: https://github.com/sphinx-doc/sphinx/pull/4413 should allow us to put `ray._raylet` inside `autodoc_mock_imports` without mocking `ray` itself - need to investigate this going forward.",discussion marking ready review still ray still display already future effort allow u put inside without ray need investigate going forward,issue,positive,positive,neutral,neutral,positive,positive
1699521521,Merging is blocked on: https://github.com/ray-project/ray/issues/39113 (I've temporarily merged the changes from https://github.com/ray-project/ray/pull/39112 in here to test the fix).,blocked temporarily test fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1699517904,"@xieus running the test on the linked PR reproduces the issue consistently in ~10s.

I spent the whole morning debugging it with @rkooo567 and will work with him on the repro (it's a bit tricky due to it being on a WIP PR and needing a lot of custom logging to understand what's going on).",running test linked issue consistently spent whole morning work bit tricky due needing lot custom logging understand going,issue,negative,negative,neutral,neutral,negative,negative
1699503644,"@edoakes Thanks for bringing this up. When this issue was identified, how long has the long-running test ran? Tring to understand the impact scope. Also, can you provide a repro script or env so that we can follow up? 

cc: @rkooo567 @jjyao  ",thanks issue long test ran understand impact scope also provide script follow,issue,negative,positive,neutral,neutral,positive,positive
1699396670,"The main issue in the example code above is that the `XGBoostTrainer` per default runs 10 boosting rounds (i.e. 11 iterations) but the `max_t` is set to 8.

When setting `max_t=11`, the code runs to completion and as expected.

It's still a bug, as we should never hang forever. I'm planning to fix this by adding this code block to `hypberband.py`

```
                elif bracket.finished() and bracket.stop_last_trials:
                    # Scheduler decides to not continue trial because the bracket
                    # reached max_t. In this case, stop the trials
                    if t.status == Trial.PAUSED or t.is_saving:
                        logger.debug(f""Bracket finished. Stopping other trial {str(t)}"")
                        tune_controller.stop_trial(t)
                    elif t.status == Trial.RUNNING:
                        # See the docstring: There can only be at most one RUNNING
                        # trial, which is the current trial.
                        logger.debug(
                            f""Bracket finished. Stopping current trial {str(t)}""
                        )
                        bracket.cleanup_trial(t)
                        action = TrialScheduler.STOP
```

(in the ""good"" section of process_bracket).

Technically, the problem is that hyperband did all the halvings (because `max_t=8`) and the bracket is finished, but because `stop_last_trials=True` per default, it does not continue the trial when processing the bracket. The bracket is then never processed again, as no more results come in. 

The fix above will stop trials when the bracket is finished and `stop_last_trials=True`.

I'm currently writing test and aim to wrap this up this week.

I do think the Hyperband implementation could use a makeover, so thank you very much for posting your custom ASHA scheduler - we'll definitely take a look! 

",main issue example code per default set setting code completion still bug never forever fix code block continue trial bracket case stop bracket finished stopping trial see one running trial current trial bracket finished stopping current trial action good section technically problem bracket finished per default continue trial bracket bracket never come fix stop bracket finished currently writing test aim wrap week think implementation could use thank much posting custom definitely take look,issue,negative,positive,positive,positive,positive,positive
1699389337,"Hey @jjyao - I understand, thanks for the heads up. @richardliaw is actually announcing this feature today so I am a bit worried about that timeline of post 2.7. I'm wondering if we can push this version through in time for the 2.7 release? But I can definitely help out with the refactor after the design if you need an extra set of hands!",hey understand thanks actually feature today bit worried post wondering push version time release definitely help design need extra set,issue,positive,positive,neutral,neutral,positive,positive
1699356207,"Did a test run and seeing the job succeeding at least https://buildkite.com/ray-project/release-tests-pr/builds/51448#018a4550-a6cc-45ad-9f46-67a2f4b920bd/301-1026 

There might still be some other issues with how the buildkite is ran and throwing `Exited with status -1 (agent lost)`. Rerunning it again to see if that's an intermittent issue with buildkite similar to previous success run https://buildkite.com/ray-project/release-tests-branch/builds/2043#0189efbc-362c-40be-86f3-e0cfd7588415",test run seeing job succeeding least might still ran throwing status agent lost see intermittent issue similar previous success run,issue,positive,negative,neutral,neutral,negative,negative
1699334947,"Hi @krfricke, thanks for taking a look at this. I do want to add, though, that since I made this issue I spent a bit of time working on a new implementation of ASHA that I posted in [this issue](https://github.com/ray-project/ray/issues/32634#issuecomment-1689018637) that more closely matches the original ASHA paper and also better suited my needs. After making that implementation, I believe that the issue that causes the current HyperBand and ASHA implementations to not work in the colab notebook I shared is that these implementations seem to only make sense when there are enough resources to start many (if not all) jobs in parallel, which is not possible with only 1 CPU in colab. ",hi thanks taking look want add though since made issue spent bit time working new implementation posted issue closely original paper also better need making implementation believe issue current work notebook seem make sense enough start many parallel possible,issue,positive,positive,positive,positive,positive,positive
1699282697,"Confirmed that this is an issue, looking into it today",confirmed issue looking today,issue,negative,positive,positive,positive,positive,positive
1699251163,"In the colab, after installing Ray nightly, it also looks like it's working correctly for me.

With `num_boost_round=20` and `checkpoint_freq=2`:

```
!find /root/ray_results/xgboost_breast_cancer/ -maxdepth 1 -name 'XGBoost*' | xargs -I {} sh -c ""ls -l {} | grep checkpoint | wc -l""
10
10
10
10
```

latest nightly installed with:

```
!pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-3.0.0.dev0-cp310-cp310-manylinux2014_x86_64.whl
```

So in short it looks like behavior is correct. The `trainer._ray_params.checkpoint_frequency` may be set differently, but that only effects the internal xgboost-ray checkpointing, not the XGBoostTrainer (Ray Tune) checkpointing.
",ray nightly also like working correctly find sh latest nightly pip install short like behavior correct may set differently effect internal ray tune,issue,positive,positive,positive,positive,positive,positive
1699234086,"I ran an example script on latest master and it seems to be working correctly for me:

```
from typing import Tuple

import ray
from ray import tune
from ray.data import Dataset
from ray.train.xgboost import XGBoostTrainer
from ray.train import Result, ScalingConfig, RunConfig, CheckpointConfig


def prepare_data() -> Tuple[Dataset, Dataset]:
    dataset = ray.data.read_csv(""s3://anonymous@air-example-data/breast_cancer.csv"")
    train_dataset, valid_dataset = dataset.train_test_split(test_size=0.3)
    return train_dataset, valid_dataset


def train_xgboost(num_workers: int, use_gpu: bool = False) -> Result:
    train_dataset, valid_dataset = prepare_data()

    # XGBoost specific params
    params = {
        ""tree_method"": ""approx"",
        ""objective"": ""binary:logistic"",
        ""eval_metric"": [""logloss"", ""error""],
    }

    trainer = XGBoostTrainer(
        scaling_config=ScalingConfig(num_workers=num_workers, use_gpu=use_gpu),
        run_config=RunConfig(checkpoint_config=CheckpointConfig(checkpoint_frequency=2)),
        label_column=""target"",
        params=params,
        datasets={""train"": train_dataset, ""valid"": valid_dataset},
        num_boost_round=100,
    )
    tuner = tune.Tuner(trainer, )
    result = tuner.fit()[0]
    print(result.metrics)

    return result


ray.init(num_cpus=3)
result = train_xgboost(num_workers=2, use_gpu=False)
```

Example with `checkpoint_frequency=1`:

```
ls -al /Users/kai/ray_results/XGBoostTrainer_2023-08-30_15-46-11/XGBoostTrainer_94b51_00000_0_2023-08-30_15-46-12 | grep checkpoint | wc -l
     101
```

Example with `checkpoint_frequency=2`:

```
ls -al /Users/kai/ray_results/XGBoostTrainer_2023-08-30_15-50-27/XGBoostTrainer_2cd94_00000_0_2023-08-30_15-50-27/ | grep checkpoint | wc -l
      50
```



",ran example script latest master working correctly import import ray ray import tune import import import result return bool false result specific objective binary logistic error trainer target train valid tuner trainer result print return result result example example,issue,negative,positive,neutral,neutral,positive,positive
1699219009,"Thanks for your investigation, I'm taking a look today.

I agree it's confusing that we have several different checkpoint frequencies. Fwiw, the internal saving callback is used for the XGBoost-Ray-internal fault tolerance, and not for reporting to Ray Tune.

The confusion comes from the fact that XGBoost-Ray was implemented before Ray Train was a thing. We'll likely migrate it into Ray Train at some point, but until then we obviously want to make sure it works as expected.

",thanks investigation taking look today agree several different internal saving used fault tolerance ray tune confusion come fact ray train thing likely migrate ray train point obviously want make sure work,issue,positive,positive,neutral,neutral,positive,positive
1699111411,"The root cause was that the lineage reconstruction is not triggered when the streaming ref generator is out of scope.

We can fix it by keeping the generator stream until the lineage is released. However, this approach requires some work, and it is safer merging this in 2.8 than cherry picking it for 2.7. Given the ray data is the only library that uses the lineage reconstruction, and there's the workaround in data layer (not GC streaming generator until the DAG is finished), it is for now safe to work around the problem in data layer. We will mark the issue as P0 + ray 2.8 after we close the PR ^",root cause lineage reconstruction triggered streaming ref generator scope fix keeping generator stream lineage however approach work cherry given ray data library lineage reconstruction data layer streaming generator dag finished safe work around problem data layer mark issue ray close,issue,negative,positive,positive,positive,positive,positive
1698959059,"It's still a valid issue on Ray 2.6.3 (I'm using `kuberay-operator-0.5.0` Helm Chart on GKE). Just a basic setup as show in the quick start guide with port-forward and the code (run in a notebook):
```
import ray
ray.init(""127.0.0.1:8266"")
```

fails with:
```
2023-08-30 13:01:44,006	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 127.0.0.1:8266...
2023-08-30 13:01:49,650	ERROR utils.py:1395 -- Failed to connect to GCS. Please check `gcs_server.out` for more details.
2023-08-30 13:01:49,651	WARNING utils.py:1401 -- Unable to connect to GCS (ray head) at 127.0.0.1:8266. Check that (1) Ray with matching version started successfully at the specified address, (2) this node can reach the specified address, and (3) there is no firewall setting preventing access.
```

@jjyao - could you provide some update on this?",still valid issue ray helm chart basic setup show quick start guide code run notebook import ray ray cluster address error connect please check warning unable connect ray head check ray matching version successfully address node reach address setting access could provide update,issue,negative,positive,positive,positive,positive,positive
1698843454,"@larrylian Yes. init tmp dir logic needs to move to after `_write_cluster_info_to_kv`, once the proper values have been fetched from GCS.

There is also the issue of session_name being used in initialization of stats collection in `gcs_server_main` before the kv client has been initialized (so old session_name cannot be fetched). It's not clear that we can move the initialization of the stats object to a later point, though if we can that would be the easiest fix. 

Resetting session_name later (once it has been fetched from the kv store) is not super ideal since the StatsConfig object is not thread-safe and the tags in particular are quite read-heavy, so we probably wouldn't want to serialize on a mutex.",yes logic need move proper fetched also issue used collection client old fetched clear move object later point though would easiest fix later fetched store super ideal since object particular quite probably would want serialize,issue,positive,positive,positive,positive,positive,positive
1698765507,"Hi @f2010126,

I think there may be a misunderstanding of terms here. Let me clarify.

`LightningTrainer` is used for distributed training. The `ScalingConfig` only configures each `LightningTrainer`. Thus, a `worker` is the number of distributed workers _each LightningTrainer_ uses.

A _trial_ is a Ray Tune trial. If you run 10 trials, you'll run 10 LightningTrainers, each occupying the resources specified in the `ScalingConfig`.

If you use 

```
scaling_config = ScalingConfig(
        num_workers=2, use_gpu=use_gpu, resources_per_worker={""CPU"": 2, ""GPU"": 2}
    )
```

this means, _each trial_ will start 2 workers, and each worker will occupy 2 CPUs and 2 GPUs. Thus, if your cluster has 4 GPUs, exactly one trial can run at the same time.

If you use e.g.

```
scaling_config = ScalingConfig(
        num_workers=2, use_gpu=use_gpu, resources_per_worker={""CPU"": 1, ""GPU"": 1}
    )
```

this would mean that each trial will still start 2 workers, but each worker will only occupy 1 CPU and 1 GPU. In the same cluster this means that 2 trials can run at the same time.

Note that the LightningTrainer itself also occupies 1 CPU. So if your nodes have exactly 2 CPUs and 2 GPUs, you should consider passing `trainer_resources={""CPU"": 0}` to the scaling config.

Multi node + multi GPU training is one of the core use cases for Ray Train and Ray Tune :-) It's mostly a matter of configuration.







",hi think may misunderstanding let clarify used distributed training thus worker number distributed ray tune trial run run use start worker occupy thus cluster exactly one trial run time use would mean trial still start worker occupy cluster run time note also exactly consider passing scaling node training one core use ray train ray tune mostly matter configuration,issue,negative,positive,positive,positive,positive,positive
1698518244,"Tests passing: https://buildkite.com/ray-project/oss-ci-build-pr/builds/34302#018a43a2-db8f-4fcb-bd6d-089960e495e6

Ok to merge. (Just needed to resolve merge conflict)",passing merge resolve merge conflict,issue,negative,neutral,neutral,neutral,neutral,neutral
1698430512,"If this is not a release-blocker, can we mark it as unstable and cherry pick the PR. It'll keep the release manager less confused. Thankks",mark unstable cherry pick keep release manager le confused,issue,negative,negative,negative,negative,negative,negative
1698418686,"> Oh I see, so is your workflow:
> 
> 1. Start Ray with `ray start --head` in a particular mount point on NFS, which contains all the necessary code for your app.
> 2. Start your Serve app. This app works.
> 3. Kill Serve and restart it. This app doesn't have the code changes.
> 
> One issue I see with that workflow is that I don't think the Ray cluster will automatically update with any code changes you make in the mount point. You would need to restart the Ray cluster altogether. One way to do this is to not start a long-lived Ray cluster, and instead use `serve run` without `ray start --head`.

Sorry for the unclarity, but the 3rd step doesn't comply. I made changes to the code and then just ran `serve deploy config.yaml` (without killing Serve). It results that code changes weren't detected and the replica was not updated  (see https://github.com/ray-project/ray/issues/38714#issuecomment-1691303260).
In fact, killing & restarting Serve is my current workaround to enforce deployment updating, which proves that restarting the cluster is not necessary.",oh see start ray ray start head particular mount point necessary code start serve work kill serve restart code one issue see think ray cluster automatically update code make mount point would need restart ray cluster altogether one way start ray cluster instead use serve run without ray start head sorry unclarity step comply made code ran serve deploy without killing serve code replica see fact killing serve current enforce deployment cluster necessary,issue,negative,negative,neutral,neutral,negative,negative
1698384024,"> @scv119 For the CI , since there is no runtime XPU device yet, is there a way to skip tests (for time being) for the XPU side? Could you please review in that case? I see that the TPU integration is merged. Also in order to fully extend CI for XPUs does ray have any plans to use/support a device (XPU) which Intel might be able to provision (after discussions with Ray and Intel)? This would help us to plan our developments better.

@scv119 Could we know how NVIDIA GPU is tested right now? Do you have a real GPU hardware for CI?",since device yet way skip time side could please review case see integration also order fully extend ray device might able provision ray would help u plan better could know tested right real hardware,issue,positive,positive,positive,positive,positive,positive
1698359249,"> Hi @abhilash1910 and @harborn. Thanks for applying the feedback regarding cluster heterogeneity. Right now the Ray team is in the critical stretch before Ray Summit so our bandwidth for shepherding PRs is lower. I expect us to be able to pick this back up after Ray Summit.

This PR has been reviewed for more than 2 months, and I have re-implemented the function, can you speed it up? I understand that the current implementation of XPU support should not conflict with the future unified accelerator plan.",hi thanks feedback regarding cluster heterogeneity right ray team critical stretch ray summit lower expect u able pick back ray summit function speed understand current implementation support conflict future unified accelerator plan,issue,negative,positive,positive,positive,positive,positive
1698343440,"cc @scv119 we just discovered this issue, and actually the fix PR is already out. ",discovered issue actually fix already,issue,negative,neutral,neutral,neutral,neutral,neutral
1698342798,"unless we are also migrating other flavors of data tests together, we would need to maintain civ1 to be passing.

like all these are failing now on:

https://buildkite.com/ray-project/oss-ci-build-pr/builds/34197#018a40c8-d1ad-4ad1-83d7-273b71b437d4/1922-2180",unless also data together would need maintain passing like failing,issue,negative,neutral,neutral,neutral,neutral,neutral
1698335478,"Okay, the RTD build is now working - I had to make sure _not_ to import ray in `conf.py` because you can't `autodoc_mock_imports` on anything that is already initialized. The build now works!",build working make sure import ray ca anything already build work,issue,negative,positive,positive,positive,positive,positive
1698275340,@scv119 @rickyyx Can you help taking a look at this as you have previously reviewed the release log and/ or on call for core? ,help taking look previously release log call core,issue,negative,negative,negative,negative,negative,negative
1698259746,TODO: skip the windows test for the new rt env agent tests,skip test new agent,issue,negative,positive,positive,positive,positive,positive
1698256303,This PR fixes a bug about data fault tolerance,bug data fault tolerance,issue,negative,neutral,neutral,neutral,neutral,neutral
1698211252,"@TracebaK Just FYI we decided to revive the `serve.start` api https://github.com/ray-project/ray/pull/38362 So feel free to continue use it to start Serve going forward 🙂

Also note, `serve.run` is intended to use to for deploy apps and not to intended to start serve. Proxy related options will likely to be ineffective in a few release on `serve.run`. ",decided revive feel free continue use start serve going forward also note intended use deploy intended start serve proxy related likely ineffective release,issue,positive,positive,positive,positive,positive,positive
1698190180,"+1.  Also, if I click on ""Deploy Ray Serve in production"", the right hand TOC looks as if the bottom link got clicked: 
<img width=""303"" alt=""Screenshot 2023-08-29 at 2 48 12 PM"" src=""https://github.com/ray-project/ray/assets/5459654/e08f4bcd-675b-47c8-ac36-6412f54d99fe"">
",also click deploy ray serve production right hand bottom link got,issue,negative,positive,positive,positive,positive,positive
1698190111,test passed for last night's build https://buildkite.com/ray-project/release-tests-branch/builds/2091#018a3f4b-2574-4389-a3a4-710474554909,test last night build,issue,negative,neutral,neutral,neutral,neutral,neutral
1698085954,@c21 can you confirm that this is a release blocker? Thanks,confirm release blocker thanks,issue,negative,positive,positive,positive,positive,positive
1698053078,"Somehow CI keeps getting cancelled. To fix master asap, I want to merge this asap. I will monitor the post submits closely.",somehow getting fix master want merge monitor post closely,issue,negative,neutral,neutral,neutral,neutral,neutral
1698008904,Hi @abhilash1910 and @harborn. Thanks for applying the feedback regarding cluster heterogeneity. Right now the Ray team is in the critical stretch before Ray Summit so our bandwidth for shepherding PRs is lower. I expect us to be able to pick this back up after Ray Summit.,hi thanks feedback regarding cluster heterogeneity right ray team critical stretch ray summit lower expect u able pick back ray summit,issue,negative,positive,positive,positive,positive,positive
1697964471,"Reproducible, this time I got

```
Running: 54.0/54.0 CPU, 4.0/4.0 GPU, 769.36 MiB/4.01 GiB object_store_memory:   0%|          | 0/200 [00:04<?, ?it/s]
Running: 52.0/54.0 CPU, 4.0/4.0 GPU, 780.87 MiB/4.01 GiB object_store_memory:   0%|          | 0/200 [00:04<?, ?it/s]
Running: 52.0/54.0 CPU, 4.0/4.0 GPU, 799.0 MiB/4.01 GiB object_store_memory:   0%|          | 0/200 [00:05<?, ?it/s] 
Running: 52.0/54.0 CPU, 4.0/4.0 GPU, 799.0 MiB/4.01 GiB object_store_memory:   0%|          | 1/200 [00:05<16:39,  5.02s/it]
Running: 54.0/54.0 CPU, 4.0/4.0 GPU, 782.99 MiB/4.01 GiB object_store_memory:   0%|          | 1/200 [00:05<16:39,  5.02s/it]
Running: 54.0/54.0 CPU, 4.0/4.0 GPU, 782.99 MiB/4.01 GiB object_store_memory: 100%|██████████| 1/1 [00:05<00:00,  5.02s/it]  
```

This time I printed out more sample predictions and saw the same memory address for two images which are definitely different:
```
<PIL.Image.Image image mode=RGB size=153x202 at 0x795AE0904910>
Label:  coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch
<PIL.Image.Image image mode=RGB size=470x329 at 0x795AE0904910>
Label:  tench, Tinca tinca
```

so I wonder if the memory location must be the location of the batch or something, not the location of the image.",reproducible time got running gib running gib running gib running gib running gib running gib time printed sample saw memory address two definitely different image label coho coho salmon blue jack silver salmon image label tench wonder memory location must location batch something location image,issue,negative,neutral,neutral,neutral,neutral,neutral
1697892966,"@kevin85421 comments addressed, please take another look!",please take another look,issue,negative,neutral,neutral,neutral,neutral,neutral
1697889385,"It seems that ""BackendExecutor(max_retries)"" is a duplicate function with ""FailureConfig"". After deprecating this, we rely on TuneController to control retries, right?",duplicate function rely control right,issue,negative,positive,positive,positive,positive,positive
1697839354,@xwjiang2010 Premerge is failing due to a linting issue https://buildkite.com/ray-project/premerge/builds/3943#018a4241-4bd4-4f2f-947a-02fd1ee16a80/187-427 Can you fix it so we can get this in?,failing due issue fix get,issue,negative,negative,negative,negative,negative,negative
1697829819,"Oh I see, so is your workflow:

1. Start Ray with `ray start --head` in a particular mount point on NFS, which contains all the necessary code for your app.
2. Start your Serve app. This app works.
3. Kill Serve and restart it. This app doesn't have the code changes.

One issue I see with that workflow is that I don't think the Ray cluster will automatically update with any code changes you make in the mount point. You would need to restart the Ray cluster altogether. One way to do this is to not start a long-lived Ray cluster, and instead use `serve run` without `ray start --head`.",oh see start ray ray start head particular mount point necessary code start serve work kill serve restart code one issue see think ray cluster automatically update code make mount point would need restart ray cluster altogether one way start ray cluster instead use serve run without ray start head,issue,negative,positive,neutral,neutral,positive,positive
1697812270,"The test failures are unrelated:

* [`Linkcheck`](https://buildkite.com/ray-project/oss-ci-build-pr/builds/34040#018a3d52-c0a6-4d45-bc54-b377b0687df1/6-4720) is failing on `master`:

<img width=""1345"" alt=""Screen Shot 2023-08-29 at 9 50 55 AM"" src=""https://github.com/ray-project/ray/assets/92341594/790f8ec1-c830-47a1-ab57-e706a60cf2a0"">
",test unrelated failing master screen shot,issue,negative,neutral,neutral,neutral,neutral,neutral
1697811487,"I also see that this was updated just a week ago on nightly: https://github.com/ray-project/ray/commit/424126302c647735e7d26989f886e5cbe54c394e

I tried installing ray from the nightly wheel in colab and testing, and am getting the same result: when checkpoint_frequency is set in the Tuner, the callback is made with a frequency of 0.",also see week ago nightly tried ray nightly wheel testing getting result set tuner made frequency,issue,negative,neutral,neutral,neutral,neutral,neutral
1697783975,"@krfricke I found a further problem with the current handling of checkpoint_frequency. As you mentioned, there is code currently in the training loop [here](https://github.com/ray-project/ray/blob/407e2f29a740c0a33b386e6afc3025ea8dbc3264/python/ray/train/gbdt_trainer.py#L304) that adds a checkpoint based on the checkpoint frequency. However, if the CheckpointConfig is based to the RunConfig in a Tuner, instead of the trainer, then the checkpoint frequency is actually set to 0 [here](https://github.com/ray-project/ray/blob/407e2f29a740c0a33b386e6afc3025ea8dbc3264/python/ray/tune/impl/tuner_internal.py#L665), and the callback is not created correctly.",found problem current handling code currently training loop based frequency however based tuner instead trainer frequency actually set correctly,issue,negative,neutral,neutral,neutral,neutral,neutral
1697750267,"I am trying to connect to my ray-cluster with prefect, which means I will need to use the client.

Does your response mean that it is not possible to connect to the client through an ingress?",trying connect prefect need use client response mean possible connect client ingres,issue,negative,negative,negative,negative,negative,negative
1697642084,"> LGTM, just one question: Did we change/fix the RLlib docs occurrences, where `algo.save()` is used (and assumed to return a path as a str)?

Good point. I've just taken a look and there's at least one occurrence where we haven't. Will update now!",one question used assumed return path good point taken look least one occurrence update,issue,negative,positive,positive,positive,positive,positive
1697621940,"We have tried a few different things including starting it first. This was an attempt to connect to a remote cluster like in [remote client docs](https://docs.ray.io/en/latest/cluster/running-applications/job-submission/ray-client.html).
Here are some more details:
We always start the head node with `ray start --head`

If we just run the script we get the above error.

If we change the script address to `ray://<head-ip>:6379` and run we get: `ConnectionError: ray client connection timeout`

If we first start ray on the client node  with `ray start --address=<head-ip>:6379 ` and change the script address to `auto` the script **hangs forever**

If we first start ray on the client node  with `ray start --address=<head-ip>:6379 --num-cpus=10` and change the script address to `auto` the script works then.

Perhaps ray has a bug when the client machine has a high number of CPUs? Our machines each have ~250 cpu cores
",tried different starting first attempt connect remote cluster like remote client always start head node ray start head run script get error change script address ray run get ray client connection first start ray client node ray start change script address auto script forever first start ray client node ray start change script address auto script work perhaps ray bug client machine high number,issue,negative,positive,neutral,neutral,positive,positive
1697606289,Please wait with picking this. Tests seem to break. I need to take another look. Thanks!,please wait seem break need take another look thanks,issue,positive,positive,positive,positive,positive,positive
1697425387,I said that the examples I found was not all of the examples of code that needed to be removed in 2.6,said found code removed,issue,negative,neutral,neutral,neutral,neutral,neutral
1697305759,The task information is not working with GCS HA now (basically all task data is stored in memory). It is actually very tricky to fix it because task information adds a lot of pressure to the storage (Redis) if we start persisting them in Redis storage. ,task information working ha basically task data memory actually tricky fix task information lot pressure storage start persisting storage,issue,negative,neutral,neutral,neutral,neutral,neutral
1697300732,Synced offline. We cannot revert this PR (it is very critical to revert for windows failure from autoscaler). @rickyyx is investigating it since it could be a real bug.,revert critical revert failure investigating since could real bug,issue,negative,negative,neutral,neutral,negative,negative
1697298901,"I think this behavior is actually removed. We can probably close it. 

Imo the right behavior is to retry with some backoff + reset the failure count when it succeeds again. ",think behavior actually removed probably close right behavior retry reset failure count,issue,negative,negative,neutral,neutral,negative,negative
1697262318,"> @JalinWang It looks like your config doesn't use a `runtime_env`. How are you making your code available to the Ray cluster?

@shrekris-anyscale  Thanks for the reply! Actually, I work on an NFS with the same mount point on all nodes, where the ray cluster is started :smile: ",like use making code available ray cluster thanks reply actually work mount point ray cluster smile,issue,positive,positive,positive,positive,positive,positive
1697129800,"1. I think this issue need a more detailed design is required.  

a. self._session_name is generated in first.
b. Then init tmp dir by `self._session_name` in _init_temp(self).
c. Then start gcs server
d.  If not overwirte ""session_name""  in `self._write_cluster_info_to_kv` ,  it may result in inconsistency between 'tmp_dir' and the 'session_name' in Redis, potentially causing certain issues.

```
None.__init__():
...
 if head:
        # date including microsecond
        date_str = datetime.datetime.today().strftime(""%Y-%m-%d_%H-%M-%S_%f"")
        self._session_name = f""session_{date_str}_{os.getpid()}""

....
self._init_temp()
...

self.start_head_processes()
        self.start_gcs_server()
        assert self.get_gcs_client() is not None
        self._write_cluster_info_to_kv()
```


2. We should update self._session_name var and others keys in python , if these keys are persisted.   These variables may be used in the subsequent process.

3. We should print log , if these keys are persisted and resue them.",think issue need detailed design first self start server may result inconsistency potentially causing certain head date microsecond assert none update python may used subsequent process print log resue,issue,negative,positive,positive,positive,positive,positive
1696923222,"@peytondmurray this looks great, thanks so much! we should merge this asap, once we removed the rtd build issues.",great thanks much merge removed build,issue,positive,positive,positive,positive,positive,positive
1696913644,We decided to make the API public in Ray 2.8 instead (we will still update the documentation for 2.7 as an alpha feature),decided make public ray instead still update documentation alpha feature,issue,negative,neutral,neutral,neutral,neutral,neutral
1696912302,Please rewrite the PR title & description!,please rewrite title description,issue,negative,neutral,neutral,neutral,neutral,neutral
1696835758,This issue only occurs when we go down this code path: https://github.com/ray-project/ray/blob/47c84c6dc4a1ac4f3a79834593c10117f1c4bec4/python/ray/data/datasource/file_meta_provider.py#L403-L415,issue go code path,issue,negative,neutral,neutral,neutral,neutral,neutral
1696814039,"[Close to real use case example](https://github.com/ray-project/ray/files/12460729/error_demo.zip)
",close real use case example,issue,negative,positive,positive,positive,positive,positive
1696767892,"> This is used to detect idleness of the cluster

oh interesting - do you know the caller? we should probably use the idleness info now available in the GCS for this. ",used detect idleness cluster oh interesting know caller probably use idleness available,issue,negative,positive,positive,positive,positive,positive
1696745153,"> shuffle_100gb.aws, that runs pretty fast

this test still needs to run for like 30+ minutes.. ",pretty fast test still need run like,issue,positive,positive,positive,positive,positive,positive
1696727448,"Ah that's right - can I take this PR over and do this sequentially? There are quite a few picks for Train, I can coordinate picking them all in order of when they were merged to master.",ah right take sequentially quite train order master,issue,negative,positive,positive,positive,positive,positive
1696723489,"```

File ""<doctest data-loading-preprocessing.rst[6]>"", line 43, in <module>
  | AttributeError: 'StandardScaler' object has no attribute 'serialize'
```

@matthewdeng I think you need to pick your PR first",file line module object attribute think need pick first,issue,negative,positive,positive,positive,positive,positive
1696708841,"Hi!
I’m getting the same error on an HPC SLURM cluster:
```
global_state_accessor.cc:356: This node has an IP address of 172.16.204.7, but we cannot find a local Raylet with the same address. This can happen when you connect to the Ray cluster with a different IP address or when connecting to a container.
```
However, the experiments start running after this error and finish. So can this error be safely ignored?",hi getting error cluster node address find local raylet address happen connect ray cluster different address container however start running error finish error safely,issue,negative,positive,positive,positive,positive,positive
1696708501,"https://buildkite.com/ray-project/premerge/builds/3877#018a3f41-e689-4da5-9c52-833c24c46aa4

test still passing. so I think it is good.",test still passing think good,issue,negative,positive,positive,positive,positive,positive
1696708451,"I met this problem when I converted Spark Dataframe to Ray Dataset.  Dataframe was created using raydp spark cluster. After I started training process with TorchTrainer. 
I used `ray.data.from_spark()` for conversation. ",met problem converted spark ray spark cluster training process used conversation,issue,negative,neutral,neutral,neutral,neutral,neutral
1696704635,"I think after the `_find_job_driver_and_ray_error_logs`, all the logs are never read again, so they are safe to delete (wrap the dir with `with`)",think never read safe delete wrap,issue,negative,positive,positive,positive,positive,positive
1696703729,"> Maybe run a release test

could you help me run it?",maybe run release test could help run,issue,negative,neutral,neutral,neutral,neutral,neutral
1696699327,"> hi @yiwei00000, ` ray.data.read_csv` is a lazy operation, it won't read data into memory, until `ds.max`. And the second `ds.max` will read data from the source again. If you want to cache data in memory, add `ds = ds.materialize()` after `read_csv`.



> hi @yiwei00000, ` ray.data.read_csv` is a lazy operation, it won't read data into memory, until `ds.max`. And the second `ds.max` will read data from the source again. If you want to cache data in memory, add `ds = ds.materialize()` after `read_csv`.

Thank you for your help. It's very useful to me

",hi lazy operation wo read data memory second read data source want cache data memory add hi lazy operation wo read data memory second read data source want cache data memory add thank help useful,issue,positive,negative,neutral,neutral,negative,negative
1696689593,This is used to detect idleness of the cluster,used detect idleness cluster,issue,negative,neutral,neutral,neutral,neutral,neutral
1696687426,"Hi, does this issue solved? I tried @justinvyu suggestions to save the model using `TorchCheckpoint.from_dict(...)` or `TorchCheckpoint.from_state_dict(model.state_dict())` result in the same error as well. ",hi issue tried save model result error well,issue,negative,neutral,neutral,neutral,neutral,neutral
1696687059,This will still have the wrong session id right? the node.py generates the session id,still wrong session id right session id,issue,negative,negative,negative,negative,negative,negative
1696685245,Thank you! This is very important task to support multi cloud! ,thank important task support cloud,issue,positive,positive,positive,positive,positive,positive
1696629662,"> LGTM. But do you know why there are so many RPC requets for this?

I think we are periodically checking for job status when dashboard is open? ",know many think periodically job status dashboard open,issue,negative,positive,positive,positive,positive,positive
1696605303,Only failing test seems to be an unrelated lightning trainer test.,failing test unrelated lightning trainer test,issue,negative,neutral,neutral,neutral,neutral,neutral
1696592063,Closing at the request of @simran-2797. Any additional missing examples will be reported in a future issue.,request additional missing future issue,issue,negative,negative,neutral,neutral,negative,negative
1696588229,"Hi @allenwang28,

Thanks for the contribution. Given multiple teams are trying to add support of different accelerators to Ray. We'd like to come up with a design first to see how we should support them in a unified way and the task is on me (#38504). Do you mind waiting until the design is done? I'll start to work on it in 2-3 weeks after finishing the Ray 2.7 release. Does this timeline work for you?",hi thanks contribution given multiple trying add support different ray like come design first see support unified way task mind waiting design done start work finishing ray release work,issue,positive,positive,positive,positive,positive,positive
1696587105,"Moved the sync point back to where it was and fixed a bug in the retry logic. It's now 1.97 seconds, which is around where it was originally. Note that if we move the sync point to later in time as the earlier revision did, we can further optimize to ~1.05 at the expense of brittleness. ",sync point back fixed bug retry logic around originally note move sync point later time revision optimize expense brittleness,issue,negative,positive,positive,positive,positive,positive
1696576930,"> do you have a link to run in concern (rather than the screenshot)?

**Status Quo**
Oh yes, this is one of the current run that times out only at module level:  https://buildkite.com/ray-project/oss-ci-build-pr/builds/34044#018a3d71-843f-46d7-8a5a-f37ddb52e2dc

See the timestamp (15min = 21:45:52 -> 22:00:09) 
![image](https://github.com/ray-project/ray/assets/11676094/d28f45ea-1ccb-44bd-b1e0-e5da72acc75e)


**With this PR** 
A timeout of a single pytest will be raised with traceback of threads so one could actually know where the test driver is stuck at, see https://buildkite.com/ray-project/oss-ci-build-pr/builds/33844#018a2efb-e4f6-4fd6-b102-92f13dcbc8b8

![image](https://github.com/ray-project/ray/assets/11676094/12eb7ade-d1e3-4fec-acc6-2729df05222d)







------



> I think timeout limiting should be done in bazel `--test_timeout` flag rather than in pytest, and one should adjust the timeout or size attribute of a bazel pytest rule to provide different timeout values for different tests.

I think the reason to enforce a per pytest timeout is that a signle python test module might contain multiple pytest test functions, and the bazel flag set the timeout for the entire module only. I feel setting timeout on individual pytest is also needed since this provides isolation of individual tests within the same module: there's no point waiting for a single pytest to eat up the entire module timeout time since a module could contain quite a number of pytest tests. 


------

> I do not think this affects bazel tests?

This has an impact: see this: https://buildkite.com/ray-project/oss-ci-build-pr/builds/33844#018a2eea-3261-47ac-9ba3-e32d7f3d9ba2


",link run concern rather status quo oh yes one current run time module level see min image single raised one could actually know test driver stuck see image think limiting done flag rather one adjust size attribute rule provide different different think reason enforce per python test module might contain multiple test flag set entire module feel setting individual also since isolation individual within module point waiting single eat entire module time since module could contain quite number think impact see,issue,negative,negative,neutral,neutral,negative,negative
1696562600,"> @chaowanggg Thanks for the work. Let's fill in necessary info on the PR, especially on why this is critical for Ray 2.7. Thanks!

Thanks for the remind. Done!",thanks work let fill necessary especially critical ray thanks thanks remind done,issue,positive,positive,positive,positive,positive,positive
1696555339,"> Closing this because of the new RLModules API does not have this problem. User can decide to not use a preprocessor and all forward methods of the RL Module will receive unflattened data.

How is it done in RL Module?",new problem user decide use forward module receive unflattened data done module,issue,negative,positive,positive,positive,positive,positive
1696540849,"@chaowanggg Thanks for the work. Let's fill in necessary info on the PR, especially on why this is critical for Ray 2.7. Thanks!",thanks work let fill necessary especially critical ray thanks,issue,negative,positive,neutral,neutral,positive,positive
1696525611,"Today's master is passing https://buildkite.com/ray-project/release-tests-branch/builds/2084#_.
I guess it should be unrelated to this test. closing this issue. Will keep eyes on it. ",today master passing guess unrelated test issue keep,issue,negative,neutral,neutral,neutral,neutral,neutral
1696504974,@rkooo567 sorry I forgot this item :(  I'll sync with @vitsai today to get a clear timeline.,sorry forgot item sync today get clear,issue,negative,negative,negative,negative,negative,negative
1696495563,"What's the progress of this task? There's one issue that's blocked by this, so I'd like to understand the timeline",progress task one issue blocked like understand,issue,negative,neutral,neutral,neutral,neutral,neutral
1696490540,Reassgining to @vitsai as part of the idle node project. ,part idle node project,issue,negative,neutral,neutral,neutral,neutral,neutral
1696486506,Can we cherrypick this one? @angelinalg  I may have another doc fix up soon.,one may another doc fix soon,issue,negative,neutral,neutral,neutral,neutral,neutral
1696483800,"Assigning @scv119 as ray core codeowner, and someone who reviewed the AWS neuron core PR",ray core someone neuron core,issue,negative,neutral,neutral,neutral,neutral,neutral
1696483717,"Seems that the `max_actor_restarts` is a xgboost_ray exclusive feature. Can you explain what's your use case and why `FailureConfig` doesn't fit your use case?

@krfricke Can you take a look? ",exclusive feature explain use case fit use case take look,issue,negative,positive,positive,positive,positive,positive
1696452411,"Seems you are using ray client which needs to transfer the large object to the remote cluster for `ray.put`.

Can you avoid using Ray client (it's not recommended to use) and use Ray jobs (https://docs.ray.io/en/releases-2.6.1/cluster/running-applications/job-submission/index.html) instead?",ray client need transfer large object remote cluster avoid ray client use use ray instead,issue,negative,positive,neutral,neutral,positive,positive
1696446902,"From your script, seems the memory is occupied by Ray objects and Ray objects will only be freed when there is no reference to it. In your case, `output_ids` holds references to all Ray objects so they cannot be freed even if you manually trigger Python GC.",script memory ray ray freed reference case ray freed even manually trigger python,issue,positive,neutral,neutral,neutral,neutral,neutral
1696442207,"```
(raylet) [2023-08-22 14:38:50,704 E 22974 2742920] (raylet) logging.cc:97: Unhandled exception: N5boost10wrapexceptINS_6system12system_errorEEE. what(): pipe_select_interrupter: Too many open files [system:24]
```

Can you increase the limit via `ulimit`?",raylet raylet unhandled exception many open system increase limit via,issue,negative,positive,positive,positive,positive,positive
1696439220,"> On the client node I didnt see raylet.out there.

The client node needs to be the Ray head node or Ray worker node, but seems your client node doesn't have Ray running. Did you run `ray start` on the client node?",client node didnt see client node need ray head node ray worker node client node ray running run ray start client node,issue,negative,neutral,neutral,neutral,neutral,neutral
1696434381,"@TheisFerre,

Is it possible for you to use Ray job submission (https://docs.ray.io/en/releases-2.6.1/cluster/running-applications/job-submission/index.html) instead of Ray client (we don't recommend using ray client anymore)",possible use ray job submission instead ray client recommend ray client,issue,negative,neutral,neutral,neutral,neutral,neutral
1696428603,@alanwguo could you reply? I know you are working on pydantic version related things.,could reply know working version related,issue,negative,neutral,neutral,neutral,neutral,neutral
1696422212,@fostiropoulos How did you run pylint with the above script. What's your full command?,run script full command,issue,negative,positive,positive,positive,positive,positive
1696410776,Time for `ray start --head` has gone back down from 5.9s to 1.05s,time ray start head gone back,issue,negative,neutral,neutral,neutral,neutral,neutral
1696406034,"> Thanks for the review, all comments have been addressed. In a followup PR I'll update the GPU-GKE doc (update it to remove the manual taints/tolerations, which are automatically handled by GKE) and make this tutorial link to that doc.

@kevin85421 Actually, I went ahead and made the update in this PR, so no need for a followup.",thanks review update doc update remove manual automatically handled make tutorial link doc actually went ahead made update need,issue,negative,positive,neutral,neutral,positive,positive
1696369071,What's the before and after time for `ray start --head`?,time ray start head,issue,negative,neutral,neutral,neutral,neutral,neutral
1696353122,Verified: this workspace is using an older version of Ray; the code is from before Ricky's PR to add logic for autoscaler V2. The error will not happen in the latest code. Feel free to close the issue,older version ray code add logic error happen latest code feel free close issue,issue,negative,positive,positive,positive,positive,positive
1696350612,"Hi @wangp-nhlab, that code example is more focusing on introducing Ray Train AIPs, while not trying to maximize the GPU memory util. You can always increase the batch size to use more GPU Memory.",hi code example ray train trying maximize memory always increase batch size use memory,issue,negative,neutral,neutral,neutral,neutral,neutral
1696341886,"Hi @rkooo567 , what would your recommendation be for a more graceful behavior here?  ",hi would recommendation graceful behavior,issue,negative,neutral,neutral,neutral,neutral,neutral
1696312670,"> 

Yes, that is because I merged the PR to fix the issue after reverting it. However, per our discussion before, the CI did not trigger mac test cases before merged into master. And I already made a new PR to fix the mac test cases error",yes fix issue however per discussion trigger mac test master already made new fix mac test error,issue,negative,positive,positive,positive,positive,positive
1696310808,"@max-509 sorry for the confusion. I just noticed that the support for bytes was introduced for cross-language usages only. It's not commonly used in real cases, thus got broken. 
Could you elaborate on your usage? Why do you need to use bytes instead of pyarrow table objects?",sorry confusion support commonly used real thus got broken could elaborate usage need use instead table,issue,negative,negative,neutral,neutral,negative,negative
1696275795,"> I cannot reproduce this issue. After restarting the service (which, by the way, was launched through docker-compose), everything worked again. 

@psydok have you seen this issue again? It sounds like it only happened once, and then when you restarted, batching started working again. Is that correct?",reproduce issue service way everything worked seen issue like working correct,issue,negative,neutral,neutral,neutral,neutral,neutral
1696262206,@JalinWang It looks like your config doesn't use a `runtime_env`. How are you making your code available to the Ray cluster?,like use making code available ray cluster,issue,negative,positive,positive,positive,positive,positive
1696225488,Hello @raulchen. `from_arrow_refs` supports bytes of arrow table streaming format. [Link to documentation](https://docs.ray.io/en/latest/_modules/ray/data/read_api.html#from_arrow_refs) ,hello arrow table streaming format link documentation,issue,negative,neutral,neutral,neutral,neutral,neutral
1696220417,This looks like a random failure. There are 2 runs during the weekend with no code change. [The other one](https://buildkite.com/ray-project/release-tests-branch/builds/2078#018a34ea-1fd2-4ce3-ac29-1ab33c4ab174) succeeded. I'm re-running to confirm. ,like random failure weekend code change one confirm,issue,negative,negative,negative,negative,negative,negative
1696217226,"I believe the above ""early termination"" was added to avoid attempting to serialize non-serializable objects. The following test fails:
```
    def test_not_serializing_objects():
        scanner = _PyObjScanner(source_type=Source)
        not_serializable = NotSerializable()
        my_objs = [not_serializable, {""key"": Source()}]

>       found = scanner.find_nodes(my_objs)

tests/test_py_obj_scanner.py:46:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
py_obj_scanner.py:97: in find_nodes
    self.dump(obj)
../cloudpickle/cloudpickle_fast.py:733: in dump
    return Pickler.dump(self, obj)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <test_py_obj_scanner.NotSerializable object at 0x12392dbe0>

    def __reduce__(self):
>       raise Exception(""don't even try to serialize me."")
E       Exception: don't even try to serialize me.
```

I'm not sure if this is actually a requirement -- for Serve at least, we only use the py_obj_scanner on objects that'll be serialized anyways.",believe early termination added avoid serialize following test scanner key source found dump return self self object self raise exception even try serialize exception even try serialize sure actually requirement serve least use anyways,issue,negative,positive,neutral,neutral,positive,positive
1696216768,"Closing as this is not a Ray bug. If you still see issues, feel free to reopen.",ray bug still see feel free reopen,issue,positive,positive,positive,positive,positive,positive
1696214575,"hey @max-509, `from_arrow_refs` is supposed to take object refs of pyarrow tables. But in your script, the `from_arrow_refs` variable is an object ref of bytes. This is not a supported usage. You need to change `read_parquet_bytes` to return a pyarrow table. ",hey supposed take object table script variable object ref usage need change return table,issue,negative,neutral,neutral,neutral,neutral,neutral
1696206453,"> do we have plan to add this back?

We can but we don't have a clear policy of what tests need to run in asan mode (most of our tests don't run in asan mode).",plan add back clear policy need run mode run mode,issue,negative,positive,neutral,neutral,positive,positive
1696160998,"I post a PR, hope it would fix. https://github.com/ray-project/ray/pull/39012",post hope would fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1696152618,"This seems to be because the `reducer_override` hook is not written to recursively scan non-native objects. If I make the following patch, it resolves the issue:

```
diff --git a/python/ray/dag/py_obj_scanner.py b/python/ray/dag/py_obj_scanner.py
index 20798b8441..195fc87207 100644
--- a/python/ray/dag/py_obj_scanner.py
+++ b/python/ray/dag/py_obj_scanner.py
@@ -82,6 +82,7 @@ class _PyObjScanner(ray.cloudpickle.CloudPickler, Generic[SourceType, Transforme
             self._found.append(obj)
             return _get_node, (id(self), index)
         else:
+            return super().reducer_override(obj)
             index = len(self._objects)
             self._objects.append(obj)
             return _get_object, (id(self), index)
```

Though, the proper fix is probably a bit more subtle than this.",hook written scan make following patch issue git index class generic return id self index else return super index return id self index though proper fix probably bit subtle,issue,positive,neutral,neutral,neutral,neutral,neutral
1696148422,"Should be fixed by #38959, I triggered another run here: https://buildkite.com/ray-project/release-tests-pr/builds/51140",fixed triggered another run,issue,negative,positive,neutral,neutral,positive,positive
1696130168,"hi @yiwei00000, ` ray.data.read_csv` is a lazy operation, it won't read data into memory, until `ds.max`. And the second `ds.max` will read data from the source again. 
If you want to cache data in memory, add `ds = ds.materialize()` after `read_csv`. ",hi lazy operation wo read data memory second read data source want cache data memory add,issue,negative,negative,negative,negative,negative,negative
1696119097,"This code will change observation space to screen pixels

```python
import gymnasium as gym
import numpy as np

from ray.rllib.algorithms.dreamer.dreamer import DreamerConfig
from supersuit.generic_wrappers import resize_v1

class NormalizedImageEnv(gym.ObservationWrapper):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.observation_space = self.observation_space['pixels']
        self.observation_space = gym.spaces.Box(
            -1.0,
            1.0,
            shape=self.observation_space.shape,
            dtype=np.float32,
        )

    def observation(self, observation):
        observation = observation['pixels']
        obs = (observation.astype(np.float32) / 128.0) - 1.0
        return obs

def create_env(*args, **kwargs):
    orig_env = gym.make(""HalfCheetah-v5"", render_mode='rgb_array', **kwargs)
    pixel_env = gym.wrappers.PixelObservationWrapper(orig_env)
    return resize_v1(NormalizedImageEnv(pixel_env), x_size=64, y_size=64)

gym.register(""cheetah-pixels"", entry_point=create_env)
config = DreamerConfig().training(gamma=0.9, lr=0.01).resources(num_gpus=0).rollouts(num_rollout_workers=0)  
config.disable_env_checking = True
algo = config.build(env=""cheetah-pixels"")
```
But dreamer will still fail executing nn model. 
",code change observation space screen python import gymnasium gym import import import class self super observation self observation observation observation return return true dreamer still fail model,issue,negative,positive,neutral,neutral,positive,positive
1696102786,"> > Thanks for raising this and following up. This is indeed a bug, and it should be fixed here: #38319
> > The fix will be included in Ray 2.7.
> > As a workaround, you can set the `storage_path` to a relative local directory, which will then not trigger the buggy code path.
> > ```
> > from ray import air, tune
> > 
> > tuner = tune.Tuner(
> >     train_fn,
> >     run_config=air.RunConfig(storage_path=""./"")
> > )
> > tuner.fit()
> > ```
> 
> Thanks @krfricke for following up on this.
> 
> After a long time away from ray, I've tried using a windows, python 3.11 daily release wheel dated 27th Aug. I've noticed that the training runs smoothly as opposed to the last time I tried it. However, a warning: `2023-08-28 15:01:08,910 WARNING syncer.py:459 -- Last sync command failed with the following error: Traceback (most recent call last): File ""C:\Users\user\AppData\Roaming\Python\Python311\site-packages\ray\train\_internal\syncer.py"", line 457, in _launch_sync_process self.wait() File ""C:\Users\user\AppData\Roaming\Python\Python311\site-packages\ray\train\_internal\syncer.py"", line 531, in wait raise e File ""C:\Users\user\AppData\Roaming\Python\Python311\site-packages\ray\train\_internal\syncer.py"", line 529, in wait self._sync_process.wait(timeout=self.sync_timeout) File ""C:\Users\user\AppData\Roaming\Python\Python311\site-packages\ray\train\_internal\syncer.py"", line 204, in wait raise exception File ""C:\Users\user\AppData\Roaming\Python\Python311\site-packages\ray\train\_internal\syncer.py"", line 167, in entrypoint result = self._fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^ File ""C:\Users\user\AppData\Roaming\Python\Python311\site-packages\ray\train\_internal\storage.py"", line 218, in _upload_to_fs_path _upload_to_uri_with_exclude_fsspec( File ""C:\Users\user\AppData\Roaming\Python\Python311\site-packages\ray\train\_internal\storage.py"", line 231, in _upload_to_uri_with_exclude_fsspec _pyarrow_fs_copy_files( File ""C:\Users\user\AppData\Roaming\Python\Python311\site-packages\ray\train\_internal\storage.py"", line 112, in _pyarrow_fs_copy_files return pyarrow.fs.copy_files( ^^^^^^^^^^^^^^^^^^^^^^ File ""C:\Users\user\AppData\Roaming\Python\Python311\site-packages\pyarrow\fs.py"", line 269, in copy_files _copy_files_selector(source_fs, source_sel, File ""pyarrow\_fs.pyx"", line 1616, in pyarrow._fs._copy_files_selector File ""pyarrow\error.pxi"", line 100, in pyarrow.lib.check_status pyarrow.lib.ArrowInvalid: GetFileInfo() yielded path 'C:/Users/user/ray_results/CustomEnv/PPO_CartPoleEnv_b2108_00000_0_clip_param=0.0705,gamma=0.8471,grad_clip=34.6097,kl_coeff=0.0096,kl_target=0.0009,lambda=0.9016,lr=0_2023-08-28_13-50-26', which is outside base dir 'C:/Users/user/ray_results\CustomEnv'` is still showing up.
> 
> This implies that the changes made so far may have broken how pyarrow interacts with ray rllib for local storage. In addition to this, the checkpoints and the experiment and trial data are being stored in separate locations (checkpoints stored in custom specified location, while experiment info is being stored in the `~/ray_results` folder Could you have any possible idea why this is so? Thanks

@krfricke
I've also noticed that the experiment is failing as a result of WinError 5 Permission Error. Here's the output:
`'[PopulationBasedTraining] [Exploit] Cloning trial e92c0_00000 (score = 12.100000) into trial e92c0_00001 (score = 9.410000)

2023-08-28 20:33:06,648	INFO pbt.py:910 -- 

[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of triale92c0_00001:
gamma : 0.9839158975252484 --- (* 1.2) --> 1.180699077030298
lr : 0.0015649859894848056 --- (* 1.2) --> 0.0018779831873817665
grad_clip : 39.75839304235868 --- (* 0.8) --> 31.80671443388695
kl_coeff : 0.004035156399555937 --- (resample) --> 0.005909956917987553
kl_target : 0.0028334910746871805 --- (resample) --> 0.002281142487438168
vf_loss_coeff : 0.015258376080097048 --- (* 0.8) --> 0.012206700864077638
clip_param : 0.10821188925532442 --- (* 1.2) --> 0.1298542671063893
lambda_ : 0.9462381761663003 --- (resample) --> 0.9537723166174547

(pid=11444) DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
(PPO pid=11444) 2023-08-28 20:33:28,541	WARNING algorithm_config.py:2574 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
(PPO pid=11444) 2023-08-28 20:33:28,542	WARNING algorithm_config.py:2588 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.
(PPO pid=11444) 2023-08-28 20:33:28,543	WARNING algorithm_config.py:672 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.
(PPO pid=11444) Restored on 127.0.0.1 from checkpoint: Checkpoint(filesystem=local, path=C:/Users/user/ray_results\Palka5\PPO_CartPoleEnv_e92c0_00000_0_clip_param=0.1082,gamma=0.9839,grad_clip=39.7584,kl_coeff=0.0040,kl_target=0.0028,lambda=0.9965,lr=0_2023-08-28_19-49-53\checkpoint_000001)
(PPO pid=23904) Checkpoint successfully created at: Checkpoint(filesystem=local, path=C:/Users/user/ray_results\Palka5\PPO_CartPoleEnv_e92c0_00000_0_clip_param=0.1082,gamma=0.9839,grad_clip=39.7584,kl_coeff=0.0040,kl_target=0.0028,lambda=0.9965,lr=0_2023-08-28_19-49-53\checkpoint_000004)
(PPO pid=23904) Checkpoint successfully created at: Checkpoint(filesystem=local, path=C:/Users/user/ray_results\Palka5\PPO_CartPoleEnv_e92c0_00000_0_clip_param=0.1082,gamma=0.9839,grad_clip=39.7584,kl_coeff=0.0040,kl_target=0.0028,lambda=0.9965,lr=0_2023-08-28_19-49-53\checkpoint_000005)
2023-08-28 20:43:45,520	WARNING tune_controller.py:868 -- Trial controller checkpointing failed: [WinError 5] Access is denied: 'C:/Users/user/ray_results\\Palka5\\.tmp_experiment_state_954faf77-b899-4492-9fda-3fea4afd812c' -> 'C:/Users/user/ray_results\\Palka5\\experiment_state-2023-08-28_19-49-53.json'
(PPO pid=11444) [C:\arrow\cpp\src\arrow\filesystem\s3fs.cc:2598](file:///C:/arrow/cpp/src/arrow/filesystem/s3fs.cc:2598):  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit...

File [~\AppData\Roaming\Python\Python311\site-packages\ray\tune\execution\tune_controller.py:481](https://file+.vscode-resource.vscode-cdn.net/c%3A/_Nuvo2/preDeploy/~/AppData/Roaming/Python/Python311/site-packages/ray/tune/execution/tune_controller.py:481), in TuneController.save_to_dir(self, experiment_dir)
    478 with open(tmp_file_name, ""w"") as f:
    479     json.dump(runner_state, f, indent=2, cls=TuneFunctionEncoder)
--> 481 os.replace(
    482     tmp_file_name,
    483     os.path.join(experiment_dir, self.experiment_state_file_name),
    484 )
    486 self._search_alg.save_to_dir(experiment_dir, session_str=self._session_str)
    487 self._callbacks.save_to_dir(experiment_dir, session_str=self._session_str)

PermissionError: [WinError 5] Access is denied: 'C:/Users/user/ray_results\\Palka5\\.tmp_experiment_state_954faf77-b899-4492-9fda-3fea4afd812c' -> 'C:/Users/user/ray_results\\Palka5\\experiment_state-2023-08-28_19-49-53.json'`. 
Please advice.",thanks raising following indeed bug fixed fix included ray set relative local directory trigger buggy code path ray import air tune tuner thanks following long time away ray tried python daily release wheel th training smoothly opposed last time tried however warning warning last sync command following error recent call last file line file line wait raise file line wait file line wait raise exception file line result file line file line file line return file line file line file line path outside base still showing made far may broken ray local storage addition experiment trial data separate custom location experiment folder could possible idea thanks also experiment failing result permission error output exploit trial score trial score explore perturbed gamma resample resample resample raise error future warning setting set set want implement custom exploration behaviour please modify method hand default exploration must done warning setting set exploration prior exploration setting set warning create given property successfully successfully warning trial controller access file arrow even though could lead segmentation fault exit file self open access please advice,issue,negative,positive,neutral,neutral,positive,positive
1696041901,"@sihanwang41 this is awesome, thank you.

Let's hold off on merging it until after all of the changes needed for 2.7 are in to avoid merge conflict pain.",awesome thank let hold avoid merge conflict pain,issue,negative,positive,positive,positive,positive,positive
1696033475,"This was fixed by https://github.com/ray-project/ray/pull/38478

See passing run here: https://buildkite.com/ray-project/release-tests-pr/builds/49493#018a04ec-d583-4c3f-a4fa-9dcf0e403145",fixed see passing run,issue,negative,positive,neutral,neutral,positive,positive
1696030123,"@rkooo567 Looks like this commit is causing windows://python/ray/tests:test_autoscaler_e2e to fail (with some seemingly unrelated errors for session connection, it fails to do `ray.init` at a certain test). The test has been super flaky (pass rate < 1/5) in the master. 

I am proposing this as the root cause (or somehow related) for 2 reasons: 
1. This revert has been passing the test consistently (3 runs all PASS) https://buildkite.com/ray-project/oss-ci-build-pr/builds/33769#018a2dff-5102-4e7d-9f64-f96918c7a740
2. The CI history shows the original commit as the first fail run since a very stable stream of pass: 
https://b534fd88.us1a.app.preset.io/superset/dashboard/p/2N3WZkX4q5l/

<img width=""1309"" alt=""image"" src=""https://github.com/ray-project/ray/assets/11676094/d63db2a5-ab54-417d-a289-a34ee7ae9e36"">
<img width=""1287"" alt=""image"" src=""https://github.com/ray-project/ray/assets/11676094/c5f259d8-bc13-4fae-b105-345eebdd0f77"">
",like commit causing fail seemingly unrelated session connection certain test test super flaky pas rate master root cause somehow related revert passing test consistently pas history original commit first fail run since stable stream pas image image,issue,positive,positive,neutral,neutral,positive,positive
1696012597,"Ok - apparently the test was still failing https://buildkite.com/ray-project/oss-ci-build-pr/builds/33778#018a2eab-317f-4146-9c73-30e94e9658f8

But the error logs are gone - this should still be the right thing to do so I would love to have this merge . ",apparently test still failing error gone still right thing would love merge,issue,negative,positive,positive,positive,positive,positive
1695960095,"> Hmm I have a couple questions.
> 
> 1. What's the motivation to change conftest to use force? Is it necessary?
> 2. Why do we use timeout 2 -> timeout 5?

re1: 
- IMO it's the right thing to do (so that no procs are leaked to the next test sessions). 

re2:
- oh yeah, good point",couple motivation change use force necessary use right thing next test session oh yeah good point,issue,positive,positive,positive,positive,positive,positive
1695957711,"Probably due to this error
```
Traceback (most recent call last):
  File ""anyscale_job_wrapper.py"", line 120, in collect_metrics
    subprocess.run(
  File ""/home/ray/anaconda3/lib/python3.8/subprocess.py"", line 495, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
  File ""/home/ray/anaconda3/lib/python3.8/subprocess.py"", line 1028, in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
  File ""/home/ray/anaconda3/lib/python3.8/subprocess.py"", line 1894, in _communicate
    self.wait(timeout=self._remaining_time(endtime))
  File ""/home/ray/anaconda3/lib/python3.8/subprocess.py"", line 1083, in wait
    return self._wait(timeout=timeout)
  File ""/home/ray/anaconda3/lib/python3.8/subprocess.py"", line 1798, in _wait
    raise TimeoutExpired(self.args, timeout)
subprocess.TimeoutExpired: Command '['python', 'prometheus_metrics.py', '28800.067440183997', '--path', '/tmp/metrics_test_out.json']' timed out after 899.9999466970003 seconds
```

https://console.anyscale-staging.com/o/anyscale-internal/jobs/prodjob_mp9agi84kdfqwnbcn7f1vrx8y6",probably due error recent call last file line file line run input file line communicate input file line file line wait return file line raise command path timed,issue,negative,negative,neutral,neutral,negative,negative
1695947218,"Hi @krfricke, I agree that it initially appears that the inputted checkpoint frequency should be respected based on the line you shared. However, If you look [here](https://github.com/ray-project/xgboost_ray/blob/bd7e799ea04de12d2d4e40e28830d8c2b7f95515/xgboost_ray/main.py#L597), you can see that each RayXGBoostActor stores a checkpoint_frequency as an instance variable, and only pickles the model at the frequency based on this instance variable. However, this instance variable is set based on the RayParams passed to the train function (see [here](https://github.com/ray-project/xgboost_ray/blob/bd7e799ea04de12d2d4e40e28830d8c2b7f95515/xgboost_ray/main.py#L1115)), and the value of checkpoint_frequency in the RayParams is never overwritten. 

If you would like to play around with this, feel free to copy this [colab notebook](https://colab.research.google.com/drive/1QPaCDuLlgCsRtOaKZHX0eYrk4cUOtbAq?usp=sharing) which I used to test the issue. If you keyboard interrupt the training and then try to resume, you can see that it resumes only based on the last checkpoint of a multiple of 5.",hi agree initially frequency based line however look see instance variable model frequency based instance variable however instance variable set based train function see value never would like play around feel free copy notebook used test issue keyboard interrupt training try resume see based last multiple,issue,negative,positive,neutral,neutral,positive,positive
1695945098,"Probably due to this error
```
Traceback (most recent call last):
  File ""workloads/autoscaling_multi_deployment.py"", line 211, in <module>
    main()
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py"", line 1157, in __call__
    return self.main(*args, **kwargs)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py"", line 1078, in main
    rv = self.invoke(ctx)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py"", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py"", line 783, in invoke
    return __callback(*args, **kwargs)
  File ""workloads/autoscaling_multi_deployment.py"", line 164, in main
    http_host = str(serve_client._http_config.host)
AttributeError: 'NoneType' object has no attribute '_http_config'
```

https://console.anyscale-staging.com/o/anyscale-internal/jobs/prodjob_lsvivp85rmd97l43twekwejg8v",probably due error recent call last file line module main file line return file line main file line invoke return file line invoke return file line main object attribute,issue,negative,positive,neutral,neutral,positive,positive
1695932173,Maybe you can add `set -x` at the top of the file and capture the output so that you can see what the actual bash calls are. Then study the output to figure out which address is wrong.,maybe add set top file capture output see actual bash study output figure address wrong,issue,negative,neutral,neutral,neutral,neutral,neutral
1695925747,"This is the sbatch script, while the vllm repo I think only calls `ray.init`

```
#!/bin/bash
# shellcheck disable=SC2206
# THIS FILE IS GENERATED BY AUTOMATION SCRIPT! PLEASE REFER TO ORIGINAL SCRIPT!
# THIS FILE IS MODIFIED AUTOMATICALLY FROM TEMPLATE AND SHOULD BE RUNNABLE!
#SBATCH --partition=develbooster
#SBATCH --job-name=test_0828-1638
#SBATCH --output=test_0828-1638-%j.log

### This script works for any number of nodes, Ray will find and manage all resources
#SBATCH --nodes=2
#SBATCH --exclusive
### Give all resources to a single Ray task, ray can manage the resources internally
#SBATCH --ntasks-per-node=1
#SBATCH --gpus-per-task=4
#SBATCH --cpus-per-task=24
#SBATCH --account=transfernetx

# Load modules or your own conda environment here
# module load pytorch/v1.4.0-gpu
# conda activate ${CONDA_ENV}


# ===== DO NOT CHANGE THINGS HERE UNLESS YOU KNOW WHAT YOU ARE DOING =====
# This script is a modification to the implementation suggest by gregSchwartz18 here:
# https://github.com/ray-project/ray/issues/826#issuecomment-522116599
redis_password=$(uuidgen)
export redis_password

eval ""$(/p/home/jusers/puccetti1/juwels/puccetti1/miniconda3/bin/conda shell.bash hook)"" # init conda
conda activate llm

export CUDA_VISIBLE_DEVICES=0,1,2,3

nodes=$(scontrol show hostnames ""$SLURM_JOB_NODELIST"") # Getting the node names
nodes_array=($nodes)

node_1=${nodes_array[0]}
ip=$(srun --nodes=1 --ntasks=1 -w ""$node_1"" hostname --ip-address)
# ip=""${node_1}i""

# if we detect a space character in the head node IP, we'll
# convert it to an ipv4 address. This step is optional.
# if [[ ""$ip"" == *"" ""* ]]; then
#   IFS=' ' read -ra ADDR <<< ""$ip""
#   if [[ ${#ADDR[0]} -gt 16 ]]; then
#     ip=${ADDR[1]}
#   else
#     ip=${ADDR[0]}
#   fi
#   echo ""IPV6 address detected. We split the IPV4 address as $ip""
# fi

port=6379
ip_head=$ip:$port
export ip_head
echo ""IP Head: $ip_head""

echo ""STARTING HEAD at $node_1""
srun --nodes=1 --ntasks=1 -w ""$node_1"" \
  ray start --head --node-ip-address=""$ip"" --port=$port --redis-password=""$redis_password"" --block &
sleep 10

worker_num=$(($SLURM_JOB_NUM_NODES - 1)) #number of nodes other than the head node
for ((i = 1; i <= worker_num; i++)); do
  node_i=${nodes_array[$i]}
  echo ""STARTING WORKER $i at $node_i""
  echo ""IP Head: $ip_head""
  this_node_ip=$(srun --nodes=1 --ntasks=1 -w ""$node_i"" hostname --ip-address)
  echo ""Node: $node_i IP Local: $this_node_ip""
  srun --nodes=1 --ntasks=1 -w ""$node_i"" \
    ray start \
    --address ""$ip_head"" \
    --node-ip-address ${this_node_ip} \
    --num-cpus 24 \
    --num-gpus 4 \
    --redis-password=""$redis_password"" \
    --block &

  sleep 5
done

# ===== Call your code below =====
export i=wikihow_chatGPT

echo ""STARTING python command""
python -u /p/home/jusers/puccetti1/juwels/puccetti1/llm/llm-foundry/scripts/inference/vllm_generate.py \
--name_or_path /p/fastdata/mmlaion/puccetti1/llama-2-models-hf/llama-2-70b-chat/ \
--temperature 1.0 \
--top_p 0.95 \
--top_k 50 \
--max_new_tokens 256 \
--prompts ""file::/p/home/jusers/puccetti1/juwels/puccetti1/llm/M4/data/$i.jsonl"" \
--max_prompts 3000 \
--output_path ""/p/home/jusers/puccetti1/juwels/puccetti1/llm/llm-foundry/outputs/CHANGE-it/llama-2-70b-chat/repubblica/$i/"" \
--num_nodes 2
```",script think file script please refer original script file automatically template runnable script work number ray find manage exclusive give single ray task ray manage internally load environment module load activate change unless know script modification implementation suggest export hook activate export show getting node detect space character head node convert address step optional read else fi echo address split address fi port export echo head echo starting head ray start head port block sleep number head node echo starting worker echo head echo node local ray start address block sleep done call code export echo starting python command python temperature file,issue,negative,positive,neutral,neutral,positive,positive
1695923252,"Probably due to this error
```
Traceback (most recent call last):
  File ""workloads/xgboost_benchmark.py"", line 195, in <module>
    main(args)
  File ""workloads/xgboost_benchmark.py"", line 154, in main
    training_time = run_xgboost_training(data_path, num_workers, cpus_per_worker)
  File ""workloads/xgboost_benchmark.py"", line 77, in wrapper
    raise p.exception
  File ""workloads/xgboost_benchmark.py"", line 60, in run
    super(MyProcess, self).run()
  File ""/home/ray/anaconda3/lib/python3.8/multiprocessing/process.py"", line 108, in run
    self._target(*self._args, **self._kwargs)
  File ""workloads/xgboost_benchmark.py"", line 106, in run_xgboost_training
    checkpoint = LegacyXGBoostCheckpoint.from_checkpoint(result.checkpoint)
  File ""/home/ray/anaconda3/lib/python3.8/site-packages/ray/air/checkpoint.py"", line 491, in from_checkpoint
    local_path=other._local_path,
AttributeError: 'Checkpoint' object has no attribute '_local_path'
```

https://console.anyscale-staging.com/o/anyscale-internal/jobs/prodjob_ud8nrrtp1upmi6rrza8qykahpe",probably due error recent call last file line module main file line main file line wrapper raise file line run super self file line run file line file line object attribute,issue,negative,positive,neutral,neutral,positive,positive
1695922059,"> I'll close this issue now, and we can continue any new issues on the new Github repo :)


Hi @richardliaw  This issue still persists. Could you please reopen it?",close issue continue new new hi issue still could please reopen,issue,negative,positive,positive,positive,positive,positive
1695920473,"@chris-aeviator `ray-serve` uses the [autoscalar](https://docs.ray.io/en/latest/serve/scaling-and-resource-allocation.html#autoscaling) to allocate resources, which, as noted 
> The Ray Serve Autoscaler is an application-level autoscaler that sits on top of the [Ray Autoscaler](https://docs.ray.io/en/latest/cluster/getting-started.html#cluster-index). Concretely, this means that the Ray Serve autoscaler asks Ray to start a number of replica actors based on the request demand. If the Ray Autoscaler determines there aren’t enough available CPUs to place these actors, it responds by requesting more Ray nodes. The underlying cloud provider will then respond by adding more nodes. Similarly, when Ray Serve scales down and terminates some replica actors, it will try to do so in a way that results in the most nodes having no Ray actors or tasks running on them, at which point the Ray autoscaler will remove those nodes.
",allocate noted ray serve top ray concretely ray serve ray start number replica based request demand ray enough available place ray underlying cloud provider respond similarly ray serve scale replica try way ray running point ray remove,issue,negative,positive,positive,positive,positive,positive
1695915063,"You did not show how you are using the commands. In the logs you can see:
`To add another node to this Ray cluster, run ray start --address='10.11.241.83:6379`

and
```
To connect to this Ray cluster:
import ray
ray.init(_node_ip_address='10.11.241.83')
```

and
```
To submit a Ray job using the Ray Jobs CLI:
RAY_ADDRESS='http://127.0.0.1:8265' ray job submit --working-dir . -- python my_script.py
```

The error states `This node has an IP address of 10.13.23.83, but we cannot find a local Raylet with the same address. This can happen when you connect to the Ray cluster with a different IP address or when connecting to a container.`

Which command did you use?",show see add another node ray cluster run ray start connect ray cluster import ray submit ray job ray ray job submit python error node address find local raylet address happen connect ray cluster different address command use,issue,negative,neutral,neutral,neutral,neutral,neutral
1695826371,"Hi, I have this issue while trying to use ray to run [vllm](https://vllm.readthedocs.io/en/latest/serving/distributed_serving.html) in slurm, just curious if you think it might be related to his issue or I am doing smth else wrong. 

Thanks in advance, this is the error I get

```
IP Head: 10.11.241.83:6379
STARTING HEAD at jwb0097
2023-08-28 16:38:58,785	INFO usage_lib.py:407 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2023-08-28 16:38:58,785	INFO scripts.py:722 -- [37mLocal node IP[39m: [1m10.11.241.83[22m
2023-08-28 16:39:03,559	SUCC scripts.py:759 -- [32m--------------------[39m
2023-08-28 16:39:03,560	SUCC scripts.py:760 -- [32mRay runtime started.[39m
2023-08-28 16:39:03,560	SUCC scripts.py:761 -- [32m--------------------[39m
2023-08-28 16:39:03,560	INFO scripts.py:763 -- [36mNext steps[39m
2023-08-28 16:39:03,560	INFO scripts.py:766 -- To add another node to this Ray cluster, run
2023-08-28 16:39:03,560	INFO scripts.py:769 -- [1m  ray start --address='10.11.241.83:6379'[22m
2023-08-28 16:39:03,560	INFO scripts.py:778 -- To connect to this Ray cluster:
2023-08-28 16:39:03,560	INFO scripts.py:780 -- [35mimport[39m[26m ray
2023-08-28 16:39:03,560	INFO scripts.py:781 -- ray[35m.[39m[26minit(_node_ip_address[35m=[39m[26m[33m'10.11.241.83'[39m[26m)
2023-08-28 16:39:03,560	INFO scripts.py:793 -- To submit a Ray job using the Ray Jobs CLI:
2023-08-28 16:39:03,560	INFO scripts.py:794 -- [1m  RAY_ADDRESS='http://127.0.0.1:8265' ray job submit --working-dir . -- python my_script.py[22m
2023-08-28 16:39:03,560	INFO scripts.py:803 -- See https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html 
2023-08-28 16:39:03,560	INFO scripts.py:807 -- for more information on submitting Ray jobs to the Ray cluster.
2023-08-28 16:39:03,560	INFO scripts.py:812 -- To terminate the Ray runtime, run
2023-08-28 16:39:03,560	INFO scripts.py:813 -- [1m  ray stop[22m
2023-08-28 16:39:03,560	INFO scripts.py:816 -- To view the status of the cluster, use
2023-08-28 16:39:03,561	INFO scripts.py:817 --   [1mray status[22m[26m
2023-08-28 16:39:03,562	INFO scripts.py:821 -- To monitor and debug Ray, view the dashboard at 
2023-08-28 16:39:03,562	INFO scripts.py:822 --   [1m127.0.0.1:8265[22m[26m
2023-08-28 16:39:03,562	INFO scripts.py:829 -- [4mIf connection to the dashboard fails, check your firewall settings and network configuration.[24m
2023-08-28 16:39:03,562	INFO scripts.py:929 -- [36m[1m--block[22m[39m
2023-08-28 16:39:03,562	INFO scripts.py:930 -- This command will now block forever until terminated by a signal.
2023-08-28 16:39:03,562	INFO scripts.py:933 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
STARTING WORKER 1 at jwb0117
IP Head: 10.11.241.83:6379
Node: jwb0117 IP Local: 10.11.241.95
[2023-08-28 16:39:09,171 I 20806 20806] global_state_accessor.cc:356: This node has an IP address of 10.11.241.95, but we cannot find a local Raylet with the same address. This can happen when you connect to the Ray cluster with a different IP address or when connecting to a container.
2023-08-28 16:39:09,121	INFO scripts.py:904 -- [37mLocal node IP[39m: [1m10.11.241.95[22m
2023-08-28 16:39:09,173	SUCC scripts.py:916 -- [32m--------------------[39m
2023-08-28 16:39:09,173	SUCC scripts.py:917 -- [32mRay runtime started.[39m
2023-08-28 16:39:09,173	SUCC scripts.py:918 -- [32m--------------------[39m
2023-08-28 16:39:09,173	INFO scripts.py:920 -- To terminate the Ray runtime, run
2023-08-28 16:39:09,173	INFO scripts.py:921 -- [1m  ray stop[22m
2023-08-28 16:39:09,173	INFO scripts.py:929 -- [36m[1m--block[22m[39m
2023-08-28 16:39:09,173	INFO scripts.py:930 -- This command will now block forever until terminated by a signal.
2023-08-28 16:39:09,173	INFO scripts.py:933 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
STARTING python command
########################### Tensor Parallel 8
2023-08-28 16:39:15,431	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 10.11.241.83:6379...
[2023-08-28 16:39:15,438 I 10957 10957] global_state_accessor.cc:356: This node has an IP address of 10.13.23.83, but we cannot find a local Raylet with the same address. This can happen when you connect to the Ray cluster with a different IP address or when connecting to a container.
2023-08-28 16:39:15,439	INFO worker.py:1612 -- Connected to Ray cluster. View the dashboard at [1m[32m127.0.0.1:8265 [39m[22m

```",hi issue trying use ray run curious think might related issue else wrong thanks advance error get head starting head usage collection default without user confirmation terminal disable add command cluster run following command ray starting cluster see node add another node ray cluster run ray start connect ray cluster ray submit ray job ray ray job submit python see information ray ray cluster terminate ray run ray view status cluster use monitor ray view dashboard connection dashboard check network command block forever signal running message printed terminate unexpectedly exit graceful thus starting worker head node local node address find local raylet address happen connect ray cluster different address container node terminate ray run ray command block forever signal running message printed terminate unexpectedly exit graceful thus starting python command tensor parallel ray cluster address node address find local raylet address happen connect ray cluster different address container connected ray cluster view dashboard,issue,negative,negative,neutral,neutral,negative,negative
1695789608,"Thanks for opening this issue @mvanness354!

It's surprising the run config setting isn't used correctly - from a glance it should be respected here:

https://github.com/ray-project/ray/blob/master/python/ray/train/gbdt_trainer.py#L312-L314

There we add a checkpointing callback that should be respected by XGBoost-Ray. I'm happy to take a look to see where things go wrong, but I may have to defer this a bit.",thanks opening issue surprising run setting used correctly glance add happy take look see go wrong may defer bit,issue,positive,positive,positive,positive,positive,positive
1695719311,"> Note that the extra config is only added to PythonGcsClient. This is to reduce blast radius and if it works well we will apply it to rest of applicable cases.

What's the reason why this is only observed from PythonClient although the reconnection issue still should happen from cpp clients? ",note extra added reduce blast radius work well apply rest applicable reason although reconnection issue still happen,issue,negative,neutral,neutral,neutral,neutral,neutral
1695715774,@chaowanggg I still see this is broken in the master,still see broken master,issue,negative,negative,negative,negative,negative,negative
1695694619,Release test that has the state API performance here; https://buildkite.com/ray-project/release-tests-pr/builds/50879,release test state performance,issue,negative,neutral,neutral,neutral,neutral,neutral
1695679125,I am 90% sure the issue is from implicit resources. I already reassigned the issue to @jjyao. The issue is already marked as a release blocker,sure issue implicit already issue issue already marked release blocker,issue,negative,positive,positive,positive,positive,positive
1695660860,"After checking I can confirm that https://github.com/ray-project/ray/issues/32508#issuecomment-1439088056 is no longer relevant, as long as you are using recent (i've checked on `96a6aea3d0ac550e3e9967390a12c37232300c78`) version of Gymnasium with v5 of environments that support `frame_skip`.

Dreamer expects pixel observations of an environment, so potentially [PixelObservationWrapper](https://github.com/openai/gym/blob/master/gym/wrappers/pixel_observation.py) is required, and then ""pixels"" instead of ""obs"" should be fed to environment. [here](https://github.com/ray-project/ray/blob/master/rllib/algorithms/dreamer/dreamer_torch_policy.py#L67).

There's also [DreamerV3](https://github.com/ray-project/ray/tree/master/rllib/algorithms/dreamerv3) on the main, but no documentation on site.",confirm longer relevant long recent checked version gymnasium support dreamer environment potentially instead fed environment also main documentation site,issue,negative,positive,positive,positive,positive,positive
1695647531,"> Thanks for raising this and following up. This is indeed a bug, and it should be fixed here: #38319
> 
> The fix will be included in Ray 2.7.
> 
> As a workaround, you can set the `storage_path` to a relative local directory, which will then not trigger the buggy code path.
> 
> ```
> from ray import air, tune
> 
> tuner = tune.Tuner(
>     train_fn,
>     run_config=air.RunConfig(storage_path=""./"")
> )
> tuner.fit()
> ```

Thanks @krfricke for following up on this. 

After a long time away from ray, I've tried using a windows, python 3.11 daily release wheel dated 27th Aug. I've noticed that the training runs smoothly as opposed to the last time I tried it. However, a warning: `2023-08-28 15:01:08,910	WARNING syncer.py:459 -- Last sync command failed with the following error:
Traceback (most recent call last):
  File ""C:\Users\user\AppData\Roaming\Python\Python311\site-packages\ray\train\_internal\syncer.py"", line 457, in _launch_sync_process
    self.wait()
  File ""C:\Users\user\AppData\Roaming\Python\Python311\site-packages\ray\train\_internal\syncer.py"", line 531, in wait
    raise e
  File ""C:\Users\user\AppData\Roaming\Python\Python311\site-packages\ray\train\_internal\syncer.py"", line 529, in wait
    self._sync_process.wait(timeout=self.sync_timeout)
  File ""C:\Users\user\AppData\Roaming\Python\Python311\site-packages\ray\train\_internal\syncer.py"", line 204, in wait
    raise exception
  File ""C:\Users\user\AppData\Roaming\Python\Python311\site-packages\ray\train\_internal\syncer.py"", line 167, in entrypoint
    result = self._fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\user\AppData\Roaming\Python\Python311\site-packages\ray\train\_internal\storage.py"", line 218, in _upload_to_fs_path
    _upload_to_uri_with_exclude_fsspec(
  File ""C:\Users\user\AppData\Roaming\Python\Python311\site-packages\ray\train\_internal\storage.py"", line 231, in _upload_to_uri_with_exclude_fsspec
    _pyarrow_fs_copy_files(
  File ""C:\Users\user\AppData\Roaming\Python\Python311\site-packages\ray\train\_internal\storage.py"", line 112, in _pyarrow_fs_copy_files
    return pyarrow.fs.copy_files(
           ^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\user\AppData\Roaming\Python\Python311\site-packages\pyarrow\fs.py"", line 269, in copy_files
    _copy_files_selector(source_fs, source_sel,
  File ""pyarrow\_fs.pyx"", line 1616, in pyarrow._fs._copy_files_selector
  File ""pyarrow\error.pxi"", line 100, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: GetFileInfo() yielded path 'C:/Users/user/ray_results/CustomEnv/PPO_CartPoleEnv_b2108_00000_0_clip_param=0.0705,gamma=0.8471,grad_clip=34.6097,kl_coeff=0.0096,kl_target=0.0009,lambda=0.9016,lr=0_2023-08-28_13-50-26', which is outside base dir 'C:/Users/user/ray_results\CustomEnv'` is still showing up. 

This implies that the changes made so far may have broken how pyarrow interacts with ray rllib for local storage. In addition to this, the checkpoints and the experiment and trial data are being stored in separate locations (checkpoints stored in custom specified location, while experiment info is being stored in the `~/ray_results` folder Could you have any possible idea why this is so? Thanks ",thanks raising following indeed bug fixed fix included ray set relative local directory trigger buggy code path ray import air tune tuner thanks following long time away ray tried python daily release wheel th training smoothly opposed last time tried however warning warning last sync command following error recent call last file line file line wait raise file line wait file line wait raise exception file line result file line file line file line return file line file line file line path outside base still showing made far may broken ray local storage addition experiment trial data separate custom location experiment folder could possible idea thanks,issue,negative,negative,neutral,neutral,negative,negative
1695632318,"Ahh okay, i have been trying to fix this over the summer, and with both ray 2.6.2 and ray 2.5 i had the issue, but i get a different error on 2.6.3, which was just uploaded a week before this post, and as we were using your docker images, with the same image for the server and local image we tested with 2.6.2 last time.

Though getting the actual number is nice, thank you",trying fix summer ray ray issue get different error week post docker image server local image tested last time though getting actual number nice thank,issue,negative,positive,positive,positive,positive,positive
1695626995,Let's merge the master PR first. Thanks,let merge master first thanks,issue,negative,positive,positive,positive,positive,positive
1695587663,@rickyyx @rkooo567 Please get this merged into master first (master is unfrozen for now). Thanks,please get master first master unfrozen thanks,issue,positive,positive,positive,positive,positive,positive
1695586333,Please merge into master first and create cherry pick PR. Thanks,please merge master first create cherry pick thanks,issue,positive,positive,positive,positive,positive,positive
1695585277,"@raulchen Master is un-frozen for now. Please merge as usual, and I will approve the cherry-pick PR",master please merge usual approve,issue,negative,negative,negative,negative,negative,negative
1695418178,"@scv119 For the CI , since there is no runtime XPU device yet, is there a way to  skip tests (for time being) for the XPU side? 
Could you please review in that case? I see that the TPU integration is merged. 
Also in order to fully extend CI for XPUs does ray have any plans to use/support  a device (XPU) which Intel might be able to provision (after discussions with Ray and Intel)?  This would help us to plan our developments better. ",since device yet way skip time side could please review case see integration also order fully extend ray device might able provision ray would help u plan better,issue,positive,positive,positive,positive,positive,positive
1695330372,"> Leaving a reminder here to re-enable the new persistence path for relevant tests when merging master, since I disabled the flag in #38965

Removed the flag in this PR, ptal",leaving reminder new persistence path relevant master since disabled flag removed flag,issue,negative,positive,positive,positive,positive,positive
1694944679,"Leaving a reminder here to re-enable the new persistence path for relevant tests when merging master, since I disabled the flag in https://github.com/ray-project/ray/pull/38965",leaving reminder new persistence path relevant master since disabled flag,issue,negative,positive,positive,positive,positive,positive
1694911634,"This is not fixed. I think the original issue was not due to timeout, but it is a real bug that hangs the test",fixed think original issue due real bug test,issue,negative,positive,positive,positive,positive,positive
1694808033,Do we know when this is likely to come in as a fix?,know likely come fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1694803890,"@krfricke I'm going to merge in master to re-trigger the tests. (I took off the `tests-ok` label because I saw a different test that failed, but can add it back after this run)",going merge master took label saw different test add back run,issue,negative,neutral,neutral,neutral,neutral,neutral
1694649462,"We could still have basic auto-complete support without having ""proper"" typing/type hint constructs.

I'm not sure if it's desirable or not, but ray could use a simple `TypeVar` to passthrough the decorated class as-is. Granted, this will not be the ""proper"" spec, namely it won't have `ObjectRef` nor `remote` types correctly shows, but _at least_ it will auto-complete the function names, which reduces the chance of typing errors.

I believe it would be a nice feature many users can benefit from in the interim while we wait for better typing support from Python.",could still basic support without proper hint sure desirable ray could use simple decorated class proper spec namely wo remote correctly function chance believe would nice feature many benefit interim wait better support python,issue,positive,positive,positive,positive,positive,positive
1694638415,"By fixing do you mean modifying to work without image observations, or do you mean something else?",fixing mean work without image mean something else,issue,negative,negative,negative,negative,negative,negative
1694629384,@matthewdeng I just sent you a connection request on LinkedIn to set up the meeting.,sent connection request set meeting,issue,negative,neutral,neutral,neutral,neutral,neutral
1694597926,"A better solution to this is PR #35581 (in my opinion, since I made the PR :) ) which removes GPUtil in favor of gpustat which is already used elsewhere in ray. Unfortunately that PR has not been reviewed.",better solution opinion since made favor already used elsewhere ray unfortunately,issue,positive,neutral,neutral,neutral,neutral,neutral
1694594295,@ericl This should now be unblocked - the doctest jobs are now running with the new persistence flag enabled (https://github.com/ray-project/ray/pull/38876).,unblocked running new persistence flag,issue,negative,positive,positive,positive,positive,positive
1694589175,"Hey @matthewdeng,

Thanks for reaching out! The engineer who is leading this integration is away for this week. Can we schedule the call for next week, Wednesday, September 6th at 9 AM EST?",hey thanks reaching engineer leading integration away week schedule call next week th,issue,positive,positive,neutral,neutral,positive,positive
1694576951,This was consolidated in a different test update PR,consolidated different test update,issue,negative,neutral,neutral,neutral,neutral,neutral
1694558692,`master` is un-frozen now. Let's merge into master. But I don't think we should pick into 2.7,master let merge master think pick,issue,negative,neutral,neutral,neutral,neutral,neutral
1694394074,"I also hit this issue by following the [Prepare the Python environment](https://docs.ray.io/en/latest/ray-contribute/development.html#prepare-the-python-environment) and [Building Ray (Python only)](https://docs.ray.io/en/latest/ray-contribute/development.html#prepare-the-python-environment) steps.

I am running Python 3.8.10 in a virtualenv, on Ubuntu 20.04 LTS (WSL)

Investigating this I found that the underlying issue has been discussed in #31458 with a solution that appeared to work for some people in #31829

For my case, I found that the workaround added there was not being applied because `os.path.dirname(__file__)` was returning a relative path where my `sys.path` expected it to be absolute.

I have made a PR #38948 which makes the directory path absolute and issues a print statement when this workaround is being used

@clarng could you take a look?
",also hit issue following prepare python environment building ray python running python investigating found underlying issue solution work people case found added applied relative path absolute made directory path absolute print statement used could take look,issue,negative,positive,neutral,neutral,positive,positive
1694348111,"Ok, reverted the changes for now.

It also looks like test_cluster was fixed in another PR, so this leaves only two test suites in this PR.",also like fixed another leaf two test,issue,negative,positive,neutral,neutral,positive,positive
1694317751,It seems to work if I reduce the number of CPUs passed to ray even further (now 72 instead of 90).,work reduce number ray even instead,issue,negative,neutral,neutral,neutral,neutral,neutral
1694236691,"Thanks! 

Based on your hint, I tried to use the ray documenation for guidance on finding an AMIs. However, I was not really successful in doing that. Anyway, just for the record, here how I found a proper AMI for my purpose (small EBS-volume but CUDA 11 support)

`aws ec2 describe-images --region us-west-2 --filters ""Name=name,Values=*Deep Learning AMI GPU CUDA 11*""`

and then I just skimmed through the available AMIs. Dont know if that is a good way and will always provide AMIs that can be used for a ray-cluster, but at least I was able to deploy a ray-cluster using first AMI (ami-0e274207681ab9030) that I picked.

",thanks based hint tried use ray guidance finding however really successful anyway record found proper ami purpose small support region deep learning ami skimmed available dont know good way always provide used least able deploy first ami picked,issue,positive,positive,positive,positive,positive,positive
1694100095,"@can-anyscale , without this change, the lint will fail after branch cut on the release branch. there is some logic in `setup.py` that disables `ray-cpp` when the version contains ""dev"".",without change lint fail branch cut release branch logic version dev,issue,negative,negative,negative,negative,negative,negative
1694087470,@woshiyyya This is a subset of https://github.com/ray-project/ray/pull/38875. Your PR has a bunch of tests in the `Doc tests and examples (AIR)` suite that should be covered in a separate PR.,subset bunch doc air suite covered separate,issue,negative,neutral,neutral,neutral,neutral,neutral
1694066226,"The job exit code is logged and shown in job driver logs I guess? If so, users are able to see them in the driver logs in ray dashboard automatically.",job exit code logged shown job driver guess able see driver ray dashboard automatically,issue,negative,positive,positive,positive,positive,positive
1694061964,"> Yes other checks we can use manual judgement, `buildkite/premerge` must pass. cc @aslonnie

It's passing now.",yes use manual must pas passing,issue,negative,neutral,neutral,neutral,neutral,neutral
1694039673,"Yes other checks we can use manual judgement, `buildkite/premerge` must pass. cc @aslonnie ",yes use manual must pas,issue,negative,neutral,neutral,neutral,neutral,neutral
1694036681,"Hey @zhe-thoughts this PR is ready for merge. The CI has passed before, and I only pushed a small comment update after that. ",hey ready merge small comment update,issue,negative,negative,neutral,neutral,negative,negative
1694035755,@matthewdeng can I have your approval since it's late stage. Thanks,approval since late stage thanks,issue,positive,negative,neutral,neutral,negative,negative
1694031423,Here is a very similar example testing timeout behavior without sleeping: https://github.com/ray-project/ray/blob/master/python/ray/serve/tests/test_new_handle_api.py#L49,similar example testing behavior without sleeping,issue,negative,neutral,neutral,neutral,neutral,neutral
1694031203,"No this has nothing to do with cancellation, just needs to be rewritten to use explicit coordination. Let me send an example.",nothing cancellation need use explicit let send example,issue,negative,neutral,neutral,neutral,neutral,neutral
1694028388,"Yea, like need to implement task cancel for gRPC",yea like need implement task cancel,issue,negative,neutral,neutral,neutral,neutral,neutral
1694024900,"Gotcha, this must have to do with the cancel feature. Will look into it",must cancel feature look,issue,negative,neutral,neutral,neutral,neutral,neutral
1694021719,I guess `buildkite/premerge` is mandatory to pass before merging. Let me retry it.,guess mandatory pas let retry,issue,negative,neutral,neutral,neutral,neutral,neutral
1694019123,"> ""buildkite/premerge"" is a x

The failure is unrelated (it's flaky on master)",failure unrelated flaky master,issue,negative,negative,negative,negative,negative,negative
1694007287,Current code has the assumption that each host only has one type of accelerators. The number of accelerators is in `num_gpus`.,current code assumption host one type number,issue,negative,neutral,neutral,neutral,neutral,neutral
1694004015,@jjyao please help review this one,please help review one,issue,positive,neutral,neutral,neutral,neutral,neutral
1693991401,"That's likely because of AMI (Deep learning AMI) baked with number of ML frameworks (please see AMI description). We can always switch to [CPU](https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/aws/example-full.yaml#L19) and use some latest AMI (based on OS).

```
aws ec2 describe-images --image-ids ami-0387d929287ab193e --region us-west-2
{
    ""Images"": [
        {
            ""Architecture"": ""x86_64"",
            ""CreationDate"": ""2022-06-09T17:20:48.000Z"",
            ""ImageId"": ""ami-0387d929287ab193e"",
            ""ImageLocation"": ""amazon/Deep Learning AMI (Ubuntu 18.04) Version 61.0"",
            ""ImageType"": ""machine"",
            ""Public"": true,
            ""OwnerId"": ""898082745236"",
            ""PlatformDetails"": ""Linux/UNIX"",
            ""UsageOperation"": ""RunInstances"",
            ""State"": ""available"",
            ""BlockDeviceMappings"": [
                {
                    ""DeviceName"": ""/dev/sda1"",
                    ""Ebs"": {
                        ""DeleteOnTermination"": true,
                        ""Iops"": 3000,
                        ""SnapshotId"": ""snap-04db122cb9d19ae16"",
                        ""VolumeSize"": 140,
                        ""VolumeType"": ""gp3"",
                        ""Encrypted"": false
                    }
                },
                {
                    ""DeviceName"": ""/dev/sdb"",
                    ""VirtualName"": ""ephemeral0""
                },
                {
                    ""DeviceName"": ""/dev/sdc"",
                    ""VirtualName"": ""ephemeral1""
                }
            ],
            ""Description"": ""MXNet-1.9, TensorFlow-2.7, PyTorch-1.11, Neuron, & others. NVIDIA CUDA, cuDNN, NCCL, Intel MKL-DNN, Docker, NVIDIA-Docker & EFA support. For fully managed experience, check: https://aws.amazon.com/sagemaker"",
            ""EnaSupport"": true,
            ""Hypervisor"": ""xen"",
            ""ImageOwnerAlias"": ""amazon"",
            ""Name"": ""Deep Learning AMI (Ubuntu 18.04) Version 61.0"",
            ""RootDeviceName"": ""/dev/sda1"",
            ""RootDeviceType"": ""ebs"",
            ""SriovNetSupport"": ""simple"",
            ""VirtualizationType"": ""hvm""
        }
    ]
}
```",likely ami deep learning ami baked number please see ami description always switch use latest ami based o region architecture learning ami version machine public true state available true false ephemeral ephemeral description neuron docker support fully experience check true name deep learning ami version simple,issue,positive,positive,positive,positive,positive,positive
1693988047,"@jjyao this makes the assumption the same host only has one accelerators. Is this usually true? (Since users care ""how many accelerators they use per node)",assumption host one usually true since care many use per node,issue,positive,positive,positive,positive,positive,positive
1693985540,That makes total sense. I guess this issue will be gone when you remove them as part of the re-implementation?,total sense guess issue gone remove part,issue,negative,neutral,neutral,neutral,neutral,neutral
1693979650,"I tried this out locally and it looks good to me:

<img width=""1780"" alt=""Screen Shot 2023-08-25 at 3 02 14 PM"" src=""https://github.com/ray-project/ray/assets/113316/1dd73c33-34d4-4bda-8a25-a8b248997336"">
",tried locally good screen shot,issue,negative,positive,positive,positive,positive,positive
1693978524,"Currently accelerator type as a custom resource is an implementation detail, users shouldn't use those resources directly but rather do `task.options(num_gpus=1, accelerator_type=""A100"")`. As an implementation detail, I'm considering to reimplement it as labels instead of custom resources.",currently accelerator type custom resource implementation detail use directly rather implementation detail considering instead custom,issue,negative,positive,neutral,neutral,positive,positive
1693975660,"A usability improvement for state API, would be great to get this in. cc @zhe-thoughts 

TODO: waiting for test results. Adding `test-ok` once ready.",usability improvement state would great get waiting test ready,issue,positive,positive,positive,positive,positive,positive
1693975201,"Thanks for the review, all comments have been addressed.  In a followup PR I'll update the GPU-GKE doc (update it to remove the manual taints/tolerations, which are automatically handled by GKE) and make this tutorial link to that doc.",thanks review update doc update remove manual automatically handled make tutorial link doc,issue,negative,positive,positive,positive,positive,positive
1693972931,"The Actor creation throughput went back up above 800 since June 30th and remains above the threshold. I think we can close this ticket. 

cc: @scv119 

![image](https://github.com/ray-project/ray/assets/12957223/6164eb0b-a42b-4e17-a9d5-55344f4fe20d)
 ",actor creation throughput went back since june th remains threshold think close ticket image,issue,negative,neutral,neutral,neutral,neutral,neutral
1693966151,"Ah thanks all for the suggestions, I didn't want to disable the tests because I don't want to cause other changes to lose signal, and I didn't want to change the point where the flag is read, because I intentionally want to update the default value.

I ended up taking the suggestion from https://github.com/ray-project/ray/pull/38912#discussion_r1306090155 and setting this flag directly here - I think this is a clean enough solution and will revert it once I can update the corresponding tests! 

",ah thanks want disable want cause lose signal want change point flag read intentionally want update default value ended taking suggestion setting flag directly think clean enough solution revert update corresponding,issue,positive,positive,positive,positive,positive,positive
1693905350,@zhe-thoughts  - need this library to run some release tests. The PR does not change any Ray code.,need library run release change ray code,issue,negative,neutral,neutral,neutral,neutral,neutral
1693904366,Thanks for quickly accommodating my fix!,thanks quickly accommodating fix,issue,negative,positive,positive,positive,positive,positive
1693878647,"Couple suggestions to break this issue down more (instead of ""unifying"", we can clarify what is being reported).
1. For the data progress bar, make it clear it is reporting progress about a data operation, and not general progress.
2. Hide the GPUs field if the data operation doesn't use GPUs.

This is in the spirit of loose coupling between libraries.",couple break issue instead clarify data progress bar make clear progress data operation general progress hide field data operation use spirit loose coupling,issue,positive,positive,neutral,neutral,positive,positive
1693874970,"Trying to verify bk pipeline set up with authentication on pr build but runs into the following error:

Could not get Ray AIR secrets: An error occurred (AccessDeniedException) when calling the GetSecretValue operation: User: arn:aws:sts::029272617770:assumed-role/bk-runner-queue-medium-pr-Role/i-090edb965e5aece01 is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:us-west-2:029272617770:secret:oss-ci/ray-air-test-secrets20221014164754935800000002-UONblX because no identity-based policy allows the secretsmanager:GetSecretValue action

may just have to run and verify post submit...",trying verify pipeline set authentication build following error could get ray air error calling operation user arn authorized perform resource arn policy action may run verify post submit,issue,negative,positive,neutral,neutral,positive,positive
1693868351,"if you just want to get things merged without passing the tests, there is an easier way: temporarily disable the tests on the pipeline, or mark them as soft fail.

or you can also alter the code point where this env var is being read?
",want get without passing easier way temporarily disable pipeline mark soft fail also alter code point read,issue,negative,negative,negative,negative,negative,negative
1693864100,"@zhe-thoughts I encountered no conflicts when rebasing with the upstream master branch locally. I still went ahead and rebased, then force-pushed. Thanks!

",upstream master branch locally still went ahead thanks,issue,negative,positive,neutral,neutral,positive,positive
1693862751,"ah, I guess @matthewdeng and @justinvyu maybe just integrate this PR with your PR then, so you can test it all in one place, and we don't need this PR anymore",ah guess maybe integrate test one place need,issue,negative,neutral,neutral,neutral,neutral,neutral
1693820513,"@kevin85421 sorry, need a rebase. There's conflict now",sorry need rebase conflict,issue,negative,negative,negative,negative,negative,negative
1693798698,"Yes, there are also test coverage for timeouts https://github.com/ray-project/ray/blob/master/python/ray/serve/tests/test_grpc.py#L668",yes also test coverage,issue,negative,neutral,neutral,neutral,neutral,neutral
1693795148,"FYI I'm working on adding test cases and ensuring things work as expected now.

@alexeykudinkin yea we can, need to nail down exactly the behavior we want for it. Can you similarly detect when clients disconnect? And do we support a timeout like HTTP atm @GeneDer?",working test work yea need nail exactly behavior want similarly detect disconnect support like,issue,positive,positive,positive,positive,positive,positive
1693775098,"Ran into this issue. Just needed a note to run `pip install GPUtil` or add to requirements and this error goes away. 

However, this appears to result in my case, in the Ray instance just hanging indefinitely on:

`2023-08-25 18:24:03,231 INFO worker.py:1636 -- Started a local Ray instance.`

Which I assume is independent of vLLM, so I have posted asked about this in the Ray GitHub. 
 ",ran issue note run pip install add error go away however result case ray instance hanging indefinitely local ray assume independent posted ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1693772913,"Hey @zhe-thoughts , this would be good to merge. All tests are passing now. Thank you so much!",hey would good merge passing thank much,issue,positive,positive,positive,positive,positive,positive
1693770274,"This issue still appears to persist. Attempted to run @Vamsi995's environment (except for Rocky 8.8 instead of CentOS) with no success. 


This was after downgrading from `ray==2.6.3 grcpio==1.57.0`",issue still persist run environment except rocky instead success,issue,positive,positive,positive,positive,positive,positive
1693742925,"> 

thx! lol i am confused for 1min in my terminal when I do git pull.",confused min terminal git pull,issue,negative,negative,negative,negative,negative,negative
1693742823,@GeneDer pls address Alexey's comment and then assign Zhe w/ `tests-ok` once it looks good,address comment assign good,issue,negative,positive,positive,positive,positive,positive
1693734043,"@sihanwang41 `premerge` build failed (probably unrelated), but that's a blocking build so it can't be merged. Please rebase for latest fixes of other tests.",build probably unrelated blocking build ca please rebase latest,issue,negative,positive,positive,positive,positive,positive
1693686496,Looks no longer flaky - potentially fixed by stopping those back mac nodes.,longer flaky potentially fixed stopping back mac,issue,negative,positive,neutral,neutral,positive,positive
1693632116,"Add tests-ok lable, all failures are not related. The change is not touching any code.",add related change touching code,issue,negative,positive,positive,positive,positive,positive
1693629756,"@zhe-thoughts this one's good to merge
@matthewdeng / @ericl to stamp as well",one good merge stamp well,issue,positive,positive,positive,positive,positive,positive
1693619411,@sihanwang41 could you help check if this is `tests-ok`? Thanks!,could help check thanks,issue,positive,positive,positive,positive,positive,positive
1693605323,"Hi @zhe-thoughts could we merge this one too? It's a small change, and once it's merged we can more gracefully handle missing serve dependencies.",hi could merge one small change gracefully handle missing serve,issue,negative,negative,negative,negative,negative,negative
1693592107,"Memory profile isn't increasing as linearly as before, but it still looks sketchy:

![Screen Shot 2023-08-25 at 9 07 19 AM](https://github.com/ray-project/ray/assets/9871461/4a6ff0de-1a2a-4075-a0e8-912cdd0715d2)",memory profile increasing linearly still sketchy screen shot,issue,negative,neutral,neutral,neutral,neutral,neutral
1693560547,"Since this is a big change, @architkulkarni could you make a pass of tests and apply `tests-ok` after that? Thanks",since big change could make pas apply thanks,issue,negative,positive,neutral,neutral,positive,positive
1693489673,"Right, Dreamer imagines a trajectory under the hood. I was trying to evaluate this versus my home-baked implementation on an environment that supports rendering. If it's still relevant I can try fixing this.",right dreamer imago trajectory hood trying evaluate versus implementation environment rendering still relevant try fixing,issue,negative,positive,positive,positive,positive,positive
1693482737,"I'm having the same issue. I think the problem might be this:

From the [docs page](https://docs.ray.io/en/latest/rllib/rllib-algorithms.html#model-based-rl), ""Dreamer is an __image-only__ model-based RL method"". (emphasis mine)

So my guess is we are all getting this error when trying to use Dreamer with an environment that does not provide images.",issue think problem might page dreamer method emphasis mine guess getting error trying use dreamer environment provide,issue,negative,neutral,neutral,neutral,neutral,neutral
1693429234,@jjyao interesting... does this mean it's likely to also be some kind of memory fragmentation issue similar to what @iycheng saw in the GCS? We can try re-running the full serve repro using jemalloc to see if it also addresses that problem.,interesting mean likely also kind memory fragmentation issue similar saw try full serve see also problem,issue,positive,positive,positive,positive,positive,positive
1693426413,"@ddelange a release branch will be cut from master today (Friday, Aug 25th). So everything currently in master will be included.

Btw, we've added a basic Python 3.11 release test to avoid breakages now",release branch cut master today th everything currently master included added basic python release test avoid,issue,negative,neutral,neutral,neutral,neutral,neutral
1693293154,"Is it ""increase"" or ""allocated""? If this is true, when we submit 32 tasks, the driver memory should increase by 2GB< but  i don't think the ever happens",increase true submit driver memory increase think ever,issue,positive,positive,positive,positive,positive,positive
1693047816,Hi @edoakes :wave: will ray 2.7 release from `master` branch?,hi wave ray release master branch,issue,negative,neutral,neutral,neutral,neutral,neutral
1693046623,hi 👋 will this be part of the ray 2.7 release? or more generally: will ray 2.7 release from `master` branch?,hi part ray release generally ray release master branch,issue,negative,positive,neutral,neutral,positive,positive
1692961075,"Is that okay to compare the file timestamp and then reload?

According to [doc](https://docs.ray.io/en/latest/ray-observability/user-guides/debug-apps/general-debugging.html#outdated-function-definitions), this behavior may be caused by Serve's using `importlib` to load the deployment codes.",compare file reload according doc behavior may serve load deployment,issue,negative,neutral,neutral,neutral,neutral,neutral
1692909973,"Hi @woshiyyya , I don't think that's it. Documentation calls the `num_workers` as  number of workers (Ray actors) to launch.
I want 2 Actors. For the ScalingConfig I gave, I expect the ray tune to run across 2 nodes (each being one worker) and each worker/node has access to 2 GPUs. 

```
scaling_config = ScalingConfig(
        num_workers=gpus_per_trial, use_gpu=use_gpu, resources_per_worker={""CPU"": 2, ""GPU"":1}
    )
```
When I use the scaling config you suggest, and check the ray status:
```
Usage:
 5.0/128.0 CPU (5.0 used of 10.0 reserved in placement groups)
 2.0/4.0 GPU (2.0 used of 4.0 reserved in placement groups)
 0B/691.96GiB memory
 0B/300.55GiB object_store_memory

Demands:
 (no resource demands)

```

It's only using 2 GPUs, leaving out the other 2. When I use the following to try and use all 4:

```
scaling_config = ScalingConfig(
        # no of other nodes?
        num_workers=gpus_per_trial, use_gpu=use_gpu, resources_per_worker={""CPU"": 2, ""GPU"": 2}
    )
```


My run hangs.:
```
Trial status: 1 RUNNING | 1 PENDING
Current time: 2023-08-25 09:33:48. Total running time: 4min 34s
Logical resource usage: 5.0/128 CPUs, 4.0/4 GPUs (0.0/2.0 accelerator_type:G)

```

Ray Status Output:

```
Usage:
 5.0/128.0 CPU (5.0 used of 5.0 reserved in placement groups)
 4.0/4.0 GPU (4.0 used of 4.0 reserved in placement groups)
 0B/691.96GiB memory
 0B/300.55GiB object_store_memory

Demands:
 {'CPU': 1.0} * 1, {'CPU': 2.0, 'GPU': 2.0} * 2 (PACK): 1+ pending placement groups

```

Does Ray Tune and Ray Train in general not work for the multinode combined with multi GPU use case?",hi think documentation number ray launch want gave expect ray tune run across one worker access use scaling suggest check ray status usage used reserved placement used reserved placement memory resource leaving use following try use run trial status running pending current time total running time min logical resource usage ray status output usage used reserved placement used reserved placement memory pack pending placement ray tune ray train general work combined use case,issue,negative,positive,neutral,neutral,positive,positive
1692825258,"> I feel in general Ray doesn't enforce any limitations (not sure if it's a good thing or not): for example, we don't have limitations on custom resource name. So not sure what the limitations we should put on labels.

I can understand your meaning. However, I think it would be better to add parameter limitations at the beginning rather than having no restrictions.   
For example, if we need to add limitations later, there may be compatibility issues. If we add limitations now and later relax some of restrictions, it can ensure compatibility",feel general ray enforce sure good thing example custom resource name sure put understand meaning however think would better add parameter beginning rather example need add later may compatibility add later relax ensure compatibility,issue,positive,positive,positive,positive,positive,positive
1692802914,"I feel in general Ray doesn't enforce any limitations (not sure if it's a good thing or not): for example, we don't have limitations on custom resource name. So not sure what the limitations we should put on labels.",feel general ray enforce sure good thing example custom resource name sure put,issue,positive,positive,positive,positive,positive,positive
1692755265,"@zhe-thoughts yes, I think it's unrelated to my changes, but I will rebase and rebuild.",yes think unrelated rebase rebuild,issue,negative,neutral,neutral,neutral,neutral,neutral
1692730235,"<img width=""876"" alt=""Screenshot 2023-08-24 at 9 10 57 PM"" src=""https://github.com/ray-project/ray/assets/898023/c1ae8f2f-bd17-413e-b21f-2990dfa3e658"">

Seems using jemalloc fixed the ""memory leak""

```
import ray
from ray import serve

ray.init()

@ray.remote(concurrency_groups={""TEST_CONCURRENCY_GROUP"": 1}, runtime_env={""env_vars"": {
    ""LD_PRELOAD"": ""/usr/lib/x86_64-linux-gnu/libjemalloc.so.2""}})
class Executor:
    @ray.method(concurrency_group=""TEST_CONCURRENCY_GROUP"")
    async def hi(self):
        return ""hi""


@ray.remote
class Caller:
    def __init__(self, executors):
        self._executors = executors

        self._task = asyncio.create_task(self.run())

    async def run(self):
        for i in range(1000000):
            start = time.time()
            print(f""Starting iteration {i}!"")
            await asyncio.gather(*[self.do_calls() for _ in range(1000)])
            print(f""Iteration finished in {time.time()-start}s. Sleeping for 30s."")
            time.sleep(30)

    async def do_calls(self):
        for _ in range(100):
            result = await random.choice(self._executors).hi.remote()

executors = [Executor.remote() for _ in range(10)]
callers = [Caller.remote(executors) for _ in range(20)]

import time;time.sleep(1000000000)
```",fixed memory leak import ray ray import serve class executor hi self return hi class caller self run self range start print starting iteration await range print iteration finished sleeping self range result await range range import time,issue,negative,positive,neutral,neutral,positive,positive
1692699871,"See the metrics dropped back to normal on release tests. Keep this open for one more day to make sure it's consistent, before closing the issue.",see metric back normal release keep open one day make sure consistent issue,issue,negative,positive,positive,positive,positive,positive
1692679736,"If we first wait for the old sync, then launch a new sync in report, I think we should be uploading the most up to date artifacts.",first wait old sync launch new sync report think date,issue,negative,positive,positive,positive,positive,positive
1692669071,"> The code should be blocked at this point though, as we're waiting on the previous sync to finish up. What files are going to be missed?

There's no guarantee the previous sync actually picked up a file that was written just before we called report. The sync could have started long before that point.",code blocked point though waiting previous sync finish going guarantee previous sync actually picked file written report sync could long point,issue,negative,negative,neutral,neutral,negative,negative
1692666849,"The code should be blocked at this point though, as we're waiting on the previous sync to finish up. What files are going to be missed?",code blocked point though waiting previous sync finish going,issue,negative,negative,negative,negative,negative,negative
1692663854,"> -- wait on previous sync --> | launch new one | --- wait on the new one --> | let training continue

Unfortunately I think we have to do this one, otherwise you might miss some recently written files.",wait previous sync launch new one wait new one let training continue unfortunately think one otherwise might miss recently written,issue,negative,negative,neutral,neutral,negative,negative
1692657604,"> `sync_on_checkpoint` forces a blocking artifact sync to happen on trial checkpoints.

@ericl Should this instead force a WAIT on the previous task, then schedule a new one, and not necessarily block on the new one to finish?

```
-- wait on previous sync --> | launch new one | ---> let training continue

vs.

-- wait on previous sync --> | launch new one | --- wait on the new one --> | let training continue
```
",blocking artifact sync happen trial instead force wait previous task schedule new one necessarily block new one finish wait previous sync launch new one let training continue wait previous sync launch new one wait new one let training continue,issue,negative,positive,neutral,neutral,positive,positive
1692655535,"> Why do we need to have such limitations? @jjyao 
1. Limit the length of the string to avoid crashes or performance degradation caused by excessively long string.
2. Restrict character types to facilitate future expansion and avoid unpredictable issues caused by special characters.
3. Like  the rule of kubernetes (K8s) labels. [https://kubernetes.io/docs/concepts/overview/worki](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)
[ng-with-objects/labels/](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)


But i see our node_id length is 56, eg:`109c4a2ba1154a0959714e5b503ec475be669da3c625e482c8d2b1ed`.
We should increase this max length to 96 or 128 ?",need limit length string avoid performance degradation excessively long string restrict character facilitate future expansion avoid unpredictable special like rule see length increase length,issue,negative,positive,neutral,neutral,positive,positive
1692630835,Hi @zhe-thoughts - could you help review? This is to fix the release blocker. Ready to be merged.,hi could help review fix release blocker ready,issue,positive,positive,positive,positive,positive,positive
1692559128,"> 

I see, I believe dashboard test is not the issue. The PR with the feature of task profile also passes dashboard test https://buildkite.com/ray-project/oss-ci-build-pr/builds/33287#018a213c-c386-4560-bd9c-6c7d656d1689",see believe dashboard test issue feature task profile also dashboard test,issue,negative,neutral,neutral,neutral,neutral,neutral
1692558194,"Here's another example of an issue on `master` that gets fixed here:

![image](https://github.com/ray-project/ray/assets/14017872/01febe6c-96ed-48b3-965b-f43363c7549a)

and on the branch:

![image](https://github.com/ray-project/ray/assets/14017872/0327a78e-6752-4392-b8cb-8311bc9311af)",another example issue master fixed image branch image,issue,negative,positive,neutral,neutral,positive,positive
1692555620,"> @kevin85421 do you remember if I need to explicitly add the new file to some table of contents or sidebar, or will it automatically appear?

You should also update `_toc.yml` and `examples.md`.",remember need explicitly add new file table content automatically appear also update,issue,negative,positive,positive,positive,positive,positive
1692535938,"@zhe-thoughts I'm not sure why the commit message is not the PR title, I changed the message for the individual commit in this PR, could you check if the message is fixed now? (Tests are still running, just wanted to check first)",sure commit message title message individual commit could check message fixed still running check first,issue,positive,positive,positive,positive,positive,positive
1692528532,"@kevin85421 do you remember if I need to explicitly add the new file to some table of contents or sidebar, or will it automatically appear?",remember need explicitly add new file table content automatically appear,issue,negative,positive,positive,positive,positive,positive
1692491850,@edoakes or @architkulkarni : I need a committer to approve this PR. Thanks,need committer approve thanks,issue,negative,positive,positive,positive,positive,positive
1692459167,"@sihanwang41 ""per route"" part is currently broken due to the way we log those metrics unfortunately (current as before this PR). Part of the advantage for this PR is also so we don't need to deal with the broken routes 😅 ",per route part currently broken due way log metric unfortunately current part advantage also need deal broken,issue,negative,negative,negative,negative,negative,negative
1692456659,"For `Changed Serve Grafana dashboard to use per application instead of per route`,  what if we use fastapi and expose multiple routes inside the same application? Can you help to make sure it is not broken?

Oh, just see the Ed's comments. +1 on it.",serve dashboard use per application instead per route use expose multiple inside application help make sure broken oh see,issue,negative,positive,neutral,neutral,positive,positive
1692451940,"Thx Ed! Prefer to merge(very low risk), this will help to make our future pr being more safer to merge and stabilize release branch tests. ",prefer merge low risk help make future merge stabilize release branch,issue,negative,neutral,neutral,neutral,neutral,neutral
1692450096,"Two high-level suggestions:

1. Drop the gRPC method panes from Serve dashboard (if these breakdowns are needed, can click into Grafana).
2. Make both filter-by-route and filter-by-gRPC-method possible & opt-in in the grafana dashboard.",two drop method serve dashboard click make possible dashboard,issue,negative,neutral,neutral,neutral,neutral,neutral
1692443341,"actually we don't need this any more sorry, we can fix it on runtime side",actually need sorry fix side,issue,negative,negative,negative,negative,negative,negative
1692431985,"@zhe-thoughts just improving test stability, not sure if we want to merge such changes",improving test stability sure want merge,issue,positive,positive,positive,positive,positive,positive
1692415584,"<img width=""833"" alt=""Screenshot 2023-08-24 at 2 11 08 PM"" src=""https://github.com/ray-project/ray/assets/10713559/5eb640bb-d86b-4976-9295-cdd36e70477f"">

@zcin could you fix the commit message before I merge? Thanks",could fix commit message merge thanks,issue,positive,positive,positive,positive,positive,positive
1692387504,"I see this same problem also running on VMs (I am not on kubernetes like the original poster, so I'm not sure that @kevin85421 's explanation about `securityContext` explains it. I'm running the [Azure VM example setup](https://github.com/ray-project/ray/blob/eacc763c84d47c9c5b86b26a32fd62c685be84e6/python/ray/autoscaler/azure/example-full.yaml)).

I see jobs stuck in pending both when launched from the `ray job submit` CLI (as OP) and also from the Python SDK:

```py
import os
from ray.job_submission import JobSubmissionClient
from ray.runtime_env import RuntimeEnv

runtime_env = RuntimeEnv(
    container={
        ""image"": ""rayproject/ray:latest-cpu""
    }
)

client = JobSubmissionClient(os.environ.get(""RAY_ADDRESS"", ""http://127.0.0.1:8265""))
job_id = client.submit_job(
    entrypoint=""python batch_inference"", runtime_env=runtime_env
)
print(job_id)
```

### My use case

I have about a dozen different ML classifiers -- each with Python 3 entry points but otherwise having conflicting sets of dependencies (Python packages and other binaries such as, rarely, Octave) from each other.   My users want to specify _which_ model to use for batch predictions over their data.  Today (without Ray) each of these classifiers has its own Docker image.

I would be ok making tradeoffs such as distinct pools of worker nodes for each type of classification jobs (1 worker pool per Docker image) if that helps at all?  Most of the time only 1 job is running, but we do want to be able to launch jobs with a configurable container image on-the-fly.

This is the number-1 thing stopping me from using Ray.",see problem also running like original poster sure explanation running azure example setup see stuck pending ray job submit also python import o import import image client python print use case dozen different python entry otherwise conflicting python rarely octave want specify model use batch data today without ray docker image would making distinct worker type classification worker pool per docker image time job running want able launch container image thing stopping ray,issue,negative,positive,positive,positive,positive,positive
1692384349,"Hi @zhe-thoughts, I added a new commit https://github.com/ray-project/ray/pull/38647/commits/ec19d1556f06587f82b7d554ad71cfc6cfda566a, and I will let you know when the test finishes. Thanks!",hi added new commit let know test thanks,issue,positive,positive,positive,positive,positive,positive
1692378254,"> I will make a pr

the lint error is already fixed on master now.",make lint error already fixed master,issue,negative,positive,neutral,neutral,positive,positive
1692364309,"release test results shows no regression: https://buildkite.com/ray-project/release-tests-pr/builds/50647#_

Here is the markdown table without the standard deviation columns:

**Microbenchmark**
| Test | PR Mean | Master Mean |
|-|-|-|  
|single_client_get_calls_Plasma_Store|8191.397828897867|8754.54837558509|
|single_client_put_calls_Plasma_Store|5950.781351496489|5805.6190037114475|
|multi_client_put_calls_Plasma_Store|12508.674795978346|12328.904127041107|
|single_client_put_gigabytes|18.709760270942965|18.70292143153877|
|single_client_tasks_and_get_batch|9.183295069936973|10.13312476274177|
|multi_client_put_gigabytes|35.30873118551752|33.44696484008924|
|single_client_get_object_containing_10k_refs|12.009593215218109|12.572914297494414|
|single_client_wait_1k_refs|5.373579544066916|5.222760366810684|
|single_client_tasks_sync|1322.1138680734907|1250.6927703468014|
|single_client_tasks_async|10330.497540502094|10155.057998952925|
|multi_client_tasks_async|28766.259966864785|28610.611093226977| 
|1_1_actor_calls_sync|2296.3201099103444|2364.054996165639|
|1_1_actor_calls_async|8061.908989194427|7904.524280685511|
|1_1_actor_calls_concurrent|4929.390867491981|4651.124446037253|
|1_n_actor_calls_async|10377.759468470731|10422.447829835268|
|n_n_actor_calls_async|31616.114730587513|31448.210093781374|
|n_n_actor_calls_with_arg_async|3276.2797891334635|3314.5477356339725|  
|1_1_async_actor_calls_sync|1398.165194440429|1435.433361939216|
|1_1_async_actor_calls_async|2600.9835379147335|2663.710515913417|
|1_1_async_actor_calls_with_args_async|1806.7003521217032|2228.5100979164145|
|1_n_async_actor_calls_async|8816.314397378708|9130.414689243764|
|n_n_async_actor_calls_async|25491.383836229623|25352.533913754334|
|placement_group_create/removal|964.6617904534511|971.006710906266|
|client__get_calls|1149.1034030827395|1246.7998641205618|
|client__put_calls|868.4259530900624|856.0470825647293| 
|client__put_gigabytes|0.09294580970822773|0.09389291088347684|
|client__tasks_and_put_batch|11423.837284126172|11492.383790305477|
|client__1_1_actor_calls_sync|534.3120830074164|597.6050396035735|
|client__1_1_actor_calls_async|1069.6284836293883|1132.1865899393335|
|client__1_1_actor_calls_concurrent|1080.0613705402573|1140.4856726411672|
|client__tasks_and_get_batch|1.0587207412099382|1.032023820135421|


**stress_test_many_tasks**
Here is the table comparing the stage times between PR and Master:

| Test | PR | Master |
|-|-|-|
|stage_0_time|8.574087142944336|9.261889934539795|
|stage_1_time|231.2929391860962|241.75919651985168|
|stage_1_avg_iteration_time|23.1292822599411|24.175902938842775|
|stage_1_max_iteration_time|23.98018503189087|25.38587999343872|
|stage_1_min_iteration_time|22.175074577331543|22.179909706115723|
|stage_2_time|298.8414103984833|332.69430351257324|
|stage_2_avg_iteration_time|59.76809492111206|66.53853507041931|
|stage_2_max_iteration_time|60.468520402908325|67.55399942398071|
|stage_2_min_iteration_time|59.03271770477295|64.24040246009827|
|stage_3_creation_time|3.4297873973846436|3.596503734588623|
|stage_3_time|2694.396409511566|2872.1433601379395|
|stage_4_spread|0.7248670115643631|0.7298901387524407|",release test regression markdown table without standard deviation test mean master mean table stage time master test master,issue,negative,negative,negative,negative,negative,negative
1692362788,"Failed tests:
```


//python/ray/serve:test_http_headers                                    TIMEOUT in 3 out of 3 in 60.1s
--
  | Stats over 3 runs: max = 60.1s, min = 60.1s, avg = 60.1s, dev = 0.0s
  | C:/tmp/4lhdprva/execroot/com_github_ray_project_ray/bazel-out/x64_windows-opt/testlogs/python/ray/serve/test_http_headers/test.log
  | C:/tmp/4lhdprva/execroot/com_github_ray_project_ray/bazel-out/x64_windows-opt/testlogs/python/ray/serve/test_http_headers/test_attempts/attempt_1.log
  | C:/tmp/4lhdprva/execroot/com_github_ray_project_ray/bazel-out/x64_windows-opt/testlogs/python/ray/serve/test_http_headers/test_attempts/attempt_2.log
  | //python/ray/tests:test_cancel                                          TIMEOUT in 3 out of 3 in 300.1s
  | Stats over 3 runs: max = 300.1s, min = 300.1s, avg = 300.1s, dev = 0.0s
  | C:/tmp/4lhdprva/execroot/com_github_ray_project_ray/bazel-out/x64_windows-opt/testlogs/python/ray/tests/test_cancel/test.log
  | C:/tmp/4lhdprva/execroot/com_github_ray_project_ray/bazel-out/x64_windows-opt/testlogs/python/ray/tests/test_cancel/test_attempts/attempt_1.log
  | C:/tmp/4lhdprva/execroot/com_github_ray_project_ray/bazel-out/x64_windows-opt/testlogs/python/ray/tests/test_cancel/test_attempts/attempt_2.log
  | //python/ray/serve:test_autoscaling_policy                               FAILED in 3 out of 3 in 66.9s
  | Stats over 3 runs: max = 66.9s, min = 65.4s, avg = 66.2s, dev = 0.6s
  | C:/tmp/4lhdprva/execroot/com_github_ray_project_ray/bazel-out/x64_windows-opt/testlogs/python/ray/serve/test_autoscaling_policy/test.log
  | C:/tmp/4lhdprva/execroot/com_github_ray_project_ray/bazel-out/x64_windows-opt/testlogs/python/ray/serve/test_autoscaling_policy/test_attempts/attempt_1.log
  | C:/tmp/4lhdprva/execroot/com_github_ray_project_ray/bazel-out/x64_windows-opt/testlogs/python/ray/serve/test_autoscaling_policy/test_attempts/attempt_2.log


```

Unrelated, this is a docs-only change",min dev min dev min dev unrelated change,issue,negative,neutral,neutral,neutral,neutral,neutral
1692359452,this is required for 2.7 to fix the same deployment name issue. Also boost the UX in the dashboard.,fix deployment name issue also boost dashboard,issue,negative,neutral,neutral,neutral,neutral,neutral
1692351950,"Hi @f2010126 , I think it's the problem of scaling config, here you specified 

```python
scaling_config = ScalingConfig(
        num_workers=2, use_gpu=use_gpu, resources_per_worker={""CPU"": 2, ""GPU"": gpus_per_trial}
    )
```
This ScalingConfig is for per trial. It will try to allocate num_workers * gpus_per_trial GPUs for 1 trial. 

So the correct configuration would be 

```python
scaling_config = ScalingConfig(
        num_workers=gpus_per_trial, use_gpu=use_gpu, resources_per_worker={""CPU"": 2, ""GPU"":1}
    )
```

In this case, you will launch `num_samples` trials, each trial has `gpus_per_trial` workers, each worker has 1 GPU.",hi think problem scaling python per trial try allocate trial correct configuration would python case launch trial worker,issue,negative,neutral,neutral,neutral,neutral,neutral
1692349608,Please verify `tests-ok` and then I will review / merge. Thanks!,please verify review merge thanks,issue,positive,positive,positive,positive,positive,positive
1692348484,"@alanwguo or @sihanwang41 could you help clarify whether this is a must-have for 2.7?

Could you also verify this is `tests-ok` and ready for merging?",could help clarify whether could also verify ready,issue,positive,positive,positive,positive,positive,positive
1692312017,"There are some differences between this PR and the current `master` build. Normally this would be a bad thing because we aren't changing anything user-facing here, but so far I think this only _solves_ bugs in the docs. For example, switching to use `autodoc_mock_imports` fixed these type names. Here's the `master` build:

![rllib-models-master](https://github.com/ray-project/ray/assets/14017872/9be61067-2bde-4c5e-bc56-e5af92517721)

And here's this branch:

![rllib-models-PR](https://github.com/ray-project/ray/assets/14017872/5c1363cf-8f7b-4f35-a30b-a6e0eb74ed52)

I'm going to spend a bit longer scanning the differences to see if there's anything bad here. @angelinalg if you have time, I'd appreciate feedback as well.",current master build normally would bad thing anything far think example switching use fixed type master build branch going spend bit longer scanning see anything bad time appreciate feedback well,issue,negative,negative,negative,negative,negative,negative
1692292639,"> I don't believe we are intending to add vendor-specific resources directly to the Ray API--- the pattern followed should be similar to #37998 for all new accelerator types.

Thanks - my mistake. I'll pull out the API specific changes and align with the `neuron_cores` approach.",believe intending add directly ray pattern similar new accelerator thanks mistake pull specific align approach,issue,negative,positive,neutral,neutral,positive,positive
1692291386,"Thanks! Overall looks good, but looks like some tests are still failing, e.g. https://buildkite.com/ray-project/oss-ci-build-pr/builds/33562#018a28db-c51c-4654-936d-5ac96794a93a",thanks overall good like still failing,issue,positive,positive,positive,positive,positive,positive
1692240767,@alanwguo When you get a chance could you take another look?,get chance could take another look,issue,negative,neutral,neutral,neutral,neutral,neutral
1692215386,"Nice!

On Thu, Aug 24, 2023, 10:43 AM Stephanie Wang ***@***.***>
wrote:

> For a single-row block [{""field"": 100MB np.array}]:
>
> Before:
> In [11]: %timeit b.build()
> 385 ms ± 8.88 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
>
> After:
> In [13]: %timeit b.build()
> 116 µs ± 3.59 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/ray-project/ray/pull/38833#issuecomment-1692152255>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAADUSTVV4C5L4HF7YBA77LXW6HCXANCNFSM6AAAAAA35LQNKM>
> .
> You are receiving this because your review was requested.Message ID:
> ***@***.***>
>
",nice wang wrote block field per loop mean dev loop per loop mean dev reply directly view review id,issue,negative,positive,neutral,neutral,positive,positive
1692188624,This will be addressed by https://github.com/ray-project/ray/pull/38817. We shouldn't have to install e.g. pytorch just to build docs - we should only have to install it if we are actually going to use it. So the list of dependencies will get much smaller and simpler to solve for doc builds once this gets merged.,install build install actually going use list get much smaller simpler solve doc,issue,negative,neutral,neutral,neutral,neutral,neutral
1692183030,I don't believe we are intending to add vendor-specific resources directly to the Ray API--- the pattern followed should be similar to https://github.com/ray-project/ray/pull/37998 for all new accelerator types.,believe intending add directly ray pattern similar new accelerator,issue,negative,positive,neutral,neutral,positive,positive
1692152255,"For a single-row block `[{""field"": 100MB np.array}]`:

Before:
In [11]: %timeit b.build()
385 ms ± 8.88 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

After:
In [13]: %timeit b.build()
116 µs ± 3.59 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)
",block field per loop mean dev loop per loop mean dev,issue,negative,negative,negative,negative,negative,negative
1692127226,discovered during switching one of the release tests over; we're going to target this to be after branch cut,discovered switching one release going target branch cut,issue,negative,neutral,neutral,neutral,neutral,neutral
1692104284,Someone just broke lint so the PR is also not mergable until that is fixed (the premerge row is red),someone broke lint also fixed row red,issue,negative,positive,neutral,neutral,positive,positive
1692087633,"Ok, good with me. Here's what should remain after everything is all cleaned up:

```
train._internal.checkpoint_manager.CheckpointManager
train.Checkpoint
train._internal.session.TrainingResult
train._internal.session.FutureTrainingResult (hopefully temporary)
```",good remain everything hopefully temporary,issue,positive,positive,positive,positive,positive,positive
1692071666,"> 

It makes sense, but it's a bit confusing (why is a `checkpoint` a training result). We also use `trial.checkpoint` above, which returns the actual checkpoint and not the result.

Let's keep it as is for now (the whole situation is very confusing right now anyway - two different checkpoint, three checkpoint managers, training results, tracked checkpoints...). But I'd like to specifically make time to clean up the leftovers from the old code path.",sense bit training result also use actual result let keep whole situation right anyway two different three training tracked like specifically make time clean old code path,issue,positive,positive,positive,positive,positive,positive
1692057822,"Log still doesn't show up, hmm, I'll take this over for you",log still show take,issue,negative,neutral,neutral,neutral,neutral,neutral
1692047707,"Yeah, agree this is not a release blocker, more context is in https://github.com/ray-project/ray/issues/36044#issuecomment-1692044309 .",yeah agree release blocker context,issue,positive,neutral,neutral,neutral,neutral,neutral
1692044309,"The user impacted is able to work around, so it's not blocking them. btw this happens in the context of Ray Data+Train without streaming split integration. Do not think this is a release blocker for Ray 2.7 for now.",user impacted able work around blocking context ray without streaming split integration think release blocker ray,issue,negative,positive,positive,positive,positive,positive
1691980581,@edoakes Your welcome and thank you for helping with the PR and overall for the Ray project.,welcome thank helping overall ray project,issue,positive,positive,positive,positive,positive,positive
1691958156,"> @stephanie-wang could you help me understand why this is a release blocker (is it OK to include it in 2.8 instead?)

I don't think it should be a release blocker (there's no regression) but it is a bug in spilling behavior and was affecting some Data user workloads. I'll defer to @c21 about the latter.",could help understand release blocker include instead think release blocker regression bug behavior affecting data user defer latter,issue,negative,neutral,neutral,neutral,neutral,neutral
1691954621,"> Agreed this is release blocker
> 
> @stephanie-wang could you confirm that CI is OK, before I merge?

Yes the failures look unrelated!",agreed release blocker could confirm merge yes look unrelated,issue,positive,neutral,neutral,neutral,neutral,neutral
1691930332,@matthewdeng can you give a green light here before I merge?,give green light merge,issue,negative,positive,neutral,neutral,positive,positive
1691914116,Thanks @zhe-thoughts ! I'll let you know. Just waiting for tests now to all pass.,thanks let know waiting pas,issue,negative,positive,positive,positive,positive,positive
1691867493,@stephanie-wang could you help me understand why this is a release blocker (is it OK to include it in 2.8 instead?),could help understand release blocker include instead,issue,negative,neutral,neutral,neutral,neutral,neutral
1691863553,"@sven1977 @krfricke @kouroshHakha : could one of you confirm that CI is OK, before I merge?",could one confirm merge,issue,negative,neutral,neutral,neutral,neutral,neutral
1691858591,I'll wait for an approval from a Ray committer first. ,wait approval ray committer first,issue,negative,positive,positive,positive,positive,positive
1691794660,maybe fail a test intentionally and see if the log prints?,maybe fail test intentionally see log,issue,negative,negative,negative,negative,negative,negative
1691555692,"I'm running on a local install.

I checked again on the logs. They are located in (with different logs for various datetimes, but they're all the same in content):
'C:\Users\_________\AppData\Local\Temp\ray\session_2023-08-23_12-47-44_328823_34212\logs\...'

All the log files are empty apart from **gcs_server.out**. Here's the content:
<details>
<summary>\AppData\Local\Temp\ray\session_2023-08-23_12-47-44_328823_34212\logs\gcs_server.out</summary>
<pre>
  <code>
[2023-08-23 12:47:44,370 I 41356 40156] (gcs_server.exe) io_service_pool.cc:35: IOServicePool is running with 1 io_service.
[2023-08-23 12:47:44,378 I 41356 40156] (gcs_server.exe) event.cc:234: Set ray event level to warning
[2023-08-23 12:47:44,378 I 41356 40156] (gcs_server.exe) event.cc:342: Ray Event initialized for GCS
[2023-08-23 12:47:44,378 I 41356 40156] (gcs_server.exe) gcs_server.cc:74: GCS storage type is StorageType::IN_MEMORY
[2023-08-23 12:47:44,379 I 41356 40156] (gcs_server.exe) gcs_init_data.cc:44: Loading job table data.
[2023-08-23 12:47:44,379 I 41356 40156] (gcs_server.exe) gcs_init_data.cc:56: Loading node table data.
[2023-08-23 12:47:44,379 I 41356 40156] (gcs_server.exe) gcs_init_data.cc:68: Loading cluster resources table data.
[2023-08-23 12:47:44,379 I 41356 40156] (gcs_server.exe) gcs_init_data.cc:95: Loading actor table data.
[2023-08-23 12:47:44,380 I 41356 40156] (gcs_server.exe) gcs_init_data.cc:108: Loading actor task spec table data.
[2023-08-23 12:47:44,380 I 41356 40156] (gcs_server.exe) gcs_init_data.cc:81: Loading placement group table data.
[2023-08-23 12:47:44,380 I 41356 40156] (gcs_server.exe) gcs_init_data.cc:48: Finished loading job table data, size = 0
[2023-08-23 12:47:44,380 I 41356 40156] (gcs_server.exe) gcs_init_data.cc:60: Finished loading node table data, size = 0
[2023-08-23 12:47:44,380 I 41356 40156] (gcs_server.exe) gcs_init_data.cc:72: Finished loading cluster resources table data, size = 0
[2023-08-23 12:47:44,380 I 41356 40156] (gcs_server.exe) gcs_init_data.cc:99: Finished loading actor table data, size = 0
[2023-08-23 12:47:44,380 I 41356 40156] (gcs_server.exe) gcs_init_data.cc:112: Finished loading actor task spec table data, size = 0
[2023-08-23 12:47:44,380 I 41356 40156] (gcs_server.exe) gcs_init_data.cc:86: Finished loading placement group table data, size = 0
[2023-08-23 12:47:44,380 I 41356 40156] (gcs_server.exe) gcs_server.cc:164: No existing server cluster ID found. Generating new ID: a1d4cb0f4085e98a3f7e5ff3904e3fd0b747d5766d8861605bf34d4a
[2023-08-23 12:47:44,380 I 41356 40156] (gcs_server.exe) gcs_server.cc:658: Autoscaler V2 enabled: 0
[2023-08-23 12:47:44,381 I 41356 40156] (gcs_server.exe) grpc_server.cc:129: GcsServer server started, listening on port 64673.
[2023-08-23 12:47:44,426 I 41356 40156] (gcs_server.exe) gcs_server.cc:255: GcsNodeManager: 
- RegisterNode request count: 0
- DrainNode request count: 0
- GetAllNodeInfo request count: 0
- GetInternalConfig request count: 0

GcsActorManager: 
- RegisterActor request count: 0
- CreateActor request count: 0
- GetActorInfo request count: 0
- GetNamedActorInfo request count: 0
- GetAllActorInfo request count: 0
- KillActor request count: 0
- ListNamedActors request count: 0
- Registered actors count: 0
- Destroyed actors count: 0
- Named actors count: 0
- Unresolved actors count: 0
- Pending actors count: 0
- Created actors count: 0
- owners_: 0
- actor_to_register_callbacks_: 0
- actor_to_create_callbacks_: 0
- sorted_destroyed_actor_list_: 0

GcsResourceManager: 
- GetResources request count: 0
- GetAllAvailableResources request count0
- ReportResourceUsage request count: 0
- GetAllResourceUsage request count: 0

GcsPlacementGroupManager: 
- CreatePlacementGroup request count: 0
- RemovePlacementGroup request count: 0
- GetPlacementGroup request count: 0
- GetAllPlacementGroup request count: 0
- WaitPlacementGroupUntilReady request count: 0
- GetNamedPlacementGroup request count: 0
- Scheduling pending placement group count: 0
- Registered placement groups count: 0
- Named placement group count: 0
- Pending placement groups count: 0
- Infeasible placement groups count: 0

GcsPublisher {}

[runtime env manager] ID to URIs table:
[runtime env manager] URIs reference table:

GcsTaskManager: 
-Total num task events reported: 0
-Total num status task events dropped: 0
-Total num profile events dropped: 0
-Total num bytes of task event stored: 0MiB
-Current num of task events stored: 0
-Total num of actor creation tasks: 0
-Total num of actor tasks: 0
-Total num of normal tasks: 0
-Total num of driver tasks: 0


[2023-08-23 12:47:44,426 I 41356 40156] (gcs_server.exe) gcs_server.cc:872: Event stats:


Global stats: 30 total (16 active)
Queueing time: mean = 4.585 ms, max = 45.625 ms, min = 2.500 us, total = 137.562 ms
Execution time:  mean = 1.532 ms, total = 45.947 ms
Event stats:
	InternalKVGcsService.grpc_client.InternalKVPut - 6 total (6 active), CPU time: mean = 0.000 s, total = 0.000 s
	GcsInMemoryStore.GetAll - 6 total (0 active), CPU time: mean = 14.767 us, total = 88.600 us
	InternalKVGcsService.grpc_server.InternalKVPut - 6 total (4 active), CPU time: mean = 2.050 us, total = 12.300 us
	GcsInMemoryStore.Put - 5 total (2 active), CPU time: mean = 9.164 ms, total = 45.819 ms
	PeriodicalRunner.RunFnPeriodically - 4 total (2 active, 1 running), CPU time: mean = 1.325 us, total = 5.300 us
	RayletLoadPulled - 1 total (1 active), CPU time: mean = 0.000 s, total = 0.000 s
	UNKNOWN - 1 total (1 active), CPU time: mean = 0.000 s, total = 0.000 s
	GcsInMemoryStore.Get - 1 total (0 active), CPU time: mean = 22.600 us, total = 22.600 us


[2023-08-23 12:47:44,426 I 41356 40156] (gcs_server.exe) gcs_server.cc:873: GcsTaskManager Event stats:


Global stats: 0 total (0 active)
Queueing time: mean = -nan(ind) s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
Execution time:  mean = -nan(ind) s, total = 0.000 s
Event stats:


[2023-08-23 12:47:56,476 W 41356 6664] (gcs_server.exe) metric_exporter.cc:212: [1] Export metrics to agent failed: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details: . This won't affect Ray, but you can lose metrics from the cluster.
[2023-08-23 12:48:44,438 I 41356 40156] (gcs_server.exe) gcs_server.cc:255: GcsNodeManager: 
- RegisterNode request count: 0
- DrainNode request count: 0
- GetAllNodeInfo request count: 0
- GetInternalConfig request count: 0

GcsActorManager: 
- RegisterActor request count: 0
- CreateActor request count: 0
- GetActorInfo request count: 0
- GetNamedActorInfo request count: 0
- GetAllActorInfo request count: 0
- KillActor request count: 0
- ListNamedActors request count: 0
- Registered actors count: 0
- Destroyed actors count: 0
- Named actors count: 0
- Unresolved actors count: 0
- Pending actors count: 0
- Created actors count: 0
- owners_: 0
- actor_to_register_callbacks_: 0
- actor_to_create_callbacks_: 0
- sorted_destroyed_actor_list_: 0

GcsResourceManager: 
- GetResources request count: 0
- GetAllAvailableResources request count0
- ReportResourceUsage request count: 0
- GetAllResourceUsage request count: 0

GcsPlacementGroupManager: 
- CreatePlacementGroup request count: 0
- RemovePlacementGroup request count: 0
- GetPlacementGroup request count: 0
- GetAllPlacementGroup request count: 0
- WaitPlacementGroupUntilReady request count: 0
- GetNamedPlacementGroup request count: 0
- Scheduling pending placement group count: 0
- Registered placement groups count: 0
- Named placement group count: 0
- Pending placement groups count: 0
- Infeasible placement groups count: 0

GcsPublisher {}

[runtime env manager] ID to URIs table:
[runtime env manager] URIs reference table:

GcsTaskManager: 
-Total num task events reported: 0
-Total num status task events dropped: 0
-Total num profile events dropped: 0
-Total num bytes of task event stored: 0MiB
-Current num of task events stored: 0
-Total num of actor creation tasks: 0
-Total num of actor tasks: 0
-Total num of normal tasks: 0
-Total num of driver tasks: 0


[2023-08-23 12:48:44,438 I 41356 40156] (gcs_server.exe) gcs_server.cc:872: Event stats:


Global stats: 327 total (4 active)
Queueing time: mean = 2.524 ms, max = 45.625 ms, min = 2.030 us, total = 825.445 ms
Execution time:  mean = 193.377 us, total = 63.234 ms
Event stats:
	GcsInMemoryStore.Put - 79 total (0 active), CPU time: mean = 596.677 us, total = 47.137 ms
	InternalKVGcsService.grpc_server.InternalKVPut - 76 total (0 active), CPU time: mean = 10.321 us, total = 784.416 us
	InternalKVGcsService.grpc_client.InternalKVPut - 72 total (0 active), CPU time: mean = 5.984 us, total = 430.849 us
	RayletLoadPulled - 60 total (1 active), CPU time: mean = 4.130 us, total = 247.828 us
	UNKNOWN - 20 total (1 active), CPU time: mean = 6.982 us, total = 139.636 us
	GcsInMemoryStore.GetAll - 6 total (0 active), CPU time: mean = 14.767 us, total = 88.600 us
	GCSServer.deadline_timer.debug_state_dump - 6 total (1 active), CPU time: mean = 1.941 ms, total = 11.647 ms
	PeriodicalRunner.RunFnPeriodically - 4 total (0 active), CPU time: mean = 678.900 us, total = 2.716 ms
	GcsInMemoryStore.Get - 2 total (0 active), CPU time: mean = 14.127 us, total = 28.253 us
	InternalKVGcsService.grpc_server.InternalKVGet - 1 total (0 active), CPU time: mean = 14.884 us, total = 14.884 us
	GCSServer.deadline_timer.debug_state_event_stats_print - 1 total (1 active, 1 running), CPU time: mean = 0.000 s, total = 0.000 s


[2023-08-23 12:48:44,438 I 41356 40156] (gcs_server.exe) gcs_server.cc:873: GcsTaskManager Event stats:


Global stats: 0 total (0 active)
Queueing time: mean = -nan(ind) s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
Execution time:  mean = -nan(ind) s, total = 0.000 s
Event stats:


[2023-08-23 12:49:44,452 I 41356 40156] (gcs_server.exe) gcs_server.cc:255: GcsNodeManager: 
- RegisterNode request count: 0
- DrainNode request count: 0
- GetAllNodeInfo request count: 0
- GetInternalConfig request count: 0

GcsActorManager: 
- RegisterActor request count: 0
- CreateActor request count: 0
- GetActorInfo request count: 0
- GetNamedActorInfo request count: 0
- GetAllActorInfo request count: 0
- KillActor request count: 0
- ListNamedActors request count: 0
- Registered actors count: 0
- Destroyed actors count: 0
- Named actors count: 0
- Unresolved actors count: 0
- Pending actors count: 0
- Created actors count: 0
- owners_: 0
- actor_to_register_callbacks_: 0
- actor_to_create_callbacks_: 0
- sorted_destroyed_actor_list_: 0

GcsResourceManager: 
- GetResources request count: 0
- GetAllAvailableResources request count0
- ReportResourceUsage request count: 0
- GetAllResourceUsage request count: 0

GcsPlacementGroupManager: 
- CreatePlacementGroup request count: 0
- RemovePlacementGroup request count: 0
- GetPlacementGroup request count: 0
- GetAllPlacementGroup request count: 0
- WaitPlacementGroupUntilReady request count: 0
- GetNamedPlacementGroup request count: 0
- Scheduling pending placement group count: 0
- Registered placement groups count: 0
- Named placement group count: 0
- Pending placement groups count: 0
- Infeasible placement groups count: 0

GcsPublisher {}

[runtime env manager] ID to URIs table:
[runtime env manager] URIs reference table:

GcsTaskManager: 
-Total num task events reported: 0
-Total num status task events dropped: 0
-Total num profile events dropped: 0
-Total num bytes of task event stored: 0MiB
-Current num of task events stored: 0
-Total num of actor creation tasks: 0
-Total num of actor tasks: 0
-Total num of normal tasks: 0
-Total num of driver tasks: 0


[2023-08-23 12:49:44,452 I 41356 40156] (gcs_server.exe) gcs_server.cc:872: Event stats:


Global stats: 630 total (4 active)
Queueing time: mean = 2.401 ms, max = 45.625 ms, min = 2.030 us, total = 1.512 s
Execution time:  mean = 125.330 us, total = 78.958 ms
Event stats:
	GcsInMemoryStore.Put - 151 total (0 active), CPU time: mean = 313.577 us, total = 47.350 ms
	InternalKVGcsService.grpc_server.InternalKVPut - 148 total (0 active), CPU time: mean = 12.783 us, total = 1.892 ms
	InternalKVGcsService.grpc_client.InternalKVPut - 144 total (0 active), CPU time: mean = 6.774 us, total = 975.435 us
	RayletLoadPulled - 120 total (1 active), CPU time: mean = 4.320 us, total = 518.408 us
	UNKNOWN - 40 total (1 active), CPU time: mean = 6.561 us, total = 262.457 us
	GCSServer.deadline_timer.debug_state_dump - 12 total (1 active), CPU time: mean = 2.069 ms, total = 24.831 ms
	GcsInMemoryStore.GetAll - 6 total (0 active), CPU time: mean = 14.767 us, total = 88.600 us
	PeriodicalRunner.RunFnPeriodically - 4 total (0 active), CPU time: mean = 678.900 us, total = 2.716 ms
	GcsInMemoryStore.Get - 2 total (0 active), CPU time: mean = 14.127 us, total = 28.253 us
	GCSServer.deadline_timer.debug_state_event_stats_print - 2 total (1 active, 1 running), CPU time: mean = 140.755 us, total = 281.510 us
	InternalKVGcsService.grpc_server.InternalKVGet - 1 total (0 active), CPU time: mean = 14.884 us, total = 14.884 us


[2023-08-23 12:49:44,452 I 41356 40156] (gcs_server.exe) gcs_server.cc:873: GcsTaskManager Event stats:


Global stats: 0 total (0 active)
Queueing time: mean = -nan(ind) s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
Execution time:  mean = -nan(ind) s, total = 0.000 s
Event stats:


[2023-08-23 12:50:44,453 I 41356 40156] (gcs_server.exe) gcs_server.cc:255: GcsNodeManager: 
- RegisterNode request count: 0
- DrainNode request count: 0
- GetAllNodeInfo request count: 0
- GetInternalConfig request count: 0

GcsActorManager: 
- RegisterActor request count: 0
- CreateActor request count: 0
- GetActorInfo request count: 0
- GetNamedActorInfo request count: 0
- GetAllActorInfo request count: 0
- KillActor request count: 0
- ListNamedActors request count: 0
- Registered actors count: 0
- Destroyed actors count: 0
- Named actors count: 0
- Unresolved actors count: 0
- Pending actors count: 0
- Created actors count: 0
- owners_: 0
- actor_to_register_callbacks_: 0
- actor_to_create_callbacks_: 0
- sorted_destroyed_actor_list_: 0

GcsResourceManager: 
- GetResources request count: 0
- GetAllAvailableResources request count0
- ReportResourceUsage request count: 0
- GetAllResourceUsage request count: 0

GcsPlacementGroupManager: 
- CreatePlacementGroup request count: 0
- RemovePlacementGroup request count: 0
- GetPlacementGroup request count: 0
- GetAllPlacementGroup request count: 0
- WaitPlacementGroupUntilReady request count: 0
- GetNamedPlacementGroup request count: 0
- Scheduling pending placement group count: 0
- Registered placement groups count: 0
- Named placement group count: 0
- Pending placement groups count: 0
- Infeasible placement groups count: 0

GcsPublisher {}

[runtime env manager] ID to URIs table:
[runtime env manager] URIs reference table:

GcsTaskManager: 
-Total num task events reported: 0
-Total num status task events dropped: 0
-Total num profile events dropped: 0
-Total num bytes of task event stored: 0MiB
-Current num of task events stored: 0
-Total num of actor creation tasks: 0
-Total num of actor tasks: 0
-Total num of normal tasks: 0
-Total num of driver tasks: 0


[2023-08-23 12:50:44,453 I 41356 40156] (gcs_server.exe) gcs_server.cc:872: Event stats:


Global stats: 932 total (4 active)
Queueing time: mean = 2.255 ms, max = 45.625 ms, min = 2.030 us, total = 2.102 s
Execution time:  mean = 101.100 us, total = 94.225 ms
Event stats:
	GcsInMemoryStore.Put - 223 total (0 active), CPU time: mean = 213.287 us, total = 47.563 ms
	InternalKVGcsService.grpc_server.InternalKVPut - 220 total (0 active), CPU time: mean = 12.331 us, total = 2.713 ms
	InternalKVGcsService.grpc_client.InternalKVPut - 216 total (0 active), CPU time: mean = 6.376 us, total = 1.377 ms
	RayletLoadPulled - 179 total (1 active), CPU time: mean = 4.219 us, total = 755.142 us
	UNKNOWN - 60 total (1 active), CPU time: mean = 6.083 us, total = 364.977 us
	GCSServer.deadline_timer.debug_state_dump - 18 total (1 active), CPU time: mean = 2.113 ms, total = 38.035 ms
	GcsInMemoryStore.GetAll - 6 total (0 active), CPU time: mean = 14.767 us, total = 88.600 us
	PeriodicalRunner.RunFnPeriodically - 4 total (0 active), CPU time: mean = 678.900 us, total = 2.716 ms
	GCSServer.deadline_timer.debug_state_event_stats_print - 3 total (1 active, 1 running), CPU time: mean = 189.673 us, total = 569.020 us
	GcsInMemoryStore.Get - 2 total (0 active), CPU time: mean = 14.127 us, total = 28.253 us
	InternalKVGcsService.grpc_server.InternalKVGet - 1 total (0 active), CPU time: mean = 14.884 us, total = 14.884 us


[2023-08-23 12:50:44,453 I 41356 40156] (gcs_server.exe) gcs_server.cc:873: GcsTaskManager Event stats:


Global stats: 0 total (0 active)
Queueing time: mean = -nan(ind) s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
Execution time:  mean = -nan(ind) s, total = 0.000 s
Event stats:


[2023-08-23 12:51:44,460 I 41356 40156] (gcs_server.exe) gcs_server.cc:255: GcsNodeManager: 
- RegisterNode request count: 0
- DrainNode request count: 0
- GetAllNodeInfo request count: 0
- GetInternalConfig request count: 0

GcsActorManager: 
- RegisterActor request count: 0
- CreateActor request count: 0
- GetActorInfo request count: 0
- GetNamedActorInfo request count: 0
- GetAllActorInfo request count: 0
- KillActor request count: 0
- ListNamedActors request count: 0
- Registered actors count: 0
- Destroyed actors count: 0
- Named actors count: 0
- Unresolved actors count: 0
- Pending actors count: 0
- Created actors count: 0
- owners_: 0
- actor_to_register_callbacks_: 0
- actor_to_create_callbacks_: 0
- sorted_destroyed_actor_list_: 0

GcsResourceManager: 
- GetResources request count: 0
- GetAllAvailableResources request count0
- ReportResourceUsage request count: 0
- GetAllResourceUsage request count: 0

GcsPlacementGroupManager: 
- CreatePlacementGroup request count: 0
- RemovePlacementGroup request count: 0
- GetPlacementGroup request count: 0
- GetAllPlacementGroup request count: 0
- WaitPlacementGroupUntilReady request count: 0
- GetNamedPlacementGroup request count: 0
- Scheduling pending placement group count: 0
- Registered placement groups count: 0
- Named placement group count: 0
- Pending placement groups count: 0
- Infeasible placement groups count: 0

GcsPublisher {}

[runtime env manager] ID to URIs table:
[runtime env manager] URIs reference table:

GcsTaskManager: 
-Total num task events reported: 0
-Total num status task events dropped: 0
-Total num profile events dropped: 0
-Total num bytes of task event stored: 0MiB
-Current num of task events stored: 0
-Total num of actor creation tasks: 0
-Total num of actor tasks: 0
-Total num of normal tasks: 0
-Total num of driver tasks: 0


[2023-08-23 12:51:44,460 I 41356 40156] (gcs_server.exe) gcs_server.cc:872: Event stats:


Global stats: 1234 total (4 active)
Queueing time: mean = 3.752 ms, max = 947.978 ms, min = 1.525 us, total = 4.631 s
Execution time:  mean = 89.193 us, total = 110.064 ms
Event stats:
	GcsInMemoryStore.Put - 295 total (0 active), CPU time: mean = 162.026 us, total = 47.798 ms
	InternalKVGcsService.grpc_server.InternalKVPut - 292 total (0 active), CPU time: mean = 11.896 us, total = 3.474 ms
	InternalKVGcsService.grpc_client.InternalKVPut - 288 total (0 active), CPU time: mean = 6.330 us, total = 1.823 ms
	RayletLoadPulled - 238 total (1 active), CPU time: mean = 4.866 us, total = 1.158 ms
	UNKNOWN - 80 total (1 active), CPU time: mean = 6.067 us, total = 485.393 us
	GCSServer.deadline_timer.debug_state_dump - 24 total (1 active), CPU time: mean = 2.151 ms, total = 51.625 ms
	GcsInMemoryStore.GetAll - 6 total (0 active), CPU time: mean = 14.767 us, total = 88.600 us
	PeriodicalRunner.RunFnPeriodically - 4 total (0 active), CPU time: mean = 678.900 us, total = 2.716 ms
	GCSServer.deadline_timer.debug_state_event_stats_print - 4 total (1 active, 1 running), CPU time: mean = 213.333 us, total = 853.332 us
	GcsInMemoryStore.Get - 2 total (0 active), CPU time: mean = 14.127 us, total = 28.253 us
	InternalKVGcsService.grpc_server.InternalKVGet - 1 total (0 active), CPU time: mean = 14.884 us, total = 14.884 us


[2023-08-23 12:51:44,460 I 41356 40156] (gcs_server.exe) gcs_server.cc:873: GcsTaskManager Event stats:


Global stats: 0 total (0 active)
Queueing time: mean = -nan(ind) s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
Execution time:  mean = -nan(ind) s, total = 0.000 s
Event stats:


[2023-08-23 12:52:44,469 I 41356 40156] (gcs_server.exe) gcs_server.cc:255: GcsNodeManager: 
- RegisterNode request count: 0
- DrainNode request count: 0
- GetAllNodeInfo request count: 0
- GetInternalConfig request count: 0

GcsActorManager: 
- RegisterActor request count: 0
- CreateActor request count: 0
- GetActorInfo request count: 0
- GetNamedActorInfo request count: 0
- GetAllActorInfo request count: 0
- KillActor request count: 0
- ListNamedActors request count: 0
- Registered actors count: 0
- Destroyed actors count: 0
- Named actors count: 0
- Unresolved actors count: 0
- Pending actors count: 0
- Created actors count: 0
- owners_: 0
- actor_to_register_callbacks_: 0
- actor_to_create_callbacks_: 0
- sorted_destroyed_actor_list_: 0

GcsResourceManager: 
- GetResources request count: 0
- GetAllAvailableResources request count0
- ReportResourceUsage request count: 0
- GetAllResourceUsage request count: 0

GcsPlacementGroupManager: 
- CreatePlacementGroup request count: 0
- RemovePlacementGroup request count: 0
- GetPlacementGroup request count: 0
- GetAllPlacementGroup request count: 0
- WaitPlacementGroupUntilReady request count: 0
- GetNamedPlacementGroup request count: 0
- Scheduling pending placement group count: 0
- Registered placement groups count: 0
- Named placement group count: 0
- Pending placement groups count: 0
- Infeasible placement groups count: 0

GcsPublisher {}

[runtime env manager] ID to URIs table:
[runtime env manager] URIs reference table:

GcsTaskManager: 
-Total num task events reported: 0
-Total num status task events dropped: 0
-Total num profile events dropped: 0
-Total num bytes of task event stored: 0MiB
-Current num of task events stored: 0
-Total num of actor creation tasks: 0
-Total num of actor tasks: 0
-Total num of normal tasks: 0
-Total num of driver tasks: 0


[2023-08-23 12:52:44,469 I 41356 40156] (gcs_server.exe) gcs_server.cc:872: Event stats:


Global stats: 1536 total (4 active)
Queueing time: mean = 3.433 ms, max = 947.978 ms, min = 1.525 us, total = 5.273 s
Execution time:  mean = 81.996 us, total = 125.946 ms
Event stats:
	GcsInMemoryStore.Put - 367 total (0 active), CPU time: mean = 130.799 us, total = 48.003 ms
	InternalKVGcsService.grpc_server.InternalKVPut - 364 total (0 active), CPU time: mean = 11.959 us, total = 4.353 ms
	InternalKVGcsService.grpc_client.InternalKVPut - 360 total (0 active), CPU time: mean = 6.328 us, total = 2.278 ms
	RayletLoadPulled - 297 total (1 active), CPU time: mean = 5.624 us, total = 1.670 ms
	UNKNOWN - 100 total (1 active), CPU time: mean = 6.063 us, total = 606.347 us
	GCSServer.deadline_timer.debug_state_dump - 30 total (1 active), CPU time: mean = 2.170 ms, total = 65.088 ms
	GcsInMemoryStore.GetAll - 6 total (0 active), CPU time: mean = 14.767 us, total = 88.600 us
	GCSServer.deadline_timer.debug_state_event_stats_print - 5 total (1 active, 1 running), CPU time: mean = 219.841 us, total = 1.099 ms
	PeriodicalRunner.RunFnPeriodically - 4 total (0 active), CPU time: mean = 678.900 us, total = 2.716 ms
	GcsInMemoryStore.Get - 2 total (0 active), CPU time: mean = 14.127 us, total = 28.253 us
	InternalKVGcsService.grpc_server.InternalKVGet - 1 total (0 active), CPU time: mean = 14.884 us, total = 14.884 us


[2023-08-23 12:52:44,469 I 41356 40156] (gcs_server.exe) gcs_server.cc:873: GcsTaskManager Event stats:


Global stats: 0 total (0 active)
Queueing time: mean = -nan(ind) s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
Execution time:  mean = -nan(ind) s, total = 0.000 s
Event stats:


[2023-08-23 12:53:44,471 I 41356 40156] (gcs_server.exe) gcs_server.cc:255: GcsNodeManager: 
- RegisterNode request count: 0
- DrainNode request count: 0
- GetAllNodeInfo request count: 0
- GetInternalConfig request count: 0

GcsActorManager: 
- RegisterActor request count: 0
- CreateActor request count: 0
- GetActorInfo request count: 0
- GetNamedActorInfo request count: 0
- GetAllActorInfo request count: 0
- KillActor request count: 0
- ListNamedActors request count: 0
- Registered actors count: 0
- Destroyed actors count: 0
- Named actors count: 0
- Unresolved actors count: 0
- Pending actors count: 0
- Created actors count: 0
- owners_: 0
- actor_to_register_callbacks_: 0
- actor_to_create_callbacks_: 0
- sorted_destroyed_actor_list_: 0

GcsResourceManager: 
- GetResources request count: 0
- GetAllAvailableResources request count0
- ReportResourceUsage request count: 0
- GetAllResourceUsage request count: 0

GcsPlacementGroupManager: 
- CreatePlacementGroup request count: 0
- RemovePlacementGroup request count: 0
- GetPlacementGroup request count: 0
- GetAllPlacementGroup request count: 0
- WaitPlacementGroupUntilReady request count: 0
- GetNamedPlacementGroup request count: 0
- Scheduling pending placement group count: 0
- Registered placement groups count: 0
- Named placement group count: 0
- Pending placement groups count: 0
- Infeasible placement groups count: 0

GcsPublisher {}

[runtime env manager] ID to URIs table:
[runtime env manager] URIs reference table:

GcsTaskManager: 
-Total num task events reported: 0
-Total num status task events dropped: 0
-Total num profile events dropped: 0
-Total num bytes of task event stored: 0MiB
-Current num of task events stored: 0
-Total num of actor creation tasks: 0
-Total num of actor tasks: 0
-Total num of normal tasks: 0
-Total num of driver tasks: 0


[2023-08-23 12:53:44,471 I 41356 40156] (gcs_server.exe) gcs_server.cc:872: Event stats:


Global stats: 1839 total (4 active)
Queueing time: mean = 3.255 ms, max = 947.978 ms, min = -0.000 s, total = 5.986 s
Execution time:  mean = 77.704 us, total = 142.897 ms
Event stats:
	GcsInMemoryStore.Put - 439 total (0 active), CPU time: mean = 109.881 us, total = 48.238 ms
	InternalKVGcsService.grpc_server.InternalKVPut - 436 total (0 active), CPU time: mean = 12.038 us, total = 5.249 ms
	InternalKVGcsService.grpc_client.InternalKVPut - 432 total (0 active), CPU time: mean = 6.283 us, total = 2.714 ms
	RayletLoadPulled - 357 total (1 active), CPU time: mean = 5.926 us, total = 2.115 ms
	UNKNOWN - 120 total (1 active), CPU time: mean = 6.057 us, total = 726.807 us
	GCSServer.deadline_timer.debug_state_dump - 36 total (1 active), CPU time: mean = 2.209 ms, total = 79.537 ms
	GcsInMemoryStore.GetAll - 6 total (0 active), CPU time: mean = 14.767 us, total = 88.600 us
	GCSServer.deadline_timer.debug_state_event_stats_print - 6 total (1 active, 1 running), CPU time: mean = 244.923 us, total = 1.470 ms
	PeriodicalRunner.RunFnPeriodically - 4 total (0 active), CPU time: mean = 678.900 us, total = 2.716 ms
	GcsInMemoryStore.Get - 2 total (0 active), CPU time: mean = 14.127 us, total = 28.253 us
	InternalKVGcsService.grpc_server.InternalKVGet - 1 total (0 active), CPU time: mean = 14.884 us, total = 14.884 us


[2023-08-23 12:53:44,471 I 41356 40156] (gcs_server.exe) gcs_server.cc:873: GcsTaskManager Event stats:


Global stats: 0 total (0 active)
Queueing time: mean = -nan(ind) s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
Execution time:  mean = -nan(ind) s, total = 0.000 s
Event stats:


[2023-08-23 12:54:44,480 I 41356 40156] (gcs_server.exe) gcs_server.cc:255: GcsNodeManager: 
- RegisterNode request count: 0
- DrainNode request count: 0
- GetAllNodeInfo request count: 0
- GetInternalConfig request count: 0

GcsActorManager: 
- RegisterActor request count: 0
- CreateActor request count: 0
- GetActorInfo request count: 0
- GetNamedActorInfo request count: 0
- GetAllActorInfo request count: 0
- KillActor request count: 0
- ListNamedActors request count: 0
- Registered actors count: 0
- Destroyed actors count: 0
- Named actors count: 0
- Unresolved actors count: 0
- Pending actors count: 0
- Created actors count: 0
- owners_: 0
- actor_to_register_callbacks_: 0
- actor_to_create_callbacks_: 0
- sorted_destroyed_actor_list_: 0

GcsResourceManager: 
- GetResources request count: 0
- GetAllAvailableResources request count0
- ReportResourceUsage request count: 0
- GetAllResourceUsage request count: 0

GcsPlacementGroupManager: 
- CreatePlacementGroup request count: 0
- RemovePlacementGroup request count: 0
- GetPlacementGroup request count: 0
- GetAllPlacementGroup request count: 0
- WaitPlacementGroupUntilReady request count: 0
- GetNamedPlacementGroup request count: 0
- Scheduling pending placement group count: 0
- Registered placement groups count: 0
- Named placement group count: 0
- Pending placement groups count: 0
- Infeasible placement groups count: 0

GcsPublisher {}

[runtime env manager] ID to URIs table:
[runtime env manager] URIs reference table:

GcsTaskManager: 
-Total num task events reported: 0
-Total num status task events dropped: 0
-Total num profile events dropped: 0
-Total num bytes of task event stored: 0MiB
-Current num of task events stored: 0
-Total num of actor creation tasks: 0
-Total num of actor tasks: 0
-Total num of normal tasks: 0
-Total num of driver tasks: 0


[2023-08-23 12:54:44,480 I 41356 40156] (gcs_server.exe) gcs_server.cc:872: Event stats:


Global stats: 2141 total (4 active)
Queueing time: mean = 3.438 ms, max = 947.978 ms, min = -0.000 s, total = 7.360 s
Execution time:  mean = 74.130 us, total = 158.713 ms
Event stats:
	GcsInMemoryStore.Put - 511 total (0 active), CPU time: mean = 94.979 us, total = 48.534 ms
	InternalKVGcsService.grpc_server.InternalKVPut - 508 total (0 active), CPU time: mean = 12.276 us, total = 6.236 ms
	InternalKVGcsService.grpc_client.InternalKVPut - 504 total (0 active), CPU time: mean = 6.236 us, total = 3.143 ms
	RayletLoadPulled - 416 total (1 active), CPU time: mean = 6.363 us, total = 2.647 ms
	UNKNOWN - 140 total (1 active), CPU time: mean = 6.114 us, total = 856.024 us
	GCSServer.deadline_timer.debug_state_dump - 42 total (1 active), CPU time: mean = 2.208 ms, total = 92.735 ms
	GCSServer.deadline_timer.debug_state_event_stats_print - 7 total (1 active, 1 running), CPU time: mean = 245.003 us, total = 1.715 ms
	GcsInMemoryStore.GetAll - 6 total (0 active), CPU time: mean = 14.767 us, total = 88.600 us
	PeriodicalRunner.RunFnPeriodically - 4 total (0 active), CPU time: mean = 678.900 us, total = 2.716 ms
	GcsInMemoryStore.Get - 2 total (0 active), CPU time: mean = 14.127 us, total = 28.253 us
	InternalKVGcsService.grpc_server.InternalKVGet - 1 total (0 active), CPU time: mean = 14.884 us, total = 14.884 us


[2023-08-23 12:54:44,480 I 41356 40156] (gcs_server.exe) gcs_server.cc:873: GcsTaskManager Event stats:


Global stats: 0 total (0 active)
Queueing time: mean = -nan(ind) s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
Execution time:  mean = -nan(ind) s, total = 0.000 s
Event stats:


[2023-08-23 12:55:44,481 I 41356 40156] (gcs_server.exe) gcs_server.cc:255: GcsNodeManager: 
- RegisterNode request count: 0
- DrainNode request count: 0
- GetAllNodeInfo request count: 0
- GetInternalConfig request count: 0

GcsActorManager: 
- RegisterActor request count: 0
- CreateActor request count: 0
- GetActorInfo request count: 0
- GetNamedActorInfo request count: 0
- GetAllActorInfo request count: 0
- KillActor request count: 0
- ListNamedActors request count: 0
- Registered actors count: 0
- Destroyed actors count: 0
- Named actors count: 0
- Unresolved actors count: 0
- Pending actors count: 0
- Created actors count: 0
- owners_: 0
- actor_to_register_callbacks_: 0
- actor_to_create_callbacks_: 0
- sorted_destroyed_actor_list_: 0

GcsResourceManager: 
- GetResources request count: 0
- GetAllAvailableResources request count0
- ReportResourceUsage request count: 0
- GetAllResourceUsage request count: 0

GcsPlacementGroupManager: 
- CreatePlacementGroup request count: 0
- RemovePlacementGroup request count: 0
- GetPlacementGroup request count: 0
- GetAllPlacementGroup request count: 0
- WaitPlacementGroupUntilReady request count: 0
- GetNamedPlacementGroup request count: 0
- Scheduling pending placement group count: 0
- Registered placement groups count: 0
- Named placement group count: 0
- Pending placement groups count: 0
- Infeasible placement groups count: 0

GcsPublisher {}

[runtime env manager] ID to URIs table:
[runtime env manager] URIs reference table:

GcsTaskManager: 
-Total num task events reported: 0
-Total num status task events dropped: 0
-Total num profile events dropped: 0
-Total num bytes of task event stored: 0MiB
-Current num of task events stored: 0
-Total num of actor creation tasks: 0
-Total num of actor tasks: 0
-Total num of normal tasks: 0
-Total num of driver tasks: 0


[2023-08-23 12:55:44,481 I 41356 40156] (gcs_server.exe) gcs_server.cc:872: Event stats:


Global stats: 2442 total (4 active)
Queueing time: mean = 3.878 ms, max = 947.978 ms, min = -0.000 s, total = 9.469 s
Execution time:  mean = 71.971 us, total = 175.753 ms
Event stats:
	GcsInMemoryStore.Put - 583 total (0 active), CPU time: mean = 83.684 us, total = 48.788 ms
	InternalKVGcsService.grpc_server.InternalKVPut - 580 total (0 active), CPU time: mean = 12.577 us, total = 7.295 ms
	InternalKVGcsService.grpc_client.InternalKVPut - 576 total (0 active), CPU time: mean = 6.164 us, total = 3.551 ms
	RayletLoadPulled - 475 total (1 active), CPU time: mean = 6.485 us, total = 3.081 ms
	UNKNOWN - 159 total (1 active), CPU time: mean = 6.154 us, total = 978.420 us
	GCSServer.deadline_timer.debug_state_dump - 48 total (1 active), CPU time: mean = 2.232 ms, total = 107.138 ms
	GCSServer.deadline_timer.debug_state_event_stats_print - 8 total (1 active, 1 running), CPU time: mean = 259.461 us, total = 2.076 ms
	GcsInMemoryStore.GetAll - 6 total (0 active), CPU time: mean = 14.767 us, total = 88.600 us
	PeriodicalRunner.RunFnPeriodically - 4 total (0 active), CPU time: mean = 678.900 us, total = 2.716 ms
	GcsInMemoryStore.Get - 2 total (0 active), CPU time: mean = 14.127 us, total = 28.253 us
	InternalKVGcsService.grpc_server.InternalKVGet - 1 total (0 active), CPU time: mean = 14.884 us, total = 14.884 us


[2023-08-23 12:55:44,481 I 41356 40156] (gcs_server.exe) gcs_server.cc:873: GcsTaskManager Event stats:


Global stats: 0 total (0 active)
Queueing time: mean = -nan(ind) s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
Execution time:  mean = -nan(ind) s, total = 0.000 s
Event stats:


[2023-08-23 12:56:44,487 I 41356 40156] (gcs_server.exe) gcs_server.cc:255: GcsNodeManager: 
- RegisterNode request count: 0
- DrainNode request count: 0
- GetAllNodeInfo request count: 0
- GetInternalConfig request count: 0

GcsActorManager: 
- RegisterActor request count: 0
- CreateActor request count: 0
- GetActorInfo request count: 0
- GetNamedActorInfo request count: 0
- GetAllActorInfo request count: 0
- KillActor request count: 0
- ListNamedActors request count: 0
- Registered actors count: 0
- Destroyed actors count: 0
- Named actors count: 0
- Unresolved actors count: 0
- Pending actors count: 0
- Created actors count: 0
- owners_: 0
- actor_to_register_callbacks_: 0
- actor_to_create_callbacks_: 0
- sorted_destroyed_actor_list_: 0

GcsResourceManager: 
- GetResources request count: 0
- GetAllAvailableResources request count0
- ReportResourceUsage request count: 0
- GetAllResourceUsage request count: 0

GcsPlacementGroupManager: 
- CreatePlacementGroup request count: 0
- RemovePlacementGroup request count: 0
- GetPlacementGroup request count: 0
- GetAllPlacementGroup request count: 0
- WaitPlacementGroupUntilReady request count: 0
- GetNamedPlacementGroup request count: 0
- Scheduling pending placement group count: 0
- Registered placement groups count: 0
- Named placement group count: 0
- Pending placement groups count: 0
- Infeasible placement groups count: 0

GcsPublisher {}

[runtime env manager] ID to URIs table:
[runtime env manager] URIs reference table:

GcsTaskManager: 
-Total num task events reported: 0
-Total num status task events dropped: 0
-Total num profile events dropped: 0
-Total num bytes of task event stored: 0MiB
-Current num of task events stored: 0
-Total num of actor creation tasks: 0
-Total num of actor tasks: 0
-Total num of normal tasks: 0
-Total num of driver tasks: 0


[2023-08-23 12:56:44,487 I 41356 40156] (gcs_server.exe) gcs_server.cc:872: Event stats:


Global stats: 2744 total (4 active)
Queueing time: mean = 3.676 ms, max = 947.978 ms, min = -0.000 s, total = 10.087 s
Execution time:  mean = 70.326 us, total = 192.975 ms
Event stats:
	GcsInMemoryStore.Put - 655 total (0 active), CPU time: mean = 74.797 us, total = 48.992 ms
	InternalKVGcsService.grpc_server.InternalKVPut - 652 total (0 active), CPU time: mean = 12.812 us, total = 8.353 ms
	InternalKVGcsService.grpc_client.InternalKVPut - 648 total (0 active), CPU time: mean = 6.124 us, total = 3.969 ms
	RayletLoadPulled - 534 total (1 active), CPU time: mean = 6.752 us, total = 3.606 ms
	UNKNOWN - 179 total (1 active), CPU time: mean = 5.946 us, total = 1.064 ms
	GCSServer.deadline_timer.debug_state_dump - 54 total (1 active), CPU time: mean = 2.254 ms, total = 121.707 ms
	GCSServer.deadline_timer.debug_state_event_stats_print - 9 total (1 active, 1 running), CPU time: mean = 270.683 us, total = 2.436 ms
	GcsInMemoryStore.GetAll - 6 total (0 active), CPU time: mean = 14.767 us, total = 88.600 us
	PeriodicalRunner.RunFnPeriodically - 4 total (0 active), CPU time: mean = 678.900 us, total = 2.716 ms
	GcsInMemoryStore.Get - 2 total (0 active), CPU time: mean = 14.127 us, total = 28.253 us
	InternalKVGcsService.grpc_server.InternalKVGet - 1 total (0 active), CPU time: mean = 14.884 us, total = 14.884 us


[2023-08-23 12:56:44,487 I 41356 40156] (gcs_server.exe) gcs_server.cc:873: GcsTaskManager Event stats:


Global stats: 0 total (0 active)
Queueing time: mean = -nan(ind) s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
Execution time:  mean = -nan(ind) s, total = 0.000 s
Event stats:


[2023-08-23 12:57:44,494 I 41356 40156] (gcs_server.exe) gcs_server.cc:255: GcsNodeManager: 
- RegisterNode request count: 0
- DrainNode request count: 0
- GetAllNodeInfo request count: 0
- GetInternalConfig request count: 0

GcsActorManager: 
- RegisterActor request count: 0
- CreateActor request count: 0
- GetActorInfo request count: 0
- GetNamedActorInfo request count: 0
- GetAllActorInfo request count: 0
- KillActor request count: 0
- ListNamedActors request count: 0
- Registered actors count: 0
- Destroyed actors count: 0
- Named actors count: 0
- Unresolved actors count: 0
- Pending actors count: 0
- Created actors count: 0
- owners_: 0
- actor_to_register_callbacks_: 0
- actor_to_create_callbacks_: 0
- sorted_destroyed_actor_list_: 0

GcsResourceManager: 
- GetResources request count: 0
- GetAllAvailableResources request count0
- ReportResourceUsage request count: 0
- GetAllResourceUsage request count: 0

GcsPlacementGroupManager: 
- CreatePlacementGroup request count: 0
- RemovePlacementGroup request count: 0
- GetPlacementGroup request count: 0
- GetAllPlacementGroup request count: 0
- WaitPlacementGroupUntilReady request count: 0
- GetNamedPlacementGroup request count: 0
- Scheduling pending placement group count: 0
- Registered placement groups count: 0
- Named placement group count: 0
- Pending placement groups count: 0
- Infeasible placement groups count: 0

GcsPublisher {}

[runtime env manager] ID to URIs table:
[runtime env manager] URIs reference table:

GcsTaskManager: 
-Total num task events reported: 0
-Total num status task events dropped: 0
-Total num profile events dropped: 0
-Total num bytes of task event stored: 0MiB
-Current num of task events stored: 0
-Total num of actor creation tasks: 0
-Total num of actor tasks: 0
-Total num of normal tasks: 0
-Total num of driver tasks: 0


[2023-08-23 12:57:44,494 I 41356 40156] (gcs_server.exe) gcs_server.cc:872: Event stats:


Global stats: 3047 total (4 active)
Queueing time: mean = 3.504 ms, max = 947.978 ms, min = -0.000 s, total = 10.677 s
Execution time:  mean = 68.486 us, total = 208.677 ms
Event stats:
	GcsInMemoryStore.Put - 727 total (0 active), CPU time: mean = 67.757 us, total = 49.259 ms
	InternalKVGcsService.grpc_server.InternalKVPut - 724 total (0 active), CPU time: mean = 12.601 us, total = 9.123 ms
	InternalKVGcsService.grpc_client.InternalKVPut - 720 total (0 active), CPU time: mean = 6.199 us, total = 4.463 ms
	RayletLoadPulled - 594 total (1 active), CPU time: mean = 7.002 us, total = 4.159 ms
	UNKNOWN - 199 total (1 active), CPU time: mean = 5.849 us, total = 1.164 ms
	GCSServer.deadline_timer.debug_state_dump - 60 total (1 active), CPU time: mean = 2.249 ms, total = 134.948 ms
	GCSServer.deadline_timer.debug_state_event_stats_print - 10 total (1 active, 1 running), CPU time: mean = 271.241 us, total = 2.712 ms
	GcsInMemoryStore.GetAll - 6 total (0 active), CPU time: mean = 14.767 us, total = 88.600 us
	PeriodicalRunner.RunFnPeriodically - 4 total (0 active), CPU time: mean = 678.900 us, total = 2.716 ms
	GcsInMemoryStore.Get - 2 total (0 active), CPU time: mean = 14.127 us, total = 28.253 us
	InternalKVGcsService.grpc_server.InternalKVGet - 1 total (0 active), CPU time: mean = 14.884 us, total = 14.884 us


[2023-08-23 12:57:44,494 I 41356 40156] (gcs_server.exe) gcs_server.cc:873: GcsTaskManager Event stats:


Global stats: 0 total (0 active)
Queueing time: mean = -nan(ind) s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
Execution time:  mean = -nan(ind) s, total = 0.000 s
Event stats:


[2023-08-23 12:58:44,506 I 41356 40156] (gcs_server.exe) gcs_server.cc:255: GcsNodeManager: 
- RegisterNode request count: 0
- DrainNode request count: 0
- GetAllNodeInfo request count: 0
- GetInternalConfig request count: 0

GcsActorManager: 
- RegisterActor request count: 0
- CreateActor request count: 0
- GetActorInfo request count: 0
- GetNamedActorInfo request count: 0
- GetAllActorInfo request count: 0
- KillActor request count: 0
- ListNamedActors request count: 0
- Registered actors count: 0
- Destroyed actors count: 0
- Named actors count: 0
- Unresolved actors count: 0
- Pending actors count: 0
- Created actors count: 0
- owners_: 0
- actor_to_register_callbacks_: 0
- actor_to_create_callbacks_: 0
- sorted_destroyed_actor_list_: 0

GcsResourceManager: 
- GetResources request count: 0
- GetAllAvailableResources request count0
- ReportResourceUsage request count: 0
- GetAllResourceUsage request count: 0

GcsPlacementGroupManager: 
- CreatePlacementGroup request count: 0
- RemovePlacementGroup request count: 0
- GetPlacementGroup request count: 0
- GetAllPlacementGroup request count: 0
- WaitPlacementGroupUntilReady request count: 0
- GetNamedPlacementGroup request count: 0
- Scheduling pending placement group count: 0
- Registered placement groups count: 0
- Named placement group count: 0
- Pending placement groups count: 0
- Infeasible placement groups count: 0

GcsPublisher {}

[runtime env manager] ID to URIs table:
[runtime env manager] URIs reference table:

GcsTaskManager: 
-Total num task events reported: 0
-Total num status task events dropped: 0
-Total num profile events dropped: 0
-Total num bytes of task event stored: 0MiB
-Current num of task events stored: 0
-Total num of actor creation tasks: 0
-Total num of actor tasks: 0
-Total num of normal tasks: 0
-Total num of driver tasks: 0


[2023-08-23 12:58:44,507 I 41356 40156] (gcs_server.exe) gcs_server.cc:872: Event stats:


Global stats: 3350 total (4 active)
Queueing time: mean = 3.393 ms, max = 947.978 ms, min = -0.000 s, total = 11.368 s
Execution time:  mean = 67.045 us, total = 224.602 ms
Event stats:
	GcsInMemoryStore.Put - 799 total (0 active), CPU time: mean = 61.880 us, total = 49.442 ms
	InternalKVGcsService.grpc_server.InternalKVPut - 796 total (0 active), CPU time: mean = 12.332 us, total = 9.816 ms
	InternalKVGcsService.grpc_client.InternalKVPut - 792 total (0 active), CPU time: mean = 6.123 us, total = 4.850 ms
	RayletLoadPulled - 654 total (1 active), CPU time: mean = 7.114 us, total = 4.652 ms
	UNKNOWN - 219 total (1 active), CPU time: mean = 5.949 us, total = 1.303 ms
	GCSServer.deadline_timer.debug_state_dump - 66 total (1 active), CPU time: mean = 2.252 ms, total = 148.634 ms
	GCSServer.deadline_timer.debug_state_event_stats_print - 11 total (1 active, 1 running), CPU time: mean = 278.006 us, total = 3.058 ms
	GcsInMemoryStore.GetAll - 6 total (0 active), CPU time: mean = 14.767 us, total = 88.600 us
	PeriodicalRunner.RunFnPeriodically - 4 total (0 active), CPU time: mean = 678.900 us, total = 2.716 ms
	GcsInMemoryStore.Get - 2 total (0 active), CPU time: mean = 14.127 us, total = 28.253 us
	InternalKVGcsService.grpc_server.InternalKVGet - 1 total (0 active), CPU time: mean = 14.884 us, total = 14.884 us


[2023-08-23 12:58:44,507 I 41356 40156] (gcs_server.exe) gcs_server.cc:873: GcsTaskManager Event stats:


Global stats: 0 total (0 active)
Queueing time: mean = -nan(ind) s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
Execution time:  mean = -nan(ind) s, total = 0.000 s
Event stats:


[2023-08-23 12:59:44,520 I 41356 40156] (gcs_server.exe) gcs_server.cc:255: GcsNodeManager: 
- RegisterNode request count: 0
- DrainNode request count: 0
- GetAllNodeInfo request count: 0
- GetInternalConfig request count: 0

GcsActorManager: 
- RegisterActor request count: 0
- CreateActor request count: 0
- GetActorInfo request count: 0
- GetNamedActorInfo request count: 0
- GetAllActorInfo request count: 0
- KillActor request count: 0
- ListNamedActors request count: 0
- Registered actors count: 0
- Destroyed actors count: 0
- Named actors count: 0
- Unresolved actors count: 0
- Pending actors count: 0
- Created actors count: 0
- owners_: 0
- actor_to_register_callbacks_: 0
- actor_to_create_callbacks_: 0
- sorted_destroyed_actor_list_: 0

GcsResourceManager: 
- GetResources request count: 0
- GetAllAvailableResources request count0
- ReportResourceUsage request count: 0
- GetAllResourceUsage request count: 0

GcsPlacementGroupManager: 
- CreatePlacementGroup request count: 0
- RemovePlacementGroup request count: 0
- GetPlacementGroup request count: 0
- GetAllPlacementGroup request count: 0
- WaitPlacementGroupUntilReady request count: 0
- GetNamedPlacementGroup request count: 0
- Scheduling pending placement group count: 0
- Registered placement groups count: 0
- Named placement group count: 0
- Pending placement groups count: 0
- Infeasible placement groups count: 0

GcsPublisher {}

[runtime env manager] ID to URIs table:
[runtime env manager] URIs reference table:

GcsTaskManager: 
-Total num task events reported: 0
-Total num status task events dropped: 0
-Total num profile events dropped: 0
-Total num bytes of task event stored: 0MiB
-Current num of task events stored: 0
-Total num of actor creation tasks: 0
-Total num of actor tasks: 0
-Total num of normal tasks: 0
-Total num of driver tasks: 0


[2023-08-23 12:59:44,520 I 41356 40156] (gcs_server.exe) gcs_server.cc:872: Event stats:


Global stats: 3652 total (4 active)
Queueing time: mean = 3.297 ms, max = 947.978 ms, min = -0.000 s, total = 12.042 s
Execution time:  mean = 66.080 us, total = 241.323 ms
Event stats:
	GcsInMemoryStore.Put - 871 total (0 active), CPU time: mean = 57.015 us, total = 49.660 ms
	InternalKVGcsService.grpc_server.InternalKVPut - 868 total (0 active), CPU time: mean = 12.301 us, total = 10.677 ms
	InternalKVGcsService.grpc_client.InternalKVPut - 864 total (0 active), CPU time: mean = 6.180 us, total = 5.340 ms
	RayletLoadPulled - 713 total (1 active), CPU time: mean = 7.296 us, total = 5.202 ms
	UNKNOWN - 239 total (1 active), CPU time: mean = 5.855 us, total = 1.399 ms
	GCSServer.deadline_timer.debug_state_dump - 72 total (1 active), CPU time: mean = 2.262 ms, total = 162.888 ms
	GCSServer.deadline_timer.debug_state_event_stats_print - 12 total (1 active, 1 running), CPU time: mean = 275.748 us, total = 3.309 ms
	GcsInMemoryStore.GetAll - 6 total (0 active), CPU time: mean = 14.767 us, total = 88.600 us
	PeriodicalRunner.RunFnPeriodically - 4 total (0 active), CPU time: mean = 678.900 us, total = 2.716 ms
	GcsInMemoryStore.Get - 2 total (0 active), CPU time: mean = 14.127 us, total = 28.253 us
	InternalKVGcsService.grpc_server.InternalKVGet - 1 total (0 active), CPU time: mean = 14.884 us, total = 14.884 us


[2023-08-23 12:59:44,520 I 41356 40156] (gcs_server.exe) gcs_server.cc:873: GcsTaskManager Event stats:


Global stats: 0 total (0 active)
Queueing time: mean = -nan(ind) s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
Execution time:  mean = -nan(ind) s, total = 0.000 s
Event stats:


[2023-08-23 13:00:44,535 I 41356 40156] (gcs_server.exe) gcs_server.cc:255: GcsNodeManager: 
- RegisterNode request count: 0
- DrainNode request count: 0
- GetAllNodeInfo request count: 0
- GetInternalConfig request count: 0

GcsActorManager: 
- RegisterActor request count: 0
- CreateActor request count: 0
- GetActorInfo request count: 0
- GetNamedActorInfo request count: 0
- GetAllActorInfo request count: 0
- KillActor request count: 0
- ListNamedActors request count: 0
- Registered actors count: 0
- Destroyed actors count: 0
- Named actors count: 0
- Unresolved actors count: 0
- Pending actors count: 0
- Created actors count: 0
- owners_: 0
- actor_to_register_callbacks_: 0
- actor_to_create_callbacks_: 0
- sorted_destroyed_actor_list_: 0

GcsResourceManager: 
- GetResources request count: 0
- GetAllAvailableResources request count0
- ReportResourceUsage request count: 0
- GetAllResourceUsage request count: 0

GcsPlacementGroupManager: 
- CreatePlacementGroup request count: 0
- RemovePlacementGroup request count: 0
- GetPlacementGroup request count: 0
- GetAllPlacementGroup request count: 0
- WaitPlacementGroupUntilReady request count: 0
- GetNamedPlacementGroup request count: 0
- Scheduling pending placement group count: 0
- Registered placement groups count: 0
- Named placement group count: 0
- Pending placement groups count: 0
- Infeasible placement groups count: 0

GcsPublisher {}

[runtime env manager] ID to URIs table:
[runtime env manager] URIs reference table:

GcsTaskManager: 
-Total num task events reported: 0
-Total num status task events dropped: 0
-Total num profile events dropped: 0
-Total num bytes of task event stored: 0MiB
-Current num of task events stored: 0
-Total num of actor creation tasks: 0
-Total num of actor tasks: 0
-Total num of normal tasks: 0
-Total num of driver tasks: 0


[2023-08-23 13:00:44,535 I 41356 40156] (gcs_server.exe) gcs_server.cc:872: Event stats:


Global stats: 3955 total (4 active)
Queueing time: mean = 3.211 ms, max = 947.978 ms, min = -0.000 s, total = 12.700 s
Execution time:  mean = 65.336 us, total = 258.405 ms
Event stats:
	GcsInMemoryStore.Put - 943 total (0 active), CPU time: mean = 52.896 us, total = 49.881 ms
	InternalKVGcsService.grpc_server.InternalKVPut - 940 total (0 active), CPU time: mean = 12.316 us, total = 11.577 ms
	InternalKVGcsService.grpc_client.InternalKVPut - 936 total (0 active), CPU time: mean = 6.166 us, total = 5.771 ms
	RayletLoadPulled - 773 total (1 active), CPU time: mean = 7.344 us, total = 5.677 ms
	UNKNOWN - 259 total (1 active), CPU time: mean = 5.746 us, total = 1.488 ms
	GCSServer.deadline_timer.debug_state_dump - 78 total (1 active), CPU time: mean = 2.275 ms, total = 177.477 ms
	GCSServer.deadline_timer.debug_state_event_stats_print - 13 total (1 active, 1 running), CPU time: mean = 283.577 us, total = 3.687 ms
	GcsInMemoryStore.GetAll - 6 total (0 active), CPU time: mean = 14.767 us, total = 88.600 us
	PeriodicalRunner.RunFnPeriodically - 4 total (0 active), CPU time: mean = 678.900 us, total = 2.716 ms
	GcsInMemoryStore.Get - 2 total (0 active), CPU time: mean = 14.127 us, total = 28.253 us
	InternalKVGcsService.grpc_server.InternalKVGet - 1 total (0 active), CPU time: mean = 14.884 us, total = 14.884 us


[2023-08-23 13:00:44,535 I 41356 40156] (gcs_server.exe) gcs_server.cc:873: GcsTaskManager Event stats:


Global stats: 0 total (0 active)
Queueing time: mean = -nan(ind) s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
Execution time:  mean = -nan(ind) s, total = 0.000 s
Event stats:


[2023-08-23 13:01:44,537 I 41356 40156] (gcs_server.exe) gcs_server.cc:255: GcsNodeManager: 
- RegisterNode request count: 0
- DrainNode request count: 0
- GetAllNodeInfo request count: 0
- GetInternalConfig request count: 0

GcsActorManager: 
- RegisterActor request count: 0
- CreateActor request count: 0
- GetActorInfo request count: 0
- GetNamedActorInfo request count: 0
- GetAllActorInfo request count: 0
- KillActor request count: 0
- ListNamedActors request count: 0
- Registered actors count: 0
- Destroyed actors count: 0
- Named actors count: 0
- Unresolved actors count: 0
- Pending actors count: 0
- Created actors count: 0
- owners_: 0
- actor_to_register_callbacks_: 0
- actor_to_create_callbacks_: 0
- sorted_destroyed_actor_list_: 0

GcsResourceManager: 
- GetResources request count: 0
- GetAllAvailableResources request count0
- ReportResourceUsage request count: 0
- GetAllResourceUsage request count: 0

GcsPlacementGroupManager: 
- CreatePlacementGroup request count: 0
- RemovePlacementGroup request count: 0
- GetPlacementGroup request count: 0
- GetAllPlacementGroup request count: 0
- WaitPlacementGroupUntilReady request count: 0
- GetNamedPlacementGroup request count: 0
- Scheduling pending placement group count: 0
- Registered placement groups count: 0
- Named placement group count: 0
- Pending placement groups count: 0
- Infeasible placement groups count: 0

GcsPublisher {}

[runtime env manager] ID to URIs table:
[runtime env manager] URIs reference table:

GcsTaskManager: 
-Total num task events reported: 0
-Total num status task events dropped: 0
-Total num profile events dropped: 0
-Total num bytes of task event stored: 0MiB
-Current num of task events stored: 0
-Total num of actor creation tasks: 0
-Total num of actor tasks: 0
-Total num of normal tasks: 0
-Total num of driver tasks: 0


[2023-08-23 13:01:44,537 I 41356 40156] (gcs_server.exe) gcs_server.cc:872: Event stats:


Global stats: 4257 total (4 active)
Queueing time: mean = 3.137 ms, max = 947.978 ms, min = -0.000 s, total = 13.353 s
Execution time:  mean = 64.408 us, total = 274.186 ms
Event stats:
	GcsInMemoryStore.Put - 1015 total (0 active), CPU time: mean = 49.350 us, total = 50.090 ms
	InternalKVGcsService.grpc_server.InternalKVPut - 1012 total (0 active), CPU time: mean = 12.318 us, total = 12.466 ms
	InternalKVGcsService.grpc_client.InternalKVPut - 1008 total (0 active), CPU time: mean = 6.144 us, total = 6.194 ms
	RayletLoadPulled - 832 total (1 active), CPU time: mean = 7.333 us, total = 6.101 ms
	UNKNOWN - 279 total (1 active), CPU time: mean = 5.758 us, total = 1.606 ms
	GCSServer.deadline_timer.debug_state_dump - 84 total (1 active), CPU time: mean = 2.270 ms, total = 190.686 ms
	GCSServer.deadline_timer.debug_state_event_stats_print - 14 total (1 active, 1 running), CPU time: mean = 299.661 us, total = 4.195 ms
	GcsInMemoryStore.GetAll - 6 total (0 active), CPU time: mean = 14.767 us, total = 88.600 us
	PeriodicalRunner.RunFnPeriodically - 4 total (0 active), CPU time: mean = 678.900 us, total = 2.716 ms
	GcsInMemoryStore.Get - 2 total (0 active), CPU time: mean = 14.127 us, total = 28.253 us
	InternalKVGcsService.grpc_server.InternalKVGet - 1 total (0 active), CPU time: mean = 14.884 us, total = 14.884 us


[2023-08-23 13:01:44,537 I 41356 40156] (gcs_server.exe) gcs_server.cc:873: GcsTaskManager Event stats:


Global stats: 0 total (0 active)
Queueing time: mean = -nan(ind) s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
Execution time:  mean = -nan(ind) s, total = 0.000 s
Event stats:


[2023-08-23 13:02:44,553 I 41356 40156] (gcs_server.exe) gcs_server.cc:255: GcsNodeManager: 
- RegisterNode request count: 0
- DrainNode request count: 0
- GetAllNodeInfo request count: 0
- GetInternalConfig request count: 0

GcsActorManager: 
- RegisterActor request count: 0
- CreateActor request count: 0
- GetActorInfo request count: 0
- GetNamedActorInfo request count: 0
- GetAllActorInfo request count: 0
- KillActor request count: 0
- ListNamedActors request count: 0
- Registered actors count: 0
- Destroyed actors count: 0
- Named actors count: 0
- Unresolved actors count: 0
- Pending actors count: 0
- Created actors count: 0
- owners_: 0
- actor_to_register_callbacks_: 0
- actor_to_create_callbacks_: 0
- sorted_destroyed_actor_list_: 0

GcsResourceManager: 
- GetResources request count: 0
- GetAllAvailableResources request count0
- ReportResourceUsage request count: 0
- GetAllResourceUsage request count: 0

GcsPlacementGroupManager: 
- CreatePlacementGroup request count: 0
- RemovePlacementGroup request count: 0
- GetPlacementGroup request count: 0
- GetAllPlacementGroup request count: 0
- WaitPlacementGroupUntilReady request count: 0
- GetNamedPlacementGroup request count: 0
- Scheduling pending placement group count: 0
- Registered placement groups count: 0
- Named placement group count: 0
- Pending placement groups count: 0
- Infeasible placement groups count: 0

GcsPublisher {}

[runtime env manager] ID to URIs table:
[runtime env manager] URIs reference table:

GcsTaskManager: 
-Total num task events reported: 0
-Total num status task events dropped: 0
-Total num profile events dropped: 0
-Total num bytes of task event stored: 0MiB
-Current num of task events stored: 0
-Total num of actor creation tasks: 0
-Total num of actor tasks: 0
-Total num of normal tasks: 0
-Total num of driver tasks: 0


[2023-08-23 13:02:44,553 I 41356 40156] (gcs_server.exe) gcs_server.cc:872: Event stats:


Global stats: 4560 total (4 active)
Queueing time: mean = 3.076 ms, max = 947.978 ms, min = -0.000 s, total = 14.025 s
Execution time:  mean = 63.817 us, total = 291.008 ms
Event stats:
	GcsInMemoryStore.Put - 1087 total (0 active), CPU time: mean = 46.262 us, total = 50.287 ms
	InternalKVGcsService.grpc_server.InternalKVPut - 1084 total (0 active), CPU time: mean = 12.337 us, total = 13.374 ms
	InternalKVGcsService.grpc_client.InternalKVPut - 1080 total (0 active), CPU time: mean = 6.116 us, total = 6.605 ms
	RayletLoadPulled - 892 total (1 active), CPU time: mean = 7.261 us, total = 6.477 ms
	UNKNOWN - 299 total (1 active), CPU time: mean = 5.696 us, total = 1.703 ms
	GCSServer.deadline_timer.debug_state_dump - 90 total (1 active), CPU time: mean = 2.280 ms, total = 205.184 ms
	GCSServer.deadline_timer.debug_state_event_stats_print - 15 total (1 active, 1 running), CPU time: mean = 302.035 us, total = 4.531 ms
	GcsInMemoryStore.GetAll - 6 total (0 active), CPU time: mean = 14.767 us, total = 88.600 us
	PeriodicalRunner.RunFnPeriodically - 4 total (0 active), CPU time: mean = 678.900 us, total = 2.716 ms
	GcsInMemoryStore.Get - 2 total (0 active), CPU time: mean = 14.127 us, total = 28.253 us
	InternalKVGcsService.grpc_server.InternalKVGet - 1 total (0 active), CPU time: mean = 14.884 us, total = 14.884 us


[2023-08-23 13:02:44,553 I 41356 40156] (gcs_server.exe) gcs_server.cc:873: GcsTaskManager Event stats:


Global stats: 0 total (0 active)
Queueing time: mean = -nan(ind) s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
Execution time:  mean = -nan(ind) s, total = 0.000 s
Event stats:


[2023-08-23 13:03:44,559 I 41356 40156] (gcs_server.exe) gcs_server.cc:255: GcsNodeManager: 
- RegisterNode request count: 0
- DrainNode request count: 0
- GetAllNodeInfo request count: 0
- GetInternalConfig request count: 0

GcsActorManager: 
- RegisterActor request count: 0
- CreateActor request count: 0
- GetActorInfo request count: 0
- GetNamedActorInfo request count: 0
- GetAllActorInfo request count: 0
- KillActor request count: 0
- ListNamedActors request count: 0
- Registered actors count: 0
- Destroyed actors count: 0
- Named actors count: 0
- Unresolved actors count: 0
- Pending actors count: 0
- Created actors count: 0
- owners_: 0
- actor_to_register_callbacks_: 0
- actor_to_create_callbacks_: 0
- sorted_destroyed_actor_list_: 0

GcsResourceManager: 
- GetResources request count: 0
- GetAllAvailableResources request count0
- ReportResourceUsage request count: 0
- GetAllResourceUsage request count: 0

GcsPlacementGroupManager: 
- CreatePlacementGroup request count: 0
- RemovePlacementGroup request count: 0
- GetPlacementGroup request count: 0
- GetAllPlacementGroup request count: 0
- WaitPlacementGroupUntilReady request count: 0
- GetNamedPlacementGroup request count: 0
- Scheduling pending placement group count: 0
- Registered placement groups count: 0
- Named placement group count: 0
- Pending placement groups count: 0
- Infeasible placement groups count: 0

GcsPublisher {}

[runtime env manager] ID to URIs table:
[runtime env manager] URIs reference table:

GcsTaskManager: 
-Total num task events reported: 0
-Total num status task events dropped: 0
-Total num profile events dropped: 0
-Total num bytes of task event stored: 0MiB
-Current num of task events stored: 0
-Total num of actor creation tasks: 0
-Total num of actor tasks: 0
-Total num of normal tasks: 0
-Total num of driver tasks: 0


[2023-08-23 13:03:44,560 I 41356 40156] (gcs_server.exe) gcs_server.cc:872: Event stats:


Global stats: 4862 total (4 active)
Queueing time: mean = 3.027 ms, max = 947.978 ms, min = -0.000 s, total = 14.715 s
Execution time:  mean = 63.140 us, total = 306.987 ms
Event stats:
	GcsInMemoryStore.Put - 1159 total (0 active), CPU time: mean = 43.564 us, total = 50.490 ms
	InternalKVGcsService.grpc_server.InternalKVPut - 1156 total (0 active), CPU time: mean = 12.455 us, total = 14.398 ms
	InternalKVGcsService.grpc_client.InternalKVPut - 1152 total (0 active), CPU time: mean = 6.098 us, total = 7.025 ms
	RayletLoadPulled - 951 total (1 active), CPU time: mean = 7.204 us, total = 6.851 ms
	UNKNOWN - 319 total (1 active), CPU time: mean = 5.675 us, total = 1.810 ms
	GCSServer.deadline_timer.debug_state_dump - 96 total (1 active), CPU time: mean = 2.278 ms, total = 218.732 ms
	GCSServer.deadline_timer.debug_state_event_stats_print - 16 total (1 active, 1 running), CPU time: mean = 302.052 us, total = 4.833 ms
	GcsInMemoryStore.GetAll - 6 total (0 active), CPU time: mean = 14.767 us, total = 88.600 us
	PeriodicalRunner.RunFnPeriodically - 4 total (0 active), CPU time: mean = 678.900 us, total = 2.716 ms
	GcsInMemoryStore.Get - 2 total (0 active), CPU time: mean = 14.127 us, total = 28.253 us
	InternalKVGcsService.grpc_server.InternalKVGet - 1 total (0 active), CPU time: mean = 14.884 us, total = 14.884 us


[2023-08-23 13:03:44,560 I 41356 40156] (gcs_server.exe) gcs_server.cc:873: GcsTaskManager Event stats:


Global stats: 0 total (0 active)
Queueing time: mean = -nan(ind) s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
Execution time:  mean = -nan(ind) s, total = 0.000 s
Event stats:


[2023-08-23 13:04:44,573 I 41356 40156] (gcs_server.exe) gcs_server.cc:255: GcsNodeManager: 
- RegisterNode request count: 0
- DrainNode request count: 0
- GetAllNodeInfo request count: 0
- GetInternalConfig request count: 0

GcsActorManager: 
- RegisterActor request count: 0
- CreateActor request count: 0
- GetActorInfo request count: 0
- GetNamedActorInfo request count: 0
- GetAllActorInfo request count: 0
- KillActor request count: 0
- ListNamedActors request count: 0
- Registered actors count: 0
- Destroyed actors count: 0
- Named actors count: 0
- Unresolved actors count: 0
- Pending actors count: 0
- Created actors count: 0
- owners_: 0
- actor_to_register_callbacks_: 0
- actor_to_create_callbacks_: 0
- sorted_destroyed_actor_list_: 0

GcsResourceManager: 
- GetResources request count: 0
- GetAllAvailableResources request count0
- ReportResourceUsage request count: 0
- GetAllResourceUsage request count: 0

GcsPlacementGroupManager: 
- CreatePlacementGroup request count: 0
- RemovePlacementGroup request count: 0
- GetPlacementGroup request count: 0
- GetAllPlacementGroup request count: 0
- WaitPlacementGroupUntilReady request count: 0
- GetNamedPlacementGroup request count: 0
- Scheduling pending placement group count: 0
- Registered placement groups count: 0
- Named placement group count: 0
- Pending placement groups count: 0
- Infeasible placement groups count: 0

GcsPublisher {}

[runtime env manager] ID to URIs table:
[runtime env manager] URIs reference table:

GcsTaskManager: 
-Total num task events reported: 0
-Total num status task events dropped: 0
-Total num profile events dropped: 0
-Total num bytes of task event stored: 0MiB
-Current num of task events stored: 0
-Total num of actor creation tasks: 0
-Total num of actor tasks: 0
-Total num of normal tasks: 0
-Total num of driver tasks: 0


[2023-08-23 13:04:44,573 I 41356 40156] (gcs_server.exe) gcs_server.cc:872: Event stats:


Global stats: 5165 total (4 active)
Queueing time: mean = 2.972 ms, max = 947.978 ms, min = -0.000 s, total = 15.350 s
Execution time:  mean = 62.718 us, total = 323.936 ms
Event stats:
	GcsInMemoryStore.Put - 1231 total (0 active), CPU time: mean = 41.218 us, total = 50.740 ms
	InternalKVGcsService.grpc_server.InternalKVPut - 1228 total (0 active), CPU time: mean = 12.535 us, total = 15.393 ms
	InternalKVGcsService.grpc_client.InternalKVPut - 1224 total (0 active), CPU time: mean = 6.060 us, total = 7.417 ms
	RayletLoadPulled - 1011 total (1 active), CPU time: mean = 7.141 us, total = 7.220 ms
	UNKNOWN - 339 total (1 active), CPU time: mean = 5.631 us, total = 1.909 ms
	GCSServer.deadline_timer.debug_state_dump - 102 total (1 active), CPU time: mean = 2.286 ms, total = 233.182 ms
	GCSServer.deadline_timer.debug_state_event_stats_print - 17 total (1 active, 1 running), CPU time: mean = 307.563 us, total = 5.229 ms
	GcsInMemoryStore.GetAll - 6 total (0 active), CPU time: mean = 14.767 us, total = 88.600 us
	PeriodicalRunner.RunFnPeriodically - 4 total (0 active), CPU time: mean = 678.900 us, total = 2.716 ms
	GcsInMemoryStore.Get - 2 total (0 active), CPU time: mean = 14.127 us, total = 28.253 us
	InternalKVGcsService.grpc_server.InternalKVGet - 1 total (0 active), CPU time: mean = 14.884 us, total = 14.884 us


[2023-08-23 13:04:44,573 I 41356 40156] (gcs_server.exe) gcs_server.cc:873: GcsTaskManager Event stats:


Global stats: 0 total (0 active)
Queueing time: mean = -nan(ind) s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
Execution time:  mean = -nan(ind) s, total = 0.000 s
Event stats:


[2023-08-23 13:05:44,581 I 41356 40156] (gcs_server.exe) gcs_server.cc:255: GcsNodeManager: 
- RegisterNode request count: 0
- DrainNode request count: 0
- GetAllNodeInfo request count: 0
- GetInternalConfig request count: 0

GcsActorManager: 
- RegisterActor request count: 0
- CreateActor request count: 0
- GetActorInfo request count: 0
- GetNamedActorInfo request count: 0
- GetAllActorInfo request count: 0
- KillActor request count: 0
- ListNamedActors request count: 0
- Registered actors count: 0
- Destroyed actors count: 0
- Named actors count: 0
- Unresolved actors count: 0
- Pending actors count: 0
- Created actors count: 0
- owners_: 0
- actor_to_register_callbacks_: 0
- actor_to_create_callbacks_: 0
- sorted_destroyed_actor_list_: 0

GcsResourceManager: 
- GetResources request count: 0
- GetAllAvailableResources request count0
- ReportResourceUsage request count: 0
- GetAllResourceUsage request count: 0

GcsPlacementGroupManager: 
- CreatePlacementGroup request count: 0
- RemovePlacementGroup request count: 0
- GetPlacementGroup request count: 0
- GetAllPlacementGroup request count: 0
- WaitPlacementGroupUntilReady request count: 0
- GetNamedPlacementGroup request count: 0
- Scheduling pending placement group count: 0
- Registered placement groups count: 0
- Named placement group count: 0
- Pending placement groups count: 0
- Infeasible placement groups count: 0

GcsPublisher {}

[runtime env manager] ID to URIs table:
[runtime env manager] URIs reference table:

GcsTaskManager: 
-Total num task events reported: 0
-Total num status task events dropped: 0
-Total num profile events dropped: 0
-Total num bytes of task event stored: 0MiB
-Current num of task events stored: 0
-Total num of actor creation tasks: 0
-Total num of actor tasks: 0
-Total num of normal tasks: 0
-Total num of driver tasks: 0


[2023-08-23 13:05:44,581 I 41356 40156] (gcs_server.exe) gcs_server.cc:872: Event stats:


Global stats: 5468 total (4 active)
Queueing time: mean = 2.934 ms, max = 947.978 ms, min = -0.000 s, total = 16.042 s
Execution time:  mean = 62.177 us, total = 339.986 ms
Event stats:
	GcsInMemoryStore.Put - 1303 total (0 active), CPU time: mean = 39.127 us, total = 50.982 ms
	InternalKVGcsService.grpc_server.InternalKVPut - 1300 total (0 active), CPU time: mean = 12.550 us, total = 16.314 ms
	InternalKVGcsService.grpc_client.InternalKVPut - 1296 total (0 active), CPU time: mean = 6.093 us, total = 7.897 ms
	RayletLoadPulled - 1071 total (1 active), CPU time: mean = 7.089 us, total = 7.593 ms
	UNKNOWN - 359 total (1 active), CPU time: mean = 5.625 us, total = 2.019 ms
	GCSServer.deadline_timer.debug_state_dump - 108 total (1 active), CPU time: mean = 2.285 ms, total = 246.815 ms
	GCSServer.deadline_timer.debug_state_event_stats_print - 18 total (1 active, 1 running), CPU time: mean = 306.596 us, total = 5.519 ms
	GcsInMemoryStore.GetAll - 6 total (0 active), CPU time: mean = 14.767 us, total = 88.600 us
	PeriodicalRunner.RunFnPeriodically - 4 total (0 active), CPU time: mean = 678.900 us, total = 2.716 ms
	GcsInMemoryStore.Get - 2 total (0 active), CPU time: mean = 14.127 us, total = 28.253 us
	InternalKVGcsService.grpc_server.InternalKVGet - 1 total (0 active), CPU time: mean = 14.884 us, total = 14.884 us


[2023-08-23 13:05:44,581 I 41356 40156] (gcs_server.exe) gcs_server.cc:873: GcsTaskManager Event stats:


Global stats: 0 total (0 active)
Queueing time: mean = -nan(ind) s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
Execution time:  mean = -nan(ind) s, total = 0.000 s
Event stats:


[2023-08-23 13:06:44,582 I 41356 40156] (gcs_server.exe) gcs_server.cc:255: GcsNodeManager: 
- RegisterNode request count: 0
- DrainNode request count: 0
- GetAllNodeInfo request count: 0
- GetInternalConfig request count: 0

GcsActorManager: 
- RegisterActor request count: 0
- CreateActor request count: 0
- GetActorInfo request count: 0
- GetNamedActorInfo request count: 0
- GetAllActorInfo request count: 0
- KillActor request count: 0
- ListNamedActors request count: 0
- Registered actors count: 0
- Destroyed actors count: 0
- Named actors count: 0
- Unresolved actors count: 0
- Pending actors count: 0
- Created actors count: 0
- owners_: 0
- actor_to_register_callbacks_: 0
- actor_to_create_callbacks_: 0
- sorted_destroyed_actor_list_: 0

GcsResourceManager: 
- GetResources request count: 0
- GetAllAvailableResources request count0
- ReportResourceUsage request count: 0
- GetAllResourceUsage request count: 0

GcsPlacementGroupManager: 
- CreatePlacementGroup request count: 0
- RemovePlacementGroup request count: 0
- GetPlacementGroup request count: 0
- GetAllPlacementGroup request count: 0
- WaitPlacementGroupUntilReady request count: 0
- GetNamedPlacementGroup request count: 0
- Scheduling pending placement group count: 0
- Registered placement groups count: 0
- Named placement group count: 0
- Pending placement groups count: 0
- Infeasible placement groups count: 0

GcsPublisher {}

[runtime env manager] ID to URIs table:
[runtime env manager] URIs reference table:

GcsTaskManager: 
-Total num task events reported: 0
-Total num status task events dropped: 0
-Total num profile events dropped: 0
-Total num bytes of task event stored: 0MiB
-Current num of task events stored: 0
-Total num of actor creation tasks: 0
-Total num of actor tasks: 0
-Total num of normal tasks: 0
-Total num of driver tasks: 0


[2023-08-23 13:06:44,582 I 41356 40156] (gcs_server.exe) gcs_server.cc:872: Event stats:


Global stats: 5770 total (4 active)
Queueing time: mean = 2.952 ms, max = 947.978 ms, min = -0.000 s, total = 17.033 s
Execution time:  mean = 61.787 us, total = 356.512 ms
Event stats:
	GcsInMemoryStore.Put - 1375 total (0 active), CPU time: mean = 37.300 us, total = 51.287 ms
	InternalKVGcsService.grpc_server.InternalKVPut - 1372 total (0 active), CPU time: mean = 12.647 us, total = 17.352 ms
	InternalKVGcsService.grpc_client.InternalKVPut - 1368 total (0 active), CPU time: mean = 6.136 us, total = 8.394 ms
	RayletLoadPulled - 1130 total (1 active), CPU time: mean = 7.159 us, total = 8.090 ms
	UNKNOWN - 379 total (1 active), CPU time: mean = 5.664 us, total = 2.147 ms
	GCSServer.deadline_timer.debug_state_dump - 114 total (1 active), CPU time: mean = 2.285 ms, total = 260.532 ms
	GCSServer.deadline_timer.debug_state_event_stats_print - 19 total (1 active, 1 running), CPU time: mean = 308.602 us, total = 5.863 ms
	GcsInMemoryStore.GetAll - 6 total (0 active), CPU time: mean = 14.767 us, total = 88.600 us
	PeriodicalRunner.RunFnPeriodically - 4 total (0 active), CPU time: mean = 678.900 us, total = 2.716 ms
	GcsInMemoryStore.Get - 2 total (0 active), CPU time: mean = 14.127 us, total = 28.253 us
	InternalKVGcsService.grpc_server.InternalKVGet - 1 total (0 active), CPU time: mean = 14.884 us, total = 14.884 us


[2023-08-23 13:06:44,582 I 41356 40156] (gcs_server.exe) gcs_server.cc:873: GcsTaskManager Event stats:


Global stats: 0 total (0 active)
Queueing time: mean = -nan(ind) s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
Execution time:  mean = -nan(ind) s, total = 0.000 s
Event stats:


[2023-08-23 13:07:44,583 I 41356 40156] (gcs_server.exe) gcs_server.cc:255: GcsNodeManager: 
- RegisterNode request count: 0
- DrainNode request count: 0
- GetAllNodeInfo request count: 0
- GetInternalConfig request count: 0

GcsActorManager: 
- RegisterActor request count: 0
- CreateActor request count: 0
- GetActorInfo request count: 0
- GetNamedActorInfo request count: 0
- GetAllActorInfo request count: 0
- KillActor request count: 0
- ListNamedActors request count: 0
- Registered actors count: 0
- Destroyed actors count: 0
- Named actors count: 0
- Unresolved actors count: 0
- Pending actors count: 0
- Created actors count: 0
- owners_: 0
- actor_to_register_callbacks_: 0
- actor_to_create_callbacks_: 0
- sorted_destroyed_actor_list_: 0

GcsResourceManager: 
- GetResources request count: 0
- GetAllAvailableResources request count0
- ReportResourceUsage request count: 0
- GetAllResourceUsage request count: 0

GcsPlacementGroupManager: 
- CreatePlacementGroup request count: 0
- RemovePlacementGroup request count: 0
- GetPlacementGroup request count: 0
- GetAllPlacementGroup request count: 0
- WaitPlacementGroupUntilReady request count: 0
- GetNamedPlacementGroup request count: 0
- Scheduling pending placement group count: 0
- Registered placement groups count: 0
- Named placement group count: 0
- Pending placement groups count: 0
- Infeasible placement groups count: 0

GcsPublisher {}

[runtime env manager] ID to URIs table:
[runtime env manager] URIs reference table:

GcsTaskManager: 
-Total num task events reported: 0
-Total num status task events dropped: 0
-Total num profile events dropped: 0
-Total num bytes of task event stored: 0MiB
-Current num of task events stored: 0
-Total num of actor creation tasks: 0
-Total num of actor tasks: 0
-Total num of normal tasks: 0
-Total num of driver tasks: 0


[2023-08-23 13:07:44,583 I 41356 40156] (gcs_server.exe) gcs_server.cc:872: Event stats:


Global stats: 6072 total (4 active)
Queueing time: mean = 3.045 ms, max = 947.978 ms, min = -0.000 s, total = 18.490 s
Execution time:  mean = 61.234 us, total = 371.811 ms
Event stats:
	GcsInMemoryStore.Put - 1447 total (0 active), CPU time: mean = 35.627 us, total = 51.553 ms
	InternalKVGcsService.grpc_server.InternalKVPut - 1444 total (0 active), CPU time: mean = 12.616 us, total = 18.218 ms
	InternalKVGcsService.grpc_client.InternalKVPut - 1440 total (0 active), CPU time: mean = 6.178 us, total = 8.897 ms
	RayletLoadPulled - 1189 total (1 active), CPU time: mean = 7.169 us, total = 8.523 ms
	UNKNOWN - 399 total (1 active), CPU time: mean = 5.698 us, total = 2.273 ms
	GCSServer.deadline_timer.debug_state_dump - 120 total (1 active), CPU time: mean = 2.278 ms, total = 273.393 ms
	GCSServer.deadline_timer.debug_state_event_stats_print - 20 total (1 active, 1 running), CPU time: mean = 305.334 us, total = 6.107 ms
	GcsInMemoryStore.GetAll - 6 total (0 active), CPU time: mean = 14.767 us, total = 88.600 us
	PeriodicalRunner.RunFnPeriodically - 4 total (0 active), CPU time: mean = 678.900 us, total = 2.716 ms
	GcsInMemoryStore.Get - 2 total (0 active), CPU time: mean = 14.127 us, total = 28.253 us
	InternalKVGcsService.grpc_server.InternalKVGet - 1 total (0 active), CPU time: mean = 14.884 us, total = 14.884 us


[2023-08-23 13:07:44,583 I 41356 40156] (gcs_server.exe) gcs_server.cc:873: GcsTaskManager Event stats:


Global stats: 0 total (0 active)
Queueing time: mean = -nan(ind) s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
Execution time:  mean = -nan(ind) s, total = 0.000 s
Event stats:


[2023-08-23 13:08:44,587 I 41356 40156] (gcs_server.exe) gcs_server.cc:255: GcsNodeManager: 
- RegisterNode request count: 0
- DrainNode request count: 0
- GetAllNodeInfo request count: 0
- GetInternalConfig request count: 0

GcsActorManager: 
- RegisterActor request count: 0
- CreateActor request count: 0
- GetActorInfo request count: 0
- GetNamedActorInfo request count: 0
- GetAllActorInfo request count: 0
- KillActor request count: 0
- ListNamedActors request count: 0
- Registered actors count: 0
- Destroyed actors count: 0
- Named actors count: 0
- Unresolved actors count: 0
- Pending actors count: 0
- Created actors count: 0
- owners_: 0
- actor_to_register_callbacks_: 0
- actor_to_create_callbacks_: 0
- sorted_destroyed_actor_list_: 0

GcsResourceManager: 
- GetResources request count: 0
- GetAllAvailableResources request count0
- ReportResourceUsage request count: 0
- GetAllResourceUsage request count: 0

GcsPlacementGroupManager: 
- CreatePlacementGroup request count: 0
- RemovePlacementGroup request count: 0
- GetPlacementGroup request count: 0
- GetAllPlacementGroup request count: 0
- WaitPlacementGroupUntilReady request count: 0
- GetNamedPlacementGroup request count: 0
- Scheduling pending placement group count: 0
- Registered placement groups count: 0
- Named placement group count: 0
- Pending placement groups count: 0
- Infeasible placement groups count: 0

GcsPublisher {}

[runtime env manager] ID to URIs table:
[runtime env manager] URIs reference table:

GcsTaskManager: 
-Total num task events reported: 0
-Total num status task events dropped: 0
-Total num profile events dropped: 0
-Total num bytes of task event stored: 0MiB
-Current num of task events stored: 0
-Total num of actor creation tasks: 0
-Total num of actor tasks: 0
-Total num of normal tasks: 0
-Total num of driver tasks: 0


[2023-08-23 13:08:44,587 I 41356 40156] (gcs_server.exe) gcs_server.cc:872: Event stats:


Global stats: 6374 total (4 active)
Queueing time: mean = 3.181 ms, max = 947.978 ms, min = -0.001 s, total = 20.275 s
Execution time:  mean = 61.087 us, total = 389.370 ms
Event stats:
	GcsInMemoryStore.Put - 1519 total (0 active), CPU time: mean = 34.233 us, total = 52.000 ms
	InternalKVGcsService.grpc_server.InternalKVPut - 1516 total (0 active), CPU time: mean = 12.729 us, total = 19.297 ms
	InternalKVGcsService.grpc_client.InternalKVPut - 1512 total (0 active), CPU time: mean = 6.244 us, total = 9.441 ms
	RayletLoadPulled - 1248 total (1 active), CPU time: mean = 7.367 us, total = 9.195 ms
	UNKNOWN - 419 total (1 active), CPU time: mean = 5.728 us, total = 2.400 ms
	GCSServer.deadline_timer.debug_state_dump - 126 total (1 active), CPU time: mean = 2.284 ms, total = 287.756 ms
	GCSServer.deadline_timer.debug_state_event_stats_print - 21 total (1 active, 1 running), CPU time: mean = 306.433 us, total = 6.435 ms
	GcsInMemoryStore.GetAll - 6 total (0 active), CPU time: mean = 14.767 us, total = 88.600 us
	PeriodicalRunner.RunFnPeriodically - 4 total (0 active), CPU time: mean = 678.900 us, total = 2.716 ms
	GcsInMemoryStore.Get - 2 total (0 active), CPU time: mean = 14.127 us, total = 28.253 us
	InternalKVGcsService.grpc_server.InternalKVGet - 1 total (0 active), CPU time: mean = 14.884 us, total = 14.884 us


[2023-08-23 13:08:44,587 I 41356 40156] (gcs_server.exe) gcs_server.cc:873: GcsTaskManager Event stats:


Global stats: 0 total (0 active)
Queueing time: mean = -nan(ind) s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
Execution time:  mean = -nan(ind) s, total = 0.000 s
Event stats:


[2023-08-23 13:09:44,591 I 41356 40156] (gcs_server.exe) gcs_server.cc:255: GcsNodeManager: 
- RegisterNode request count: 0
- DrainNode request count: 0
- GetAllNodeInfo request count: 0
- GetInternalConfig request count: 0

GcsActorManager: 
- RegisterActor request count: 0
- CreateActor request count: 0
- GetActorInfo request count: 0
- GetNamedActorInfo request count: 0
- GetAllActorInfo request count: 0
- KillActor request count: 0
- ListNamedActors request count: 0
- Registered actors count: 0
- Destroyed actors count: 0
- Named actors count: 0
- Unresolved actors count: 0
- Pending actors count: 0
- Created actors count: 0
- owners_: 0
- actor_to_register_callbacks_: 0
- actor_to_create_callbacks_: 0
- sorted_destroyed_actor_list_: 0

GcsResourceManager: 
- GetResources request count: 0
- GetAllAvailableResources request count0
- ReportResourceUsage request count: 0
- GetAllResourceUsage request count: 0

GcsPlacementGroupManager: 
- CreatePlacementGroup request count: 0
- RemovePlacementGroup request count: 0
- GetPlacementGroup request count: 0
- GetAllPlacementGroup request count: 0
- WaitPlacementGroupUntilReady request count: 0
- GetNamedPlacementGroup request count: 0
- Scheduling pending placement group count: 0
- Registered placement groups count: 0
- Named placement group count: 0
- Pending placement groups count: 0
- Infeasible placement groups count: 0

GcsPublisher {}

[runtime env manager] ID to URIs table:
[runtime env manager] URIs reference table:

GcsTaskManager: 
-Total num task events reported: 0
-Total num status task events dropped: 0
-Total num profile events dropped: 0
-Total num bytes of task event stored: 0MiB
-Current num of task events stored: 0
-Total num of actor creation tasks: 0
-Total num of actor tasks: 0
-Total num of normal tasks: 0
-Total num of driver tasks: 0


[2023-08-23 13:09:44,591 I 41356 40156] (gcs_server.exe) gcs_server.cc:872: Event stats:


Global stats: 6675 total (4 active)
Queueing time: mean = 3.280 ms, max = 947.978 ms, min = -0.001 s, total = 21.894 s
Execution time:  mean = 60.811 us, total = 405.911 ms
Event stats:
	GcsInMemoryStore.Put - 1591 total (0 active), CPU time: mean = 32.841 us, total = 52.249 ms
	InternalKVGcsService.grpc_server.InternalKVPut - 1588 total (0 active), CPU time: mean = 12.728 us, total = 20.213 ms
	InternalKVGcsService.grpc_client.InternalKVPut - 1584 total (0 active), CPU time: mean = 6.216 us, total = 9.846 ms
	RayletLoadPulled - 1307 total (1 active), CPU time: mean = 7.406 us, total = 9.680 ms
	UNKNOWN - 438 total (1 active), CPU time: mean = 5.784 us, total = 2.533 ms
	GCSServer.deadline_timer.debug_state_dump - 132 total (1 active), CPU time: mean = 2.287 ms, total = 301.848 ms
	GCSServer.deadline_timer.debug_state_event_stats_print - 22 total (1 active, 1 running), CPU time: mean = 304.278 us, total = 6.694 ms
	GcsInMemoryStore.GetAll - 6 total (0 active), CPU time: mean = 14.767 us, total = 88.600 us
	PeriodicalRunner.RunFnPeriodically - 4 total (0 active), CPU time: mean = 678.900 us, total = 2.716 ms
	GcsInMemoryStore.Get - 2 total (0 active), CPU time: mean = 14.127 us, total = 28.253 us
	InternalKVGcsService.grpc_server.InternalKVGet - 1 total (0 active), CPU time: mean = 14.884 us, total = 14.884 us


[2023-08-23 13:09:44,591 I 41356 40156] (gcs_server.exe) gcs_server.cc:873: GcsTaskManager Event stats:


Global stats: 0 total (0 active)
Queueing time: mean = -nan(ind) s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
Execution time:  mean = -nan(ind) s, total = 0.000 s
Event stats:


[2023-08-23 13:10:44,595 I 41356 40156] (gcs_server.exe) gcs_server.cc:255: GcsNodeManager: 
- RegisterNode request count: 0
- DrainNode request count: 0
- GetAllNodeInfo request count: 0
- GetInternalConfig request count: 0

GcsActorManager: 
- RegisterActor request count: 0
- CreateActor request count: 0
- GetActorInfo request count: 0
- GetNamedActorInfo request count: 0
- GetAllActorInfo request count: 0
- KillActor request count: 0
- ListNamedActors request count: 0
- Registered actors count: 0
- Destroyed actors count: 0
- Named actors count: 0
- Unresolved actors count: 0
- Pending actors count: 0
- Created actors count: 0
- owners_: 0
- actor_to_register_callbacks_: 0
- actor_to_create_callbacks_: 0
- sorted_destroyed_actor_list_: 0

GcsResourceManager: 
- GetResources request count: 0
- GetAllAvailableResources request count0
- ReportResourceUsage request count: 0
- GetAllResourceUsage request count: 0

GcsPlacementGroupManager: 
- CreatePlacementGroup request count: 0
- RemovePlacementGroup request count: 0
- GetPlacementGroup request count: 0
- GetAllPlacementGroup request count: 0
- WaitPlacementGroupUntilReady request count: 0
- GetNamedPlacementGroup request count: 0
- Scheduling pending placement group count: 0
- Registered placement groups count: 0
- Named placement group count: 0
- Pending placement groups count: 0
- Infeasible placement groups count: 0

GcsPublisher {}

[runtime env manager] ID to URIs table:
[runtime env manager] URIs reference table:

GcsTaskManager: 
-Total num task events reported: 0
-Total num status task events dropped: 0
-Total num profile events dropped: 0
-Total num bytes of task event stored: 0MiB
-Current num of task events stored: 0
-Total num of actor creation tasks: 0
-Total num of actor tasks: 0
-Total num of normal tasks: 0
-Total num of driver tasks: 0


[2023-08-23 13:10:44,595 I 41356 40156] (gcs_server.exe) gcs_server.cc:872: Event stats:


Global stats: 6977 total (4 active)
Queueing time: mean = 3.243 ms, max = 947.978 ms, min = -0.001 s, total = 22.629 s
Execution time:  mean = 60.421 us, total = 421.558 ms
Event stats:
	GcsInMemoryStore.Put - 1663 total (0 active), CPU time: mean = 31.577 us, total = 52.513 ms
	InternalKVGcsService.grpc_server.InternalKVPut - 1660 total (0 active), CPU time: mean = 12.842 us, total = 21.318 ms
	InternalKVGcsService.grpc_client.InternalKVPut - 1656 total (0 active), CPU time: mean = 6.292 us, total = 10.419 ms
	RayletLoadPulled - 1366 total (1 active), CPU time: mean = 7.385 us, total = 10.088 ms
	UNKNOWN - 458 total (1 active), CPU time: mean = 5.762 us, total = 2.639 ms
	GCSServer.deadline_timer.debug_state_dump - 138 total (1 active), CPU time: mean = 2.281 ms, total = 314.750 ms
	GCSServer.deadline_timer.debug_state_event_stats_print - 23 total (1 active, 1 running), CPU time: mean = 303.650 us, total = 6.984 ms
	GcsInMemoryStore.GetAll - 6 total (0 active), CPU time: mean = 14.767 us, total = 88.600 us
	PeriodicalRunner.RunFnPeriodically - 4 total (0 active), CPU time: mean = 678.900 us, total = 2.716 ms
	GcsInMemoryStore.Get - 2 total (0 active), CPU time: mean = 14.127 us, total = 28.253 us
	InternalKVGcsService.grpc_server.InternalKVGet - 1 total (0 active), CPU time: mean = 14.884 us, total = 14.884 us


[2023-08-23 13:10:44,595 I 41356 40156] (gcs_server.exe) gcs_server.cc:873: GcsTaskManager Event stats:


Global stats: 0 total (0 active)
Queueing time: mean = -nan(ind) s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
Execution time:  mean = -nan(ind) s, total = 0.000 s
Event stats:


[2023-08-23 13:11:44,605 I 41356 40156] (gcs_server.exe) gcs_server.cc:255: GcsNodeManager: 
- RegisterNode request count: 0
- DrainNode request count: 0
- GetAllNodeInfo request count: 0
- GetInternalConfig request count: 0

GcsActorManager: 
- RegisterActor request count: 0
- CreateActor request count: 0
- GetActorInfo request count: 0
- GetNamedActorInfo request count: 0
- GetAllActorInfo request count: 0
- KillActor request count: 0
- ListNamedActors request count: 0
- Registered actors count: 0
- Destroyed actors count: 0
- Named actors count: 0
- Unresolved actors count: 0
- Pending actors count: 0
- Created actors count: 0
- owners_: 0
- actor_to_register_callbacks_: 0
- actor_to_create_callbacks_: 0
- sorted_destroyed_actor_list_: 0

GcsResourceManager: 
- GetResources request count: 0
- GetAllAvailableResources request count0
- ReportResourceUsage request count: 0
- GetAllResourceUsage request count: 0

GcsPlacementGroupManager: 
- CreatePlacementGroup request count: 0
- RemovePlacementGroup request count: 0
- GetPlacementGroup request count: 0
- GetAllPlacementGroup request count: 0
- WaitPlacementGroupUntilReady request count: 0
- GetNamedPlacementGroup request count: 0
- Scheduling pending placement group count: 0
- Registered placement groups count: 0
- Named placement group count: 0
- Pending placement groups count: 0
- Infeasible placement groups count: 0

GcsPublisher {}

[runtime env manager] ID to URIs table:
[runtime env manager] URIs reference table:

GcsTaskManager: 
-Total num task events reported: 0
-Total num status task events dropped: 0
-Total num profile events dropped: 0
-Total num bytes of task event stored: 0MiB
-Current num of task events stored: 0
-Total num of actor creation tasks: 0
-Total num of actor tasks: 0
-Total num of normal tasks: 0
-Total num of driver tasks: 0


[2023-08-23 13:11:44,605 I 41356 40156] (gcs_server.exe) gcs_server.cc:872: Event stats:


Global stats: 7280 total (4 active)
Queueing time: mean = 3.197 ms, max = 947.978 ms, min = -0.001 s, total = 23.276 s
Execution time:  mean = 60.091 us, total = 437.460 ms
Event stats:
	GcsInMemoryStore.Put - 1735 total (0 active), CPU time: mean = 30.432 us, total = 52.800 ms
	InternalKVGcsService.grpc_server.InternalKVPut - 1732 total (0 active), CPU time: mean = 12.831 us, total = 22.223 ms
	InternalKVGcsService.grpc_client.InternalKVPut - 1728 total (0 active), CPU time: mean = 6.348 us, total = 10.969 ms
	RayletLoadPulled - 1426 total (1 active), CPU time: mean = 7.355 us, total = 10.488 ms
	UNKNOWN - 478 total (1 active), CPU time: mean = 5.694 us, total = 2.722 ms
	GCSServer.deadline_timer.debug_state_dump - 144 total (1 active), CPU time: mean = 2.278 ms, total = 328.013 ms
	GCSServer.deadline_timer.debug_state_event_stats_print - 24 total (1 active, 1 running), CPU time: mean = 308.291 us, total = 7.399 ms
	GcsInMemoryStore.GetAll - 6 total (0 active), CPU time: mean = 14.767 us, total = 88.600 us
	PeriodicalRunner.RunFnPeriodically - 4 total (0 active), CPU time: mean = 678.900 us, total = 2.716 ms
	GcsInMemoryStore.Get - 2 total (0 active), CPU time: mean = 14.127 us, total = 28.253 us
	InternalKVGcsService.grpc_server.InternalKVGet - 1 total (0 active), CPU time: mean = 14.884 us, total = 14.884 us


[2023-08-23 13:11:44,605 I 41356 40156] (gcs_server.exe) gcs_server.cc:873: GcsTaskManager Event stats:


Global stats: 0 total (0 active)
Queueing time: mean = -nan(ind) s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
Execution time:  mean = -nan(ind) s, total = 0.000 s
Event stats:


[2023-08-23 13:12:44,608 I 41356 40156] (gcs_server.exe) gcs_server.cc:255: GcsNodeManager: 
- RegisterNode request count: 0
- DrainNode request count: 0
- GetAllNodeInfo request count: 0
- GetInternalConfig request count: 0

GcsActorManager: 
- RegisterActor request count: 0
- CreateActor request count: 0
- GetActorInfo request count: 0
- GetNamedActorInfo request count: 0
- GetAllActorInfo request count: 0
- KillActor request count: 0
- ListNamedActors request count: 0
- Registered actors count: 0
- Destroyed actors count: 0
- Named actors count: 0
- Unresolved actors count: 0
- Pending actors count: 0
- Created actors count: 0
- owners_: 0
- actor_to_register_callbacks_: 0
- actor_to_create_callbacks_: 0
- sorted_destroyed_actor_list_: 0

GcsResourceManager: 
- GetResources request count: 0
- GetAllAvailableResources request count0
- ReportResourceUsage request count: 0
- GetAllResourceUsage request count: 0

GcsPlacementGroupManager: 
- CreatePlacementGroup request count: 0
- RemovePlacementGroup request count: 0
- GetPlacementGroup request count: 0
- GetAllPlacementGroup request count: 0
- WaitPlacementGroupUntilReady request count: 0
- GetNamedPlacementGroup request count: 0
- Scheduling pending placement group count: 0
- Registered placement groups count: 0
- Named placement group count: 0
- Pending placement groups count: 0
- Infeasible placement groups count: 0

GcsPublisher {}

[runtime env manager] ID to URIs table:
[runtime env manager] URIs reference table:

GcsTaskManager: 
-Total num task events reported: 0
-Total num status task events dropped: 0
-Total num profile events dropped: 0
-Total num bytes of task event stored: 0MiB
-Current num of task events stored: 0
-Total num of actor creation tasks: 0
-Total num of actor tasks: 0
-Total num of normal tasks: 0
-Total num of driver tasks: 0


[2023-08-23 13:12:44,608 I 41356 40156] (gcs_server.exe) gcs_server.cc:872: Event stats:


Global stats: 7583 total (4 active)
Queueing time: mean = 3.150 ms, max = 947.978 ms, min = -0.001 s, total = 23.885 s
Execution time:  mean = 59.722 us, total = 452.870 ms
Event stats:
	GcsInMemoryStore.Put - 1807 total (0 active), CPU time: mean = 29.322 us, total = 52.985 ms
	InternalKVGcsService.grpc_server.InternalKVPut - 1804 total (0 active), CPU time: mean = 12.872 us, total = 23.221 ms
	InternalKVGcsService.grpc_client.InternalKVPut - 1800 total (0 active), CPU time: mean = 6.415 us, total = 11.547 ms
	RayletLoadPulled - 1486 total (1 active), CPU time: mean = 7.394 us, total = 10.987 ms
	UNKNOWN - 498 total (1 active), CPU time: mean = 5.609 us, total = 2.793 ms
	GCSServer.deadline_timer.debug_state_dump - 150 total (1 active), CPU time: mean = 2.272 ms, total = 340.842 ms
	GCSServer.deadline_timer.debug_state_event_stats_print - 25 total (1 active, 1 running), CPU time: mean = 305.920 us, total = 7.648 ms
	GcsInMemoryStore.GetAll - 6 total (0 active), CPU time: mean = 14.767 us, total = 88.600 us
	PeriodicalRunner.RunFnPeriodically - 4 total (0 active), CPU time: mean = 678.900 us, total = 2.716 ms
	GcsInMemoryStore.Get - 2 total (0 active), CPU time: mean = 14.127 us, total = 28.253 us
	InternalKVGcsService.grpc_server.InternalKVGet - 1 total (0 active), CPU time: mean = 14.884 us, total = 14.884 us


[2023-08-23 13:12:44,608 I 41356 40156] (gcs_server.exe) gcs_server.cc:873: GcsTaskManager Event stats:


Global stats: 0 total (0 active)
Queueing time: mean = -nan(ind) s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
Execution time:  mean = -nan(ind) s, total = 0.000 s
Event stats:


[2023-08-23 13:13:44,610 I 41356 40156] (gcs_server.exe) gcs_server.cc:255: GcsNodeManager: 
- RegisterNode request count: 0
- DrainNode request count: 0
- GetAllNodeInfo request count: 0
- GetInternalConfig request count: 0

GcsActorManager: 
- RegisterActor request count: 0
- CreateActor request count: 0
- GetActorInfo request count: 0
- GetNamedActorInfo request count: 0
- GetAllActorInfo request count: 0
- KillActor request count: 0
- ListNamedActors request count: 0
- Registered actors count: 0
- Destroyed actors count: 0
- Named actors count: 0
- Unresolved actors count: 0
- Pending actors count: 0
- Created actors count: 0
- owners_: 0
- actor_to_register_callbacks_: 0
- actor_to_create_callbacks_: 0
- sorted_destroyed_actor_list_: 0

GcsResourceManager: 
- GetResources request count: 0
- GetAllAvailableResources request count0
- ReportResourceUsage request count: 0
- GetAllResourceUsage request count: 0

GcsPlacementGroupManager: 
- CreatePlacementGroup request count: 0
- RemovePlacementGroup request count: 0
- GetPlacementGroup request count: 0
- GetAllPlacementGroup request count: 0
- WaitPlacementGroupUntilReady request count: 0
- GetNamedPlacementGroup request count: 0
- Scheduling pending placement group count: 0
- Registered placement groups count: 0
- Named placement group count: 0
- Pending placement groups count: 0
- Infeasible placement groups count: 0

GcsPublisher {}

[runtime env manager] ID to URIs table:
[runtime env manager] URIs reference table:

GcsTaskManager: 
-Total num task events reported: 0
-Total num status task events dropped: 0
-Total num profile events dropped: 0
-Total num bytes of task event stored: 0MiB
-Current num of task events stored: 0
-Total num of actor creation tasks: 0
-Total num of actor tasks: 0
-Total num of normal tasks: 0
-Total num of driver tasks: 0


[2023-08-23 13:13:44,610 I 41356 40156] (gcs_server.exe) gcs_server.cc:872: Event stats:


Global stats: 7885 total (4 active)
Queueing time: mean = 3.116 ms, max = 947.978 ms, min = -0.001 s, total = 24.569 s
Execution time:  mean = 59.482 us, total = 469.013 ms
Event stats:
	GcsInMemoryStore.Put - 1879 total (0 active), CPU time: mean = 28.349 us, total = 53.267 ms
	InternalKVGcsService.grpc_server.InternalKVPut - 1876 total (0 active), CPU time: mean = 12.981 us, total = 24.352 ms
	InternalKVGcsService.grpc_client.InternalKVPut - 1872 total (0 active), CPU time: mean = 6.442 us, total = 12.060 ms
	RayletLoadPulled - 1545 total (1 active), CPU time: mean = 7.335 us, total = 11.333 ms
	UNKNOWN - 518 total (1 active), CPU time: mean = 5.550 us, total = 2.875 ms
	GCSServer.deadline_timer.debug_state_dump - 156 total (1 active), CPU time: mean = 2.271 ms, total = 354.322 ms
	GCSServer.deadline_timer.debug_state_event_stats_print - 26 total (1 active, 1 running), CPU time: mean = 305.982 us, total = 7.956 ms
	GcsInMemoryStore.GetAll - 6 total (0 active), CPU time: mean = 14.767 us, total = 88.600 us
	PeriodicalRunner.RunFnPeriodically - 4 total (0 active), CPU time: mean = 678.900 us, total = 2.716 ms
	GcsInMemoryStore.Get - 2 total (0 active), CPU time: mean = 14.127 us, total = 28.253 us
	InternalKVGcsService.grpc_server.InternalKVGet - 1 total (0 active), CPU time: mean = 14.884 us, total = 14.884 us


[2023-08-23 13:13:44,610 I 41356 40156] (gcs_server.exe) gcs_server.cc:873: GcsTaskManager Event stats:


Global stats: 0 total (0 active)
Queueing time: mean = -nan(ind) s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
Execution time:  mean = -nan(ind) s, total = 0.000 s
Event stats:


[2023-08-23 13:14:44,625 I 41356 40156] (gcs_server.exe) gcs_server.cc:255: GcsNodeManager: 
- RegisterNode request count: 0
- DrainNode request count: 0
- GetAllNodeInfo request count: 0
- GetInternalConfig request count: 0

GcsActorManager: 
- RegisterActor request count: 0
- CreateActor request count: 0
- GetActorInfo request count: 0
- GetNamedActorInfo request count: 0
- GetAllActorInfo request count: 0
- KillActor request count: 0
- ListNamedActors request count: 0
- Registered actors count: 0
- Destroyed actors count: 0
- Named actors count: 0
- Unresolved actors count: 0
- Pending actors count: 0
- Created actors count: 0
- owners_: 0
- actor_to_register_callbacks_: 0
- actor_to_create_callbacks_: 0
- sorted_destroyed_actor_list_: 0

GcsResourceManager: 
- GetResources request count: 0
- GetAllAvailableResources request count0
- ReportResourceUsage request count: 0
- GetAllResourceUsage request count: 0

GcsPlacementGroupManager: 
- CreatePlacementGroup request count: 0
- RemovePlacementGroup request count: 0
- GetPlacementGroup request count: 0
- GetAllPlacementGroup request count: 0
- WaitPlacementGroupUntilReady request count: 0
- GetNamedPlacementGroup request count: 0
- Scheduling pending placement group count: 0
- Registered placement groups count: 0
- Named placement group count: 0
- Pending placement groups count: 0
- Infeasible placement groups count: 0

GcsPublisher {}

[runtime env manager] ID to URIs table:
[runtime env manager] URIs reference table:

GcsTaskManager: 
-Total num task events reported: 0
-Total num status task events dropped: 0
-Total num profile events dropped: 0
-Total num bytes of task event stored: 0MiB
-Current num of task events stored: 0
-Total num of actor creation tasks: 0
-Total num of actor tasks: 0
-Total num of normal tasks: 0
-Total num of driver tasks: 0


[2023-08-23 13:14:44,626 I 41356 40156] (gcs_server.exe) gcs_server.cc:872: Event stats:


Global stats: 8188 total (4 active)
Queueing time: mean = 3.074 ms, max = 947.978 ms, min = -0.001 s, total = 25.170 s
Execution time:  mean = 59.310 us, total = 485.634 ms
Event stats:
	GcsInMemoryStore.Put - 1951 total (0 active), CPU time: mean = 27.466 us, total = 53.586 ms
	InternalKVGcsService.grpc_server.InternalKVPut - 1948 total (0 active), CPU time: mean = 12.973 us, total = 25.271 ms
	InternalKVGcsService.grpc_client.InternalKVPut - 1944 total (0 active), CPU time: mean = 6.502 us, total = 12.640 ms
	RayletLoadPulled - 1605 total (1 active), CPU time: mean = 7.301 us, total = 11.717 ms
	UNKNOWN - 538 total (1 active), CPU time: mean = 5.521 us, total = 2.970 ms
	GCSServer.deadline_timer.debug_state_dump - 162 total (1 active), CPU time: mean = 2.274 ms, total = 368.443 ms
	GCSServer.deadline_timer.debug_state_event_stats_print - 27 total (1 active, 1 running), CPU time: mean = 302.191 us, total = 8.159 ms
	GcsInMemoryStore.GetAll - 6 total (0 active), CPU time: mean = 14.767 us, total = 88.600 us
	PeriodicalRunner.RunFnPeriodically - 4 total (0 active), CPU time: mean = 678.900 us, total = 2.716 ms
	GcsInMemoryStore.Get - 2 total (0 active), CPU time: mean = 14.127 us, total = 28.253 us
	InternalKVGcsService.grpc_server.InternalKVGet - 1 total (0 active), CPU time: mean = 14.884 us, total = 14.884 us


[2023-08-23 13:14:44,626 I 41356 40156] (gcs_server.exe) gcs_server.cc:873: GcsTaskManager Event stats:


Global stats: 0 total (0 active)
Queueing time: mean = -nan(ind) s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
Execution time:  mean = -nan(ind) s, total = 0.000 s
Event stats:


[2023-08-23 13:15:45,419 I 41356 40156] (gcs_server.exe) gcs_server.cc:255: GcsNodeManager: 
- RegisterNode request count: 0
- DrainNode request count: 0
- GetAllNodeInfo request count: 0
- GetInternalConfig request count: 0

GcsActorManager: 
- RegisterActor request count: 0
- CreateActor request count: 0
- GetActorInfo request count: 0
- GetNamedActorInfo request count: 0
- GetAllActorInfo request count: 0
- KillActor request count: 0
- ListNamedActors request count: 0
- Registered actors count: 0
- Destroyed actors count: 0
- Named actors count: 0
- Unresolved actors count: 0
- Pending actors count: 0
- Created actors count: 0
- owners_: 0
- actor_to_register_callbacks_: 0
- actor_to_create_callbacks_: 0
- sorted_destroyed_actor_list_: 0

GcsResourceManager: 
- GetResources request count: 0
- GetAllAvailableResources request count0
- ReportResourceUsage request count: 0
- GetAllResourceUsage request count: 0

GcsPlacementGroupManager: 
- CreatePlacementGroup request count: 0
- RemovePlacementGroup request count: 0
- GetPlacementGroup request count: 0
- GetAllPlacementGroup request count: 0
- WaitPlacementGroupUntilReady request count: 0
- GetNamedPlacementGroup request count: 0
- Scheduling pending placement group count: 0
- Registered placement groups count: 0
- Named placement group count: 0
- Pending placement groups count: 0
- Infeasible placement groups count: 0

GcsPublisher {}

[runtime env manager] ID to URIs table:
[runtime env manager] URIs reference table:

GcsTaskManager: 
-Total num task events reported: 0
-Total num status task events dropped: 0
-Total num profile events dropped: 0
-Total num bytes of task event stored: 0MiB
-Current num of task events stored: 0
-Total num of actor creation tasks: 0
-Total num of actor tasks: 0
-Total num of normal tasks: 0
-Total num of driver tasks: 0


[2023-08-23 13:15:45,419 I 41356 40156] (gcs_server.exe) gcs_server.cc:872: Event stats:


Global stats: 8490 total (4 active)
Queueing time: mean = 3.037 ms, max = 947.978 ms, min = -0.001 s, total = 25.782 s
Execution time:  mean = 59.138 us, total = 502.079 ms
Event stats:
	GcsInMemoryStore.Put - 2023 total (0 active), CPU time: mean = 26.605 us, total = 53.822 ms
	InternalKVGcsService.grpc_server.InternalKVPut - 2020 total (0 active), CPU time: mean = 13.001 us, total = 26.263 ms
	InternalKVGcsService.grpc_client.InternalKVPut - 2016 total (0 active), CPU time: mean = 6.540 us, total = 13.186 ms
	RayletLoadPulled - 1664 total (1 active), CPU time: mean = 7.212 us, total = 12.001 ms
	UNKNOWN - 558 total (1 active), CPU time: mean = 5.473 us, total = 3.054 ms
	GCSServer.deadline_timer.debug_state_dump - 168 total (1 active), CPU time: mean = 2.276 ms, total = 382.390 ms
	GCSServer.deadline_timer.debug_state_event_stats_print - 28 total (1 active, 1 running), CPU time: mean = 304.130 us, total = 8.516 ms
	GcsInMemoryStore.GetAll - 6 total (0 active), CPU time: mean = 14.767 us, total = 88.600 us
	PeriodicalRunner.RunFnPeriodically - 4 total (0 active), CPU time: mean = 678.900 us, total = 2.716 ms
	GcsInMemoryStore.Get - 2 total (0 active), CPU time: mean = 14.127 us, total = 28.253 us
	InternalKVGcsService.grpc_server.InternalKVGet - 1 total (0 active), CPU time: mean = 14.884 us, total = 14.884 us


[2023-08-23 13:15:45,419 I 41356 40156] (gcs_server.exe) gcs_server.cc:873: GcsTaskManager Event stats:


Global stats: 0 total (0 active)
Queueing time: mean = -nan(ind) s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
Execution time:  mean = -nan(ind) s, total = 0.000 s
Event stats:
  </code>
</pre>
</details>

I also ran the step-by-step you've provided;
```
# install python3.9 from the windows app store
>python -m venv d:\temp\cpython_39
>d:\temp\cpython_39\Scripts\activate
>pip install ray[default]
>python -c ""import ray; ray.init()""
```
The install ran without issues, but the last line returned the error **OSError: [Errno 0] AssignProcessToJobObject() failed**.",running local install checked different various content log empty apart content summary code running set ray event level warning ray event storage type loading job table data loading node table data loading cluster table data loading actor table data loading actor task spec table data loading placement group table data finished loading job table data size finished loading node table data size finished loading cluster table data size finished loading actor table data size finished loading actor task spec table data size finished loading placement group table data size server cluster id found generating new id server listening port request count request count request count request count request count request count request count request count request count request count request count registered count count count unresolved count pending count count request count request count request count request count request count request count request count request count request count request count pending placement group count registered placement count placement group count pending placement count infeasible placement count manager id table manager reference table task status task profile task event mib task actor creation actor normal driver event global total active time mean min u total execution time mean total event total active time mean total total active time mean u total u total active time mean u total u total active time mean total total active running time mean u total u total active time mean total unknown total active time mean total total active time mean u total u event global total active time mean min total execution time mean total event export metric agent error message connect error wo affect ray lose metric cluster request count request count request count request count request count request count request count request count request count request count request count registered count count count unresolved count pending count count request count request count request count request count request count request count request count request count request count request count pending placement group count registered placement count placement group count pending placement count infeasible placement count manager id table manager reference table task status task profile task event mib task actor creation actor normal driver event global total active time mean min u total execution time mean u total event total active time mean u total total active time mean u total u total active time mean u total u total active time mean u total u unknown total active time mean u total u total active time mean u total u total active time mean total total active time mean u total total active time mean u total u total active time mean u total u total active running time mean total event global total active time mean min total execution time mean total event request count request count request count request count request count request count request count request count request count request count request count registered count count count unresolved count pending count count request count request count request count request count request count request count request count request count request count request count pending placement group count registered placement count placement group count pending placement count infeasible placement count manager id table manager reference table task status task profile task event mib task actor creation actor normal driver event global total active time mean min u total execution time mean u total event total active time mean u total total active time mean u total total active time mean u total u total active time mean u total u unknown total active time mean u total u total active time mean total total active time mean u total u total active time mean u total total active time mean u total u total active running time mean u total u total active time mean u total u event global total active time mean min total execution time mean total event request count request count request count request count request count request count request count request count request count request count request count registered count count count unresolved count pending count count request count request count request count request count request count request count request count request count request count request count pending placement group count registered placement count placement group count pending placement count infeasible placement count manager id table manager reference table task status task profile task event mib task actor creation actor normal driver event global total active time mean min u total execution time mean u total event total active time mean u total total active time mean u total total active time mean u total total active time mean u total u unknown total active time mean u total u total active time mean total total active time mean u total u total active time mean u total total active running time mean u total u total active time mean u total u total active time mean u total u event global total active time mean min total execution time mean total event request count request count request count request count request count request count request count request count request count request count request count registered count count count unresolved count pending count count request count request count request count request count request count request count request count request count request count request count pending placement group count registered placement count placement group count pending placement count infeasible placement count manager id table manager reference table task status task profile task event mib task actor creation actor normal driver event global total active time mean min u total execution time mean u total event total active time mean u total total active time mean u total total active time mean u total total active time mean u total unknown total active time mean u total u total active time mean total total active time mean u total u total active time mean u total total active running time mean u total u total active time mean u total u total active time mean u total u event global total active time mean min total execution time mean total event request count request count request count request count request count request count request count request count request count request count request count registered count count count unresolved count pending count count request count request count request count request count request count request count request count request count request count request count pending placement group count registered placement count placement group count pending placement count infeasible placement count manager id table manager reference table task status task profile task event mib task actor creation actor normal driver event global total active time mean min u total execution time mean u total event total active time mean u total total active time mean u total total active time mean u total total active time mean u total unknown total active time mean u total u total active time mean total total active time mean u total u total active running time mean u total total active time mean u total total active time mean u total u total active time mean u total u event global total active time mean min total execution time mean total event request count request count request count request count request count request count request count request count request count request count request count registered count count count unresolved count pending count count request count request count request count request count request count request count request count request count request count request count pending placement group count registered placement count placement group count pending placement count infeasible placement count manager id table manager reference table task status task profile task event mib task actor creation actor normal driver event global total active time mean min total execution time mean u total event total active time mean u total total active time mean u total total active time mean u total total active time mean u total unknown total active time mean u total u total active time mean total total active time mean u total u total active running time mean u total total active time mean u total total active time mean u total u total active time mean u total u event global total active time mean min total execution time mean total event request count request count request count request count request count request count request count request count request count request count request count registered count count count unresolved count pending count count request count request count request count request count request count request count request count request count request count request count pending placement group count registered placement count placement group count pending placement count infeasible placement count manager id table manager reference table task status task profile task event mib task actor creation actor normal driver event global total active time mean min total execution time mean u total event total active time mean u total total active time mean u total total active time mean u total total active time mean u total unknown total active time mean u total u total active time mean total total active running time mean u total total active time mean u total u total active time mean u total total active time mean u total u total active time mean u total u event global total active time mean min total execution time mean total event request count request count request count request count request count request count request count request count request count request count request count registered count count count unresolved count pending count count request count request count request count request count request count request count request count request count request count request count pending placement group count registered placement count placement group count pending placement count infeasible placement count manager id table manager reference table task status task profile task event mib task actor creation actor normal driver event global total active time mean min total execution time mean u total event total active time mean u total total active time mean u total total active time mean u total total active time mean u total unknown total active time mean u total u total active time mean total total active running time mean u total total active time mean u total u total active time mean u total total active time mean u total u total active time mean u total u event global total active time mean min total execution time mean total event request count request count request count request count request count request count request count request count request count request count request count registered count count count unresolved count pending count count request count request count request count request count request count request count request count request count request count request count pending placement group count registered placement count placement group count pending placement count infeasible placement count manager id table manager reference table task status task profile task event mib task actor creation actor normal driver event global total active time mean min total execution time mean u total event total active time mean u total total active time mean u total total active time mean u total total active time mean u total unknown total active time mean u total total active time mean total total active running time mean u total total active time mean u total u total active time mean u total total active time mean u total u total active time mean u total u event global total active time mean min total execution time mean total event request count request count request count request count request count request count request count request count request count request count request count registered count count count unresolved count pending count count request count request count request count request count request count request count request count request count request count request count pending placement group count registered placement count placement group count pending placement count infeasible placement count manager id table manager reference table task status task profile task event mib task actor creation actor normal driver event global total active time mean min total execution time mean u total event total active time mean u total total active time mean u total total active time mean u total total active time mean u total unknown total active time mean u total total active time mean total total active running time mean u total total active time mean u total u total active time mean u total total active time mean u total u total active time mean u total u event global total active time mean min total execution time mean total event request count request count request count request count request count request count request count request count request count request count request count registered count count count unresolved count pending count count request count request count request count request count request count request count request count request count request count request count pending placement group count registered placement count placement group count pending placement count infeasible placement count manager id table manager reference table task status task profile task event mib task actor creation actor normal driver event global total active time mean min total execution time mean u total event total active time mean u total total active time mean u total total active time mean u total total active time mean u total unknown total active time mean u total total active time mean total total active running time mean u total total active time mean u total u total active time mean u total total active time mean u total u total active time mean u total u event global total active time mean min total execution time mean total event request count request count request count request count request count request count request count request count request count request count request count registered count count count unresolved count pending count count request count request count request count request count request count request count request count request count request count request count pending placement group count registered placement count placement group count pending placement count infeasible placement count manager id table manager reference table task status task profile task event mib task actor creation actor normal driver event global total active time mean min total execution time mean u total event total active time mean u total total active time mean u total total active time mean u total total active time mean u total unknown total active time mean u total total active time mean total total active running time mean u total total active time mean u total u total active time mean u total total active time mean u total u total active time mean u total u event global total active time mean min total execution time mean total event request count request count request count request count request count request count request count request count request count request count request count registered count count count unresolved count pending count count request count request count request count request count request count request count request count request count request count request count pending placement group count registered placement count placement group count pending placement count infeasible placement count manager id table manager reference table task status task profile task event mib task actor creation actor normal driver event global total active time mean min total execution time mean u total event total active time mean u total total active time mean u total total active time mean u total total active time mean u total unknown total active time mean u total total active time mean total total active running time mean u total total active time mean u total u total active time mean u total total active time mean u total u total active time mean u total u event global total active time mean min total execution time mean total event request count request count request count request count request count request count request count request count request count request count request count registered count count count unresolved count pending count count request count request count request count request count request count request count request count request count request count request count pending placement group count registered placement count placement group count pending placement count infeasible placement count manager id table manager reference table task status task profile task event mib task actor creation actor normal driver event global total active time mean min total execution time mean u total event total active time mean u total total active time mean u total total active time mean u total total active time mean u total unknown total active time mean u total total active time mean total total active running time mean u total total active time mean u total u total active time mean u total total active time mean u total u total active time mean u total u event global total active time mean min total execution time mean total event request count request count request count request count request count request count request count request count request count request count request count registered count count count unresolved count pending count count request count request count request count request count request count request count request count request count request count request count pending placement group count registered placement count placement group count pending placement count infeasible placement count manager id table manager reference table task status task profile task event mib task actor creation actor normal driver event global total active time mean min total execution time mean u total event total active time mean u total total active time mean u total total active time mean u total total active time mean u total unknown total active time mean u total total active time mean total total active running time mean u total total active time mean u total u total active time mean u total total active time mean u total u total active time mean u total u event global total active time mean min total execution time mean total event request count request count request count request count request count request count request count request count request count request count request count registered count count count unresolved count pending count count request count request count request count request count request count request count request count request count request count request count pending placement group count registered placement count placement group count pending placement count infeasible placement count manager id table manager reference table task status task profile task event mib task actor creation actor normal driver event global total active time mean min total execution time mean u total event total active time mean u total total active time mean u total total active time mean u total total active time mean u total unknown total active time mean u total total active time mean total total active running time mean u total total active time mean u total u total active time mean u total total active time mean u total u total active time mean u total u event global total active time mean min total execution time mean total event request count request count request count request count request count request count request count request count request count request count request count registered count count count unresolved count pending count count request count request count request count request count request count request count request count request count request count request count pending placement group count registered placement count placement group count pending placement count infeasible placement count manager id table manager reference table task status task profile task event mib task actor creation actor normal driver event global total active time mean min total execution time mean u total event total active time mean u total total active time mean u total total active time mean u total total active time mean u total unknown total active time mean u total total active time mean total total active running time mean u total total active time mean u total u total active time mean u total total active time mean u total u total active time mean u total u event global total active time mean min total execution time mean total event request count request count request count request count request count request count request count request count request count request count request count registered count count count unresolved count pending count count request count request count request count request count request count request count request count request count request count request count pending placement group count registered placement count placement group count pending placement count infeasible placement count manager id table manager reference table task status task profile task event mib task actor creation actor normal driver event global total active time mean min total execution time mean u total event total active time mean u total total active time mean u total total active time mean u total total active time mean u total unknown total active time mean u total total active time mean total total active running time mean u total total active time mean u total u total active time mean u total total active time mean u total u total active time mean u total u event global total active time mean min total execution time mean total event request count request count request count request count request count request count request count request count request count request count request count registered count count count unresolved count pending count count request count request count request count request count request count request count request count request count request count request count pending placement group count registered placement count placement group count pending placement count infeasible placement count manager id table manager reference table task status task profile task event mib task actor creation actor normal driver event global total active time mean min total execution time mean u total event total active time mean u total total active time mean u total total active time mean u total total active time mean u total unknown total active time mean u total total active time mean total total active running time mean u total total active time mean u total u total active time mean u total total active time mean u total u total active time mean u total u event global total active time mean min total execution time mean total event request count request count request count request count request count request count request count request count request count request count request count registered count count count unresolved count pending count count request count request count request count request count request count request count request count request count request count request count pending placement group count registered placement count placement group count pending placement count infeasible placement count manager id table manager reference table task status task profile task event mib task actor creation actor normal driver event global total active time mean min total execution time mean u total event total active time mean u total total active time mean u total total active time mean u total total active time mean u total unknown total active time mean u total total active time mean total total active running time mean u total total active time mean u total u total active time mean u total total active time mean u total u total active time mean u total u event global total active time mean min total execution time mean total event request count request count request count request count request count request count request count request count request count request count request count registered count count count unresolved count pending count count request count request count request count request count request count request count request count request count request count request count pending placement group count registered placement count placement group count pending placement count infeasible placement count manager id table manager reference table task status task profile task event mib task actor creation actor normal driver event global total active time mean min total execution time mean u total event total active time mean u total total active time mean u total total active time mean u total total active time mean u total unknown total active time mean u total total active time mean total total active running time mean u total total active time mean u total u total active time mean u total total active time mean u total u total active time mean u total u event global total active time mean min total execution time mean total event request count request count request count request count request count request count request count request count request count request count request count registered count count count unresolved count pending count count request count request count request count request count request count request count request count request count request count request count pending placement group count registered placement count placement group count pending placement count infeasible placement count manager id table manager reference table task status task profile task event mib task actor creation actor normal driver event global total active time mean min total execution time mean u total event total active time mean u total total active time mean u total total active time mean u total total active time mean u total unknown total active time mean u total total active time mean total total active running time mean u total total active time mean u total u total active time mean u total total active time mean u total u total active time mean u total u event global total active time mean min total execution time mean total event request count request count request count request count request count request count request count request count request count request count request count registered count count count unresolved count pending count count request count request count request count request count request count request count request count request count request count request count pending placement group count registered placement count placement group count pending placement count infeasible placement count manager id table manager reference table task status task profile task event mib task actor creation actor normal driver event global total active time mean min total execution time mean u total event total active time mean u total total active time mean u total total active time mean u total total active time mean u total unknown total active time mean u total total active time mean total total active running time mean u total total active time mean u total u total active time mean u total total active time mean u total u total active time mean u total u event global total active time mean min total execution time mean total event request count request count request count request count request count request count request count request count request count request count request count registered count count count unresolved count pending count count request count request count request count request count request count request count request count request count request count request count pending placement group count registered placement count placement group count pending placement count infeasible placement count manager id table manager reference table task status task profile task event mib task actor creation actor normal driver event global total active time mean min total execution time mean u total event total active time mean u total total active time mean u total total active time mean u total total active time mean u total unknown total active time mean u total total active time mean total total active running time mean u total total active time mean u total u total active time mean u total total active time mean u total u total active time mean u total u event global total active time mean min total execution time mean total event request count request count request count request count request count request count request count request count request count request count request count registered count count count unresolved count pending count count request count request count request count request count request count request count request count request count request count request count pending placement group count registered placement count placement group count pending placement count infeasible placement count manager id table manager reference table task status task profile task event mib task actor creation actor normal driver event global total active time mean min total execution time mean u total event total active time mean u total total active time mean u total total active time mean u total total active time mean u total unknown total active time mean u total total active time mean total total active running time mean u total total active time mean u total u total active time mean u total total active time mean u total u total active time mean u total u event global total active time mean min total execution time mean total event request count request count request count request count request count request count request count request count request count request count request count registered count count count unresolved count pending count count request count request count request count request count request count request count request count request count request count request count pending placement group count registered placement count placement group count pending placement count infeasible placement count manager id table manager reference table task status task profile task event mib task actor creation actor normal driver event global total active time mean min total execution time mean u total event total active time mean u total total active time mean u total total active time mean u total total active time mean u total unknown total active time mean u total total active time mean total total active running time mean u total total active time mean u total u total active time mean u total total active time mean u total u total active time mean u total u event global total active time mean min total execution time mean total event request count request count request count request count request count request count request count request count request count request count request count registered count count count unresolved count pending count count request count request count request count request count request count request count request count request count request count request count pending placement group count registered placement count placement group count pending placement count infeasible placement count manager id table manager reference table task status task profile task event mib task actor creation actor normal driver event global total active time mean min total execution time mean u total event total active time mean u total total active time mean u total total active time mean u total total active time mean u total unknown total active time mean u total total active time mean total total active running time mean u total total active time mean u total u total active time mean u total total active time mean u total u total active time mean u total u event global total active time mean min total execution time mean total event request count request count request count request count request count request count request count request count request count request count request count registered count count count unresolved count pending count count request count request count request count request count request count request count request count request count request count request count pending placement group count registered placement count placement group count pending placement count infeasible placement count manager id table manager reference table task status task profile task event mib task actor creation actor normal driver event global total active time mean min total execution time mean u total event total active time mean u total total active time mean u total total active time mean u total total active time mean u total unknown total active time mean u total total active time mean total total active running time mean u total total active time mean u total u total active time mean u total total active time mean u total u total active time mean u total u event global total active time mean min total execution time mean total event also ran provided install python store python pip install ray default python import ray install ran without last line returned error,issue,positive,negative,negative,negative,negative,negative
1691486644,"Hey @ArturNiederfahrenhorst , we are moving ""back to"" DeveloperAPI b/c these classes/functions must - per our LINTer - have some API tag.",hey moving back must per linter tag,issue,negative,neutral,neutral,neutral,neutral,neutral
1691481684,"Hey @kouroshHakha , the deprecation warnings for all exploration classes had already been fixed in an earlier PR.",hey deprecation exploration class already fixed,issue,negative,positive,neutral,neutral,positive,positive
1691395799,"Btw, until @pcmoritz gives a green light, I will mark the feature as alpha. I will still proceed merging PRs needed for the proposal (and try to get it in by 2.7), so please review it! https://docs.google.com/document/d/1LzKPL7wxz5tUJJLMMZ7BlfdDlYclAzMdzolTezU06ck/edit#heading=h.ehfmi7oa003b",green light mark feature alpha still proceed proposal try get please review,issue,negative,positive,neutral,neutral,positive,positive
1691381338,The initial proposal is out. https://docs.google.com/document/d/1uibRyJOXdG5kR0ndCDW3E1rX6dtqcqohEPKDrOffYHw/edit. I will start coding once the decision is made. ,initial proposal start decision made,issue,negative,neutral,neutral,neutral,neutral,neutral
1691346387,"Oh, I found the code finally
https://github.com/ray-project/ray/blob/8ace253330c75faa9b0f696e899045f43927b80c/python/ray/serve/_private/replica.py#L120-L122

I'll check the behavior of `importlib` and try to fix it. Any suggestion will be appreciated.",oh found code finally check behavior try fix suggestion,issue,negative,neutral,neutral,neutral,neutral,neutral
1691303260,"One more case: ( changed `_frontend` to `deploy_frontend` already while the error still reports `_fronted`)
![image](https://github.com/ray-project/ray/assets/20621784/610af6b6-97eb-4e26-9bcf-4026dcd1fd4b)
I guess it's related to `__pycache__` and `importlib.reload` is needed somewhere

Updated: deleting `__pycache__` makes no difference",one case already error still image guess related somewhere difference,issue,negative,neutral,neutral,neutral,neutral,neutral
1691272738,"@zhe-thoughts it is a small error message improvement. Do you think it is okay to get this in, or we don't allow it unless it is critical P0/P1? ",small error message improvement think get allow unless critical,issue,negative,negative,negative,negative,negative,negative
1691253099,Marked it P1 as this never fails 3 times in a row (only yellow),marked never time row yellow,issue,negative,positive,neutral,neutral,positive,positive
1691249186,"cc @zhe-thoughts this is test that's needed for multi cloud work, and it is test-only changes (low risk)",test cloud work low risk,issue,negative,neutral,neutral,neutral,neutral,neutral
1691163717,@krfricke ok to merge if looks good. Are you ok with the slight API behavior change here?,merge good slight behavior change,issue,negative,positive,positive,positive,positive,positive
1691127415,"I am facing the same issue trying to restore a training interruption using `Tuner.restore`

```
    results = tuner.fit()
  File ""/scratch/sk10691/conda-envs/main/lib/python3.10/site-packages/ray/tune/tuner.py"", line 347, in fit
    return self._local_tuner.fit()
  File ""/scratch/sk10691/conda-envs/main/lib/python3.10/site-packages/ray/tune/impl/tuner_internal.py"", line 590, in fit
    analysis = self._fit_resume(trainable, param_space)
  File ""/scratch/sk10691/conda-envs/main/lib/python3.10/site-packages/ray/tune/impl/tuner_internal.py"", line 738, in _fit_resume
    analysis = run(**args)
  File ""/scratch/sk10691/conda-envs/main/lib/python3.10/site-packages/ray/tune/tune.py"", line 1036, in run
    runner = trial_runner_cls(**runner_kwargs)
  File ""/scratch/sk10691/conda-envs/main/lib/python3.10/site-packages/ray/tune/execution/tune_controller.py"", line 149, in __init__
    super().__init__(
  File ""/scratch/sk10691/conda-envs/main/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py"", line 258, in __init__
    self.resume(
  File ""/scratch/sk10691/conda-envs/main/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py"", line 506, in resume
    trials = self.restore_from_dir()
  File ""/scratch/sk10691/conda-envs/main/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py"", line 444, in restore_from_dir
    trial = Trial.from_json_state(trial_json_state)
  File ""/scratch/sk10691/conda-envs/main/lib/python3.10/site-packages/ray/tune/experiment/trial.py"", line 1136, in from_json_state
    trial_state = json.loads(json_state, cls=TuneFunctionDecoder)
  File ""/scratch/sk10691/conda-envs/main/lib/python3.10/json/__init__.py"", line 359, in loads
    return cls(**kw).decode(s)
  File ""/scratch/sk10691/conda-envs/main/lib/python3.10/json/decoder.py"", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/scratch/sk10691/conda-envs/main/lib/python3.10/json/decoder.py"", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
  File ""/scratch/sk10691/conda-envs/main/lib/python3.10/site-packages/ray/tune/utils/serialization.py"", line 39, in object_hook
    return self._from_cloudpickle(obj)
  File ""/scratch/sk10691/conda-envs/main/lib/python3.10/site-packages/ray/tune/utils/serialization.py"", line 43, in _from_cloudpickle
    return cloudpickle.loads(hex_to_binary(obj[""value""]))
  File ""/scratch/sk10691/conda-envs/main/lib/python3.10/site-packages/ray/_private/serialization.py"", line 105, in _actor_handle_deserializer
    return ray.actor.ActorHandle._deserialization_helper(serialized_obj, outer_id)
  File ""/scratch/sk10691/conda-envs/main/lib/python3.10/site-packages/ray/actor.py"", line 1292, in _deserialization_helper
    return worker.core_worker.deserialize_and_register_actor_handle(
  File ""python/ray/_raylet.pyx"", line 3503, in ray._raylet.CoreWorker.deserialize_and_register_actor_handle
  File ""python/ray/_raylet.pyx"", line 3472, in ray._raylet.CoreWorker.make_actor_handle
  File ""/scratch/sk10691/conda-envs/main/lib/python3.10/site-packages/ray/_private/function_manager.py"", line 574, in load_actor_class
    actor_class = self._load_actor_class_from_gcs(
  File ""/scratch/sk10691/conda-envs/main/lib/python3.10/site-packages/ray/_private/function_manager.py"", line 669, in _load_actor_class_from_gcs
    class_name = ensure_str(class_name)
  File ""/scratch/sk10691/conda-envs/main/lib/python3.10/site-packages/ray/_private/utils.py"", line 239, in ensure_str
    assert isinstance(s, bytes)
```",facing issue trying restore training interruption file line fit return file line fit analysis trainable file line analysis run file line run runner file line super file line file line resume file line trial file line file line return file line decode end file line end file line return file line return value file line return file line return file line file line file line file line file line assert,issue,positive,positive,positive,positive,positive,positive
1691091466,"Given the performance of https://github.com/ray-project/ray/pull/38432, I don't think we should mark this as done yet :)",given performance think mark done yet,issue,negative,neutral,neutral,neutral,neutral,neutral
1691037574,"@zhe-thoughts - yes, I checked the CI and the failure tests are not related.",yes checked failure related,issue,negative,negative,negative,negative,negative,negative
1691027851,This PR is bugfix to resolve data release test blocker - https://github.com/ray-project/ray/issues/38790 . cc @zhe-thoughts for review.,resolve data release test blocker review,issue,negative,neutral,neutral,neutral,neutral,neutral
1690984617,"@alexeykudinkin  @edoakes could you help me understand whether this is important enough to include in Ray 2.7? If not, we can still commit to master after Friday",could help understand whether important enough include ray still commit master,issue,positive,positive,positive,positive,positive,positive
1690926661,"I'm working on this in stages so the PR doesn't get overwhelming, and because there's a couple of different things we need to to make the Sphinx upgrade smooth. I'll add to this checklist as I go:

- [x] Remove unnecessary doc dependencies. External modules for which documentation isn't being hosted shouldn't need to be installed to build the docs (e.g. tensorflow, torch, etc should be mocked out with `autodoc_mock_modules`)
- [x] Attempt an upgrade to the latest version of Sphinx and all Sphinx-related dependencies. The docs will likely be broken/look very different due to changes in `pydata-sphinx-theme` and `sphinx-book-theme`. Determine the work needed to get the docs into a presentable state given the new styles.",working get overwhelming couple different need make sphinx upgrade smooth add go remove unnecessary doc external documentation need build torch attempt upgrade latest version sphinx likely different due determine work get presentable state given new,issue,negative,positive,positive,positive,positive,positive
1690923108,"The failing test is flaking on master with the exact same error message.

@zhe-thoughts can you merge? This is needed to flip on the new persistence backend.",failing test master exact error message merge flip new persistence,issue,negative,positive,positive,positive,positive,positive
1690888359,Branch is freezed at this point. Please ask Zhe for approval and merge 🙏,branch point please ask approval merge,issue,positive,neutral,neutral,neutral,neutral,neutral
1690886639,"Doesn't seem like test failures are related.

@edoakes can you help merge this one?",seem like test related help merge one,issue,positive,neutral,neutral,neutral,neutral,neutral
1690885074,"LGTM.

One thing we should address in a follow-up PR before public launch in 2.8: the default filter of severity level is ""Warning"" and ""Error"". Currently, we only show ""Error"" which is inaccurate and confusing.",one thing address public launch default filter severity level warning error currently show error inaccurate,issue,negative,neutral,neutral,neutral,neutral,neutral
1690865166,"Hi, @kouroshHakha ,Thanks for your reply. But from my perspective, I can not see the difference between script i use and you sent above. So you means I don't need to change the image, just use the script you sent to me, it will work? I can not understand why this will effect the ray.train._checkpoint.

By the way, how could I download the 2.6.1 version of this script? I downloaded the 2.6.1 release of the ray, But I didn't find any dir about 04_finetuning_llms",hi thanks reply perspective see difference script use sent need change image use script sent work understand effect way could version script release ray find,issue,negative,positive,positive,positive,positive,positive
1690793040,"Major changes in response to comments:
- Removed client-side template since it is redundant with passing Nil.
- Removed STRICT auth since it is currently unused.",major response removed template since redundant passing nil removed strict since currently unused,issue,negative,negative,neutral,neutral,negative,negative
1690791632,"Looks like this is an issue in the `PyObjScanner`:

```
 20 def test_replace_nested_in_obj():
 21     class Outer:
 22         def __init__(self, inner):
 23             self._inner = inner
 24
 25     scanner = _PyObjScanner(source_type=Source)
 26     my_objs = [Outer(Source())]
 27
 28     found = scanner.find_nodes(my_objs)
 29     assert len(found) == 1
 30
 31     replaced = scanner.replace_nodes({obj: 1 for obj in found})
 32     assert replaced == [Outer(1)]
```

This fails on `assert len(found) == 1` because the object isn't found.

@ericl any ideas here?",like issue class outer self inner inner scanner outer source found assert found found assert outer assert found object found,issue,negative,neutral,neutral,neutral,neutral,neutral
1690760294,"Root cause is the newly added blocking RPC when initializing the GCS client during ray start, which takes 5 seconds and occurs before all other processes are started. This RPC basically serializes the rest of ray start after GCS server completely starts.",root cause newly added blocking client ray start basically rest ray start server completely,issue,negative,positive,positive,positive,positive,positive
1690759160,Can check this with `ray start --head`.,check ray start head,issue,negative,neutral,neutral,neutral,neutral,neutral
1690749056,"This may cause some performance degradation in spill-based benchmarks (like dataset shuffle), but we can adjust the threshold if that happens.

I'm also going to remove the dead code in plasma/store.cc but this can be reviewed first.",may cause performance degradation like shuffle adjust threshold also going remove dead code first,issue,negative,positive,neutral,neutral,positive,positive
1690744401,Please make sure to follow up with comments here! ,please make sure follow,issue,positive,positive,positive,positive,positive,positive
1690743595,"> Do we need to review it?

The regression was checked during releases. This was just to add to the logs. ",need review regression checked add,issue,negative,neutral,neutral,neutral,neutral,neutral
1690742691,"Rerun the release test ([link](https://buildkite.com/ray-project/release-tests-pr/builds/50475#018a246b-bf72-4059-b7a3-c8877b8da418)), and verified the time dropped to be same as before:

```
[INFO 2023-08-23 22:17:30,577] log.py: 41  Observed the following results:
  |  
  | metadata_load_time = 37.145771741867065
  | success = 1
```",rerun release test link time following success,issue,positive,positive,positive,positive,positive,positive
1690718419,Assign it to Ruiyang since he is working on this one activately.,assign since working one,issue,negative,neutral,neutral,neutral,neutral,neutral
1690714707,"Not being able to create child pages under Managed Kubernetes does not surprise me. Max has had to do some hacks to get around long build times that are related to large side navs, or he might've introduced limits to clean up the side nav bloat.

Re: doc tests - when you build locally, you should be running the doc tests and the CI also runs the full suite of doc tests. You don't need to do anything else.",able create child surprise get around long build time related large side might clean side bloat doc build locally running doc also full suite doc need anything else,issue,positive,positive,positive,positive,positive,positive
1690703230,"Ok, I believe we've narrowed down that this is an issue related to `max_concurrency` / concurrency groups. Here's the updated repro:

```
import asyncio
import random
import time

import ray
from ray import serve

@ray.remote(concurrency_groups={""TEST_CONCURRENCY_GROUP"": 1})
class Executor:
    @ray.method(concurrency_group=""TEST_CONCURRENCY_GROUP"")
    async def hi(self):
        return ""hi""


@ray.remote
class Caller:
    def __init__(self, executors):
        self._executors = executors

        self._task = asyncio.create_task(self.run())

    async def run(self):
        for i in range(1000000):
            start = time.time()
            print(f""Starting iteration {i}!"")
            await asyncio.gather(*[self.do_calls() for _ in range(1000)])
            print(f""Iteration finished in {time.time()-start}s. Sleeping for 30s."")
            time.sleep(30)

    async def do_calls(self):
        for _ in range(100):
            result = await random.choice(self._executors).hi.remote()

executors = [Executor.remote() for _ in range(10)]
callers = [Caller.remote(executors) for _ in range(20)]

import time;time.sleep(1000000000)
```

With this, there's a clear leak in the `Executor` actors:

<img width=""1433"" alt=""Screen Shot 2023-08-23 at 3 02 25 PM"" src=""https://github.com/ray-project/ray/assets/9871461/c4d317c8-1f1b-483b-b147-16ad36bc6700"">
",believe issue related concurrency import import random import time import ray ray import serve class executor hi self return hi class caller self run self range start print starting iteration await range print iteration finished sleeping self range result await range range import time clear leak executor screen shot,issue,negative,negative,neutral,neutral,negative,negative
1690696314,"Btw, the `ray::A` memory is very stable and not increased in my repo. Only the driver's memory increased. Testing with master branch.",ray memory stable driver memory testing master branch,issue,negative,neutral,neutral,neutral,neutral,neutral
1690690605,"This seems like a memory fragmentation issue and not leak.

The memory increase is reasonable because the tasks are submitting really fast and it'll be queued locally to avoid crashing the actor.

Later the memory didn't drop because of either the fragmentation or the system doesn't release them.


Run the following script:

```
import ray

ray.init()

@ray.remote
class A:
    cntr = 0

    def print(self):
        print(self.cntr)
        self.cntr += 1

a = A.remote()
for i in range(200000):
    a.print.remote()
import time

import gc
while True:
    import time
    time.sleep(10)
    gc.collect()
```

with 

glibc/tcmalloc/jemalloc and the memory usage is 1.2g/1.1g/300mb

If you inject some gc or just slowdown the speed, none of them will show the memory increasing. Close this issue.",like memory fragmentation issue leak memory increase reasonable really fast locally avoid actor later memory drop either fragmentation system release run following script import ray class print self print range import time import true import time memory usage inject slowdown speed none show memory increasing close issue,issue,negative,positive,positive,positive,positive,positive
1690681430,"> consider adding a unit test if possible.

I think I'll do this after the PR is merged since this one is urgent.",consider unit test possible think since one urgent,issue,negative,neutral,neutral,neutral,neutral,neutral
1690662463,hmm actually let's merge it for now and address it in the next PR...,actually let merge address next,issue,negative,neutral,neutral,neutral,neutral,neutral
1690661961,@vitsai can you address the comments? It'd be great to merge this by the branch cut.,address great merge branch cut,issue,negative,positive,positive,positive,positive,positive
1690659919,"Actually for the context of this test, there's not much difference. It doesn't depend on sleep timing. Infinite sleep is basically same as signal actor. ",actually context test much difference depend sleep timing infinite sleep basically signal actor,issue,negative,positive,neutral,neutral,positive,positive
1690652059,"> But I think long sleep will do the same job anyway.

If signal actor works then I think it's strictly better than sleep. (not blocking this PR).",think long sleep job anyway signal actor work think strictly better sleep blocking,issue,negative,positive,positive,positive,positive,positive
1690649931,@jjyao Maybe signal actor. But I think long sleep will do the same job anyway. ,maybe signal actor think long sleep job anyway,issue,negative,negative,neutral,neutral,negative,negative
1690623425,"Also, somehow windows failed to compile :( not sure what's going on.",also somehow compile sure going,issue,negative,positive,positive,positive,positive,positive
1690620125,My recommendation is P1 (behind other eng work) - hopeful to land in 2.7 but ok if later in patch release/merge into branch.,recommendation behind work hopeful land later patch branch,issue,positive,negative,negative,negative,negative,negative
1690606994,"Added a unit test and all comments are addressed. 
I used another test PR to run the release tests. But not sure if there is an infra issue, the job always stucks at waiting for the image building https://buildkite.com/ray-project/release-tests-pr/builds/50420#018a235a-0b83-47db-a45c-9d971c5313b7
I'll use this PR to run release tests again. If it still won't run. I think we can also merge this PR first.",added unit test used another test run release sure infra issue job always waiting image building use run release still wo run think also merge first,issue,negative,positive,positive,positive,positive,positive
1690598550,"> Nice. How can we verify if our ray is built with jemalloc properly btw?
it's easy:
```
$ nm bazel-bin/gcs_server | grep jemalloc
0000000000cb1db0 t je_jemalloc_postfork_child
0000000000cb1b80 t je_jemalloc_postfork_parent
0000000000cb19e0 t je_jemalloc_prefork
000000000020fea0 t jemalloc_constructor
```",nice verify ray built properly easy,issue,positive,positive,positive,positive,positive,positive
1690594527,I'll add the comment later given the cut is 5pm today.,add comment later given cut today,issue,negative,neutral,neutral,neutral,neutral,neutral
1690553152,"This is due to auto-cancellation on executor shutdown? This is unfortunate.

Short term, the risk of leaking resources might be worse than a few scary messages. That said, we should fix the messages:

1. For the task exception messages, is this since we aren't properly catching/suppressing these? I thought we actually did ray.get() the tasks after cancellation to suppress the errors before. This could be a regression [**in data**].

2. For the worker died messages, it seems like we should not be logging these for explicit cancellations [**in core**].",due executor shutdown unfortunate short term risk might worse scary said fix task exception since properly thought actually cancellation suppress could regression data worker like logging explicit core,issue,negative,negative,negative,negative,negative,negative
1690537634,"Hi. Has this issue been resolved in Ray 2.4 or later? I'm trying to multiple ray tune jobs on a single compute cluster at a time and I'm blocked by that issue. Till now I was using ray 2.0.0, but I'll be happy to upgrade if this is fixed.",hi issue resolved ray later trying multiple ray tune single compute cluster time blocked issue till ray happy upgrade fixed,issue,positive,positive,positive,positive,positive,positive
1690495095,"Thanks @rickyyx I just created two new  issues related to GCS FT: 
https://github.com/ray-project/ray/issues/38786 
https://github.com/ray-project/ray/issues/38785
hope they are related!",thanks two new related hope related,issue,positive,positive,neutral,neutral,positive,positive
1690491888,I will take a look in the coming sprint! If that's not too late.,take look coming sprint late,issue,negative,negative,negative,negative,negative,negative
1690490053,"> Test has been failing for far too long. Jailing.

why did we downgrade it from P0? what's the jailing policy like? I am afraid this would slow down fixing any failed test if they have been failing for a while due to the lower priority and the jailed status. ",test failing far long downgrade policy like afraid would slow fixing test failing due lower priority status,issue,negative,negative,negative,negative,negative,negative
1690463208,"Merged it because the previous run succeeds, and I only made doc changes.",previous run made doc,issue,negative,negative,negative,negative,negative,negative
1690461421,I need to think about how to set up unit test for added file under `python/ray/train/examples/experiment_tracking/*`. Not sure how to handle credentials for online mode. Should we set up test accounts for comet and wandb? Or do we already have those?,need think set unit test added file sure handle mode set test comet already,issue,negative,positive,positive,positive,positive,positive
1690408627,Staging already running with PA. ,staging already running pa,issue,negative,neutral,neutral,neutral,neutral,neutral
1690388790,"Some tests could not be run (for reasons not related to this), resizing the set to include only the 3 tests:
- microbenchmark
- dask_on_ray_100gb_sort
- stress_test_state_api_scale

^ These should also cover a good number of core APIs. 

Successful runs on PR: https://buildkite.com/ray-project/release-tests-pr/builds/49955#_
",could run related set include also cover good number core successful,issue,positive,positive,positive,positive,positive,positive
1690376995,"Hi, @zzkcaesar can you use nightly?

alternatively you can use 2.6.1 version of this script found here:
https://github.com/ray-project/ray/tree/workspace_templates_2.6.1/doc/source/templates/04_finetuning_llms_with_deepspeed",hi use nightly alternatively use version script found,issue,negative,neutral,neutral,neutral,neutral,neutral
1690357659,"> Wow, that sounds painful (both the dependencies as well as the speed is pretty abysmal). Do the hugging face datasets have a well-defined dataformat under the hood (like parquet)? If yes, could we use that format directly to read them (maybe combined with some metadata we get from the huggingface library)?
> 
> Just putting out this idea, maybe it is a bad idea :D

Yeah, under the hood, they use a memory mapped Arrow table, so ideally we would just be able to do distributed reads from that. When we looked into datasets API for our implementation though, the publicly available sharding methods didn't seem to split the dataset across nodes as intended. Amog dug around and potentially found some private APIs which may be helpful, but we decided to do that in a future PR since we'll need to ask the datasets developers some questions about them.

This PR at the very least allows for streaming reads for datasets which won't fit in memory, like the RedPajamas dataset, so I figured it would be worthwhile to get in.",wow painful well speed pretty abysmal hugging face hood like parquet yes could use format directly read maybe combined get library idea maybe bad idea yeah hood use memory arrow table ideally would able distributed implementation though publicly available seem split across intended dug around potentially found private may helpful decided future since need ask least streaming wo fit memory like figured would get,issue,positive,positive,neutral,neutral,positive,positive
1690352476,"@rkooo567 Apologies for the barrage of suggestions. They are mostly for capitalization of `Actors` and `Tasks`, which I believe we do when we are talking about Ray Actors and Tasks. Please correct if I misinterpreted. The remaining suggestions are copy edits. Thanks for updating the documentation!!
",barrage mostly capitalization believe talking ray please correct copy thanks documentation,issue,positive,positive,positive,positive,positive,positive
1690302717,"Failures
- Windows tests:test_streaming_generator::test_status_task_events_metrics is not flaky on master. But it seems unrelated
- rllib:TestLearnerGroupAsyncUpdate flaky on master and unrelated
- tune:test_commands flaky on master and unrelated
- chaos network delay test flaky on master and unrelated",flaky master unrelated flaky master unrelated tune flaky master unrelated chaos network delay test flaky master unrelated,issue,negative,neutral,neutral,neutral,neutral,neutral
1690218090,@volks73 merged 🎉 thanks again for contributing. This has been on the backlog for a long time and happy to see it done :),thanks backlog long time happy see done,issue,positive,positive,positive,positive,positive,positive
1690186765,"For safety, plumbed through a runtime flag to just GCS that can be passed as a hidden parameter to ray.init to toggle the behavior here.",safety flag hidden parameter toggle behavior,issue,negative,negative,negative,negative,negative,negative
1689788202,"@AchimA thanks for the report. If the logs are empty, perhaps you are connecting to a remote machine?",thanks report empty perhaps remote machine,issue,negative,neutral,neutral,neutral,neutral,neutral
1689772737,"I should add more information is that I tried install the ""ray.train._checkpoint"" by using pip. But it show that there is no package called ray.train._checkpoint. And I have searched google, also no useful info.",add information tried install pip show package also useful,issue,negative,positive,positive,positive,positive,positive
1689760573,"I've tested this on two different machines.

Ray works fine on my home pc, where I'm working with conda.

On my work machine I get the error as show in the issue by @joel-thomas-wilson.
The error occurs when running the following in my jupyter lab notebook:
``` python
import ray
ray.init()
```
On the machine with the issue, I've installed **Python 3.11.4** through **_MS Store_**.

The logs in `%TEMP%/ray/session*/logs/*` (as by suggestion by @mattip) are empty.",tested two different ray work fine home working work machine get error show issue error running following lab notebook python import ray machine issue python temp suggestion empty,issue,negative,positive,neutral,neutral,positive,positive
1689599682,"Here's the doc PR https://github.com/ray-project/ray/pull/38776

Note that we don't currently have a plan to support ""interrupting a single threaded actor when it is executing"". Please create an issue if this is an important use case for you. ",doc note currently plan support interrupting single threaded actor please create issue important use case,issue,positive,positive,positive,positive,positive,positive
1689522504,"This is a performance issue for training, why is this not getting fixed. I still see the issue on 2.6.3.",performance issue training getting fixed still see issue,issue,negative,positive,neutral,neutral,positive,positive
1689355856,"> Looks good, but I don't think we can merge until you fix the issue with the filter inputs UI.


https://github.com/ray-project/ray/assets/125417081/d8d93593-9d6d-4282-97fd-2288c6e3df81

As discussed in the #proj-ray-metrics channel, the filter input UI issue—where the value did not appear in the <AutoComplete>—has already been resolved.",good think merge fix issue filter channel filter input value appear already resolved,issue,positive,positive,positive,positive,positive,positive
1689307802,"Thanks for raising this @jadkins99 !
This will be fixed with this PR here (which will be merged today):
https://github.com/ray-project/ray/pull/38461

There will also be a better README that describes all one needs to know for running other, more custom (non gym-registered) envs.",thanks raising fixed today also better one need know running custom non,issue,positive,positive,positive,positive,positive,positive
1689298274,"Hi @kevin85421 @iycheng 

I wonder if there is any update on this issue? IMO this is more than an [observability-ux](https://github.com/ray-project/ray/labels/observability-ux) problem. Job level FT is not well documented, and it's very confusing for user about how to develop resilient Ray job applications...",hi wonder update issue problem job level well user develop resilient ray job,issue,negative,neutral,neutral,neutral,neutral,neutral
1689259424,@zcin needed to merge master due to failing required build,merge master due failing build,issue,negative,negative,negative,negative,negative,negative
1689154487,"Wow, that sounds painful (both the dependencies as well as the speed is pretty abysmal). Do the hugging face datasets have a well-defined dataformat under the hood (like parquet)? If yes, could we use that format directly to read them (maybe combined with some metadata we get from the huggingface library)?

Just putting out this idea, maybe it is a bad idea :D",wow painful well speed pretty abysmal hugging face hood like parquet yes could use format directly read maybe combined get library idea maybe bad idea,issue,positive,negative,negative,negative,negative,negative
1689130017,"> There's no active progress/plan to support this feature now

Thanks.",active support feature thanks,issue,positive,positive,neutral,neutral,positive,positive
1689128627,"@architkulkarni fixed linter's failures.

Current failures don't seem to be related.",fixed linter current seem related,issue,negative,positive,neutral,neutral,positive,positive
1689098994,"Quick read benchmark results:
- Tested streamed read with the [RedPajama-Data-1T](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T) dataset, using the `arxiv` train subset. 
- According to [this blog post](https://simonwillison.net/2023/Apr/17/redpajama-data/), this dataset is ~87GB with 100 total files.
- Read into Ray Data took 7830.77 seconds total.
- **Estimated throughput is then 87GB / 7830 seconds = 11.11 MBps.**",quick read tested read train subset according post total read ray data took total throughput,issue,negative,positive,positive,positive,positive,positive
1689093816,Let's go through all P0s in Th meeting and decide what to cherry pick>? ,let go th meeting decide cherry pick,issue,negative,neutral,neutral,neutral,neutral,neutral
1689092315,"To summarize the mess of dependencies in this PR:
- The new changes in the PR requires HF dataset's `IterableDataset.iter()` method, which is first introduced in datasets 2.8. This is the minimum version of `datasets` we need.
- However, datasets 2.8, 2.9, and 2.10 require `dill<0.3.7`, but we pin `dill==0.3.7`. So, we try a higher datasets version.
- datasets 2.11 and above requires pyarrow 8, so this will be incompatible with tests which use pyarrow 6.
- datasets 2.14 adds support for dill 0.3.7.

So, we want to use `datasets==2.14.0`. Because pinning this in `requirements` files would cause a lot of conflicts (e.g. the pyarrow 6/8 issue), we directly install this in `pipeline.build.yml` (and similar files) as an override job command, after installing base dependencies. Next, I dealt with the dependency conflicts for using `datasets==2.14.0`:
- The combination of `datasets==2.14.0` and `mosaicml==0.12.1` causes a torch error like the following:
```
E   OSError: /opt/miniconda/lib/python3.8/site-packages/torchtext/lib/libtorchtext.so: undefined symbol: _ZN2at4_ops10select_int4callERKNS_6TensorElN3c106SymIntE
```
- This is caused due to having two libraries with incompatible torch dependencies. The root cause of this was that `mosaicml==0.12.1` was downgrading torch to 1.13.1. So, we use `mosaicml==0.15.0` which upgrades torch to 2.0.1.
- Finally, using the combination of `dataset==2.14.0` and `mosaicml==0.15.0` causes a protobuf error because there is an incompatibility with 4.x, which is solved by pinning at `protobuf==3.20.1`.",summarize mess new method first minimum version need however require dill pin try higher version incompatible use support dill want use pinning would cause lot issue directly install similar override job command base next dealt dependency combination torch error like following undefined symbol due two incompatible torch root cause torch use torch finally combination error incompatibility pinning,issue,negative,negative,neutral,neutral,negative,negative
1689087984,"Btw, did we create a PR to raise better error when include-dashboard=True in non-default installation? ",create raise better error installation,issue,negative,positive,positive,positive,positive,positive
1689076610,"The script itself works, but because we created new working_dir with the same path, Ray does not re-create it. I will make another PR to change the test to use env var which should be easier.",script work new path ray make another change test use easier,issue,negative,positive,positive,positive,positive,positive
1689070122,"Nice catch! 

Q: is there any way to make this a common config everywhere? 
Q: what;s the downside of doing this? Why is it not enabled by default from grpc? ",nice catch way make common everywhere downside default,issue,negative,positive,positive,positive,positive,positive
1689069930,"Failed tests:
- WIndows test_actor_cancel
- Windows test_autoscaler_e2e 
- rllib:TestLearnerGroupAsyncUpdate 
- rllib:test_memory_leak_ppo 
- rllib:learning_tests_pendulum_cql    
- tune:test_client      
- tune:test_cluster_searcher
- chaos network delay test
- test_client_builder
- air:test_legacy_dataset_config.py::test_deterministic_per_epoch_preprocessor[1]

They're all flaky on master except `air:test_legacy_dataset_config` but I'm going to say that's unrelated based on the name.",tune tune chaos network delay test air flaky master except air going say unrelated based name,issue,negative,neutral,neutral,neutral,neutral,neutral
1689062155,Ah - this was added as a temp debug print and forgot to remove when submitting. I'm ok with either logging or removing it.,ah added temp print forgot remove either logging removing,issue,negative,neutral,neutral,neutral,neutral,neutral
1689058711,Didn't seem to pass the premerge tests (lint failure),seem pas lint failure,issue,negative,negative,negative,negative,negative,negative
1689029306,Failing tests are unrelated. Going to merge.,failing unrelated going merge,issue,negative,neutral,neutral,neutral,neutral,neutral
1689026782,"@stephanie-wang +1 to deduping this example from https://docs.ray.io/en/latest/ray-air/examples/torch_detection.html#fine-tuning-a-torch-object-detection-model.

Would prefer to have 1 canonical object detection training example instead of 2. 

What are the main gaps in that example? Would we be able to just fill those gaps?",example would prefer canonical object detection training example instead main example would able fill,issue,negative,positive,positive,positive,positive,positive
1689018637,"Hi all, I made some modifications to the ASHAv2 implementation from above:

- Trials are promoted from lower rungs before higher rungs. By extension, trials that have not started yet in any rungs are started before any actual promotions (as suggested by @MahdiNazemi). 
- Rungs are ""killed"", i.e. all paused trials are terminated, if the remaining paused trials have no chance of promoting. The logic for when rungs can be killed is in comments in the updated code. This makes it so that when all trials in the last rung as terminated (because they have reached `max_t`), the whole tune job is terminated, assuming there are no more possible promotions. This is also nice for using with a `ResourceChangingScheduler`, so the scheduler knows correctly when to allocate new resources.
- The `stop_last_trials` parameter from the base ASHA is added back in to allow trials to run past `max_t` if desired.
- Some additional logic was added so that pairing with `ResourceChangingScheduler` works correctly.

The only thing still different from the original ASHA paper is the stopping criterion. In the original ASHA paper, when there are no promotable trials, new trials are spawned, and the tuning run is stopped when a certain number of total trials are spawned (see the section ""Using n as ASHA’s stopping criterion"" from the [ASHA paper](https://arxiv.org/pdf/1810.05934.pdf), as well as Figure 3 in [this blog](https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/)). I decided not to add this for now as it was not needed for my use case, but I'm sure it could be added. 

Anyway, here's the code. I would happily make a PR for this, but I don't think the Ray team will want to have two different ASHA schedulers.

Edit: I forgot to mention that if you want to use this scheduler with `XGBoostTrainer`, there is a bug that prevents checkpointing from working currently and thus the paused trials are actually restarted from iteration 0 when resumed. Luckily, there is a workaround by commenting out [this if statement](https://github.com/ray-project/ray/blob/eea8e89285b3ed3104b727a2f4816f5c0548c2e0/python/ray/tune/trainable/function_trainable.py#L543). I will link an issue for this when I get around to creating one.

```python
from ray.tune.execution import trial_runner
from ray.tune.execution.tune_controller import TuneController
from ray.tune.experiment import Trial
from ray.tune.schedulers import FIFOScheduler, TrialScheduler, AsyncHyperBandScheduler
from ray.tune.schedulers.async_hyperband import _Bracket
import logging
from typing import Dict, List, Optional, Union, Generator
import numpy as np


logger = logging.getLogger(__name__)

class _Rung:
    """"""Simple class to store each Rung of an ASHA bracket""""""

    def __init__(
        self,
        milestone: int
    ):
        self.milestone = milestone
        self.started = []
        self.paused = []
        self.recorded = {}
        self.killed = False

    def kill_paused_trials(self, tune_controller: ""TuneController""):
        for trial in self.paused:
            if trial.status is not Trial.ERROR:
                tune_controller.stop_trial(trial)
        self.killed = True

    def __str__ (self):
        s = f""""""
        Milestone: {self.milestone}
        Started: {self.started}
        Paused: {self.paused}
        Recorded: {self.recorded}
        Killed: {self.killed}
        """"""
        return s


class _BracketV2(_Bracket):
    """"""Bookkeeping system to track the cutoffs.
    Rungs are created in reversed order so that we can more easily find
    the correct rung corresponding to the current iteration of the result.
    Example:
        >>> b = _BracketV2(1, 10, 2, 3)
        >>> b.on_result(trial1, 1, 2)  # CONTINUE
        >>> b.on_result(trial2, 1, 4)  # CONTINUE
        >>> b.cutoff(b._rungs[-1][1]) == 3.0  # rungs are reversed
        >>> b.on_result(trial3, 1, 1)  # STOP
        >>> b.cutoff(b._rungs[0][1]) == 2.0
    """"""

    def __init__(self, min_t: int, max_t: int, reduction_factor: float, s: int, stop_last_trials: bool = True):
        self.rf = reduction_factor
        MAX_RUNGS = int(np.log(max_t / min_t) / np.log(self.rf) - s + 1)
        self._rungs = [_Rung(min_t * self.rf ** (k + s)) for k in reversed(range(MAX_RUNGS))]
        self._stop_last_trials = stop_last_trials

    def cutoff(self, rung: _Rung) -> Union[None, int, float, complex, np.ndarray]:
        if len(rung.recorded) < self.rf:
            return None
        return np.nanpercentile(list(rung.recorded.values()), (1 - 1 / self.rf) * 100)

    def top_k_ids(self, rung: _Rung) -> List[str]:
        entries = list(rung.recorded.items())
        k = int(len(entries) / self.rf)
        top_rung = sorted(entries, key=lambda kv: kv[1], reverse=True)[0:k]
        return [tid for tid, value in top_rung]

    def on_result(
        self,
        trial: Trial,
        cur_iter: int,
        cur_rew: Optional[float],
        complete: bool = False,
    ) -> str:

        action = TrialScheduler.CONTINUE
        if cur_rew is None:
            logger.warning(
                ""Reward attribute is None! Consider""
                "" reporting using a different field.""
            )
            return action
        for rung_id, rung in enumerate(self._rungs):

            if (
                cur_iter >= rung.milestone
                and trial.trial_id in rung.recorded
                and not self._stop_last_trials
            ):
                # If our result has been recorded for this trial already, the
                # decision to continue training has already been made. Thus we can
                # skip new cutoff calculation and just continue training.
                # We can also break as milestones are descending.
                break
            if cur_iter < rung.milestone or trial.trial_id in rung.recorded:
                continue
            else:
                rung.recorded[trial.trial_id] = cur_rew
                # top_k_trial_ids = self.top_k_ids(recorded)
                if complete or trial.status != Trial.RUNNING:
                    break
                # if trial.trial_id not in top_k_trial_ids:

                if rung_id > 0:
                    # set_trace()
                    action = TrialScheduler.PAUSE
                        # action = TrialScheduler.STOP
                    rung.paused += [trial]
                break
        if action == TrialScheduler.PAUSE:
            print(trial, cur_iter)
        return action

    def debug_str(self) -> str:
        iters = "" | "".join(
            [
                ""Iter {:.3f}: {} [{} paused]"".format(
                    rung.milestone, self.cutoff(rung), len(rung.paused)
                )
                for rung in self._rungs
            ]
        )
        return ""Bracket: "" + iters

    def _promotable_trials_per_rung(self, rung: _Rung) -> Generator[Trial, None, None]:
        for tid in self.top_k_ids(rung):
            paused_trials = {p.trial_id: p for p in rung.paused}
            if tid in paused_trials:
                yield paused_trials[tid]

    def promotable_trials(self) -> Generator[Trial, None, None]:
        for rung in self._rungs:
            for trial in self._promotable_trials_per_rung(rung):
                yield trial

    def unpause_trial(self, trial: Trial) -> None:
        for i, rung in enumerate(self._rungs):
            if trial in rung.paused:
                rung.paused.pop(rung.paused.index(trial))
                if i > 0:
                    prev_rung = self._rungs[i-1]
                    prev_rung.started += [trial]
                else:
                    raise Exception(""ATTEMPTING TO UNPAUSE TRIAL "", trial, "" AT HIGHEST RUNG"")

            assert trial not in rung.paused


class ASHAv2(AsyncHyperBandScheduler):
    """"""Implements the Async Successive Halving with better termination.
    This should provide similar theoretical performance as HyperBand but
    avoid straggler issues that HyperBand faces. One implementation detail
    is when using multiple brackets, trial allocation to bracket is done
    randomly with over a softmax probability.
    See https://arxiv.org/abs/1810.05934
    Args:
        time_attr (str): A training result attr to use for comparing time.
            Note that you can pass in something non-temporal such as
            `training_iteration` as a measure of progress, the only requirement
            is that the attribute should increase monotonically.
        metric (str): The training result objective value attribute. Stopping
            procedures will use this attribute. If None but a mode was passed,
            the `ray.tune.result.DEFAULT_METRIC` will be used per default.
        mode (str): One of {min, max}. Determines whether objective is
            minimizing or maximizing the metric attribute.
        max_t (float): max time units per trial. Trials will be stopped after
            max_t time units (determined by time_attr) have passed.
        grace_period (float): Only stop trials at least this old in time.
            The units are the same as the attribute named by `time_attr`.
        reduction_factor (float): Used to set halving rate and amount. This
            is simply a unit-less scalar.
        brackets (int): Number of brackets. Each bracket has a different
            halving rate, specified by the reduction factor.
    """"""

    def __init__(
        self,
        time_attr: str = ""training_iteration"",
        metric: Optional[str] = None,
        mode: Optional[str] = None,
        max_t: int = 100,
        grace_period: int = 1,
        reduction_factor: float = 4,
        brackets: int = 1,
        stop_last_trials: bool = True
    ):
        super().__init__(
            time_attr=time_attr,
            metric=metric,
            mode=mode,
            max_t=max_t,
            grace_period=grace_period,
            reduction_factor=reduction_factor,
            brackets=brackets,
            stop_last_trials=stop_last_trials
        )

        # Tracks state for new trial add
        self._brackets = [
            _BracketV2(grace_period, max_t, reduction_factor, s)
            for s in range(brackets)
        ]
        self._num_paused = 0
        self._trial_info: Dict[str, _BracketV2] = {}  # Stores Trial -> Bracket

    def on_trial_result(
        self, tune_controller: ""TuneController"", trial: Trial, result: Dict
    ) -> str:
        action = TrialScheduler.CONTINUE
        if self._time_attr not in result or self._metric not in result:
            return action
        if result[self._time_attr] >= self._max_t and self._stop_last_trials:
            action = TrialScheduler.STOP
        else:
            bracket = self._trial_info[trial.trial_id]
            action = bracket.on_result(
                trial,
                result[self._time_attr],
                self._metric_op * result[self._metric],
            )
        if action == TrialScheduler.STOP:
            self._num_stopped += 1
        if action == TrialScheduler.PAUSE:
            self._num_paused += 1
        return action

    def on_trial_complete(
        self, tune_controller: ""TuneController"", trial: Trial, result: Dict
    ):
        if self._time_attr not in result or self._metric not in result:
            return
        bracket = self._trial_info[trial.trial_id]
        bracket.on_result(
            trial,
            result[self._time_attr],
            self._metric_op * result[self._metric],
            complete=True,
        )
        del self._trial_info[trial.trial_id]

    def _choose_new_trial_to_run(self, tune_controller: ""TuneController"") -> Optional[Trial]:
        for trial in tune_controller.get_trials():
            for bracket in self._brackets:
                if trial.status == Trial.PENDING and tune_controller.trial_executor.has_resources_for_trial(trial) and trial not in bracket._rungs[-1].started:
                    bracket._rungs[-1].started.append(trial)
                    print(f""CHOOSING NEW TRIAL {trial} TO RUN"")
                    return trial
        return None

    def _choose_paused_trial_to_run(self, tune_controller: ""TuneController"") -> Optional[Trial]:
        # Note: this only chooses paused trials NOT paused because they 
        # reached the end of their rung, but paused for some other reason,
        # e.g. from a ResourceChangingScheduler
        for bracket in self._brackets:
            for i, rung in enumerate(reversed(bracket._rungs)):
                for trial in rung.started:
                    if trial.status == Trial.PAUSED and trial in rung.started and trial.trial_id not in rung.recorded:
                        # If the trial was started, is currently paused, and was never recorded (i.e. reached rung milestone)
                        # then it was paused for some other reason (e.g. a ResourceChangingScheduler), and can be resumed
                        print(f""UNPAUSING TRIAL {trial} FROM RUNG {rung.milestone}"")
                        return trial

        return None

    def _choose_promotable_trial_to_run(self, tune_controller: ""TuneController"") -> Optional[Trial]:
        for bracket in self._brackets:
            # Iterate rungs lowest to highest
            # so that earlier trials promote faster
            rungs_lowest_to_highest = list(reversed(bracket._rungs))
            for rung_id, rung in enumerate(rungs_lowest_to_highest):
                for trial in bracket._promotable_trials_per_rung(rung):
                    # print(f""HAS RESOURCES FOR PROMOTABLE TRIAL {trial}: {tune_controller.trial_executor.has_resources_for_trial(trial)}"")
                    # print(""ACTOR RESOURCES"", tune_controller._actor_manager.get_live_actors_resources())
                    # live_resources = tune_controller._actor_manager.get_live_actors_resources()
                    # # used_cpu, total_cpu, used_gpu, total_gpu, _ = tune_controller._resource_updater._get_used_avail_resources(live_resources)

                    # used_cpu = live_resources.pop(""CPU"", 0)
                    # total_cpu = tune_controller._resource_updater._avail_resources.cpu
                    # used_gpu = live_resources.pop(""GPU"", 0)
                    # total_gpu = tune_controller._resource_updater._avail_resources.gpu
                    
                    # avail_cpu, avail_gpu = total_cpu - used_cpu, total_gpu - used_gpu
                    # print(f""AVAILABLE RESOURCES: CPU {avail_cpu}, GPU {avail_gpu}"")
                    # print(""TRIAL RESOURCES"", trial.placement_group_factory.required_resources)
                    if trial and tune_controller.trial_executor.has_resources_for_trial(trial) and trial.status == Trial.PAUSED:
                        assert trial.status == Trial.PAUSED
                        # logger.warning(f""Promoting trial {trial}."")
                        print(f""PROMOTING TRIAL {trial} from rung {rung.milestone}"")
                        bracket.unpause_trial(trial)
                        return trial

                if rung_id > 0:
                    prev_rung = rungs_lowest_to_highest[rung_id - 1]
                    if prev_rung.killed and \
                    not rung.killed and \
                    len(rung.started) == len(rung.recorded) and \
                    len(bracket.top_k_ids(rung)) == 0:
                        for trial in rung.paused:
                            bracket.unpause_trial(trial)
                            return trial
                    

        return None
    


    def _try_terminate_finished_rungs(self, bracket: _BracketV2, tune_controller: ""TuneController""):
        # When there are no more trials that can possibly be
        # promoted from a given rung, that rung can be ""terminated"", i.e.
        # all paused trialed can be terminated.
        
        # Note: such early stopping is only possible because no new trials are spawned, 
        # unlike the original ASHA paper which spawns a new trial in the lowest rung
        # when no trials can be promoted, see Alg 2 of https://arxiv.org/pdf/1810.05934.pdf.
        # Consequently, this ASHA will terminate when all trials have been promoted
        # as far as possible, and all trials in the last rung finish, unlike the original ASHA
        # which terminates once a provided number of trials are spawned.

        
        rungs_lowest_to_highest = list(reversed(bracket._rungs))
        for rung_id, rung in enumerate(rungs_lowest_to_highest):
            if rung_id == 0:
                if not rung.killed and len(rung.recorded) == len(tune_controller.get_trials()) and \
                not any(bracket._promotable_trials_per_rung(rung)):
                    # If all trials have been recorded in lowest rung and
                    # there are not promotable trials, can kill all remaining trials in rung
                    print(f""KILLING RUNG {rung.milestone}"")
                    rung.kill_paused_trials(tune_controller)

            else:
                prev_rung = rungs_lowest_to_highest[rung_id - 1]
                if prev_rung.killed and \
                not rung.killed and \
                len(rung.started) == len(rung.recorded) and \
                len(rung.started) > 0 and \
                not any(bracket._promotable_trials_per_rung(rung)):
                    # If the previous rung has been terminated, and
                    # all started trials in current rung have finished, and
                    # the rung has no promotable trials, can be terminated
                    print(f""KILLING RUNG {rung.milestone}"")
                    rung.kill_paused_trials(tune_controller)

    def choose_trial_to_run(self, tune_controller: ""TuneController"") -> Trial:

        # First try to start any new trials
        trial = self._choose_new_trial_to_run(tune_controller)

        # Next, try to start trials paused for some reason
        # other than finishing a rung (e.g. a ResourceChangingScheduler)
        if not trial:
            trial = self._choose_paused_trial_to_run(tune_controller)

        # Then, look for a promotable trial
        if not trial:
            trial = self._choose_promotable_trial_to_run(tune_controller)

        # Check if any rungs can be terminated
        for bracket in self._brackets:
            self._try_terminate_finished_rungs(bracket, tune_controller)

        return trial

    def debug_string(self) -> str:
        out = ""Using ASHAv2: num_stopped={}"".format(self._num_stopped)
        out += ""\n"" + ""\n"".join([b.debug_str() for b in self._brackets])
        return out
```
",hi made implementation lower higher extension yet actual chance logic code last rung whole tune job assuming possible also nice correctly allocate new parameter base added back allow run past desired additional logic added work correctly thing still different original paper stopping criterion original paper promotable new tuning run stopped certain number total see section stopping criterion paper well figure decided add use case sure could added anyway code would happily make think ray team want two different edit forgot mention want use bug working currently thus actually iteration luckily statement link issue get around one python import import import trial import import import logging import list optional union generator import logger class simple class store rung bracket self milestone milestone false self trial trial true self milestone return class bookkeeping system track reversed order easily find correct rung corresponding current iteration result example trial continue trial continue reversed trial stop self float bool true reversed range cutoff self rung union none float complex return none return list self rung list list sorted return tid tid value self trial trial optional float complete bool false action none reward attribute none consider different field return action rung enumerate result trial already decision continue training already made thus skip new cutoff calculation continue training also break descending break continue else complete break action action trial break action print trial return action self iter rung rung return bracket self rung generator trial none none tid rung tid yield tid self generator trial none none rung trial rung yield trial self trial trial none rung enumerate trial trial trial else raise exception trial trial highest rung assert trial class successive better termination provide similar theoretical performance avoid straggler one implementation detail multiple trial allocation bracket done randomly probability see training result use time note pas something measure progress requirement attribute increase monotonically metric training result objective value attribute stopping use attribute none mode used per default mode one min whether objective metric attribute float time per trial stopped time determined float stop least old time attribute float used set rate amount simply scalar number bracket different rate reduction self metric optional none mode optional none float bool true super state new trial add range trial bracket self trial trial result action result result return action result action else bracket action trial result result action action return action self trial trial result result result return bracket trial result result self optional trial trial bracket trial trial trial print choosing new trial trial run return trial return none self optional trial note end rung reason bracket rung enumerate reversed trial trial trial currently never rung milestone reason print unpausing trial trial rung return trial return none self optional trial bracket iterate highest promote faster list reversed rung enumerate trial rung print promotable trial trial trial print actor print available print trial trial trial assert trial trial print trial trial rung trial return trial rung trial trial return trial return none self bracket possibly given rung rung note early stopping possible new unlike original paper new trial rung see consequently terminate far possible last rung finish unlike original provided number list reversed rung enumerate rung rung promotable kill rung print killing rung else rung previous rung current rung finished rung promotable print killing rung self trial first try start new trial next try start reason finishing rung trial trial look promotable trial trial trial check bracket bracket return trial self return,issue,positive,positive,neutral,neutral,positive,positive
1689014048,"I wonder if it's actually a bug in the error message. When I switched it to 10, I got 
`WARNING actor_pool_map_operator.py:267 -- To ensure full parallelization across an actor pool of size 4, the specified batch size should be at most 5. Your configured batch size for this operator was 10.`",wonder actually bug error message switched got warning ensure full parallelization across actor pool size batch size batch size operator,issue,negative,positive,positive,positive,positive,positive
1688991669,"> Is it possible to add warnings for any unrecognized attribute? I feel that is the trade off for UX, if I set `porttttt: 9000` (typo for `port`) we will ignore the unrecognized attribute, and user might mistakenly think he is setting the port number.

Yes, we should still be doing validation, we should just do it differently: instead of mandating that no extra fields be allowed we have to make sure that _required_ fields are provided and if they are not -- we fail outright. In that case should be fairly obvious to the user what the problem is.",possible add unrecognized attribute feel trade set typo port ignore unrecognized attribute user might mistakenly think setting port number yes still validation differently instead extra make sure provided fail outright case fairly obvious user problem,issue,negative,neutral,neutral,neutral,neutral,neutral
1688973429,@angelinalg Thank you so much for the polish! The whole example now feels much more natural and fluid!!,thank much polish whole example much natural fluid,issue,positive,positive,positive,positive,positive,positive
1688973348,"Hi @akshay-anyscale, I just talked with @shrekris-anyscale, we haven't done any benchmarking with bigger payloads size between same node and corss code, and it is near the branch cut, I propose to guard it with feature flag. (Or we only merge before we see a clear benchmarking improvements for specific workloads and no regression, which i think it is not likely to happen before 2.7). cc: @edoakes ",hi done bigger size node code near branch cut propose guard feature flag merge see clear specific regression think likely happen,issue,negative,positive,neutral,neutral,positive,positive
1688972980,"It also fails on Mac but at a different point for a different reason. 

(ray) mstack@mstack601GQ ~ % ray start --head
Usage stats collection is disabled.

Local node IP: 127.0.0.1

--------------------
Ray runtime started.
--------------------

Next steps
  
  To connect to this Ray cluster:
    import ray
    ray.init()
  
  To submit a Ray job using the Ray Jobs CLI:
    RAY_ADDRESS='http://127.0.0.1:8265' ray job submit --working-dir . -- python my_script.py
  
  See https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html 
  for more information on submitting Ray jobs to the Ray cluster.
  
  To terminate the Ray runtime, run
    ray stop
  
  To view the status of the cluster, use
    ray status
  
  To monitor and debug Ray, view the dashboard at 
    127.0.0.1:8265
  
  If connection to the dashboard fails, check your firewall settings and network configuration.
(ray) mstack@mstack601GQ ~ % ray microbenchmark
Tip: set TESTS_TO_RUN='pattern' to run a subset of benchmarks
2023-08-22 14:33:35,578	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 127.0.0.1:6379...
2023-08-22 14:33:35,589	INFO worker.py:1612 -- Connected to Ray cluster. View the dashboard at http://127.0.0.1:8265 
single client get calls (Plasma Store) per second 6971.21 +- 223.35
single client put calls (Plasma Store) per second 6897.52 +- 442.34
multi client put calls (Plasma Store) per second 15883.03 +- 83.82
single client put gigabytes per second 56.51 +- 5.2
single client tasks and get batch per second 17.33 +- 0.52
(raylet) Spilled 3760 MiB, 48 objects, write throughput 2633 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.
(raylet) Spilled 7840 MiB, 99 objects, write throughput 3740 MiB/s.
(raylet) Spilled 12480 MiB, 157 objects, write throughput 4308 MiB/s.
(raylet) Spilled 17280 MiB, 217 objects, write throughput 4536 MiB/s.
multi client put gigabytes per second 15.95 +- 3.2
single client get object containing 10k refs per second 24.89 +- 0.61
single client wait 1k refs per second 9.21 +- 0.23
single client tasks sync per second 1780.81 +- 70.39
single client tasks async per second 17619.83 +- 334.14
multi client tasks async per second 19416.17 +- 110.76
1:1 actor calls sync per second 5488.15 +- 29.32
1:1 actor calls async per second 17483.99 +- 53.74
1:1 actor calls concurrent per second 10960.58 +- 87.3
1:n actor calls async per second 19048.88 +- 127.67
2023-08-22 14:38:30,088	WARNING worker.py:2006 -- WARNING: 42 PYTHON worker processes have been started on node: 0af7e9fd614a2175920c890c0f24bea942ed470c5cb0e1b8753fe327 with address: 127.0.0.1. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).
n:n actor calls async per second 37300.6 +- 1014.14
2023-08-22 14:38:49,864	WARNING worker.py:2006 -- WARNING: 50 PYTHON worker processes have been started on node: 0af7e9fd614a2175920c890c0f24bea942ed470c5cb0e1b8753fe327 with address: 127.0.0.1. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).
(raylet) [2023-08-22 14:38:50,704 E 22974 2742920] (raylet) logging.cc:97: Unhandled exception: N5boost10wrapexceptINS_6system12system_errorEEE. what(): pipe_select_interrupter: Too many open files [system:24]
(raylet) [2023-08-22 14:38:50,710 E 22974 2742920] (raylet) logging.cc:104: Stack trace: 
(raylet)  0   raylet                              0x00000001027e5264 _ZN3raylsERNSt3__113basic_ostreamIcNS0_11char_traitsIcEEEERKNS_10StackTraceE + 84 ray::operator<<()
(raylet) 1   raylet                              0x00000001027e5450 _ZN3ray16TerminateHandlerEv + 228 ray::TerminateHandler()
(raylet) 2   libc++abi.dylib                     0x00000001a8c0bf48 _ZSt11__terminatePFvvE + 16 std::__terminate()
(raylet) 3   libc++abi.dylib                     0x00000001a8c0ed34 __cxa_get_exception_ptr + 0 __cxa_get_exception_ptr
(raylet) 4   libc++abi.dylib                     0x00000001a8c0ece0 _ZN10__cxxabiv1L22exception_cleanup_funcE19_Unwind_Reason_CodeP17_Unwind_Exception + 0 __cxxabiv1::exception_cleanup_func()
(raylet) 5   raylet                              0x0000000102d7104c _ZN5boost15throw_exceptionINS_6system12system_errorEEEvRKT_RKNS_15source_locationE + 72 boost::throw_exception<>()
(raylet) 6   raylet                              0x0000000102d71090 _ZN5boost4asio6detail14do_throw_errorERKNS_6system10error_codeEPKcRKNS_15source_locationE + 48 boost::asio::detail::do_throw_error()
(raylet) 7   raylet                              0x0000000102d62164 _ZN5boost4asio6detail23pipe_select_interrupterD2Ev + 0 boost::asio::detail::pipe_select_interrupter::~pipe_select_interrupter()
(raylet) 8   raylet                              0x0000000102d5f278 _ZN5boost4asio6detail14kqueue_reactorC2ERNS0_17execution_contextE + 460 boost::asio::detail::kqueue_reactor::kqueue_reactor()
(raylet) 9   raylet                              0x0000000102614e18 _ZN5boost4asio6detail16service_registry6createINS1_14kqueue_reactorENS0_17execution_contextEEEPNS5_7serviceEPv + 36 boost::asio::detail::service_registry::create<>()
(raylet) 10  raylet                              0x0000000102d6718c _ZN5boost4asio6detail16service_registry14do_use_serviceERKNS0_17execution_context7service3keyEPFPS4_PvES9_ + 172 boost::asio::detail::service_registry::do_use_service()
(raylet) 11  raylet                              0x0000000102d636d0 _ZN5boost4asio6detail28reactive_socket_service_baseC2ERNS0_17execution_contextE + 56 boost::asio::detail::reactive_socket_service_base::reactive_socket_service_base()
(raylet) 12  raylet                              0x00000001027590c0 _ZN5boost4asio6detail16service_registry6createINS1_23reactive_socket_serviceINS0_2ip3tcpEEENS0_10io_contextEEEPNS0_17execution_context7serviceEPv + 64 boost::asio::detail::service_registry::create<>()
(raylet) 13  raylet                              0x0000000102d6718c _ZN5boost4asio6detail16service_registry14do_use_serviceERKNS0_17execution_context7service3keyEPFPS4_PvES9_ + 172 boost::asio::detail::service_registry::do_use_service()
(raylet) 14  raylet                              0x0000000102757a18 _Z9CheckFreei + 92 CheckFree()
(raylet) 15  raylet                              0x00000001024a2d9c _ZN3ray6raylet10WorkerPool15GetNextFreePortEPi + 148 ray::raylet::WorkerPool::GetNextFreePort()
(raylet) 16  raylet                              0x00000001024a466c _ZN3ray6raylet10WorkerPool14RegisterWorkerERKNSt3__110shared_ptrINS0_15WorkerInterfaceEEEixNS2_8functionIFvNS_6StatusEiEEE + 512 ray::raylet::WorkerPool::RegisterWorker()
(raylet) 17  raylet                              0x00000001023db648 _ZN3ray6raylet11NodeManager35ProcessRegisterClientRequestMessageERKNSt3__110shared_ptrINS_16ClientConnectionEEEPKh + 952 ray::raylet::NodeManager::ProcessRegisterClientRequestMessage()
(raylet) 18  raylet                              0x00000001023daf64 _ZN3ray6raylet11NodeManager20ProcessClientMessageERKNSt3__110shared_ptrINS_16ClientConnectionEEExPKh + 1296 ray::raylet::NodeManager::ProcessClientMessage()
(raylet) 19  raylet                              0x0000000102480b30 _ZNSt3__110__function6__funcIZN3ray6raylet6Raylet12HandleAcceptERKN5boost6system10error_codeEE3$_2NS_9allocatorISA_EEFvNS_10shared_ptrINS2_16ClientConnectionEEExRKNS_6vectorIhNSB_IhEEEEEEclEOSF_OxSK_ + 56 std::__1::__function::__func<>::operator()()
(raylet) 20  raylet                              0x00000001027345d0 _ZN3ray16ClientConnection14ProcessMessageERKN5boost6system10error_codeE + 828 ray::ClientConnection::ProcessMessage()
(raylet) 21  raylet                              0x0000000102742184 _ZN12EventTracker15RecordExecutionERKNSt3__18functionIFvvEEENS0_10shared_ptrI11StatsHandleEE + 108 EventTracker::RecordExecution()
(raylet) 22  raylet                              0x000000010273e7c4 _ZN5boost4asio6detail7read_opINS0_19basic_stream_socketINS0_7generic15stream_protocolENS0_15any_io_executorEEENS0_17mutable_buffers_1EPKNS0_14mutable_bufferENS1_14transfer_all_tEZN3ray16ClientConnection20ProcessMessageHeaderERKNS_6system10error_codeEE3$_7EclESG_mi + 568 boost::asio::detail::read_op<>::operator()()
(raylet) 23  raylet                              0x000000010273eb40 _ZN5boost4asio6detail23reactive_socket_recv_opINS0_17mutable_buffers_1ENS1_7read_opINS0_19basic_stream_socketINS0_7generic15stream_protocolENS0_15any_io_executorEEES3_PKNS0_14mutable_bufferENS1_14transfer_all_tEZN3ray16ClientConnection20ProcessMessageHeaderERKNS_6system10error_codeEE3$_7EES8_E11do_completeEPvPNS1_19scheduler_operationESJ_m + 292 boost::asio::detail::reactive_socket_recv_op<>::do_complete()
(raylet) 24  raylet                              0x0000000102d65ed8 _ZN5boost4asio6detail9scheduler10do_run_oneERNS1_27conditionally_enabled_mutex11scoped_lockERNS1_21scheduler_thread_infoERKNS_6system10error_codeE + 624 boost::asio::detail::scheduler::do_run_one()
(raylet) 25  raylet                              0x0000000102d5aa50 _ZN5boost4asio6detail9scheduler3runERNS_6system10error_codeE + 200 boost::asio::detail::scheduler::run()
(raylet) 26  raylet                              0x0000000102d5a938 _ZN5boost4asio10io_context3runEv + 32 boost::asio::io_context::run()
(raylet) 27  raylet                              0x0000000102377e64 main + 3400 main
(raylet) 28  dyld                                0x00000001a88fbf28 start + 2236 start
(raylet) 
(raylet) *** SIGABRT received at time=1692740330 ***
(raylet) PC: @        0x1a8c1c764  (unknown)  __pthread_kill
(raylet) [2023-08-22 14:38:50,710 E 22974 2742920] (raylet) logging.cc:361: *** SIGABRT received at time=1692740330 ***
(raylet) [2023-08-22 14:38:50,710 E 22974 2742920] (raylet) logging.cc:361: PC: @        0x1a8c1c764  (unknown)  __pthread_kill
(raylet) [2023-08-22 14:38:50,710 E 24441 2766905] core_worker.cc:201: Failed to register worker e17551908e3b22f0f854073943f5ba1760d8d7dcd4330180833c3af6 to Raylet. IOError: [RayletClient] Unable to register worker with raylet. No such file or directory
2023-08-22 14:38:52,412	WARNING worker.py:2006 -- Raylet is terminated: ip=127.0.0.1, id=0af7e9fd614a2175920c890c0f24bea942ed470c5cb0e1b8753fe327. Termination is unexpected. Possible reasons include: (1) SIGKILL by the user or system OOM killer, (2) Invalid memory access from Raylet causing SIGSEGV or SIGBUS, (3) Other termination signals. Last 20 lines of the Raylet logs:
    12  raylet                              0x00000001027590c0 _ZN5boost4asio6detail16service_registry6createINS1_23reactive_socket_serviceINS0_2ip3tcpEEENS0_10io_contextEEEPNS0_17execution_context7serviceEPv + 64 boost::asio::detail::service_registry::create<>()
    13  raylet                              0x0000000102d6718c _ZN5boost4asio6detail16service_registry14do_use_serviceERKNS0_17execution_context7service3keyEPFPS4_PvES9_ + 172 boost::asio::detail::service_registry::do_use_service()
    14  raylet                              0x0000000102757a18 _Z9CheckFreei + 92 CheckFree()
    15  raylet                              0x00000001024a2d9c _ZN3ray6raylet10WorkerPool15GetNextFreePortEPi + 148 ray::raylet::WorkerPool::GetNextFreePort()
    16  raylet                              0x00000001024a466c _ZN3ray6raylet10WorkerPool14RegisterWorkerERKNSt3__110shared_ptrINS0_15WorkerInterfaceEEEixNS2_8functionIFvNS_6StatusEiEEE + 512 ray::raylet::WorkerPool::RegisterWorker()
    17  raylet                              0x00000001023db648 _ZN3ray6raylet11NodeManager35ProcessRegisterClientRequestMessageERKNSt3__110shared_ptrINS_16ClientConnectionEEEPKh + 952 ray::raylet::NodeManager::ProcessRegisterClientRequestMessage()
    18  raylet                              0x00000001023daf64 _ZN3ray6raylet11NodeManager20ProcessClientMessageERKNSt3__110shared_ptrINS_16ClientConnectionEEExPKh + 1296 ray::raylet::NodeManager::ProcessClientMessage()
    19  raylet                              0x0000000102480b30 _ZNSt3__110__function6__funcIZN3ray6raylet6Raylet12HandleAcceptERKN5boost6system10error_codeEE3$_2NS_9allocatorISA_EEFvNS_10shared_ptrINS2_16ClientConnectionEEExRKNS_6vectorIhNSB_IhEEEEEEclEOSF_OxSK_ + 56 std::__1::__function::__func<>::operator()()
    20  raylet                              0x00000001027345d0 _ZN3ray16ClientConnection14ProcessMessageERKN5boost6system10error_codeE + 828 ray::ClientConnection::ProcessMessage()
    21  raylet                              0x0000000102742184 _ZN12EventTracker15RecordExecutionERKNSt3__18functionIFvvEEENS0_10shared_ptrI11StatsHandleEE + 108 EventTracker::RecordExecution()
    22  raylet                              0x000000010273e7c4 _ZN5boost4asio6detail7read_opINS0_19basic_stream_socketINS0_7generic15stream_protocolENS0_15any_io_executorEEENS0_17mutable_buffers_1EPKNS0_14mutable_bufferENS1_14transfer_all_tEZN3ray16ClientConnection20ProcessMessageHeaderERKNS_6system10error_codeEE3$_7EclESG_mi + 568 boost::asio::detail::read_op<>::operator()()
    23  raylet                              0x000000010273eb40 _ZN5boost4asio6detail23reactive_socket_recv_opINS0_17mutable_buffers_1ENS1_7read_opINS0_19basic_stream_socketINS0_7generic15stream_protocolENS0_15any_io_executorEEES3_PKNS0_14mutable_bufferENS1_14transfer_all_tEZN3ray16ClientConnection20ProcessMessageHeaderERKNS_6system10error_codeEE3$_7EES8_E11do_completeEPvPNS1_19scheduler_operationESJ_m + 292 boost::asio::detail::reactive_socket_recv_op<>::do_complete()
    24  raylet                              0x0000000102d65ed8 _ZN5boost4asio6detail9scheduler10do_run_oneERNS1_27conditionally_enabled_mutex11scoped_lockERNS1_21scheduler_thread_infoERKNS_6system10error_codeE + 624 boost::asio::detail::scheduler::do_run_one()
    25  raylet                              0x0000000102d5aa50 _ZN5boost4asio6detail9scheduler3runERNS_6system10error_codeE + 200 boost::asio::detail::scheduler::run()
    26  raylet                              0x0000000102d5a938 _ZN5boost4asio10io_context3runEv + 32 boost::asio::io_context::run()
    27  raylet                              0x0000000102377e64 main + 3400 main
    28  dyld                                0x00000001a88fbf28 start + 2236 start
    
    [2023-08-22 14:38:50,710 E 22974 2742920] (raylet) logging.cc:361: *** SIGABRT received at time=1692740330 ***
    [2023-08-22 14:38:50,710 E 22974 2742920] (raylet) logging.cc:361: PC: @        0x1a8c1c764  (unknown)  __pthread_kill

Traceback (most recent call last):
  File ""/Users/mstack/venv/ray/bin/ray"", line 8, in <module>
    sys.exit(main())
  File ""/Users/mstack/venv/ray/lib/python3.8/site-packages/ray/scripts/scripts.py"", line 2474, in main
    return cli()
  File ""/Users/mstack/venv/ray/lib/python3.8/site-packages/click/core.py"", line 1157, in __call__
    return self.main(*args, **kwargs)
  File ""/Users/mstack/venv/ray/lib/python3.8/site-packages/click/core.py"", line 1078, in main
    rv = self.invoke(ctx)
  File ""/Users/mstack/venv/ray/lib/python3.8/site-packages/click/core.py"", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/Users/mstack/venv/ray/lib/python3.8/site-packages/click/core.py"", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/Users/mstack/venv/ray/lib/python3.8/site-packages/click/core.py"", line 783, in invoke
    return __callback(*args, **kwargs)
  File ""/Users/mstack/venv/ray/lib/python3.8/site-packages/ray/scripts/scripts.py"", line 1822, in microbenchmark
    main()
  File ""/Users/mstack/venv/ray/lib/python3.8/site-packages/ray/_private/ray_perf.py"", line 241, in main
    results += timeit(
  File ""/Users/mstack/venv/ray/lib/python3.8/site-packages/ray/_private/ray_microbenchmark_helpers.py"", line 26, in timeit
    fn()
  File ""/Users/mstack/venv/ray/lib/python3.8/site-packages/ray/_private/ray_perf.py"", line 239, in actor_multi2_direct_arg
    ray.get([c.small_value_batch_arg.remote(n) for c in clients])
  File ""/Users/mstack/venv/ray/lib/python3.8/site-packages/ray/_private/auto_init_hook.py"", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File ""/Users/mstack/venv/ray/lib/python3.8/site-packages/ray/_private/client_mode_hook.py"", line 103, in wrapper
    return func(*args, **kwargs)
  File ""/Users/mstack/venv/ray/lib/python3.8/site-packages/ray/_private/worker.py"", line 2495, in get
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.",also mac different point different reason ray ray start head usage collection disabled local node ray next connect ray cluster import ray submit ray job ray ray job submit python see information ray ray cluster terminate ray run ray stop view status cluster use ray status monitor ray view dashboard connection dashboard check network configuration ray ray tip set run subset ray cluster address connected ray cluster view dashboard single client get plasma store per second single client put plasma store per second client put plasma store per second single client put per second single client get batch per second raylet mib write throughput set disable message raylet mib write throughput raylet mib write throughput raylet mib write throughput client put per second single client get object per second single client wait per second single client sync per second single client per second client per second actor sync per second actor per second actor concurrent per second actor per second warning warning python worker node address could result large number due blocked see discussion actor per second warning warning python worker node address could result large number due blocked see discussion raylet raylet unhandled exception many open system raylet raylet stack trace raylet raylet ray raylet raylet ray raylet raylet raylet raylet raylet boost raylet raylet boost raylet raylet boost raylet raylet boost raylet raylet boost raylet raylet boost raylet raylet boost raylet raylet boost raylet raylet boost raylet raylet raylet raylet ray raylet raylet ray raylet raylet ray raylet raylet ray raylet raylet raylet raylet ray raylet raylet raylet raylet boost raylet raylet boost raylet raylet boost raylet raylet boost raylet raylet boost raylet raylet main main raylet start start raylet raylet received raylet unknown raylet raylet received raylet raylet unknown raylet register worker raylet unable register worker raylet file directory warning raylet termination unexpected possible include user system killer invalid memory access raylet causing termination last raylet raylet boost raylet boost raylet raylet ray raylet ray raylet ray raylet ray raylet raylet ray raylet raylet boost raylet boost raylet boost raylet boost raylet boost raylet main main start start raylet received raylet unknown recent call last file line module main file line main return file line return file line main file line invoke return file line invoke return file line invoke return file line main file line main file line file line file line return file line wrapper return file line get raise value actor unexpectedly finishing task,issue,positive,positive,neutral,neutral,positive,positive
1688931241,"Is it possible to add warnings for any unrecognized attribute? 
I feel that is the trade off for UX, if I set `porttttt: 9000` (typo for `port`)  we will ignore the unrecognized attribute, and user might mistakenly think he is setting the port number.
 ",possible add unrecognized attribute feel trade set typo port ignore unrecognized attribute user might mistakenly think setting port number,issue,negative,neutral,neutral,neutral,neutral,neutral
1688922568,"Ensure Ray Dashboard's Serve system page also shows the gRPC port

<img width=""1679"" alt=""image"" src=""https://github.com/ray-project/ray/assets/7553988/f2012866-020f-43f7-805a-9ae16735a44c"">
",ensure ray dashboard serve system page also port image,issue,negative,neutral,neutral,neutral,neutral,neutral
1688918137,"> Do you have a proposal for the API for this part? ray start sounds like the right place to do it.

Yeah, so I think we should probably be very generic and not TPU specific which I have ideas for:

At a high level, Ray currently offers users a way to provide 1) the resource type and 2) the number of resources. The limitation we're touching on is that now need to introduce a new schema for users to tell Ray 3) how those resources are linked. 

For instance:
- In case of TPU pod slices, it's physical (ICI)
- In case of multiple VMs in the same zone, it could be ""zonal network"" (DCN)
- In case of multiple VMs in the same region but different zones, it could be ""regional network""
- but by default, Ray currently implicitly assumes either ""network"" or intra-raylet (e.g. the various placement group strategies already defined today).

As a starting point (and drawing inspiration from resource representations in Ray) a V0 of an extensible API we can iterate on would be to introduce a new (optional) logical field in the GCS that represents these relationships, perhaps like this (note I'm using Python dataclasses for illustrations but it probably could/should be represented by protobufs):

```
@dataclasses.dataclass
class GroupingType(Enum):
    name: str
    priority: int

@dataclasses.dataclass
class Grouping:
    id: str
    type: GroupingType

# Predefined types
TPU_ICI = GroupingType('TPU_ICI', 1)
ZONE_NETWORK = GroupingType('ZONE_NETWORK', 2)
REGION_NETWORK = GroupingType('REGION_NETWORK', 3)
```
where imagine TPU VMs within a TPU pod could be represented by:
```
[Grouping(id=tpu_pod_name, type=GroupingType.TPU_ICI),
 Grouping(id='us-central2-b', GroupingType.ZONE_NETWORK),
 Grouping(id='us-central', GroupingType.REGION_NETWORK)]
```
- `GroupingType` represents a named representation of grouping with an associated priority, i.e. `TPU_ICI` is ""more local"" than `ZONE_NETWORK` which is ""more local"" than `REGION_NETWORK` (and implicitly intra raylet is the most local).
- We set a default set of pre-defined groupings and users can extend on it easily.

With this interface:
- NodeProviders could trivially access the zone of a particular node or relationships between nodes and set them as tags. Then similar to resource autodetection, we could add mechanisms to report grouping information from the tags
  - (and the GCP node provider would define TPU specific behavior)
- Users could specify this as part of `ray start`, i.e. `ray start --num-tpus=4 --tpu-grouping=tpu-pod-1 --zone-grouping=us-central2-b --region-grouping=us-central2 --custom-grouping=""{'id': 'us', 'type': {'name': 'CONTINENTAL_NETWORK', 'priority': 4}}""`, i.e.:
  - This machine/associated IP/raylet has 4 TPU chips attached to it,
  - It's part of the TPU cluster `tpu-pod-1`,
  - It is in `us-central2-b` which is region `us-central2`,
  - User wants to have a denotion of a continental network, which is less local than `REGION_NETWORK` and this is of the continental network of 'us'.


Then imagine we have 3 TPU pod slices, each with two workers. 2 of the TPU pods are in the same data center, and one of the TPU pods is in a different data center. GCS could ultimately have access to a view like this:
```
{
    TPU_ICI: { 
        ""TPU-POD-1"": [""10.139.0.1"", ""10.139.0.2""],
        ""TPU-POD-2"": [""10.140.0.1"", ""10.140.0.2""],
        ""TPU-POD-3"": [""10.141.0.1"", ""10.141.0.2""],
    },
    ZONE_NETWORK: {
        ""us-central2-b"": [""10.139.0.1"", ""10.139.0.2"", ""10.140.0.1"", ""10.140.0.2""],
        ""us-west4-a"": [""10.141.0.1"", ""10.141.0.2""],
    },
    REGION_NETWORK: {
        ""us-central2"": [""10.139.0.1"", ""10.139.0.2"", ""10.140.0.1"", ""10.140.0.2""],
        ""us-west4"": [""10.141.0.1"", ""10.141.0.2""],
    },
}
```
This level of view could help us define a new placement group strategy that leverages this grouping information, e.g. possibly:
```
PlacementGroup([{""TPU"": 4}, {""TPU"": 4}], strategy=GroupingStrategy(level=GroupingStrategies.TPU_ICI))
```
which would tell Ray that I want to run on 2 nodes each with 4 TPU chips that are linked by ICI.

(I'm not sure if this makes sense from the placement group strategy as I have no idea how it works!)",proposal part ray start like right place yeah think probably generic specific high level ray currently way provide resource type number limitation touching need introduce new schema tell ray linked instance case pod physical case multiple zone could zonal network case multiple region different could regional network default ray currently implicitly either network various placement group already defined today starting point drawing inspiration resource ray extensible iterate would introduce new optional logical field perhaps like note python probably class name priority class grouping id type imagine within pod could grouping grouping grouping representation grouping associated priority local local implicitly raylet local set default set extend easily interface could trivially access zone particular node set similar resource could add report grouping information node provider would define specific behavior could specify part ray start ray start chip attached part cluster region user continental network le local continental network imagine pod two data center one different data center could ultimately access view like level view could help u define new placement group strategy grouping information possibly would tell ray want run chip linked sure sense placement group strategy idea work,issue,positive,positive,neutral,neutral,positive,positive
1688901376,"Great points @architkulkarni! 

Per your last point:
> Is the mental model of the user more like ""I want to run this on a v4-16 and I don't want to think about the exact number of hosts, chips, etc"", or more like ""I want to run this on 64 hosts and I don't want to look up the TPU type""? 

My mental model of this is actually ""I want to run on a `v4-16` but I don't have a way to tell Ray which *specific* machines to run on because there is a relationship between particular raylets.""

> Can you walk through the example ...

Regarding the example, sorry - I picked an unfortunate TPU configuration lol, let's use a v4-16 as an example:
```
@ray.remote
def my_function():
  ...

# E.g. a v4-16:
num_tpu_hosts = 2
pg = placement_group([{""TPU"": 4} for _ in range(num_tpu_hosts)], strategy=""TPU_AFFINITY"")

my_function.options(
    scheduling_strategy=PlacementGroupSchedulingStrategy(placement_group=pg)).remote()
```

so a v4-16 has two hosts (`num_tpu_hosts = 2`) and each host has 4 TPU chips (`{""TPU"": 4}`).


> For example it might make sense to push down some of the logic about [{""TPU"": 4} for _ in range(num_tpu_hosts)], strategy=""TPU_AFFINITY"" into the Ray code itself, so the user-facing API would just be something like f.options( scheduling_strategy=TPUSchedulingStrategy(ray.tpu.V4_16)).remote(). But maybe this is too TPU-specific to include in Ray, and instead it should be done at the level of ML libraries on top of Ray.

My hunch is that I agree with you - pushing `pg = placement_group([{""TPU"": 4} for _ in range(num_tpu_hosts)], strategy=""TPU_AFFINITY"")` is probably too TPU specific for Ray Core and a better fit for one of the higher level AIR libraries (at least for now).

> Do you have a proposal for the API for this part? ray start sounds like the right place to do it.

Yes, will follow up in the next comment.


",great per last point mental model user like want run want think exact number chip like want run want look type mental model actually want run way tell ray specific run relationship particular walk example regarding example sorry picked unfortunate configuration let use example range two host chip example might make sense push logic range ray code would something like maybe include ray instead done level top ray hunch agree pushing range probably specific ray core better fit one higher level air least proposal part ray start like right place yes follow next comment,issue,positive,positive,neutral,neutral,positive,positive
1688895744,"@richardliaw @scv119 Do you approve the API change? I'll review the PR, perhaps you can suggest other reviewers too.",approve change review perhaps suggest,issue,negative,neutral,neutral,neutral,neutral,neutral
1688870152,After discussion with @sihanwang41 also added an env variable `RAY_SERVE_ENABLE_PROXY_LOCALITY_ROUTING` which can be used to turn off the locality routing for http proxies in case we need it.,discussion also added variable used turn locality routing case need,issue,negative,neutral,neutral,neutral,neutral,neutral
1688864681,"Checked out test failures and all seems unrelated.

@edoakes @architkulkarni can you folks help merging this in?",checked test unrelated help,issue,negative,neutral,neutral,neutral,neutral,neutral
1688793203,"@woshiyyya - could you help check if all CI test failures are related? Let's probably retrigger the CI again, as many CI tests are failing.",could help check test related let probably many failing,issue,negative,positive,positive,positive,positive,positive
1688778983,"@zcin good catch! Missed it for `DeploymentSchema` (and will do it for RayActorOptionsSchema as a result as well)

@GeneDer yes, i'm gonna be adding/fixing tests for it ",good catch result well yes gon na,issue,positive,positive,positive,positive,positive,positive
1688744025,"The test failures are unrelated:

* `Serve Release Tests` ([`tune_serve_golden_notebook_client_smoke_test`](https://buildkite.com/ray-project/oss-ci-build-pr/builds/32739#018a0bb4-3ea2-463f-853c-7e7a4b03b72f/3951-4717)) is failing on `master`:

<img width=""1341"" alt=""Screen Shot 2023-08-22 at 11 52 11 AM"" src=""https://github.com/ray-project/ray/assets/92341594/3a106752-b0af-4b1a-9347-27bfc2b27f63"">

* `Windows Build & Test` ([`test_cancel` and `test_autoscaler_e2e`](https://buildkite.com/ray-project/oss-ci-build-pr/builds/32768#018a101b-b7de-49bf-bece-b7ab8d685c18/6560-9913)) are flaky on `master`:

<img width=""1334"" alt=""Screen Shot 2023-08-22 at 11 46 24 AM"" src=""https://github.com/ray-project/ray/assets/92341594/a5f5ef7f-c50d-40d1-9c6b-700f05378a47"">

<img width=""1337"" alt=""Screen Shot 2023-08-22 at 11 46 10 AM"" src=""https://github.com/ray-project/ray/assets/92341594/fab5fbd4-4545-44cf-b730-7fb95b6852d9"">",test unrelated serve release failing master screen shot build test flaky master screen shot screen shot,issue,negative,neutral,neutral,neutral,neutral,neutral
1688739898,"> Given self._consecutive_health_check_failures = 0 is only a side affect of calling self.try_update_status(HTTPProxyStatus.HEALTHY), I think it be better if we can explicitly just set the consecutive failures counter to 0 instead of needing to call try_update_status, but not a blocker.

Yeah that's a good point. I'd prefer to stick with the current implementation for now since it makes sense semantically to me (we're trying to update the status back to `HEALTHY` after a successful health check). However, I could see why that might be confusing (since the state transition matrix gets more convoluted). Since we're close to branch cut, I'll leave this as is, and we can change it in the future if it's confusing.",given side affect calling think better explicitly set consecutive counter instead needing call blocker yeah good point prefer stick current implementation since sense semantically trying update status back healthy successful health check however could see might since state transition matrix convoluted since close branch cut leave change future,issue,positive,positive,positive,positive,positive,positive
1688737759,"The test failures are unrelated:

* `Windows Build & Test` ([`test_cancel` and `test_autoscaler_e2e`](https://buildkite.com/ray-project/oss-ci-build-pr/builds/32768#018a101b-b7de-49bf-bece-b7ab8d685c18/6560-9913)) are flaky on `master`:

<img width=""1334"" alt=""Screen Shot 2023-08-22 at 11 46 24 AM"" src=""https://github.com/ray-project/ray/assets/92341594/a5f5ef7f-c50d-40d1-9c6b-700f05378a47"">

<img width=""1337"" alt=""Screen Shot 2023-08-22 at 11 46 10 AM"" src=""https://github.com/ray-project/ray/assets/92341594/fab5fbd4-4545-44cf-b730-7fb95b6852d9"">

* `Doctest` ([`workflow:doctest`](https://buildkite.com/ray-project/oss-ci-build-pr/builds/32768#018a1026-db0b-4dcb-96a8-351a434b78e8/1692-2238)) is unrelated.

* `Serve Release Tests` ([`tune_serve_golden_notebook_client_smoke_test`](https://buildkite.com/ray-project/oss-ci-build-pr/builds/32768#018a1026-dafa-47d2-bf93-4ad91bb32c24/3951-4748)) has been failing on `master`:

<img width=""1333"" alt=""Screen Shot 2023-08-22 at 11 48 17 AM"" src=""https://github.com/ray-project/ray/assets/92341594/6d4df945-f320-439d-af62-1bc5bc1ecb86"">


",test unrelated build test flaky master screen shot screen shot unrelated serve release failing master screen shot,issue,negative,neutral,neutral,neutral,neutral,neutral
1688731662,"I'm not sure if this makes sense for the proxy. The proxy can only become `HEALTHY` after being in the `STARTING` state. If it ever becomes `UNHEALTHY`, it's killed and restarted. The controller doesn't wait for it to become `HEALTHY` again.

Would we wait for the proxy to pass 3 health checks before transitioning them from `STARTING` to `HEALTHY`? That seems to be the only impact that this change would have.",sure sense proxy proxy become healthy starting state ever becomes unhealthy controller wait become healthy would wait proxy pas health starting healthy impact change would,issue,positive,positive,positive,positive,positive,positive
1688725337,Test failure link added!,test failure link added,issue,negative,negative,negative,negative,negative,negative
1688721644,"could you add a link to a test failure example, like in the PR comment?",could add link test failure example like comment,issue,negative,negative,negative,negative,negative,negative
1688621036,"@rkooo567 , are you planning on looking at this? If this is a p0, do we need to get in for ray 2.7?",looking need get ray,issue,negative,neutral,neutral,neutral,neutral,neutral
1688614912,"We need a stamp from ray-docs owner as well, @ericl , @pcmoritz . For context, a recent change unexpectedly downgraded pydantic to 1.9.2, and we have seen some weird behaviors in ray.init with that downgrade. This PR upgrades it back.",need stamp owner well context recent change unexpectedly seen weird downgrade back,issue,negative,negative,neutral,neutral,negative,negative
1688609703,"@alexeykudinkin @edoakes please take another look! 
`_locality_routing` is turned on for HTTP proxies but otherwise off by default. The flag is turned on for `test_handle_prefers_replicas_on_same_node`. Tests in `test_replica_scheduler.py` are also run with the flag both on and off.",please take another look turned otherwise default flag turned also run flag,issue,negative,neutral,neutral,neutral,neutral,neutral
1688608368,"@rkooo567  Yes, this is fixed by reverting my PR. Feel free to close this issue",yes fixed feel free close issue,issue,positive,positive,positive,positive,positive,positive
1688608286,"> Hi! Yes, I am happy to contribute! What's the best way to proceed? cheers

I'm thinking extending a Future class (as a wrapper of ray.ObjectRef) returned when calling submit(). Calling Future.result() will behave just like get_unordered_next(), but waits for this specific future instead of any future, and pop that future if it's finished just like in get_unordered_next(). In this way users have a result() API to wait for future returned in submit(), and we can also keep the original behavior unchanged with the same APIs in ActorPool?

But while I was trying to implement it I found get_unordered_next() method has some bugs (#38607 and #38635). I'm thinking maybe I should fix those two first before implementing this Future API?",hi yes happy contribute best way proceed thinking extending future class wrapper returned calling submit calling behave like specific future instead future pop future finished like way result wait future returned submit also keep original behavior unchanged trying implement found method thinking maybe fix two first future,issue,positive,positive,positive,positive,positive,positive
1688599278,"The PR description has a repro. I think any image folder will work, but the one used there is downloaded from the private bucket `s3://imagenetmini-1000/train`.

The expected behavior is that there should be no error messages at all.",description think image folder work one used private bucket behavior error,issue,negative,neutral,neutral,neutral,neutral,neutral
1688544020,@martalist we've run into a number of issues related to serialization in order to support Pydantic 2 in Ray Serve. We are currently targeting the 2.8 release to address these.,run number related serialization order support ray serve currently release address,issue,negative,neutral,neutral,neutral,neutral,neutral
1688514704,"Failed tests:
- serve:test_standalone_3   unrelated
- tests:test_actor_cancel  unrelated
- windows test_autoscaler_e2e unrelated and flaky on master (https://flaky-tests.ray.io/)
- doc GPU tests unrelateD
- `tune_serve_golden_notebook_client_smoke_test` unrelated
- chaos network delay test unrelated",serve unrelated unrelated unrelated flaky master doc unrelated unrelated chaos network delay test unrelated,issue,negative,neutral,neutral,neutral,neutral,neutral
