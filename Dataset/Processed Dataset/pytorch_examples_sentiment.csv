id,original_comment,processed_comment,source,sentiment_VADER,sentiment_textblob,sentiment_pattern,sentiment_bert,sentiment_spacy,max_voted_sentiment
2002175146,Sure yeah I'd be review a new version. One example which might very compelling is a llama 7b like inference using libtorch. A smaller decoder model would also be fun ,sure yeah review new version one example might compelling llama like inference smaller model would also fun,issue,positive,positive,positive,positive,positive,positive
2002171697,@msaroufim Do you think this tutorial is valuable to add? I prototyped first version #777  4 years back. But there was no reviews for it. I can raise one more PR for it again.,think tutorial valuable add first version back raise one,issue,negative,positive,positive,positive,positive,positive
1998331051,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!,thank contributor license agreement accept code meta open source project thanks,issue,positive,positive,neutral,neutral,positive,positive
1998010191,"### <span aria-hidden=""true"">âœ…</span> Deploy Preview for *pytorch-examples-preview* canceled.


|  Name | Link |
|:-:|------------------------|
|<span aria-hidden=""true"">ğŸ”¨</span> Latest commit | 1e81da30df9bc699c925f51382f94f77340c5345 |
|<span aria-hidden=""true"">ğŸ”</span> Latest deploy log | https://app.netlify.com/sites/pytorch-examples-preview/deploys/65f3499e1a783700081bd700 |",span true deploy preview name link span true hammer latest commit span true latest deploy log,issue,positive,positive,positive,positive,positive,positive
1998009794,"Hi @nicolasrosa! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20pytorch%2Fexamples%20%231239). Thanks!",hi thank pull request welcome community action order merge pull request code require sign contributor license agreement seem one file process order u review merge please sign behalf someone else employer individual may sufficient employer may need sign corporate tooling perform afterwards pull request tagged process may take hour please give time u received error please contact u thanks,issue,positive,positive,positive,positive,positive,positive
1994498950,"@bowen480 
Yes, I upgraded PyTorch to get rid of this error.",yes get rid error,issue,negative,neutral,neutral,neutral,neutral,neutral
1994210799,"> @ravijo your PyTorch version is quite old, might be best to upgrade

do you solve this problem?
",version quite old might best upgrade solve problem,issue,positive,positive,positive,positive,positive,positive
1971774372,"Make sure you've got the latest version of CUDA and cuDNN installed along with the latest NVIDIA GPU drivers.
Install CUDA toolkit and make sure to match the CUDA version with the one supported by your PyTorch installation.
You can try Anaconda or Miniconda to manage your python environment as they help avoid conflicts with system packages.
Install PyTorch with GPU support using the appropriate version for your CUDA installation
If you're using multiple GPUs, consider installing NVIDIA NCCL (NVIDIA Collective Communication Library) for optimized GPU communication
Set the following environment variables in your training script to enable multi-GPU training
Execute your training script with the necessary commands to utilize multiple GPUs

Hope it Helps!!!

",make sure got latest version along latest install make sure match version one installation try anaconda manage python environment help avoid system install support appropriate version installation multiple consider collective communication library communication set following environment training script enable training execute training script necessary utilize multiple hope,issue,positive,positive,positive,positive,positive,positive
1953291444,"transform=transforms.Compose this line needs, blank.
For vision transformer example, need to run in mac mps.",line need blank vision transformer example need run mac,issue,negative,neutral,neutral,neutral,neutral,neutral
1953070039,@ebrahimpichka so because this works in CI with no issues what we can do is add an option to argparser to pass in an `no-ssl` flag which would disable verification. if you wanna send me a PR for that i can merge easily,work add option pas flag would disable verification wan na send merge easily,issue,negative,positive,positive,positive,positive,positive
1947503064,"I can just bypass the SSL verification by passing `verify=False` into the get request, for both gat and gcn.
But I'm not sure if it causes any security issues or not. @msaroufim 

```py
    # Load the dataset
    cora_url = 'https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz'
    path = './cora'

    if os.path.isfile(os.path.join(path, 'cora.content')) and os.path.isfile(os.path.join(path, 'cora.cites')):
        print('Dataset already downloaded...')
    else:
        print('Downloading dataset...')
        with requests.get(cora_url, stream=True, verify=False) as tgz_file:
            with tarfile.open(fileobj=tgz_file.raw, mode='r:gz') as tgz_object:
                tgz_object.extractall()

```",bypass verification passing get request gat sure security load path path path print already else print,issue,positive,positive,positive,positive,positive,positive
1943091096,"I am happy to abide by the terms of the standard BSD-3 license, but am leery about the CLA. I'm not comfortable at the moment signing something I don't understand which is outside the purview of the typical attribution-only licensing pattern. I'll go ahead and close this and base the examples off my fork, sorry for the wasted CI cycles!",happy abide standard license leery comfortable moment something understand outside purview typical pattern go ahead close base fork sorry wasted,issue,positive,negative,neutral,neutral,negative,negative
1931468845,"> @HassanBinHaroon If you use ""**python main.py -b 512 --dist-backend nccl -a resnet18 imagenet/**"" does it run smoothly?

@Jaiaid Yes, it absolutely runs smoothly. ",use python run smoothly yes absolutely smoothly,issue,negative,positive,positive,positive,positive,positive
1930725085,"@HassanBinHaroon 
If you use ""**python main.py -b 512 --dist-backend nccl -a resnet18 imagenet/**"" does it run smoothly?",use python run smoothly,issue,negative,positive,positive,positive,positive,positive
1930536313,"@Jaiaid I am using just SINGLE GPU for training. I am not forcefully adjusting the rank and It's -1 by default. The command that I have been using is ""**python main.py -b 512 --dist-backend gloo -a resnet18 imagenet/**"" ",single training forcefully rank default command python,issue,negative,negative,negative,negative,negative,negative
1930528348,"@HassanBinHaroon 
If you are using one rank in one GPU than nccl backend should be fine but if there are multiple rank using a single GPU than it will be an issue. I guess I should improve the message.
In your case, are you using multiple rank/process in a single GPU machine?",one rank one fine multiple rank single issue guess improve message case multiple single machine,issue,positive,negative,negative,negative,negative,negative
1930513083,"@Jaiaid Yes, I have tried running code --dist-backend nccl. It logs the user warning that (I think) you recently added and code executes smoothly BTW. ",yes tried running code user warning think recently added code smoothly,issue,negative,positive,positive,positive,positive,positive
1929973549,"@HassanBinHaroon 
yes they should come with pytorch
yes, it should otherwise throw an error from NCCL (something like internal check error), to avoid that you have to do `--dist-backend gloo`
BTW have you tried actually using NCCL to run your code?",yes come yes otherwise throw error something like internal check error avoid tried actually run code,issue,negative,neutral,neutral,neutral,neutral,neutral
1929101659,"> @HassanBinHaroon yes it should be --dist-backend gloo (without the quote). You do not need to give quote for cmd line args but if in your system nccl version <2.5. It should be fine to use nccl

@Jaiaid **Does the nccl and gloo comes with PyTorch?** 
If yes, then how to check their availability and version?

**I am checking the availability using the following commands. Is it the right procedure?**

""import torch.distributed as dist

print(dist.is_available())  # Should print True if distributed is available
print(dist.is_nccl_available())  # Should print True if NCCL is available
print(dist.is_gloo_available())  # Should print True if Gloo is available
""
**Moreover, I am checking the nccl version using the following command, Again, please enlighten is it correct procedure?**

""print(torch.cuda.nccl.version)""

**The output is (2,19,3)**

**Does it mean that I have to be compulsorily using --dist-backend gloo?**

Please elaborate, Thanks!",yes without quote need give quote line system version fine use come yes check availability version availability following right procedure import print print true distributed available print print true available print print true available moreover version following command please enlighten correct procedure print output mean compulsorily please elaborate thanks,issue,positive,positive,positive,positive,positive,positive
1927868376,"> @HassanBinHaroon yes it should be --dist-backend gloo (without the quote). You do not need to give quote for cmd line args but if in your system nccl version <2.5. It should be fine to use nccl

@Jaiaid Thanks!",yes without quote need give quote line system version fine use thanks,issue,positive,positive,positive,positive,positive,positive
1927626853,"@HassanBinHaroon 
yes
it should be --dist-backend gloo (without the quote). You do not need to give quote for cmd line args
but if in your system nccl version <2.5. It should be fine to use nccl ",yes without quote need give quote line system version fine use,issue,positive,positive,positive,positive,positive,positive
1922107770,"re-committing  this change with slight modification of warning rather than assertion. Also the GPU count variable is set to one to work with later world_size variable initiation. Although I think semantically it is confusing for CPU only platform.
This is done to help users avoid some possible frustration when using default dist backend which is NCCL.",change slight modification warning rather assertion also count variable set one work later variable initiation although think semantically platform done help avoid possible frustration default,issue,negative,negative,neutral,neutral,negative,negative
1905313148,"@msaroufim FYI, rebased to main so this change is tested by the CI job and passed the test.",main change tested job test,issue,negative,positive,positive,positive,positive,positive
1903333833,Could you join this group https://discord.gg/s79KKh8T - I'm Seraphim there and we can chat more quickly,could join group seraph chat quickly,issue,negative,positive,positive,positive,positive,positive
1903196559,"> Hi @lancerts a really valuable contribution would be adding a CI job to test all our C++ examples, if this is something you're interested in please lmk!

Yeah, sure, definitely interested in this work.",hi really valuable contribution would job test something interested please yeah sure definitely interested work,issue,positive,positive,positive,positive,positive,positive
1902848473,"Hi @lancerts a really valuable contribution would be adding a CI job to test all our C++ examples, if this is something you're interested in please lmk!",hi really valuable contribution would job test something interested please,issue,positive,positive,positive,positive,positive,positive
1891299271,"fixed in https://github.com/pytorch/examples/pull/1214 via ` push_back(Functional(
        [](torch::Tensor input) { return torch::log_softmax(input, 1); }))`",fixed via functional torch input return torch input,issue,negative,positive,neutral,neutral,positive,positive
1882390629,"Is there any update on this issue?
**Update**: I have checked the current code again, and they have called `dist.all_reduce()` before comparing it with `best_acc`, so I think everything is fine now.",update issue update checked current code think everything fine,issue,negative,positive,positive,positive,positive,positive
1880482458,"I am also facing the same issue...Any solution to this?
",also facing issue solution,issue,negative,neutral,neutral,neutral,neutral,neutral
1877141414,"and i have this problem on save and load sharded too...
https://github.com/pytorch/pytorch/issues/103627
how can i solve it?",problem save load sharded solve,issue,negative,neutral,neutral,neutral,neutral,neutral
1869715592,"Hey there, I agree with your overall suggestion. I believe the file name should just indicate the model name and any other details can be included in the TXT file itself or even another file if required. Also please note that the model info itself would be stored in the pt file itself. 

I am happy to take a look and submit a PR implementing those suggestions",hey agree overall suggestion believe file name indicate model name included file even another file also please note model would file happy take look submit,issue,positive,positive,positive,positive,positive,positive
1868415517,"@rmib200 
I forgot the details already, but can you please upgrade your PyTorch and check it please?",forgot already please upgrade check please,issue,positive,neutral,neutral,neutral,neutral,neutral
1858762456,"Traceback (most recent call last):
  File ""/home/xxx/.local/lib/python3.9/site-packages/torch/multiprocessing/spawn.py"", line 74, in _wrap
    fn(i, *args)
  File ""/home/xxx/work/xxx/xxx-main/src/multigpu.py"", line 102, in main
    trainer = Trainer(model, train_data, optimizer, rank, save_every)
  File ""/home/xxx/work/xxx/xxx-main/src/multigpu.py"", line 50, in __init__
    self.model = DDP(model, device_ids=[gpu_id])
  File ""/home/xxx/.local/lib/python3.9/site-packages/torch/nn/parallel/distributed.py"", line 795, in __init__
    _verify_param_shape_across_processes(self.process_group, parameters)
  File ""/home/xxx/.local/lib/python3.9/site-packages/torch/distributed/utils.py"", line 265, in _verify_param_shape_across_processes
    return dist._verify_params_across_processes(process_group, tensors, logger)
RuntimeError: DDP expects same model across all ranks, but Rank 0 has 2 params, while rank 1 has inconsistent 0 params.

------------------------------------------
none is modified",recent call last file line file line main trainer trainer model rank file line model file line file line return logger model across rank rank inconsistent none,issue,negative,negative,negative,negative,negative,negative
1858762124,"and after a long time, the APP is crash:

[E ProcessGroupNCCL.cpp:475] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800263 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:475] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800316 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:489] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:495] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:916] [Rank 1] NCCL watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800316 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:489] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:495] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:916] [Rank 0] NCCL watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=2, Timeout(ms)=1800000) ran for 1800263 milliseconds before timing out.",long time crash rank watchdog caught collective operation ran timing rank watchdog caught collective operation ran timing timed due asynchronous nature subsequent might run data avoid data inconsistency taking entire process rank watchdog thread exception rank watchdog caught collective operation ran timing timed due asynchronous nature subsequent might run data avoid data inconsistency taking entire process rank watchdog thread exception rank watchdog caught collective operation ran timing,issue,negative,negative,negative,negative,negative,negative
1858756315,"Hi
I came across the same issue when I run multigpu.py(your demo sample code):

please see the scree shot:

root:~/work/ $ python ./src/multigpu.py --batch_size=32 --total_epochs=30 --save_every=5

but nothing output.

From nvidia-smi output, I can see GPU is running 100%, but memory is very low, it seems GPU is hanging there, but nothing output in terminal, I also can see 2 process is running (because I set 2 GPU to use).



output of nvidia-smi:

_> +---------------------------------------------------------------------------------------+
> | NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |
> |-----------------------------------------+----------------------+----------------------+
> | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
> | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
> |                                         |                      |               MIG M. |
> |=========================================+======================+======================|
> |   0  Tesla T4                       Off | 00000000:63:00.0 Off |                    0 |
> | N/A   76C    P0              48W /  70W |    225MiB / 15360MiB |    100%   E. Process |
> |                                         |                      |                  N/A |
> +-----------------------------------------+----------------------+----------------------+
> |   1  Tesla T4                       Off | 00000000:C3:00.0 Off |                    0 |
> | N/A   66C    P0              41W /  70W |    225MiB / 15360MiB |    100%   E. Process |
> |                                         |                      |                  N/A |
> +-----------------------------------------+----------------------+----------------------+
> |   2  Tesla T4                       Off | 00000000:E3:00.0 Off |                    0 |
> | N/A   40C    P8              11W /  70W |      5MiB / 15360MiB |      0%   E. Process |
> |                                         |                      |                  N/A |
> +-----------------------------------------+----------------------+----------------------+
>                                                                                          
> +---------------------------------------------------------------------------------------+
> | Processes:                                                                            |
> |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
> |        ID   ID                                                             Usage      |
> |=======================================================================================|
> |    0   N/A  N/A     37920      C   .../.conda/envs/bin/python      220MiB |
> |    1   N/A  N/A     37921      C   .../.conda/envs/bin/python      220MiB |
> +---------------------------------------------------------------------------------------+_

So what's wrong? I ran other scripts, all cannot be executed like above one...

output of nvtop:

 Device 0 [Tesla T4] PCIe GEN 3@16x RX: 0.000 KiB/s TX: 0.000 KiB/s Device 1 [Tesla T4] PCIe GEN 3@16x RX: 0.000 KiB/s TX: 0.000 KiB/s
 GPU 1590MHz MEM 5000MHz TEMP  76Â°C FAN N/A% POW  48 /  70 W        GPU 1590MHz MEM 5000MHz TEMP  66Â°C FAN N/A% POW  40 /  70 W
 GPU[|||||||||||||||||||||||100%] MEM[|           0.639Gi/15.000Gi] GPU[|||||||||||||||||||||||100%] MEM[|           0.639Gi/15.000Gi]

 Device 2 [Tesla T4] PCIe GEN 1@16x RX: 0.000 KiB/s TX: 0.000 KiB/s
 GPU 300MHz  MEM 405MHz  TEMP  40Â°C FAN N/A% POW  10 /  70 W
 GPU[                         0%] MEM[|           0.424Gi/15.000Gi]
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
100â”‚GPU0 %â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ 100â”‚GPU1 %â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ 100â”‚GPU2 %                                                  â”‚
   â”‚GPU0 mem%                                               â”‚    â”‚GPU1 mem%                                               â”‚    â”‚GPU2 mem%                                               â”‚
   â”‚                                                        â”‚    â”‚                                                        â”‚    â”‚                                                        â”‚
   â”‚                                                        â”‚    â”‚                                                        â”‚    â”‚                                                        â”‚
 75â”‚                                                        â”‚  75â”‚                                                        â”‚  75â”‚                                                        â”‚
   â”‚                                                        â”‚    â”‚                                                        â”‚    â”‚                                                        â”‚
   â”‚                                                        â”‚    â”‚                                                        â”‚    â”‚                                                        â”‚
   â”‚                                                        â”‚    â”‚                                                        â”‚    â”‚                                                        â”‚
   â”‚                                                        â”‚    â”‚                                                        â”‚    â”‚                                                        â”‚
 50â”‚                                                        â”‚  50â”‚                                                        â”‚  50â”‚                                                        â”‚
   â”‚                                                        â”‚    â”‚                                                        â”‚    â”‚                                                        â”‚
   â”‚                                                        â”‚    â”‚                                                        â”‚    â”‚                                                        â”‚
   â”‚                                                        â”‚    â”‚                                                        â”‚    â”‚                                                        â”‚
 25â”‚                                                        â”‚  25â”‚                                                        â”‚  25â”‚                                                        â”‚
   â”‚                                                        â”‚    â”‚                                                        â”‚    â”‚                                                        â”‚
   â”‚                                                        â”‚    â”‚                                                        â”‚    â”‚                                                        â”‚
   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚    â”‚                                                        â”‚
  0â”‚                                                        â”‚   0â”‚                                                        â”‚   0â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
   â””28sâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€21sâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€14sâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€7sâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€0sâ”˜    â””28sâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€21sâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€14sâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€7sâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€0sâ”˜    â””28sâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€21sâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€14sâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€7sâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€0s

I test many times. I think maybe DDP API has some issue or bug.
Could you find any reason of this?  thanks.",hi came across issue run sample code please see scree shot root python nothing output output see running memory low hanging nothing output terminal also see process running set use output driver version version name volatile fan temp compute mig mib mib process mib mib process mib mib process type process name memory id id usage mib mib wrong ran executed like one output device gen device gen mem temp fan pow mem temp fan pow mem mem device gen mem temp fan pow mem mem mem mem test many time think maybe issue bug could find reason thanks,issue,positive,positive,neutral,neutral,positive,positive
1848326269,"same problem and I sloved by:
1. keep all multi-process processing job/function/param in CPU
2. move model/param/data to MPS in multi-process processing job/function by:
    model.to(torch.device(""mps""))
    in_tensor.to(torch.device(""mps""))

Mostly, multi-process communication needs to be implemented through other methods to ensure normal parameter updates.",problem keep move mostly communication need ensure normal parameter,issue,negative,positive,positive,positive,positive,positive
1837450862,"Greetings, 
I just came across this and would also like to be a part of it. As of now, I can only see that toolformer is the only model that is not taken up or assigned to anyone. Is it fine if I work on this? And also, is yes, I would need some assistance as to where to start from on this.
Regards,
Sarthak <3",came across would also like part see model taken assigned anyone fine work also yes would need assistance start,issue,positive,positive,positive,positive,positive,positive
1837300957,"Just had the same problem and debugged it. You need to put 
  torch.cuda.set_device(rank)
before dist.init_process_group()",problem need put rank,issue,negative,negative,negative,negative,negative,negative
1820292291,"* test failures are related to being unable to import init_device_mesh (?), no gpu's available (?), and lastly need to modify these tests to launch via the .sh files associated with each example (to run torchscript):
~~~
Traceback (most recent call last):
  File ""tensor_parallelism/tensor_parallel_example.py"", line 5, in <module>
    from torch.distributed._tensor.device_mesh import init_device_mesh
ImportError: cannot import name 'init_device_mesh' from 'torch.distributed._tensor.device_mesh' (/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/torch/distributed/_tensor/device_mesh.py)
tensor parallel example failed
Traceback (most recent call last):
  File ""tensor_parallelism/sequence_parallel_example.py"", line 5, in <module>
    from torch.distributed._tensor.device_mesh import init_device_mesh
ImportError: cannot import name 'init_device_mesh' from 'torch.distributed._tensor.device_mesh' (/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/torch/distributed/_tensor/device_mesh.py)
sequence parallel example failed
Traceback (most recent call last):
  File ""tensor_parallelism/two_d_parallel_example.py"", line 18, in <module>
    from torch.distributed._tensor.device_mesh import init_device_mesh
ImportError: cannot import name 'init_device_mesh' from 'torch.distributed._tensor.device_mesh' (/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/torch/distributed/_tensor/device_mesh.py)
2D parallel example failed
Requires at least 8 GPUs to run, but got 0.
~~~

",test related unable import available lastly need modify launch via associated example run recent call last file line module import import name tensor parallel example recent call last file line module import import name sequence parallel example recent call last file line module import import name parallel example least run got,issue,negative,negative,neutral,neutral,negative,negative
1789504744,"Hello, Can you share your environment, command, etc? suitable to debug. Thanks",hello share environment command suitable thanks,issue,positive,positive,positive,positive,positive,positive
1783659171,"I can confirm this example is badly broken.  I added some code to compare individual labels to predictions and discovered the forward pass of ViT always returns the same tensor.  No matter the input. The tensor it returns is different each time I run it, even if I load the same weights from the save file and don't do any training. It's no wonder it can't do better than 2.3. Always giving the same prediction should accidentally hit on the correct label about as often as random guessing.",confirm example badly broken added code compare individual discovered forward pas always tensor matter input tensor different time run even load save file training wonder ca better always giving prediction accidentally hit correct label often random guessing,issue,negative,negative,neutral,neutral,negative,negative
1752150326,"Hi @soumith and @bmccann ,
Could you please review it or guide me with who can help me out ?",hi could please review guide help,issue,positive,neutral,neutral,neutral,neutral,neutral
1751778149,"Hi folks yeah please go for it, no need to ask me for permission just send a PR and tag me so I can review",hi yeah please go need ask permission send tag review,issue,positive,neutral,neutral,neutral,neutral,neutral
1751705152,"Hello, I am new to contributing to PyTorch, @msaroufim Can I try contributing a new example ""UNet Image Segmenation""? 

Thank You",hello new try new example image thank,issue,negative,positive,positive,positive,positive,positive
1742807274,"Hi, I get the same issue on windows11 trying to use CUDA. It works with CPU though.

@msaroufim That's probably not an issue with pickle but with shared memory in CUDA on Windows. The EOFError appears asynchronously after the cuda exception.

```
File ""D:\Python3.7.3\lib\site-packages\torch\multiprocessing\reductions.py"", line 232, in reduce_tensor
    event_sync_required) = storage._share_cuda_()
RuntimeError: cuda runtime error (71) : operation not supported at C:\w\1\s\windows\pytorch\torch/csrc/generic/StorageSharing.cpp:245
```

my output:
```
(vpyenv) PS D:\_projects\examples\mnist_hogwild> python main.py --cuda
Using device: cuda
Traceback (most recent call last):
  File ""D:\_projects\examples\mnist_hogwild\main.py"", line 99, in <module>
    p.start()
  File ""D:\Program Files\Python311\Lib\multiprocessing\process.py"", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File ""D:\Program Files\Python311\Lib\multiprocessing\context.py"", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\Program Files\Python311\Lib\multiprocessing\context.py"", line 336, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File ""D:\Program Files\Python311\Lib\multiprocessing\popen_spawn_win32.py"", line 94, in __init__
    reduction.dump(process_obj, to_child)
  File ""D:\Program Files\Python311\Lib\multiprocessing\reduction.py"", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
  File ""D:\_projects\examples\mnist_hogwild\vpyenv\Lib\site-packages\torch\multiprocessing\reductions.py"", line 261, in reduce_tensor
    event_sync_required) = storage._share_cuda_()
                           ^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\_projects\examples\mnist_hogwild\vpyenv\Lib\site-packages\torch\storage.py"", line 943, in _share_cuda_
    return self._untyped_storage._share_cuda_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: operation not supported
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

(vpyenv) PS D:\_projects\examples\mnist_hogwild> Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""D:\Program Files\Python311\Lib\multiprocessing\spawn.py"", line 120, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\Program Files\Python311\Lib\multiprocessing\spawn.py"", line 130, in _main
    self = reduction.pickle.load(from_parent)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: Ran out of input
```

The issue seems to be caused by the CUDA IPC operations support for windows (see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations).

> ""IPC is the way by which multiple processes or threads communicate with each other. IPC in OS obtains modularity, computational speedup, and data sharing.""

From Pytorch Doc:

>  ""They [IPC Operations] are not supported on Windows. Something like doing multiprocessing on CUDA tensors cannot succeed, there are two alternatives for this.
> 
> 1. Donâ€™t use multiprocessing. Set the num_worker of [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) to zero.
> 
> 2. Share CPU tensors instead. Make sure your custom DataSet returns CPU tensors.""

Basically saying it's not possible?
Please find a fix somehow for Windows, there must be a way. This would enable me to save so much VRAM by not having to duplicate models etc. I really need this.",hi get issue trying use work though probably issue pickle memory exception file line error operation output python device recent call last file line module file line start self file line return file line return file line file line dump file protocol file line file line return error operation kernel might call might incorrect consider passing compile enable recent call last file string line module file line file line self ran input issue support see way multiple communicate o computational data doc something like succeed two use set zero share instead make sure custom basically saying possible please find fix somehow must way would enable save much duplicate really need,issue,positive,positive,neutral,neutral,positive,positive
1740128839,"Hi @subramen and @msaroufim ,
I've made the changes and created a pull request. Could one of you please review it?",hi made pull request could one please review,issue,negative,neutral,neutral,neutral,neutral,neutral
1724120612,"Cool! I think that I can try PINNs and another modern solution using graph nets, so please assign me this problem.",cool think try another modern solution graph please assign problem,issue,negative,positive,positive,positive,positive,positive
1724106014,Yeah that's what I had in mind but open to other cool sounding models ,yeah mind open cool sounding,issue,positive,positive,positive,positive,positive,positive
1724036955,"@msaroufim When you say ""Differentiable physics"", do you mean Physic-informed deep learning?  Or do you have some other architecture in mind?",say differentiable physic mean deep learning architecture mind,issue,negative,negative,negative,negative,negative,negative
1712037979,"I also have a similar issue. When loading the model (with DistributedDataParallel) from checkpoint A and fine-tuning it in a parallel mode I get an 85% accuracy while doing the same on a non-parallel mode I get 67%. Both of these experiments are only on 1 gpu, the distributed one is with 10 workers. Do you @d4l3k maybe know why this is the case and which accuracy is actually correct?
",also similar issue loading model parallel mode get accuracy mode get distributed one maybe know case accuracy actually correct,issue,negative,neutral,neutral,neutral,neutral,neutral
1709479778,"@msaroufim I believe this issue has already been resolved.

I've examined the code, and the difference in the presence of batch_idx between the train function and the test function is because the train function prints training logs for each batch size. In contrast, the test function does not have any training log outputs, so batch_idx appears unnecessary there. Furthermore, the test function is currently running smoothly without any issues.

```
if batch_idx % args.log_interval == 0:  
    print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(  
        epoch, batch_idx * len(data), len(train_loader.dataset),  
        100. * batch_idx / len(train_loader), loss.item()))  
    if args.dry_run:  
        break
```
I don't think the above part is necessary for the test function, so it doesn't seem necessary to add batch_idx within the test function.

I would appreciate it if you could consider closing this issue.",believe issue already resolved code difference presence train function test function train function training batch size contrast test function training log unnecessary furthermore test function currently running smoothly without print epoch epoch data break think part necessary test function seem necessary add within test function would appreciate could consider issue,issue,positive,neutral,neutral,neutral,neutral,neutral
1708926907,If you wanna contribute some fixes would be happy to merge,wan na contribute would happy merge,issue,positive,positive,positive,positive,positive,positive
1691082641,"Hi, do you have a problem with the application getting stuck after starting multiple nodes? On my side, too, running the official multi-node example would get stuck",hi problem application getting stuck starting multiple side running official example would get stuck,issue,negative,neutral,neutral,neutral,neutral,neutral
1681217972,"Hmm I think this was a recent regression https://github.com/pytorch/examples/pull/1064

Could you try commenting out the mps specific bits from the code and see if that solves the problem?",think recent regression could try specific code see problem,issue,negative,neutral,neutral,neutral,neutral,neutral
1676457084,Wanna send out a PR? We also dont have any good tests for the C++ code so would love to merge something here,wan na send also dont good code would love merge something,issue,negative,positive,positive,positive,positive,positive
1670219912,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`.

If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",wo notify release get touch new version available rather skip next major minor version let know ignore major version ignore minor version change mind resolve,issue,negative,positive,neutral,neutral,positive,positive
1670218058,"Closing for now, would be useful to understand how the bug was detected and fixed with some logs",would useful understand bug fixed,issue,negative,positive,positive,positive,positive,positive
1670125809,Make a separate PR. I'm still not sure what's going on with docbuikd since I can't read the build failures ,make separate still sure going since ca read build,issue,negative,positive,positive,positive,positive,positive
1670087782,"Thanks @msaroufim 
I did not add the doc card for the example.
should I add it in another PR or it's not required?",thanks add doc card example add another,issue,negative,positive,positive,positive,positive,positive
1669916891,"I didn't get an error, but only two elements are being used from that tensor and the train() function, has a tensor of size 2, I thought it's just a simple typo here.",get error two used tensor train function tensor size thought simple typo,issue,negative,neutral,neutral,neutral,neutral,neutral
1668929335,@svekars could I please get access to the netlify logs? I can't see why this is failing,could please get access ca see failing,issue,negative,neutral,neutral,neutral,neutral,neutral
1657065249,"The fix seems to be merged in #1176, so I should close this issue. Note that the issue from the previous comment `num_heads` is still here.",fix close issue note issue previous comment still,issue,negative,negative,negative,negative,negative,negative
1653005753,"By the way, I found an another similar problem as below:
 
https://github.com/pytorch/examples/blob/6fc19c76b9f5550d8d3cb38284845ac7ed14223c/vision_transformer/main.py#L220C1-L221C47

The default `num_heads` should be 16 (same as the `help`) or the value in `help` should be 12 (same as the default value).",way found another similar problem default help value help default value,issue,positive,neutral,neutral,neutral,neutral,neutral
1650118209,"Sounds good, I will work on making a minimal prototype by end of Aug.",good work making minimal prototype end,issue,negative,positive,positive,positive,positive,positive
1648705757,"@sekyondaMeta thanks, removed the version specification from the requirements file",thanks removed version specification file,issue,negative,positive,positive,positive,positive,positive
1647851731,"> @sekyondaMeta @svekars any idea what's going on with the doc build here?

I wonder if this failure here is the culprit: 
https://github.com/pytorch/examples/actions/runs/5627940947/job/15251219562?pr=1174

One of the changes added numpy 1.25.1 and there is probably a requirement somewhere else for 1.24.2",idea going doc build wonder failure culprit one added probably requirement somewhere else,issue,negative,negative,negative,negative,negative,negative
1646757201,"The original controlnet model is big and I am not able to test it in my machine. I also need to add a couple of more components to the model. I have a question: Should we make a small minimal implementation, like [min-GPT](https://github.com/pytorch/examples/tree/6fc19c76b9f5550d8d3cb38284845ac7ed14223c/distributed/minGPT-ddp) or do an implementation of full model? I think the former approach will be good for an example and latter one is already available at [ControlNet](https://github.com/lllyasviel/ControlNet).",original model big able test machine also need add couple model question make small minimal implementation like implementation full model think former approach good example latter one already available,issue,positive,positive,positive,positive,positive,positive
1646368161,@sekyondaMeta @svekars any idea what's going on with the doc build here?,idea going doc build,issue,negative,neutral,neutral,neutral,neutral,neutral
1646367148,Hi @arunppsg are you still interested in merging this? I think you're close,hi still interested think close,issue,negative,positive,positive,positive,positive,positive
1646366164,Flake since this happened 3 weeks ago and never again,flake since ago never,issue,negative,neutral,neutral,neutral,neutral,neutral
1641103500,"First, it seemed the issue was with the new example card in the doc.
After checking a few times, the build with the default commits (without my changes) once passed [ac8d3ef](https://github.com/pytorch/examples/pull/1174/commits/ac8d3efeaf3b4ef7116a8c13f0fbe95d62f4f7bb) and once failed 
[d5fe1d2](https://github.com/pytorch/examples/pull/1174/commits/d5fe1d2cc3d5851981eb278e6f82895a0547bd98) the checks.

Couldn't find the issue with the build since it builds locally with no issues.",first issue new example card doc time build default without could find issue build since locally,issue,negative,positive,positive,positive,positive,positive
1636776087,"Hi, I see the GNN example is taken care of as a GCN example. Would other GNN variants such as GAT, GraphSage, etc. be helpful, or would they be counted as kinda duplicates?",hi see example taken care example would gat helpful would,issue,positive,neutral,neutral,neutral,neutral,neutral
1633180847,æ‚¨å¥½ï¼Œæˆ‘å·²æ”¶åˆ°ï¼Thank you for your answer sheet. I have received your answer sheet.,answer sheet received answer sheet,issue,negative,neutral,neutral,neutral,neutral,neutral
1633180425,"> I was able to get dcgan operating successfully at 128x128 by adding the convolutional layers described above and then running with ngf 128 and ndf 32. When I attempted to go to 512 I was not able to get a stable result. I'm attempting to add the white noise to the discriminator to see if that helps.
> 
> ** I ended up abandoning dcgan and am now over using bmsggan which is a variation on progressive gans. Its handling higher resolutions much better **

Hi, Michael Powers, have you tried 256x256 with the generated image and does it works well?",able get operating successfully convolutional running go able get stable result add white noise discriminator see ended variation progressive handling higher much better hi tried image work well,issue,positive,positive,positive,positive,positive,positive
1615375486,"What is the expected outcome in this context? The model and its pretrained weights can be obtained from OpenAI's Whisper repository. One possible approach is to utilize the model class and showcase the process of training or fine-tuning, as the original repository does not include training functionality.",outcome context model whisper repository one possible approach utilize model class showcase process training original repository include training functionality,issue,negative,positive,positive,positive,positive,positive
1607611567,Sure yeah! That sound cool ,sure yeah sound cool,issue,positive,positive,positive,positive,positive,positive
1606563472,@msaroufim I would like to contribute to the OpenAI whisper implementation. Can I take this up?,would like contribute whisper implementation take,issue,negative,neutral,neutral,neutral,neutral,neutral
1587685317,"We have some other doc issue @svekars is looking into, so will merge your PR for now. Some low probability we need to forward fix the rst pages but let's see",doc issue looking merge low probability need forward fix let see,issue,negative,neutral,neutral,neutral,neutral,neutral
1586798755,@msaroufim I was trying to create my own mask rule. Found out where I went wrong and fixed it.,trying create mask rule found went wrong fixed,issue,negative,negative,negative,negative,negative,negative
1586374359,"> ```python
> def generate_square_subsequent_mask(seq_len):
>   mask = (torch.triu(torch.ones(seq_len, seq_len)) == 1).transpose(0, 1)
>   mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask== 1, float(0.0))
>   return mask
> ```
> 
> I'm unable to understand this way of generating masks used in the source code.

What exactly is confusing you?

Running it prints a reasonable looking mask to me

```pyhon
generate_square_subsequent_mask(10)
tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
```",python mask mask mask float float return mask unable understand way generating used source code exactly running reasonable looking mask tensor,issue,negative,negative,neutral,neutral,negative,negative
1586373989,"> I'm seeing a lot of nan values when I print the attn_output_weights in nn.MultiheadAttention in the decoder block. Is it expected or is it due to a fault in the logic?

Could you please share a repro? That's certainly not expected",seeing lot nan print block due fault logic could please share certainly,issue,negative,positive,neutral,neutral,positive,positive
1586093226,"```python
def generate_square_subsequent_mask(seq_len):
  mask = (torch.triu(torch.ones(seq_len, seq_len)) == 1).transpose(0, 1)
  mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask== 1, float(0.0))
  return mask
  ```
  I'm unable to understand this way of generating masks used in the source code.",python mask mask mask float float return mask unable understand way generating used source code,issue,negative,negative,negative,negative,negative,negative
1586044772,I'm seeing a lot of nan values when I print the attn_output_weights in nn.MultiheadAttention in the decoder block. Is it expected or is it due to a fault in the logic?,seeing lot nan print block due fault logic,issue,negative,negative,negative,negative,negative,negative
1583072599,Hi @mechatronicsengineer95 this might be a better fit for the tutorials repo - we have a hackathon ongoing so this is good timing,hi might better fit ongoing good timing,issue,positive,positive,positive,positive,positive,positive
1581815485,"Looks like this is not fixed yet. Right?
I can try fixing it and make a PR if no one else is working on it

Edit: I realized I mishandled the dataset which led to this error. After deleting the extracted folders, it was fixed. So, I guess there is no need to fix anything.",like fixed yet right try fixing make one else working edit led error extracted fixed guess need fix anything,issue,negative,positive,positive,positive,positive,positive
1579263705,"Thanks @HemanthSai7 

@yishengpei vision transformer was completed already but would be happy to review swin transformer",thanks vision transformer already would happy review transformer,issue,positive,positive,positive,positive,positive,positive
1579129761,"Hey! I am wondering whether the Vision Transformer model is taken or not. I am willing to contribute. Or otherwise, would you be interested if I work on the Swin Transformer model? Many thanks.",hey wondering whether vision transformer model taken willing contribute otherwise would interested work transformer model many thanks,issue,positive,positive,positive,positive,positive,positive
1578075436,"Wanted to give updates on my task. I have completed preparing the dataset(tokenization, data-loading, etc) for the translation task and will start with Positional Embeddings and other layers.",give task translation task start positional,issue,negative,neutral,neutral,neutral,neutral,neutral
1576769588,"@msaroufim Sorry, but owing to university examinations I will not be able to participate this time. However, if anybody wants to take FlowNet, they are welcome to do so. :)",sorry owing university able participate time however anybody take welcome,issue,negative,positive,positive,positive,positive,positive
1574050800,"Please reference this issue in your PRs, like ""Re #1131"".",please reference issue like,issue,positive,neutral,neutral,neutral,neutral,neutral
1573851357,"@HemanthSai7 yes but the model should be pure PyTorch, bonus point for a from scratch tokenizer!
@bhavyashahh Sure!
@QasimKhan5x Sounds good, either of those models work",yes model pure bonus point scratch sure good either work,issue,positive,positive,positive,positive,positive,positive
1573629542,"@msaroufim I'd like to implement a text to 3d model. At the moment, I'm deciding between Test2Mesh and CLIP-Forge. DreamFusion seems a little complex. ",like implement text model moment little complex,issue,negative,negative,negative,negative,negative,negative
1573058142,"~~Can I take up implementing Controlnet - guided diffusion?~~

Apologies, I could not complete it. If someone else is interested, feel free to take it up - I am no longer working on it.",take diffusion could complete someone else interested feel free take longer working,issue,positive,positive,positive,positive,positive,positive
1572742581,"@guptaaryan16 @HemanthSai7 @JoseLuisC99 @abhi-glitchhg assigned some models to you, lemme know if you need any help to get it over the finish line. Thanks!",assigned know need help get finish line thanks,issue,positive,positive,positive,positive,positive,positive
1572636441,Hey @msaroufim I would like to work on stable diffusion and some others topics as well. Thanks,hey would like work stable diffusion well thanks,issue,positive,positive,positive,positive,positive,positive
1572315595,@JoseLuisC99 any task or model you like! As long as it's from scratch in pure Pytorch ,task model like long scratch pure,issue,negative,positive,neutral,neutral,positive,positive
1571554354,Hi @msaroufim I would like to implement language translation using encoder-decoder architecture. Can I take this?,hi would like implement language translation architecture take,issue,negative,neutral,neutral,neutral,neutral,neutral
1571398142,"I would like to contribute to Graph Neural Network. However, is there some specific task or model in mind or can I choose any?",would like contribute graph neural network however specific task model mind choose,issue,negative,neutral,neutral,neutral,neutral,neutral
1571393503,@abhi-glitchhg please do just keep in mind the implementation has to be from scratch and not just call the torcvision constructor ,please keep mind implementation scratch call constructor,issue,negative,neutral,neutral,neutral,neutral,neutral
1571280397,"I would like to add a video vision transformer model. 


Edit: Video ViTs are already present in Torchvision, can i still go ahead with this idea? thanks",would like add video vision transformer model edit video already present still go ahead idea thanks,issue,positive,positive,neutral,neutral,positive,positive
1570926717,"I would love to contribute to FlowNet. Can I take this up?

",would love contribute take,issue,positive,positive,positive,positive,positive,positive
1561491282,"@svekars any idea if the doc build is flaking for any reason?

@HamidShojanazeri do you mind rebasing on main to see if the error goes away",idea doc build reason mind main see error go away,issue,negative,positive,positive,positive,positive,positive
1560552742,"> General comment - this example does not use activation checkpointing due to the timing of this PR (it wasn't added in FSDP until after this PR). But I think it would be good to update this example with it, to make sure it's present as activation checkpointing is one of our biggest throughput boosters.

Added the AC and rate_lmiter as well+ model checkpointings.",general comment example use activation due timing added think would good update example make sure present activation one biggest throughput added model,issue,positive,positive,positive,positive,positive,positive
1560550401,"@msaroufim , @hudeven sorry for the delay I addressed the comments and made the code more modular, would be great if we could merge this.",sorry delay made code modular would great could merge,issue,negative,positive,positive,positive,positive,positive
1552844243,"@panchul hello, recently i want to run rpc on two physical machines, how to run the py file. can u share command. The two machines are cpu and gpu. Thanks.",hello recently want run two physical run file share command two thanks,issue,positive,positive,neutral,neutral,positive,positive
1548184181,Hi @Krish2002 yes please go for it,hi yes please go,issue,positive,neutral,neutral,neutral,neutral,neutral
1547471949,hey!  I would love to contribute to Stable diffusion. Can i take this up ?,hey would love contribute stable diffusion take,issue,positive,positive,positive,positive,positive,positive
1546763905,Ah I believe this should be fixed if you just `pip install tqdm`,ah believe fixed pip install,issue,negative,positive,neutral,neutral,positive,positive
1528657462," python /path/to/launch.py --nnode=1 --node_rank=0 --nproc_per_node=1 example.py --local_world_size=1

There are 4 gpus (same) on machine

the error is 

```
 warnings.warn(
[7320] Initializing process group with: {'MASTER_ADDR': '127.0.0.1', 'MASTER_PORT': '29500', 'RANK': '0', 'WORLD_SIZE': '1'}
[7320]: world_size = 1, rank = 0, backend=nccl 
[7320] rank = 0, world_size = 1, n = 4, device_ids = [0, 1, 2, 3] 
Traceback (most recent call last):
  File ""example.py"", line 97, in <module>
    spmd_main(args.local_world_size, args.local_rank)
  File ""example.py"", line 83, in spmd_main
    demo_basic(local_world_size, local_rank)
  File ""example.py"", line 39, in demo_basic
    ddp_model = DDP(model, device_ids)
  File ""/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py"", line 484, in __init__
    self._log_and_throw(
  File ""/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py"", line 604, in _log_and_throw
    raise err_type(err_msg)
ValueError: device_ids can only be None or contain a single element.
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 7320) of binary: /opt/conda/bin/python
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py"", line 193, in <module>
    main()
  File ""/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py"", line 189, in main
    launch(args)
  File ""/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py"", line 174, in launch
    run(args)
  File ""/opt/conda/lib/python3.8/site-packages/torch/distributed/run.py"", line 710, in run
    elastic_launch(
  File ""/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py"", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py"", line 259, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
example.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-04-29_05:08:15
  host      : ml2
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 7320)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
```",python machine error process group rank rank recent call last file line module file line file line model file line file line raise none contain single element error binary recent call last file line module main file line main launch file line launch run file line run file line return list file line raise root cause first failure time host rank enable see,issue,negative,negative,negative,negative,negative,negative
1523576875,@msaroufim it seems CI timed out. should i reduce the dataset size and the number of epochs?,timed reduce size number,issue,negative,neutral,neutral,neutral,neutral,neutral
1522711464,@niyarrbarman can you please retrigger CI by pushing any commit? Something weird happened to your last commit where the CI thinks you're installing a pip binary called `torchtorchvision`,please pushing commit something weird last commit pip binary,issue,positive,negative,negative,negative,negative,negative
1521295437,"> Thank you! Your code was quite clean and enjoyable to read, I learnt something new by reading your patch extractor code. Left a few minor nits as feedback. Please address and we can merge this in

I have added all the suggested changes.",thank code quite clean enjoyable read learnt something new reading patch extractor code left minor feedback please address merge added,issue,positive,positive,positive,positive,positive,positive
1521141656,"@ravijo your PyTorch version is quite old, might be best to upgrade",version quite old might best upgrade,issue,positive,positive,positive,positive,positive,positive
1521141124,"Ah as expected tqdm broke CI, might just be best to remove that dependency",ah broke might best remove dependency,issue,negative,positive,positive,positive,positive,positive
1514078596,"Hi @sty945! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20pytorch%2Fexamples%20%231139). Thanks!",hi sty thank pull request welcome community action order merge pull request code require sign contributor license agreement seem one file process order u review merge please sign behalf someone else employer individual may sufficient employer may need sign corporate tooling perform afterwards pull request tagged process may take hour please give time u received error please contact u thanks,issue,positive,positive,positive,positive,positive,positive
1511368538,"I also encountered the same problem. I annotated the 'mps' code and then set to use multiple Gpus. ""model = torch.nn.DataParallel allel(model). But I don't know what's the difference between this and single-node multi-GPU training mode


```
    if not torch.cuda.is_available() and not torch.backends.mps.is_available():
        print('using CPU, this will be slow')
    elif args.distributed:
        # For multiprocessing distributed, DistributedDataParallel constructor
        # should always set the single device scope, otherwise,
        # DistributedDataParallel will use all available devices.
        if torch.cuda.is_available():
            if args.gpu is not None:
                torch.cuda.set_device(args.gpu)
                model.cuda(args.gpu)
                # When using a single GPU per process and per
                # DistributedDataParallel, we need to divide the batch size
                # ourselves based on the total number of GPUs of the current node.
                args.batch_size = int(args.batch_size / ngpus_per_node)
                args.workers = int((args.workers + ngpus_per_node - 1) / ngpus_per_node)
                model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])
            else:
                model.cuda()
                # DistributedDataParallel will divide and allocate batch_size to all
                # available GPUs if device_ids are not set
                model = torch.nn.parallel.DistributedDataParallel(model)
    elif args.gpu is not None and torch.cuda.is_available():
        torch.cuda.set_device(args.gpu)
        model = model.cuda(args.gpu)
    # elif torch.backends.mps.is_available():
    #     device = torch.device(""mps"")
    #     model = model.to(device)
    else:
        # DataParallel will divide and allocate batch_size to all available GPUs
        if args.arch.startswith('alexnet') or args.arch.startswith('vgg'):
            model.features = torch.nn.DataParallel(model.features)
            model.cuda()
        else:
            model = torch.nn.DataParallel(model).cuda()


    if torch.cuda.is_available():
        if args.gpu:
            device = torch.device('cuda:{}'.format(args.gpu))
        else:
            device = torch.device(""cuda"")
    # elif torch.backends.mps.is_available():
    #     device = torch.device(""mps"")
    else:
        device = torch.device(""cpu"")
    # define loss function (criterion), optimizer, and learning rate scheduler

```
",also problem code set use multiple model model know difference training mode print slow distributed constructor always set single device scope otherwise use available none single per process per need divide batch size based total number current node model model else divide allocate available set model model none model device model device else divide allocate available else model model device else device device else device define loss function criterion learning rate,issue,negative,positive,neutral,neutral,positive,positive
1510404508,"@msaroufim I am interested for PyTorch Open-Source Contribution. Thank you for sharing New examples requested Note, would like to contribute to Diffusion Model, stable diffusion and Vision Transformer sections will keep you posted as my work progress.
Please let me know your thoughts on taking up this project. Thank you.
Regards,
Aditi ",interested contribution thank new note would like contribute diffusion model stable diffusion vision transformer keep posted work progress please let know taking project thank,issue,positive,positive,positive,positive,positive,positive
1509409083,Yeah tutorials might be a more natural fit,yeah might natural fit,issue,positive,positive,positive,positive,positive,positive
1509361558,"@msaroufim I am passing different modes/ optimizations as optional args, build a story step-by-step. So, without the notebook, I am not sure if `main.py` would be useful as is. What do you suggest? Should I work on moving the entire thing to tutorials?",passing different optional build story without notebook sure would useful suggest work moving entire thing,issue,negative,positive,positive,positive,positive,positive
1509335768,"FYI we don't typically have notebooks in `examples` might be a better fit for tutorials, so if you'd like to remove that I can review the rest",typically might better fit like remove review rest,issue,positive,positive,positive,positive,positive,positive
1509087353,"@msaroufim I would Like to contribute to examples related to graph neural networks ,Is there any specific Dataset i should choose for this or i can choose any dataset of my own choice for examples .  ",would like contribute related graph neural specific choose choose choice,issue,negative,neutral,neutral,neutral,neutral,neutral
1504274652,"I am interested as well. What is a video model? I am looking at some video examples from [tensorflow](https://www.tensorflow.org/tutorials/video/video_classification) and [keras](https://keras.io/examples/vision/video_classification/). Would a spinoff of this suffice? That dataset looks like a standard intro video dataset.

Which of these problems can be comfortably exercised on a 2080ti(12gb vram)?",interested well video model looking video would suffice like standard video comfortably ti,issue,positive,positive,positive,positive,positive,positive
1503883621,"Hey! I'm interested in contributing to the vision transformer model, but I don't have any prior open source contribution experience. Would it be okay for me to proceed with this project?",hey interested vision transformer model prior open source contribution experience would proceed project,issue,positive,positive,neutral,neutral,positive,positive
1501322289,Can you please share how you validated your fix worked?,please share fix worked,issue,positive,neutral,neutral,neutral,neutral,neutral
1491525486,I meet the same issue with you when using DDP. Is there any solution for this issue?,meet issue solution issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1485438429,@soumith just pinging again to see if this is in a good state for approval ğŸ™‚ ,see good state approval,issue,positive,positive,positive,positive,positive,positive
1483660935,I realized that I should go to the pytorch forums to find the answer to this question. So I'll close this issue.,go find answer question close issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1480547725,"And why is the suffix of the model I trained named model and the suffix of the model I provided named pth? As shown below, mosaic.pth was provided by the author, and painting.model was trained by me. I want to know why the sizes and suffixes of these two files are different because my training model does not work well
![image](https://user-images.githubusercontent.com/125473669/227093622-eeda3c88-dd07-4d96-b98d-7e1c6006d5ec.png)

",suffix model trained model suffix model provided shown provided author trained want know size two different training model work well image,issue,negative,neutral,neutral,neutral,neutral,neutral
1479006628,"> Hi, I'm facing the same issue with `rpc_parameter_server.py` example. Has this issue been resolved? I set `--world_size=3` for 3 nodes of isolated docker containers and it shows the following error messages.
> 
> ```
> Traceback (most recent call last):
>   File ""/opt/conda/lib/python3.10/multiprocessing/process.py"", line 314, in _bootstrap
>     self.run()
>   File ""/opt/conda/lib/python3.10/multiprocessing/process.py"", line 108, in run
>     self._target(*self._args, **self._kwargs)
>   File ""/home/fed/rpc_parameter_server.py"", line 231, in run_worker
>     run_training_loop(rank, num_gpus, train_loader, test_loader)
>   File ""/home/fed/rpc_parameter_server.py"", line 190, in run_training_loop
>     dist_autograd.backward(cid, [loss])
> RuntimeError: Error on Node 0: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [32, 1, 3, 3]] is at version 6; expected version 5 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
> Exception raised from unpack at /opt/conda/conda-bld/pytorch_1670525552843/work/torch/csrc/autograd/saved_variable.cpp:184 (most recent call first):
> frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f96e916c457 in /opt/conda/lib/python3.10/site-packages/torch/lib/libc10.so)
> frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f96e91363ec in /opt/conda/lib/python3.10/site-packages/torch/lib/libc10.so)
> frame #2: torch::autograd::SavedVariable::unpack(std::shared_ptr<torch::autograd::Node>) const + 0x7fc (0x7f971f969f2c in /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
> frame #3: torch::autograd::generated::ConvolutionBackward0::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0xfd (0x7f971ee154fd in /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
> frame #4: <unknown function> + 0x43a2f8b (0x7f971f93df8b in /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
> frame #5: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x1638 (0x7f971f937878 in /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
> frame #6: torch::distributed::autograd::DistEngine::execute_graph_task_until_ready_queue_empty(torch::autograd::NodeTask&&, bool) + 0x434 (0x7f9720374a64 in /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
> frame #7: <unknown function> + 0x4dda536 (0x7f9720375536 in /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
> frame #8: <unknown function> + 0x14a8e51 (0x7f971ca43e51 in /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
> frame #9: c10::ThreadPool::main_loop(unsigned long) + 0x285 (0x7f96e915dd85 in /opt/conda/lib/python3.10/site-packages/torch/lib/libc10.so)
> frame #10: <unknown function> + 0xdbbf4 (0x7f9731a4dbf4 in /opt/conda/lib/python3.10/site-packages/torch/lib/../../../../libstdc++.so.6)
> frame #11: <unknown function> + 0x76db (0x7f97678cb6db in /lib/x86_64-linux-gnu/libpthread.so.0)
> frame #12: clone + 0x3f (0x7f9766e4f61f in /lib/x86_64-linux-gnu/libc.so.6
> ```
This problem cannot be solved, and the only choice is to manually place the model layer on different graphics cards to achieve mixed parallelism.
",hi facing issue example issue resolved set isolated docker following error recent call last file line file line run file line rank file line loss error node one gradient computation operation version version instead hint enable anomaly detection find operation compute gradient true exception raised unpack recent call first frame frame char char unsigned frame torch torch frame torch frame unknown function frame torch torch torch torch torch frame torch torch bool frame unknown function frame unknown function frame unsigned long frame unknown function frame unknown function frame clone problem choice manually place model layer different graphic achieve mixed parallelism,issue,negative,negative,neutral,neutral,negative,negative
1478704101,"@steenblikrs I'd like to help you but I can't test it now as I don't want to downlaod 42 Gigs of Data. However this error looks very familiar and in my experience is just a multiprocessing + dataloader issue.

The way you'd fix this is just put everything into a `def main():` and then a 
```
if __name__ == ""__main__"": 
     main()
``` 

So now when multiple processes are created (forked) it won't execute all the code again and you won't run into this issue. 

https://stackoverflow.com/questions/64654838/pytorch-tutorial-freeze-support-issue

There are many threads on the internet regarding this issue , just try to look for them and you'll find them. But my guess is when you put everything into the __main__ syntax it will be fine.

",like help ca test want data however error familiar experience issue way fix put everything main main multiple forked wo execute code wo run issue many regarding issue try look find guess put everything syntax fine,issue,positive,positive,positive,positive,positive,positive
1464284037,"Hi, I'm facing the same issue with `rpc_parameter_server.py` example. Has this issue been resolved? 
I set `--world_size=3` for 3 nodes of isolated docker containers and it shows the following error messages.

```
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/multiprocessing/process.py"", line 314, in _bootstrap
    self.run()
  File ""/opt/conda/lib/python3.10/multiprocessing/process.py"", line 108, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/fed/rpc_parameter_server.py"", line 231, in run_worker
    run_training_loop(rank, num_gpus, train_loader, test_loader)
  File ""/home/fed/rpc_parameter_server.py"", line 190, in run_training_loop
    dist_autograd.backward(cid, [loss])
RuntimeError: Error on Node 0: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [32, 1, 3, 3]] is at version 6; expected version 5 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
Exception raised from unpack at /opt/conda/conda-bld/pytorch_1670525552843/work/torch/csrc/autograd/saved_variable.cpp:184 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f96e916c457 in /opt/conda/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f96e91363ec in /opt/conda/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: torch::autograd::SavedVariable::unpack(std::shared_ptr<torch::autograd::Node>) const + 0x7fc (0x7f971f969f2c in /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #3: torch::autograd::generated::ConvolutionBackward0::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0xfd (0x7f971ee154fd in /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #4: <unknown function> + 0x43a2f8b (0x7f971f93df8b in /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #5: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x1638 (0x7f971f937878 in /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #6: torch::distributed::autograd::DistEngine::execute_graph_task_until_ready_queue_empty(torch::autograd::NodeTask&&, bool) + 0x434 (0x7f9720374a64 in /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #7: <unknown function> + 0x4dda536 (0x7f9720375536 in /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #8: <unknown function> + 0x14a8e51 (0x7f971ca43e51 in /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #9: c10::ThreadPool::main_loop(unsigned long) + 0x285 (0x7f96e915dd85 in /opt/conda/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0xdbbf4 (0x7f9731a4dbf4 in /opt/conda/lib/python3.10/site-packages/torch/lib/../../../../libstdc++.so.6)
frame #11: <unknown function> + 0x76db (0x7f97678cb6db in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #12: clone + 0x3f (0x7f9766e4f61f in /lib/x86_64-linux-gnu/libc.so.6
```",hi facing issue example issue resolved set isolated docker following error recent call last file line file line run file line rank file line loss error node one gradient computation operation version version instead hint enable anomaly detection find operation compute gradient true exception raised unpack recent call first frame frame char char unsigned frame torch torch frame torch frame unknown function frame torch torch torch torch torch frame torch torch bool frame unknown function frame unknown function frame unsigned long frame unknown function frame unknown function frame clone,issue,negative,negative,neutral,neutral,negative,negative
1458071203,"I had the same error. Replacing
`push_back(Functional(torch::log_softmax, 1));`
by
`push_back(torch::nn::LogSoftmax(1));`
might help you.",error functional torch torch might help,issue,negative,neutral,neutral,neutral,neutral,neutral
1451099156,IIRC the distributed examples are likely to break if we upgrade - cc @rohan-varma if you or anyone on the team is interested in fixing this,distributed likely break upgrade anyone team interested fixing,issue,negative,positive,positive,positive,positive,positive
1440497037,"### <span aria-hidden=""true"">âœ…</span> Deploy Preview for *pytorch-examples-preview* ready!


|  Name | Link |
|:-:|------------------------|
|<span aria-hidden=""true"">ğŸ”¨</span> Latest commit | 0163ad9abf36017e6e91ac8d68f24b9467e94bbd |
|<span aria-hidden=""true"">ğŸ”</span> Latest deploy log | https://app.netlify.com/sites/pytorch-examples-preview/deploys/63f654bdeba45f00083cec27 |
|<span aria-hidden=""true"">ğŸ˜</span> Deploy Preview | [https://deploy-preview-1117--pytorch-examples-preview.netlify.app](https://deploy-preview-1117--pytorch-examples-preview.netlify.app) |
|<span aria-hidden=""true"">ğŸ“±</span> Preview on mobile | <details><summary> Toggle QR Code... </summary><br /><br />![QR Code](https://app.netlify.com/qr-code/eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1cmwiOiJodHRwczovL2RlcGxveS1wcmV2aWV3LTExMTctLXB5dG9yY2gtZXhhbXBsZXMtcHJldmlldy5uZXRsaWZ5LmFwcCJ9.ZDJkg_sswEPnA9YBsEXQNB7k5vTZJfqqitoVLdkkr_U)<br /><br />_Use your smartphone camera to open QR code link._</details> |
---

_To edit notification comments on pull requests, go to your [Netlify site settings](https://app.netlify.com/sites/pytorch-examples-preview/settings/deploys#deploy-notifications)._",span true deploy preview ready name link span true hammer latest commit span true latest deploy log span true deploy preview span true preview mobile summary toggle code code camera open code edit notification pull go site,issue,positive,positive,positive,positive,positive,positive
1440294969,"@mohammadpz 
Can you confirm that the email on your website is correct? Because I sent you an email about this a while ago, but anyway thank you very much!â¤ï¸",confirm correct sent ago anyway thank much,issue,negative,positive,positive,positive,positive,positive
1440167733,"Hi there!
Thank you for including me in this conversation, @msaroufim! I actually haven't received an email from @viveks-codes, or I may have missed it somehow.
Nonetheless, my code is licensed under MIT, so I have no issue with it being integrated into PyTorch as long as the license is updated to acknowledge the source of the original code.
Thanks!",hi thank conversation actually received may somehow nonetheless code licensed issue long license acknowledge source original code thanks,issue,positive,positive,positive,positive,positive,positive
1439883840,"I appreciate your comments on my pull request. I recognise your concern for giving the original author the due credit and adhering to the MIT licence. I've sent an email to @mohammadpz, I've been in touch with the creator of the original code and asked for guidance on how to appropriately acknowledge their contribution. 

I will amend my pull request to include the correct attribution and licencing information as soon as I obtain the information I need. If you would like, I can also add documentation and an argparse, as you requested, to the code, among other changes.

I appreciate you giving this issue your time and attention. I'm eager to collaborate with you to merge this code into the PyTorch repository.

best,
Vivek ",appreciate pull request concern giving original author due credit sent touch creator original code guidance appropriately acknowledge contribution amend pull request include correct attribution information soon obtain information need would like also add documentation code among appreciate giving issue time attention eager collaborate merge code repository best,issue,positive,positive,positive,positive,positive,positive
1439253135,"Yeah seems a bit convoluted now, this should work with both models the corresponding `model.py` the RNN and transformer one. Since you found this, would you like to fix it? I can help merge your code",yeah bit convoluted work corresponding transformer one since found would like fix help merge code,issue,positive,neutral,neutral,neutral,neutral,neutral
1437837797,AFAIK Colab only provides a single GPU - so fundamentally for any sort of distributed training either you need to provision a multi GPU machine from your favorite cloud provider or build one yourself. Unfortunately there's no low barrier to entry,single fundamentally sort distributed training either need provision machine favorite cloud provider build one unfortunately low barrier entry,issue,negative,positive,positive,positive,positive,positive
1437832686,Curious what kind of end to end speedup you ended up seeing with this? I don't have any benchmarks running in CI,curious kind end end ended seeing running,issue,positive,positive,positive,positive,positive,positive
1437832208,Sorry for the super late response but for ssupport requests you might get a faster answer on https://discuss.pytorch.org/,sorry super late response might get faster answer,issue,positive,negative,negative,negative,negative,negative
1437830990,"Yeah it seems like here we use the TransformerEncoder https://github.com/pytorch/examples/blob/main/word_language_model/model.py#L113 and not the Transformer layer

If you'd like to simplify this example please go for it",yeah like use transformer layer like simplify example please go,issue,positive,neutral,neutral,neutral,neutral,neutral
1435092665,"Good to see you, @suraj813! 

I will start a new issue with the latest updated codebase and I might start new issues for more examples to add :D",good see start new issue latest might start new add,issue,negative,positive,positive,positive,positive,positive
1435008302,"I'll take a look, I haven't read the forward forward paper yet to give great feedback but planning on it very soon",take look read forward forward paper yet give great feedback soon,issue,positive,positive,positive,positive,positive,positive
1434956351,"@soumith @msaroufim, could you kindly take a look at my pull request (https://github.com/pytorch/examples/pull/1114) and provide feedback? Thank you so much for your time and efforts! â¤ï¸",could kindly take look pull request provide feedback thank much time,issue,positive,positive,positive,positive,positive,positive
1433321169,"whoops missed this over the holidays - thanks for catching this @BalajiAI! Fixing this on the tutorials repo.

@sudomaze Hey Mazen ğŸ‘‹ğŸ¾, thanks for updating the tutorials! Your request might be better served as its own issue, do you mind opening a new one? thanks again!

Closing this thread as the OP's issue is resolved",whoop thanks catching fixing hey thanks request might better issue mind opening new one thanks thread issue resolved,issue,positive,positive,positive,positive,positive,positive
1431856620,"@hudeven I am considering to work updating DDP Tutorial codebase, can you validate the implementation that I have here: https://github.com/sudomaze/ttorch/blob/main/examples/ddp/run.py

This was tested by multiple people. I am planning to make it more readable and add it to `pytorch/examples`. What are your thoughts?",considering work tutorial validate implementation tested multiple people make readable add,issue,negative,neutral,neutral,neutral,neutral,neutral
1430370756,This still doesn't seem to be helping in my case :-(,still seem helping case,issue,negative,neutral,neutral,neutral,neutral,neutral
1427310533,"How about a colab instance? I would like to run Style Transfer there, is there one already available?

Thank you very much in advance, as this would make it much easier to use.",instance would like run style transfer one already available thank much advance would make much easier use,issue,positive,positive,positive,positive,positive,positive
1426802370,"hi vivek, this sounds like a good example to have. would love to see your contribution.",hi like good example would love see contribution,issue,positive,positive,positive,positive,positive,positive
1413028698,"FWIW distributed support for PyTorch on Windows isn't great, would suggest dual booting linux or getting a cloud instane",distributed support great would suggest dual booting getting cloud,issue,positive,positive,positive,positive,positive,positive
1387647456,"Appreciate the response @ewtang, but I was trying the MNIST Hogwild, which I think is different the the PyTorch MNIST example, and thus has different arguments... I changed the title of the issue to be more specific.",appreciate response trying think different example thus different title issue specific,issue,negative,neutral,neutral,neutral,neutral,neutral
1379764658,"If you check most vision CNN papers you will find they train with the same hyperparameters: SGD optimizer, 90 epochs, initial learning rate 0.1 that decreases by a tenth every 30 epochs.
",check vision find train initial learning rate tenth every,issue,negative,neutral,neutral,neutral,neutral,neutral
1379727367,"> > Hello, not sure if I should open a new issue for this, but are the pretrained models trained with default hyperparameters? And do all the pretrained models match the accuracies from the original papers? It seems unlikely that the default setting can achieve the best result for every model.
> 
> In the past when I trained the models from scratch, I recall being able to reproduce the accuracy for almost all models.
> 
> MobileNet might have its own hyperparameters, but the remaining models should be the same .

Thanks for the response! It's a good thing that one setting can work well for different models.",hello sure open new issue trained default match original unlikely default setting achieve best result every model past trained scratch recall able reproduce accuracy almost might thanks response good thing one setting work well different,issue,positive,positive,positive,positive,positive,positive
1379079097,"> Hello, not sure if I should open a new issue for this, but are the pretrained models trained with default hyperparameters? And do all the pretrained models match the accuracies from the original papers? It seems unlikely that the default setting can achieve the best result for every model.

In the past when I trained the models from scratch, I recall being able to reproduce the accuracy for almost all models.

MobileNet might have its own hyperparameters, but the remaining models should be the same .",hello sure open new issue trained default match original unlikely default setting achieve best result every model past trained scratch recall able reproduce accuracy almost might,issue,positive,positive,positive,positive,positive,positive
1378281740,"Hello, not sure if I should open a new issue for this, but are the pretrained models trained with default hyperparameters? And do all the pretrained models match the accuracies from the original papers? It seems unlikely that the default setting can achieve the best result for every model.",hello sure open new issue trained default match original unlikely default setting achieve best result every model,issue,positive,positive,positive,positive,positive,positive
1373827940,Hi. Thanks for implementing this profiler. Do you know where it got moved to?,hi thanks profiler know got,issue,negative,positive,positive,positive,positive,positive
1372811811,Closing as it's inactive for a while. Please feel free to reopen if there is some further progress on this PR,inactive please feel free reopen progress,issue,positive,positive,positive,positive,positive,positive
1371892957,"> I tried to make my code work, thanks a lot!!

hello! I have encountered the same problem. Is it necessary to use the sampler to scramble the data in the iter-only iteration? Will the training be misled into unhealthy states? May I ask how you solved it?
What's more, if batch_size=1, is it necessary to scramble the data? ",tried make code work thanks lot hello problem necessary use sampler scramble data iteration training misled unhealthy may ask necessary scramble data,issue,negative,negative,neutral,neutral,negative,negative
1368195768,"Please check arguments allowed:
```
minst % python main.py -h
usage: main.py [-h] [--batch-size N] [--test-batch-size N] [--epochs N]
               [--lr LR] [--gamma M] [--no-cuda] [--no-mps] [--dry-run]
               [--seed S] [--log-interval N] [--save-model]

PyTorch MNIST Example

optional arguments:
  -h, --help           show this help message and exit
  --batch-size N       input batch size for training (default: 64)
  --test-batch-size N  input batch size for testing (default: 1000)
  --epochs N           number of epochs to train (default: 14)
  --lr LR              learning rate (default: 1.0)
  --gamma M            Learning rate step gamma (default: 0.7)
  --no-cuda            disables CUDA training
  --no-mps             disables macOS GPU training
  --dry-run            quickly check a single pass
  --seed S             random seed (default: 1)
  --log-interval N     how many batches to wait before logging training status
  --save-model         For Saving the current Model
```",please check python usage gamma seed example optional help show help message exit input batch size training default input batch size testing default number train default learning rate default gamma learning rate step gamma default training training quickly check single pas seed random seed default many wait logging training status saving current model,issue,positive,positive,neutral,neutral,positive,positive
1349683290,"I am using DDP in one node that has 8 GPU and 24 CPUs. I want to know how I can fix the following error. Also, as for loss, gradients, acc, and checkpoints, how do we take care of them if we have a single node multi-GPU situation? Could you please link me to a full example? Write now since more than one GPU accesses same file, the error is raised.
```
        with open (opt.outf+namefile,'a') as file:
            s = '{}, {},{:.15f}\n'.format(
                epoch,batch_idx,loss.data.item())
            print(s)
            file.write(s)
```

```
load models                                                                                                                                                           
Training network pretrained on imagenet.                                                                                                                              
training data: 3125 batches                                                                                                                                           
load models                                                                                                                                                           
Training network pretrained on imagenet.                                                                                                                              
Train Epoch: 1 [0/50000 (0%)]   Loss: 0.047746550291777                                                                                                               
Train Epoch: 1 [0/50000 (0%)]   Loss: 0.047966860234737                                                                                                               
Train Epoch: 1 [0/50000 (0%)]   Loss: 0.047879129648209                                                                                                               
Train Epoch: 1 [0/50000 (0%)]   Loss: 0.047865282744169                                                                                                               
Traceback (most recent call last):                                                                                                                                    
  File ""train.py"", line 1330, in <module>                                                                                                                             
    _runnetwork(epoch,trainingdata)                                                                                                                                   
  File ""train.py"", line 1308, in _runnetwork                                                                                                                          
    file.write(s)                                                                                                                                                     
OSError: [Errno 5] Input/output error                                                                                                                                 
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 58392 closing signal SIGTERM                                                                    
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 58393 closing signal SIGTERM                                                                    
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 58394 closing signal SIGTERM                                                                    
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 3 (pid: 58395) of binary: /anaconda/envs/azureml_py38/bin/python                 
Traceback (most recent call last):                                                                                                                                    
  File ""/anaconda/envs/azureml_py38/lib/python3.8/runpy.py"", line 194, in _run_module_as_main                                                                         
    return _run_code(code, main_globals, None,                                                                                                                        
  File ""/anaconda/envs/azureml_py38/lib/python3.8/runpy.py"", line 87, in _run_code                                                                                    
    exec(code, run_globals)                                                                                                                                           
  File ""/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/distributed/launch.py"", line 193, in <module>                                                   
    main()                                                                                                                                                            
  File ""/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/distributed/launch.py"", line 189, in main                                                       
    launch(args)                                                                                                                                                      
  File ""/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/distributed/launch.py"", line 174, in launch
    run(args)
  File ""/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/distributed/run.py"", line 752, in run
    elastic_launch(
  File ""/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/distributed/launcher/api.py"", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/distributed/launcher/api.py"", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED

```

My other questions are do I need a DistributedSampler for DataLoader if I only have one node?

Here's how  I run my code:
`$ time python -m torch.distributed.launch --nproc_per_node=4 train.py --data MYDATA  --out MYOUTPUT   --gpuids 0 1 2 3 --batchsize 16`

And here's the part of code I have changed from it being DataParallel to now being DistributedDataParallel:

```

parser.add_argument(""--local_rank"", default=0, type=int)

torch.cuda.set_device(opt.local_rank)
torch.distributed.init_process_group(backend='nccl',
                                     init_method='env://',
                                     timeout=datetime.timedelta(seconds=5400))



net = torch.nn.parallel.DistributedDataParallel(net.cuda(),
        device_ids=[opt.local_rank],
        output_device=opt.local_rank)
```

Do I also need to have distributed optimizer and distributed gradients too?
",one node want know fix following error also loss take care single node situation could please link full example write since one file error raised open file epoch print load training network training data load training network train epoch loss train epoch loss train epoch loss train epoch loss recent call last file line module epoch file line error warning sending process signal warning sending process signal warning sending process signal error binary recent call last file line return code none file line code file line module main file line main launch file line launch run file line run file line return list file line raise need one node run code time python data part code net also need distributed distributed,issue,negative,positive,neutral,neutral,positive,positive
1348028736,"> @IdiosyncraticDragon Nice catch. Thanks for the fix! I left a minor comment.

Thank you for your suggestions~~, it is a better modification.",nice catch thanks fix left minor comment thank better modification,issue,positive,positive,positive,positive,positive,positive
1335474090,"Hi @pbelevich! 

Thank you for your pull request. 

We **require** contributors to sign our **Contributor License Agreement**, and yours needs attention.

You currently have a record in our system, but the CLA is no longer valid, and will need to be **resubmitted**.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20pytorch%2Fexamples%20%23876). Thanks!",hi thank pull request require sign contributor license agreement need attention currently record system longer valid need process order u review merge please sign behalf someone else employer individual may sufficient employer may need sign corporate tooling perform afterwards pull request tagged process may take hour please give time u received error please contact u thanks,issue,positive,positive,neutral,neutral,positive,positive
1334773758,"The CI failure msg is like:

```
./run_python_examples.sh: line 63: startrun: command not found
python: can't open file 'distributed/tensor_parallelism/example.py': [Errno 2] No such file or directory
tensor parallel example failed
python: can't open file 'distributed/ddp/main.py': [Errno 2] No such file or directory
ddp example failed
```

Which does not make sense because the file is here and we didn't change anything regarding with DDP. Will first merge this one and work with POC to figure out why this shows up in CI. Cannot repro locally on my side.",failure like line command found python ca open file file directory tensor parallel example python ca open file file directory example make sense file change anything regarding first merge one work figure locally side,issue,negative,negative,neutral,neutral,negative,negative
1330853872,"Oof, thanks for bringing this up @elfarouk and helping to improve the quality of this example!!",thanks helping improve quality example,issue,positive,positive,positive,positive,positive,positive
1327093605,"and my code is here
```
from math import gamma
import os
import torch
import argparse
from tqdm import tqdm
from utils.scheduler import GradualWarmupScheduler
from modeling.model import CNN
from modeling.loss import CTCLoss
from utils.dataset import CharDict, LoadData, ImageTransform
from utils.utils import paser_config, edit_distance_score, setup_logger
from torch.utils.data import DataLoader

import torch.distributed as dist
import torch.multiprocessing as mp

from torch.nn.parallel import DistributedDataParallel as DDP

class Trainer:

    def __init__(self, config_file):
        self.configs = paser_config(config_file)
        # os.environ['CUDA_VISIBLE_DEVICES'] = self.configs['trainer']['gpus']
        self.build_dataloader()
        self.build_model()
        self.start_epoch = 0
        self.max_epochs = self.configs['trainer']['epochs']
        self.save_dir = os.path.join(self.configs['trainer']['output_dir'], self.configs['name'])
        if not os.path.exists(self.save_dir) : os.makedirs(self.save_dir)
        log_file_mode = 'a' if self.configs['trainer'][""resume_ckpt""] else 'w'
        self.logger = setup_logger(log_file_path=os.path.join(self.save_dir, 'train.log'), log_file_mode=log_file_mode)
        self.checkpoint = {
            'epoch': 0,
            'history_acc': [],
            'history_eds': [],
            'model': {},
            'optimizer': {},
            'lr_scheduler': {},
            'configs': self.configs
        }
        if self.configs['trainer'][""finetune_ckpt""]:
            self.model.load_state_dict(torch.load(self.configs['trainer'][""finetune_ckpt""])['model'], False)
            #ckpt = torch.load(self.configs['trainer'][""finetune_ckpt""])['model']
            #self.model.load_state_dict({k: v for k, v in ckpt.items() if 'fc' not in k},False)
        elif self.configs['trainer'][""resume_ckpt""]:
            self.checkpoint = torch.load(self.configs['trainer'][""resume_ckpt""])
            self.model.load_state_dict(self.checkpoint['model'])
            self.optimizer.load_state_dict(self.checkpoint['optimizer'])
            self.lr_scheduler.load_state_dict(self.checkpoint['lr_scheduler'])
            self.checkpoint['model'].clear()
            self.checkpoint['optimizer'].clear()
            self.checkpoint['lr_scheduler'].clear()
            self.start_epoch = self.checkpoint['epoch'] + 1
        # warp dp-model
        # self.model = torch.nn.DataParallel(self.model)
    def setup(self,rank, world_size):
        os.environ['MASTER_ADDR'] = 'localhost'
        os.environ['MASTER_PORT'] = '12355'
        # initialize the process group
        dist.init_process_group(""nccl"", rank=rank, world_size=world_size)
    def cleanup(self):
        dist.destroy_process_group()
    def train(self,rank,world_size):
        self.setup(rank,world_size)
        self.model = self.model.to(rank)
        self.model = DDP(self.model, device_ids=[rank])
        for epoch in range(self.start_epoch, self.max_epochs):
            self.model.train()
            self.checkpoint['epoch'] = epoch
            for i, datas in enumerate(self.train_dataloader):
                img, targets, target_lens = datas[""img""], datas[""target""], datas[""target_len""]
                img = img.to(rank)
                preds = self.model(img)
                loss = self.criterion(preds, targets.to(rank), target_lens.to(rank))
                loss.backward()
                self.optimizer.step()
                self.optimizer.zero_grad()
                # log info
                if i%10 == 0:
                    batch_acc, batch_eds = self.metrics(preds, targets, target_lens)
                    msg = ""Epoch: %d/%d, "" % (epoch, self.max_epochs) + \
                          ""Batch: %d/%d, ""%(i, len(self.train_dataloader)) + \
                          ""Lr: %.6f, "" %  self.scheduler_warmup.get_last_lr()[0] + \
                          ""Loss: %.3f, "" % loss.item() + \
                          ""Acc: %.3f, EDS: %.3f"" % (batch_acc, batch_eds)
                    self.logger.info(msg)
            self.scheduler_warmup.step()
            self.cleanup()
            self.eval()

    @torch.no_grad()
    def eval(self):
        self.model.eval()
        nbatch = len(self.test_dataloader)
        acc, eds = 0, 0
        for datas in tqdm(self.test_dataloader, desc=""Testing...""):
            img, targets, target_lens = datas[""img""], datas[""target""], datas[""target_len""]
            preds = self.model(img.cuda())
            batch_acc, batch_eds = self.metrics(preds, targets, target_lens)
            acc += batch_acc
            eds += batch_eds
        mean_acc = acc / nbatch
        mean_eds = eds / nbatch

        self.save_model(mean_acc, mean_eds)
        return mean_acc, mean_eds

    def metrics(self, preds, targets, target_lens):
        """"""WARNING:
            This function will consume a lot of time. Don't use it frequently.
        """"""
        bs = preds.size(0)
        preds_prob,  preds_idx = preds.permute(0,2,1).detach().softmax(dim=2).max(2)
        decode_idx, decode_prob,_ = self.chardict.ctc_decode(preds_idx.cpu().numpy(), preds_prob.cpu().numpy())
        preds_texts = [self.chardict.idx2text(i, reserve_char='\a') for i in decode_idx]
        target_texts = [self.chardict.idx2text(t[:l], reserve_char='') for t, l in zip(targets, target_lens)]
        ed_score = 0.0
        n_correct = 0
        for s1, s2 in zip(preds_texts, target_texts):
            ed_score += edit_distance_score(s1, s2)
            n_correct += (s1 == s2)
        ed_score /= bs
        batch_acc = n_correct / bs
        return batch_acc, ed_score

    def save_model(self, cur_acc, cur_eds):
        best_acc_path = os.path.join(self.save_dir, ""model_best_acc.pth"")
        best_eds_path = os.path.join(self.save_dir, ""model_best_eds.pth"")
        model_last_path = os.path.join(self.save_dir, ""model_last.pth"")
        self.checkpoint['history_acc'].append(cur_acc)
        self.checkpoint['history_eds'].append(cur_eds)
        self.checkpoint['model'] = self.model.module.state_dict()
        self.checkpoint['optimizer'] = self.optimizer.state_dict()
        self.checkpoint['lr_scheduler'] = self.lr_scheduler.state_dict()

        torch.save(self.checkpoint, model_last_path)
        self.logger.info(""Current acc: %.3f, eds: %.3f"" % (cur_acc, cur_eds))
        self.logger.info(""Save current epoch model to: %s"" % model_last_path)
        best_acc = max(self.checkpoint['history_acc'])
        best_eds = max(self.checkpoint['history_eds'])
        if cur_acc >= best_acc:
            torch.save(self.checkpoint, best_acc_path)
            self.logger.info(""Best acc: %.3f"", cur_acc)
            self.logger.info(""Save best Acc model to: %s"" % best_acc_path)
        if cur_eds >= best_eds:
            torch.save(self.checkpoint, best_eds_path)
            self.logger.info(""Best eds: %.3f"", cur_eds)
            self.logger.info(""Save best EDS model to: %s"" % best_eds_path)

        # release
        self.checkpoint['model'].clear()
        self.checkpoint['optimizer'].clear()
        self.checkpoint['lr_scheduler'].clear()

    def build_model(self):
        in_dim = 1 if self.configs['dataset']['img_mode'] == 'gray' else 3
        out_dim = self.configs['dataset']['ncls']
        self.model = CNN(in_dim, out_dim)
        self.optimizer = getattr(torch.optim, self.configs['optimizer']['type'])(
            self.model.parameters(), **self.configs['optimizer']['args'])
        #set lr_decay
        lr_scheduler_type = self.configs['lr_scheduler']['type']
        if lr_scheduler_type == ""StepLR"":
            self.lr_scheduler = getattr(torch.optim.lr_scheduler, self.configs['lr_scheduler']['type'])(
                self.optimizer, **self.configs['lr_scheduler']['args'])
        else:
            self.lr_scheduler = getattr(torch.optim.lr_scheduler,self.configs['lr_scheduler']['type'])(
                self.optimizer,5
            )
        self.criterion = CTCLoss()

    def build_dataloader(self):
        self.chardict = CharDict(
            self.configs['dataset']['dict'], self.configs['dataset']['ncls'])
        imtrans = ImageTransform(
            self.configs['dataset']['img_mode'], self.configs['dataset']['img_size'])
        trainset = LoadData(
            self.configs['dataset']['trainset'], self.chardict, imtrans)
        self.train_dataloader = DataLoader(
            trainset, self.configs['dataset']['batch_size'], shuffle=True, collate_fn=trainset.collate_fn, num_workers=16)
        testset = LoadData(
            self.configs['dataset']['testset'], self.chardict, imtrans)
        self.test_dataloader = DataLoader(
            testset, self.configs['dataset']['batch_size'], shuffle=False, collate_fn=trainset.collate_fn, num_workers=16)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--config_file', default='config/pycrnn.yaml', type=str)
    args = parser.parse_args()
    trainer = Trainer(args.config_file)
    world_size = 4
    mp.spawn(trainer.train,
            args=(world_size, ),
            nprocs = world_size,
            join=True)
```",code math import gamma import o import torch import import import import import import import import import import import class trainer self else false false warp setup self rank initialize process group cleanup self train self rank rank rank rank epoch range epoch enumerate target rank loss rank rank log epoch epoch batch loss self testing target return metric self warning function consume lot time use zip zip return self current save current epoch model best save best model best save best model release self else set else self parser trainer trainer,issue,positive,negative,negative,negative,negative,negative
1326530590,"same error
evionment infomation
------------------------------------------------------
Is debug build:False
CUDA used to build PyTorch:11.3
ROCM used to build PyTorch:N/A
o5:Ubuntu18.04.6LT5(x86_64)
GCC version:(ubuntu 7.5.0-3ubuntul~18.04)7.5.0
clang version:Could not collectCMake version:Could not collect
Libc version:glibc-2.17
Python version:3.7.11 (default,Jul 27 2021,14:32:16)[GCC 7.5.0](64-bit runtime)
Python platform:Linux-3.10.0-1062.4.3.e17.x86_64-x86_64-with-debian-buster-sid
Is CUDA available:True
CUDA runtime version:11.3.109
CUDA MODULE_LOADING set to:
GPU models and configuration:
GPU 0:
NVIDIA A100 80GB PCIe
GPU 1:
NVIDIA A100 80GB PCIe
GPU 2:
NVIDIA A100 80GB PCIe
GPU 3:
NVIDIA A100 80GB PCIe
Nvidia driver version:515.65.01
cuDNN version:Probably one of the following:/usr/lib/x86_64-linux-gnu/libcudnn.so.8.2.0
/usr/lib/x86_64-1inux-gnu/libcudnn_adv_infer.so.8.2.0/usr/lib/x86_64-1inux-gnu/libcudnn_adv_train.so.8.2.0/usr/lib/x86_64-1inux-gnu/libcudnn_cnn_infer.so.8.2.0/usr/lib/x86_64-1inux-gnu/libcudnn_cnn_train.so.8.2.0/usr/lib/x86_64-1inux-gnu/libcudnn_ops_infer.so.8.2.0/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.2.0HIP runtime version:N/AMIopen runtime version:N/A
Is XNNPACK available:True
versions of relevant libraries:[pip3]numpy==1.21.5
pip3]numpydoc==1.2
[pip3]torch==1.10.2+cu113
[pip3]torchtext==0.11.2
[pip3]torchvision==0.11.3+cu113
[conda]No relevant packages
---------------------------------------------------------------------------------------
`
from torchtext.vocab import build_vocab_from_iterator
from torchtext.data.utils import get_tokenizer
from data.wikitext2_data import WikiText2
import random
import argparse
import time
import math
from torch.nn import TransformerEncoder, TransformerEncoderLayer
import torch
import torch.distributed as dist
import torch.distributed.autograd as dist_autograd
import torch.distributed.rpc as rpc
import torch.multiprocessing as mp
import torch.optim as optim
from torch import nn
from torch.distributed.nn import RemoteModule
from torch.distributed.optim import DistributedOptimizer
from torch.distributed.rpc import RRef
from torch.distributed.rpc import TensorPipeRpcBackendOptions
from torch.nn.parallel import DistributedDataParallel as DDP
torch.autograd.set_detect_anomaly(True)

NUM_EMBEDDINGS = 100
EMBEDDING_DIM = 2
batch_size = 100
num_workers = 2
# train_iter, val_iter, test_iter = WikiText2()
total_loss = 0.
train_iter, val_iter, test_iter = WikiText2(root='./data')
tokenizer = get_tokenizer('basic_english')
vocab = build_vocab_from_iterator(
    map(tokenizer, train_iter), specials=[""<unk>""])
vocab.set_default_index(vocab[""<unk>""])
ntokens = len(vocab)  # the size of vocabulary
emsize = 4096  # embedding dimension
nhid = 4096  # the dimension of the feedforward network model in nn.TransformerEncoder
nlayers = 8  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder
nhead = 16  # the number of heads in the multiheadattention models
dropout = 0.2  # the dropout value


def data_process(raw_text_iter):
    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long)
            for item in raw_text_iter]
    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))


train_iter, val_iter, test_iter = WikiText2()
train_data = data_process(train_iter)
val_data = data_process(val_iter)
test_data = data_process(test_iter)


class PositionalEncoding(nn.Module):

    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(
            0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_parameter('pe', nn.Parameter(pe, requires_grad=False))

    def forward(self, x):
        x_ = x + self.pe[:x.size(0), :]
        return self.dropout(x_)


class Encoder(nn.Module):
    def __init__(self, ntoken, ninp, dropout=0.5):
        super(Encoder, self).__init__()
        self.pos_encoder = PositionalEncoding(ninp, dropout)
        self.encoder = nn.Embedding(ntoken, ninp)
        self.ninp = ninp
        self.init_weights()

    def init_weights(self):
        initrange = 0.1
        self.encoder.weight.data.uniform_(-initrange, initrange)

    def forward(self, src):

        src_ = src.t()
        src__ = self.encoder(src_) * math.sqrt(self.ninp)
        return self.pos_encoder(src__).cpu()


class Decoder(nn.Module):
    def __init__(self, ntoken, ninp):
        super(Decoder, self).__init__()
        self.decoder = nn.Linear(ninp, ntoken)
        self.init_weights()

    def init_weights(self):
        initrange = 0.1
        self.decoder.bias.data.zero_()
        self.decoder.weight.data.uniform_(-initrange, initrange)

    def forward(self, inp):
        # Need batch dimension first for output of pipeline.

        return self.decoder(inp).permute(1, 0, 2).contiguous().view(-1, ntokens)


class MID(nn.Module):
    def __init__(self, emsize, nhead, nhid, dropout, device):
        super(MID, self).__init__()
        tmp_list = []
        nlayers = 1
        self.emsize = emsize
        for i in range(nlayers):
            transformer_block = TransformerEncoderLayer(
                emsize, nhead, nhid, dropout)
            tmp_list.append(transformer_block)
        self.rnn0 = nn.Sequential(*tmp_list)
        self.device = device
        # self.init_weights()

    def init_weights(self):
        initrange = 0.1
        self.decoder.bias.data.zero_()
        self.decoder.weight.data.uniform_(-initrange, initrange)

    def forward(self, inp):
        # Need batch dimension first for output of pipeline.
        # if torch.cuda.is_available():
        if type(inp) in (tuple, list):
            # print(""*""*100, inp[0].device, self.device)
            # device = inp[0].device

            inp = torch.cat([torch.unsqueeze(t, dim=0)
                            for t in inp]).cuda(self.device)  # .reshape(-1, self.emsize)
            # inp = inp.cuda(0)
        return self.rnn0(inp)  # .cpu()


class RNNModel(nn.Module):
    r""""""
    A distributed RNN model which puts embedding table and decoder parameters on
    a remote parameter server, and locally holds parameters for the LSTM module.
    The structure of the RNN model is borrowed from the word language model
    example. See https://github.com/pytorch/examples/blob/main/word_language_model/model.py
    """"""

    def __init__(self, remote_emb_module, device='cpu'):
        super(RNNModel, self).__init__()
        # setup embedding table remotely
        self.remote_emb_module = remote_emb_module
        # setup LSTM locally
        self.decoder_rref = DDP(Decoder(ntokens, emsize).cuda(device),
                                device_ids=[device])
        self.device = device

    def forward(self, input):
        # pass input to the remote embedding table and fetch emb tensor back
        emb = self.remote_emb_module[0].forward(input)
        emb = self.remote_emb_module[1].forward(emb)
        # emb = self.remote_emb_module[2].forward(emb)
        if type(emb) in (tuple, list):
            emb = torch.cat([torch.unsqueeze(t, dim=0)
                            for t in emb])
        # print(""#""*100, emb.device, self.device, self.decoder_rref.device)
        # print(""#""*100, emb.device, self.device)
        return self.decoder_rref(emb.cuda(self.device))


class HybridModel(torch.nn.Module):
    r""""""
      The model consists of a sparse part and a dense part.
      1) The dense part is an nn.Linear module that is replicated across all trainers using DistributedDataParallel.
      2) The sparse part is a Remote Module that holds an nn.EmbeddingBag on the parameter server.
      This remote model can get a Remote Reference to the embedding table on the parameter server.
      """"""

    def __init__(self, remote_emb_module, device):
        print(f""Init HybridModel on device {device}"")
        super(HybridModel, self).__init__()
        self.remote_emb_module = remote_emb_module
        self.fc = DDP(torch.nn.Linear(EMBEDDING_DIM, 8).cuda(device),
                      device_ids=[device])
        self.device = device

    def forward(self, indices):
        print(self.remote_emb_module[0].device, indices[:, 0].device)
        print(self.remote_emb_module[1].device, indices[:, 1].device)

        emb_lookup1 = self.remote_emb_module[0].forward(indices[:, 0])
        emb_lookup2 = self.remote_emb_module[1].forward(indices[:, 1])
        if type(emb_lookup1) in (tuple, list):
            emb_lookup1 = torch.cat(
                emb_lookup1, dim=0).reshape(-1, EMBEDDING_DIM)
        if type(emb_lookup2) in (tuple, list):
            emb_lookup2 = torch.cat(
                emb_lookup2, dim=0).reshape(-1, EMBEDDING_DIM)

        # print(f""curr rank: {torch.distributed.get_rank()}, ""
        #       f""self.device: {self.device}, "")

        return self.fc(
            emb_lookup1.cuda(self.device) + emb_lookup2.cuda(self.device))


def _run_trainer(remote_emb_module, rank):
    r""""""
      Each trainer runs a forward pass which involves an embedding lookup on the
      parameter server and running nn.Linear locally. During the backward pass,
      DDP is responsible for aggregating the gradients for the dense part
      (nn.Linear) and distributed autograd ensures gradients updates are
      propagated to the parameter server.
      """"""

    # model = RNNModel( emsize, ntokens, nhid, nlayers)
    print(f""Setup the model on {rank}"")
    model = RNNModel(remote_emb_module=remote_emb_module, device=rank)

    model_parameter_rrefs = []
    for rm in model.remote_emb_module:
        model_parameter_rrefs += rm.remote_parameters()

    for param in model.decoder_rref.parameters():
        model_parameter_rrefs.append(RRef(param))
    device = torch.device(""cpu"")

    def batchify(data, bsz):
        # Divide the dataset into bsz parts.
        nbatch = data.size(0) // bsz
        # Trim off any extra elements that wouldn't cleanly fit (remainders).
        data = data.narrow(0, 0, nbatch * bsz)
        # Evenly divide the data across the bsz batches.
        data = data.view(bsz, -1).t().contiguous()
        return data.to(device)

    batch_size = 20
    eval_batch_size = 10
    global train_data, val_data, test_data
    train_data = batchify(train_data, batch_size)
    val_data = batchify(val_data, eval_batch_size)
    test_data = batchify(test_data, eval_batch_size)
    bptt = 35

    def get_batch(source, i):
        seq_len = min(bptt, len(source) - 1 - i)
        data = source[i:i+seq_len]
        target = source[i+1:i+1+seq_len].view(-1)
        # Need batch dimension first for pipeline parallelism.
        return data.t(), target

    # setup distributed optimizer
    opt = DistributedOptimizer(
        optim.SGD,
        model_parameter_rrefs,
        lr=0.1
    )

    criterion = torch.nn.CrossEntropyLoss()

    # Train only for 50 batches to keep script execution time low.
    nbatches = min(50 * bptt, train_data.size(0) - 1)
    max_epochs = 3

    def train():
        total_loss = 0.0
        for batch, i in enumerate(range(0, nbatches, bptt)):
            data, targets = get_batch(train_data, i)
            targets = targets.to(rank)
            with dist_autograd.context() as context_id:
                # optimizer.zero_grad()
                # Since the Pipe is only within a single host and process the ``RRef``
                # returned by forward method is local to this node and can simply
                # retrieved via ``RRef.local_value()``.
                output = model(data)
                # Need to move targets to the device where the output of the
                # pipeline resides.
                # print('*'*100)
                # print(output.shape,ntokens,targets.shape)

                loss = criterion(
                    output, targets)

                dist_autograd.backward(context_id, [loss])
                # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)
                opt.step(context_id)
                # print(loss)
                total_loss += loss.item()
                log_interval = 10
                if batch % log_interval == 0 and batch > 0:
                    cur_loss = total_loss / log_interval
                    elapsed = time.time() - start_time
                    print('| epoch {:3d} | {:5d}/{:5d} batches | '
                          'lr {:02.2f} | ms/batch {:5.2f} | '
                          'loss {:5.2f} | ppl {:8.2f}'.format(
                              epoch, batch, nbatches // bptt, 5,
                              elapsed * 1000 / log_interval,
                              cur_loss, math.exp(cur_loss)))
                    total_loss = 0
                start_time = time.time()

    for epoch in range(1, max_epochs + 1):
        epoch_start_time = time.time()
        train()
        # val_loss = evaluate(model, val_data)
        print('-' * 89)
        # print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '
        #       'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),
        #                                  val_loss, math.exp(val_loss)))
        print('-' * 89)


def run_worker(rank, world_size):
    r""""""
      A wrapper function that initializes RPC, calls the function, and shuts down
      RPC.
      """"""

    rpc_backend_options = TensorPipeRpcBackendOptions()
    rpc_backend_options.init_method = ""tcp://localhost:29501""
    rpc_backend_options.rpc_timeout = 60000*5
    if rank == num_workers:
        rpc.init_rpc(
            ""master"",
            rank=rank,
            world_size=world_size,
            rpc_backend_options=rpc_backend_options,
        )
        remote_emb_module = [
            RemoteModule(
                ""trainer0/cuda:0"",
                Encoder,
                args=(ntokens, emsize, dropout),
                kwargs={},
            ),
            RemoteModule(
                ""trainer1/cuda:1"",
                MID,
                args=(emsize, nhead, nhid, dropout, 1),
                kwargs={},
            ),
            # RemoteModule(
            #     ""trainer2/cuda:2"",
            #     MID,
            #     args=(emsize, nhead, nhid, dropout, 2),
            #     kwargs={},
            # ),
        ]

        # Run the training loop on trainers.
        futs = []
        for trainer_rank in range(num_workers):
            trainer_name = ""trainer{}"".format(trainer_rank)
            fut = rpc.rpc_async(trainer_name,
                                _run_trainer,
                                args=(remote_emb_module, trainer_rank))
            futs.append(fut)

        # Wait for all training to finish.
        for fut in futs:
            fut.wait()
    elif rank < num_workers:
        # Initialize process group for Distributed DataParallel on trainers.
        dist.init_process_group(backend=""nccl"",
                                rank=rank,
                                world_size=num_workers,
                                init_method=""tcp://localhost:29500"")

        # Initialize RPC.
        trainer_name = ""trainer{}"".format(rank)
        worker_rpc_backend_options = TensorPipeRpcBackendOptions()
        worker_rpc_backend_options.init_method = ""tcp://localhost:29501""
        worker_rpc_backend_options.rpc_timeout = 60000*5
        for remote_rank in range(num_workers):
            if remote_rank != rank:
                worker_rpc_backend_options.set_device_map(f""trainer{remote_rank}"",
                                                          {rank: remote_rank})
        rpc.init_rpc(
            trainer_name,
            rank=rank,
            world_size=world_size,
            rpc_backend_options=worker_rpc_backend_options,
        )

    # Trainer just waits for RPCs from master.

    # block until all rpcs finish
    rpc.shutdown()


def main(args):
    run_worker(int(args.rank), int(args.world_size))


if __name__ == ""__main__"":
    # world_size = 1 + num_workers
    # mp.spawn(run_worker, args=(world_size,), nprocs=world_size, join=True)
    argparser = argparse.ArgumentParser(""training"")
    argparser.add_argument('--rank', default='0', type=str)
    argparser.add_argument('--world_size', default=3, type=int)

    args = argparser.parse_args()
    main(args)

`
reference
#60440
https://github.com/pytorch/pytorch/issues/60440
",error build false used build used build version clang version could version could collect version python version default python platform available true version set configuration driver version version probably one following hip version version available true relevant pip pip pip pip pip relevant import import import import random import import time import math import import torch import import import import import torch import import import import import import true map size vocabulary dimension dimension network model number number dropout dropout value data item item return filter lambda data class self super self position position position forward self return class self super self dropout self forward self return class self super self self forward self need batch dimension first output pipeline return class mid self dropout device super mid self range dropout device self forward self need batch dimension first output pipeline type list print device return class distributed model table remote parameter server locally module structure model word language model example see self super self setup table remotely setup locally device device device forward self input pas input remote table fetch tensor back input type list print print return class model sparse part dense dense part module replicated across sparse part remote module parameter server remote model get remote reference table parameter self device print device device super self device device device forward self index print index print index index index type list type list print curr rank return rank trainer forward pas parameter server running locally backward pas responsible dense part distributed parameter model print setup model rank model param param device data divide trim extra would cleanly fit data evenly divide data across data return device global source min source data source target source need batch dimension first pipeline parallelism return target setup distributed opt criterion train keep script execution time low min train batch enumerate range data rank since pipe within single host process returned forward method local node simply via output model data need move device output pipeline print print loss criterion output loss print loss batch batch print epoch epoch batch epoch range train evaluate model print print end epoch time valid loss epoch print rank wrapper function function rank master dropout mid dropout mid dropout run training loop range trainer fut fut wait training finish fut rank initialize process group distributed initialize trainer rank range rank trainer rank trainer master block finish main training rank main reference,issue,positive,negative,neutral,neutral,negative,negative
1322998629,"Finished scripts, reports and notebooks for Cloud ML Project 1: Roofline Analysis",finished cloud project analysis,issue,negative,neutral,neutral,neutral,neutral,neutral
1309404236,"@facebook-github-bot  Hello, can someone help to review the change ? Thanks",hello someone help review change thanks,issue,positive,positive,positive,positive,positive,positive
1308492993,@timothylimyl what was the training time with this GPU utilization? also how did you solve your issue?,training time utilization also solve issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1302745823,It seems due to changes introduced in pytorch nightly build(1.14.0.dev20221103+cpu). Investigating...,due nightly build investigating,issue,negative,negative,negative,negative,negative,negative
1277721807,"@hudeven Of course.

After cloning this repo and downloading the lsun data, I'm running this command:
`python main.py --dataset lsun --dataroot lsun --cuda --ngpu 2
`

That is resulting in this error:

```
Namespace(dataset='lsun', dataroot='lsun', workers=2, batchSize=64, imageSize=64, nz=100, ngf=64, ndf=64, niter=25, lr=0.0002, beta1=0.5, cuda=True, dry_run=False, ngpu=2, netG='', netD='', outf='.', manualSeed=None, classes='bedroom')
Random Seed:  177
Generator(
  (main): Sequential(
    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU(inplace=True)
    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (11): ReLU(inplace=True)
    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (13): Tanh()
  )
)
Discriminator(
  (main): Sequential(
    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): LeakyReLU(negative_slope=0.2, inplace=True)
    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): LeakyReLU(negative_slope=0.2, inplace=True)
    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): LeakyReLU(negative_slope=0.2, inplace=True)
    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)
    (12): Sigmoid()
  )
)
Traceback (most recent call last):
  File ""C:\Users\Windows\Documents\examples\dcgan\main.py"", line 219, in <module>
    for i, data in enumerate(dataloader, 0):
  File ""C:\Users\Windows\anaconda3\lib\site-packages\torch\utils\data\dataloader.py"", line 444, in __iter__
    return self._get_iterator()
  File ""C:\Users\Windows\anaconda3\lib\site-packages\torch\utils\data\dataloader.py"", line 390, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File ""C:\Users\Windows\anaconda3\lib\site-packages\torch\utils\data\dataloader.py"", line 1077, in __init__
    w.start()
  File ""C:\Users\Windows\anaconda3\lib\multiprocessing\process.py"", line 121, in start
    self._popen = self._Popen(self)
  File ""C:\Users\Windows\anaconda3\lib\multiprocessing\context.py"", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File ""C:\Users\Windows\anaconda3\lib\multiprocessing\context.py"", line 327, in _Popen
    return Popen(process_obj)
  File ""C:\Users\Windows\anaconda3\lib\multiprocessing\popen_spawn_win32.py"", line 93, in __init__
    reduction.dump(process_obj, to_child)
  File ""C:\Users\Windows\anaconda3\lib\multiprocessing\reduction.py"", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
TypeError: cannot pickle 'Environment' object
PS C:\Users\Windows\Documents\examples\dcgan> Namespace(dataset='lsun', dataroot='lsun', workers=2, batchSize=64, imageSize=64, nz=100, ngf=64, ndf=64, niter=25, lr=0.0002, beta1=0.5, cuda=True, dry_run=False, ngpu=2, netG='', netD='', outf='.', manualSeed=None, classes='bedroom')
Random Seed:  7161
Generator(
  (main): Sequential(
    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU(inplace=True)
    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (11): ReLU(inplace=True)
    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (13): Tanh()
  )
)
Discriminator(
  (main): Sequential(
    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): LeakyReLU(negative_slope=0.2, inplace=True)
    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): LeakyReLU(negative_slope=0.2, inplace=True)
    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): LeakyReLU(negative_slope=0.2, inplace=True)
    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)
    (12): Sigmoid()
  )
)
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\Windows\anaconda3\lib\multiprocessing\spawn.py"", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File ""C:\Users\Windows\anaconda3\lib\multiprocessing\spawn.py"", line 125, in _main
    prepare(preparation_data)
  File ""C:\Users\Windows\anaconda3\lib\multiprocessing\spawn.py"", line 236, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File ""C:\Users\Windows\anaconda3\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
  File ""C:\Users\Windows\anaconda3\lib\runpy.py"", line 268, in run_path
    return _run_module_code(code, init_globals, run_name,
  File ""C:\Users\Windows\anaconda3\lib\runpy.py"", line 97, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File ""C:\Users\Windows\anaconda3\lib\runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""C:\Users\Windows\Documents\examples\dcgan\main.py"", line 219, in <module>
    for i, data in enumerate(dataloader, 0):
  File ""C:\Users\Windows\anaconda3\lib\site-packages\torch\utils\data\dataloader.py"", line 444, in __iter__
    return self._get_iterator()
  File ""C:\Users\Windows\anaconda3\lib\site-packages\torch\utils\data\dataloader.py"", line 390, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File ""C:\Users\Windows\anaconda3\lib\site-packages\torch\utils\data\dataloader.py"", line 1077, in __init__
    w.start()
  File ""C:\Users\Windows\anaconda3\lib\multiprocessing\process.py"", line 121, in start
    self._popen = self._Popen(self)
  File ""C:\Users\Windows\anaconda3\lib\multiprocessing\context.py"", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File ""C:\Users\Windows\anaconda3\lib\multiprocessing\context.py"", line 327, in _Popen
    return Popen(process_obj)
  File ""C:\Users\Windows\anaconda3\lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
  File ""C:\Users\Windows\anaconda3\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data
    _check_not_importing_main()
  File ""C:\Users\Windows\anaconda3\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The ""freeze_support()"" line can be omitted if the program
        is not going to be frozen to produce an executable.
```

I am only able to get the program running by setting the number of workers to 0:
`python main.py --dataset lsun --dataroot lsun --cuda --ngpu 2 --workers 0
`

I'm running on Windows 11. Please let me know if there are any other relevant specs I should provide.",course data running command python resulting error random seed generator main sequential tanh discriminator main sequential sigmoid recent call last file line module data enumerate file line return file line return self file line file line start self file line return file line return file line file line dump file protocol pickle object random seed generator main sequential tanh discriminator main sequential sigmoid recent call last file string line module file line file line prepare file line prepare data file line file line return code file line code file line code file line module data enumerate file line return file line return self file line file line start self file line return file line return file line file line file line raise attempt made start new process current process finished phase probably fork start child forgotten use proper idiom main module line program going frozen produce executable able get program running setting number python running please let know relevant spec provide,issue,negative,positive,neutral,neutral,positive,positive
1274663149,"@abaruni Are you looking for this `forward` function? If you remove this `forward` function, you will get that error message.
https://github.com/pytorch/examples/blob/ca1bd9167f7216e087532160fc5b98643d53f87e/mnist/main.py#L21-L34",looking forward function remove forward function get error message,issue,negative,neutral,neutral,neutral,neutral,neutral
1274534291,"Try adding ""-j 16"" to the command.
For the imagenet distributed training, the default number of dataloader workers is 4. This controls the number of processes used to feed data. For multiple GPUs, this default number becomes a bottleneck. On my system with 8 GPUs and 80 CPU cores, when I increase the number of dataloader workers to 16 by setting ""-j 16"", the training speed of resnet50 doubled, goes from 800 img/s to 1600 img/s.
Typically, you can gradually increase this worker number and stop when you see no speedup. A larger worker number requires more CPU cores.",try command distributed training default number number used feed data multiple default number becomes bottleneck system increase number setting training speed doubled go typically gradually increase worker number stop see worker number,issue,positive,negative,neutral,neutral,negative,negative
1272852413,"i don't know mnist, (and i'm not trying to) so i'm not sure the implementation of the `forward` function. i'm sure i can figure it out, but the example needs to updated to reflect this requirement, or the example should be removed. non-functioning code should not be exposed as examples",know trying sure implementation forward function sure figure example need reflect requirement example removed code exposed,issue,positive,positive,positive,positive,positive,positive
1271915601,"@lianchengmingjue good catch! By default, torch.load() first loads the snapshot to CPU then moves to the device it was saved from(I guess it's GPU0). In this case, all ranks load the snapshot to GPU0. We should always use ""map_location"" in torch.load() to load files saved in other environment. Because it might be saved in GPUx which doesn't exist in your host and cause a failure during loading. Please feel free to send a PR for the fix. 
cc: @suraj813 ",good catch default first snapshot device saved guess case load snapshot always use load saved environment might saved exist host cause failure loading please feel free send fix,issue,positive,positive,positive,positive,positive,positive
1265815083,"@RuleNHao thanks for catching this! 

`set_epoch()` is an important suggestion to ensure random shuffling; thank you for bringing this up, I'll update the example to include this.",thanks catching important suggestion ensure random shuffling thank update example include,issue,positive,positive,positive,positive,positive,positive
1265760551,And it also need to use `sampler.set_epoch()` method at the beginning of each epoch.,also need use method beginning epoch,issue,negative,neutral,neutral,neutral,neutral,neutral
1264401840,It took me hours to find the bug. This is an unforgettable experience.,took find bug unforgettable experience,issue,negative,positive,positive,positive,positive,positive
1255778857,Thank you for your kind and detail explanation @hudeven !!,thank kind detail explanation,issue,positive,positive,positive,positive,positive,positive
1255767435,"@DY112 Good question! Because `DistributedSampler` would pad the last uncompleted batch to become a full batch by default, which leads to wrong validation metrics. To get the correct metrics, we can either 1) use single GPU to run validation(it's slow though) or 2) use `DistributedSampler` for all batches until the last batch and use auxiliary dataset + regular Dataloader for the last batch. 

Read more in https://github.com/pytorch/examples/pull/980",good question would pad last uncompleted batch become full batch default wrong validation metric get correct metric either use single run validation slow though use last batch use auxiliary regular last batch read,issue,negative,positive,neutral,neutral,positive,positive
1255605708,"General comment - this example does not use activation checkpointing due to the timing of this PR (it wasn't added in FSDP until after this PR).  
But I think it would be good to update this example with it, to make sure it's present as activation checkpointing is one of our biggest throughput boosters. 

",general comment example use activation due timing added think would good update example make sure present activation one biggest throughput,issue,positive,positive,positive,positive,positive,positive
1255579087,"Let me review - I was not even aware this PR existed until today, so thanks for the direct link.",let review even aware today thanks direct link,issue,negative,positive,positive,positive,positive,positive
1255223361,@rohan-varma @lessw2020 @HamidShojanazeri once you tell @hudeven and I that you'd like to merge the PR let us know. This has been open for a while. Feel free to close any feedback you don't believe is relevant,tell like merge let u know open feel free close feedback believe relevant,issue,positive,positive,positive,positive,positive,positive
1253736508,"Sounds good @msaroufim I'll divide this into 3 PRs. The first one is the starter code for single, multigpu and multinode training. The second one will include files for multinode training on slurm. The third one will be the minGPT code.

The notebooks you mentioned are in included in this PR because they were in the original minGPT repo. However, these files are not needed for the tutorial itself. I'll strip it down to include the minimum files that our users will need to run this tutorial!

Closing this PR",good divide first one starter code single training second one include training third one code included original however tutorial strip include minimum need run tutorial,issue,positive,positive,positive,positive,positive,positive
1253097141,"Oh sorry.
My question was about imagenet training code.

https://github.com/pytorch/examples/blob/f82f5626b6432b8d0b08d58cc91f3bdbb355a772/imagenet/main.py#L256",oh sorry question training code,issue,negative,negative,negative,negative,negative,negative
1253031559,"Thanks @suraj813! Do you mind breaking this PR into many smaller ones? Should make it far easier to get a review. Also if possible I'd minimize notebooks unless absolutely necessary, they're not the easiest to manage during code review.",thanks mind breaking many smaller make far easier get review also possible minimize unless absolutely necessary easiest manage code review,issue,positive,positive,positive,positive,positive,positive
1252682503,"Hi @DY112 , there are many examples in this repo. Could you share a code pointer of the example you are talking about?",hi many could share code pointer example talking,issue,negative,positive,positive,positive,positive,positive
1247242122,"@hudeven , is there a bot merge command to check this in ?",bot merge command check,issue,negative,neutral,neutral,neutral,neutral,neutral
1247105321,"This was caused by the breaking change open ai gym made so it should be fixed by https://github.com/pytorch/examples/pull/1051

Will monitor and close all the issues if that's the case",breaking change open ai gym made fixed monitor close case,issue,negative,positive,neutral,neutral,positive,positive
1243553713,i also have the same question. Look like nobody answers that,also question look like nobody,issue,negative,neutral,neutral,neutral,neutral,neutral
1239249526,"> Shouldn't the batch_size be divided by the world_size rather than GPUs per node? Eg. if we have 2 nodes with 4 GPUs each, shouldn't we divide batch_size by 8 rather than by 4?

I also believe that using a global batch size as the sum of all per-GPU batch sizes is easier to understand. 

However, in this issue and #982 I just fix the annotation and make it consistent with the annotation in the argument parser (second code block above) to keep backward compatibility.",divided rather per node divide rather also believe global batch size sum batch size easier understand however issue fix annotation make consistent annotation argument parser second code block keep backward compatibility,issue,negative,positive,neutral,neutral,positive,positive
1232967393,"I ran into this issue and I had two problems. 

The first was I put the mnist/ directory at the same level as build/. This means I needed to change the code to go up one directory as such: auto dataset = torch::data::datasets::MNIST(""../mnist"") Note the two dots in front of /mnist. If you have the mnist directory inside of build/ then don't change this part.

The second issue is that I had outsmarted myself and deleted the .gz files in the mnist directory. Don't do that. Leave it just as the Python script pulled it.

After these, I was able to print out the file contents.",ran issue two first put directory level change code go one directory auto torch note two front directory inside change part second issue directory leave python script able print file content,issue,negative,positive,positive,positive,positive,positive
1227852591,"Hi @msaroufim, please see the log from `run_python_examples.sh` attached with the latest changes from my branch. Ran those on M1 MacBook Air 8 GB + macOS Ventura 13.0 Beta (22A5295i)
[examples_out.txt](https://github.com/pytorch/examples/files/9428724/examples_out.txt)
 ",hi please see log attached latest branch ran air beta ai,issue,negative,positive,positive,positive,positive,positive
1225109340,"@ShiboXing it's a known issue https://github.com/pytorch/pytorch/issues/80733 As it's fixed in 1.12.1 already and examples depend on the latest pytorch version, there is no action item for this issue. I'm closing it.",known issue fixed already depend latest version action item issue,issue,negative,positive,positive,positive,positive,positive
1225106001,"@sevaroy good catch! `datasets.ImageFolder` requires folder in this format: `root/label_name/xxx.png`. Here is a quick workaround: create a sub folder containing all images under train2014. e.g. `train2014/no_label/xxx.png`. The labels are not used in this examples anyway.

Please feel free to submit a PR to update the README accordingly. Otherwise, I will update it next week.

",good catch folder format quick create sub folder train used anyway please feel free submit update accordingly otherwise update next week,issue,positive,positive,positive,positive,positive,positive
1224991068,"@ShiboXing thanks for reporting the issue! Let's see if we can get any solution from https://github.com/fyu/lsun/issues/46 

Before that, I'm fixing the flaky test by using fake data in #1044 ",thanks issue let see get solution fixing flaky test fake data,issue,negative,negative,negative,negative,negative,negative
1224611027,"> I think the print for `--gpu` is useful. To reduce the confusion, we can add some clarification to the print message. Alternative: as DDP is recommended to replace DataParallel, we should probably remove the usage of DataParallel in examples and update the print message accordingly.

I'm confused. This PR is for updating the README. What's being printed?",think print useful reduce confusion add clarification print message alternative replace probably remove usage update print message accordingly confused printed,issue,negative,negative,neutral,neutral,negative,negative
1222777719,"Hi @ShiboXing , I'm not able to reproduce the issue on my side(torch==1.12.1 in Ubuntu 22.04 LTS) and CI is green as well. Here is a simpler alternative: `mp.set_start_method('spawn', force=True)`. Please feel free to submit a PR for it. ",hi able reproduce issue side green well simpler alternative please feel free submit,issue,positive,positive,positive,positive,positive,positive
1222716146,It seems to be a transient issue. CI run passed after a retry,transient issue run retry,issue,negative,neutral,neutral,neutral,neutral,neutral
1218465424,"As a similar change is already merged in https://github.com/pytorch/examples/pull/1034 , I'm closing this one. ",similar change already one,issue,negative,neutral,neutral,neutral,neutral,neutral
1218463705,"Hi @JoohyungLee0106 As a similar change is already merged in https://github.com/pytorch/examples/pull/1034 , I'm closing this one. Thanks for your contribution!",hi similar change already one thanks contribution,issue,negative,positive,neutral,neutral,positive,positive
1217549437,"## CLI for distributed training

```
python main.py --dist-url 'tcp://localhost:23456' --multiprocessing-distributed --world-size 1 --rank 0
```

## Output before this change to the description

```
Epoch: [0][626/626]     Time  3.895 ( 3.075)    Data  2.053 ( 2.954)    Loss 4.4438e+00 (5.4188e+00)    Acc@1  13.70 (  7.03)   Acc@5  39.04 ( 18.14)
Epoch: [0][626/626]     Time  3.892 ( 3.075)    Data  2.063 ( 1.639)    Loss 4.3967e+00 (5.4165e+00)    Acc@1  10.27 (  7.05)   Acc@5  34.93 ( 18.32)
Test: [ 1/25]   Time  6.812 ( 6.812)    Loss 4.5951e+00 (4.5951e+00)    Acc@1  18.75 ( 18.75)   Acc@5  33.98 ( 33.98)
Test: [ 1/25]   Time  7.230 ( 7.230)    Loss 4.3132e+00 (4.3132e+00)    Acc@1  18.36 ( 18.36)   Acc@5  38.28 ( 38.28)
Test: [ 1/25]   Time  7.227 ( 7.227)    Loss 4.3881e+00 (4.3881e+00)    Acc@1  16.80 ( 16.80)   Acc@5  39.45 ( 39.45)
Test: [ 1/25]   Time  7.235 ( 7.235)    Loss 4.4228e+00 (4.4228e+00)    Acc@1  16.02 ( 16.02)   Acc@5  35.16 ( 35.16)
Test: [ 1/25]   Time  7.232 ( 7.232)    Loss 4.1915e+00 (4.1915e+00)    Acc@1  21.48 ( 21.48)   Acc@5  43.36 ( 43.36)
Test: [ 1/25]   Time  7.236 ( 7.236)    Loss 4.4230e+00 (4.4230e+00)    Acc@1  14.45 ( 14.45)   Acc@5  37.89 ( 37.89)
Test: [ 1/25]   Time  7.226 ( 7.226)    Loss 4.3215e+00 (4.3215e+00)    Acc@1  17.97 ( 17.97)   Acc@5  40.62 ( 40.62)
Test: [ 1/25]   Time  7.232 ( 7.232)    Loss 4.7117e+00 (4.7117e+00)    Acc@1  16.80 ( 16.80)   Acc@5  34.77 ( 34.77)
Test: [ 2/25]   Time  1.595 ( 4.415)    Loss 5.0998e+00 (4.7614e+00)    Acc@1  10.94 ( 12.70)   Acc@5  21.88 ( 29.88)
Test: [ 2/25]   Time  1.609 ( 4.422)    Loss 5.3650e+00 (4.8939e+00)    Acc@1   9.77 ( 12.89)   Acc@5  20.70 ( 27.93)
Test: [ 2/25]   Time  1.721 ( 4.477)    Loss 5.2558e+00 (4.9837e+00)    Acc@1   9.77 ( 13.28)   Acc@5  26.95 ( 30.86)
Test: [ 2/25]   Time  2.808 ( 4.810)    Loss 5.1714e+00 (4.8833e+00)    Acc@1   8.20 ( 13.48)   Acc@5  23.05 ( 28.52)
Test: [ 2/25]   Time  2.419 ( 4.825)    Loss 5.2175e+00 (4.7045e+00)    Acc@1   9.77 ( 15.62)   Acc@5  23.44 ( 33.40)
Test: [ 2/25]   Time  2.535 ( 4.883)    Loss 5.1320e+00 (4.7226e+00)    Acc@1   8.98 ( 13.67)   Acc@5  20.31 ( 29.30)
Test: [ 2/25]   Time  2.630 ( 4.928)    Loss 5.1651e+00 (4.7766e+00)    Acc@1   7.81 ( 12.30)   Acc@5  18.75 ( 29.10)
Test: [ 2/25]   Time  4.529 ( 5.877)    Loss 5.4293e+00 (4.8754e+00)    Acc@1  10.94 ( 14.45)   Acc@5  22.27 ( 31.45)
Test: [ 3/25]   Time  2.922 ( 3.922)    Loss 4.4344e+00 (4.7407e+00)    Acc@1  18.36 ( 14.71)   Acc@5  39.84 ( 31.90)
Test: [ 3/25]   Time  2.620 ( 4.080)    Loss 4.3729e+00 (4.7131e+00)    Acc@1  13.28 ( 13.41)   Acc@5  38.28 ( 31.77)
Test: [ 3/25]   Time  2.552 ( 4.136)    Loss 4.5808e+00 (4.7113e+00)    Acc@1  19.92 ( 14.84)   Acc@5  33.59 ( 30.60)
Test: [ 3/25]   Time  2.769 ( 4.140)    Loss 4.7109e+00 (4.7067e+00)    Acc@1  13.67 ( 14.97)   Acc@5  32.42 ( 33.07)
Test: [ 3/25]   Time  3.682 ( 4.171)    Loss 4.5020e+00 (4.6749e+00)    Acc@1  17.19 ( 14.19)   Acc@5  34.77 ( 31.51)
Test: [ 3/25]   Time  3.934 ( 4.296)    Loss 4.4882e+00 (4.8186e+00)    Acc@1  16.41 ( 14.32)   Acc@5  35.16 ( 32.29)
Test: [ 3/25]   Time  3.245 ( 4.337)    Loss 4.6248e+00 (4.6900e+00)    Acc@1  18.75 ( 15.36)   Acc@5  36.72 ( 31.77)
Test: [ 3/25]   Time  2.902 ( 4.886)    Loss 4.5363e+00 (4.7624e+00)    Acc@1  16.41 ( 15.10)   Acc@5  33.59 ( 32.16)
Test: [ 4/25]   Time  2.811 ( 3.808)    Loss 4.2294e+00 (4.5874e+00)    Acc@1  20.31 ( 16.31)   Acc@5  39.84 ( 34.77)
Test: [ 4/25]   Time  2.823 ( 3.808)    Loss 4.1939e+00 (4.5820e+00)    Acc@1  15.23 ( 14.94)   Acc@5  36.72 ( 32.13)
Test: [ 4/25]   Time  2.884 ( 3.849)    Loss 4.4240e+00 (4.6122e+00)    Acc@1  16.80 ( 14.84)   Acc@5  38.28 ( 33.20)
Test: [ 4/25]   Time  2.570 ( 3.895)    Loss 4.3319e+00 (4.6005e+00)    Acc@1  17.58 ( 15.92)   Acc@5  38.28 ( 33.40)
Test: [ 4/25]   Time  3.069 ( 3.989)    Loss 4.4434e+00 (4.7248e+00)    Acc@1  14.45 ( 14.36)   Acc@5  37.50 ( 33.59)
Test: [ 4/25]   Time  4.047 ( 4.072)    Loss 4.2282e+00 (4.5919e+00)    Acc@1  18.75 ( 14.75)   Acc@5  40.23 ( 33.89)
Test: [ 4/25]   Time  4.876 ( 4.160)    Loss 3.8349e+00 (4.5143e+00)    Acc@1  21.48 ( 16.41)   Acc@5  43.36 ( 34.77)
Test: [ 4/25]   Time  2.673 ( 4.332)    Loss 4.1783e+00 (4.6163e+00)    Acc@1  17.97 ( 15.82)   Acc@5  38.67 ( 33.79)
Test: [ 5/25]   Time  2.773 ( 3.601)    Loss 4.5387e+00 (4.5776e+00)    Acc@1   7.81 ( 14.61)   Acc@5  23.05 ( 32.42)
Test: [ 5/25]   Time  2.749 ( 3.629)    Loss 4.6735e+00 (4.6244e+00)    Acc@1   7.03 ( 13.28)   Acc@5  22.27 ( 31.02)
Test: [ 5/25]   Time  2.520 ( 3.695)    Loss 4.4369e+00 (4.6672e+00)    Acc@1   5.86 ( 12.66)   Acc@5  23.44 ( 31.56)
Test: [ 5/25]   Time  2.540 ( 3.765)    Loss 4.8061e+00 (4.6347e+00)    Acc@1   5.08 ( 12.81)   Acc@5  21.88 ( 31.48)
Test: [ 5/25]   Time  3.691 ( 3.854)    Loss 4.6923e+00 (4.6188e+00)    Acc@1   5.47 ( 13.83)   Acc@5  21.09 ( 30.94)
Test: [ 5/25]   Time  2.474 ( 3.961)    Loss 4.6251e+00 (4.6181e+00)    Acc@1   5.86 ( 13.83)   Acc@5  25.00 ( 32.03)
Test: [ 5/25]   Time  3.448 ( 4.018)    Loss 4.6722e+00 (4.5459e+00)    Acc@1   4.69 ( 14.06)   Acc@5  22.66 ( 32.34)
Test: [ 6/25]   Time  2.511 ( 3.443)    Loss 4.1431e+00 (4.5442e+00)    Acc@1  12.89 ( 13.22)   Acc@5  37.89 ( 32.16)
Test: [ 6/25]   Time  2.672 ( 3.446)    Loss 4.1134e+00 (4.5003e+00)    Acc@1  15.23 ( 14.71)   Acc@5  36.33 ( 33.07)
Test: [ 6/25]   Time  2.434 ( 3.485)    Loss 4.0700e+00 (4.5677e+00)    Acc@1  14.45 ( 12.96)   Acc@5  40.62 ( 33.07)
Test: [ 6/25]   Time  2.510 ( 3.556)    Loss 4.2970e+00 (4.5785e+00)    Acc@1  14.45 ( 13.09)   Acc@5  35.94 ( 32.23)
Test: [ 6/25]   Time  2.416 ( 3.615)    Loss 4.1162e+00 (4.5351e+00)    Acc@1  14.45 ( 13.93)   Acc@5  38.28 ( 32.16)
Test: [ 5/25]   Time  6.944 ( 4.435)    Loss 4.6041e+00 (4.5864e+00)    Acc@1   6.64 ( 13.28)   Acc@5  22.27 ( 30.16)
Test: [ 6/25]   Time  2.487 ( 3.715)    Loss 4.2010e+00 (4.5486e+00)    Acc@1  13.67 ( 13.80)   Acc@5  35.16 ( 32.55)
Test: [ 6/25]   Time  2.598 ( 3.781)    Loss 4.2247e+00 (4.4923e+00)    Acc@1  13.67 ( 14.00)   Acc@5  37.50 ( 33.20)
Test: [ 7/25]   Time  2.716 ( 3.339)    Loss 4.6129e+00 (4.5540e+00)    Acc@1   7.03 ( 12.33)   Acc@5  25.39 ( 31.19)
Test: [ 7/25]   Time  2.660 ( 3.367)    Loss 4.4446e+00 (4.5501e+00)    Acc@1  11.33 ( 12.72)   Acc@5  32.42 ( 32.98)
Test: [ 7/25]   Time  2.541 ( 3.411)    Loss 4.5479e+00 (4.5741e+00)    Acc@1  10.55 ( 12.72)   Acc@5  27.73 ( 31.58)
Test: [ 7/25]   Time  2.660 ( 3.478)    Loss 4.4152e+00 (4.5179e+00)    Acc@1   9.38 ( 13.28)   Acc@5  24.22 ( 31.03)
Test: [ 6/25]   Time  2.450 ( 4.104)    Loss 3.9969e+00 (4.4882e+00)    Acc@1  13.28 ( 13.28)   Acc@5  39.06 ( 31.64)
Test: [ 7/25]   Time  3.181 ( 3.639)    Loss 4.5005e+00 (4.5417e+00)    Acc@1   9.38 ( 13.17)   Acc@5  26.17 ( 31.64)
Test: [ 7/25]   Time  3.615 ( 3.757)    Loss 4.4929e+00 (4.4924e+00)    Acc@1   9.77 ( 13.39)   Acc@5  30.47 ( 32.81)
Test: [ 8/25]   Time  2.980 ( 3.294)    Loss 4.5620e+00 (4.5550e+00)    Acc@1  20.70 ( 13.38)   Acc@5  37.50 ( 31.98)
Test: [ 8/25]   Time  3.326 ( 3.362)    Loss 4.3578e+00 (4.5261e+00)    Acc@1  20.31 ( 13.67)   Acc@5  36.72 ( 33.45)
Test: [ 8/25]   Time  3.147 ( 3.378)    Loss 4.2328e+00 (4.5314e+00)    Acc@1  21.48 ( 13.82)   Acc@5  41.80 ( 32.86)
Test: [ 7/25]   Time  6.387 ( 3.866)    Loss 4.4137e+00 (4.4879e+00)    Acc@1  11.72 ( 14.29)   Acc@5  31.64 ( 32.87)
Test: [ 7/25]   Time  2.852 ( 3.925)    Loss 4.6470e+00 (4.5108e+00)    Acc@1  10.94 ( 12.95)   Acc@5  28.12 ( 31.14)
Test: [ 8/25]   Time  2.532 ( 3.500)    Loss 4.3012e+00 (4.5117e+00)    Acc@1  19.14 ( 13.92)   Acc@5  39.06 ( 32.57)
Test: [ 8/25]   Time  2.807 ( 3.639)    Loss 4.2406e+00 (4.4609e+00)    Acc@1  17.19 ( 13.87)   Acc@5  39.45 ( 33.64)
Test: [ 9/25]   Time  2.836 ( 3.243)    Loss 4.8076e+00 (4.5831e+00)    Acc@1   8.98 ( 12.89)   Acc@5  28.91 ( 31.64)
Test: [ 8/25]   Time  4.840 ( 3.649)    Loss 4.3024e+00 (4.4910e+00)    Acc@1  21.09 ( 14.26)   Acc@5  39.06 ( 32.03)
Test: [ 9/25]   Time  2.545 ( 3.271)    Loss 4.7827e+00 (4.5546e+00)    Acc@1  12.89 ( 13.59)   Acc@5  30.08 ( 33.07)
Test: [ 9/25]   Time  2.632 ( 3.295)    Loss 4.7684e+00 (4.5578e+00)    Acc@1   9.77 ( 13.37)   Acc@5  26.95 ( 32.20)
Test: [ 8/25]   Time  2.386 ( 3.733)    Loss 4.3798e+00 (4.4945e+00)    Acc@1  17.19 ( 13.48)   Acc@5  35.16 ( 31.64)
Test: [ 9/25]   Time  2.492 ( 3.388)    Loss 4.6824e+00 (4.5306e+00)    Acc@1  14.84 ( 14.02)   Acc@5  29.30 ( 32.20)
Test: [ 9/25]   Time  2.676 ( 3.532)    Loss 4.8735e+00 (4.5068e+00)    Acc@1  10.94 ( 13.54)   Acc@5  26.56 ( 32.86)
Test: [ 9/25]   Time  2.609 ( 3.533)    Loss 4.9078e+00 (4.5373e+00)    Acc@1   8.20 ( 13.59)   Acc@5  25.39 ( 31.29)
Test: [10/25]   Time  2.710 ( 3.215)    Loss 4.0249e+00 (4.5016e+00)    Acc@1  21.48 ( 14.38)   Acc@5  42.97 ( 34.06)
Test: [ 9/25]   Time  2.574 ( 3.604)    Loss 4.7729e+00 (4.5254e+00)    Acc@1  10.55 ( 13.15)   Acc@5  28.52 ( 31.29)
Test: [10/25]   Time  2.787 ( 3.244)    Loss 4.0289e+00 (4.5049e+00)    Acc@1  23.05 ( 14.34)   Acc@5  45.31 ( 33.52)
Test: [10/25]   Time  2.828 ( 3.332)    Loss 3.9568e+00 (4.4732e+00)    Acc@1  18.75 ( 14.49)   Acc@5  42.58 ( 33.24)
Test: [ 8/25]   Time  6.387 ( 4.181)    Loss 4.1686e+00 (4.4480e+00)    Acc@1  22.66 ( 15.33)   Acc@5  41.80 ( 33.98)
Test: [10/25]   Time  2.692 ( 3.449)    Loss 4.0354e+00 (4.4871e+00)    Acc@1  21.88 ( 14.41)   Acc@5  39.84 ( 32.15)
Test: [10/25]   Time  5.750 ( 3.494)    Loss 4.1097e+00 (4.5358e+00)    Acc@1  17.19 ( 13.32)   Acc@5  39.84 ( 32.46)
Test: [11/25]   Time  2.865 ( 3.183)    Loss 4.9359e+00 (4.5411e+00)    Acc@1   8.59 ( 13.85)   Acc@5  27.34 ( 33.45)
Test: [11/25]   Time  2.681 ( 3.193)    Loss 4.8317e+00 (4.5346e+00)    Acc@1   7.03 ( 13.67)   Acc@5  24.22 ( 32.67)
Test: [11/25]   Time  2.790 ( 3.283)    Loss 4.7339e+00 (4.4969e+00)    Acc@1   9.38 ( 14.03)   Acc@5  26.95 ( 32.67)
Test: [10/25]   Time  3.844 ( 3.628)    Loss 4.3394e+00 (4.5068e+00)    Acc@1  17.58 ( 13.59)   Acc@5  38.67 ( 32.03)
Test: [11/25]   Time  2.608 ( 3.373)    Loss 4.7666e+00 (4.5125e+00)    Acc@1   8.98 ( 13.92)   Acc@5  30.47 ( 32.00)
Test: [ 9/25]   Time  4.023 ( 4.164)    Loss 4.7927e+00 (4.4863e+00)    Acc@1  10.94 ( 14.84)   Acc@5  25.39 ( 33.03)
Test: [10/25]   Time  6.074 ( 3.786)    Loss 3.9423e+00 (4.4503e+00)    Acc@1  19.14 ( 14.10)   Acc@5  38.67 ( 33.44)
Test: [12/25]   Time  2.869 ( 3.166)    Loss 4.7111e+00 (4.5493e+00)    Acc@1  13.28 ( 13.64)   Acc@5  30.47 ( 32.49)
Test: [12/25]   Time  3.000 ( 3.168)    Loss 4.4832e+00 (4.5363e+00)    Acc@1  14.84 ( 13.93)   Acc@5  32.42 ( 33.37)
Test: [11/25]   Time  2.517 ( 3.527)    Loss 4.9363e+00 (4.5459e+00)    Acc@1   7.42 ( 13.03)   Acc@5  20.31 ( 30.97)
Test: [11/25]   Time  4.094 ( 3.548)    Loss 4.8684e+00 (4.5660e+00)    Acc@1  10.16 ( 13.03)   Acc@5  26.17 ( 31.89)
Test: [12/25]   Time  3.653 ( 3.314)    Loss 4.2980e+00 (4.4804e+00)    Acc@1  15.62 ( 14.16)   Acc@5  35.55 ( 32.91)
Test: [12/25]   Time  2.700 ( 3.316)    Loss 4.5083e+00 (4.5122e+00)    Acc@1  17.19 ( 14.19)   Acc@5  33.59 ( 32.13)
Test: [11/25]   Time  2.335 ( 3.654)    Loss 4.6767e+00 (4.4709e+00)    Acc@1   9.77 ( 13.71)   Acc@5  26.95 ( 32.85)
Test: [13/25]   Time  2.573 ( 3.122)    Loss 5.0317e+00 (4.5744e+00)    Acc@1   7.42 ( 13.43)   Acc@5  23.05 ( 32.57)
Test: [13/25]   Time  2.627 ( 3.125)    Loss 4.8960e+00 (4.5760e+00)    Acc@1   8.20 ( 13.22)   Acc@5  21.88 ( 31.67)
Test: [12/25]   Time  3.069 ( 3.489)    Loss 4.3842e+00 (4.5324e+00)    Acc@1  13.67 ( 13.09)   Acc@5  32.81 ( 31.12)
Test: [12/25]   Time  2.934 ( 3.497)    Loss 4.6575e+00 (4.5736e+00)    Acc@1  14.84 ( 13.18)   Acc@5  30.08 ( 31.74)
Test: [10/25]   Time  4.872 ( 4.234)    Loss 4.0457e+00 (4.4422e+00)    Acc@1  20.70 ( 15.43)   Acc@5  41.80 ( 33.91)
Test: [13/25]   Time  2.611 ( 3.262)    Loss 5.1442e+00 (4.5608e+00)    Acc@1   7.81 ( 13.70)   Acc@5  21.48 ( 31.31)
Test: [13/25]   Time  2.720 ( 3.268)    Loss 5.0624e+00 (4.5251e+00)    Acc@1   7.42 ( 13.64)   Acc@5  21.09 ( 32.00)
Test: [14/25]   Time  2.706 ( 3.092)    Loss 4.5627e+00 (4.5735e+00)    Acc@1  14.84 ( 13.53)   Acc@5  32.42 ( 32.56)
Test: [14/25]   Time  2.848 ( 3.105)    Loss 4.7091e+00 (4.5855e+00)    Acc@1  12.89 ( 13.20)   Acc@5  31.64 ( 31.67)
Test: [13/25]   Time  2.425 ( 3.407)    Loss 5.0764e+00 (4.5742e+00)    Acc@1   7.03 ( 12.62)   Acc@5  20.70 ( 30.32)
Test: [13/25]   Time  2.424 ( 3.415)    Loss 5.1287e+00 (4.6163e+00)    Acc@1   8.20 ( 12.80)   Acc@5  17.19 ( 30.62)
Test: [11/25]   Time  2.694 ( 4.094)    Loss 4.9414e+00 (4.4876e+00)    Acc@1   8.98 ( 14.84)   Acc@5  24.22 ( 33.03)
Test: [14/25]   Time  2.661 ( 3.219)    Loss 4.7166e+00 (4.5719e+00)    Acc@1  12.11 ( 13.59)   Acc@5  29.30 ( 31.17)
Test: [14/25]   Time  2.640 ( 3.223)    Loss 4.4980e+00 (4.5232e+00)    Acc@1  16.41 ( 13.84)   Acc@5  31.25 ( 31.95)
Test: [15/25]   Time  2.931 ( 3.082)    Loss 4.7343e+00 (4.5842e+00)    Acc@1  13.28 ( 13.52)   Acc@5  26.95 ( 32.19)
Test: [14/25]   Time  2.537 ( 3.352)    Loss 4.6828e+00 (4.6211e+00)    Acc@1  13.28 ( 12.83)   Acc@5  30.47 ( 30.61)
Test: [14/25]   Time  2.663 ( 3.354)    Loss 4.5089e+00 (4.5696e+00)    Acc@1  12.89 ( 12.64)   Acc@5  30.86 ( 30.36)
Test: [12/25]   Time  7.025 ( 3.935)    Loss 4.6633e+00 (4.4869e+00)    Acc@1  13.67 ( 13.70)   Acc@5  30.86 ( 32.68)
Test: [12/25]   Time  2.608 ( 3.971)    Loss 4.6276e+00 (4.4993e+00)    Acc@1  14.45 ( 14.81)   Acc@5  32.81 ( 33.01)
Test: [15/25]   Time  2.587 ( 3.177)    Loss 4.8514e+00 (4.5906e+00)    Acc@1  10.16 ( 13.36)   Acc@5  23.05 ( 30.63)
Test: [15/25]   Time  2.619 ( 3.183)    Loss 4.5763e+00 (4.5267e+00)    Acc@1  11.33 ( 13.67)   Acc@5  31.64 ( 31.93)
Test: [16/25]   Time  2.533 ( 3.047)    Loss 4.4726e+00 (4.5773e+00)    Acc@1  13.67 ( 13.53)   Acc@5  29.69 ( 32.03)
Test: [15/25]   Time  2.483 ( 3.294)    Loss 4.7859e+00 (4.6321e+00)    Acc@1  10.55 ( 12.68)   Acc@5  25.78 ( 30.29)
Test: [15/25]   Time  2.594 ( 3.303)    Loss 4.8230e+00 (4.5865e+00)    Acc@1  11.33 ( 12.55)   Acc@5  26.56 ( 30.10)
Test: [16/25]   Time  2.444 ( 3.131)    Loss 4.6994e+00 (4.5974e+00)    Acc@1  10.55 ( 13.18)   Acc@5  32.42 ( 30.74)
Test: [16/25]   Time  2.433 ( 3.136)    Loss 4.6096e+00 (4.5319e+00)    Acc@1  12.50 ( 13.60)   Acc@5  32.81 ( 31.98)
Test: [13/25]   Time  2.623 ( 3.867)    Loss 5.0181e+00 (4.5392e+00)    Acc@1   7.81 ( 14.27)   Acc@5  21.48 ( 32.12)
Test: [13/25]   Time  3.074 ( 3.869)    Loss 4.9094e+00 (4.5194e+00)    Acc@1  10.16 ( 13.43)   Acc@5  21.88 ( 31.85)
Test: [15/25]   Time  7.398 ( 3.391)    Loss 4.7329e+00 (4.5953e+00)    Acc@1  10.55 ( 13.02)   Acc@5  28.12 ( 31.43)
Test: [17/25]   Time  2.759 ( 3.030)    Loss 4.6868e+00 (4.5837e+00)    Acc@1  10.16 ( 13.33)   Acc@5  27.73 ( 31.78)
Test: [16/25]   Time  2.361 ( 3.245)    Loss 4.4335e+00 (4.5769e+00)    Acc@1  16.02 ( 12.77)   Acc@5  36.72 ( 30.52)
Test: [17/25]   Time  2.538 ( 3.101)    Loss 4.6088e+00 (4.5364e+00)    Acc@1  10.94 ( 13.44)   Acc@5  31.64 ( 31.96)
Test: [14/25]   Time  2.847 ( 3.796)    Loss 4.5900e+00 (4.5245e+00)    Acc@1  14.06 ( 13.48)   Acc@5  30.47 ( 31.75)
Test: [14/25]   Time  2.912 ( 3.799)    Loss 4.4699e+00 (4.5342e+00)    Acc@1  14.45 ( 14.29)   Acc@5  34.77 ( 32.31)
Test: [16/25]   Time  4.241 ( 3.353)    Loss 4.6688e+00 (4.6344e+00)    Acc@1  13.67 ( 12.74)   Acc@5  31.25 ( 30.35)
Test: [18/25]   Time  2.890 ( 3.023)    Loss 4.5093e+00 (4.5796e+00)    Acc@1  11.33 ( 13.22)   Acc@5  32.03 ( 31.79)
Test: [17/25]   Time  2.561 ( 3.204)    Loss 4.7007e+00 (4.5842e+00)    Acc@1  11.33 ( 12.68)   Acc@5  28.91 ( 30.42)
Test: [17/25]   Time  4.433 ( 3.208)    Loss 4.5780e+00 (4.5962e+00)    Acc@1  15.23 ( 13.30)   Acc@5  34.38 ( 30.95)
Test: [16/25]   Time  4.310 ( 3.449)    Loss 4.5052e+00 (4.5897e+00)    Acc@1  13.67 ( 13.06)   Acc@5  31.64 ( 31.45)
Test: [18/25]   Time  2.700 ( 3.079)    Loss 4.6175e+00 (4.5409e+00)    Acc@1  12.89 ( 13.41)   Acc@5  33.98 ( 32.07)
Test: [15/25]   Time  3.033 ( 3.745)    Loss 4.8122e+00 (4.5437e+00)    Acc@1  13.67 ( 13.49)   Acc@5  27.34 ( 31.46)
Test: [17/25]   Time  2.563 ( 3.307)    Loss 4.5421e+00 (4.6289e+00)    Acc@1  10.16 ( 12.59)   Acc@5  26.56 ( 30.12)
Test: [18/25]   Time  2.612 ( 3.171)    Loss 4.5539e+00 (4.5825e+00)    Acc@1  14.45 ( 12.78)   Acc@5  35.94 ( 30.73)
Test: [19/25]   Time  2.831 ( 3.013)    Loss 4.8314e+00 (4.5928e+00)    Acc@1  11.72 ( 13.14)   Acc@5  28.12 ( 31.60)
Test: [18/25]   Time  2.962 ( 3.194)    Loss 4.5261e+00 (4.5923e+00)    Acc@1  12.89 ( 13.28)   Acc@5  33.98 ( 31.12)
Test: [17/25]   Time  2.546 ( 3.396)    Loss 4.6470e+00 (4.5930e+00)    Acc@1  12.11 ( 13.01)   Acc@5  30.47 ( 31.39)
Test: [19/25]   Time  2.837 ( 3.066)    Loss 4.8992e+00 (4.5598e+00)    Acc@1   9.38 ( 13.20)   Acc@5  25.39 ( 31.72)
Test: [15/25]   Time  5.232 ( 3.894)    Loss 4.5020e+00 (4.5321e+00)    Acc@1  14.45 ( 14.30)   Acc@5  34.38 ( 32.45)
Test: [18/25]   Time  2.449 ( 3.259)    Loss 4.7222e+00 (4.6341e+00)    Acc@1  10.94 ( 12.50)   Acc@5  30.47 ( 30.14)
Test: [16/25]   Time  2.817 ( 3.687)    Loss 4.3837e+00 (4.5337e+00)    Acc@1  15.62 ( 13.62)   Acc@5  36.33 ( 31.76)
Test: [19/25]   Time  2.420 ( 3.132)    Loss 4.8809e+00 (4.5982e+00)    Acc@1  12.11 ( 12.75)   Acc@5  24.61 ( 30.41)
Test: [19/25]   Time  2.402 ( 3.153)    Loss 4.8065e+00 (4.6036e+00)    Acc@1  13.67 ( 13.30)   Acc@5  28.91 ( 31.00)
Test: [20/25]   Time  2.668 ( 2.995)    Loss 4.4010e+00 (4.5832e+00)    Acc@1  17.97 ( 13.38)   Acc@5  34.38 ( 31.74)
Test: [18/25]   Time  2.856 ( 3.366)    Loss 4.6275e+00 (4.5950e+00)    Acc@1  13.67 ( 13.04)   Acc@5  33.59 ( 31.51)
Test: [16/25]   Time  2.637 ( 3.816)    Loss 4.6459e+00 (4.5392e+00)    Acc@1  12.11 ( 14.16)   Acc@5  29.69 ( 32.28)
Test: [19/25]   Time  2.558 ( 3.222)    Loss 4.8856e+00 (4.6473e+00)    Acc@1  10.16 ( 12.38)   Acc@5  28.12 ( 30.04)
Test: [17/25]   Time  3.007 ( 3.647)    Loss 4.5673e+00 (4.5356e+00)    Acc@1  13.28 ( 13.60)   Acc@5  33.98 ( 31.89)
Test: [20/25]   Time  3.823 ( 3.104)    Loss 4.6678e+00 (4.5652e+00)    Acc@1  12.50 ( 13.16)   Acc@5  27.73 ( 31.52)
Test: [21/25]   Time  2.944 ( 2.993)    Loss 4.7276e+00 (4.5901e+00)    Acc@1  11.33 ( 13.28)   Acc@5  26.56 ( 31.49)
Test: [19/25]   Time  2.646 ( 3.328)    Loss 5.0021e+00 (4.6164e+00)    Acc@1   9.77 ( 12.87)   Acc@5  21.09 ( 30.96)
Test: [20/25]   Time  2.629 ( 3.192)    Loss 4.5478e+00 (4.6424e+00)    Acc@1  15.62 ( 12.54)   Acc@5  35.55 ( 30.31)
Test: [20/25]   Time  4.028 ( 3.196)    Loss 4.6694e+00 (4.6069e+00)    Acc@1  14.84 ( 13.38)   Acc@5  33.98 ( 31.15)
Test: [21/25]   Time  2.577 ( 3.079)    Loss 4.7773e+00 (4.5753e+00)    Acc@1   8.98 ( 12.97)   Acc@5  25.00 ( 31.21)
Test: [20/25]   Time  5.339 ( 3.242)    Loss 4.7487e+00 (4.6057e+00)    Acc@1  12.50 ( 12.73)   Acc@5  26.17 ( 30.20)
Test: [18/25]   Time  2.941 ( 3.608)    Loss 4.6276e+00 (4.5408e+00)    Acc@1  15.23 ( 13.69)   Acc@5  30.86 ( 31.84)
Test: [22/25]   Time  2.918 ( 2.990)    Loss 4.5833e+00 (4.5898e+00)    Acc@1  12.50 ( 13.25)   Acc@5  30.47 ( 31.45)
Test: [20/25]   Time  2.617 ( 3.292)    Loss 4.4709e+00 (4.6091e+00)    Acc@1  14.84 ( 12.97)   Acc@5  31.25 ( 30.98)
Test: [17/25]   Time  4.927 ( 3.881)    Loss 4.4670e+00 (4.5349e+00)    Acc@1  14.45 ( 14.18)   Acc@5  35.94 ( 32.49)
Test: [21/25]   Time  2.588 ( 3.164)    Loss 4.6519e+00 (4.6428e+00)    Acc@1  12.11 ( 12.52)   Acc@5  26.17 ( 30.12)
Test: [21/25]   Time  2.582 ( 3.167)    Loss 4.7848e+00 (4.6154e+00)    Acc@1   9.38 ( 13.19)   Acc@5  25.00 ( 30.86)
Test: [21/25]   Time  2.399 ( 3.202)    Loss 4.8602e+00 (4.6179e+00)    Acc@1  10.16 ( 12.61)   Acc@5  21.88 ( 29.80)
Test: [19/25]   Time  2.679 ( 3.559)    Loss 4.7599e+00 (4.5523e+00)    Acc@1  10.16 ( 13.51)   Acc@5  30.08 ( 31.74)
Test: [22/25]   Time  3.293 ( 3.088)    Loss 4.5035e+00 (4.5720e+00)    Acc@1  15.23 ( 13.07)   Acc@5  31.64 ( 31.23)
Test: [21/25]   Time  2.522 ( 3.255)    Loss 4.8662e+00 (4.6214e+00)    Acc@1  12.11 ( 12.93)   Acc@5  24.61 ( 30.67)
Test: [23/25]   Time  2.782 ( 2.980)    Loss 4.1212e+00 (4.5694e+00)    Acc@1  15.62 ( 13.35)   Acc@5  40.62 ( 31.84)
Test: [18/25]   Time  2.857 ( 3.824)    Loss 4.8566e+00 (4.5528e+00)    Acc@1   9.77 ( 13.93)   Acc@5  28.12 ( 32.25)
Test: [22/25]   Time  2.435 ( 3.134)    Loss 4.4594e+00 (4.6083e+00)    Acc@1  12.50 ( 13.16)   Acc@5  31.64 ( 30.89)
Test: [22/25]   Time  2.534 ( 3.135)    Loss 4.6419e+00 (4.6428e+00)    Acc@1  14.06 ( 12.59)   Acc@5  31.25 ( 30.17)
Test: [22/25]   Time  2.348 ( 3.163)    Loss 4.4723e+00 (4.6112e+00)    Acc@1  12.89 ( 12.62)   Acc@5  31.64 ( 29.88)
Test: [23/25]   Time  2.617 ( 3.068)    Loss 4.1614e+00 (4.5542e+00)    Acc@1  16.41 ( 13.21)   Acc@5  42.97 ( 31.74)
Test: [20/25]   Time  2.957 ( 3.529)    Loss 4.8711e+00 (4.5682e+00)    Acc@1  10.16 ( 13.34)   Acc@5  26.56 ( 31.48)
Test: [22/25]   Time  2.657 ( 3.228)    Loss 4.5897e+00 (4.6199e+00)    Acc@1  12.50 ( 12.91)   Acc@5  33.59 ( 30.81)
Test: [24/25]   Time  2.711 ( 2.969)    Loss 4.0543e+00 (4.5480e+00)    Acc@1  16.80 ( 13.49)   Acc@5  45.31 ( 32.41)
Test: [19/25]   Time  2.642 ( 3.762)    Loss 4.9300e+00 (4.5727e+00)    Acc@1   7.42 ( 13.59)   Acc@5  25.00 ( 31.87)
Test: [23/25]   Time  2.643 ( 3.113)    Loss 3.9713e+00 (4.5806e+00)    Acc@1  22.66 ( 13.57)   Acc@5  41.02 ( 31.33)
Test: [23/25]   Time  2.718 ( 3.117)    Loss 4.2684e+00 (4.6265e+00)    Acc@1  15.23 ( 12.70)   Acc@5  37.50 ( 30.49)
Test: [25/25]   Time  1.309 ( 2.903)    Loss 4.2187e+00 (4.5424e+00)    Acc@1  25.47 ( 13.70)   Acc@5  40.57 ( 32.54)
Test: [21/25]   Time  2.614 ( 3.485)    Loss 4.9181e+00 (4.5849e+00)    Acc@1   9.77 ( 13.17)   Acc@5  21.09 ( 30.99)
Test: [24/25]   Time  2.925 ( 3.062)    Loss 3.9887e+00 (4.5306e+00)    Acc@1  17.19 ( 13.38)   Acc@5  43.36 ( 32.23)
Test: [23/25]   Time  2.569 ( 3.200)    Loss 4.3202e+00 (4.6069e+00)    Acc@1  17.19 ( 13.09)   Acc@5  36.72 ( 31.06)
Test: [20/25]   Time  2.794 ( 3.713)    Loss 4.6994e+00 (4.5790e+00)    Acc@1  16.02 ( 13.71)   Acc@5  29.69 ( 31.76)
Test: [24/25]   Time  2.693 ( 3.095)    Loss 4.3148e+00 (4.5695e+00)    Acc@1  17.58 ( 13.74)   Acc@5  39.84 ( 31.69)
Test: [23/25]   Time  4.877 ( 3.238)    Loss 4.1863e+00 (4.5928e+00)    Acc@1  17.97 ( 12.86)   Acc@5  42.58 ( 30.43)
Test: [24/25]   Time  2.810 ( 3.104)    Loss 4.3860e+00 (4.6165e+00)    Acc@1  15.62 ( 12.83)   Acc@5  37.11 ( 30.76)
Test: [25/25]   Time  1.275 ( 2.990)    Loss 3.9441e+00 (4.5207e+00)    Acc@1  28.30 ( 13.63)   Acc@5  45.28 ( 32.45)
Test: [25/25]   Time  1.606 ( 3.035)    Loss 4.0897e+00 (4.5614e+00)    Acc@1  27.36 ( 13.97)   Acc@5  42.45 ( 31.87)
Test: [25/25]   Time  1.416 ( 3.037)    Loss 3.8614e+00 (4.6037e+00)    Acc@1  25.47 ( 13.04)   Acc@5  50.94 ( 31.10)
Test: [22/25]   Time  2.815 ( 3.455)    Loss 4.8152e+00 (4.5954e+00)    Acc@1  10.94 ( 13.07)   Acc@5  24.61 ( 30.70)
Test: [24/25]   Time  2.557 ( 3.173)    Loss 4.0612e+00 (4.5841e+00)    Acc@1  19.14 ( 13.35)   Acc@5  40.23 ( 31.45)
Test: [24/25]   Time  2.266 ( 3.197)    Loss 4.2685e+00 (4.5792e+00)    Acc@1  16.80 ( 13.02)   Acc@5  37.50 ( 30.73)
Test: [21/25]   Time  2.639 ( 3.662)    Loss 4.7811e+00 (4.5886e+00)    Acc@1   8.59 ( 13.47)   Acc@5  23.05 ( 31.34)
Test: [25/25]   Time  1.584 ( 3.109)    Loss 3.9789e+00 (4.5739e+00)    Acc@1  26.42 ( 13.57)   Acc@5  45.28 ( 31.68)
Test: [25/25]   Time  1.188 ( 3.117)    Loss 4.0275e+00 (4.5699e+00)    Acc@1  26.42 ( 13.25)   Acc@5  43.40 ( 30.94)
Test: [23/25]   Time  2.810 ( 3.427)    Loss 4.1487e+00 (4.5759e+00)    Acc@1  21.09 ( 13.42)   Acc@5  39.84 ( 31.10)
Test: [22/25]   Time  2.534 ( 3.611)    Loss 4.5835e+00 (4.5884e+00)    Acc@1  11.72 ( 13.39)   Acc@5  29.69 ( 31.27)
Test: [24/25]   Time  2.573 ( 3.391)    Loss 4.1963e+00 (4.5601e+00)    Acc@1  19.14 ( 13.66)   Acc@5  41.41 ( 31.53)
Test: [23/25]   Time  2.459 ( 3.561)    Loss 4.4787e+00 (4.5836e+00)    Acc@1  13.28 ( 13.38)   Acc@5  34.38 ( 31.40)
Traceback (most recent call last):
  File ""main.py"", line 488, in <module>
    main()
  File ""main.py"", line 113, in main
    mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))
  File ""/home/chris/anaconda3/envs/p1121/lib/python3.8/site-packages/torch/multiprocessing/spawn.py"", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File ""/home/chris/anaconda3/envs/p1121/lib/python3.8/site-packages/torch/multiprocessing/spawn.py"", line 198, in start_processes
    while not context.join():
  File ""/home/chris/anaconda3/envs/p1121/lib/python3.8/site-packages/torch/multiprocessing/spawn.py"", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException:

-- Process 3 terminated with the following error:
Traceback (most recent call last):
  File ""/home/chris/anaconda3/envs/p1121/lib/python3.8/site-packages/torch/multiprocessing/spawn.py"", line 69, in _wrap
    fn(i, *args)
  File ""/home/chris/codes/SupContrast/main.py"", line 265, in main_worker
    acc1 = validate(val_loader, model, criterion, args)
  File ""/home/chris/codes/SupContrast/main.py"", line 376, in validate
    top1.all_reduce()
  File ""/home/chris/codes/SupContrast/main.py"", line 425, in all_reduce
    dist.all_reduce(total, dist.ReduceOp.SUM, async_op=False)
  File ""/home/chris/anaconda3/envs/p1121/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py"", line 1320, in all_reduce
    work = default_pg.allreduce([tensor], opts)
RuntimeError: Tensors must be CUDA and dense

/home/chris/anaconda3/envs/p1121/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 22 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
```

## Output after this change to the description


```
Epoch: [0][625/626]	Time  2.428 ( 2.901)	Data  1.542 ( 1.209)	Loss 4.5665e+00 (5.4323e+00)	Acc@1  15.23 (  6.88)	Acc@5  28.91 ( 17.97)
Epoch: [0][626/626]	Time  2.694 ( 2.901)	Data  0.267 ( 1.208)	Loss 4.6086e+00 (5.4316e+00)	Acc@1   9.59 (  6.89)	Acc@5  27.40 ( 17.98)
Test: [ 1/25]	Time  6.851 ( 6.851)	Loss 4.2568e+00 (4.2568e+00)	Acc@1  18.36 ( 18.36)	Acc@5  37.89 ( 37.89)
Test: [ 2/25]	Time  0.856 ( 3.854)	Loss 5.5109e+00 (4.8838e+00)	Acc@1   7.42 ( 12.89)	Acc@5  16.02 ( 26.95)
Test: [ 3/25]	Time  2.692 ( 3.466)	Loss 4.1102e+00 (4.6260e+00)	Acc@1  23.83 ( 16.54)	Acc@5  46.09 ( 33.33)
Test: [ 4/25]	Time  3.878 ( 3.569)	Loss 3.6381e+00 (4.3790e+00)	Acc@1  28.12 ( 19.43)	Acc@5  49.61 ( 37.40)
Test: [ 5/25]	Time  3.553 ( 3.566)	Loss 4.4859e+00 (4.4004e+00)	Acc@1   8.20 ( 17.19)	Acc@5  26.17 ( 35.16)
Test: [ 6/25]	Time  2.523 ( 3.392)	Loss 4.0588e+00 (4.3435e+00)	Acc@1  12.89 ( 16.47)	Acc@5  38.67 ( 35.74)
Test: [ 7/25]	Time  4.348 ( 3.529)	Loss 4.6805e+00 (4.3916e+00)	Acc@1   8.98 ( 15.40)	Acc@5  21.48 ( 33.71)
Test: [ 8/25]	Time  2.818 ( 3.440)	Loss 4.3927e+00 (4.3917e+00)	Acc@1  16.02 ( 15.48)	Acc@5  38.28 ( 34.28)
Test: [ 9/25]	Time  3.278 ( 3.422)	Loss 4.5077e+00 (4.4046e+00)	Acc@1  11.72 ( 15.06)	Acc@5  32.81 ( 34.11)
Test: [10/25]	Time  5.985 ( 3.678)	Loss 3.8617e+00 (4.3503e+00)	Acc@1  18.75 ( 15.43)	Acc@5  42.19 ( 34.92)
Test: [11/25]	Time  2.539 ( 3.575)	Loss 4.8288e+00 (4.3938e+00)	Acc@1   8.98 ( 14.84)	Acc@5  25.39 ( 34.06)
Test: [12/25]	Time  6.923 ( 3.854)	Loss 4.7528e+00 (4.4237e+00)	Acc@1  10.94 ( 14.52)	Acc@5  28.12 ( 33.56)
Test: [13/25]	Time  2.936 ( 3.783)	Loss 5.1270e+00 (4.4778e+00)	Acc@1   7.03 ( 13.94)	Acc@5  21.09 ( 32.60)
Test: [14/25]	Time  2.809 ( 3.714)	Loss 4.7717e+00 (4.4988e+00)	Acc@1  13.28 ( 13.90)	Acc@5  27.34 ( 32.23)
Test: [15/25]	Time  2.830 ( 3.655)	Loss 5.0285e+00 (4.5341e+00)	Acc@1  10.55 ( 13.67)	Acc@5  25.00 ( 31.74)
Test: [16/25]	Time  3.047 ( 3.617)	Loss 4.6476e+00 (4.5412e+00)	Acc@1  11.33 ( 13.53)	Acc@5  34.77 ( 31.93)
Test: [17/25]	Time  2.916 ( 3.575)	Loss 4.7874e+00 (4.5557e+00)	Acc@1  11.33 ( 13.40)	Acc@5  30.86 ( 31.87)
Test: [18/25]	Time  2.886 ( 3.537)	Loss 4.9873e+00 (4.5797e+00)	Acc@1  11.72 ( 13.30)	Acc@5  25.00 ( 31.49)
Test: [19/25]	Time  2.723 ( 3.494)	Loss 4.9336e+00 (4.5983e+00)	Acc@1  12.11 ( 13.24)	Acc@5  26.95 ( 31.25)
Test: [20/25]	Time  3.005 ( 3.470)	Loss 5.1356e+00 (4.6252e+00)	Acc@1   9.38 ( 13.05)	Acc@5  24.22 ( 30.90)
Test: [21/25]	Time  2.591 ( 3.428)	Loss 5.1685e+00 (4.6511e+00)	Acc@1   9.38 ( 12.87)	Acc@5  20.31 ( 30.39)
Test: [22/25]	Time  2.740 ( 3.397)	Loss 4.8744e+00 (4.6612e+00)	Acc@1  13.28 ( 12.89)	Acc@5  27.34 ( 30.26)
Test: [23/25]	Time  2.716 ( 3.367)	Loss 4.4706e+00 (4.6529e+00)	Acc@1  19.53 ( 13.18)	Acc@5  35.16 ( 30.47)
Test: [24/25]	Time  2.654 ( 3.337)	Loss 4.3260e+00 (4.6393e+00)	Acc@1  18.36 ( 13.40)	Acc@5  41.41 ( 30.92)
Test: [25/25]	Time  1.349 ( 3.258)	Loss 3.6529e+00 (4.6226e+00)	Acc@1  30.19 ( 13.68)	Acc@5  50.94 ( 31.26)
 *   Acc@1 12.990 Acc@5 30.758
```

## Making the title more informative

$\rightarrow$ Done",distributed training python rank output change description epoch time data loss epoch time data loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss recent call last file line module main file line main file line spawn return join daemon file line file line join raise process following error recent call last file line file line validate model criterion file line validate file line total file line work tensor must dense appear semaphore clean shutdown appear output change description epoch time data loss epoch time data loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss making title informative done,issue,negative,negative,neutral,neutral,negative,negative
1217167412,"@Deeeerek No. There was a try in https://github.com/pytorch/examples/pull/203 I think it should be easier and cleaner to support fp16 nowadays with amp from pytorch: https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/ 
Please feel free to contribute if you like!",try think easier cleaner support nowadays please feel free contribute like,issue,positive,positive,positive,positive,positive,positive
1214380382,"> Hi, I met the same difficulty and I had solved it. Check the function 'dset.ImageFolder' which is used to create 'dataset'. The function's '__init__'  uses ""default_loader"" that returns ""img.convert('RGB')"". It indicates that although your image is of single channel, you will get a image of three channels after the function ""img.convert(RGB)"". And the way to solve the problem is that you could use ""img.convert('L)"". A better way I suggests is rewrite the function ""ImageFolder"" on a new python file.

",hi met difficulty check function used create function although image single channel get image three function way solve problem could use better way rewrite function new python file,issue,negative,positive,positive,positive,positive,positive
1211421264,"It seems  the current update https://github.com/pytorch/examples/blob/main/imagenet/main.py#L424 :(
falls back to the wrong version.....

I was trying to fix this bug and fortunately found this. It would be great it's updated 
total = torch.FloatTensor([self.sum, self.count]).cuda(self.sum.device)",current update back wrong version trying fix bug fortunately found would great total,issue,negative,positive,positive,positive,positive,positive
1210101871,No worries! Please ask in pytorch-lightning or pytorch repo if needed. I will close this issue then.,please ask close issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1210018219,"Sorry I didn't realize I was in the examples repo. Yes I understand that most people use the default argument, but silently failing is a major issue which is hard to debug. ",sorry realize yes understand people use default argument silently failing major issue hard,issue,negative,negative,negative,negative,negative,negative
1209938261,"Hi @dfarhi , I'm not able to reproduce it with torch==1.12.1+cu102 in Ubuntu 22.04 LTS. Is it still reproducible on your side? ",hi able reproduce still reproducible side,issue,negative,positive,positive,positive,positive,positive
1209929119,"Hi @tonydavis629 , the issue seems unrelated to this repo. Do you encounter the issue with any example in this repo? 

BTW, `strict` is true by default for `module.load_state_dict()` in PyTorch. Usually, we don't have set it. i.e. `model.load_state_dict(state_dict)` is most people use.
https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict",hi issue unrelated encounter issue example strict true default usually set people use,issue,negative,positive,neutral,neutral,positive,positive
1209909473,"I think the print for `--gpu` is useful. To reduce the confusion, we can add some clarification to the print message. Alternative: as DDP is recommended to replace DataParallel, we should probably remove the usage of DataParallel in examples and update the print message accordingly.",think print useful reduce confusion add clarification print message alternative replace probably remove usage update print message accordingly,issue,negative,positive,positive,positive,positive,positive
1209848793,"@mattsta Thanks for your contribution! To move forward, would you mind signing the CLA following the instruction above?",thanks contribution move forward would mind following instruction,issue,negative,positive,neutral,neutral,positive,positive
1198659719,@gagandeep987123 Please feel free to reopen this issue if it's not resolved by https://github.com/pytorch/examples/pull/1025,please feel free reopen issue resolved,issue,positive,positive,positive,positive,positive,positive
1197575481,"Maybe you use the torch.load() without 'map_location=lambda storage, loc: storage'. The original checkpoint saved the tensor on different GPUs, then the torch.load() will also create another process to map the GPU.",maybe use without storage storage original saved tensor different also create another process map,issue,negative,positive,positive,positive,positive,positive
1194010167,"> So far our tests aren't in a place where we can guarantee some model performance, the case could be made that maybe we should? But so far we don't have any such plans

I came across TorchDrift https://torchdrift.org/   
(It is found under [PyTorch ecosystem](https://pytorch.org/ecosystem/))

It sounds like a tool that can help ensure our models accuracy specs",far place guarantee model performance case could made maybe far came across found ecosystem like tool help ensure accuracy spec,issue,positive,positive,neutral,neutral,positive,positive
1188115396,"@gagandeep987123 thanks for reporting the issue! It seems to be deadlock caused by ""fork"" start method(default in Linux). In Mac, the default is ""spawn"" so it works fine. You could try to add ""mp.set_start_method(""spawn"")"" before any process starts.",thanks issue deadlock fork start method default mac default spawn work fine could try add spawn process,issue,negative,positive,positive,positive,positive,positive
1186242982,"This is too long for anyone to spend time helping you, I'd suggest you produce a much shorter example and if not possible play with changing the input shapes to what you're looking for and shape all the shape mismatch errors you see after that. Also it may be best to ask for support on https://discuss.pytorch.org/",long anyone spend time helping suggest produce much shorter example possible play input looking shape shape mismatch see also may best ask support,issue,positive,positive,positive,positive,positive,positive
1179791300,"So far our tests aren't in a place where we can guarantee some model performance, the case could be made that maybe we should? But so far we don't have any such plans",far place guarantee model performance case could made maybe far,issue,negative,positive,neutral,neutral,positive,positive
1179208398,"Issue seems to be that `classroom` dataset download is timing out https://github.com/pytorch/examples/runs/7245163532?check_suite_focus=true#step:5:414 - taking a look

EDIT: Failure seems to be a flake - tests run just fine here https://github.com/pytorch/examples/pull/1021

Will monitor and close this issue tomorrow if no new failure happens",issue classroom timing taking look edit failure flake run fine monitor close issue tomorrow new failure,issue,negative,negative,neutral,neutral,negative,negative
1175790808,Hmm I'd be curious how many of the existing examples break with this change. I don't have an M1 and can't easily integrate into Github actions - do you mind running `run_python_examples.sh` locally and pasting the logs here,curious many break change ca easily integrate mind running locally pasting,issue,positive,positive,positive,positive,positive,positive
1175726022,"SAME ERROR. 
nobody can solve this problem! ",error nobody solve problem,issue,negative,neutral,neutral,neutral,neutral,neutral
1172492885,Hi @mesllo typically to avoid the need to ssh and copy over data to each node it's worth using a job scheduler. A good option is torchx which has builtin support for slurm https://github.com/pytorch/torchx - cc @d4l3k - there are of course other options in open source so feel free to use whatever you feel is most convenient,hi typically avoid need copy data node worth job good option support course open source feel free use whatever feel convenient,issue,positive,positive,positive,positive,positive,positive
1162193693,"Shouldn't the batch_size be divided by the world_size rather than GPUs per node? Eg. if we have 2 nodes with 4 GPUs each, shouldn't we divide batch_size by 8 rather than by 4?",divided rather per node divide rather,issue,negative,neutral,neutral,neutral,neutral,neutral
1161730716,"> Q: * linting changes in which case could you specify how you linted?
A: use  autopep8 to lint the python code and max-line-length is 120
> * `[INFO]` before some print statements, not sure if this is consistent with the rest of the repo
A: if this is not acceptable this time I could remove the change
> * Because imagenet is hard to download you've added support for a `FakeData` class which is a great change, Some new documentation where you use `--data` instead of `--dummy`?
A: original  `data` is a position arg, we could NOT set it when using `--dummy`, so change `data` to an optional arg
> * You expanded support for new architectures dramatically, did you test them all?
A: yep, `--dummy` was performed with x86 CPU
> 
> I'd appreciate reading a PR description to better understand what this change is about and document in the long term the benefits to users. I'd also recommend just making the change to support `--dummy` data first and then we can do another PR where we integrate a Github Action-based linter

",case could specify use lint python code print sure consistent rest acceptable time could remove change hard added support class great change new documentation use data instead dummy original data position could set dummy change data optional expanded support new dramatically test yep dummy appreciate reading description better understand change document long term also recommend making change support dummy data first another integrate linter,issue,positive,positive,positive,positive,positive,positive
1160136295,"OK, it turned out to be a RAM frequency issue, my ram was set as 2133mhz by default, and this seemingly would result in crashes, and a slew of other issues, when especially under heavy load, it would not show itself otherwise, and no amount of stress testing/etc would reveal the issue. setting it to 3000mhz/3200mhz fixed my issues thankfully.   
This might be an issue for 12th gen cpu/mobo that use DDR4 memories, so if you have one and experience similar issue, definitely make sure to check your ram frequencies. I",turned ram frequency issue ram set default seemingly would result slew especially heavy load would show otherwise amount stress would reveal issue setting fixed thankfully might issue th gen use one experience similar issue definitely make sure check ram,issue,positive,positive,neutral,neutral,positive,positive
1157175525,@msaroufim could u help assign some people help on code review?,could help assign people help code review,issue,positive,neutral,neutral,neutral,neutral,neutral
1157088817,"Hi, @msaroufim. Currently, tensor parallel is only supported on CUDA because we need to use NCCL. So we cannot have a CPU version. I think down the road, PyTorch Distributed and other PyTorch components will have more GPU based library or features enabled. So it's worth considering to have a GPU runner.",hi currently tensor parallel need use version think road distributed based library worth considering runner,issue,negative,positive,neutral,neutral,positive,positive
1157022180,"Hi @fduwjj this repo doesn't have GPU runners right now, is it possible to have a test run for a single batch on CPU just as a sanity check? https://github.com/pytorch/examples/runs/6908535986?check_suite_focus=true#step:5:627

Otherwise I think we can consider adding GPU runners for examples but this would be something new - maybe @svekars or @seemethere have some thoughts here",hi right possible test run single batch sanity check otherwise think consider would something new maybe,issue,negative,positive,neutral,neutral,positive,positive
1156968403,"@msaroufim Hi, I have a PR here: https://github.com/pytorch/examples/pull/1014. Can you kindly take a look? One caveat is that these tests need to be run with 8 GPUs. Not sure if this is something practical. Thanks!",hi kindly take look one caveat need run sure something practical thanks,issue,positive,positive,positive,positive,positive,positive
1156307254,"You can try ImageNet training example [[imagenet.py](https://github.com/BIGBALLON/distribuuuu/blob/master/tutorial/imagenet.py)]

### More

Please check [tutorial](https://github.com/BIGBALLON/distribuuuu/blob/master/tutorial) for detailed Distributed Training tutorials:

- Single Node Single GPU Card Training [[snsc.py](https://github.com/BIGBALLON/distribuuuu/blob/master/tutorial/snsc.py)]
- Single Node Multi-GPU Cards Training (with DataParallel) [[snmc_dp.py](https://github.com/BIGBALLON/distribuuuu/blob/master/tutorial/snmc_dp.py)]
- Multiple Nodes Multi-GPU Cards Training (with DistributedDataParallel)
    - torch.distributed.launch [[mnmc_ddp_launch.py](https://github.com/BIGBALLON/distribuuuu/blob/master/tutorial/mnmc_ddp_launch.py)]
    - torch.multiprocessing [[mnmc_ddp_mp.py](https://github.com/BIGBALLON/distribuuuu/blob/master/tutorial/mnmc_ddp_mp.py)]
- Slurm Workload Manager [[mnmc_ddp_slurm.py](https://github.com/BIGBALLON/distribuuuu/blob/master/tutorial/mnmc_ddp_slurm.py)]
- ImageNet training example [[imagenet.py](https://github.com/BIGBALLON/distribuuuu/blob/master/tutorial/imagenet.py)]



### Core function

``` python
def setup_distributed(backend=""nccl"", port=None):
    """"""Initialize distributed training environment.
    support both slurm and torch.distributed.launch
    see torch.distributed.init_process_group() for more details
    """"""
    num_gpus = torch.cuda.device_count()

    if ""SLURM_JOB_ID"" in os.environ:
        rank = int(os.environ[""SLURM_PROCID""])
        world_size = int(os.environ[""SLURM_NTASKS""])
        node_list = os.environ[""SLURM_NODELIST""]
        addr = subprocess.getoutput(f""scontrol show hostname {node_list} | head -n1"")
        # specify master port
        if port is not None:
            os.environ[""MASTER_PORT""] = str(port)
        elif ""MASTER_PORT"" not in os.environ:
            os.environ[""MASTER_PORT""] = ""29566""
        if ""MASTER_ADDR"" not in os.environ:
            os.environ[""MASTER_ADDR""] = addr
        os.environ[""WORLD_SIZE""] = str(world_size)
        os.environ[""LOCAL_RANK""] = str(rank % num_gpus)
        os.environ[""RANK""] = str(rank)
    else:
        rank = int(os.environ[""RANK""])
        world_size = int(os.environ[""WORLD_SIZE""])

    torch.cuda.set_device(rank % num_gpus)

    dist.init_process_group(
        backend=backend,
        world_size=world_size,
        rank=rank,
    )
```",try training example please check tutorial detailed distributed training single node single card training single node training multiple training manager training example core function python initialize distributed training environment support see rank show head specify master port port none port rank rank rank else rank rank rank,issue,positive,negative,negative,negative,negative,negative
1156300314,"Hi, @mesllo, you can check https://github.com/BIGBALLON/distribuuuu for both ``launch`` and ``Slurm`` usages.

Please check [tutorial](https://github.com/BIGBALLON/distribuuuu/blob/master/tutorial) for detailed Distributed Training tutorials:

- Single Node Single GPU Card Training [[snsc.py](https://github.com/BIGBALLON/distribuuuu/blob/master/tutorial/snsc.py)]
- Single Node Multi-GPU Cards Training (with DataParallel) [[snmc_dp.py](https://github.com/BIGBALLON/distribuuuu/blob/master/tutorial/snmc_dp.py)]
- Multiple Nodes Multi-GPU Cards Training (with DistributedDataParallel)
    - torch.distributed.launch [[mnmc_ddp_launch.py](https://github.com/BIGBALLON/distribuuuu/blob/master/tutorial/mnmc_ddp_launch.py)]
    - torch.multiprocessing [[mnmc_ddp_mp.py](https://github.com/BIGBALLON/distribuuuu/blob/master/tutorial/mnmc_ddp_mp.py)]
- Slurm Workload Manager [[mnmc_ddp_slurm.py](https://github.com/BIGBALLON/distribuuuu/blob/master/tutorial/mnmc_ddp_slurm.py)]
- ImageNet training example [[imagenet.py](https://github.com/BIGBALLON/distribuuuu/blob/master/tutorial/imagenet.py)]



### core function

``` python
def setup_distributed(backend=""nccl"", port=None):
    """"""Initialize distributed training environment.
    support both slurm and torch.distributed.launch
    see torch.distributed.init_process_group() for more details
    """"""
    num_gpus = torch.cuda.device_count()

    if ""SLURM_JOB_ID"" in os.environ:
        rank = int(os.environ[""SLURM_PROCID""])
        world_size = int(os.environ[""SLURM_NTASKS""])
        node_list = os.environ[""SLURM_NODELIST""]
        addr = subprocess.getoutput(f""scontrol show hostname {node_list} | head -n1"")
        # specify master port
        if port is not None:
            os.environ[""MASTER_PORT""] = str(port)
        elif ""MASTER_PORT"" not in os.environ:
            os.environ[""MASTER_PORT""] = ""29566""
        if ""MASTER_ADDR"" not in os.environ:
            os.environ[""MASTER_ADDR""] = addr
        os.environ[""WORLD_SIZE""] = str(world_size)
        os.environ[""LOCAL_RANK""] = str(rank % num_gpus)
        os.environ[""RANK""] = str(rank)
    else:
        rank = int(os.environ[""RANK""])
        world_size = int(os.environ[""WORLD_SIZE""])

    torch.cuda.set_device(rank % num_gpus)

    dist.init_process_group(
        backend=backend,
        world_size=world_size,
        rank=rank,
    )
```",hi check launch please check tutorial detailed distributed training single node single card training single node training multiple training manager training example core function python initialize distributed training environment support see rank show head specify master port port none port rank rank rank else rank rank rank,issue,positive,negative,negative,negative,negative,negative
1154952035,Has anyone managed to run the Imagenet distributed example on SLURM using multiple nodes?,anyone run distributed example multiple,issue,negative,neutral,neutral,neutral,neutral,neutral
1146989757,"> @pritamdamania87 @fduwjj can we please add tests for this example here? https://github.com/pytorch/examples/blob/main/run_python_examples.sh
> 
> Our DDP RPC examples are outdated and I'd like to avoid having that happen to this example as well #991
> 
> Once that's done you may want to consider also adding the example to the pytorch.org/examples page https://github.com/pytorch/examples/blob/main/docs/source/index.rst

Will do. QQ here, the script needs a multiple GPU running environment, can I use srun in the test script or what's your guidance on it. I didn't see the example of how we test DDP here. Thanks!",please add example outdated like avoid happen example well done may want consider also example page script need multiple running environment use test script guidance see example test thanks,issue,positive,negative,neutral,neutral,negative,negative
1144280298,"For distributed training on Kubernetes I'd recommend using TorchX w/ Kubernetes+Volcano since we already do this all for you: 

* https://pytorch.org/torchx/main/quickstart.html
* https://pytorch.org/torchx/main/schedulers/kubernetes.html

To your actual problem: you need to specify `--master-address` to point to the same machine and they're currently set to `test` which doesn't exist. You can set the kubernetes pods hostname so they're different for the two pods and then change them to point to the first node. Kubernete's batch interface isn't great for this which is why in TorchX we recommend using Volcano since it has proper gang semantics",distributed training recommend since already actual problem need specify point machine currently set test exist set different two change point first node batch interface great recommend volcano since proper gang semantics,issue,positive,positive,positive,positive,positive,positive
1144271954,@xbfu you may be interested in using https://github.com/pytorch/torchx which is our recommended way of launching cloud jobs @d4l3k as FYI,may interested way cloud,issue,negative,positive,positive,positive,positive,positive
1144269721,"This answer should help https://discuss.pytorch.org/t/access-all-weights-of-a-model/77672/2?u=marksaroufim

In general https://discuss.pytorch.org/ is a better forum for Q&A",answer help general better forum,issue,positive,positive,positive,positive,positive,positive
1135201883,"@pritamdamania87 @fduwjj can we please add tests for this example here? https://github.com/pytorch/examples/blob/main/run_python_examples.sh

Our DDP RPC examples are outdated and I'd like to avoid having that happen to this example as well https://github.com/pytorch/examples/issues/991

Once that's done you may want to consider also adding the example to the pytorch.org/examples page https://github.com/pytorch/examples/blob/main/docs/source/index.rst",please add example outdated like avoid happen example well done may want consider also example page,issue,positive,negative,negative,negative,negative,negative
1131690524,"> bmsggan

no chang7ing, I couldn't find it. If you found ,please share here.

",could find found please share,issue,positive,neutral,neutral,neutral,neutral,neutral
1126313625,"I suggest closing this issue as #1003 has been merged to master. Thanks, @msaroufim for the feedback!

---

I can make another example that aligns with what the issue is suggesting in the matter of using a more proper dataset, TripletLoss, and implementing a simple CNN rather than ResNet18 which was used in #1003 implementation.",suggest issue master thanks feedback make another example issue suggesting matter proper simple rather used implementation,issue,negative,positive,neutral,neutral,positive,positive
1125674534,"Hmm is this example from this repo? 

Regardless the error is on that you're missing a `DIR` param. Check out the code to get more clarity on what you're supposed to do",example regardless error missing param check code get clarity supposed,issue,negative,negative,negative,negative,negative,negative
1121254879,"Maybe try those hyperparameters, and if they lead to the expected accuracy, perhaps create a pull request to update the README file accordingly?",maybe try lead accuracy perhaps create pull request update file accordingly,issue,negative,neutral,neutral,neutral,neutral,neutral
1121254126,"Just quoting from this [blog article](https://blog.paperspace.com/popular-deep-learning-architectures-alexnet-vgg-googlenet/):
> The model uses a stochastic gradient descent optimization function with batch size, momentum, and weight decay set to 128, 0.9, and 0.0005 respectively. All the layers use an equal learning rate of 0.001. ",article model stochastic gradient descent optimization function batch size momentum weight decay set respectively use equal learning rate,issue,negative,neutral,neutral,neutral,neutral,neutral
1120317234,Can you please setup a test here as well? https://github.com/pytorch/examples/blob/main/run_python_examples.sh,please setup test well,issue,positive,neutral,neutral,neutral,neutral,neutral
1116466532,"I found these: 
- https://discuss.pytorch.org/t/why-parameter-batch-first-is-needed/25769/2
- https://stackoverflow.com/a/49473068

I'm assuming that this is due to the underlying CUDA kernel being better suited for batches having sequence dimension first, followed by the batch dimension",found assuming due underlying kernel better sequence dimension first batch dimension,issue,negative,positive,positive,positive,positive,positive
1111118888,"Thanks @soumith for clarifying what that `if` block is doing. IIUC the two conditions are to cater to both `nn.DataParallel` and `DistributedDataParallel`

What I had missed was that inputs don't need to be on GPU, `nn.DataParallel` manages moving it to GPUs under the hood
```
x = torch.rand(4,1)  # on CPU
model = nn.DataParallel(model)  # replicated on all GPUs
y = model(x)  # gathered on GPU0
```

So when `args.gpu == None` (i.e. multi-GPU training with `nn.DataParallel`), the first if block keeps `images` on CPU. The second if block moves `targets` to GPU0 in case of  DataParallel, or to the respective `rank` if using DistributedDataParallel.

 
Closing the issue, @I-Doctor please reopen if you have follow-up queries!",thanks block two cater need moving hood model model replicated model none training first block second block case respective rank issue please reopen,issue,negative,negative,neutral,neutral,negative,negative
1109340290,"what @suraj813 suggested is incorrect.

>  I'm not sure why these four lines of codes move 'images' to gpu only when I choose only one gpu(args.gpu is not None) and move 'target' to gpu even with argument device=None(args.gpu=None).


the reason is this.
When you do single GPU training, you dont use `nn.DataParallel`, you simply send `images` directly to your model. So `images` is moved onto GPU 0 in a straight-forward manner.

When you do multi-GPU training with `nn.DataParallel`, you keep `images` on CPU and then send it to `nn.DataParallel`, which will slice `images` into as many Tensors as there are GPUs (for example if `images` had 64 images and there  are 4 GPUs, `images` is sliced into four separate Tensors of `16` images each). These individual sliced Tensors are then sent to their respective GPUs. In the 4-GPU example, if we had sent `images` onto GPU 0 and then called `nn.DataParallel` there would be a wasteful move of `images` first to GPU0, and then each of the 3 slices of images will then each be moved from GPU0 to GPU1, GPU0 to GPU2 and GPU0 to GPU3 respectively.",incorrect sure four move choose one none move even argument reason single training dont use simply send directly model onto manner training keep send slice many example sliced four separate individual sliced sent respective example sent onto would wasteful move first respectively,issue,positive,positive,positive,positive,positive,positive
1106848853,"Nope crystal clear, I'm actually very curious what the changes will end up being",nope crystal clear actually curious end,issue,negative,positive,neutral,neutral,positive,positive
1104635196,@msaroufim Feel free to assign this to me and make any edits/clarifications to the text. ,feel free assign make text,issue,positive,positive,positive,positive,positive,positive
1100593473,"I think that's because if you don't use `fake.detach()` in `output = netD(fake.detach()).view(-1)` then `fake` is just some middle variable in the whole computational Graph, it will track from `netG()` to `netD()`. and when you can `optimizerD.step()` all grad information except leaf nodes are released. which means no more gradient information about `netG()` in the computational Graph. then you use `errG.backward()` it will cause an error",think use output fake middle variable whole computational graph track grad information except leaf gradient information computational graph use cause error,issue,negative,negative,neutral,neutral,negative,negative
1100449912,Hi @constroy I apologize for the delay but I ended up deprecating this specific example because I figured it may be better to point folks to `pytorch/functorch`,hi apologize delay ended specific example figured may better point,issue,negative,positive,positive,positive,positive,positive
1096756656,"I'd like to take this. Just to clarify, we should update all requirements files (starting [with here](https://github.com/pytorch/examples/blob/main/distributed/rpc/pipeline/requirements.txt), to `torch==1.9.0`. 

I did a quick check of 
```
find . -name '*requirements.txt' -exec grep -i 'torch' {} \; -print
```

and it looks like there are others that use torch but don't pin a version. Does it make sense to pin all of them and then do some kind of automated CI/CD check? Maybe that's phase 2 or 3. :) 
",like take clarify update starting quick check find like use torch pin version make sense pin kind check maybe phase,issue,positive,positive,positive,positive,positive,positive
1096399571,"> > DCGAN is quite old. Check the latest papers on GANs and you will find many large resolution models/examples. You need a dataset with high resolution images as well (of course).
> > [â€¦](#)
> > On Thu, 21 Mar 2019 at 16:56, Chi Nok Enoch K _**@**_.***> wrote: Not sure if this thread is still active, but did anyone try to generate 128x128 images and upscale to 512x512 per @bartolsthoorn https://github.com/bartolsthoorn 's suggestion? â€” You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#70 (comment)](https://github.com/pytorch/examples/issues/70#issuecomment-475289841)>, or mute the thread https://github.com/notifications/unsubscribe-auth/AAGgWk-LFMMdIzfFfUK6Fg2YBQHumo6yks5vY6vBgaJpZM4MDwky .
> 
> > DCGAN is quite old. Check the latest papers on GANs and you will find many large resolution models/examples. You need a dataset with high resolution images as well (of course).
> > [â€¦](#)
> > On Thu, 21 Mar 2019 at 16:56, Chi Nok Enoch K _**@**_.***> wrote: Not sure if this thread is still active, but did anyone try to generate 128x128 images and upscale to 512x512 per @bartolsthoorn https://github.com/bartolsthoorn 's suggestion? â€” You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#70 (comment)](https://github.com/pytorch/examples/issues/70#issuecomment-475289841)>, or mute the thread https://github.com/notifications/unsubscribe-auth/AAGgWk-LFMMdIzfFfUK6Fg2YBQHumo6yks5vY6vBgaJpZM4MDwky .
> 
> Could you name Some GANS , which could be made to work easily for any input sizes?

Hello, have you found it? Can you share it?",quite old check latest find many large resolution need high resolution well course mar chi wrote sure thread still active anyone try generate upscale per suggestion reply directly view comment mute thread quite old check latest find many large resolution need high resolution well course mar chi wrote sure thread still active anyone try generate upscale per suggestion reply directly view comment mute thread could name could made work easily input size hello found share,issue,positive,positive,positive,positive,positive,positive
1092063055,"@dbl001 feel free to raise a PR for this, I'm happy to review it",feel free raise happy review,issue,positive,positive,positive,positive,positive,positive
1084736938,I'll submit a PR in a couple of days for this in accordance with the existing examples of this repo. ,submit couple day accordance,issue,negative,neutral,neutral,neutral,neutral,neutral
1080265752,"@msaroufim Now, I understand. Current version seems much clear.
```
running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward
```
It means updating the running_reward with episode reward by moving averaging, right?",understand current version much clear episode reward moving right,issue,positive,positive,positive,positive,positive,positive
1077455790,"> DCGAN is quite old. Check the latest papers on GANs and you will find many large resolution models/examples. You need a dataset with high resolution images as well (of course).
> [â€¦](#)
> On Thu, 21 Mar 2019 at 16:56, Chi Nok Enoch K ***@***.***> wrote: Not sure if this thread is still active, but did anyone try to generate 128x128 images and upscale to 512x512 per @bartolsthoorn <https://github.com/bartolsthoorn> 's suggestion? â€” You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#70 (comment)](https://github.com/pytorch/examples/issues/70#issuecomment-475289841)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AAGgWk-LFMMdIzfFfUK6Fg2YBQHumo6yks5vY6vBgaJpZM4MDwky> .


> DCGAN is quite old. Check the latest papers on GANs and you will find many large resolution models/examples. You need a dataset with high resolution images as well (of course).
> [â€¦](#)
> On Thu, 21 Mar 2019 at 16:56, Chi Nok Enoch K ***@***.***> wrote: Not sure if this thread is still active, but did anyone try to generate 128x128 images and upscale to 512x512 per @bartolsthoorn <https://github.com/bartolsthoorn> 's suggestion? â€” You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#70 (comment)](https://github.com/pytorch/examples/issues/70#issuecomment-475289841)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AAGgWk-LFMMdIzfFfUK6Fg2YBQHumo6yks5vY6vBgaJpZM4MDwky> .

Could you name Some GANS , which could be made to work easily for any input sizes?
",quite old check latest find many large resolution need high resolution well course mar chi wrote sure thread still active anyone try generate upscale per suggestion reply directly view comment mute thread quite old check latest find many large resolution need high resolution well course mar chi wrote sure thread still active anyone try generate upscale per suggestion reply directly view comment mute thread could name could made work easily input size,issue,positive,positive,positive,positive,positive,positive
1077453621,"> I ended up abandoning dcgan and am now over using bmsggan which is a variation on progressive gans. Its handling higher resolutions much better

is it working now EvanZ?
",ended variation progressive handling higher much better working,issue,negative,positive,positive,positive,positive,positive
1075715716,"@msaroufim, Added test for `actor_critic` to the script and updated corresponding error messages. The script runs locally without any problem. here's the [log](https://www.dropbox.com/s/nk5rgfyad1xee04/log.txt?dl=0) for both `reinforce` and `actor_critic` after running the script locally. ",added test script corresponding error script locally without problem log reinforce running script locally,issue,negative,neutral,neutral,neutral,neutral,neutral
1071996363,"https://github.com/pytorch/examples/blob/0cb38ebb1b6e50426464b3485435c0c6affc2b65/imagenet/main.py#L310
```
        loss.backward()
```

When I remove this line, process 1 no longer allocates memory on GPU 0, so it all happens when error backpropagation.

Does anyone have some insights?",remove line process longer memory error anyone,issue,negative,neutral,neutral,neutral,neutral,neutral
1071029368,Hi @rohan-varma the RPC example we have in this repo is from 1.4 do you think we should just go ahead and remove it?,hi example think go ahead remove,issue,negative,neutral,neutral,neutral,neutral,neutral
1070324865,@colesbury @mreso any suggestions on who would be  a good person at Meta to review C++ examples?,would good person meta review,issue,negative,positive,positive,positive,positive,positive
1070319945,"This seems like a generally useful pattern, maybe worth having this be argument for a fault tolerant `torch.save()` - I'm make the request and elaborate the need on `pytorch/pytorch` ",like generally useful pattern maybe worth argument fault tolerant make request elaborate need,issue,negative,positive,positive,positive,positive,positive
1070318153,"Hi @pbelevich would you still like to contribute this example or should we close it? Our RPC example is still quite cold, dates back to pytorch 1.4 it looks like",hi would still like contribute example close example still quite cold back like,issue,positive,negative,negative,negative,negative,negative
1070317575,"Gotcha do you mind then adding a test here for `python actor_critic.py` as well? https://github.com/pytorch/examples/blob/main/run_python_examples.sh and make sure it runs locally

It seems right now that only reinforce is running, you can look at that test to figure out what you need to change. ",mind test python well make sure locally right reinforce running look test figure need change,issue,positive,positive,positive,positive,positive,positive
1070274925,"@constroy @Chillee @jamesr66a  could we add a tests for fx stuff here so I can accept community contributions https://github.com/pytorch/examples/blob/main/run_python_examples.sh

The examples won't work for very long otherwise",could add stuff accept community wo work long otherwise,issue,negative,negative,neutral,neutral,negative,negative
1066433896,"Let's abandon it, this is too long ago...",let abandon long ago,issue,negative,negative,neutral,neutral,negative,negative
1066217754,"3 fixes in one, wow! Will need some time to test but tagging @mrshenli in case he can code review at a glance",one wow need time test case code review glance,issue,positive,positive,neutral,neutral,positive,positive
1066170824,"@msaroufim, The changes were only made to `actor_critic.py` file. The model achieves target average reward in 930 episodes.
 
Log with default seed: [Actor-Critic Log](https://www.dropbox.com/s/xq282asb6suz9ff/ac_log.txt?dl=0) ",made file model target average reward log default seed log,issue,positive,negative,negative,negative,negative,negative
1065984817,Created a quick PR to address the general problem stated above. Hopefully this suffices.,quick address general problem stated hopefully,issue,negative,positive,positive,positive,positive,positive
1065983071,"I'm sorry I don't have this environment set up anymore but I believe the error was a result of just inserting a save path that is syntactically valid but didn't actually exist. Such as a save path in a directory that doesn't exist.

Regardless I think there probably should be more fail safes around the actual saving of the file to prevent loss of progress in training a model, as I imagine this could happen with many other OS related issues that the user might want to manually resolve and attempt to re-save (e.g. out of disk space on save path, drive unplugged during save, network path disconnected, etc)",sorry environment set believe error result save path syntactically valid actually exist save path directory exist regardless think probably fail around actual saving file prevent loss progress training model imagine could happen many o related user might want manually resolve attempt disk space save path drive unplugged save network path disconnected,issue,positive,negative,neutral,neutral,negative,negative
1065959698,Cool I'd suggest making sure the example still works by running the test `python reinforce.py` and printing the logs here,cool suggest making sure example still work running test python printing,issue,positive,positive,positive,positive,positive,positive
1065559475,"Yes, I can update the link in the readme today ",yes update link today,issue,negative,neutral,neutral,neutral,neutral,neutral
1064494345,"Should we upgrade â€™from text.legacy import dataâ€™ the older pytorch call in train.py line 9?

> On Mar 9, 2022, at 7:33 PM, Suraj Subramanian ***@***.***> wrote:
> 
> 
> Fixed in #591 <https://github.com/pytorch/examples/pull/591>
> â€”
> Reply to this email directly, view it on GitHub <https://github.com/pytorch/examples/issues/304#issuecomment-1063617212>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AAXWFW2RF5765FRAZPDF2FDU7FUQ5ANCNFSM4EQEGSQQ>.
> Triage notifications on the go with GitHub Mobile for iOS <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675> or Android <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. 
> You are receiving this because you authored the thread.
> 

",upgrade import data older call line mar wrote fixed reply directly view triage go mobile android thread,issue,negative,positive,positive,positive,positive,positive
1064374187,"@msaroufim Sorry I haven't used Pytorch in a while, forgot a lot of stuff ğŸ˜…",sorry used forgot lot stuff,issue,negative,negative,negative,negative,negative,negative
1063770157,"I forgot about this PR lmfao, I just saw a bunch of emails from GitHub and I was like wait wtf
I honestly can't remember why I did this :P",forgot saw bunch like wait honestly ca remember,issue,positive,positive,positive,positive,positive,positive
1063694512,"Are you building from source, would just suggest trying out a more recent version of pytorch",building source would suggest trying recent version,issue,negative,neutral,neutral,neutral,neutral,neutral
1063693595,I would double check and print what `X_var` is before you pass it to `net()`,would double check print pas net,issue,negative,neutral,neutral,neutral,neutral,neutral
1063691005,"You can take a look at this https://github.com/pytorch/examples/blob/main/imagenet/main.py#L236

you're basically pointing -e to a directory from which the script will create a validation data loader here https://github.com/pytorch/examples/blob/main/imagenet/main.py#L226",take look basically pointing directory script create validation data loader,issue,negative,neutral,neutral,neutral,neutral,neutral
1063689438,"I'd suggest running nvidia-smi in the background to figure out what's going on but if getting out of memory the easiest things to do are smaller batch size, smaller model, smaller input",suggest running background figure going getting memory easiest smaller batch size smaller model smaller input,issue,negative,neutral,neutral,neutral,neutral,neutral
1063687505,"> but why the result is not well enough

Could just be try more epochs and if it's still bad then try changing other hyperparameters or your model

> Another question is that why you set the random seedï¼Ÿ

The random seed is fixed so that the results you get are always the same which is helpful to guarantee convergence, reproducibility and sane behavior in tests",result well enough could try still bad try model another question set random random seed fixed get always helpful guarantee convergence reproducibility sane behavior,issue,negative,negative,negative,negative,negative,negative
1063678875,"It sounds like you're using a pretrained model and would like to fine-tune it

This tutorial may help https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html",like model would like tutorial may help,issue,positive,neutral,neutral,neutral,neutral,neutral
1063668026,@Times125 I'd try a fresh environment as is it's not clear how I can help repro. Please reopen if I'm mistaken,time try fresh environment clear help please reopen mistaken,issue,positive,positive,positive,positive,positive,positive
1063666393,Hi @kumar-shridhar could you share more detail on the script you're working with and the issue you're seeing?,hi could share detail script working issue seeing,issue,negative,neutral,neutral,neutral,neutral,neutral
1063660036,"Closing due to lack of activity, feel free to reopen if necessary.",due lack activity feel free reopen necessary,issue,negative,positive,neutral,neutral,positive,positive
1063659541,"Thanks @I-Doctor, [this is](https://github.com/pytorch/examples/blob/792d336019a28a679e29cf174e10cee80ead8722/imagenet/main.py#L284-L287) a weird snippet that has multiple occurrences in this script. The second condition (if `torch.cuda.is_available()`) can be merged into the first (`if args.gpu is not None`).  Would you like to raise a PR for this?",thanks weird snippet multiple script second condition first none would like raise,issue,negative,negative,neutral,neutral,negative,negative
1063648284,"Hi, it's not clear what example are you referring to. If you're looking to debug your code, maybe try posting on the forums - make sure to include the actual error you run into as well.",hi clear example looking code maybe try posting make sure include actual error run well,issue,positive,positive,positive,positive,positive,positive
1063645580,Looks outdated. Closing but reopen if you'd like to add any clarification or updated results that aren't working as expected.,outdated reopen like add clarification working,issue,negative,negative,negative,negative,negative,negative
1063607479,"Closing, fix is to update pillow version",fix update pillow version,issue,negative,neutral,neutral,neutral,neutral,neutral
1063597198,"Seems outdated, closing this issue. Please reopen if it persists in current pytorch versions",outdated issue please reopen current,issue,negative,negative,negative,negative,negative,negative
1063594772,The pytorch hub has a DCGAN trained on FashionGen. AFAIK there aren't LSUN or Imagenet trained DCGAN hosted on the pytorch site,hub trained trained site,issue,negative,neutral,neutral,neutral,neutral,neutral
1063591909,"@rallen10 Thanks for raising this. I agree, this example's learning rate is (way) too high and the exploding loss resolves by setting it to something like 0.001. 

I'll leave this open for a ""good first issue""; along with fixing the learning rate, this example could also use a refactor ;)",thanks raising agree example learning rate way high loss setting something like leave open good first issue along fixing learning rate example could also use,issue,positive,positive,positive,positive,positive,positive
1063588850,You can redirect stdout to a file to persist. I don't think we need this in the examples. Thanks!,redirect file persist think need thanks,issue,negative,positive,positive,positive,positive,positive
1063586348,"The issue is not clear, closing. Please reopen if you'd like to add details on what example caused this error",issue clear please reopen like add example error,issue,positive,positive,positive,positive,positive,positive
1063574092,"@suiyuan2009 it's been a while since you posted this, but in the chance that you're still interested in this I can help you with raising a PR for this",since posted chance still interested help raising,issue,positive,positive,positive,positive,positive,positive
1063538267,Does the error go away if you run on a more recent version of ubuntu?,error go away run recent version,issue,negative,neutral,neutral,neutral,neutral,neutral
1063523398,@romanoss is correct although it may be worth making a simple change to make this example work on cpu as well,correct although may worth making simple change make example work well,issue,negative,positive,positive,positive,positive,positive
1063519473,@robmarkcole would you like to make the PR to fix this?,would like make fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1063518310,Closing since old and no description provided,since old description provided,issue,negative,positive,neutral,neutral,positive,positive
1063517223,Hi @bionicles I'd suggest flagging the variables you had trouble understanding and feel free to make the PR and I'll merge,hi suggest flagging trouble understanding feel free make merge,issue,negative,positive,neutral,neutral,positive,positive
1063516218,Thank you for the helpful workaround @sakaia it seems like we no longer have any dependencies in in the minist example except for torch and torchvision so this should no longer be an issue https://github.com/pytorch/examples/blob/main/mnist/requirements.txt,thank helpful like longer example except torch longer issue,issue,positive,neutral,neutral,neutral,neutral,neutral
1063513387,We stopped supporting googlenet since it's behavior is funky,stopped supporting since behavior funky,issue,negative,positive,positive,positive,positive,positive
1063512185,"I'm not sure which code you're running exactly since it seems to be imported within your own custom directory

But from your error message you can see `assert os.path.exists(path)` so I'd add a print statement like `print(path)` before that line to make sure your path does indeed exist and debug accordingly",sure code running exactly since within custom directory error message see assert path add print statement like print path line make sure path indeed exist accordingly,issue,positive,positive,positive,positive,positive,positive
1063506713,"I just tried running this in google colab and it works just fine, I'd double check if you're using a recent version of PyTorch

![Screen Shot 2022-03-09 at 4 09 55 PM](https://user-images.githubusercontent.com/3282513/157560944-4d0bc1de-30da-4d5a-bc8a-60dd51a62732.png)
",tried running work fine double check recent version screen shot,issue,negative,positive,positive,positive,positive,positive
1063501762,"It was just probably an easy choice, if you think a line change would make this example converge faster by all means please make a PR",probably easy choice think line change would make example converge faster please make,issue,positive,positive,positive,positive,positive,positive
1063498948,Hi @MichelleYang2017 do you mind elaborating a bit more on which code snippet isn't behaving the way you expect it to?,hi mind bit code snippet way expect,issue,negative,neutral,neutral,neutral,neutral,neutral
1063498056,Hi @kirk86 would you like to make a PR for this?,hi kirk would like make,issue,negative,neutral,neutral,neutral,neutral,neutral
1063492109,Closing OP should try a newer version of PyTorch and a smaller batch size,try version smaller batch size,issue,negative,neutral,neutral,neutral,neutral,neutral
1063491657,"Hi @TheMrGhostman I'd suggest creating a full repro that others can run and help you debug the issue

Just scanning your error message

`RuntimeError: Assertion `x >= 0. && x <= 1.' failed. input value should be between 0~1, but got -nan at /pytorch/aten/src/THNN/generic/BCECriterion.c:62`

So add a `breakpoint()` or `print()` statement to figure out why you're passing in a nan which means you have a bug higher up in your code instead of a value between 0-1.",hi suggest full run help issue scanning error message assertion input value got add print statement figure passing nan bug higher code instead value,issue,positive,positive,positive,positive,positive,positive
1063487020,@smartnet-club would you like to contribute the fix?,would like contribute fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1063486372,Make sure you use the out directory argument here? https://github.com/fyu/lsun,make sure use directory argument,issue,negative,positive,positive,positive,positive,positive
1063482501,"`torchvision` supports a couple of datasets out of the box

If you click this https://pytorch.org/vision/0.8/datasets.html

You'll see a bunch of datasets, click on one you're interested in and then click source to find a url",couple box click see bunch click one interested click source find,issue,negative,positive,positive,positive,positive,positive
1063480394,"Hi @piyush01123 you're welcome to try producing an example, I'd suggest taking a look at existing examples to see how to get going",hi welcome try example suggest taking look see get going,issue,negative,positive,positive,positive,positive,positive
1063416082,"I'm not sure this error applies to this repo (base) âœ  examples git:(main) grep -r ""yolov3"" .    ",sure error base git main,issue,negative,negative,neutral,neutral,negative,negative
1063405821,@rodrigodesalvobraz I'd suggest you try out your improved version and see if it converges faster or to a better result and make a PR if it does,suggest try version see faster better result make,issue,negative,positive,positive,positive,positive,positive
1063399377,Python 2 is no longer supported by the PSF so unless it's super easy it's unlikely that we'll a fix merged for this,python longer unless super easy unlikely fix,issue,positive,positive,neutral,neutral,positive,positive
1063398486,Since main PR was also closed,since main also closed,issue,negative,positive,neutral,neutral,positive,positive
1063396694,"Seems OK to me, feel free to make a PR if you'd like to see this change in ",feel free make like see change,issue,positive,positive,positive,positive,positive,positive
1063395651,"I just checked the original paper and it doesn't make it too clear how much RAM is actually needed https://arxiv.org/abs/1611.05431

Best way is generally to benchmark and see, the PyTorch profiler may help you debug obvious issues https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/",checked original paper make clear much ram actually best way generally see profiler may help obvious,issue,positive,positive,positive,positive,positive,positive
1063390588,"This seems to work just fine, I just downloaaded the link and it's not forbidden
![Screen Shot 2022-03-09 at 1 29 30 PM](https://user-images.githubusercontent.com/3282513/157539303-0d7437af-b1c1-4cb0-a9b6-dea830de525b.png)
",work fine link forbidden screen shot,issue,negative,positive,positive,positive,positive,positive
1063389138,"It's difficult to debug without a repro but my best guess is this expected behavior because of a periodic weight synchronization since you're using so many GPUs. Try using no sync mode to confirm for sure.

![Screen Shot 2022-03-09 at 1 27 04 PM](https://user-images.githubusercontent.com/3282513/157539030-0a6d449e-346d-4de1-8c55-cbbf7d029404.png)

",difficult without best guess behavior periodic weight synchronization since many try sync mode confirm sure screen shot,issue,negative,positive,positive,positive,positive,positive
1063381110,I'dd suggest making it clearer which example this is and how to repro this error. Although just skimming it feels like there's a few good first issues here for folks,suggest making clearer example error although skimming like good first,issue,negative,positive,positive,positive,positive,positive
1063378604,"I'm not sure a priori, best way is to benchmark and see",sure best way see,issue,positive,positive,positive,positive,positive,positive
1063377652,"The first number Accuracy at 5 means that any of your model 5 highest probability answers must match the expected answer you can see exactly how this is implemented here https://github.com/pytorch/examples/blob/886b74e17ce2df1397f363fdb9a3f940dcec03ba/imagenet/main.py#L388

Feel free to add print statements to get more intuition",first number accuracy model highest probability must match answer see exactly feel free add print get intuition,issue,positive,positive,positive,positive,positive,positive
1063373580,Hi @carlamao could you please share instructions and how to reproduce this error?,hi could please share reproduce error,issue,negative,neutral,neutral,neutral,neutral,neutral
1063369414,Feel free to submit a PR with this fix,feel free submit fix,issue,positive,positive,positive,positive,positive,positive
1063367922,Weight generally means a value between 0 to 1 if that's not correct feel free to make a PR for the fix,weight generally value correct feel free make fix,issue,positive,positive,positive,positive,positive,positive
1063366382,"I'm guessing the decision was arbitrary, would you like to try out the version with 3 layers and see if that has better results? and make the PR if it does?",guessing decision arbitrary would like try version see better make,issue,positive,positive,positive,positive,positive,positive
1063364526,Link now 404 so this looks like a stale issue,link like stale issue,issue,negative,negative,negative,negative,negative,negative
1063363565,Hi @JIElite would you like to make a PR with your suggested improvement?,hi would like make improvement,issue,positive,neutral,neutral,neutral,neutral,neutral
1063359735,Generally the way I've seen this work is you'd have the same dictionary for test and train and then unseen words would be replaced by an unknown ,generally way seen work dictionary test train unseen would unknown,issue,negative,negative,neutral,neutral,negative,negative
1063357011,I've definitely moved on; wouldn't be able to spend more time on this.,definitely would able spend time,issue,negative,positive,positive,positive,positive,positive
1063354608,"I'd encourage you to contribute this example, would be happy to review",encourage contribute example would happy review,issue,positive,positive,positive,positive,positive,positive
1063326409,Agreed there's one already and I'm working on an improved one,agreed one already working one,issue,negative,neutral,neutral,neutral,neutral,neutral
1063326064,"You can go ahead and create a PR, no permissions are needed. Although I would suggest creating `.py` files instead since we don't have notebooks in this repo.

One way I like doing is 
```
git checkout -b my_branch
make changes
git add .
git commit -m ""changes""
gh pr create
```",go ahead create although would suggest instead since one way like git make git add git commit create,issue,positive,neutral,neutral,neutral,neutral,neutral
1063319995,"Super strange, that said for M1 updates I'd suggest following this thread instead https://github.com/pytorch/pytorch/issues/47702#issuecomment-965625139",super strange said suggest following thread instead,issue,negative,positive,neutral,neutral,positive,positive
1063317960,Hi @MaxVanDijck would you like to make a PR for this?,hi would like make,issue,negative,neutral,neutral,neutral,neutral,neutral
1063317312,Would you like to make a PR for this? Just make sure that code still runs and updated README as needed,would like make make sure code still,issue,positive,positive,positive,positive,positive,positive
1063310985,@Feez would you like to make a PR for this? Could you also show us the full error you're getting?,would like make could also show u full error getting,issue,negative,positive,positive,positive,positive,positive
1063296192,"Hi @figurine2017 @jekbradbury I realize this is kinda ridiculously late but are you still interested in merging this if so it'd be great to fix merge conflicts, add a test and a README. If you've long moved on feel free to close this PR",hi figurine realize ridiculously late still interested great fix merge add test long feel free close,issue,positive,positive,positive,positive,positive,positive
1063283875,I think it's fine `inpt` doesn't feel like the right name either,think fine feel like right name either,issue,positive,positive,positive,positive,positive,positive
1063277872,"Too many merge conflicts for a small change, I'd suggest reopening if you'd still like to see this in and briefly describing why this change is beneficial",many merge small change suggest still like see briefly change beneficial,issue,positive,positive,neutral,neutral,positive,positive
1063268249,Hi @zergey could you please also add a test here https://github.com/pytorch/examples/blob/main/run_python_examples.sh,hi could please also add test,issue,negative,neutral,neutral,neutral,neutral,neutral
1063261161,@dribnet I'm happy to merge this but it probably needs a test. Take a look at this for some inspiration https://github.com/pytorch/examples/blob/main/run_python_examples.sh,happy merge probably need test take look inspiration,issue,positive,positive,positive,positive,positive,positive
1063253953,"It's an interesting contribution for sure but I'm think it's best to stick boring methods that work, I can't imagine this repo will be maintainable if we want to add all state of the art work in it ",interesting contribution sure think best stick boring work ca imagine maintainable want add state art work,issue,positive,positive,positive,positive,positive,positive
1063251231,@Shyellen could you update this PR to remove your name from the directory? And make sure you can merge this back to the README in `mnist/README.md`,could update remove name directory make sure merge back,issue,negative,positive,positive,positive,positive,positive
1063250724,"@msaroufim ideally for each DP example we should create a counterpart example that uses DDP. We haven't deprecated DP yet, since we don't have enough resources right now to work on feature parity. However we already encourage our users to migrate to DDP if they can. We would be sending a mixed signal if we had examples covered by DP, but not DDP.",ideally example create counterpart example yet since enough right work feature parity however already encourage migrate would sending mixed signal covered,issue,positive,positive,positive,positive,positive,positive
1063235448,@mrshenli @rohan-varma @cbalioglu do we want to deprecate the DP example and create an updated DDP one,want deprecate example create one,issue,negative,neutral,neutral,neutral,neutral,neutral
1063233106,I'm not sure throwing an exception is the correct behavior since it's better to have the program running to know why nan happened - feel free to reopen if you disagree,sure throwing exception correct behavior since better program running know nan feel free reopen disagree,issue,positive,positive,positive,positive,positive,positive
1063228788,"This is a bit tricky to merge for a few reasons because the number of code changes is 62K lines. I would suggest you remove any binary files from your submission aka the `.pth` files, change existing files into `.py` files instead of notebooks and resubmit a fresh PR whenever you're ready",bit tricky merge number code would suggest remove binary submission aka change instead resubmit fresh whenever ready,issue,positive,positive,positive,positive,positive,positive
1063216714,"Not a bad addition per se but it only shows up in two places

```
(base) âœ  examples git:(main) grep -r ""args.no_cuda"" .
./mnist/main.py:    use_cuda = not args.no_cuda and torch.cuda.is_available()
./vae/main.py:args.cuda = not args.no_cuda and torch.cuda.is_available()
```

I'm thinking the more general ask is an interface for appropriate CLI arguments that people can use as a starting point",bad addition per se two base git main thinking general ask interface appropriate people use starting point,issue,negative,negative,negative,negative,negative,negative
1063202790,Thanks @SamuelMarks - I'm thinking it'd be cool to render actual docs for the library then since everything is now consistent! ,thanks thinking cool render actual library since everything consistent,issue,positive,positive,positive,positive,positive,positive
1063193167,Hi @rohan-varma @mrshenli is this an example you'd still like to see merged in?,hi example still like see,issue,negative,neutral,neutral,neutral,neutral,neutral
1063186724,"Thank you for you for this PR, we already had a code of conduct added with #694 so will close this PR",thank already code conduct added close,issue,negative,neutral,neutral,neutral,neutral,neutral
1063185423,Hi @andreh7 I recognize this a very late review but do you mind adding your image to the appropriate README.md and showing me what it looks like?,hi recognize late review mind image appropriate showing like,issue,negative,positive,neutral,neutral,positive,positive
1063182956,Hi @zkneupper it's generally a challenge to merge PRs that fix linting issues over the whole repo especially if other code is being merged since you constantly have to deal with merge conflicts. I'd suggest opening a github issue with this request and we can merge this after we've added pending code.,hi generally challenge merge fix whole especially code since constantly deal merge suggest opening issue request merge added pending code,issue,negative,positive,neutral,neutral,positive,positive
1060587359,"> i got this too , did you solve it?
æˆ‘è§£å†³äº†ï¼Œé—®é¢˜çš„åŸå› åœ¨äºå››ä¸ªmnistæ–‡ä»¶æ²¡æœ‰ä¸‹è½½åˆ°./dataä¸­ï¼Œä½ éœ€è¦ä½¿ç”¨è¿™ä¸€ä¸ªå‘½ä»¤ä¸‹è½½ä¸‹æ¥å°±å¥½äº†ï¼š
`~/projects/pytorch_cpp/cpp/mnist/build$ python3 ../../tools/download_mnist.py -d ./data`
then do `make` 
Done
",got solve python make done,issue,negative,neutral,neutral,neutral,neutral,neutral
1060193457,Thanks @EmilPi ! Running CI now and will merge after it turns green :),thanks running merge turn green,issue,negative,neutral,neutral,neutral,neutral,neutral
1054416453,"@nairbv We could do that, but the flow we have done before is to rename the `master` branch to `main` using the internal tools. This also migrates the branch protections which is helpful. See the steps in https://github.com/pytorch/pytorch/issues/62540",could flow done rename master branch main internal also branch helpful see,issue,negative,positive,neutral,neutral,positive,positive
1054374132,"Hey @nairbv, this looks good to me, but would appreciate a glance from you before @musebc merges this/changes the default branch internally.",hey good would appreciate glance default branch internally,issue,positive,positive,positive,positive,positive,positive
1031579780,"> > @soumith This is not true. Detaching `fake` from the graph is necessary to avoid forward-passing the noise through `G` when we actually update the generator. If we do not detach, then, although `fake` is not needed for gradient update of `D,` it will still be added to the computational graph and as a consequence of `backward` pass which clears all the variables in the graph (`retain_graph=False` by default), `fake` won't be available when G is updated.
> 
> If I understand correctly, then if i created a new noise input for G, there's no need for the detach() call?

Do you mean like creating fake1=netG(noise) which is same as fake that was before disconnection. Even i have the same doubt can someone please clarify this?",true fake graph necessary avoid noise actually update generator detach although fake gradient update still added computational graph consequence backward pas graph default fake wo available understand correctly new noise input need detach call mean like noise fake disconnection even doubt someone please clarify,issue,negative,negative,negative,negative,negative,negative
1029619844,Issue seems to be persistent. You could implement your own distributed average meter or use something like [TorchMetrics](https://torchmetrics.readthedocs.io/en/stable/pages/lightning.html) which supports metrics across GPUs - I haven't used this myself but it seems to be well maintained,issue persistent could implement distributed average meter use something like metric across used well,issue,positive,negative,negative,negative,negative,negative
1029471989,"Any update on this? Is this issue still persistent?
",update issue still persistent,issue,negative,neutral,neutral,neutral,neutral,neutral
1020885321,just try smaller batch size,try smaller batch size,issue,negative,neutral,neutral,neutral,neutral,neutral
1013804191,"Thank you @Kord96 . The CI did report failure as it should after your change.
Now after I merged #944 , I am re-running CI to double check that it passes, then I will merge.",thank report failure change double check merge,issue,negative,negative,negative,negative,negative,negative
1005677666,"Thank you for your patient explanation. I have tried the paremeters settings  you recommended,  my model finally start to output what i want. Thank you again, It really helped me a lot.",thank patient explanation tried model finally start output want thank really lot,issue,positive,positive,neutral,neutral,positive,positive
1005482398,"> Have you found out the real cause of this mistake rather than learn rate settings? i have met the same issue.

I have fixed this issue, and published a paper at ACL'21. You may refer to my implementation [PETER](https://github.com/lileipisces/PETER) for the parameters settings of learning rate (1.0) and gradient clipping (1.0).

My hypothesis is that when the gradients are restricted to a very small range (0.25 in the original implementation), the search space of model parameters is small, and the model is unable to learn any pattern from the data.",found real cause mistake rather learn rate met issue fixed issue paper may refer implementation peter learning rate gradient clipping hypothesis restricted small range original implementation search space model small model unable learn pattern data,issue,negative,negative,neutral,neutral,negative,negative
1005459139,Have you found out the real cause of this mistake rather than learn rate settings? i have met the same issue.,found real cause mistake rather learn rate met issue,issue,negative,positive,positive,positive,positive,positive
998277324,"FYI: 4 years later and I'm having the same problem with `time_sequence_prediction` not being able to reproduce the expected results. My loss explodes after the 4th or 5th epoch. Changing the output layer to `Linear`, as is posed as the solution to this problem in #250, does not seem to fix this.

I'm guessing it's still a problem with [learning rate being too high](https://github.com/pytorch/examples/issues/243#issuecomment-341247405), but as the example doesn't work as it is currently written. ",later problem able reproduce loss th th epoch output layer linear solution problem seem fix guessing still problem learning rate high example work currently written,issue,negative,positive,positive,positive,positive,positive
979983944,"I find some Github repos which use both LMDB and num_workers, and finally, successfully work. But I don't know why?
You guys can find the examples here.
[stylegan2 dataset](https://github.com/rosinality/stylegan2-pytorch/blob/bef283a1c24087da704d16c30abc8e36e63efa0e/dataset.py#L10)

[clipBert dataset](https://github.com/jayleicn/ClipBERT/blob/7adfe795c6056190885c14ec0c3cb8f12b50238a/src/datasets/dataset_base.py#L196)

@jgoodson
@neillbyrne
@ruotianluo
@airsplay  [the solution](https://github.com/pytorch/vision/issues/689#issuecomment-787215916)",find use finally successfully work know find solution,issue,positive,positive,positive,positive,positive,positive
978140094,Did you try to train for one epoch with learning rate = 0 (`--lr 0`)?,try train one epoch learning rate,issue,negative,neutral,neutral,neutral,neutral,neutral
977222454,"@jamesr66a I have double checked, and made a minor fix. I confirmed it is working correctly but I don't see the ""Merge"" button.

PS: I see a few issues on the master branch related to CI and one or two scripts and I will create GitHub issues for that separately.",double checked made minor fix confirmed working correctly see merge button see master branch related one two create separately,issue,negative,positive,neutral,neutral,positive,positive
975832091,"> Looks good! Just curious, what does the print-out look like?

The print out should look the same. 
I feel the CI for this repo isn't extensive. I will run a couple of local tests to double check, and paste here the print-out",good curious look like print look feel extensive run couple local double check paste,issue,positive,positive,positive,positive,positive,positive
962916068,"in the training time, has some warning, the follow picture show the warning.
the warn one:
![image](https://user-images.githubusercontent.com/46562492/140707748-f730be72-dda6-4dfc-a3c3-628ed31f05ba.png)
the warn two:
![image](https://user-images.githubusercontent.com/46562492/140707871-703fa8e9-0f61-434a-a2f7-d96d7ab31551.png)
the warn three:
![image](https://user-images.githubusercontent.com/46562492/140707966-30b5d51a-937a-41d8-98cb-90a538fa557c.png)



",training time warning follow picture show warning warn one image warn two image warn three image,issue,negative,neutral,neutral,neutral,neutral,neutral
952425898,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!,thank contributor license agreement accept code open source project thanks,issue,positive,positive,neutral,neutral,positive,positive
950881620,I run into this problem while using multi-GPU training in a Docker container. The problem was caused by the low shared memory that was given to Docker container. You need to specify shared memory size with this argument while running docker `--shm-size 8G`,run problem training docker container problem low memory given docker container need specify memory size argument running docker,issue,negative,neutral,neutral,neutral,neutral,neutral
950579127,This issue is caused because data loading is too slow.,issue data loading slow,issue,negative,negative,negative,negative,negative,negative
949603118,"> I am having the same error in the 1st epoch itself for imagenet data. Could you please elaborate the code block @pengjiang030 which you modified?

Try removing ""model.eval()"" and also avoid ""with torch.no_grad()"" during the validation phase. I think you are getting this error because of the batchnorm layers in the model. 

Have a look at this discussion in the link below:
https://discuss.pytorch.org/t/worse-performance-when-executing-model-eval-than-model-train/107386 ",error st epoch data could please elaborate code block try removing also avoid validation phase think getting error model look discussion link,issue,negative,positive,positive,positive,positive,positive
937421617,"@Wang-Zhenxing Hi, have you found the answer yet?",hi found answer yet,issue,negative,neutral,neutral,neutral,neutral,neutral
927253562,"> 
> 
> @soumith This is not true. Detaching `fake` from the graph is necessary to avoid forward-passing the noise through `G` when we actually update the generator. If we do not detach, then, although `fake` is not needed for gradient update of `D,` it will still be added to the computational graph and as a consequence of `backward` pass which clears all the variables in the graph (`retain_graph=False` by default), `fake` won't be available when G is updated.

If I understand correctly, then if i created a new noise input for G, there's no need for the detach() call?",true fake graph necessary avoid noise actually update generator detach although fake gradient update still added computational graph consequence backward pas graph default fake wo available understand correctly new noise input need detach call,issue,negative,negative,neutral,neutral,negative,negative
917446279,"At line 165 hidden is defined so there is no issue here. 
Please note the if-statement at line 165. ",line hidden defined issue please note line,issue,negative,negative,negative,negative,negative,negative
913871213,"Hi Dongqi, did you have any followups to this issue? I am running into a closely related issue on an M1 Mac Mini where the default MNIST example does not seem to have good test time evaluation (accuracy is always chance ~10%), but the training loss goes down a significant amount. However, on a 2017 x86 Macbook Pro, the test accuracy reaches ~99% within 2 epochs, with similar training loss as the M1 machine. I'm happy to provide any further information to get to the bottom of this. 

On both machines, I set up a Python, Conda and Pytorch Environment from scratch, following this tutorial pretty much exactly - https://towardsdatascience.com/yes-you-can-run-pytorch-natively-on-m1-macbooks-and-heres-how-35d2eaa07a83

I'm on Python 3.9.6 on both machines.",hi issue running closely related issue mac default example seem good test time evaluation accuracy always chance training loss go significant amount however pro test accuracy within similar training loss machine happy provide information get bottom set python environment scratch following tutorial pretty much exactly python,issue,positive,positive,positive,positive,positive,positive
901051186,"I have solved this problem.
This problem is caused by firewall. I test some methods to configure the firewall. It is really time-consumption.
Finally, the easiest way is close the firewall when training the model.
run the following command on both 2 nodes.
```
systemctl stop firewalld
```

after training, you can open the firewall again.",problem problem test configure really finally easiest way close training model run following command stop training open,issue,negative,neutral,neutral,neutral,neutral,neutral
900009775,"For those who has the same problem, what I found is that the sample size needs to be really large to see the network learned anything. After trained on VOC 2012, it performed not too bad, though not as beautiful as the demo.
",problem found sample size need really large see network learned anything trained bad though beautiful,issue,negative,positive,positive,positive,positive,positive
887860820,"Example code fix for tutorial: https://github.com/pytorch/tutorials/pull/1619 

The BUILD error seems unrelated to this PR.  PTAL",example code fix tutorial build error unrelated,issue,negative,neutral,neutral,neutral,neutral,neutral
879640067,I got the same problem too and solve it by switch to torch1.9.0,got problem solve switch torch,issue,negative,neutral,neutral,neutral,neutral,neutral
860735175,"Check the original ""Attention is all you need"" paper (https://arxiv.org/pdf/1706.03762.pdf) in section 3.4, the authors did this multiplication. ",check original attention need paper section multiplication,issue,negative,positive,positive,positive,positive,positive
853864544,"> ![æ“·å–](https://user-images.githubusercontent.com/30921855/117557589-fbc54000-b0a6-11eb-8334-e57423b0b5f0.JPG)
> 
> As the figure above, I am wondering the meaning inside the parentheses. Dose it mean the corresponding metric in validation?
> 
> Take ACC@1 as example, the value between the inside and outside are quite large.
> 
> Thanks.

Those are the current value and average value inside the parenthesis. Take Acc@1 for example:

95.51 (89.70) 

Current Val = 95.51
Average Val = 89.70

you may check how it was calculated in this function:
https://github.com/pytorch/examples/blob/cbb760d5e50a03df667cdc32a61f75ac28e11cbf/imagenet/main.py#L376-L380

",figure wondering meaning inside parenthesis dose mean corresponding metric validation take example value inside outside quite large thanks current value average value inside parenthesis take example current average may check calculated function,issue,positive,negative,neutral,neutral,negative,negative
849246578,Is the random seed same one in your settings? It would be helpful if you have a try on other models and give more runtime information. ,random seed one would helpful try give information,issue,negative,negative,negative,negative,negative,negative
848522752,I also have the same result. I got the espcn psnr value is lower than the bicubic method. Is there any reason why this pytorch/examples espcn code has lower psnr???,also result got value lower method reason code lower,issue,negative,neutral,neutral,neutral,neutral,neutral
843681968,"> ![æ“·å–](https://user-images.githubusercontent.com/30921855/117557589-fbc54000-b0a6-11eb-8334-e57423b0b5f0.JPG)
> 
> As the figure above, I am wondering the meaning inside the parentheses. Dose it mean the corresponding metric in validation?
> 
> Take ACC@1 as example, the value between the inside and outside are quite large.
> 
> Thanks.

Hello, I have the same problem with you. Do you know how to solve it?",figure wondering meaning inside parenthesis dose mean corresponding metric validation take example value inside outside quite large thanks hello problem know solve,issue,positive,positive,neutral,neutral,positive,positive
840850622,"This changes the semantics of the example. After support passing `RemoteModule` as an arg over RPC (https://github.com/pytorch/pytorch/pull/57695), no semantics need to be changed.",semantics example support passing semantics need,issue,negative,neutral,neutral,neutral,neutral,neutral
828729209,"All of these models are from [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf).

As you can find in section 4.1 of the above research paper, these models are trained on ImageNet 2012 classification dataset of 1000 classes. The models are trained on 1.28 million training images, and  evaluated on the 50k validation images.  They also obtain a final result on the 100k test images, They evaluate both top-1 and top-5 error rates.",deep residual learning image recognition find section research paper trained classification class trained million training validation also obtain final result test evaluate error,issue,negative,neutral,neutral,neutral,neutral,neutral
826352062,"> I am having the same error in the 1st epoch itself for imagenet data. Could you please elaborate the code block @pengjiang030 which you modified?

What is your dataset? ImageNet?",error st epoch data could please elaborate code block,issue,negative,positive,positive,positive,positive,positive
826351109,"The Distributed Data Parallel (DDP) wraps the model used to [output the gathered predictions from all devices](https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/scatter_gather.py). 
The number of GPUs may restricts the experiment to achieve the accuracy provided by original paper. [Distributed-BatchNorm-PyTorch(DBP)](https://github.com/PoonKinWang/Distributed-BatchNorm-PyTorch) can simulates multi-GPUs to train the network. ",distributed data parallel model used output number may experiment achieve accuracy provided original paper train network,issue,negative,positive,positive,positive,positive,positive
811774805,"we cannot create sub-folders while accessing datasets directly from kaggle..
how to solve the said problem in such situations, where we cannot create subfolders?  ",create directly solve said problem create,issue,negative,positive,neutral,neutral,positive,positive
811397654,"@a-pushkin Reshape works, thank you.

reshape(fake_labels.sizes()) would be better in case that batch.data.size(0) != batch_size",reshape work thank reshape would better case,issue,positive,positive,positive,positive,positive,positive
809812597,"thanks for this comment.. it helped me fix my issue. for windows i added another replace statement and at the top added system.

`from platform import system`

```
 if (system() == 'Windows'):
        save_model_filename = ""epoch_"" + str(args.epochs) + ""_"" + str(time.ctime()).replace(' ', '_').replace(':','_') + ""_"" + str(args.content_weight) + ""_"" + str(args.style_weight) + "".pth""
    else:
        save_model_filename = ""epoch_"" + str(args.epochs) + ""_"" + str(time.ctime()).replace(' ', '_') + ""_"" + str(args.content_weight) + ""_"" + str(args.style_weight) + "".model""
```",thanks comment fix issue added another replace statement top added system platform import system system else,issue,positive,positive,positive,positive,positive,positive
806414803,"Setting the batch size to 1 will work, however it will significantly slow down GPU training. Reshaping network outputs (which get a bogus second dimension of size one) just so allows training with batches larger than one:
```
    for (torch::data::Example<>& batch : *data_loader) {
      const auto batch_size = batch.data.size(0);
      // Train discriminator with real images.
      discriminator->zero_grad();
      torch::Tensor real_images = batch.data.to(device);
      torch::Tensor real_labels =
          torch::empty(batch_size, device).uniform_(0.8, 1.0);
      torch::Tensor real_output =
          discriminator->forward(real_images).reshape({batch_size});

      torch::Tensor d_loss_real =
          torch::binary_cross_entropy(real_output, real_labels);
      d_loss_real.backward();

      // Train discriminator with fake images.
      torch::Tensor noise =
          torch::randn({batch.data.size(0), kNoiseSize, 1, 1}, device);
      torch::Tensor fake_images = generator->forward(noise);
      torch::Tensor fake_labels = torch::zeros(batch.data.size(0), device);
      torch::Tensor fake_output =
          discriminator->forward(fake_images.detach()).reshape({batch_size});
      torch::Tensor d_loss_fake =
          torch::binary_cross_entropy(fake_output, fake_labels);
      d_loss_fake.backward();

      torch::Tensor d_loss = d_loss_real + d_loss_fake;
      discriminator_optimizer.step();

      // Train generator.
      generator->zero_grad();
      fake_labels.fill_(1);
      fake_output = discriminator->forward(fake_images).reshape({batch_size});
      torch::Tensor g_loss =
          torch::binary_cross_entropy(fake_output, fake_labels);
      g_loss.backward();
      generator_optimizer.step();
      batch_index++;
      if (batch_index % kLogInterval == 0) {
        std::printf(""\r[%2ld/%2ld][%3ld/%3ld] D_loss: %.4f | G_loss: %.4f\n"",
                    epoch, kNumberOfEpochs, batch_index, batches_per_epoch,
                    d_loss.item<float>(), g_loss.item<float>());
      }
```

Call `.reshape({batch_size})` on results of the `forward` calls.",setting batch size work however significantly slow training network get bogus second dimension size one training one torch batch auto train discriminator real torch device torch torch device torch forward torch torch train discriminator fake torch noise torch device torch forward noise torch torch device torch forward torch torch torch train generator forward torch torch epoch float float call forward,issue,negative,negative,negative,negative,negative,negative
804335932,"@pritamdamania87 ,
I get the below exec logs
root@ixt-hw-01:/var/lib/jenkins/examples/distributed/rpc/pipeline# python main.py
Processing batch 0
[W tensorpipe_agent.cpp:546] RPC agent for worker1 encountered error when reading incoming request from master: EOF: end of file (this is expected to happen during shutdown)
[W tensorpipe_agent.cpp:546] RPC agent for worker2 encountered error when reading incoming request from master: EOF: end of file (this is expected to happen during shutdown)
[W tensorpipe_agent.cpp:546] RPC agent for worker2 encountered error when reading incoming request from worker1: EOF: end of file (this is expected to happen during shutdown)
Traceback (most recent call last):
  File ""main.py"", line 249, in <module>
    mp.spawn(run_worker, args=(world_size, num_split), nprocs=world_size, join=True)
  File ""/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py"", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File ""/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py"", line 188, in start_processes
    while not context.join():
  File ""/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py"", line 150, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException:

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py"", line 59, in _wrap
    fn(i, *args)
  File ""/var/lib/jenkins/examples/distributed/rpc/pipeline/main.py"", line 231, in run_worker
    run_master(num_split)
  File ""/var/lib/jenkins/examples/distributed/rpc/pipeline/main.py"", line 214, in run_master
    outputs = model(inputs)
  File ""/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 879, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/var/lib/jenkins/examples/distributed/rpc/pipeline/main.py"", line 169, in forward
    return torch.cat(torch.futures.wait_all(out_futures))
  File ""/opt/conda/lib/python3.6/site-packages/torch/futures/__init__.py"", line 196, in wait_all
    return [fut.wait() for fut in torch._C._collect_all(cast(List[torch._C.Future], futures)).wait()]
RuntimeError: RPCErr:1:RPC ran for more than set timeout (60000 ms) and will now be marked with an error

Breaking in rpc_async() while processing - ResNetShard2 times out. 
In case of ROCm, with increased timeout once the test case is executed the complied kernel are stored in cache and every subsequent execution works properly with 60 sec timeout.  Experimented with different timeout and found > 300 sec works for ROCm. Believe in case of CUDA all/most of the kernels are available at the start and 60 sec timeout is sufficient. ",get root python batch agent worker error reading incoming request master end file happen shutdown agent worker error reading incoming request master end file happen shutdown agent worker error reading incoming request worker end file happen shutdown recent call last file line module file line spawn return join daemon file line file line join raise process following error recent call last file line file line file line model file line result input file line forward return file line return fut cast list ran set marked error breaking time case test case executed kernel cache every subsequent execution work properly sec experimented different found sec work believe case available start sec sufficient,issue,negative,positive,neutral,neutral,positive,positive
804296493,"@pruthvistony I guess the default timeout is 60s, does it actually take > 60s to compile the kernels? I was wondering if you could point us to the line which actually times out with the defaults? I just wanted to double check if there wasn't something else that might be causing this.",guess default actually take compile wondering could point u line actually time double check something else might causing,issue,negative,neutral,neutral,neutral,neutral,neutral
795273541,"Could help me understand where the forward calculation happened? 
In my understand, whenever each Trainer run `model_output = self.param_server_rref.rpc_sync().forward(self.rank, x)` , the forward calculation is done by the ""parameter server"", because the `self.param_server_rref.owner()` is the ""parameter server"". And the RPC docs says `rref.rpc_sync()` run on worker `rref.owner()`. Plz correct me if I am misunderstand, thank you!",could help understand forward calculation understand whenever trainer run forward calculation done parameter server parameter server run worker correct misunderstand thank,issue,negative,neutral,neutral,neutral,neutral,neutral
795228605,"Hi, is there any examples for multiple gpu training? e.g. each gpu runs one trainer.",hi multiple training one trainer,issue,negative,neutral,neutral,neutral,neutral,neutral
792437144,"```bash
python main.py -a resnet50 \
--dist-url 'tcp://127.0.0.1:12306' \
--dist-backend 'nccl' \
--multiprocessing-distributed \
--world-size 1 --rank 0 \
--batch-size 1024 \
--evaluate \
--pretrained 
/data/ILSVRC 
```
## Result(OLD):

```bash
# real batch size is 128, we need 391 iterations
# all GPUs do the same thing
Use GPU: 0 for training
Use GPU: 1 for training
Use GPU: 2 for training
Use GPU: 3 for training
Use GPU: 7 for training
Use GPU: 5 for training
Use GPU: 6 for training
Use GPU: 4 for training
=> using pre-trained model 'resnet50'
=> using pre-trained model 'resnet50'
=> using pre-trained model 'resnet50'
=> using pre-trained model 'resnet50'
=> using pre-trained model 'resnet50'
=> using pre-trained model 'resnet50'
=> using pre-trained model 'resnet50'
=> using pre-trained model 'resnet50'
...
...
Test: [380/391] Time  1.111 ( 1.148)    Loss 7.9076e-01 (9.6697e-01)    Acc@1  76.56 ( 75.96)   Acc@5  97.66 ( 92.90)
Test: [380/391] Time  1.114 ( 1.151)    Loss 7.9076e-01 (9.6697e-01)    Acc@1  76.56 ( 75.96)   Acc@5  97.66 ( 92.90)
Test: [390/391] Time  1.428 ( 1.141)    Loss 1.7668e+00 (9.6293e-01)    Acc@1  45.00 ( 76.01)   Acc@5  87.50 ( 92.93)
 * Acc@1 76.012 Acc@5 92.934
Test: [390/391] Time  1.529 ( 1.143)    Loss 1.7668e+00 (9.6293e-01)    Acc@1  45.00 ( 76.01)   Acc@5  87.50 ( 92.93)
 * Acc@1 76.012 Acc@5 92.934
Test: [390/391] Time  1.577 ( 1.144)    Loss 1.7668e+00 (9.6293e-01)    Acc@1  45.00 ( 76.01)   Acc@5  87.50 ( 92.93)
 * Acc@1 76.012 Acc@5 92.934
Test: [370/391] Time  1.368 ( 1.206)    Loss 1.0068e+00 (9.6430e-01)    Acc@1  70.31 ( 76.01)   Acc@5  93.75 ( 92.94)
Test: [390/391] Time  1.460 ( 1.150)    Loss 1.7668e+00 (9.6293e-01)    Acc@1  45.00 ( 76.01)   Acc@5  87.50 ( 92.93)
 * Acc@1 76.012 Acc@5 92.934
Test: [390/391] Time  1.573 ( 1.151)    Loss 1.7668e+00 (9.6293e-01)    Acc@1  45.00 ( 76.01)   Acc@5  87.50 ( 92.93)
 * Acc@1 76.012 Acc@5 92.934
Test: [390/391] Time  1.768 ( 1.154)    Loss 1.7668e+00 (9.6293e-01)    Acc@1  45.00 ( 76.01)   Acc@5  87.50 ( 92.93)
Test: [390/391] Time  1.776 ( 1.151)    Loss 1.7668e+00 (9.6293e-01)    Acc@1  45.00 ( 76.01)   Acc@5  87.50 ( 92.93)
Test: [380/391] Time  1.062 ( 1.202)    Loss 7.9076e-01 (9.6697e-01)    Acc@1  76.56 ( 75.96)   Acc@5  97.66 ( 92.90)
Test: [390/391] Time  1.387 ( 1.201)    Loss 1.7668e+00 (9.6293e-01)    Acc@1  45.00 ( 76.01)   Acc@5  87.50 ( 92.93)
 * Acc@1 76.012 Acc@5 92.934
 * Acc@1 76.012 Acc@5 92.934
 * Acc@1 76.012 Acc@5 92.934
```

## Result(NEW):

```bash
# real batch size is 1024(128 per GPU), we need 49 iterations
Use GPU: 7 for training
Use GPU: 2 for training
Use GPU: 5 for training
Use GPU: 6 for training
Use GPU: 0 for training
Use GPU: 1 for training
Use GPU: 4 for training
Use GPU: 3 for training
=> using pre-trained model 'resnet50'
=> using pre-trained model 'resnet50'
=> using pre-trained model 'resnet50'
=> using pre-trained model 'resnet50'
=> using pre-trained model 'resnet50'
=> using pre-trained model 'resnet50'
=> using pre-trained model 'resnet50'
=> using pre-trained model 'resnet50'
Test: [ 0/49]   Time  5.584 ( 5.584)    Loss 9.5338e-01 (9.5338e-01)    Acc@1  75.10 ( 75.10)   Acc@5  92.87 ( 92.87)
Test: [ 0/49]   Time  5.491 ( 5.491)    Loss 9.5338e-01 (9.5338e-01)    Acc@1  75.10 ( 75.10)   Acc@5  92.87 ( 92.87)
Test: [ 0/49]   Time  5.806 ( 5.806)    Loss 9.5338e-01 (9.5338e-01)    Acc@1  75.10 ( 75.10)   Acc@5  92.87 ( 92.87)
...
...
Test: [40/49]   Time  1.089 ( 1.243)    Loss 1.0235e+00 (9.6341e-01)    Acc@1  73.63 ( 76.01)   Acc@5  93.55 ( 92.93)
Test: [40/49]   Time  1.089 ( 1.239)    Loss 1.0235e+00 (9.6341e-01)    Acc@1  73.63 ( 76.01)   Acc@5  93.55 ( 92.93)
 * Acc@1 76.012 Acc@5 92.934
 * Acc@1 76.012 Acc@5 92.934
 * Acc@1 76.012 Acc@5 92.934
 * Acc@1 76.012 Acc@5 92.934
 * Acc@1 76.012 Acc@5 92.934
 * Acc@1 76.012 Acc@5 92.934
 * Acc@1 76.012 Acc@5 92.934
 * Acc@1 76.012 Acc@5 92.934
```",bash python rank evaluate result old bash real batch size need thing use training use training use training use training use training use training use training use training model model model model model model model model test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss test time loss result new bash real batch size per need use training use training use training use training use training use training use training use training model model model model model model model model test time loss test time loss test time loss test time loss test time loss,issue,negative,negative,neutral,neutral,negative,negative
790204658,"Hi @pmeier, yes I believe the models should licensed under the same license file as the pytorch examples repo which is BSD-3-Clause",hi yes believe licensed license file clause,issue,negative,neutral,neutral,neutral,neutral,neutral
783755551,"I can also reformulate what I've changed.
I've replaced the outdated onnx_caffe2.backend with the current onnxruntime, since the old implementation doesn't seem to work anymore.",also reformulate outdated current since old implementation seem work,issue,negative,negative,negative,negative,negative,negative
782796733,"Changing the kernel size to 2 and the stride to 4 in the last Conv2D of the Discriminator seems to fix that error, but just want to make sure I'm not crazy here.",kernel size stride last discriminator fix error want make sure crazy,issue,negative,negative,neutral,neutral,negative,negative
782795811,"When I use the D and G code given above for 128 I am getting the following error:

```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-279-6b5de1c111f4> in <module>
     23         label = torch.full((b_size,), real_label, dtype=torch.float, device=device)
     24         # Forward pass real batch through D
---> 25         output = netD(real_cpu).view(-1)
     26         # Calculate loss on all-real batch
     27         errD_real = criterion(output, label)

~/metal-band-logo-generator/.ai/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    725             result = self._slow_forward(*input, **kwargs)
    726         else:
--> 727             result = self.forward(*input, **kwargs)
    728         for hook in itertools.chain(
    729                 _global_forward_hooks.values(),

<ipython-input-276-1702f960857a> in forward(self, input)
     30 
     31     def forward(self, input):
---> 32         return self.main(input)

~/metal-band-logo-generator/.ai/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    725             result = self._slow_forward(*input, **kwargs)
    726         else:
--> 727             result = self.forward(*input, **kwargs)
    728         for hook in itertools.chain(
    729                 _global_forward_hooks.values(),

~/metal-band-logo-generator/.ai/lib/python3.7/site-packages/torch/nn/modules/container.py in forward(self, input)
    115     def forward(self, input):
    116         for module in self:
--> 117             input = module(input)
    118         return input
    119 

~/metal-band-logo-generator/.ai/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    725             result = self._slow_forward(*input, **kwargs)
    726         else:
--> 727             result = self.forward(*input, **kwargs)
    728         for hook in itertools.chain(
    729                 _global_forward_hooks.values(),

~/metal-band-logo-generator/.ai/lib/python3.7/site-packages/torch/nn/modules/conv.py in forward(self, input)
    421 
    422     def forward(self, input: Tensor) -> Tensor:
--> 423         return self._conv_forward(input, self.weight)
    424 
    425 class Conv3d(_ConvNd):

~/metal-band-logo-generator/.ai/lib/python3.7/site-packages/torch/nn/modules/conv.py in _conv_forward(self, input, weight)
    418                             _pair(0), self.dilation, self.groups)
    419         return F.conv2d(input, weight, self.bias, self.stride,
--> 420                         self.padding, self.dilation, self.groups)
    421 
    422     def forward(self, input: Tensor) -> Tensor:

RuntimeError: Calculated padded input size per channel: (2 x 2). Kernel size: (4 x 4). Kernel size can't be greater than actual input size
```",use code given getting following error recent call last module label forward pas real batch output calculate loss batch criterion output label self input result input else result input hook fa forward self input forward self input return input self input result input else result input hook forward self input forward self input module self input module input return input self input result input else result input hook forward self input forward self input tensor tensor return input class self input weight return input weight forward self input tensor tensor calculated input size per channel kernel size kernel size ca greater actual input size,issue,negative,positive,positive,positive,positive,positive
778463703,"Paths within < > are placeholders, replace this with a valid path and allowed chars with paths.",within replace valid path,issue,negative,neutral,neutral,neutral,neutral,neutral
769972586,@osalpekar Thanks for all your help and thanks to @lasagnaphil for the patches! I have verified the build and everything works great! Ready for merging :),thanks help thanks build everything work great ready,issue,positive,positive,positive,positive,positive,positive
769713404,"> æˆ‘æ­£åœ¨ä½¿ç”¨Imagenetæ•°æ®é›†è®­ç»ƒResnetæ¨¡å‹ã€‚ç”±äºæŸäº›åŸå› ï¼Œç¬¬ä¸€æ‰¹åçš„ç²¾åº¦ä¸º100ï¼Œå¹¶ä¸”æŸå¤±æ°¸è¿œä¸ºé›¶ã€‚è¿™æ˜¯è¯¦ç»†ä¿¡æ¯ã€‚ä»»ä½•å¸®åŠ©æ˜¯æå¤§çš„èµèµã€‚æ˜¯ä»€ä¹ˆå¯¼è‡´æ­¤é”™è¯¯ã€‚
> 
> 2020-04-30 15:57:05-è°ƒè¯•-è¿è¡Œå‚æ•°ï¼šå‘½åç©ºé—´ï¼ˆbatch_size = 256ï¼Œdataset ='imagenet'ï¼Œepochs = 1ï¼Œvaluate = Noneï¼Œgpus ='0'ï¼Œinput_size = Noneï¼Œlr = 0.1ï¼Œ model ='resnet_binary'ï¼Œmodel_config =''ï¼ŒåŠ¨é‡= 0.9ï¼Œä¼˜åŒ–å™¨='SGD'ï¼Œprint_freq = 10ï¼Œresults_dir ='ã€‚/ results'ï¼Œresume =''ï¼Œsave ='2020-04-30_15-57-05 'ï¼Œstart_epoch = 0ï¼Œtype ='torch.FloatTensor'ï¼Œweight_decay = 0.0001ï¼Œworker = 8ï¼‰
> 2020-04-30 15:57:05-ä¿¡æ¯-åˆ›å»ºæ¨¡å‹resnet_binary
> 2020-04-30 15:57:05-ä¿¡æ¯-ä½¿ç”¨é…ç½®åˆ›å»ºçš„æ¨¡å‹ï¼š{'input_size'ï¼šæ— ï¼Œ'dataset'ï¼š'imagenet'}
> 2020-04-30 15:57:05-INFO-å‚æ•°æ•°é‡ï¼š11689512
> 2020-04-30 15:57:05-ä¿¡æ¯-åŸ¹è®­åˆ¶åº¦ï¼š{0ï¼š{'optimizer'ï¼š'SGD'ï¼Œ'lr'ï¼š0.1ï¼Œ'weight_decay'ï¼š0.0001ï¼Œ'momentum'ï¼š0.9}ï¼Œ30ï¼š{ 'lr'ï¼š0.01}ï¼Œ60ï¼š{'lr'ï¼š0.001ï¼Œ'weight_decay'ï¼š0}ï¼Œ90ï¼š{'lr'ï¼š0.0001}}
> 2020-04-30 15 : 57:05-è°ƒè¯•-ä¼˜åŒ–-è®¾ç½®æ–¹æ³•= SGD
> 2020-04-30 15:57:05-è°ƒè¯•-ä¼˜åŒ–-è®¾ç½®lr = 0.1
> 2020-04-30 15:57:05-è°ƒè¯•-ä¼˜åŒ–-è®¾ç½®åŠ¨é‡= 0.9
> 2020-04-30 15:57:05 -è°ƒè¯•-ä¼˜åŒ–-è®¾ç½®weight_decay = 0.0001
> 2020-04-30 15:58:06-ä¿¡æ¯-åŸ¹è®­-æ—¶æœŸï¼š[0] [0/176]æ—¶é—´60.928ï¼ˆ60.928ï¼‰æ•°æ®2.290ï¼ˆ2.290ï¼‰æŸå¤±71.5708ï¼ˆ71.5708ï¼‰ç²¾ç¡®@ 1 0.391ï¼ˆ0.391ï¼‰Prec @ 5 0.391ï¼ˆ0.391ï¼‰
> 2020-04-30 16:07:37-ä¿¡æ¯-åŸ¹è®­-çºªå…ƒï¼š[0] [10/176]æ—¶é—´54.159ï¼ˆ57.415ï¼‰æ•°æ®0.007ï¼ˆ0.211ï¼‰æŸå¤±0.0000ï¼ˆ6.5064ï¼‰Prec @ 1 100.000ï¼ˆ90.945ï¼‰Prec @ 5 100.000ï¼ˆ90.945ï¼‰
> 2020-04-30 16:16:51-ä¿¡æ¯-è®­ç»ƒ-æ—¶æœŸï¼š[0] [20/176]æ—¶é—´57.379ï¼ˆ56.445ï¼‰æ•°æ®0.000ï¼ˆ0.111ï¼‰æŸå¤±0.0000ï¼ˆ3.4081ï¼‰Prec @ 1 100.000ï¼ˆ95.257 ï¼‰Prec @ 5 100.000ï¼ˆ95.257ï¼‰
> 2020-04-30 16:26:52-ä¿¡æ¯-åŸ¹è®­-æ—¶æœŸï¼š[0] [30/176]æ—¶é—´59.362ï¼ˆ57.632ï¼‰æ•°æ®0.002ï¼ˆ0.076ï¼‰æŸå¤±0.0000ï¼ˆ2.3087ï¼‰Prec @ 1 100.000ï¼ˆ96.787ï¼‰Prec @ 5 100.000ï¼ˆ96.787ï¼‰
> 2020-04-30 16:35:55-ä¿¡æ¯-åŸ¹è®­-æ—¶æœŸï¼š[0] [40/176]æ—¶é—´53.450ï¼ˆ56.818ï¼‰æ•°æ®0.002ï¼ˆ0.058ï¼‰æŸå¤±0.0000ï¼ˆ 1.7456ï¼‰Prec @ 1 100.000ï¼ˆ97.571ï¼‰Prec @ 5 100.000ï¼ˆ97.571ï¼‰
> 2020-04-30 16:44:54-ä¿¡æ¯-åŸ¹è®­-çºªå…ƒï¼š[0] [50/176]æ—¶é—´54.136ï¼ˆ56.241ï¼‰æ•°æ®0.000ï¼ˆ0.047ï¼‰æŸå¤±0.0000ï¼ˆ1.4033ï¼‰Prec @ 1 100.000ï¼ˆ98.047ï¼‰Prec @ 5 100.000ï¼ˆ98.047ï¼‰
> 2020-04-30 16:53:55-ä¿¡æ¯-åŸ¹è®­-çºªå…ƒï¼š[0] [60/176]æ—¶é—´54.255ï¼ˆ55.887ï¼‰æ•°æ®0.002ï¼ˆ0.039ï¼‰æŸå¤±0.0000ï¼ˆ1.1733ï¼‰Prec @ 1 100.000ï¼ˆ98.367ï¼‰ ï¼‰Prec @ 5 100.000ï¼ˆ98.367ï¼‰
> 2020-04-30 17:02:56-ä¿¡æ¯-åŸ¹è®­-çºªå…ƒï¼š[0] [70/176]æ—¶é—´54.364ï¼ˆ55.641ï¼‰æ•°æ®0.000ï¼ˆ0.034ï¼‰æŸå¤±0.0000ï¼ˆ1.0080ï¼‰Prec @ 1 100.000ï¼ˆ98.597ï¼‰Prec @ 5 100.000ï¼ˆ98.597ï¼‰
> 2020-04-30 17:11:56-ä¿¡æ¯-åŸ¹è®­-æ—¶æœŸï¼š[0] [80/176]æ—¶é—´53.740ï¼ˆ55.440ï¼‰æ—¶é—´æ•°æ®0.003ï¼ˆ0.030ï¼‰æŸå¤±0.0000ï¼ˆ 0.8836ï¼‰Prec @ 1 100.000ï¼ˆ98.770ï¼‰Prec @ 5 100.000ï¼ˆ98.770ï¼‰
> 2020-04-30 17:20:58-ä¿¡æ¯-åŸ¹è®­-çºªå…ƒï¼š[0] [90/176]æ—¶é—´54.247ï¼ˆ55.305ï¼‰æ•°æ®0.000ï¼ˆ0.027ï¼‰æŸå¤±0.0000ï¼ˆ0.7865ï¼‰Prec @ 1 100.000ï¼ˆ98.905ï¼‰Prec @ 5 100.000ï¼ˆ98.905ï¼‰
> 2020-04-30 17:30:00-ä¿¡æ¯-åŸ¹è®­-çºªå…ƒï¼š[0] [100/176]æ—¶é—´54.309ï¼ˆ55.198ï¼‰æ•°æ®0.000ï¼ˆ0.024ï¼‰æŸå¤±0.0000ï¼ˆ0.7086ï¼‰Prec @ 1 100.000ï¼ˆ99.014 ï¼‰Prec @ 5 100.000ï¼ˆ99.014ï¼‰
> 2020-04-30 17:39:02-ä¿¡æ¯-åŸ¹è®­-æ—¶æœŸï¼š[0] [110/176]æ—¶é—´54.098ï¼ˆ55.102ï¼‰æ•°æ®0.000ï¼ˆ0.022ï¼‰æŸå¤±0.0000ï¼ˆ0.6448ï¼‰Prec @ 1 100.000ï¼ˆ99.103ï¼‰Prec @ 5 100.000ï¼ˆ99.103ï¼‰
> 2020-04-30 17:48:01-ä¿¡æ¯-è®­ç»ƒ-æ—¶æœŸï¼š[0] [120/176]æ—¶é—´53.637ï¼ˆ55.008ï¼‰æ•°æ®0.000ï¼ˆ0.021ï¼‰æŸå¤±0.0000ï¼ˆ 0.5915ï¼‰Prec @ 1 100.000ï¼ˆ99.177ï¼‰Prec @ 5 100.000ï¼ˆ99.177ï¼‰
> 2020-04-30 17:57:01-ä¿¡æ¯-åŸ¹è®­-çºªå…ƒï¼š[0] [130/176]æ—¶é—´53.865ï¼ˆ54.926ï¼‰æ•°æ®0.000ï¼ˆ0.019ï¼‰æŸå¤±0.0000ï¼ˆ0.5463ï¼‰Prec @ 1 100.000ï¼ˆ99.240ï¼‰Prec @ 5 100.000ï¼ˆ99.240ï¼‰
> 2020-04-30 18:06:02-ä¿¡æ¯-åŸ¹è®­-æ—¶æœŸï¼š[0] [140/176]æ—¶é—´55.712ï¼ˆ54.871ï¼‰æ•°æ®0.000ï¼ˆ0.018ï¼‰æŸå¤±0.0000ï¼ˆ0.5076ï¼‰Prec @ 1 100.000ï¼ˆ99.294 ï¼‰Prec @ 5 100.000ï¼ˆ99.294ï¼‰
> 2020-04-30 18:18:48-ä¿¡æ¯-åŸ¹è®­-çºªå…ƒï¼š[0] [150/176]æ—¶é—´111.563ï¼ˆ56.310ï¼‰æ•°æ®0.002ï¼ˆ0.017ï¼‰æŸå¤±0.0000ï¼ˆ0.4740ï¼‰Prec @ 1 100.000ï¼ˆ99.340ï¼‰Prec @ 5 100.000ï¼ˆ99.340ï¼‰
> 2020-04-30 18:37:27-ä¿¡æ¯-è®­ç»ƒ-æ—¶æœŸï¼š[0] [160/176]æ—¶é—´112.165ï¼ˆ59.761ï¼‰æ•°æ®0.001ï¼ˆ0.016ï¼‰æŸå¤±0.0000ï¼ˆ 0.4445ï¼‰Prec @ 1 100.000ï¼ˆ99.381ï¼‰Prec @ 5 100.000ï¼ˆ99.381ï¼‰
> 2020-04-30 20:48:58-ä¿¡æ¯-åŸ¹è®­-çºªå…ƒï¼š[0] [170/176]æ—¶é—´52.576ï¼ˆ102.412ï¼‰æ•°æ®0.003ï¼ˆ0.015ï¼‰æŸå¤±0.0000ï¼ˆ0.4185ï¼‰Prec @ 1 100.000ï¼ˆ99.417ï¼‰Prec @ 5 100.000ï¼ˆ99.417ï¼‰
> 2020-04-30 20:53:35-ä¿¡æ¯-è¯„ä¼°-æ—¶æœŸï¼š[0] [0/20]æ—¶é—´25.402ï¼ˆ25.402ï¼‰æ•°æ®2.827ï¼ˆ2.827ï¼‰æŸå¤±0.0000ï¼ˆ0.0000ï¼‰Prec @ 1 100.000ï¼ˆ100.000 ï¼‰Prec @ 5 100.000ï¼ˆ100.000ï¼‰
> 2020-04-30 20:57:27-ä¿¡æ¯-è¯„ä¼°-æ—¶æœŸï¼š[0] [10/20]æ—¶é—´21.789ï¼ˆ23.421ï¼‰æ•°æ®0.012ï¼ˆ0.261ï¼‰æŸå¤±0.0000ï¼ˆ0.0000ï¼‰Prec @ 1 100.000ï¼ˆ100.000ï¼‰Prec @ 5 100.000ï¼ˆ100.000ï¼‰
> 2020-04-30 21
> : 00:42-INFO-çºªå…ƒï¼š1åŸ¹è®­æŸå¤±0.4072åŸ¹è®­Prec @ 1 99.433åŸ¹è®­Prec @ 5 99.433éªŒè¯æŸå¤±0.0000éªŒè¯Prec @ 1 100.000éªŒè¯ç²¾ç¡®@ 5 100.000

Hello ! Has the problem of training accuracy you mentioned been resolved?",model hello problem training accuracy resolved,issue,negative,neutral,neutral,neutral,neutral,neutral
769478336,"> At a high level, this resulted from one trainer running a backwards pass while another was updating params with the optimizer. Still coordinating with folks internally to understand if this is expected behavior or not.

This makes sense. If the the param is modified between forward and backward passes, the autograd algorithm is no longer correct. Thanks for digging into this!

Regarding the fix, will it also work if we force a barrier before every `optimizer.step()`, which guarantees no unintentional param changes? In this way, we don't need multiple model copies. If the model is on CUDA, since all updates use the same default stream, they won't be race contention issues either. Not sure about CPU models though. ",high level one trainer running backwards pas another still internally understand behavior sense param forward backward algorithm longer correct thanks digging regarding fix also work force barrier every unintentional param way need multiple model model since use default stream wo race contention either sure though,issue,positive,positive,positive,positive,positive,positive
768842193,"> Change line number 201
> device = torch.device(""cuda:0"" if model.num_gpus > 0
> device = torch.device(""cuda:0"" if model.num_gpus > 0
> and torch.cuda.is_available() else ""cpu"")
> 
> to
> 
> ```
> device = torch.device(""cuda:0"" if model.num_gpus > 0 
>     and torch.cuda.is_available() else ""cpu"")
> ```

Sorry for the late reply, but not sure if I follow this. Aren't these the same lines of code? ",change line number device device else device else sorry late reply sure follow code,issue,negative,negative,negative,negative,negative,negative
768288273,"The code comments for the KL Divergence calculation cite Appendix B of https://arxiv.org/abs/1312.6114, which states that it's meant for the Gaussian case, so if this is intended then BCE (meant for Bernouille) isn't correct and should be replaced with $log p_theta(x | z)$.",code divergence calculation cite appendix meant case intended meant correct log,issue,negative,neutral,neutral,neutral,neutral,neutral
763897429,"You could have a look at https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder. That dataset requires your images to be laid out in subfolders, each one containing a single class - if you don't have any classes, you can just make a single dummy folder containing all your images.

In general though, you'll need to construct a `Dataset` that describes how to grab each image (by providing a `__getitem__` function describing how to grab a single image, or an `__iter__` function describing how to iterate through all the images), and then you'll fetch batches from this using a `DataLoader`.

These tutorials explain things very nicely and might help you:
https://pytorch.org/tutorials/beginner/data_loading_tutorial.html
https://pytorch.org/docs/stable/data.html#dataset-types (when deciding whether to use the `__getitem__` style or the `__iter__` style - I typically use the `__getitem__` style, or what they call ""map-style"")",could look laid one single class class make single dummy folder general though need construct grab image providing function grab single image function iterate fetch explain nicely might help whether use style style typically use style call,issue,positive,positive,neutral,neutral,positive,positive
762207303,"It seems like something related with the value of ""kBatchSize"". According to the warning, the output expect [64,1,1,64], the last number 64 should stand for the batch size.  I changed the value of kBatchSize from 64 to 1, and the warning disappeared.  But I am not familiar with ATen, could someone tell me the root cause?",like something related value according warning output expect last number stand batch size value warning familiar could someone tell root cause,issue,negative,positive,positive,positive,positive,positive
759702333,"Hey @yeongkwoncho 
Why is there no description provided for your issue?",hey description provided issue,issue,negative,neutral,neutral,neutral,neutral,neutral
759670359,"Hi @vat0599! 

Thank you for your pull request and welcome to our community. We require contributors to sign our Contributor License Agreement, and we don't seem to have you on file.

In order for us to review and merge your code, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

If you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Fexamples%20%23871). Thanks!",hi vat thank pull request welcome community require sign contributor license agreement seem file order u review merge code please sign behalf someone else employer individual may sufficient employer may need sign corporate received error please contact u thanks,issue,positive,positive,positive,positive,positive,positive
757218617,"> This is on my TODO for a while. Put it here in case I forget.
 Just a reminder!",put case forget reminder,issue,negative,neutral,neutral,neutral,neutral,neutral
755868613,"@osalpekar I agree that we can just stick to MPI for this example and so I removed the NCCL code for now. Regarding the move to ProcessGroupMPI, I made some initial changes to the code based on the c10d example [here](https://github.com/pytorch/pytorch/blob/master/torch/lib/c10d/example/allreduce.cpp) but it doesn't compile! I have a lot of questions which I put as comments in the code, can you please clarify them?",agree stick example removed code regarding move made initial code based example compile lot put code please clarify,issue,positive,neutral,neutral,neutral,neutral,neutral
754981171,"> @osalpekar @mrshenli Since we haven't heard back about the ProcessGroup C++ APIs (see comments above), I created a separate file that involves NCCL for GPU communication that is somewhat based on the NCCL Example 2 [here](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/examples.html). I am not sure how to combine the CPU and GPU versions into one file because the NCCL routines will lead to errors when run on CPUs. Any ideas on the next steps?

I added a comment above about the ProcessGroup C++ APIs - I think these should significantly simplify some of the initialization boilerplate that's here for NCCL. For the sake of this example, adding support for multiple ProcessGroups might be a bit too much since these examples should demonstrate to users how to efficiently use PyTorch Distributed training - we can stick to MPI for this example and include a comment that NCCL or Gloo can be used as alternatives. I think we should be in good shape to merge this in once we move to the MPI ProcessGroup.

We also have an ongoing discussion about native DDP C++ support (https://github.com/pytorch/pytorch/issues/48959), so please feel free to contribute ideas there as well!",since back see separate file communication somewhat based example sure combine one file lead run next added comment think significantly simplify sake example support multiple might bit much since demonstrate efficiently use distributed training stick example include comment used think good shape merge move also ongoing discussion native support please feel free contribute well,issue,positive,positive,positive,positive,positive,positive
752660685,"Hi @kleinicke! 

Thank you for your pull request and welcome to our community. We require contributors to sign our Contributor License Agreement, and we don't seem to have you on file.

In order for us to review and merge your code, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

If you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Fexamples%20%23863). Thanks!",hi thank pull request welcome community require sign contributor license agreement seem file order u review merge code please sign behalf someone else employer individual may sufficient employer may need sign corporate received error please contact u thanks,issue,positive,positive,positive,positive,positive,positive
751906440,"@osalpekar @mrshenli Since we haven't heard back about the ProcessGroup C++ APIs (see comments above), I created a separate file that involves NCCL for GPU communication that is somewhat based on the NCCL Example 2 [here](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/examples.html). I am not sure how to combine the CPU and GPU versions into one file because the NCCL routines will lead to errors when run on CPUs. Any ideas on the next steps?",since back see separate file communication somewhat based example sure combine one file lead run next,issue,negative,positive,positive,positive,positive,positive
747734995,"> I am seeing this when running it on Windows 10, it is solved when I set num_workers=0 for the DataLoader()

you saved me, man!! thanks.",seeing running set saved man thanks,issue,positive,positive,positive,positive,positive,positive
747434330,"try command like this:
`python main.py --dist-url 'tcp://127.0.0.1:9002' --dist-backend 'nccl' --multiprocessing-distributed --world-size 1 --rank 0 '/home/kkkkk/datasets/mini-WebFace'`",try command like python rank,issue,negative,negative,negative,negative,negative,negative
744306757,"Thank you for solving this problem, but does this solution mean that multiple workers train in series and cannot use multiple nodes to speed up the training process in parallel? e.g. I have 3 CPUs,1 PS, and 2  workers, and the workers' speed is v1 and v2, total training data size = n,now the training time = n/2v1 + n/2v2 but what we expect training time = n/2min(v1,v2)?",thank problem solution mean multiple train series use multiple speed training process parallel speed total training data size training time expect training time,issue,negative,negative,neutral,neutral,negative,negative
744283971,"Hi,

The underlying cause for this issue is due to concurrent updates during the backwards/optimizer step portion, which we are currently working on debugging. Essentially, this error means that a weight has been updated by the optimizer from another node while the backwards pass is running at the same time on the PS. If you need to unblock, a (wip) fix over at https://github.com/pytorch/examples/pull/842 effectively removes this issue by serializing the workers. ",hi underlying cause issue due concurrent step portion currently working essentially error weight another node backwards pas running time need unblock fix effectively issue,issue,negative,positive,positive,positive,positive,positive
744259379,"I got similar problems:
RuntimeError: Error on Node 0: one of the variables needed for gradient computation has been modified by an inplace operation: [CPUFloatType [64, 32, 3, 3]] is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
Have you ever solved it?
@mayurvaid @ollien @panchul 

",got similar error node one gradient computation operation version version instead hint enable anomaly detection find operation compute gradient true ever,issue,negative,positive,positive,positive,positive,positive
742015267,@liubamboo Maybe the code structure has been changed since I worked. I suggest you to open a new issue for better getting support.,maybe code structure since worked suggest open new issue better getting support,issue,positive,positive,positive,positive,positive,positive
741510172,I solve the bug. dcgan-sample-10.png should be replace by pt file. And the vitual env pytorch version update to 1.6.0,solve bug replace file version update,issue,negative,neutral,neutral,neutral,neutral,neutral
741423097,"@mahmoodn Hello, I have the same question. But I run the download script and put the dataset into ./data folder. The same bug appears:
terminate called after throwing an instance of 'c10::Error'
  what():  Error opening images file at ./data/train-images-idx3-ubyte

where did you make the data folder? in root dir or build folder dir? Thanks",hello question run script put folder bug terminate throwing instance error opening file make data folder root build folder thanks,issue,negative,positive,positive,positive,positive,positive
738920812,"they probably have too many parameters in their Linear layers and network bandwidth is the bottleneck.

Put the DDP only around the Convolutional trunk (that big Sequential) and see if that helps",probably many linear network bottleneck put around convolutional trunk big sequential see,issue,negative,positive,positive,positive,positive,positive
738201975,"this change doesn't make sense. it maximizes the generation always, instead of keeping a randomness to it.",change make sense generation always instead keeping randomness,issue,negative,neutral,neutral,neutral,neutral,neutral
735676961,"> Hi,i wanted to work with this issue

I have figured it out. The problem lies in the gradient clipping in [Line 181 of main.py](https://github.com/pytorch/examples/blob/master/word_language_model/main.py#L181), which is intended for RNN. After I removed this line of code, and set learning rate to 0.1, the genrated text with argmax is very fluent.

`# clip_grad_norm helps prevent the exploding gradient problem in RNNs / LSTMs.
torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)`

This issue really took me a lot of time!",hi work issue figured problem gradient clipping line intended removed line code set learning rate text fluent prevent gradient problem issue really took lot time,issue,negative,positive,positive,positive,positive,positive
734064820,"> @mszhanyi Can you please attach output of the training logs for Windows to the PR?

@chauhang, Thank your reply.

The log is below. The difference is no addr/port output for filestore.

```
python C:\Users\torch\Anaconda3\envs\benchmark\lib\site-packages\torch\distributed\launch.py --nnode=1 --node_rank=0 --nproc_per_node=4 example.py --local_world_size=4
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
*****************************************
[14416]: world_size = 4, rank = 0, backend=gloo
[12584]: world_size = 4, rank = 3, backend=gloo
[15084]: world_size = 4, rank = 1, backend=gloo
[14988]: world_size = 4, rank = 2, backend=gloo




[14416] rank = 0, world_size = 4, n = 1, device_ids = [0]
[14988] rank = 2, world_size = 4, n = 1, device_ids = [2]
[15084] rank = 1, world_size = 4, n = 1, device_ids = [1]
[12584] rank = 3, world_size = 4, n = 1, device_ids = [3]
```

",please attach output training thank reply log difference output python setting environment variable process default avoid system please tune variable optimal performance application rank rank rank rank rank rank rank rank,issue,positive,negative,negative,negative,negative,negative
733506327,@mszhanyi Can you please attach output of the training logs for Windows to the PR?,please attach output training,issue,negative,neutral,neutral,neutral,neutral,neutral
729503085,"Yes, the point is that one wants to _maximize_ (equiv. minimize the opposite of) the ELBO. Therefore the sign of the KLD term is correct.",yes point one minimize opposite therefore sign term correct,issue,negative,neutral,neutral,neutral,neutral,neutral
729447788,"> did you figure out the answer, or closed the issue for lack of activity?

I think he realized that there's no problem with the KLD term",figure answer closed issue lack activity think problem term,issue,negative,negative,neutral,neutral,negative,negative
727159029,"> Bumping again :)
> Besides using the distributed sampler wouldn't we need to also aggregate the metrics from the GPUs with all_gather?
> 
> I also have an example that runs on Apex and there I am able to simply run the validation on Rank 0 (simple if branching statement) but in normal PyTorch the same logic seems to lock up. Does someone know the reason for this difference?

In my env (Pytorch 1.5.1) , it does work and won't lock up.",bumping besides distributed sampler would need also aggregate metric also example apex able simply run validation rank simple branching statement normal logic lock someone know reason difference work wo lock,issue,negative,negative,neutral,neutral,negative,negative
724359763,It would also be really helpful is we could log the filename of the truncated image,would also really helpful could log truncated image,issue,negative,positive,positive,positive,positive,positive
724153675,">  In run_bigbatch.sh just use torch.distributed.launch --nprocs_per_node=2(since i had two gpus on one node) --nnodes=$1 --node_rank=$2 --master_addr=$3 --master_port=$4 main.py --my arguments

Thanks that was helpful :) 

Actually, the argument name is ""nproc_per_node"" (without the 's').",use since two one node thanks helpful actually argument name without,issue,negative,positive,neutral,neutral,positive,positive
722018013,"Ditto

```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap
    self.run()
  File ""/usr/local/lib/python3.8/multiprocessing/process.py"", line 108, in run
    self._target(*self._args, **self._kwargs)
  File ""rpc_parameter_server.py"", line 224, in run_worker
    run_training_loop(rank, num_gpus, train_loader, test_loader)
  File ""rpc_parameter_server.py"", line 183, in run_training_loop
    dist_autograd.backward(cid, [loss])
RuntimeError: Error on Node 0: one of the variables needed for gradient computation has been modified by an inplace operation: [CUDAFloatType [128, 10]], which is output 0 of TBackward, is at version 3; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
```",ditto recent call last file line file line run file line rank file line loss error node one gradient computation operation output version version instead hint operation compute gradient variable question anywhere later good luck,issue,negative,negative,neutral,neutral,negative,negative
720644846,"I seem to experience the exact same error, Is there a way to solve this problem?",seem experience exact error way solve problem,issue,negative,positive,positive,positive,positive,positive
719686439,"Hi @bddppq! 

Thank you for your pull request. We require contributors to sign our Contributor License Agreement, and yours needs attention.

You currently have a record in our system, but we do not have a signature on file. 

In order for us to review and merge your code, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

If you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Fexamples%20%23602). Thanks!",hi thank pull request require sign contributor license agreement need attention currently record system signature file order u review merge code please sign behalf someone else employer individual may sufficient employer may need sign corporate received error please contact u thanks,issue,positive,positive,neutral,neutral,positive,positive
716626533,"The issue is that you cannot pickle LMDB env objects. Setting num_workers=0 prevents the need to pickle anything since the main process original object handles retrieving data. 

The real solution is to store the Environment variable in a class with a custom __getitem__() and __setitem__() functions that delete the LMDB Environment variable from the returned dictionary and then regenerate it when loaded.",issue pickle setting need pickle anything since main process original object data real solution store environment variable class custom delete environment variable returned dictionary regenerate loaded,issue,positive,positive,positive,positive,positive,positive
713001673,"Resolved, going to change the `view` to `reshape`.

Performance impact seems unrelated to that change.",resolved going change view reshape performance impact unrelated change,issue,negative,neutral,neutral,neutral,neutral,neutral
711159318,"could the logic of detecting the image path change to a bit normal way? It really frustrate me, that I need add every folders an empty folder.",could logic image path change bit normal way really frustrate need add every empty folder,issue,negative,positive,neutral,neutral,positive,positive
710360140,"At first I thought it had to do with using AWS spot instances, but after further investigation it seems to happen for both spot and on demand instances.",first thought spot investigation happen spot demand,issue,negative,positive,positive,positive,positive,positive
709285051,"@samra-irshad 
I have used cluster to run distributed computing using two nodes. I used following (distributed_init_method is env:// as written on pytorch website):
def setup_env(distributed_init_method, local_rank):
    assert torch.cuda.is_available(), ' cuda not available' ;
    torch.distributed.init_process_group(
        backend='nccl',
        init_method= distributed_init_method,
    );
    rank= torch.distributed.get_rank();
    world_size= torch.distributed.get_world_size();
    #print(""rank: "",rank,"" local_rank:"",local_rank,"" world_size"",world_size);
    torch.cuda.set_device(local_rank);
    return rank,world_size;
    


This function should be called in the starting of your main program. Now how to get the ip addresses on cluster. You can do the following for PBS (slurm code might be bit different): 


FS=$'\n' read -d '' -r -a lines < ${PBS_NODEFILE}

echo $lines
########## THINGS TO CHANGE #################
### see the node no and then change and change rank to 0/1 later
MASTER=$lines
#MASTER=""gpu_""
RANK=0
#RANK=1
#########################################

MPORT=""6010""

 echo ""node : ${CURRENT_NODE%%.*} nnode: ${NNODES} rank: $RANK portno: ${MPORT}"" &
#ssh -q ${lines%%.*}
ssh -q $lines \
       $(bash ./run_bigbatch.sh ${NNODES} $RANK  $MASTER  ${MPORT})


###################################################


# At end we just see the stats
qstat -f ${PBS_JOBID}

Now make a second file exactly same as above  and just change the MASTER to $MASTER echoed by this file  and change RANK=1 on that file and run it. 
In run_bigbatch.sh  just use torch.distributed.launch  --nprocs_per_node=2(since i had two gpus on one node) --nnodes=$1 --node_rank=$2 --master_addr=$3 --master_port=$4 main.py --my arguments",used cluster run distributed two used following written assert available print rank rank return rank function starting main program get cluster following code might bit different read echo change see node change change rank later echo node rank rank bash rank master end see make second file exactly change master master file change file run use since two one node,issue,negative,negative,negative,negative,negative,negative
707265861,"the example started as a migration from torch7, but at some point, someone must've contributed a patch to mimic the Keras model.",example migration torch point someone must patch mimic model,issue,negative,neutral,neutral,neutral,neutral,neutral
706471010,"did you figure out the answer, or closed the issue for lack of activity?",figure answer closed issue lack activity,issue,negative,negative,neutral,neutral,negative,negative
702230610,"I just trained now a basic transformer model using this example code, and I don't have the issue that you describe. 
For example, for the same rng seed:

When using:
input = torch.Tensor([[123]]).long().to(device)
and when using
input = torch.Tensor([[12300]]).long().to(device)
I get completely different generated text.

I'm far from being NLP expert, but I have a suggestion - is it possible that your model is under-trained? 
what are your exact train args ?

This are the exact args I used for training:
python main.py --cuda --model Transformer --lr 5 --emsize 1500 --epochs 40

",trained basic transformer model example code issue describe example seed input device input device get completely different text far expert suggestion possible model exact train exact used training python model transformer,issue,negative,positive,neutral,neutral,positive,positive
701761418,"Can someone please help me with the next steps? I think the [run_python_examples.sh](https://github.com/pytorch/examples/blob/master/run_python_examples.sh) is only relevant for the python examples. It would be better to create something like run_cpp_examples.sh to test the cpp examples including this one.
cc @yf225 @glaringlee @mrshenli @malfet @seemethere @soumith ",someone please help next think relevant python would better create something like test one,issue,positive,positive,positive,positive,positive,positive
701157146,"@henbucuoshanghai Yes a university email would do . if you dont you can just download the torrents. They are official and with permissions granted from the official site I belive. Academic torrents is not some shady, Copy right infringement website. its supported by lots of universities including Stanford, MIT, etc. 
",yes university would dont official official site belive academic shady copy right infringement lot,issue,negative,positive,neutral,neutral,positive,positive
700512526,"You can download from the official site [here](http://image-net.org/download) (you need to register and get verified)
or you can download the training/validation set provided to the Academic torrents [Here](https://academictorrents.com/browse.php?search=ImageNet) ",official site need register get set provided academic,issue,negative,neutral,neutral,neutral,neutral,neutral
699488203,"`(BCE+KLD) / args.batch_size` would result in diving the loss function by a constant, which is a scaling operation which does not affect the loss function solutions.
See the original paper <https://arxiv.org/pdf/1312.6114.pdf> (equation 10) for a discussion about the specifics of the loss function",would result diving loss function constant scaling operation affect loss function see original paper equation discussion loss function,issue,negative,positive,positive,positive,positive,positive
691700443,"@soumith Can you elaborate on the issue here? The common factor in my code with this code for me is LMDB, and it produces the exact same error. Does this have something to do with trouble pickling the lmdb instance? ",elaborate issue common factor code code exact error something trouble instance,issue,negative,positive,neutral,neutral,positive,positive
691600928,"@stunbomb you can type in the following line
`python main.py -a mobilenet_v2 --pretrained=true`",type following line python,issue,negative,neutral,neutral,neutral,neutral,neutral
687870482,"I use python 3.7.3 and pytorch 1.6.0 but I also met the similar attribute error.
My error here is,
AttributeError: 'torch.device' object has no attribute 'src_pad_idx'.",use python also met similar attribute error error object attribute,issue,negative,neutral,neutral,neutral,neutral,neutral
679409162,@yf225  Please let me know if this PR can be a worthy contribution. Please suggest if further changes are required.,please let know worthy contribution please suggest,issue,positive,positive,positive,positive,positive,positive
678103236,"> When you run the training/evaluation from the command line, you have several options that you can use to impact how the model trains on your device. The threads option tells the script how many multiprocessing threads to use.
> 
> Multiprocessing on Windows is hard. For me, the workaround is simply not to use multiprocessing when doing any kind of analysis on Windows.
> 
> `--threads 0` basically turns off multiprocessing. The best way to do multiprocessing with CUDA in Windows is to not do it.

Ok, I konw.Thank you for response.",run command line several use impact model device option script many use hard simply use kind analysis basically turn best way response,issue,positive,positive,positive,positive,positive,positive
675571567,"When you run the training/evaluation from the command line, you have several options that you can use to impact how the model trains on your device. The threads option tells the script how many multiprocessing threads to use.

Multiprocessing on Windows is hard. For me, the workaround is simply not to use multiprocessing when doing any kind of analysis on Windows.

```--threads 0``` basically turns off multiprocessing. The best way to do multiprocessing with CUDA in Windows is to not do it.",run command line several use impact model device option script many use hard simply use kind analysis basically turn best way,issue,positive,positive,positive,positive,positive,positive
674770288,"If you're using PyCharm, I suggest putting a break at this line and running in debug mode:
`model = models.__dict__[args.arch]()`
Then, you can see the exact naming of the sub-variables in the model and change them dynamically after loading. For example, you can reassign model.conv1 (if there is such a sub-variable) to a new conv layer of your choice.

`model.conv1 = my_conv1`",suggest break line running mode model see exact naming model change dynamically loading example reassign new layer choice,issue,negative,positive,positive,positive,positive,positive
673662401,@malfet Are you familiar with this repo? Should we update run_python_examples.sh (add this test in) to make this test running on each code update? cc @mrshenli ,familiar update add test make test running code update,issue,negative,positive,positive,positive,positive,positive
673112445,I guess we can consider this done when a green checkmark shows up on a daily run in the [actions tab](https://github.com/pytorch/examples/actions),guess consider done green daily run tab,issue,negative,negative,neutral,neutral,negative,negative
673012544,"Ahh, cool! It seems someone thought about this problem before me :)

So in that case I think this is good to go",cool someone thought problem case think good go,issue,negative,positive,positive,positive,positive,positive
673011644,"@mattip This popped up yesterday on a fork I did. I think we might not have to worry about it. 

![image](https://user-images.githubusercontent.com/1500149/90047688-2e144f00-dc87-11ea-8faf-a3c96cfa7143.png)

",yesterday fork think might worry image,issue,negative,neutral,neutral,neutral,neutral,neutral
673008509,"@brianjo what about [setting up azure pipelines](https://github.com/marketplace/azure-pipelines) for this repo, then only forks that turn on azure pipelines would run the cron job in the first place. Its features are very comparable to github actions, in fact they share the same infrastrucure.",setting azure turn azure would run job first place comparable fact share,issue,negative,positive,positive,positive,positive,positive
672866572,"When the kernel size is even, it is less obvious which of the pixels should be at the origin, but this is not a problem. You have seen mostly odd-sized filter kernels because they are symmetric around the origin, which is a good property.
But in DCGAN , CNN layer with stride 2, this time the odd sized kernel convolution becomes not aligned one.
",kernel size even le obvious origin problem seen mostly filter symmetric around origin good property layer stride time odd sized kernel convolution becomes one,issue,negative,positive,positive,positive,positive,positive
672117724,multiprocessing in Windows is tough. A workaround however is training the model with `--threads 0`,tough however training model,issue,negative,negative,negative,negative,negative,negative
671588951,"Hello, i am also working with example code and trying to get it work with smaller resolution 16x16 images, but it doesnt work with those dimensions. How i need to change generator and discriminator code  for DCGAN to work with 16x16 images?",hello also working example code trying get work smaller resolution doesnt work need change generator discriminator code work,issue,negative,neutral,neutral,neutral,neutral,neutral
671398037,"@mattip I think you could add  `if: github.repository == â€˜pytorch/examplesâ€™ ` to the script after the schedule directive triggers it and it will just skip the rest of the script on a fork. 
",think could add script schedule directive skip rest script fork,issue,negative,neutral,neutral,neutral,neutral,neutral
671222859,"> Can't you use a conditional ...

I don't think a [conditional](https://docs.github.com/en/actions/reference/workflow-syntax-for-github-actions#jobsjob_idif) on a [schedule](https://docs.github.com/en/actions/reference/workflow-syntax-for-github-actions#onschedule) directive works, so we would have to add one to each step. And even then the cron job would trigger, it would start a worker and check out the repo, and only then try each step to discover they all are skipped.

Perhaps running the cron job on another CI system would be a better solution.",ca use conditional think conditional schedule directive work would add one step even job would trigger would start worker check try step discover perhaps running job another system would better solution,issue,positive,positive,positive,positive,positive,positive
671077675,"> Another option would be to move from github actions (which are available on all public repos) to travis or circleci, which would require action on part of the fork to enable it. I can pivot this PR to do that, but a admin would have to enable another CI service.

That's going to generate a lot of noise; with 6.5k forks and a good fraction of new forks missing that README note, that's a lot of cron jobs being kicked off. 

Can't you use a conditional with, e.g., `github.repository_owner` (see https://docs.github.com/en/actions/reference/context-and-expression-syntax-for-github-actions#github-context)",another option would move available public travis would require action part fork enable pivot would enable another service going generate lot noise good fraction new missing note lot ca use conditional see,issue,negative,positive,positive,positive,positive,positive
670957156,cc @yf225 @glaringlee @mrshenli Can someone help me with the next steps ? I am not sure how to add the dependencies and the tests.,someone help next sure add,issue,positive,positive,positive,positive,positive,positive
669918583,"> We didn't use `nn.Transformer` in the example. However, if you want, you could apply it in the same problem. Let's say you have the training data as ""train the model with transformer"". Then, feed the src sequence ""train the model with transformer"" to the encoder side and "" train the model with"" to the decoder side. Compare the output of transformer with ""train the model with transformer"".

I am not sure that feed ""train the model with"" to the decoder side makes the length of transformer's output is 4. So maybe compare output of transformer with ""the model with transformer"" ?",use example however want could apply problem let say training data train model transformer feed sequence train model transformer side train model side compare output transformer train model transformer sure feed train model side length transformer output maybe compare output transformer model transformer,issue,negative,positive,positive,positive,positive,positive
669649065,"> I am seeing this when running it on Windows 10, it is solved when I set num_workers=0 for the DataLoader()

Perfect solution, but what are the specific reasons~",seeing running set perfect solution specific,issue,positive,positive,positive,positive,positive,positive
668217675,"thanks for the contribution, but it doesn't seem like a good representative example of anything people would want to do, and hence learn from",thanks contribution seem like good representative example anything people would want hence learn,issue,positive,positive,positive,positive,positive,positive
668193508,"Also, for running tests, an MPI installation is necessary. How will that be added to the CI pipeline?",also running installation necessary added pipeline,issue,negative,neutral,neutral,neutral,neutral,neutral
668191177,"Should add dist-mnist after this line I think, define dist-mnist() and do proper cleanup:
https://github.com/pytorch/examples/blob/master/run_python_examples.sh#L189
cc @yf225 @mrshenli @soumyadipghosh ",add line think define proper cleanup,issue,negative,neutral,neutral,neutral,neutral,neutral
668180030,"Do we need to add this new test to some `.sh` file, or will the CI automatically detect and include all new tests?",need add new test file automatically detect include new,issue,negative,positive,positive,positive,positive,positive
668108051,"@yf225 @mrshenli 
I see there is a CI like test in this repo which run all the examples, any change that fail the execution will be detected in the test. For silence failure (logic change but won't stop the execution), can we add assert within the example so it breaks if any bc breaking (logical) change is made?",see like test run change fail execution test silence failure logic change wo stop execution add assert within example breaking logical change made,issue,negative,negative,negative,negative,negative,negative
668044441,"@yf225 This is good to me. I saw there is only 1 check (Run Examples) within this repo, is that as designed?
@mrshenli can you check the logic here, it seems fine to me though.",good saw check run within designed check logic fine though,issue,positive,positive,positive,positive,positive,positive
667805357, I am facing the same issue. Is there anyway to know the ip address of node in the HPC cluster from training or in general? So that I can set os.environ['MASTER_ADDR'] and os.environ['MASTER_PORT'] variables?,facing issue anyway know address node cluster training general set,issue,negative,positive,neutral,neutral,positive,positive
667608243,@glaringlee @mrshenli Curious would this be a good addition? Thanks!,curious would good addition thanks,issue,positive,positive,positive,positive,positive,positive
666422253,"Thanks for the reply. However, the section of code you indicate seems to correspond to the calculation of _G_ in the book's pseudo-code (see more complete pseudo-code box below). This portion of the pseudo-code (and the code you indicate) applies the discount _starting_ at the timestep _t_ until the end of the episode.

However, additionally, the book applies the discount rate from the _beginning_ of the episode up to _t_ in the last line of the pseudo-code. It seems to me that it is this application of the discounting rate that is missing in the code.

![image](https://user-images.githubusercontent.com/12362395/88937720-e2ab7b00-d25a-11ea-965a-ab6dbcea23e0.png)
",thanks reply however section code indicate correspond calculation book see complete box portion code indicate discount end episode however additionally book discount rate episode last line application rate missing code image,issue,negative,positive,neutral,neutral,positive,positive
666293346,"Actually the code implementation Î³áµ—. [105] - actor_critic.py https://github.com/pytorch/examples/blob/8df8e747857261ea481e0b2492413d52bf7cc3a8/reinforcement_learning/actor_critic.py#L105
The loop recursively multiplies the gamma with the discounted reward of the timestep after it and appends at the beginning of the list.

",actually code implementation loop gamma reward beginning list,issue,positive,neutral,neutral,neutral,neutral,neutral
662531046,"@muammar To approximate a gaussian posterior, it usually works fine to use no activation function in the last layer and interpret the output as mean for a normal distribution. If we assume a constant variance for the posterior, we naturally end up with the MSE as loss function. An alternative option is proposed by [An et al.](https://www.semanticscholar.org/paper/Variational-Autoencoder-based-Anomaly-Detection-An-Cho/061146b1d7938d7a8dae70e3531a00fceb3c78e8). We can duplicate the output layer of the decoder to model the mean and variance of the normal distribution and then optimize the negative log likelihood.",approximate posterior usually work fine use activation function last layer interpret output mean normal distribution assume constant variance posterior naturally end loss function alternative option al duplicate output layer model mean variance normal distribution optimize negative log likelihood,issue,negative,negative,neutral,neutral,negative,negative
660136408,"Hi hect1995, did this ever get fix? Or found a fix else where? Ran into the same issue. ",hi ever get fix found fix else ran issue,issue,negative,neutral,neutral,neutral,neutral,neutral
655482385,"I try to do this in https://github.com/triomino/examples/blob/master/imagenet/main.py. JoinableQueue is used to gather testing results because I am not familiar with other multiprocessing api.
Now the program works but I am not sure if it is a correct or good practice. I am also waiting for an official version mentioned in https://github.com/pytorch/examples/issues/461.

Update: use dist.all_reduce() to collect testing results.
Update: DistributedSampler cause incorrect validation accuracy, as mentioned in https://github.com/pytorch/pytorch/issues/25162.",try used gather testing familiar program work sure correct good practice also waiting official version update use collect testing update cause incorrect validation accuracy,issue,positive,positive,positive,positive,positive,positive
652988019,"when I trained main.py with resum weights, nothing happend

python main.py --resume model_best.pth 
=> creating model 'darknet64'
=> loading checkpoint 'model_best.pth'
=> loaded checkpoint 'model_best.pth' (epoch 1)
(/home/czl/anaconda3) czl@czl-MS-7B53:~/code-objectdetection/pytorch-classification$ 
",trained nothing python resume model loading loaded epoch,issue,negative,neutral,neutral,neutral,neutral,neutral
652776667,"I checked that this PR did not introduce breakage by running the tests locally. There is a problem with `dcgan` and PyTorch 1.7: it is using a deprecated construct. When I do a PR to add CI, that will need to be fixed to make the CI pass.

Edit: qualify blanket statement, maybe I made a mistake.",checked introduce breakage running locally problem construct add need fixed make pas edit qualify blanket statement maybe made mistake,issue,negative,positive,neutral,neutral,positive,positive
652619839,"Addressed @osalpekar's comments in #749 as well. Sorry, I should have address those in the previous PR. ",well sorry address previous,issue,negative,negative,negative,negative,negative,negative
652529969,"> I have the same issue as like you.
> The problem was the files in the Imagenet training/validation files.
> Just remove tar files in the each training/validation folder include including folders.
> 
> It works for me :)

Works for me too! I think it was because of the way the data loader was looking for the training samples. The error states that the labels t are not given from 0 to max_label. This is because there is no label for the tar files!",issue like problem remove tar folder include work work think way data loader looking training error given label tar,issue,negative,neutral,neutral,neutral,neutral,neutral
652413584,"Current master branch is Okay. KL is firstly increasing and then decreasing, if you train more epochs, you could find this trend. 
@PyExtreme @Coderx7 You could close it",current master branch firstly increasing decreasing train could find trend could close,issue,negative,positive,positive,positive,positive,positive
651944193,"The figure below shows the impact of `@rpc.functions.async_execution` for the reinforcement learning example, where batch processing significantly speeds up training.

![batch](https://user-images.githubusercontent.com/16999635/86158950-90e6c800-bad7-11ea-9c6f-eb1d4778f28b.png)
",figure impact reinforcement learning example batch significantly training batch,issue,negative,positive,positive,positive,positive,positive
651519798,"> I met the same problem. I solved it by trying a whole day. It is interesting but needs time to figure out.
> 
> Let's say I am going to use ImageFolder(""/train/"") to read jpg files in folder train.
> The file structure is
> /train/
> -- 1.jpg
> -- 2.jpg
> -- 3.jpg
> 
> I failed to load them, leading to errors:
> RuntimeError: Found 0 images in subfolders of: ./data
> Supported image extensions are: .jpg,.JPG,.jpeg,.JPEG,.png,.PNG,.ppm,.PPM,.bmp,.BMP
> 
> I read the solution above and tried tens of times. When I changed the structure to
> /train/1/
> -- 1.jpg
> -- 2.jpg
> -- 3.jpg
> But the read in code is still -- ImageFolder(""/train/""), IT WORKS.
> 
> It seems like the program tends to recursively read in files, that is convenient in some cases.

This saves my day. Perfectly working",met problem trying whole day interesting need time figure let say going use read folder train file structure load leading found image read solution tried time structure read code still work like program read convenient day perfectly working,issue,positive,positive,positive,positive,positive,positive
651304161,I came across the same issue. Could you share how you get the classification dataset? Thanks. ,came across issue could share get classification thanks,issue,positive,positive,positive,positive,positive,positive
649697510,"> You can install and run the nightly build using:
> 
> ```
> pip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html
> ```
> 
> Per pytorch.org install matrix. You might have to install nightlies for audio, vision and others to get this to work.

Sorry, for non cuda nightly builds on Linux, the pip command is: 

pip install numpy
pip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html 

Cheers",install run nightly build pip install torch per install matrix might install audio vision get work sorry non nightly pip command pip install pip install torch,issue,negative,negative,negative,negative,negative,negative
649037235,"You can install and run the nightly build using:
```pip install numpy
pip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html
```

Per pytorch.org install matrix. You might have to install nightlies for audio, vision and others to get this to work. ",install run nightly build pip install pip install torch per install matrix might install audio vision get work,issue,negative,neutral,neutral,neutral,neutral,neutral
645513634,"So far, I saw there has not been **distributed validation** implemented in this example. Is my understanding correct?",far saw distributed validation example understanding correct,issue,negative,positive,neutral,neutral,positive,positive
644112142,"Its actually caused by your environment. There are some inconsistencies. You should setup your environment with `python3.6`, `pytorch 1.0.1` and `torchvision 0.2.2` and your problems should fade :) ",actually environment setup environment fade,issue,negative,neutral,neutral,neutral,neutral,neutral
644045158,"@soumith  Hello, I have the same issue. And I have tried to set the multiprocessing start method to `spawn`, but it has no difference and the error still exists.

Could you please tell me another way to solve it?",hello issue tried set start method spawn difference error still could please tell another way solve,issue,negative,neutral,neutral,neutral,neutral,neutral
643398721,"at the end of your command that you run, instead of DIR, put the path to the folder containing your dataset (both train and val folders)",end command run instead put path folder train,issue,negative,neutral,neutral,neutral,neutral,neutral
643307573,"> The same question about this

Correct answer is to compute metric first and then average.
You can reference to this[ implementation of PSNR for PyTorch](https://github.com/photosynthesis-team/photosynthesis.metrics) and some other metrics too",question correct answer compute metric first average reference implementation metric,issue,negative,positive,neutral,neutral,positive,positive
642621952,"> I met the same problem. I solved it by trying a whole day. It is interesting but needs time to figure out.
> 
> Let's say I am going to use ImageFolder(""/train/"") to read jpg files in folder train.
> The file structure is
> /train/
> -- 1.jpg
> -- 2.jpg
> -- 3.jpg
> 
> I failed to load them, leading to errors:
> RuntimeError: Found 0 images in subfolders of: ./data
> Supported image extensions are: .jpg,.JPG,.jpeg,.JPEG,.png,.PNG,.ppm,.PPM,.bmp,.BMP
> 
> I read the solution above and tried tens of times. When I changed the structure to
> /train/1/
> -- 1.jpg
> -- 2.jpg
> -- 3.jpg
> But the read in code is still -- ImageFolder(""/train/""), IT WORKS.
> 
> It seems like the program tends to recursively read in files, that is convenient in some cases.


Still the same error, but this trick still works !!!

Thanks !!!",met problem trying whole day interesting need time figure let say going use read folder train file structure load leading found image read solution tried time structure read code still work like program read convenient still error trick still work thanks,issue,positive,positive,positive,positive,positive,positive
642036341,Hey @fivetwentysix PyTorch distributed does not support Windows yet. We are tracking requests for this feature here: https://github.com/pytorch/pytorch/issues/37068,hey distributed support yet feature,issue,negative,neutral,neutral,neutral,neutral,neutral
640360209,"HiÂ 

It was quite a while ago, but as far I remember, I've made a silly mistake in the PBS script, where I submitted the job to a wrong machine, where the node architecture was different than what I was intending.

Sent from Yahoo Mail on Android 
 
  On Mon., 8 Jun. 2020 at 3:49 am, Ivan Sekulic<notifications@github.com> wrote:   


@Akmazad would you mind sharing how you solved it? Thanks!

â€”
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub, or unsubscribe.
  
",hi quite ago far remember made silly mistake script job wrong machine node architecture different intending sent yahoo mail android wrote would mind thanks reply directly view,issue,negative,negative,neutral,neutral,negative,negative
640204545,"I Get this error 

Error on Node 0: one of the variables needed for gradient computation has been modified by an inplace operation: [CPUFloatType [64, 32, 3, 3]] is at version 3; expected version 2 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!

@panchul , Are you getting the similar error",get error error node one gradient computation operation version version instead hint operation compute gradient variable question anywhere later good luck getting similar error,issue,negative,positive,positive,positive,positive,positive
638228486,Thanks for quick reply! Can you close this issue?,thanks quick reply close issue,issue,negative,positive,positive,positive,positive,positive
638221598,"Thank you very much ,i was a newbie when i asked this question,now i have solved it.But still thank you for your time




------------------&nbsp;åŸå§‹é‚®ä»¶&nbsp;------------------
å‘ä»¶äºº: ""bassbone""<notifications@github.com&gt;; 
å‘é€æ—¶é—´: 2020å¹´6æœˆ3æ—¥(æ˜ŸæœŸä¸‰) æ™šä¸Š10:03
æ”¶ä»¶äºº: ""pytorch/examples""<examples@noreply.github.com&gt;; 
æŠ„é€: ""Dans le ciel Ã©claire le gar&amp;#xE7;on""<2297349886@qq.com&gt;; ""Mention""<mention@noreply.github.com&gt;; 
ä¸»é¢˜: Re: [pytorch/examples] help (#580)





 
@zyj2297349886 Could you tell us more specifically about your failure?
 
â€”
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub, or unsubscribe.",thank much question still thank time gar mention mention help could tell u specifically failure reply directly view,issue,positive,negative,neutral,neutral,negative,negative
638219249,@zyj2297349886 Could you tell us more specifically about your failure?,could tell u specifically failure,issue,negative,negative,negative,negative,negative,negative
637979712,"@panchul I can only run parameter server on two nodes. When using three nodes, error arises.",run parameter server two three error,issue,negative,neutral,neutral,neutral,neutral,neutral
637975514,"@osalpekar Dear Osalpekar, I can run parameter server on two nodes without GPU. However, when running on three nodes, I got the following error: RuntimeError: Error on Node 0: one of the variables needed for gradient computation has been modified by an operation: [CPUFloatType [9216, 128]], which is output 0 of TBackward, is at version 5; expected version 4 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",dear run parameter server two without however running three got following error error node one gradient computation operation output version version instead hint enable anomaly detection find operation compute gradient true,issue,negative,positive,positive,positive,positive,positive
637559756,"I encountered the same problem and managed to fix it by modifying line 136 to make it similar to the corresponding line (# 45) of RNNModel's init_weights() method:

`nn.init.zeros_(self.decoder.weight)`",problem fix line make similar corresponding line method,issue,negative,neutral,neutral,neutral,neutral,neutral
635599847,"> Please also fix this line https://github.com/pytorch/examples/blob/master/regression/main.py#L60.
> 
> Same issue

Create a PR https://github.com/pytorch/examples/pull/781",please also fix line issue create,issue,positive,neutral,neutral,neutral,neutral,neutral
635586951,"Please also fix this line https://github.com/pytorch/examples/blob/master/regression/main.py#L60.

Same issue",please also fix line issue,issue,negative,neutral,neutral,neutral,neutral,neutral
630957191,"@yf225 I am almost done with the code. However, i am facing an issue in DataLoader part. I have raised an issue in pytorch forum as well. Here is the link:
https://discuss.pytorch.org/t/custom-dataloader/81874/3

Please, help me resolve this issue. I am extremely sorry for the bad indendation.",almost done code however facing issue part raised issue forum well link please help resolve issue extremely sorry bad,issue,positive,negative,negative,negative,negative,negative
626186996,"@nalinzie Make sure to check what your dataloader is outputting is also a single-channel image.
https://github.com/pytorch/examples/blob/b9f3b2ebb9464959bdbf0c3ac77124a704954828/dcgan/main.py#L60

You can also do a print(X.size()) right before you put anything in and out of your model to check what dimensions your tensors are actually.",make sure check also image also print right put anything model check actually,issue,negative,positive,positive,positive,positive,positive
625369717,"> @nalinzie If you share the code here then it's maybe possible to help.

[https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html](url)
i got the code from this site. The example code from this site works for the RGB image. But i am working on my own grayscale image. Therefore, I changed the number of channels nc as 1. besides that, I just keep the same. However, when i was trying to train the model, the error occured saying RuntimeError: Given groups=1, weight of size 64 1 4 4, expected input[128, 3, 64, 64] to have 1 channels, but got 3 channels instead. I dont know which part should I change to change my input to have 1 channel. 
",share code maybe possible help got code site example code site work image working image therefore number besides keep however trying train model error saying given weight size input got instead dont know part change change input channel,issue,negative,neutral,neutral,neutral,neutral,neutral
625144692,@nalinzie If you share the code here then it's maybe possible to help.,share code maybe possible help,issue,positive,neutral,neutral,neutral,neutral,neutral
625013668,"Hi, I am also trying to implement DCGAN for grayscale image using pytorch. But i got error saying 'RuntimeError: Given groups=1, weight of size 64 1 4 4, expected input[128, 3, 64, 64] to have 1 channels, but got 3 channels instead'. I already set the number of channel as 1 but still got error. do you happen to know where can I fix the problem.",hi also trying implement image got error saying given weight size input got instead already set number channel still got error happen know fix problem,issue,negative,neutral,neutral,neutral,neutral,neutral
623655435,"@codeprb Is your dataset folder structure clean? Make sure there are no unnecessary folders, zip files, floating files.",folder structure clean make sure unnecessary zip floating,issue,positive,positive,positive,positive,positive,positive
623266267,Were you able to find any solution for this @SBoulanger ? I also have the same problem.,able find solution also problem,issue,negative,positive,positive,positive,positive,positive
623160342,"> Hey Rakesh, I think part of the challenge was that you tagged/asked @osalpekar and he gave you his opinion and he didn't have information on how pytorch/examples is maintained.
> 
> We generally don't accept new examples unless they bring a lot of new value to the table. The goal of the `examples/` repo is not to maximize the number of examples, but to instead have a minimal set of concise examples with maximum information to the end user.
> 
> Because the entire turnaround of ""asking about submitting a PR"" to ""submitting a PR"" happened so quickly (within 48 hours), I missed it and didn't have a chance to respond.
> 
> I've been trying to figure out what to do with the PR, whether we should accept it or not. I don't have an answer yet. I am weighing the fact that we cover RNN as an example already in `word_language_model` and in the snli` examples.

Thanks @soumith for reaching out. I think this PR is good for beginner level example of RNN. It is minimal and simple application of RNN. ",hey think part challenge gave opinion information generally accept new unless bring lot new value table goal maximize number instead minimal set concise maximum information end user entire turnaround quickly within chance respond trying figure whether accept answer yet weighing fact cover example already thanks reaching think good beginner level example minimal simple application,issue,positive,positive,positive,positive,positive,positive
623156327,"Hey Rakesh, I think part of the challenge was that you tagged/asked @osalpekar and he gave you his opinion and he didn't have information on how pytorch/examples is maintained.

We generally don't accept new examples unless they bring a lot of new value to the table. The goal of the `examples/` repo is not to maximize the number of examples, but to instead have a minimal set of concise examples with maximum information to the end user.

Because the entire turnaround of ""asking about submitting a PR"" to ""submitting a PR"" happened so quickly (within 48 hours), I missed it and didn't have a chance to respond.

I've been trying to figure out what to do with the PR, whether we should accept it or not. I don't have an answer yet. I am weighing the fact that we cover RNN as an example already in `word_language_model` and in the snli` examples.

",hey think part challenge gave opinion information generally accept new unless bring lot new value table goal maximize number instead minimal set concise maximum information end user entire turnaround quickly within chance respond trying figure whether accept answer yet weighing fact cover example already,issue,positive,positive,neutral,neutral,positive,positive
623044083,Yes. After I upgrade the driver of GPU and install the latest version of Pytorch. I solved it.,yes upgrade driver install latest version,issue,negative,positive,positive,positive,positive,positive
621071663,"Hi @osalpekar, the PR is not moving forward. I am not sure, how much time it generally takes. I have resolved review comments. Please let me know something is required from me.",hi moving forward sure much time generally resolved review please let know something,issue,positive,positive,positive,positive,positive,positive
619071384,"> LGTM! Thanks for contributing! I added some cosmetic comments on formatting.
> 
> One general comment on the folder structure is whether we should move the three mnist examples from the root into a common parent folder.
> 
> And I am not sure who will be giving stamps to example contributions for vision applications.
> 
> cc @fmassa (for vision app) @zhangguanheng66 (for RNN) @soumith (general guidelines)

Hi @soumith , cc @mrshenli ,

We are not sure who will be giving stamps to this example contribution. This example uses `MNIST`(vision) dataset and it is good introductory example of `RNN`.

Thanks and regards,
Rakesh",thanks added cosmetic one general comment folder structure whether move three root common parent folder sure giving example vision vision general hi sure giving example contribution example vision good introductory example thanks,issue,positive,positive,positive,positive,positive,positive
618809104,"Problem solved
------------------------------
auto ReadRsv(const std::string path) {
	HANDLE filea= CreateFileA((LPCSTR)path.c_str(), GENERIC_READ, FILE_SHARE_READ | FILE_SHARE_WRITE, NULL, OPEN_EXISTING, FILE_FLAG_SEQUENTIAL_SCAN,NULL);
	int cout;
	int length;
	ReadFile(filea, &cout,4,NULL,NULL);
	std::vector<std::tuple<torch::Tensor, torch::Tensor>> rsv;
	byte* dataa = new byte[784];
	byte* datab = new byte[1];
	DWORD hasread;
	for (int i = 0; i<cout; ++i) {		
		ReadFile(filea, &length, 4, &hasread, NULL);
		ReadFile(filea, &dataa[0], 784, &hasread, NULL);
		torch::Tensor line = torch::from_blob(&dataa, { 784 },torch::kByte);
		ReadFile(filea, &datab[0], 1, &hasread, NULL);
		torch::Tensor label = torch::from_blob(&datab, { 1 }, torch::kByte);
		rsv.push_back(std::make_tuple(line,label));		
	}
	delete []dataa;
	delete []datab;
	CloseHandle(filea);
	return rsv;
}",problem auto path handle null null length null torch torch new new length null null torch line torch torch null torch label torch torch line label delete delete return,issue,negative,positive,positive,positive,positive,positive
617545809,"> LGTM! Thanks for contributing! I added some cosmetic comments on formatting.
> 
> One general comment on the folder structure is whether we should move the three mnist examples from the root into a common parent folder.
> 
> And I am not sure who will be giving stamps to example contributions for vision applications.
> 
> cc @fmassa (for vision app) @zhangguanheng66 (for RNN) @soumith (general guidelines)
Hi @mrshenli ,

Thanks for reviews.

I suggest that root folder should correspond to specific Data science area or ML algorithm, since user intent to visit an example will be based on such topics.

Regards,
Rakesh",thanks added cosmetic one general comment folder structure whether move three root common parent folder sure giving example vision vision general hi thanks suggest root folder correspond specific data science area algorithm since user intent visit example based,issue,positive,positive,neutral,neutral,positive,positive
617542294,"@zhangguanheng66 yes we can try `Transformer` in another example. 

Most people think MNIST is good introductory dataset for RNN and it is also one of the official RNN examples of `Keras tensorflow` as shown [here](https://www.tensorflow.org/guide/keras/rnn#load_mnist_dataset)

I am not sure if MNIST is good introductory dataset for `Transformer`",yes try transformer another example people think good introductory also one official shown sure good introductory transformer,issue,positive,positive,positive,positive,positive,positive
616070867,"detach just reduce the work that  G() gradient upgrade in training step of D(), because G() will train in next step",detach reduce work gradient upgrade training step train next step,issue,negative,neutral,neutral,neutral,neutral,neutral
614936652,@deepali-c The last time we were working on structures for functions like ```log_softmax``` to replace ```torch::nn::Functional```'s. I think they should be used here.,last time working like replace torch think used,issue,negative,neutral,neutral,neutral,neutral,neutral
614890561,"Hey @rakesh-malviya , Feel free to put up a PR, and we'll take a look!",hey feel free put take look,issue,positive,positive,positive,positive,positive,positive
614604427,"@soumith, @ShahriarSS, the above solution resolves this issue. Do you think this can be added as fix to the example? ",solution issue think added fix example,issue,negative,neutral,neutral,neutral,neutral,neutral
613885946,"> I met the same problem. I solved it by trying a whole day. It is interesting but needs time to figure out.
> 
> Let's say I am going to use ImageFolder(""/train/"") to read jpg files in folder train.
> The file structure is
> /train/
> -- 1.jpg
> -- 2.jpg
> -- 3.jpg
> 
> I failed to load them, leading to errors:
> RuntimeError: Found 0 images in subfolders of: ./data
> Supported image extensions are: .jpg,.JPG,.jpeg,.JPEG,.png,.PNG,.ppm,.PPM,.bmp,.BMP
> 
> I read the solution above and tried tens of times. When I changed the structure to
> /train/1/
> -- 1.jpg
> -- 2.jpg
> -- 3.jpg
> But the read in code is still -- ImageFolder(""/train/""), IT WORKS.
> 
> It seems like the program tends to recursively read in files, that is convenient in some cases.

Thanks, it's working",met problem trying whole day interesting need time figure let say going use read folder train file structure load leading found image read solution tried time structure read code still work like program read convenient thanks working,issue,positive,positive,positive,positive,positive,positive
613522455,"I also get the same error on windows 10 and NVIDIA TITAN.
Pytorch version:  1.4.0

But the same code runs on linux with the same python and pytorch version.
Code can be found here:
https://github.com/ArashJavan/DeepLIO


```
    self = reduction.pickle.load(from_parent)
EOFError: Ran out of input
```
",also get error version code python version code found self ran input,issue,negative,neutral,neutral,neutral,neutral,neutral
612486446,"> Is the checkpoint save logic for ImageNet's multiprocessing_distributed trainning above may save a checkpoint on each node ?

Yes, the code looks to me will save a checkpoint on every node. 

> I think only the rank 0 need save checkpoint.

This is true. But saving it on every node isn't wrong, and potentially will be faster when restoring from checkpoints. Because, if only rank 0 saves the model, when restoring, the the program will need to first load the checkpoint on rank 0 node, and then broadcast the model states to all other ranks. It's like file IO + network IO, and during this period of time, other ranks have to idle waiting. However, if every machine saves a checkpoint, then all nodes just need to read the checkpoint from local disk, and very likely will finish around the same time, avoiding additional delay caused by another broadcast.",save logic may save node yes code save every node think rank need save true saving every node wrong potentially faster rank model program need first load rank node broadcast model like file io network io period time idle waiting however every machine need read local disk likely finish around time additional delay another broadcast,issue,positive,negative,negative,negative,negative,negative
612207147,"Same question - given that the models across GPUs are synced after optimizer.step(), then every validate run is effectively the same. As an optimization, if we run validate in a distributed manner (like training), then how do we average the accuracies across gpus and nodes to decide saving checkpoints?",question given across every validate run effectively optimization run validate distributed manner like training average across decide saving,issue,positive,positive,positive,positive,positive,positive
610069718,"@soumith The majority of the code is reviewed by @albanD in https://github.com/pytorch/tutorials/pull/921, and this PR is ready to be merged. Thanks!",majority code ready thanks,issue,positive,positive,positive,positive,positive,positive
609359487,"Hey! I have the same question. 
Do you have the answer right now?

Base on the answer in this [post](https://discuss.pytorch.org/t/save-model-for-distributeddataparallel/47129), **pietern** replay that: ""The model weights should be identical across processes and only the BN stats should be different.""
So I think that mean all the model save from each node should be the same. But I am not so sure about that, and I don't have the machine to prove this.
",hey question answer right base answer post replay model identical across different think mean model save node sure machine prove,issue,positive,negative,neutral,neutral,negative,negative
605246677,Indeed. The relevant explanation is on page 331.,indeed relevant explanation page,issue,negative,positive,positive,positive,positive,positive
602041539,"If you want to switch to SGD then you should also use a lower learning rate, e.g. 0.001 or so.",want switch also use lower learning rate,issue,negative,neutral,neutral,neutral,neutral,neutral
601941301,"@mrshenli Updated to address all comments, and also to explicitly move tensors in and out of GPU so that they work with the latest RPC, which disallows sending CUDA tensors. Could you take another look? Thanks!",address also explicitly move work latest sending could take another look thanks,issue,negative,positive,positive,positive,positive,positive
597050598,"> @bterwijn Curious are you using the master branch of pytorch/examples repo? We should have already fixed this by using torch::Reduction::Sum instead of Reduction::Sum in #697.

the master branch doesnt have a Directory ""cpp"" with cpp examples. So we have to use the ""cpp"" branch instead. This error is still present as I'm having the same compiling error with my arch distro and libtorch 1.4:
`make[1]: Entering directory '/home/shyney/libtorch-cpp-examples/examples-cpp/cpp/mnist'
make[2]: Entering directory '/home/shyney/libtorch-cpp-examples/examples-cpp/cpp/mnist'
make[2]: Leaving directory '/home/shyney/libtorch-cpp-examples/examples-cpp/cpp/mnist'
make[2]: Entering directory '/home/shyney/libtorch-cpp-examples/examples-cpp/cpp/mnist'
[ 50%] Building CXX object CMakeFiles/mnist.dir/mnist.cpp.o
/home/shyney/libtorch-cpp-examples/examples-cpp/cpp/mnist/mnist.cpp: In function â€˜void test(Net&, c10::Device, DataLoader&, size_t)â€™:
/home/shyney/libtorch-cpp-examples/examples-cpp/cpp/mnist/mnist.cpp:102:22: error: â€˜Reductionâ€™ has not been declared
  102 |                      Reduction::Sum)
      |                      ^~~~~~~~~
make[2]: *** [CMakeFiles/mnist.dir/build.make:63: CMakeFiles/mnist.dir/mnist.cpp.o] Error 1
make[2]: Leaving directory '/home/shyney/libtorch-cpp-examples/examples-cpp/cpp/mnist'
make[1]: *** [CMakeFiles/Makefile2:76: CMakeFiles/mnist.dir/all] Error 2
make[1]: Leaving directory '/home/shyney/libtorch-cpp-examples/examples-cpp/cpp/mnist'
make: *** [Makefile:84: all] Error 2
` 
#672  

EDIT:

Ok nevermind I was using the fork from @goldsborough 
https://github.com/goldsborough/examples/tree/cpp
As this is the link posted on the libtorch documentation page. I would suggest to update the documentation page on pytorch.org to this repo as it seems to be more up to date.",curious master branch already fixed torch instead reduction master branch doesnt directory use branch instead error still present error arch make entering directory make entering directory make leaving directory make entering directory building object function void test net error reduction declared reduction make error make leaving directory make error make leaving directory make error edit fork link posted documentation page would suggest update documentation page date,issue,negative,neutral,neutral,neutral,neutral,neutral
597042451,"In the main.py, the criterion implemented with:
          criterion = nn.NLLLoss()       #   main.py  line104
In the case of TransformerModel, there has log_softmax function:
          return F.log_softmax(output, dim=-1)     #model.py  line150
But in the case of RNNModel (--model = LSTM), I cann't find some funtion related to softmax ?
Or is it not necessary to use function related to softmax, and only torch.nn.NLLLoss( ) was enough ?",criterion criterion line case function return output line case model find related necessary use function related enough,issue,negative,neutral,neutral,neutral,neutral,neutral
596487985,I have the same issue. Is there anyone who solved this issue? Please help me.,issue anyone issue please help,issue,positive,neutral,neutral,neutral,neutral,neutral
596050211,"Hi, I am a begineer for NLP and pytorch.

In the case of ""--model LSMT"" , does it need something like ""torch.nn.Softmax"" or ""torch.nn.LogSoftmax"" operation before the computation of nn.NLLLoss( ) criterion ?",hi case model need something like operation computation criterion,issue,negative,neutral,neutral,neutral,neutral,neutral
595774538,"I'm not sure if this is a correct place to ask this question, but doesn't this mask mean the network has access to the very word it's supposed to guess?",sure correct place ask question mask mean network access word supposed guess,issue,negative,positive,neutral,neutral,positive,positive
595277403,"The issue has been tracked in a separate issue.
https://github.com/pytorch/vision/issues/1938

Feel free to close this issue to avoid a duplicate one. Thanks.",issue tracked separate issue feel free close issue avoid duplicate one thanks,issue,positive,positive,positive,positive,positive,positive
594975166,"i'm declining to merge this into the example, which needs to be kept concise as a first principle",merge example need kept concise first principle,issue,negative,positive,positive,positive,positive,positive
590405375,"Regardless of the drawbacks of this specific implementation, it does have the benefit of being relatively simple so it might be worth having it as a PyTorch example? :)",regardless specific implementation benefit relatively simple might worth example,issue,positive,positive,neutral,neutral,positive,positive
589257420,@avinashsai Thanks a lot for your interest and yes we would really appreciate your contribution! :D  Please let us know if you need anything from us.,thanks lot interest yes would really appreciate contribution please let u know need anything u,issue,positive,positive,positive,positive,positive,positive
587860688,"The C++ Frontend went through many changes in the past few months. PR (https://github.com/pytorch/pytorch/pull/31005) made `Conv{1,2,3}dOptions` and `ConvTranspose{1,2,3}dOptions` different classes. For users who see this issue, consider updating your GitHub repo with the latest changes (if you haven't already), and also with the latest Libtorch version.

The updated examples have working version of this. For non-sequential models, all you need to change is: use `torch::nn::ConvTranspose2dOptions` without `.transposed(true)` argument. And it should work! If you are using a sequential model, check the tutorials on C++ Frontend OR check the description in the PR (https://github.com/pytorch/pytorch/pull/31005). ",went many past made different class see issue consider latest already also latest version working version need change use torch without true argument work sequential model check check description,issue,negative,positive,positive,positive,positive,positive
587583614,"This pull request solves the problem.  While I am just one man in this 7 billion world, my vote would be to merge it.",pull request problem one man billion world vote would merge,issue,negative,neutral,neutral,neutral,neutral,neutral
587582533,Using the fix from #686 the script works for me.  Not sure how to get this merged but the fix works,fix script work sure get fix work,issue,negative,positive,positive,positive,positive,positive
587109976,@bterwijn Curious are you using the master branch of pytorch/examples repo? We should have already fixed this by using `torch::Reduction::Sum` instead of `Reduction::Sum` in https://github.com/pytorch/examples/pull/697.,curious master branch already fixed torch instead reduction,issue,negative,neutral,neutral,neutral,neutral,neutral
586198230,"The `--batch-size` in the command line argument seems to mean ""batch size per node"".",command line argument mean batch size per node,issue,negative,negative,negative,negative,negative,negative
585071037,"https://jaan.io/what-is-variational-autoencoder-vae-tutorial/

maybe the size of the batch doesn't related to it.",maybe size batch related,issue,negative,neutral,neutral,neutral,neutral,neutral
583920355,"I also have confusion on this particular statement. The right way I think is to divide the (global) batch size by the total number of samplers, i.e args.batch_size = int(args.batch_size / ngpus_per_node * world_size). ""world_size"" refers to the number of nodes (servers), not the number of total processes/GPUs.  ",also confusion particular statement right way think divide global batch size total number number number total,issue,negative,positive,neutral,neutral,positive,positive
583714585,"The image files do not exist in that path. (https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh)

If you want to run the APP, you need to download the image file from elsewhere. 
You can get it from the path below.

- http://www.image-net.org/

@soumith
Your script doesn't work. Could you help or any comments?",image exist path want run need image file elsewhere get path script work could help,issue,negative,neutral,neutral,neutral,neutral,neutral
582417460,"> I met the same problem. I solved it by trying a whole day. It is interesting but needs time to figure out.
> 
> Let's say I am going to use ImageFolder(""/train/"") to read jpg files in folder train.
> The file structure is
> /train/
> -- 1.jpg
> -- 2.jpg
> -- 3.jpg
> 
> I failed to load them, leading to errors:
> RuntimeError: Found 0 images in subfolders of: ./data
> Supported image extensions are: .jpg,.JPG,.jpeg,.JPEG,.png,.PNG,.ppm,.PPM,.bmp,.BMP
> 
> I read the solution above and tried tens of times. When I changed the structure to
> /train/1/
> -- 1.jpg
> -- 2.jpg
> -- 3.jpg
> But the read in code is still -- ImageFolder(""/train/""), IT WORKS.
> 
> It seems like the program tends to recursively read in files, that is convenient in some cases.

å…„å¼Ÿä½ æ˜¯çœŸæ»´åˆšï¼Œæˆ‘è¯•äº†ä¸€ä¸‹ä¸è¡Œï¼Œæˆ‘çš„structureå¦‚ä¸‹ï¼šF:\Chen Qixun\Documents\MachineLearningProject\cats_and_dogs_small\train\cats\ 1.jpg 2.jpg...
æˆ‘çš„code : ImageFolder('F:/Chen Qixun/Documents/MachineLearningProject/dogs-vs-cats/train/', transform)
è¿˜æ˜¯æŠ¥é”™ï¼Œæˆ‘å¯ä¸æƒ³èŠ±ä¸€å¤©æè¿™ä¸ªï¼Œè¯·é—®ä½ è¿˜æœ‰åˆ«çš„æ€è·¯ä¹ˆï¼Œæˆ‘çš„structureåº”è¯¥æ²¡æœ‰é—®é¢˜æŠŠï¼Ÿ",met problem trying whole day interesting need time figure let say going use read folder train file structure load leading found image read solution tried time structure read code still work like program read convenient transform,issue,positive,positive,positive,positive,positive,positive
581243631,"@Johnson-yue : Whats the status of this PR? 
Does it not add any value to the current sample we have? 
would appreciate if you could decide. 
Thanks in advance",whats status add value current sample would appreciate could decide thanks advance,issue,positive,positive,neutral,neutral,positive,positive
580708861,"The problem described [here](https://stackoverflow.com/questions/30968573/stdbind-and-perfect-forwarding)

The fast solution - specify exact type of overloaded function passed to Functional:

`    push_back(Functional(static_cast<torch::Tensor(&)(const torch::Tensor&, int64_t, torch::optional<torch::ScalarType> )>(torch::log_softmax), 1, torch::nullopt));
`

",problem fast solution specify exact type function functional functional torch torch torch torch torch torch,issue,negative,positive,positive,positive,positive,positive
580319207,"> This doesn't queue up the work for execution in the background, correct? It looks like the last item in the batch will call set_result which would unblock the fut.result() calls that are waiting in the other items being processed.

Yes, it is what is implemented in this example.

> Also, this means that all callers would be blocked until all batch_size calls to the servers have been made, correct? Would the main use case of this be the same client calling the batch method on the server concurrently, and then doing a single wait for the final list of results?

Right, this is not meant to be the optimal solution for this use case. Just trying to show:

1. how to expose conventional RPC APIs, where server binds functions and client call them.
2. how batching can be implemented. 

If we want to make this efficient, the batching implementation will need to move to C++.",queue work execution background correct like last item batch call would unblock waiting yes example also would blocked made correct would main use case client calling batch method server concurrently single wait final list right meant optimal solution use case trying show expose conventional server client call want make efficient implementation need move,issue,positive,positive,neutral,neutral,positive,positive
580317361,"> -  It looks like you are calling fn on each element of the batch rather
than once on the whole batch. But that's something we can fix.

Right, I feel the batching behavior should be application specific, e.g., the application might want to create a tensor as buffer for multiple requests or it might need to run some custom aggregate fn before processing each of the requests. How did you make it generic in your current implementation? 

> - Unfortunately we determined that this ""batched callback"" approach is not
sufficient because e.g. if my batch is 100 and I have 40 clients sending
requests in parallel, I hit a deadlock. 

This can be done by using `rpc_async` and rate limit it on the client side I guess? 

> We solved this in TB by havin a
server-controlled queue that could request a batch with a given min/max
size and timeout, and/or ways to ask for the queue size. This gives a lot
of flexibility on the server side e.g. to decide which queue (i.e. which
RPC) to service in which order.

This should be possible too, similar to [this example](https://github.com/pytorch/examples/pull/692), where the server launches nested RPCs.

> - There's a thread lock, does that mean that rpc will be calling the
callbacks from multiple threads? 

Yes, each RPC launched from the client will be handled by a thread on the server side. So, they can happen concurrently. 

> If so, I don't think the locking is
correct, although technically you don't need any lock since the GIL should
cover it.

True, lock is not necessary in this example, but would be necessary if we add torchscript decorator.

In terms of C++ implementation, it can be done at different levels

Option 1: implement a custom [`RequestCallbackImpl`](https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/rpc/request_callback_impl.h) to handle request batching and [plug it in to RPC agent](https://github.com/pytorch/pytorch/blob/9ce25cce9197b86f6340036bc6224d5b75ff242d/torch/csrc/distributed/rpc/process_group_agent.cpp#L96). The existing ComputationQueue can be added to it as well I think. This only requires minimum change and can keep that critical path for other applications intact (because not every application requires the batching feature).

Option 2: implement a custom [rpc agent](https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/rpc/rpc_agent.h). This would also give you the access to the thread management on the server side. You probably don't want to worry about the messaging layer. If so, we can split the current agent into separate comm module and callback/thread management module. 

",like calling element batch rather whole batch something fix right feel behavior application specific application might want create tensor buffer multiple might need run custom aggregate make generic current implementation unfortunately determined approach sufficient batch sending parallel hit deadlock done rate limit client side guess queue could request batch given size way ask queue size lot flexibility server side decide queue service order possible similar example server thread lock mean calling multiple yes client handled thread server side happen concurrently think locking correct although technically need lock since cover true lock necessary example would necessary add decorator implementation done different option implement custom handle request plug agent added well think minimum change keep critical path intact every application feature option implement custom agent would also give access thread management server side probably want worry layer split current agent separate module management module,issue,positive,positive,neutral,neutral,positive,positive
579819830,"The main use case for this would be a bunch of threads/processes all
calling the batch method concurrently, and then waiting for a result. In
some cases, the clients might want to use an async version to queue up
multiple requests before waiting on them (e.g. if I have to evaluate 7
models for the 7 players, to take 1 game step).

Looking closely, there are a number of implementation details that look
weird to me:
-  It looks like you are calling fn on each element of the batch rather
than once on the whole batch. But that's something we can fix.
- Unfortunately we determined that this ""batched callback"" approach is not
sufficient because e.g. if my batch is 100 and I have 40 clients sending
requests in parallel, I hit a deadlock. We solved this in TB by havin a
server-controlled queue that could request a batch with a given min/max
size and timeout, and/or ways to ask for the queue size. This gives a lot
of flexibility on the server side e.g. to decide which queue (i.e. which
RPC) to service in which order.
- There's a thread lock, does that mean that rpc will be calling the
callbacks from multiple threads? If so, I don't think the locking is
correct, although technically you don't need any lock since the GIL should
cover it.

On Tue, Jan 28, 2020 at 7:39 PM Rohan Varma <notifications@github.com>
wrote:

> This doesn't queue up the work for execution in the background, correct?
> It looks like the last item in the batch will call set_result which would
> unblock the fut.result() calls that are waiting in the other items being
> processed.
>
> Also, this means that all callers would be blocked until all batch_size
> calls to the servers have been made, correct? Would the main use case of
> this be the same client calling the batch method on the server
> concurrently, and then doing a single wait for the final list of results?
>
> â€”
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/pytorch/examples/pull/702?email_source=notifications&email_token=ABLQEDKOQ54U4N7GLPC4FVTRADF3DA5CNFSM4KMWN5M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEKFRNAI#issuecomment-579540609>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ABLQEDNE4O7IGICGNYB7WWLRADF3DANCNFSM4KMWN5MQ>
> .
>
",main use case would bunch calling batch method concurrently waiting result might want use version queue multiple waiting evaluate take game step looking closely number implementation look weird like calling element batch rather whole batch something fix unfortunately determined approach sufficient batch sending parallel hit deadlock queue could request batch given size way ask queue size lot flexibility server side decide queue service order thread lock mean calling multiple think locking correct although technically need lock since cover tue rohan wrote queue work execution background correct like last item batch call would unblock waiting also would blocked made correct would main use case client calling batch method server concurrently single wait final list reply directly view,issue,negative,negative,neutral,neutral,negative,negative
579540609,"This doesn't queue up the work for execution in the background, correct? It looks like the last item in the batch will call `set_result` which would unblock the `fut.result()` calls that are waiting in the other items being processed. 

Also, this means that all callers would be blocked until all batch_size calls to the servers have been made, correct? Would the main use case of this be the same client calling the batch method on the server concurrently, and then doing a single wait for the final list of results?",queue work execution background correct like last item batch call would unblock waiting also would blocked made correct would main use case client calling batch method server concurrently single wait final list,issue,negative,positive,neutral,neutral,positive,positive
571681853,"@cheerss see https://arxiv.org/abs/1404.5997 [[pdf](https://arxiv.org/pdf/1404.5997.pdf)] (esp. sections 1-3):

> ... data parallelism is efficient when the amount of computation per weight is high (because
the weight is the unit being communicated)

It's not efficient to use DataParallel with the fully connected layers because they have lots of parameters, but relatively little computation. In theory, the fully connected layers are well suited for model parallelism, but it's kind of a pain to implement and the performance gain is small.",see data parallelism efficient amount computation per weight high weight unit efficient use fully connected lot relatively little computation theory fully connected well model parallelism kind pain implement performance gain small,issue,positive,positive,neutral,neutral,positive,positive
571561508,"Is there anyone know why ""big fully connected layers at the end are not suitable to DataParallel""?",anyone know big fully connected end suitable,issue,negative,positive,positive,positive,positive,positive
569306111,"cc: @yf225 @smessmer

I am not sure if I should merge this right now, or wait for the 14 transition to finish",sure merge right wait transition finish,issue,negative,positive,positive,positive,positive,positive
568178553,"> hi, thanks for help! this code is running, but no communication/synchronization among processes. Is there anything missing in the commit?

How did you find `no communication/synchronization among processes`?",hi thanks help code running among anything missing commit find among,issue,positive,neutral,neutral,neutral,neutral,neutral
566631485,"I ran a benchmark to test the performance of a linux workstation. I found that adding additional GPUs to the task (training imagenet) increased the number of images processed per second so that using 2 GPUs processed 1.6 times more images per second and using 4 GPUs processed about 3.5 times more images per second. Note, this benchmark tested for the average number of images processed per second over the first 100 iterations of training. Nevertheless, I am trying to resolve why previous results showed that the time to complete 90 epoch did not decrease when more GPUs were assigned to the task.",ran test performance found additional task training number per second time per second time per second note tested average number per second first training nevertheless trying resolve previous time complete epoch decrease assigned task,issue,negative,positive,neutral,neutral,positive,positive
565431890,"I guess you are referring to this line. 

`returns = (returns - returns.mean()) / (returns.std() + eps)`

Here he is preparing the performance measure J(theta) or the loss function in PyTorch as normalized returns.  PG theorem says that the gradient of the performance measure is proportional to Return * derivate of log of policy. Since we are using montecorlo the variance in the returns will be high across episodes.

By normalizing the returns, we center the mean and reduce the variance to 1 which will help the convergence of gradient ascent",guess line performance measure theta loss function theorem gradient performance measure proportional return derivate log policy since variance high across center mean reduce variance help convergence gradient ascent,issue,negative,negative,neutral,neutral,negative,negative
564494734,I have the same question. I ran the imagenet code but found no acceleration when using multiple GPUs.,question ran code found acceleration multiple,issue,negative,neutral,neutral,neutral,neutral,neutral
564008295,"ndf = number of filters in the discriminator
ngf = number of filters in the generator",number discriminator number generator,issue,negative,neutral,neutral,neutral,neutral,neutral
563218790,"@zhangguanheng66 I want to read my own dataset which is provided by my instructor. But, I did not understand how can I read",want read provided instructor understand read,issue,negative,neutral,neutral,neutral,neutral,neutral
563004039,Which dataaset are you talking about? You may reach to a specific domain to see how to create a dataset pipeline.,talking may reach specific domain see create pipeline,issue,negative,neutral,neutral,neutral,neutral,neutral
562855207,"Hello,

```
for i in range(future):# if we should predict the future
            h_t, c_t = self.lstm1(c_t2, (h_t, c_t))
            h_t2, c_t2 = self.lstm2(c_t, (h_t2, c_t2))
            outputs += [c_t2]
```
Is this correct because range(future) do nothing. Every time lstm1 gets same c_t2, that leads to same result?",hello range future predict future correct range future nothing every time result,issue,negative,neutral,neutral,neutral,neutral,neutral
561515920,"Dear darkmatter18
Thank you for your advice.
I could build mnist.cpp without error.  ",dear thank advice could build without error,issue,positive,neutral,neutral,neutral,neutral,neutral
560468357,"OK, I see. You want to extract the pattern from a sequence. IMO, during training, you could pass two constants to the decoder and the input sequence to the encoder. Then, feed the output of decoder to `CrossEntropyLoss` together with the target sequence.",see want extract pattern sequence training could pas two input sequence feed output together target sequence,issue,negative,neutral,neutral,neutral,neutral,neutral
560461981,"I check nn.Transformer for predicting next item, on a toy dataset, that is for 0, 1, 0, 1, 0, 1, I get two item sequence 0, 1, and for 1, 0, 1, 0, 1, 0, I get two item sequence 1, 0, like finding a pattern in input sequence.

The softmax probabilities represent whether neural network predicts 1 or 0 for given input sequence.

But is it correct way if I already specify the output in dataset, and should I set target embedding requires_grad to False?

Or am I making some mistake here, and Transformer is not applicable for such a problem.",check next item toy get two item sequence get two item sequence like finding pattern input sequence represent whether neural network given input sequence correct way already specify output set target false making mistake transformer applicable problem,issue,negative,negative,negative,negative,negative,negative
560456129,"I took a look at your code. It seems like a right way. However, depending on your problem, you may need `mask` to shield the inputs. Could you briefly describe your problem here?",took look code like right way however depending problem may need mask shield could briefly describe problem,issue,negative,positive,positive,positive,positive,positive
560449235,"We didn't use `nn.Transformer` in the example. However, if you want, you could apply it in the same problem. Let's say you have the training data as ""train the model with transformer"". Then, feed the src sequence ""train the model with transformer"" to the encoder side and ""<BOL> train the model with"" to the decoder side. Compare the output of transformer with ""train the model with transformer"". ",use example however want could apply problem let say training data train model transformer feed sequence train model transformer side train model side compare output transformer train model transformer,issue,negative,neutral,neutral,neutral,neutral,neutral
557820583,@shersoni610 Thanks a lot for the report! https://github.com/pytorch/examples/pull/669 should fix the issue. @soumith Would you like to review the PR to the examples repo? :D,thanks lot report fix issue would like review,issue,positive,positive,positive,positive,positive,positive
557683804,Try running `python -m spacy download en`,try running python spacy en,issue,negative,neutral,neutral,neutral,neutral,neutral
555609841,I also trained on the imagenet dataset using ResNet50 in Pytorch and found 2.85 days for 90 epoch using 1x GPU (GTX 1080 Ti) and 3.25 days using 4x GPU (GTX 1080 Ti). I also followed the instruction `$ python main.py -a resnet50 --dist-url 'tcp://127.0.0.1:FREEPORT' --dist-backend 'nccl' --multiprocessing-distributed --world-size 1 --rank 0 [imagenet-folder with train and val folders]`,also trained found day epoch ti day ti also instruction python rank train,issue,negative,negative,negative,negative,negative,negative
554695319,May be computed across all the training image of MNIST?,may across training image,issue,negative,neutral,neutral,neutral,neutral,neutral
553518795,"@xiaopengguo1406  I guess the function definitions in the link below should help. [https://github.com/pytorch/vision/blob/95131de394543a7c34bd51932bdfce21dae516c1/torchvision/utils.py#L91](url)
If the normalize argument is set to True, then whatever range of the output netG produces, even if it is outside [-1,1], the normalization will bring it in the default range of [0,1] through a linear transformation.",guess function link help normalize argument set true whatever range output even outside normalization bring default range linear transformation,issue,negative,positive,positive,positive,positive,positive
553073523,"It's a typo, IMO. You are welcome to submit a PR and fix the doc. Thanks.",typo welcome submit fix doc thanks,issue,positive,positive,positive,positive,positive,positive
552721106,"you simply adjust the learning rate according to the number of processes, so that you scale the sum down to average",simply adjust learning rate according number scale sum average,issue,negative,negative,neutral,neutral,negative,negative
551994075,"yes, I didn't notice it.
removed these attributes after your comment",yes notice removed comment,issue,negative,neutral,neutral,neutral,neutral,neutral
551672028,"thank you.
apparently I don't see your comments on the diff's page, can you please send me the direct link where I can look at the changes you requested. ",thank apparently see page please send direct link look,issue,positive,positive,neutral,neutral,positive,positive
551264650,@soumith I think we could close this issue as we released v1.3.1. The mask generated by v1.3.1 should be right.,think could close issue mask right,issue,negative,positive,positive,positive,positive,positive
551162141,"Indeed if we split the float cast and the masked_fill operations, the Transformer LM works as expected:
```
def _generate_square_subsequent_mask(self, sz):
    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
    mask = mask.float()
    mask = mask.masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
    return mask
```
",indeed split float cast transformer work self mask mask mask mask float mask float return mask,issue,negative,neutral,neutral,neutral,neutral,neutral
550017509,"I second @colesbury comment.

Alternatively, you can install a nightly build of both pytorch an torchvision ",second comment alternatively install nightly build,issue,negative,neutral,neutral,neutral,neutral,neutral
549995417,You will have to build and install torchvision from source as well. The torchvision package now has binary components that depend on a specific version of the torch package. (cc @fmassa),build install source well package binary depend specific version torch package,issue,negative,neutral,neutral,neutral,neutral,neutral
549548981,"> Hi, I found that this change did lead to the following error, while I solved it by removing the `at::` prefix. FYI, I'm using the latest stable binary libtorch for cuda 10.1 on ubuntu 16.04.
> 
> ```
> testroot@gpu:~/CNN/test_libtorch/mnist/build$ make
> Scanning dependencies of target mnist
> [ 50%] Building CXX object CMakeFiles/mnist.dir/mnist.cpp.o
> /home/testroot/CNN/test_libtorch/mnist/mnist.cpp: In function â€˜void test(Net&, c10::Device, DataLoader&, size_t)â€™:
> /home/testroot/CNN/test_libtorch/mnist/mnist.cpp:102:26: error: â€˜at::Reductionâ€™ has not been declared
>                       at::Reduction::Sum)
>                           ^
> CMakeFiles/mnist.dir/build.make:62: recipe for target 'CMakeFiles/mnist.dir/mnist.cpp.o' failed
> make[2]: *** [CMakeFiles/mnist.dir/mnist.cpp.o] Error 1
> CMakeFiles/Makefile2:72: recipe for target 'CMakeFiles/mnist.dir/all' failed
> make[1]: *** [CMakeFiles/mnist.dir/all] Error 2
> Makefile:83: recipe for target 'all' failed
> make: *** [all] Error 2
> ```

Well, I think that's because the latest stable version 1.3 was released 25 days [https://github.com/pytorch/pytorch/releases](https://github.com/pytorch/pytorch/releases), but  [3397d4](https://github.com/pytorch/pytorch/commit/3397d41b8af2824495faaefc649c3c7e47f03bfb)  is committed 20 days ago. 
You can simply remove the at:: prefix in your experiment to make it work, or use the nightly version instead of stable version. ",hi found change lead following error removing prefix latest stable binary make scanning target building object function void test net error declared recipe target make error recipe target make error recipe target make error well think latest stable version day day ago simply remove prefix experiment make work use nightly version instead stable version,issue,negative,positive,positive,positive,positive,positive
549231395,"Hi, I found that this change did lead to the following error, while I solved it by removing the `at::` prefix. FYI, I'm using the latest stable binary libtorch for cuda 10.1 on ubuntu 16.04.

```
testroot@gpu:~/CNN/test_libtorch/mnist/build$ make
Scanning dependencies of target mnist
[ 50%] Building CXX object CMakeFiles/mnist.dir/mnist.cpp.o
/home/testroot/CNN/test_libtorch/mnist/mnist.cpp: In function â€˜void test(Net&, c10::Device, DataLoader&, size_t)â€™:
/home/testroot/CNN/test_libtorch/mnist/mnist.cpp:102:26: error: â€˜at::Reductionâ€™ has not been declared
                      at::Reduction::Sum)
                          ^
CMakeFiles/mnist.dir/build.make:62: recipe for target 'CMakeFiles/mnist.dir/mnist.cpp.o' failed
make[2]: *** [CMakeFiles/mnist.dir/mnist.cpp.o] Error 1
CMakeFiles/Makefile2:72: recipe for target 'CMakeFiles/mnist.dir/all' failed
make[1]: *** [CMakeFiles/mnist.dir/all] Error 2
Makefile:83: recipe for target 'all' failed
make: *** [all] Error 2
```",hi found change lead following error removing prefix latest stable binary make scanning target building object function void test net error declared recipe target make error recipe target make error recipe target make error,issue,negative,positive,positive,positive,positive,positive
548758567,"Hi, Yes BCE usually performs better it seems. 
I included this because it was also in the keras/tensorflow official repository, I just added it for the sake of completeness and so that users can experiment freely with both options at their disposal. ",hi yes usually better included also official repository added sake completeness experiment freely disposal,issue,positive,positive,positive,positive,positive,positive
548433943,Please provide more information for the issue.,please provide information issue,issue,negative,neutral,neutral,neutral,neutral,neutral
547093081,"@zhangguanheng66 Thanks for pointing that out. Is the mask Is the correct mask supposed to be
```
tensor([[0., -inf, -inf],
        [0., 0., -inf],
        [0., 0., 0.]])
```
rather than Pytorch 1.3?
```
tensor([[0., 0., 0.],
        [-inf, 0., 0.],
        [-inf, -inf, 0.]])
```",thanks pointing mask correct mask supposed tensor rather tensor,issue,negative,positive,positive,positive,positive,positive
547089316,There was a type promotion issue in 1.3 release and has been fixed by https://github.com/pytorch/pytorch/pull/28253.,type promotion issue release fixed,issue,negative,positive,neutral,neutral,positive,positive
546449621,"fixed in latest master via https://github.com/pytorch/examples/commit/4e00723456160d910092aae567a0b8daf66c49ec

Please use pytorch 1.3.0 or possibly pytorch nightly",fixed latest master via please use possibly nightly,issue,negative,positive,positive,positive,positive,positive
545919369,"If you see the issue https://github.com/pytorch/pytorch/issues/28508, the results of masked_filled seem not correct. So yes, the results from v.1.2.0 make more sense to me.

To optimize the results, I suggest you to adjust the embedding dimension first. More transformer_encoder layers make the model more complicate (a.k.a more parameters) such that the model can learn a more complex problem. There are a lot of hyper-parameters here and I suggest you to play with them for a good sense.",see issue seem correct yes make sense optimize suggest adjust dimension first make model complicate model learn complex problem lot suggest play good sense,issue,positive,positive,positive,positive,positive,positive
545733692,"@zhangguanheng66  Good job! 

Is that mean transformer module in 1.2.0 version is right?

And may I consult you how to optimize this model? I have slack, how to contact you?",good job mean transformer module version right may consult optimize model slack contact,issue,positive,positive,positive,positive,positive,positive
545508658,"I have found the issue and reported on Github. The masked_fill func generates inconsistent results. Because of the issue, the src_mask is not set up correctly. You could print out `self.src_mask` with PyTorch v.1.2 and v.1.3 and see the difference. Once the issue is fixed, we should still see the test results at the level of 5.27-5.45.

For this type of work, it's very important to properly mask the input sequence so that the attention mechanism cannot see the words behind the prediction, during training.

If you like to optimize the results, you could simply play with the hyper-parameters.",found issue inconsistent issue set correctly could print see difference issue fixed still see test level type work important properly mask input sequence attention mechanism see behind prediction training like optimize could simply play,issue,positive,positive,neutral,neutral,positive,positive
545469699,"@SpringRi Do you have slack? I could connect you there. I noticed the same thing here and I did some experiment (copy/paste transformer/multiheadattention in pytorch v.1.3.0 to pytorch v.1.2.0). Re-run the model, which give me the results as 5.ish. This means that there are something happening in pytorch 1.3.0, which significantly changes the results. I'm still digging into it now and haven't found a reason. One thing I'm pretty sure that the transformer/multiheadattention give me same results in v1.2 and v1.3 because we have deterministic tests to cover it.",slack could connect thing experiment model give something happening significantly still digging found reason one thing pretty sure give deterministic cover,issue,positive,positive,positive,positive,positive,positive
545251951,"Hi @zhangguanheng66 , It works. I have tried pytorch==1.3.0 as your suggestion, the loss truely decreased, but the generate result seems worse than before ( same code, pytorch1.2.0, loss 5.x )

This is my loss and generated text:

1.3.0 loss
```
examples-master/word_language_model$ python main.py --epochs 30 --model Transformer --lr 5
| epoch   1 |   200/ 2983 batches | lr 5.00 | ms/batch 187.50 | loss  6.86 | ppl   949.03
| epoch   1 |   400/ 2983 batches | lr 5.00 | ms/batch 181.33 | loss  4.90 | ppl   134.91
| epoch   1 |   600/ 2983 batches | lr 5.00 | ms/batch 191.86 | loss  4.00 | ppl    54.61
| epoch   1 |   800/ 2983 batches | lr 5.00 | ms/batch 193.07 | loss  3.37 | ppl    29.06
| epoch   1 |  1000/ 2983 batches | lr 5.00 | ms/batch 191.62 | loss  2.93 | ppl    18.70
| epoch   1 |  1200/ 2983 batches | lr 5.00 | ms/batch 189.05 | loss  2.65 | ppl    14.09
| epoch   1 |  1400/ 2983 batches | lr 5.00 | ms/batch 184.46 | loss  2.44 | ppl    11.51
| epoch   1 |  1600/ 2983 batches | lr 5.00 | ms/batch 187.49 | loss  2.33 | ppl    10.24
| epoch   1 |  1800/ 2983 batches | lr 5.00 | ms/batch 189.42 | loss  2.05 | ppl     7.77
| epoch   1 |  2000/ 2983 batches | lr 5.00 | ms/batch 191.05 | loss  1.96 | ppl     7.08
| epoch   1 |  2200/ 2983 batches | lr 5.00 | ms/batch 185.15 | loss  1.85 | ppl     6.37
| epoch   1 |  2400/ 2983 batches | lr 5.00 | ms/batch 187.76 | loss  1.81 | ppl     6.11
| epoch   1 |  2600/ 2983 batches | lr 5.00 | ms/batch 187.57 | loss  1.73 | ppl     5.61
| epoch   1 |  2800/ 2983 batches | lr 5.00 | ms/batch 187.84 | loss  1.62 | ppl     5.06
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 582.36s | valid loss  0.91 | valid ppl     2.48
-----------------------------------------------------------------------------------------
| epoch   2 |   200/ 2983 batches | lr 5.00 | ms/batch 188.80 | loss  1.52 | ppl     4.56
| epoch   2 |   400/ 2983 batches | lr 5.00 | ms/batch 190.38 | loss  1.42 | ppl     4.14
| epoch   2 |   600/ 2983 batches | lr 5.00 | ms/batch 188.90 | loss  1.31 | ppl     3.70
| epoch   2 |   800/ 2983 batches | lr 5.00 | ms/batch 194.98 | loss  1.28 | ppl     3.59
| epoch   2 |  1000/ 2983 batches | lr 5.00 | ms/batch 189.09 | loss  1.16 | ppl     3.19
| epoch   2 |  1200/ 2983 batches | lr 5.00 | ms/batch 191.76 | loss  1.17 | ppl     3.22
| epoch   2 |  1400/ 2983 batches | lr 5.00 | ms/batch 189.90 | loss  1.18 | ppl     3.25
| epoch   2 |  1600/ 2983 batches | lr 5.00 | ms/batch 191.96 | loss  1.18 | ppl     3.26
| epoch   2 |  1800/ 2983 batches | lr 5.00 | ms/batch 187.34 | loss  1.09 | ppl     2.99
| epoch   2 |  2000/ 2983 batches | lr 5.00 | ms/batch 200.93 | loss  1.10 | ppl     3.01
| epoch   2 |  2200/ 2983 batches | lr 5.00 | ms/batch 195.98 | loss  1.06 | ppl     2.89
| epoch   2 |  2400/ 2983 batches | lr 5.00 | ms/batch 191.98 | loss  1.09 | ppl     2.97
| epoch   2 |  2600/ 2983 batches | lr 5.00 | ms/batch 191.44 | loss  1.05 | ppl     2.87
| epoch   2 |  2800/ 2983 batches | lr 5.00 | ms/batch 206.80 | loss  1.03 | ppl     2.80
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 597.97s | valid loss  0.51 | valid ppl     1.67
-----------------------------------------------------------------------------------------
| epoch   3 |   200/ 2983 batches | lr 5.00 | ms/batch 204.98 | loss  0.99 | ppl     2.69
| epoch   3 |   400/ 2983 batches | lr 5.00 | ms/batch 198.53 | loss  0.97 | ppl     2.63
| epoch   3 |   600/ 2983 batches | lr 5.00 | ms/batch 201.02 | loss  0.92 | ppl     2.50
| epoch   3 |   800/ 2983 batches | lr 5.00 | ms/batch 193.59 | loss  0.92 | ppl     2.51
| epoch   3 |  1000/ 2983 batches | lr 5.00 | ms/batch 199.43 | loss  0.85 | ppl     2.35
| epoch   3 |  1200/ 2983 batches | lr 5.00 | ms/batch 211.40 | loss  0.86 | ppl     2.37
| epoch   3 |  1400/ 2983 batches | lr 5.00 | ms/batch 200.27 | loss  0.90 | ppl     2.46
| epoch   3 |  1600/ 2983 batches | lr 5.00 | ms/batch 193.69 | loss  0.88 | ppl     2.42
| epoch   3 |  1800/ 2983 batches | lr 5.00 | ms/batch 190.19 | loss  0.84 | ppl     2.31
| epoch   3 |  2000/ 2983 batches | lr 5.00 | ms/batch 191.78 | loss  0.85 | ppl     2.35
| epoch   3 |  2200/ 2983 batches | lr 5.00 | ms/batch 191.93 | loss  0.83 | ppl     2.30
| epoch   3 |  2400/ 2983 batches | lr 5.00 | ms/batch 190.33 | loss  0.84 | ppl     2.32
| epoch   3 |  2600/ 2983 batches | lr 5.00 | ms/batch 188.35 | loss  0.84 | ppl     2.32
| epoch   3 |  2800/ 2983 batches | lr 5.00 | ms/batch 187.56 | loss  0.81 | ppl     2.25
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 604.64s | valid loss  0.39 | valid ppl     1.48
-----------------------------------------------------------------------------------------
| epoch   4 |   200/ 2983 batches | lr 5.00 | ms/batch 197.02 | loss  0.81 | ppl     2.24
| epoch   4 |   400/ 2983 batches | lr 5.00 | ms/batch 207.60 | loss  0.78 | ppl     2.19
| epoch   4 |   600/ 2983 batches | lr 5.00 | ms/batch 195.06 | loss  0.76 | ppl     2.14
| epoch   4 |   800/ 2983 batches | lr 5.00 | ms/batch 192.51 | loss  0.77 | ppl     2.15
| epoch   4 |  1000/ 2983 batches | lr 5.00 | ms/batch 199.79 | loss  0.72 | ppl     2.06
| epoch   4 |  1200/ 2983 batches | lr 5.00 | ms/batch 204.94 | loss  0.74 | ppl     2.09
^C-----------------------------------------------------------------------------------------
Exiting from training early
=========================================================================================
| End of training | test loss  0.39 | test ppl     1.47
=========================================================================================
```

1.3.0 generated text 
```
Herr Nevertheless Attempting Nevertheless sanctions Nevertheless sanctions Nevertheless granting Nevertheless granting Nevertheless Abrams Nevertheless Abrams Nevertheless Abrams Nevertheless Abrams Nevertheless
Abrams Nevertheless Abrams Nevertheless Abrams Nevertheless Abrams Nevertheless Abrams Nevertheless Blaze Nevertheless NME Nevertheless Villa February in convention . Nevertheless
fool Innovation , Fred Nevertheless contrasts Nevertheless sanctions Krayoxx glow Towers Kala Notes Nevertheless sanctions , Sephiroth Stokes Herr Abrams
Sarah shipyards sanctions Terri sanctions tendons sanctions Nevertheless sanctions Tigers 1854 Nevertheless sanctions Nevertheless sanctions Herr Abrams Mushrooms Flemish Nevertheless
sanctions Nevertheless sanctions forests piercing Nevertheless sanctions Nevertheless sanctions Tigers reflecting JosÃ© Nevertheless sanctions Nevertheless sanctions Nevertheless sanctions 1.e4 Initiative
Lakes X. associating wells sanctions , spiders , animators Abrams shortest postural reflecting Petroleum Nevertheless sanctions Nevertheless sanctions , Krayoxx
the Nevertheless sanctions Johnny Airports plantations Nevertheless sanctions Nevertheless sanctions Nevertheless sanctions Nina decency the Nevertheless sanctions Nevertheless sanctions Airports
souvenirs punitive Berardi townships sanctions discharged sanctions Nevertheless sanctions Nevertheless sanctions Nevertheless sanctions Nevertheless the Nevertheless spiders Nittany Mickey shipyards
spiders 1788 Turanoceratops his Nevertheless sanctions , reflecting Anchor Nevertheless sanctions , sanctions midnight inspections spiders Innovation spiders players Nevertheless
sanctions visibility granting Peach Prize Nevertheless sanctions Lionel grenadiers benevolent Hawkes UstaÅ¡e cohesion Bangaru spiders reacts spiders Revenge granting Nevertheless
sanctions airplay AVALANCHE spiders abilities Nevertheless mysticism Nevertheless Sainz hated Nevertheless sanctions Nevertheless sanctions , spiders Sephiroth Nevertheless sanctions Nevertheless
sanctions visibility participated Lasance Nevertheless the Nevertheless sanctions Sweet spiders gross sanctions Nevertheless sanctions Nevertheless sanctions Nevertheless the Nevertheless the
```

**A lot of repetition word like ""Nevertheless"" etc.**

------

pytorch 1.2.0 training loss and generated text are as follows

```
-----------------------------------------------------------------------------------------
| end of epoch  18 | time: 100.30s | valid loss  5.17 | valid ppl   175.36
-----------------------------------------------------------------------------------------
| epoch  19 |   200/ 2983 batches | lr 0.02 | ms/batch 32.54 | loss  4.56 | ppl    95.96
| epoch  19 |   400/ 2983 batches | lr 0.02 | ms/batch 32.38 | loss  4.58 | ppl    97.71
| epoch  19 |   600/ 2983 batches | lr 0.02 | ms/batch 32.33 | loss  4.41 | ppl    82.19
| epoch  19 |   800/ 2983 batches | lr 0.02 | ms/batch 32.33 | loss  4.48 | ppl    88.36
| epoch  19 |  1000/ 2983 batches | lr 0.02 | ms/batch 32.35 | loss  4.46 | ppl    86.43
| epoch  19 |  1200/ 2983 batches | lr 0.02 | ms/batch 32.35 | loss  4.46 | ppl    86.69
| epoch  19 |  1400/ 2983 batches | lr 0.02 | ms/batch 32.33 | loss  4.50 | ppl    89.66
| epoch  19 |  1600/ 2983 batches | lr 0.02 | ms/batch 32.33 | loss  4.53 | ppl    92.68
| epoch  19 |  1800/ 2983 batches | lr 0.02 | ms/batch 32.33 | loss  4.44 | ppl    84.47
| epoch  19 |  2000/ 2983 batches | lr 0.02 | ms/batch 32.34 | loss  4.47 | ppl    87.12
| epoch  19 |  2200/ 2983 batches | lr 0.02 | ms/batch 32.36 | loss  4.34 | ppl    76.44
| epoch  19 |  2400/ 2983 batches | lr 0.02 | ms/batch 32.35 | loss  4.39 | ppl    80.32
| epoch  19 |  2600/ 2983 batches | lr 0.02 | ms/batch 32.29 | loss  4.41 | ppl    82.48
| epoch  19 |  2800/ 2983 batches | lr 0.02 | ms/batch 32.34 | loss  4.33 | ppl    76.22
-----------------------------------------------------------------------------------------
| end of epoch  19 | time: 100.12s | valid loss  5.14 | valid ppl   170.71
-----------------------------------------------------------------------------------------
| epoch  20 |   200/ 2983 batches | lr 0.02 | ms/batch 32.68 | loss  4.56 | ppl    95.11
^C-----------------------------------------------------------------------------------------
Exiting from training early
=========================================================================================
| End of training | test loss  5.07 | test ppl   158.72
=========================================================================================
``` 
text
```
in the Cash 's 1950 tour in January 1648 . <eos> <eos> Nicholas Fraser writes that when Wheeler died in
1959 followed , "" Mosley shifted the issue of 1690 against learning this popularity within two months of his own
game in his lifetime . Maggie , "" hostage but remarked that U2 with a state wants what had only
Â£ 26 March 2008 campaign Edmonton McMahon did not made it . Once Hydnellum God 's performance in the paint
declined due to the addition to impact , as well as a year , as well . Unlike other Justice
League in 2014 . "" <unk> stamp , we fear that Alkan of Percival of Broadway issues in the Simpsons
@-@ highest chance of the European tour of his album album "" Ode on the "" , There he was
performance from 1987 one game "" I couldn 't a new 1930 biography of the Jamie Delano announced his current
Director "" Rooster "" were prompted by the 57th North American music for "" A success of Best War commented
that teaching success and was less of producer Nigel <unk> in politics , Suddenly Australia and recording career with Thomas
R. compliance Conrad K. <unk> in Magadheera in Wonderland written by archaeology theory of the reasons claims , they best
episodes within the poem , the murder of "" Perhaps the 1974 , leading nuances horn @-@ winning an immediate
success . <eos> The Freewheelin ' War "" Independent actor Robert 1128 the 2005 book outlined the perfect success ,
Tessa 's mainly discussing Chrono Cross based on May 21 North America despite two more ambulance address the race .
By June 2007 did not wanting work in with hypnosis that same year 2000 squad during October 2003 team made
early 1999 season Newton acclaim "" When Kody Brown A December 2011 . As superficial 1954 , The Yankees '
attitudes , she tried to hearing was "" kitsunetsuki coincided with his home <unk> on the 2012 season since a
```

This problem truely puzzles me, could you help me? If you need my printout or any other logs, please send me a email, my address is `YW5idTEwMjRAcXEuY29tCg==` (just for avoiding spiders, please `base64 -d` it) . Thanks for your help.",hi work tried suggestion loss generate result worse code loss loss text loss python model transformer epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss end epoch time valid loss valid epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss end epoch time valid loss valid epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss end epoch time valid loss valid epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss training early end training test loss test text nevertheless nevertheless nevertheless nevertheless nevertheless nevertheless nevertheless nevertheless nevertheless nevertheless nevertheless nevertheless nevertheless nevertheless nevertheless blaze nevertheless nevertheless villa convention nevertheless fool innovation nevertheless nevertheless glow kala nevertheless nevertheless nevertheless nevertheless flemish nevertheless nevertheless piercing nevertheless nevertheless reflecting nevertheless nevertheless nevertheless initiative postural reflecting petroleum nevertheless nevertheless nevertheless nevertheless nevertheless nevertheless decency nevertheless nevertheless punitive nevertheless nevertheless nevertheless nevertheless nevertheless nevertheless reflecting anchor nevertheless midnight innovation nevertheless visibility peach prize nevertheless lionel benevolent cohesion revenge nevertheless avalanche nevertheless mysticism nevertheless nevertheless nevertheless nevertheless nevertheless visibility nevertheless nevertheless sweet gross nevertheless nevertheless nevertheless nevertheless lot repetition word like nevertheless training loss text end epoch time valid loss valid epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss end epoch time valid loss valid epoch loss training early end training test loss test text cash tour wheeler issue learning popularity within two game lifetime hostage state march campaign made god performance paint declined due addition impact well year well unlike justice league stamp fear broadway highest chance tour album album ode performance one game new biography current director rooster th north music success best war teaching success le producer politics suddenly recording career compliance wonderland written archaeology theory best within poem murder perhaps leading horn winning immediate success war independent actor book outlined perfect success mainly cross based may north despite two ambulance address race june wanting work hypnosis year squad team made early season newton acclaim brown superficial tried hearing home season since problem could help need please send address please base thanks help,issue,negative,positive,positive,positive,positive,positive
545172389,"the reason it is not moved to `.cuda(args.gpu)` is because we expect the `DataParallel` or `DistributedDataParallel` to move it to relevant GPUs more uniformly. So, instead of moving `images` to GPU0, it will be more like `images[0:16]` are moved to GPU0, `images[17:32]` are moved to GPU1, etc.

",reason expect move relevant uniformly instead moving like,issue,negative,positive,positive,positive,positive,positive
545125750,"@SpringRi Just want to check the PyTorch version on your computer. Are you using PyTorch 1.3.0.
I updated my PyTorch and got consistent results as the tutorial.",want check version computer got consistent tutorial,issue,negative,positive,positive,positive,positive,positive
545106911,"`pip install torch` will fix this now. Sorry for the delay, root-causing it and issuing new binaries took time.",pip install torch fix sorry delay issuing new took time,issue,negative,negative,negative,negative,negative,negative
544770264,"> @SpringRi Thanks for opening the issue. The two cases are essentially same (a.k.a. apply `nn.Transform` model on wikitext2). Those two examples are to show the applications of `nn.Transform` module so we didn't optimize the models. For the case in pytorch/examples, I see the test loss at 5.3 plus or minor 0.05 on my end so your results are actually consistent with mine.
> 
> For the case in pytorch/tutorials, I usually see the test loss at 5.4 plus or minor 0.05. I think the printout there are not updated correctly. Could you submit a PR to update the printout there with your results? Thanks.

Hi @zhangguanheng66 , thanks for your reply. I am sorry, I don't know how to summit a PR to the [pytorch tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html#), the printout only exsits there. I only found [code](https://github.com/pytorch/tutorials/blob/master/beginner_source/transformer_tutorial.py) on github, but without printout. 

According to your reply, my question is how to optimize the transformer model for the word_language_model example?  I am very interested in this question, Thank you very much.",thanks opening issue two essentially apply model two show module optimize case see test loss plus minor end actually consistent mine case usually see test loss plus minor think correctly could submit update thanks hi thanks reply sorry know summit tutorial found code without according reply question optimize transformer model example interested question thank much,issue,positive,positive,neutral,neutral,positive,positive
544559954,"@SpringRi I could help you with the PR there if you have any questions. Also please assign me for reviewing.
",could help also please assign,issue,positive,neutral,neutral,neutral,neutral,neutral
544559311,"@SpringRi Thanks for opening the issue. The two cases are essentially same (a.k.a. apply `nn.Transform` model on wikitext2). Those two examples are to show the applications of `nn.Transform` module so we didn't optimize the models. For the case in pytorch/examples, I see the test loss at 5.3 plus or minor 0.05 on my end so your results are actually consistent with mine.

For the case in pytorch/tutorials, I usually see the test loss at 5.4 plus or minor 0.05. I think the printout there are not updated correctly. Could you submit a PR to update the printout there with your results? Thanks.",thanks opening issue two essentially apply model two show module optimize case see test loss plus minor end actually consistent mine case usually see test loss plus minor think correctly could submit update thanks,issue,positive,positive,neutral,neutral,positive,positive
543110606,"I'm not an expert here but as far as I know it's hard to use GPU with the OpenAI Gym.

In examples here you have quite simple environments and because of that you neural nets are simple too - so GPU isn't necessary.
Gym has to run on CPU so in order to perform learning on GPU you have to copy all the variables from the CPU memory to the GPU.  
Doing this at each episode (or even worse at each timestamp) will create huge bottleneck in the memory bandwidth.  
To give you some example if the model sits on the GPU you will have to copy each observation to the GPU memory every time you would like to use this data and it will be more expensive for small models than doing everything on the CPU.

Hope this helps.",expert far know hard use gym quite simple neural simple necessary gym run order perform learning copy memory episode even worse create huge bottleneck memory give example model copy observation memory every time would like use data expensive small everything hope,issue,positive,negative,negative,negative,negative,negative
542471918,"> Just for the record, the tutorial has been landed [here](https://pytorch.org/tutorials/beginner/transformer_tutorial.html).
> The word_language_model in `pytorch/examples` repo does cover `nn.Transformer`.

Hi, @zhangguanheng66 , There is a [issue](https://github.com/pytorch/tutorials/issues/701)  about the tutorials/transformer_tutorial.py , but I can not mention you there, Could you help us?

I have tried the transformer model in examples/word_language_model repo, the result is same.

This is my training log for word_language_model's transformer model
```
| epoch   6 |   200/ 2983 batches | lr 5.00 | ms/batch 32.47 | loss  4.95 | ppl   140.84
| epoch   6 |   400/ 2983 batches | lr 5.00 | ms/batch 32.18 | loss  4.97 | ppl   143.44
| epoch   6 |   600/ 2983 batches | lr 5.00 | ms/batch 32.16 | loss  4.79 | ppl   120.34
| epoch   6 |   800/ 2983 batches | lr 5.00 | ms/batch 32.14 | loss  4.85 | ppl   127.16
| epoch   6 |  1000/ 2983 batches | lr 5.00 | ms/batch 32.19 | loss  4.84 | ppl   126.60
| epoch   6 |  1200/ 2983 batches | lr 5.00 | ms/batch 32.16 | loss  4.86 | ppl   128.54
| epoch   6 |  1400/ 2983 batches | lr 5.00 | ms/batch 32.17 | loss  4.91 | ppl   135.49
| epoch   6 |  1600/ 2983 batches | lr 5.00 | ms/batch 32.17 | loss  4.96 | ppl   142.61
| epoch   6 |  1800/ 2983 batches | lr 5.00 | ms/batch 32.15 | loss  4.86 | ppl   129.50
| epoch   6 |  2000/ 2983 batches | lr 5.00 | ms/batch 32.18 | loss  4.90 | ppl   133.89
| epoch   6 |  2200/ 2983 batches | lr 5.00 | ms/batch 32.17 | loss  4.79 | ppl   120.56
| epoch   6 |  2400/ 2983 batches | lr 5.00 | ms/batch 32.23 | loss  4.84 | ppl   126.53
| epoch   6 |  2600/ 2983 batches | lr 5.00 | ms/batch 32.31 | loss  4.87 | ppl   129.85
| epoch   6 |  2800/ 2983 batches | lr 5.00 | ms/batch 32.19 | loss  4.80 | ppl   121.97
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 99.66s | valid loss  5.36 | valid ppl   212.04
-----------------------------------------------------------------------------------------
=========================================================================================
| End of training | test loss  5.27 | test ppl   194.93
=========================================================================================

Thanks
```",record tutorial landed cover hi issue mention could help u tried transformer model result training log transformer model epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss epoch loss end epoch time valid loss valid end training test loss test thanks,issue,negative,positive,positive,positive,positive,positive
541960027,"For v1.3.0 release, this affects OSX + pip. OSX + Conda is working.

I'm working on a fix and uploading new OSX binaries. Also tracking in https://github.com/pytorch/pytorch/issues/2125",release pip working working fix new also,issue,negative,positive,positive,positive,positive,positive
540888281,"@Einstellung 
> I don't really know what you said. You should use G result to update D


        ############################
        # (2) Update G network: maximize log(D(G(z)))
        ###########################
        netG.zero_grad()
        label.data.fill_(real_label)  # fake labels are real for generator cost
        output = netD(fake)
        errG = criterion(output, label)
        errG.backward()
        D_G_z2 = output.data.mean()
        optimizerG.step()

########
output = netD(fake),  output is related to  D model , not G model, when backward(D get the input G gradient)     ,the gadient  how to give the G,   because is not connect
##I think  the process,   netG.backward(the model D grad,to input G feature)
this is right???
       ",really know said use result update update network maximize log fake real generator cost output fake criterion output label output fake output related model model backward get input gradient give connect think process model grad input feature right,issue,negative,negative,negative,negative,negative,negative
540884732,"@shiyuanyin
I don't really know what you said. You should use G result to update D",really know said use result update,issue,negative,positive,positive,positive,positive,positive
540856938,"@Einstellung 
hi 
I have a question, the G model gradient update
é¦–å…ˆæœ‰ä¸‰ä¸ªç‹¬ç«‹çš„ç½‘ç»œï¼Œé‰´åˆ«å™¨ç½‘ç»œDï¼Œç”Ÿæˆå™¨ç½‘ç»œGå’Œæºç½‘ç»œS
1ã€é‰´åˆ«å™¨ç½‘ç»œé¦–å…ˆ  è¾“å…¥:åˆå¹¶çš„å€¼ç‰¹å¾å€¼ï¼Œè¾“å‡ºï¼šLogSoftmax()ï¼ŒæŸå¤±æ˜¯ç”¨1 å’Œ0 çš„æ ‡ç­¾ï¼Œ äºŒåˆ†ç±»æŸå¤±ï¼Œ  æ¢¯åº¦æ›´æ–°backward()
2 è¾“å…¥ï¼šé¦–å…ˆç”¨é‰´åˆ«å™¨ï¼Œ   è¾“å…¥ï¼šç”Ÿæˆå™¨Gçš„è¾“å‡ºç‰¹å¾ï¼Œè¾“å‡ºï¼šLogSoftmax()ï¼Œ
ç„¶åæˆ‘ä¸æ˜ç™½ï¼Œæ€ä¹ˆå’Œç”Ÿæˆå™¨Gï¼Œå…³è”èµ·æ¥æ˜µï¼Œå®ƒå’Œé‰´åˆ«å™¨æ˜¯ä¸¤ä¸ªç‹¬ç«‹çš„ç½‘ç»œï¼Œé‰´åˆ«å™¨çš„æ¢¯åº¦æ›´æ–°æ€ä¹ˆå’Œ  G è”ç³»èµ·æ¥æ˜µï¼Ÿï¼Ÿï¼Ÿï¼Œ  
Gç½‘ç»œçš„è¾“å…¥æ˜¯target æ•°æ® æ ‡ç­¾ ï¼Œè¾“å‡ºæ˜¯fc 
Dç½‘è·¯çš„è¾“å…¥æ˜¯ Gçš„ç‰¹å¾ä»¥åŠ Gä¸Sçš„æ‹¼æ¥ç‰¹å¾ã€‚è¾“å‡ºæ˜¯LogSoftmax()
",hi question model gradient update,issue,negative,neutral,neutral,neutral,neutral,neutral
540351324,"Resolved the issue by saving the separate dicts of multiple LSTM cells and loading them individually,
https://discuss.pytorch.org/t/loading-saved-models-gives-inconsistent-results-each-time/36312/24 ",resolved issue saving separate multiple loading individually,issue,negative,neutral,neutral,neutral,neutral,neutral
539625757,"> > because the decoder is implemented by MLP+Sigmoid which can be viewed as a 'Bernoulli distribution'.
> 
> Does this mean that in order to model ""continuous"" distribution like Gaussian then we should not use sigmoid as the output layer and replace it with tanh or even flattened conv layer for example ?

If I change https://github.com/pytorch/examples/blob/master/vae/main.py#L60 from ` .sigmoid()` to `.tanh()`, and https://github.com/pytorch/examples/blob/master/vae/main.py#L74 from `BCE` to `MSE`, will that make this VAE to try a Gaussian Reconstruction and work for [-1, 1]? I would appreciate any input. 
",distribution mean order model continuous distribution like use sigmoid output layer replace tanh even layer example change make try reconstruction work would appreciate input,issue,positive,negative,negative,negative,negative,negative
539619119,@soumith Please let me know if you need a reviewer.,please let know need reviewer,issue,negative,neutral,neutral,neutral,neutral,neutral
538643949,"@rojinsafavi , Is the problem still there in latest PR of VAE? I hope my inputs may help in this.
",problem still latest hope may help,issue,negative,positive,positive,positive,positive,positive
537948784,"I think I have solved the problem. As mentioned in README.md, you should follow this script,https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh to move the image to a certain directory.",think problem follow script move image certain directory,issue,negative,positive,positive,positive,positive,positive
536560981,"Just for the record, the tutorial has been landed [here](https://pytorch.org/tutorials/beginner/transformer_tutorial.html). 
The word_language_model in `pytorch/examples` repo does cover `nn.Transformer`.",record tutorial landed cover,issue,negative,neutral,neutral,neutral,neutral,neutral
536414319,"@zhangguanheng66 is preparing a tutorial afaik, he might have opinions about examples. Doesn't word_language_model not cover MultiHeadAttention already?",tutorial might cover already,issue,negative,neutral,neutral,neutral,neutral,neutral
536347464,"Ran into this again on lazy sunday, looks like Pieter added this! https://github.com/pytorch/pytorch/issues/20380",ran lazy like added,issue,negative,negative,negative,negative,negative,negative
535931559,"similar to the android app, it needs it's own issue tracker, CI etc. and is more complex in infrastructure than the simple examples.
So, put it in https://github.com/pytorch/ios-demo-app I've given you all push rights",similar android need issue tracker complex infrastructure simple put given push,issue,negative,negative,neutral,neutral,negative,negative
535768001,"this is a full-blown demo app, not a short example. let's move it to it's own repository, let me create a repo and add you",short example let move repository let create add,issue,negative,neutral,neutral,neutral,neutral,neutral
534384238,Can you use the latest [PR ](https://github.com/pytorch/examples/pull/632)for VAE and see if this still happens ? ,use latest see still,issue,negative,positive,positive,positive,positive,positive
533821058,not an expert but you can resolve it using --cuda arg inthe inference command,expert resolve inference command,issue,negative,neutral,neutral,neutral,neutral,neutral
531490993,"I noticed that this kind of structure is not suitable for language generation application; thus, I close the issue.",kind structure suitable language generation application thus close issue,issue,positive,positive,positive,positive,positive,positive
531072143,"let me tell you. The role of detach is to freeze the gradient drop. Whether it is for discriminating the network or generating the network, we update all about logD(G(z)). For the discriminant network, freezing G does not affect the overall gradient update (that is The inner function is considered to be a constant, which does not affect the outer function to find the gradient), but conversely, if D is frozen, there is no way to complete the gradient update. Therefore, we did not use the gradient of freezing D when training the generator. So, for the generator, we did calculate the gradient of D, but we didn't update the weight of D (only optimizer_g.step was written), so the discriminator will not be changed when the generator is trained. You may ask, that's why, when you train the discriminator, you need to add detach. Isn't this an extra move?
Because we freeze the gradient, we can speed up the training, so we can use it where it can be used. It is not an extra task. Then when we train the generator, because of logD(G(z)), there is no way to freeze the gradient of D, so we will not write detach here.",let tell role detach freeze gradient drop whether discriminating network generating network update discriminant network freezing affect overall gradient update inner function considered constant affect outer function find gradient conversely frozen way complete gradient update therefore use gradient freezing training generator generator calculate gradient update weight written discriminator generator trained may ask train discriminator need add detach extra move freeze gradient speed training use used extra task train generator way freeze gradient write detach,issue,negative,positive,neutral,neutral,positive,positive
530367274,"I observed the following error while executing the inception v3 sample:

`RuntimeError: Calculated padded input size per channel: (3 x 3). Kernel size: (5 x 5). Kernel size can't be greater than actual input size`

Then I came across the PR - https://github.com/pytorch/examples/pull/268 and tried to use the fix, but doesn't work for me. 

I get the following error:
```
    return type(out)(map(gather_map, zip(*outputs)))
TypeError: __new__() missing 1 required positional argument: 'aux_logits'
```
",following error inception sample calculated input size per channel kernel size kernel size ca greater actual input size came across tried use fix work get following error return type map zip missing positional argument,issue,negative,positive,neutral,neutral,positive,positive
528428641,"it looks like it is not a GPU issue - if I try to run with kCPU it still gives an error.
it is more like Ubuntu 14 issue.
what can I try to make it run?
",like issue try run still error like issue try make run,issue,negative,neutral,neutral,neutral,neutral,neutral
528389986,"I see core dump as well, but in different place (https://github.com/pytorch/examples/blob/master/cpp/mnist/mnist.cpp#L128)

I am on Ubuntu 14.04, libtorch 1.2.0, cuda 10.0, 1050Ti GPU.

```
root@poise:~/libtorch/examples/cpp/mnist/build# cmake -DCMAKE_PREFIX_PATH=/home/libtorch/pre-cxx11/ ..
-- The C compiler identification is GNU 4.8.4
-- The CXX compiler identification is GNU 4.8.4
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Looking for pthread.h
-- Looking for pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed
-- Looking for pthread_create in pthreads
-- Looking for pthread_create in pthreads - not found
-- Looking for pthread_create in pthread
-- Looking for pthread_create in pthread - found
-- Found Threads: TRUE  
-- Found CUDA: /usr/local/cuda (found version ""10.0"") 
-- Caffe2: CUDA detected: 10.0
-- Caffe2: CUDA nvcc is: /usr/local/cuda/bin/nvcc
-- Caffe2: CUDA toolkit directory: /usr/local/cuda
-- Caffe2: Header version is: 10.0
-- Found CUDNN: /usr/include  
-- Found cuDNN: v7.6.0  (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libcudnn.so)
-- Autodetected CUDA architecture(s):  6.1
-- Added CUDA NVCC flags for: -gencode;arch=compute_61,code=sm_61
-- Found torch: /home/libtorch/pre-cxx11/lib/libtorch.so  
-- Downloading MNIST dataset
Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz ...
0% |################################################################| 100%
Unzipped /home/libtorch/examples/cpp/mnist/build/data/train-images-idx3-ubyte.gz ...
Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz ...
0% |################################################################| 100%
Unzipped /home/libtorch/examples/cpp/mnist/build/data/train-labels-idx1-ubyte.gz ...
Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz ...
0% |################################################################| 100%
Unzipped /home/libtorch/examples/cpp/mnist/build/data/t10k-images-idx3-ubyte.gz ...
Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz ...
0% |################################################################| 100%
Unzipped /home/libtorch/examples/cpp/mnist/build/data/t10k-labels-idx1-ubyte.gz ...
-- Configuring done
-- Generating done
-- Build files have been written to: /home/libtorch/examples/cpp/mnist/build
root@poise:~/libtorch/examples/cpp/mnist/build# make
Scanning dependencies of target mnist
[ 50%] Building CXX object CMakeFiles/mnist.dir/mnist.cpp.o
[100%] Linking CXX executable mnist
[100%] Built target mnist
root@poise:~/libtorch/examples/cpp/mnist/build# ./mnist 
CUDA available! Training on GPU.
creating device
creating model
*** Error in `./mnist': munmap_chunk(): invalid pointer: 0x0000000001958570 ***
Aborted (core dumped)
```

",see core dump well different place ti root poise compiler identification gnu compiler identification gnu check working compiler check working compiler work compiler compiler done compile compile done check working compiler check working compiler work compiler compiler done compile compile done looking looking found test test looking looking found looking looking found found true found found version directory header version found found include library architecture added found torch done generating done build written root poise make scanning target building object linking executable built target root poise available training device model error invalid pointer aborted core,issue,negative,positive,positive,positive,positive,positive
527612309,@soumith This looks great and is ready to merge. Thanks!,great ready merge thanks,issue,positive,positive,positive,positive,positive,positive
525588365,It's merged into master recently https://github.com/pytorch/pytorch/pull/24317. Probably it will be available in the next release. Try the nightly build if you don't wanna wait :) ,master recently probably available next release try nightly build wan na wait,issue,negative,positive,neutral,neutral,positive,positive
525564905,"> What version of PyTorch are you using? Try one or more of the following
> 
> 1. Use an even smaller batch size
> 2. Update your PyTorch version, especially if it's earlier than 1.1
> 3. Get a GPU with more than 1 GB of RAM

the Pytorch version is 1.2.0+cu92",version try one following use even smaller batch size update version especially get ram version,issue,negative,neutral,neutral,neutral,neutral,neutral
525458541,"What version of PyTorch are you using? Try one or more of the following

1) Use an even smaller batch size
2) Update your PyTorch version, especially if it's earlier than 1.1
3) Get a GPU with more than 1 GB of RAM
",version try one following use even smaller batch size update version especially get ram,issue,negative,neutral,neutral,neutral,neutral,neutral
525159636,"I found that the example of ImageNet used `spawn` function. Is it the async method?

Thanks a lot.",found example used spawn function method thanks lot,issue,negative,positive,positive,positive,positive,positive
524543554,"No , I mean  saving a model and loading it with C++ frontend . I opned an issue here : https://github.com/pytorch/pytorch/issues/25142

You ll find a complete description of the issue. 

Thank you.

 ",mean saving model loading issue find complete description issue thank,issue,negative,negative,negative,negative,negative,negative
524543102,@zirid do you mean saving a model in Python using jit and loading it in C++?,mean saving model python loading,issue,negative,negative,negative,negative,negative,negative
524542694,"Can you please provide or test an example of  saving a network and loading it using jit . I ran into this problem and it seems like many had same issue. 

I tried  both the nightly and stable version of Libtorch without success. 
There is also a lack of documentation concerning (Saving and loading a model in C++)

Thank you in advance. ",please provide test example saving network loading ran problem like many issue tried nightly stable version without success also lack documentation concerning saving loading model thank advance,issue,positive,positive,positive,positive,positive,positive
523681239,"@ElegantLin It doesn't seem to be an actual issue that will effect the training in any way. It's related to the use of multiprocessing for distributed training, and multiprocessing for the data loader workers. Don't quote me on this, but I seem to remember coming across some discussion related to this between some of the PyTorch devs and I believe their conclusion was that it may not be something that is fixable. @soumith, am I right about that? 

I ended up just suppressing the warnings with the `environ['PYTHONWARNINGS'] = 'ignore:semaphore_tracker:UserWarning'` solution from [here](https://discuss.pytorch.org/t/issue-with-multiprocessing-semaphore-tracking/22943/4). It works perfectly and I don't have any issues.",seem actual issue effect training way related use distributed training data loader quote seem remember coming across discussion related believe conclusion may something fixable right ended environ solution work perfectly,issue,positive,positive,positive,positive,positive,positive
523679248,"@mdlockyer Do you fix it? If you made it, could you please provide more information about this?",fix made could please provide information,issue,negative,neutral,neutral,neutral,neutral,neutral
523679138,"@Akmazad How many GPUs did you use to reproduce this problem? It worked well when I used 8 GPUs on 2 nodes. However, it failed when I used 16 GPUs on 2 nodes. I referred to [this](https://github.com/pytorch/pytorch/issues/23720#issue-475994378) and both of you error messages are the same with me. 

Do you use a large scale of GPUs?",many use reproduce problem worked well used however used error use large scale,issue,negative,positive,positive,positive,positive,positive
522815657,"@etetteh

Thank you.

I already fixed it.

Best,
@bemoregt.",thank already fixed best,issue,positive,positive,positive,positive,positive,positive
522811643,"In this tutorial, we have seen how to write and use datasets, transforms and dataloader. torchvision package provides some common datasets and transforms. You might not even have to write custom classes. One of the more generic datasets available in torchvision is ImageFolder. It assumes that images are organized in the following way:

root/ants/xxx.png
root/ants/xxy.jpeg
root/ants/xxz.png
.
.
.
root/bees/123.jpg
root/bees/nsdf3.png
root/bees/asd932_.png

where â€˜antsâ€™, â€˜beesâ€™ etc. are class labels.

Source: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html?highlight=torchvision%20datasets%20imagefolder",tutorial seen write use package common might even write custom class one generic available organized following way bee class source,issue,negative,positive,neutral,neutral,positive,positive
521072549,"Oh, sure.  The developer might have kept it for some debugging purpose. You can create a pull request and close the issue.",oh sure developer might kept purpose create pull request close issue,issue,positive,positive,positive,positive,positive,positive
521030592,"@standbyme Thanks. IMO, this example is to show how to build a word language model with RNN or Transformer module. We are welcome to use our dataset. 
However, unless there is a specific reason, I would like to avoid the extra dependency from the domain library.",thanks example show build word language model transformer module welcome use however unless specific reason would like avoid extra dependency domain library,issue,positive,positive,positive,positive,positive,positive
520840607,"After removing the directory and creating a new one, this kind of problem disappeared",removing directory new one kind problem,issue,negative,positive,positive,positive,positive,positive
520724138,"Thanks.
My statement just now may not be clear enough. What I want to say is that this variable has not been returned and can be deleted.",thanks statement may clear enough want say variable returned,issue,positive,positive,positive,positive,positive,positive
520699980,"Yes, it does. Without it,
`tokens += len(words)` which is 
`tokens = tokens + len(words)` won't work as nothing is bound to `tokens` before this. ",yes without wo work nothing bound,issue,negative,neutral,neutral,neutral,neutral,neutral
520388556,"Finally solved:

In windows, the whole content in main.py (except the imports) should be in a main() function, then at the end of the file, the following part is needed at the top level:

```
if __name__ == ""__main__"":
    main()
```

",finally whole content except main function end file following part top level main,issue,negative,positive,positive,positive,positive,positive
519411776,"Here is a very stupid but useful implement of model_load and model_save.
In model_load, Load a std::vector<torch::Tensor> and save every weights and bias in a ""inter_layer.pt"" file. And then, with the code of torch::load(model.parameters()[i], ""inter_layer.pt""), layers parameters can be loaded.
In contrary, with the code of { std::vector <torch::Tensor> a; a.push_back(model.parameters()[i])}, layers parameters can be saved as Vector File.
Unfortunately, with my test, a complex model with many structures cannot be save and load directly. Plz comment if any easier implement exists.",stupid useful implement load torch save every bias file code torch loaded contrary code torch saved vector file unfortunately test complex model many save load directly comment easier implement,issue,positive,negative,negative,negative,negative,negative
519295202,"> I met the same problem. I solved it by trying a whole day. It is interesting but needs time to figure out.
> 
> Let's say I am going to use ImageFolder(""/train/"") to read jpg files in folder train.
> The file structure is
> /train/
> -- 1.jpg
> -- 2.jpg
> -- 3.jpg
> 
> I failed to load them, leading to errors:
> RuntimeError: Found 0 images in subfolders of: ./data
> Supported image extensions are: .jpg,.JPG,.jpeg,.JPEG,.png,.PNG,.ppm,.PPM,.bmp,.BMP
> 
> I read the solution above and tried tens of times. When I changed the structure to
> /train/1/
> -- 1.jpg
> -- 2.jpg
> -- 3.jpg
> But the read in code is still -- ImageFolder(""/train/""), IT WORKS.
> 
> It seems like the program tends to recursively read in files, that is convenient in some cases.



> I met the same problem. I solved it by trying a whole day. It is interesting but needs time to figure out.
> 
> Let's say I am going to use ImageFolder(""/train/"") to read jpg files in folder train.
> The file structure is
> /train/
> -- 1.jpg
> -- 2.jpg
> -- 3.jpg
> 
> I failed to load them, leading to errors:
> RuntimeError: Found 0 images in subfolders of: ./data
> Supported image extensions are: .jpg,.JPG,.jpeg,.JPEG,.png,.PNG,.ppm,.PPM,.bmp,.BMP
> 
> I read the solution above and tried tens of times. When I changed the structure to
> /train/1/
> -- 1.jpg
> -- 2.jpg
> -- 3.jpg
> But the read in code is still -- ImageFolder(""/train/""), IT WORKS.
> 
> It seems like the program tends to recursively read in files, that is convenient in some cases.

Thanks so much for the solution!",met problem trying whole day interesting need time figure let say going use read folder train file structure load leading found image read solution tried time structure read code still work like program read convenient met problem trying whole day interesting need time figure let say going use read folder train file structure load leading found image read solution tried time structure read code still work like program read convenient thanks much solution,issue,positive,positive,positive,positive,positive,positive
519293308,"@QimingChen 
> I met the same problem. I solved it by trying a whole day. It is interesting but needs time to figure out.
> 
> Let's say I am going to use ImageFolder(""/train/"") to read jpg files in folder train.
> The file structure is
> /train/
> -- 1.jpg
> -- 2.jpg
> -- 3.jpg
> 
> I failed to load them, leading to errors:
> RuntimeError: Found 0 images in subfolders of: ./data
> Supported image extensions are: .jpg,.JPG,.jpeg,.JPEG,.png,.PNG,.ppm,.PPM,.bmp,.BMP
> 
> I read the solution above and tried tens of times. When I changed the structure to
> /train/1/
> -- 1.jpg
> -- 2.jpg
> -- 3.jpg
> But the read in code is still -- ImageFolder(""/train/""), IT WORKS.
> 
> It seems like the program tends to recursively read in files, that is conv


Thank you so much buddy. It worked for me too.",met problem trying whole day interesting need time figure let say going use read folder train file structure load leading found image read solution tried time structure read code still work like program read thank much buddy worked,issue,positive,positive,positive,positive,positive,positive
518892197,"You can save all layers weights and bias at a vector, and torch::save and torch::load will be useful in this situation. Any easier implement ? Plz comment.",save bias vector torch torch useful situation easier implement comment,issue,positive,positive,positive,positive,positive,positive
518067217,"I think original code is
```python
for data, target in test_loader:
            ...
            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss
            ...
test_loss /= len(test_loader.dataset)
```
`F.nll_loss` would compute the **average loss** of a batch by default when we train our model. Because we want to know how well each batch performs at this time to babysit our training process

when it's test time, we want to know the **average loss on the whole test set**. So we set `reduction='sum'` then it will compute the sum loss of a batch instead of average. Then we divide the size of test set `len(test_loader.dataset)` to get this average on the whole test set.

There are many ways to compute average loss on the whole test set, like:
```python
for data, target in test_loader:
            ...
            test_loss += F.nll_loss(output, target).item() * data.size(0) # average loss * batch_size
            ...
test_loss /= len(test_loader.dataset)
```",think original code python data target output target sum batch loss would compute average loss batch default train model want know well batch time training process test time want know average loss whole test set set compute sum loss batch instead average divide size test set get average whole test set many way compute average loss whole test set like python data target output target average loss,issue,negative,positive,neutral,neutral,positive,positive
513950557,"> I also have an example that runs on Apex and there I am able to simply run the validation on Rank 0 (simple if branching statement) but in normal PyTorch the same logic seems to lock up. Does someone know the reason for this difference?

the all_reduce step will still be run, and the rank=0 node will be waiting on rank > 0 nodes to get gradients. It's dumb but I think it's fixed on PyTorch master (with the param `find_unused_parameters=True` given to `nn.DistrbutedDataParallel`).
In your case, if you are running validation only on rank 0, then you should just run validation on `model.module(input)`, rather than `model(input)`, so that you bypass all the distributed logic",also example apex able simply run validation rank simple branching statement normal logic lock someone know reason difference step still run node waiting rank get dumb think fixed master param given case running validation rank run validation input rather model input bypass distributed logic,issue,negative,negative,negative,negative,negative,negative
513728603,"Bumping again :)
Besides using the distributed sampler wouldn't we need to also aggregate the metrics from the GPUs with all_gather?

I also have an example that runs on Apex and there I am able to simply run the validation on Rank 0 (simple if branching statement) but in normal PyTorch the same logic seems to lock up. Does someone know the reason for this difference?",bumping besides distributed sampler would need also aggregate metric also example apex able simply run validation rank simple branching statement normal logic lock someone know reason difference,issue,negative,negative,neutral,neutral,negative,negative
512546386,"Training with random crop + horizontal flip brings more augmented data?
Using 5 or 10 crop in testing as a trick to gain a better result? (Combine the multiple results",training random crop horizontal flip augmented data crop testing trick gain better result combine multiple,issue,positive,neutral,neutral,neutral,neutral,neutral
512115834,"it would be a good idea, in the cpp folder",would good idea folder,issue,negative,positive,positive,positive,positive,positive
511216121,@pajola i dont think so. It was fixed to it's current state by the guy who wrote the VAE paper in https://github.com/pytorch/examples/pull/419,dont think fixed current state guy wrote paper,issue,negative,positive,neutral,neutral,positive,positive
509678885,"The first parameter is the rank, which is automatically passed by mp.spawn().",first parameter rank automatically,issue,negative,negative,negative,negative,negative,negative
509511051,"The model that trains in pytorch/examples is a toy model, it's not intended to give good results.
If you want to look at good results, a larger model (increasing hidden size etc.) or using a more powerful model like Bert might help.

For example: https://pytorch.org/hub/huggingface_pytorch-pretrained-bert_bert/",model toy model intended give good want look good model increasing hidden size powerful model like might help example,issue,positive,positive,positive,positive,positive,positive
508687084,"> I have updated the comment.

Another question is that, you said it's unnecessary to clear gradients for netD and netG between updating D and updating G. However, how do you ensure the gradients calculated in errD_fake.backward() will not be accumulated with the gradients calculated in errG.backward() ?",comment another question said unnecessary clear however ensure calculated calculated,issue,positive,negative,negative,negative,negative,negative
508682382,"> It should work like this:
> 
> 1. DO NOT need retain_graph
> 2. zero grad the netG at the begining
> 3. Remove zero_grad of netG and netD right before `Update G network`
> 
> ```
>        '''
>        --------------------------------------
>        Add netG.zero_grad() at the beginning
>        ---------------------------------------
>        '''
>         netG.zero_grad()
>         ############################
>         # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))
>         ###########################
>         # train with real
>         netD.zero_grad()
>         real_cpu = data[0].to(device)
>         batch_size = real_cpu.size(0)
>         label = torch.full((batch_size,), real_label, device=device)
> 
>         real_output = netD(real_cpu)
>         errD_real = criterion(real_output, label)
>         errD_real.backward()
>         D_x = real_output.mean().item()
> 
>         # train with fake
>         noise = torch.randn(batch_size, nz, 1, 1, device=device)
>         fake = netG(noise)
>         label.fill_(fake_label)
>         # output = netD(fake.detach())
>         fake_output = netD(fake)
>         errD_fake = criterion(fake_output, label)
>         errD_fake.backward()
>        # errD_fake.backward(retain_graph=True) # No need retain_graph
>         D_G_z1 = fake_output.mean().item()
>         errD = errD_real + errD_fake
>         optimizerD.step()
> 
>         ############################
>         # (2) Update G network: maximize log(D(G(z)))
>         ###########################
>      # The following two lines are unnecessary!!
>      #   netG.zero_grad()
>      #  netD.zero_grad()
>         label.fill_(real_label)  # fake labels are real for generator cost
>         # output = netD(fake)
>         errG = criterion(fake_output, label)
>         errG.backward()
>         D_G_z2 = fake_output.mean().item()
>         optimizerG.step()
> ```

I'm not sure but it seems that the computational graph of netG will be released when you call errD_fake.backward(). As a consequence, an exception will be raised when you call errG.backward(). More clearly, the gradients of errG need to be backpropogaed through netG but the computional graph will be lost at that time.",work like need zero grad remove right update network add beginning update network maximize log log train real data device label criterion label train fake noise fake noise output fake criterion label need update network maximize log following two unnecessary fake real generator cost output fake criterion label sure computational graph call consequence exception raised call clearly need graph lost time,issue,negative,negative,negative,negative,negative,negative
508600212,"It should work like this:
1. DO NOT need retain_graph
2. zero grad the netG at the begining
3. Remove zero_grad of netG and netD right before `Update G network`

**Edit:**
1. Need to add retain_graph

```
       '''
       --------------------------------------
       Add netG.zero_grad() at the beginning
       ---------------------------------------
       '''
        netG.zero_grad()
        ############################
        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))
        ###########################
        # train with real
        netD.zero_grad()
        real_cpu = data[0].to(device)
        batch_size = real_cpu.size(0)
        label = torch.full((batch_size,), real_label, device=device)

        real_output = netD(real_cpu)
        errD_real = criterion(real_output, label)
        errD_real.backward()
        D_x = real_output.mean().item()

        # train with fake
        noise = torch.randn(batch_size, nz, 1, 1, device=device)
        fake = netG(noise)
        label.fill_(fake_label)
        # output = netD(fake.detach())
        fake_output = netD(fake)
        errD_fake = criterion(fake_output, label)
      #  errD_fake.backward()
       errD_fake.backward(retain_graph=True)
        D_G_z1 = fake_output.mean().item()
        errD = errD_real + errD_fake
        optimizerD.step()

        ############################
        # (2) Update G network: maximize log(D(G(z)))
        ###########################
     # The following two lines are unnecessary!!
     #   netG.zero_grad()
     #  netD.zero_grad()
        label.fill_(real_label)  # fake labels are real for generator cost
        # output = netD(fake)
        errG = criterion(fake_output, label)
        errG.backward()
        D_G_z2 = fake_output.mean().item()
        optimizerG.step()
```",work like need zero grad remove right update network edit need add add beginning update network maximize log log train real data device label criterion label train fake noise fake noise output fake criterion label update network maximize log following two unnecessary fake real generator cost output fake criterion label,issue,negative,negative,negative,negative,negative,negative
506927450,"sure thing, go for it. i think it'll be helpful.

cc: @yf225 and @gchanan for review when the PR is ready",sure thing go think helpful review ready,issue,positive,positive,positive,positive,positive,positive
505758892,"Hey all. Bumping this for a clarification.

Does this not entail just adding a distributed sampler, as for train in line 209, to `val_loader` at
https://github.com/pytorch/examples/blob/1de2ff9338bacaaffa123d03ce53d7522d5dcc2e/imagenet/main.py#L218
which, for now, as @xvjiarui points out would be blowing up every process with unnecessary work?

PS: Github issue search got me here when I was opening a new issue. Kudos to them!",hey bumping clarification entail distributed sampler train line would blowing every process unnecessary work issue search got opening new issue kudos,issue,positive,negative,negative,negative,negative,negative
505322061,Have you solve the problem?  I'm having the same problem while reproducing bert's result with pytorch.,solve problem problem result,issue,negative,neutral,neutral,neutral,neutral,neutral
504966993,I met the same question. But my torch version is 1.0.0+. Seems that it is not about torch version?,met question torch version torch version,issue,negative,neutral,neutral,neutral,neutral,neutral
504135612,OK it seems that I had to run the download script first to fetch the files. Then I put them manually in a build/data/ folder.,run script first fetch put manually folder,issue,negative,positive,positive,positive,positive,positive
504102093,"Although I downloaded the required file and put that in the folder, still I get the same error quickly. It seems that something is cached and I can not find that.

```
$ ./dcgan
CUDA is available! Training on GPU.
terminate called after throwing an instance of 'c10::Error'
  what():  Error opening targets file at ./data/train-labels-idx1-ubyte (read_targets at /home/mahmood/cactus/pt/pytorch/torch/csrc/api/src/data/datasets/mnist.cpp:86)
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x6c (0x7f54d8bcb2cc in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x35be345 (0x7f54dc39d345 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch.so)
frame #2: torch::data::datasets::MNIST::MNIST(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, torch::data::datasets::MNIST::Mode) + 0x55 (0x7f54dc39dc05 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch.so)
frame #3: main + 0xb7f (0x563aeb6bb7dd in ./dcgan)
frame #4: __libc_start_main + 0xe7 (0x7f54d7c71b97 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: _start + 0x2a (0x563aeb6b912a in ./dcgan)

Aborted (core dumped)
$ ls -lh data/
total 45M
-rw-rw-r-- 1 mahmood mahmood 45M Ú˜ÙˆØ¦ÛŒÙ‡ 21  2000 train-images-idx3-ubyte
```",although file put folder still get error quickly something find available training terminate throwing instance error opening file frame char char char frame unknown function frame torch char char char torch frame main frame frame aborted core total,issue,negative,positive,positive,positive,positive,positive
502385572,"Hello,
I am a little confused by this and will greatly appreciate help in understanding.
According to my understanding detach() prevents further computations from being tracked. (I suppose it also prevent previous computations from being taken into account in the backward pass?)

Either way, wouldn't you want to track the next computation, the operation of D over fake, for the backward pass of D?
If you wanted to prevent tracking of the Generator, wouldn't it make sense to detach before applying G and then restore tracking for D right at the point where detach is now called? (With requires_grad_(True)?)
Thank you",hello little confused greatly appreciate help understanding according understanding detach tracked suppose also prevent previous taken account backward pas either way would want track next computation operation fake backward pas prevent generator would make sense detach restore right point detach true thank,issue,positive,positive,neutral,neutral,positive,positive
500228164,"@bemoregt @QimingChen @ezyang  I have met this probloem and I know the root should not be the subfolder. The dataset files are as following:
/data:
â””â”€â”€test
â””â”€â”€â”€â”€â”€â”€â”€â”€a.jpg 
â””â”€â”€â”€â”€â”€â”€â”€â”€b.jpg
â””â”€â”€â”€â”€â”€â”€â”€â”€c.jpg
â””â”€â”€â”€â”€â”€â”€â”€â”€ ...
â””â”€â”€train
â””â”€â”€â”€â”€â”€â”€â”€â”€1.jpg
â””â”€â”€â”€â”€â”€â”€â”€â”€2.jpg
â””â”€â”€â”€â”€â”€â”€â”€â”€3.jpg
â””â”€â”€â”€â”€â”€â”€â”€â”€ ... 
For example, there are 1000 images in the file ""train"" and 2000 images in ""test"", if I try this:
        image_dataset = datasets.ImageFolder('/home/data/')
        a = image_dataset.imgs[500]
        b = image_dataset.imgs[1800]
this ""a"" I get is the 500th image in the whole dataset (actually the 500th one in ""test""), this ""b"" is the 1800th image in the whole dataset (actually the 800th one in ""train"").

So how can I get one image directly from the ""test"" or ""train"" subfile, rather than  by the index in the whole dataset? Thank you a lot.
",met know root following example file train test try get th image whole actually th one test th image whole actually th one train get one image directly test train rather index whole thank lot,issue,negative,positive,neutral,neutral,positive,positive
500126517,I am thinking about the same question. Seems like original author used similar way to write loss function:https://github.com/y0ast/VAE-TensorFlow. ,thinking question like original author used similar way write loss function,issue,negative,positive,positive,positive,positive,positive
499043074,"I met this problem too.
This is the output when I run dcgan example on Win10 64bit with python 3.7 with pytorch 1.1 on Anaconda.

""
RuntimeError:
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The ""freeze_support()"" line can be omitted if the program
        is not going to be frozen to produce an executable.
""

But I didn't meet this on Ubuntu16.04 with python 3.5 with pytorch 1.0 on Anaconda.",met problem output run example win bit python attempt made start new process current process finished phase probably fork start child forgotten use proper idiom main module line program going frozen produce meet python anaconda,issue,negative,positive,positive,positive,positive,positive
498788336,"@soumith I think I've introduced this with #529. But, just like you, I have no idea why this is a problem.",think like idea problem,issue,negative,neutral,neutral,neutral,neutral,neutral
497885756,"I'm still having this same issue. It seems like it has been rather long running, with issues going back almost a year. I run into this when using `break` while looping through the loader while training using `DistributedDataParallel` on a single node with multiple GPUs. Doesn't seem to happen when using a single GPU or CPU.",still issue like rather long running going back almost year run break looping loader training single node multiple seem happen single,issue,negative,negative,neutral,neutral,negative,negative
497261890,"> In Google Colab, dcgan works
> 
> ```
> !wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette-320.tgz
> !tar xzf imagenette-320.tgz
> ```
> 
> ```
> !git clone https://github.com/pytorch/examples
> !cd examples/dcgan; python3 main.py --dataset imagenet --dataroot ../../imagenette-320 --cuda
> ```

I ran it at local platform, my pic size is 50*150. Do u have any idea to alter the code ? thanks.",work tar git clone python ran local platform pic size idea alter code thanks,issue,negative,positive,neutral,neutral,positive,positive
497217258,"On Google Colab, mnist (c++) works

```
!git clone http://github.com/pytorch/examples
!cd examples/cpp/mnist;cmake -DCMAKE_PREFIX_PATH=/usr/local/lib/python3.6/dist-packages/torch/lib/ -DTorch_DIR=/usr/local/lib/python3.6/dist-packages/torch/share/cmake/Torch/ ; make
!cd examples/cpp/mnist;./mnist
```",work git clone make,issue,negative,neutral,neutral,neutral,neutral,neutral
497216517,"In Google Colab, dcgan works

```
!wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette-320.tgz
!tar xzf imagenette-320.tgz
```

```
!git clone https://github.com/pytorch/examples
!cd examples/dcgan; python3 main.py --dataset imagenet --dataroot ../../imagenette-320 --cuda
```",work tar git clone python,issue,negative,neutral,neutral,neutral,neutral,neutral
497148508,"I was able to get dcgan operating successfully at 128x128 by adding the convolutional layers described above and then running with ngf 128 and ndf 32. When I attempted to go to 512 I was not able to get a stable result. I'm attempting to add the white noise to the discriminator to see if that helps. 

** I ended up abandoning dcgan and am now over using bmsggan which is a variation on progressive gans. Its handling higher resolutions much better **",able get operating successfully convolutional running go able get stable result add white noise discriminator see ended variation progressive handling higher much better,issue,positive,positive,positive,positive,positive,positive
497041047,A few minor updates. Not using superclass for the transformer model. Check transformer module exists before using. @cpuhrsch ,minor superclass transformer model check transformer module,issue,negative,negative,neutral,neutral,negative,negative
496304911,"the code rendering for the tutorial seems broken. We are working on fixing it, sorry about that.
Here's the tutorial section for hooks, with code: https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks",code rendering tutorial broken working fixing sorry tutorial section code,issue,negative,negative,negative,negative,negative,negative
496095361,deterministic and benchmark are orthogonal. in benchmark mode one can only filter out deterministic algorithms (i cant remember if we do this in our backend or not),deterministic orthogonal mode one filter deterministic cant remember,issue,negative,neutral,neutral,neutral,neutral,neutral
495918802,"@runzeer 

> In the ClassNLLCriterion kernel, the Assertion t >= 0 && t < n_classes failed. so I guess one of the elements of target_var is either smaller than 0 or larger than the number of classes (output size) :slight_smile: You might want to check the content of your labels for your dataset, one look like itâ€™s not valid.

https://discuss.pytorch.org/t/runtimeerror-cuda-runtime-error-59-device-side-assert-triggered-at-pytorch-torch-lib-thc-generic-thcstorage-c-36/17442

Maybe the number of classes in your datasets is not 1000, so you should change it...

like this:

```python
class TotalModel(nn.Module):
    def __init__(self, num_class=1000):
        super(TotalModel, self).__init__()
        net = resnet50(pretrained=True)
        self.div_32 = nn.Sequential(*list(net.children())[:-1])
        self.other_layers = nn.Linear(2048, num_class)

    def forward(self, in_feat):
        in_feat = self.div_32(in_feat)
        in_feat = in_feat.view(in_feat.size(0), -1)
        in_feat = self.other_layers(in_feat)
        return in_feat

if __name__ == '__main__':
    in_data = torch.randn(4, 3, 224, 224)
    net = TotalModel()
    out = net(in_data)
    print(out.size())
```",kernel assertion guess one either smaller number class output size might want check content one look like valid maybe number class change like python class self super self net list forward self return net net print,issue,positive,positive,neutral,neutral,positive,positive
495597495,"I think I have got the answer here https://github.com/pytorch/pytorch/issues/8149.
tearing doesn't occur at the granularity of a float32 (on CPU or supported nvidia GPUs), so really bad things dont happen (unless maybe you train on dtype.float64 and on CUDA)
",think got answer tearing occur granularity float really bad dont happen unless maybe train,issue,negative,negative,negative,negative,negative,negative
494572192,"@hszhao I'm sorry, I haven't actually executed this code. ",sorry actually executed code,issue,negative,negative,negative,negative,negative,negative
494518170,"@JayanthRR Thanks and got it. Btw, for the current sample code, do you think we need to increase the base lr as 0.2 when using batch size 256 with 2 nodes each 8 gpus?",thanks got current sample code think need increase base batch size,issue,positive,negative,negative,negative,negative,negative
494514974,@hszhao If you wrap your model using [DistributedDataParallel](https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel) it happens under the hood. The tutorial you are referring to tries to implement the same functionality differently. ,wrap model hood tutorial implement functionality differently,issue,negative,neutral,neutral,neutral,neutral,neutral
494494373,@TobeyYang Same question. Do we need to add average_gradients manually after loss.backward() as in https://pytorch.org/tutorials/intermediate/dist_tuto.html,question need add manually,issue,negative,neutral,neutral,neutral,neutral,neutral
493787391,"I think that's how it should be:

https://pytorch.org/docs/stable/multiprocessing.html

> The function is called as fn(i, *args), where i is the process index and args is the passed through tuple of arguments.",think function process index,issue,negative,neutral,neutral,neutral,neutral,neutral
492614661,"> [Line 322 (and 323 with the additional `import`)](https://github.com/pytorch/examples/blob/5b1f45057dc14a5e2132b45233c258a1dc2a0aab/imagenet/main.py#L322-L323) do not contain the method header contained in your screenshot. The method header is defined in [line 376](https://github.com/pytorch/examples/blob/5b1f45057dc14a5e2132b45233c258a1dc2a0aab/imagenet/main.py#L376). I cannot help you, if you work with a modified version.

I see. I give up this fancy feature in python2.",line additional import contain method header method header defined line help work version see give fancy feature python,issue,negative,neutral,neutral,neutral,neutral,neutral
492551847,"@yrsun I am puzzled that why I will use tcp when I only used one node.

Could you please tell me?

Thanks.",puzzled use used one node could please tell thanks,issue,positive,positive,positive,positive,positive,positive
492233215,"> I think nn::Sequential cause the problem, when i use nn::Module to rewrite the code, the code with nn::Module got the right answer, but the code with nn::Sequential just got compile error. I don't know how to solve the problem!!!
> 
> I tested on the very latest libtorch and find that the nn::Sequential can't work in windows with viusal studio, anyone catch the problem?

I tried to compile it with Cmake first. Then I can use Visual Studio to run the dcgan example. 

Actually I also have some questions about these examples. Could you give me your contact like wechat so I can talk with you directly? Thanks :)",think cause problem use rewrite code code got right answer code got compile error know solve problem tested latest find ca work studio anyone catch problem tried compile first use visual studio run example actually also could give contact like talk directly thanks,issue,negative,positive,positive,positive,positive,positive
492023965,"googlenet is weird enough, i think we shouldn't support it in examples/imagenet/main.py 
the network doesn't converge when trained from scratch either",weird enough think support network converge trained scratch either,issue,negative,negative,negative,negative,negative,negative
491612171,"It was a hardware problem. I returned my GPU to the vendor and after they sent me a new one, the problem no longer exists.",hardware problem returned vendor sent new one problem longer,issue,negative,positive,positive,positive,positive,positive
491347469,"> I tried evaluate on imagenet val set with official pretrained vgg16_bn with the following code.
> `python main.py -a vgg16_bn --pretrained --evaluate <SOMEPATH>/imagenet/`
> The accuracy is poor, got top1 acc 1.054 and top5 acc 2.134.
> However I tried to swap the name of the directory of train set and val set to re-evaluate acc on test set, in case there was anything wrong with my command.
> I got top1 and top5 acc over 90, which seemed to be correct with train set. I'm really confused what's wrong with my evaluation on test set.

Hey have you solved this problem? Hope for your reply.",tried evaluate set official following code python evaluate accuracy poor got top top however tried swap name directory train set set test set case anything wrong command got top top correct train set really confused wrong evaluation test set hey problem hope reply,issue,negative,positive,neutral,neutral,positive,positive
491206282,"> > solved
> > Hello, I also encountered the same problem, l don't find any checkpoints and model saved, how do you solve it, thank you!

Me too. I ran it with the command:
`python main.py -a resnet50 --dist-url 'tcp://127.0.0.1:FREEPORT' --dist-backend 'nccl' --multiprocessing-distributed --world-size 1 --rank 0 [imagenet-folder with train and val folders]`
",hello also problem find model saved solve thank ran command python rank train,issue,positive,negative,negative,negative,negative,negative
490886853,"[Line 322 (and 323 with the additional `import`)](https://github.com/pytorch/examples/blob/5b1f45057dc14a5e2132b45233c258a1dc2a0aab/imagenet/main.py#L322-L323) do not contain the method header contained in your screenshot. The method header is defined in [line  376](https://github.com/pytorch/examples/blob/5b1f45057dc14a5e2132b45233c258a1dc2a0aab/imagenet/main.py#L376). I cannot help you, if you work with a modified version.",line additional import contain method header method header defined line help work version,issue,negative,neutral,neutral,neutral,neutral,neutral
490882764,"> @zzchust I can only repeat myself:
> 
> > [Line 301](https://github.com/pytorch/examples/blob/master/imagenet/main.py#L301) is empty. Did you insert something?
> 
> If the answer is _no_, can you try to insert
> 
> ```python
> from __future__ import print_function
> ```
> 
> in the file header? I'm unable to test it myself with python2 right now.

I try your suggestion, then raise another error
![image](https://user-images.githubusercontent.com/6135871/57453472-15093b00-7299-11e9-9087-eb791311d43c.png)
",repeat line empty insert something answer try insert python import file header unable test python right try suggestion raise another error image,issue,negative,negative,negative,negative,negative,negative
490880875,"@zzchust I can only repeat myself:

> [Line 301](https://github.com/pytorch/examples/blob/master/imagenet/main.py#L301) is empty. Did you insert something?

If the answer is _no_,  can you try to insert
```python
from __future__ import print_function
```
in the file header? I'm unable to test it myself with python2 right now.  
",repeat line empty insert something answer try insert python import file header unable test python right,issue,negative,negative,negative,negative,negative,negative
490851040,"> @pmeier It raised an error:
> ![image](https://user-images.githubusercontent.com/13645332/55713150-a418fe00-5a22-11e9-9e04-14ef3b5a64df.png)

I have the same problem for python2.7, torch0.4
while use python3,torch1.0 is ok

Anyone know the reason ?",raised error image problem torch use python torch anyone know reason,issue,negative,neutral,neutral,neutral,neutral,neutral
489293027,"@Shefali2653
you can refer to this this [pull request](https://github.com/pytorch/examples/pull/424/files/0ad346dc76fd4e20e09e612c291448273b081a28) from varunagrawal.
change your code structure and solve this warning",refer pull request change code structure solve warning,issue,negative,neutral,neutral,neutral,neutral,neutral
489078081,"I think nn::Sequential cause the problem, when i use nn::Module to rewrite the code, the code with nn::Module got the right answer, but the code with nn::Sequential just got compile error. I don't know how to solve the problem!!!

I tested on the very latest libtorch and find that the nn::Sequential can't work in windows with viusal studio, anyone catch the problem?",think cause problem use rewrite code code got right answer code got compile error know solve problem tested latest find ca work studio anyone catch problem,issue,negative,positive,positive,positive,positive,positive
488926465,"Sorry to bother you after so long time, but I have met the same problem with you. Have you solve your problem?",sorry bother long time met problem solve problem,issue,negative,negative,negative,negative,negative,negative
488850030,Please provide the code for hook as you mentioned in the tutorial,please provide code hook tutorial,issue,negative,neutral,neutral,neutral,neutral,neutral
488624980,"Hi? I am getting the same warning,how can i correct it ?",hi getting warning correct,issue,negative,neutral,neutral,neutral,neutral,neutral
487965626,"Hi, have you solved this problem? I got the same problem. Thank you!",hi problem got problem thank,issue,negative,neutral,neutral,neutral,neutral,neutral
487441665,"> > When I use Distribution in imagenet with one Node, there is three print at the same time.
> > Just like this.
> > ![TIMå›¾ç‰‡20190322114836](https://user-images.githubusercontent.com/31818905/54799812-b9a7cd00-4c99-11e9-809f-6b2da57f7545.png)
> > Is it right?
> 
> Hello, perhaps you know how to download the ImageNet dataset for this program to use?
> Please tell me, thank you very much!

Sorry, I get it from my colleague.",use distribution one node three print time like right hello perhaps know program use please tell thank much sorry get colleague,issue,positive,negative,negative,negative,negative,negative
487330030,"@deepakn94 You can use `APEX` library to enable automatic mixed precision by adding a few lines of code
https://developer.nvidia.com/automatic-mixed-precision
```
from apex import amp

model, optimizer = amp.initialize(model, optimizer)

with amp.scale_loss(loss, optimizer) as scaled_loss:
    scaled_loss.backward()
```",use apex library enable automatic mixed precision code apex import model model loss,issue,negative,neutral,neutral,neutral,neutral,neutral
487214839,"The rank indicator could be updated to include a more explicit hint of what it's for (""[R:..]"").

This change omits the indicator when not distributed, it could always be included, maybe with some non-numeric value when not distributed (""[-]"").

And could be a made a proper part of ProgressMeter with a bit more plumbing.",rank indicator could include explicit hint change indicator distributed could always included maybe value distributed could made proper part bit plumbing,issue,negative,negative,negative,negative,negative,negative
486888148,"Hello, I used a shuffle function I implemented myself, and there was some bug in it. After I switched back to the shuffle function provided in Pytorch, the problem was gone and the accuracy went up. Hope this help. ",hello used shuffle function bug switched back shuffle function provided problem gone accuracy went hope help,issue,negative,neutral,neutral,neutral,neutral,neutral
486183504,"> [NVIDIA docs](https://docs.nvidia.com/deploy/xid-errors/) lists xid31 as **GPU memory page fault** due to either driver fault or user app fault. What you could do is to try to run the script without pinning the memory ([here](https://github.com/pytorch/examples/blob/5df464c46cf321ed1cc3df1e670358d7f5ae1887/mnist/main.py#L89)) and see what happens.

I changed this
https://github.com/pytorch/examples/blob/5df464c46cf321ed1cc3df1e670358d7f5ae1887/mnist/main.py#L89
to
```python
kwargs = {'num_workers': 1, 'pin_memory': False} if use_cuda else {}
``` 

It is still not working. The errors are the same as before.",memory page fault due either driver fault user fault could try run script without pinning memory see python false else still working,issue,negative,negative,negative,negative,negative,negative
486173155,"> I have trained the resnet-18 model for classification on my own dataset with examples/imagenet/main.py. And now I want to infernece the images, but there is no inference code.


Hello? Why did you close it?
I really need your help!",trained model classification want inference code hello close really need help,issue,positive,positive,positive,positive,positive,positive
486138847,[NVIDIA docs](https://docs.nvidia.com/deploy/xid-errors/) lists xid31 as **GPU memory page fault** due to either driver fault or user app fault. What you could do is to try to run the script without pinning the memory ([here](https://github.com/pytorch/examples/blob/5df464c46cf321ed1cc3df1e670358d7f5ae1887/mnist/main.py#L89)) and see what happens.,memory page fault due either driver fault user fault could try run script without pinning memory see,issue,negative,negative,negative,negative,negative,negative
485889002,"> It could be due to cuDNN missing. Did you installed pytorch via anaconda?

Thanks for your reply.

I used pip3 to install pytorch. While I have just tried to use anaconda but got the same error.

After installing anaconda, the `collect_env.py` shows:

```
Collecting environment information...
PyTorch version: 1.0.1.post2
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 16.04.6 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration: GPU 0: GeForce RTX 2070
Nvidia driver version: 410.78
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.5.0

Versions of relevant libraries:
[pip] numpy==1.14.3
[pip] numpydoc==0.8.0
[pip] torch==1.0.1.post2
[pip] torchvision==0.2.2
[conda] _tflow_select             2.3.0                       mkl  
[conda] blas                      1.0                         mkl  
[conda] mkl                       2018.0.2                      1  
[conda] mkl-service               1.1.2            py36h17a0993_4  
[conda] mkl_fft                   1.0.1            py36h3010b51_0  
[conda] mkl_random                1.0.1            py36h629b387_0  
[conda] pytorch                   1.0.1           py3.6_cuda10.0.130_cudnn7.4.2_2    pytorch
[conda] tensorflow                1.9.0           mkl_py36h6d6ce78_1  
[conda] tensorflow-base           1.9.0           mkl_py36h2ca6a6a_0  
[conda] torchvision               0.2.2                      py_3    pytorch
```
And I got the same error:
```
Traceback (most recent call last):
  File ""main.py"", line 119, in <module>
    main()
  File ""main.py"", line 112, in main
    train(args, model, device, train_loader, optimizer, epoch)
  File ""main.py"", line 36, in train
    output = model(data)
  File ""/home/zniu/applications/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File ""main.py"", line 27, in forward
    x = F.relu(self.fc1(x))
  File ""/home/zniu/applications/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/zniu/applications/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py"", line 67, in forward
    return F.linear(input, self.weight, self.bias)
  File ""/home/zniu/applications/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py"", line 1352, in linear
    ret = torch.addmm(torch.jit._unwrap_optional(bias), input, weight.t())
RuntimeError: CUDA error: an illegal memory access was encountered
```

And I've found If I run `dmesg`, it will show

```
NVRM: Xid (PCI:0000:01:00): 31, Ch 00000028, engmask 00000101, intr 00000000
```

Could it be a hardware problem? I cannot run my own code either, the same Xid 31 error will appear, but the pytorch error messages differ.",could due missing via anaconda thanks reply used pip install tried use anaconda got error anaconda environment information version post build used build o version version version python version available yes version configuration driver version version relevant pip pip pip post pip blas got error recent call last file line module main file line main train model device epoch file line train output model data file line result input file line forward file line result input file line forward return input file line linear ret bias input error illegal memory access found run show could hardware problem run code either error appear error differ,issue,negative,positive,neutral,neutral,positive,positive
485851995,It could be due to cuDNN missing. Did you installed pytorch via anaconda?,could due missing via anaconda,issue,negative,negative,negative,negative,negative,negative
485475866,"> I run the example of imagenet in https://github.com/pytorch/examples/tree/master/imagenet, althougt I can run it successfully, but it is slow, and the Volatile GPU-Util is always 0 with command 'nvidia-smi'
> 
> ```
> +-----------------------------------------------------------------------------+
> | NVIDIA-SMI 390.87                 Driver Version: 390.87                    |
> |-------------------------------+----------------------+----------------------+
> | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
> | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
> |===============================+======================+======================|
> |   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |
> | 31%   58C    P2    70W / 250W |   9584MiB / 11170MiB |      0%      Default |
> +-------------------------------+----------------------+----------------------+
>                                                                                
> +-----------------------------------------------------------------------------+
> | Processes:                                                       GPU Memory |
> |  GPU       PID   Type   Process name                             Usage      |
> |=============================================================================|
> |    0       947      G   /usr/lib/xorg/Xorg                           285MiB |
> |    0      1752      G   compiz                                       154MiB |
> |    0      1930      G   fcitx-qimpanel                                 9MiB |
> |    0      4690      G   ...quest-channel-token=4115043597718524916    72MiB |
> |    0     26519      C   python                                      9057MiB |
> +-----------------------------------------------------------------------------+
> ```

Hello, perhaps you know how to download the ImageNet dataset for this program to use?
Please tell me, thank you very much!",run example run successfully slow volatile always command driver version name volatile fan temp compute mib mib default memory type process name usage mib mib mib mib python mib hello perhaps know program use please tell thank much,issue,positive,negative,neutral,neutral,negative,negative
485474773,"> When I use Distribution in imagenet with one Node, there is three print at the same time.
> Just like this.
> ![TIMå›¾ç‰‡20190322114836](https://user-images.githubusercontent.com/31818905/54799812-b9a7cd00-4c99-11e9-809f-6b2da57f7545.png)
> Is it right?

Hello, perhaps you know how to download the ImageNet dataset for this program to use?
Please tell me, thank you very much!",use distribution one node three print time like right hello perhaps know program use please tell thank much,issue,positive,positive,positive,positive,positive,positive
485474519,"> When I tried to run the model for the example/imagenet, I encounter such error.So could you tell me how to solve the problem?
> 
> python /home/zrz/code/imagenet_dist/examples-master/imagenet/main.py -a resnet18 -/home/zrz/dataset/imagenet/imagenet2012/ILSVRC2012/raw-data/imagenet-data
> 
> => creating model 'resnet18'
> 
> Epoch: [0][ 0/320292] Time 3.459 ( 3.459) Data 0.295 ( 0.295) Loss 7.2399e+00 (7.2399e+00) Acc@1 0.00 ( 0.00) Acc@5 0.00 ( 0.00)
> 
> Epoch: [0][ 10/320292] Time 0.043 ( 0.357) Data 0.000 ( 0.027) Loss 9.4861e+00 (1.3169e+01) Acc@1 0.00 ( 0.00) Acc@5 0.00 ( 0.00)
> 
> Epoch: [0][ 20/320292] Time 0.046 ( 0.209) Data 0.000 ( 0.014) Loss 7.3722e+00 (1.0817e+01) Acc@1 0.00 ( 0.00) Acc@5 0.00 ( 0.00)
> 
> Epoch: [0][ 30/320292] Time 0.032 ( 0.154) Data 0.000 ( 0.010) Loss 6.9166e+00 (9.5394e+00) Acc@1 0.00 ( 0.00) Acc@5 0.00 ( 0.00)
> 
> /opt/conda/conda-bld/pytorch_1549630534704/work/aten/src/THCUNN/ClassNLLCriterion.cu:105: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
> 
> Traceback (most recent call last):
> 
> File ""/home/zrz/code/imagenet_dist/examples-master/imagenet/main.py"", line 417, in
> 
> ```
> main()
> ```
> File ""/home/zrz/code/imagenet_dist/examples-master/imagenet/main.py"", line 113, in main
> 
> ```
> main_worker(args.gpu, ngpus_per_node, args)
> ```
> File ""/home/zrz/code/imagenet_dist/examples-master/imagenet/main.py"", line 239, in main_worker
> 
> ```
> train(train_loader, model, criterion, optimizer, epoch, args)
> ```
> File ""/home/zrz/code/imagenet_dist/examples-master/imagenet/main.py"", line 286, in train
> 
> ```
> losses.update(loss.item(), input.size(0))
> ```
> RuntimeError: CUDA error: device-side assert triggered
> 
> terminate called after throwing an instance of 'c10::Error'
> 
> what(): CUDA error: device-side assert triggered (insert_events at /opt/conda/conda-bld/pytorch_1549630534704/work/aten/src/THC/THCCachingAllocator.cpp:470)
> 
> frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x45 (0x7f099a50acf5 in /home/zrz/miniconda3/envs/runze_env_name/lib/python3.6/site-packages/torch/lib/libc10.so)
> 
> frame #1: + 0x123b8c0 (0x7f099e7ee8c0 in /home/zrz/miniconda3/envs/runze_env_name/lib/python3.6/site-packages/torch/lib/libcaffe2_gpu.so)
> 
> frame #2: at::TensorImpl::release_resources() + 0x50 (0x7f099ac76c30 in /home/zrz/miniconda3/envs/runze_env_name/lib/python3.6/site-packages/torch/lib/libcaffe2.so)
> 
> frame #3: + 0x2a836b (0x7f099818b36b in /home/zrz/miniconda3/envs/runze_env_name/lib/python3.6/site-packages/torch/lib/libtorch.so.1)
> 
> frame #4: + 0x30eff0 (0x7f09981f1ff0 in /home/zrz/miniconda3/envs/runze_env_name/lib/python3.6/site-packages/torch/lib/libtorch.so.1)
> 
> frame #5: torch::autograd::deleteFunction(torch::autograd::Function*) + 0x2f0 (0x7f099818dd70 in /home/zrz/miniconda3/envs/runze_env_name/lib/python3.6/site-packages/torch/lib/libtorch.so.1)
> 
> frame #6: std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release() + 0x45 (0x7f09c17f87f5 in /home/zrz/miniconda3/envs/runze_env_name/lib/python3.6/site-packages/torch/lib/libtorch_python.so)
> 
> frame #7: torch::autograd::Variable::Impl::release_resources() + 0x4a (0x7f09984001ba in /home/zrz/miniconda3/envs/runze_env_name/lib/python3.6/site-packages/torch/lib/libtorch.so.1)
> 
> frame #8: + 0x12148b (0x7f09c181048b in /home/zrz/miniconda3/envs/runze_env_name/lib/python3.6/site-packages/torch/lib/libtorch_python.so)
> 
> frame #9: + 0x31a49f (0x7f09c1a0949f in /home/zrz/miniconda3/envs/runze_env_name/lib/python3.6/site-packages/torch/lib/libtorch_python.so)
> 
> frame #10: + 0x31a4e1 (0x7f09c1a094e1 in /home/zrz/miniconda3/envs/runze_env_name/lib/python3.6/site-packages/torch/lib/libtorch_python.so)
> 
> frame #11: + 0x1993cf (0x5574e4c9a3cf in /home/zrz/miniconda3/envs/runze_env_name/bin/python3.6)
> 
> frame #12: + 0xf12b7 (0x5574e4bf22b7 in /home/zrz/miniconda3/envs/runze_env_name/bin/python3.6)
> 
> frame #13: + 0xf1147 (0x5574e4bf2147 in /home/zrz/miniconda3/envs/runze_env_name/bin/python3.6)
> 
> frame #14: + 0xf115d (0x5574e4bf215d in /home/zrz/miniconda3/envs/runze_env_name/bin/python3.6)
> 
> frame #15: + 0xf115d (0x5574e4bf215d in /home/zrz/miniconda3/envs/runze_env_name/bin/python3.6)
> 
> frame #16: + 0xf115d (0x5574e4bf215d in /home/zrz/miniconda3/envs/runze_env_name/bin/python3.6)
> 
> frame #17: PyDict_SetItem + 0x3da (0x5574e4c37e7a in /home/zrz/miniconda3/envs/runze_env_name/bin/python3.6)
> 
> frame #18: PyDict_SetItemString + 0x4f (0x5574e4c4078f in /home/zrz/miniconda3/envs/runze_env_name/bin/python3.6)
> 
> frame #19: PyImport_Cleanup + 0x99 (0x5574e4ca4709 in /home/zrz/miniconda3/envs/runze_env_name/bin/python3.6)
> 
> frame #20: Py_FinalizeEx + 0x61 (0x5574e4d105f1 in /home/zrz/miniconda3/envs/runze_env_name/bin/python3.6)
> 
> frame #21: Py_Main + 0x35e (0x5574e4d1b1fe in /home/zrz/miniconda3/envs/runze_env_name/bin/python3.6)
> 
> frame #22: main + 0xee (0x5574e4be402e in /home/zrz/miniconda3/envs/runze_env_name/bin/python3.6)
> 
> frame #23: __libc_start_main + 0xf5 (0x7f09d9c2e3d5 in /lib64/libc.so.6)
> 
> frame #24: + 0x1c3e0e (0x5574e4cc4e0e in /home/zrz/miniconda3/envs/runze_env_name/bin/python3.6)

Hello, perhaps you know how to download the ImageNet dataset for this program to use?
Please tell me, thank you very much!",tried run model encounter could tell solve problem python model epoch time data loss epoch time data loss epoch time data loss epoch time data loss void long long float float block thread assertion recent call last file line main file line main file line train model criterion epoch file line train error assert triggered terminate throwing instance error assert triggered frame frame frame frame frame frame torch torch frame frame torch frame frame frame frame frame frame frame frame frame frame frame frame frame frame frame main frame frame hello perhaps know program use please tell thank much,issue,negative,positive,neutral,neutral,positive,positive
485474157,"> I have trained the resnet-18 model for classification on my own dataset with examples/imagenet/main.py. And now I want to infernece the images, but there is no inference code.

Hello, perhaps you know how to download the ImageNet dataset for this program to use?
Please tell me, thank you very much!",trained model classification want inference code hello perhaps know program use please tell thank much,issue,positive,positive,positive,positive,positive,positive
485414618,"> solved
Hello, I also encountered the same problem, l don't find any checkpoints and model saved, how do you solve it, thank you!
",hello also problem find model saved solve thank,issue,positive,neutral,neutral,neutral,neutral,neutral
484663389,"That seems to have fixed this particular error, although the script now errors out on line 377:
```
root@9b47a454adcf:/data/pytorch_examples_latest/imagenet# python main.py 
  File ""main.py"", line 377
    def __init__(self, num_batches, *meters, prefix=""""):
                                                  ^
SyntaxError: invalid syntax
```",fixed particular error although script line root python file line self invalid syntax,issue,negative,positive,positive,positive,positive,positive
484658678,"this is so weird, i cant see why this is not Py2 syntax.

Does it help to add at the top of the file  `from __future__ import print_function` ?",weird cant see syntax help add top file import,issue,negative,neutral,neutral,neutral,neutral,neutral
484549976,"Given that the power consumption is 70W, I would say the GPU is actually computing.
I think is a bug of nvidia-smi, and I have the same behaviour.
",given power consumption would say actually think bug behaviour,issue,negative,neutral,neutral,neutral,neutral,neutral
483379526,"This is an error that appears only for Py2 and not for Py3. Can someone please fix it so we can run benchmarks on both Py2 and Py3 like the earlier version did? To be clear, this is a regression in functionality.",error someone please fix run like version clear regression functionality,issue,positive,positive,positive,positive,positive,positive
483117082,"Nevermind, the extended slicing syntax looks pretty good too:

https://docs.python.org/2.3/whatsnew/section-slices.html",extended slicing syntax pretty good,issue,positive,positive,positive,positive,positive,positive
483087429,"No. I gave up.

On Mon, 15 Apr 2019 at 12:28 pm, zhicheng <notifications@github.com> wrote:

> Have you solved this issue? I have met the same problem.
>
> â€”
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/pytorch/examples/issues/538#issuecomment-483087186>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ANQj8lOdfasj-GCm3AXWK6EU_27mcUyeks5vg-PDgaJpZM4cZUor>
> .
>
",gave mon wrote issue met problem thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
482435167,"set `torch.backends.cudnn.deterministic = True` would solve the problem.
The GPU code (CuDNN library) trades off some determinism for speed.",set true would solve problem code library determinism speed,issue,negative,positive,positive,positive,positive,positive
482190041,"we improved on the error message as well, it'll be part of the next pytorch release.",error message well part next release,issue,negative,neutral,neutral,neutral,neutral,neutral
482123672,"This error is due to the filter sizes being larger than the input in the last convolutional layer of the discriminator. 

If you follow the sizes through, by the time the image reaches the final convolutional layer, it has a shape of `(batch, 512, 2, 2)`. The final convolutional layer has a 4x4 filter which is not compatible with a 2x2 image. 

It seems this example was written to only with with the `imageSize` parameter set to 64.",error due filter size input last convolutional layer discriminator follow size time image final convolutional layer shape batch final convolutional layer filter compatible image example written parameter set,issue,negative,negative,neutral,neutral,negative,negative
480781192,"It works fine for me. [Line 301](https://github.com/pytorch/examples/blob/master/imagenet/main.py#L301) is empty. Did you insert something?
",work fine line empty insert something,issue,negative,positive,positive,positive,positive,positive
479649862,"I am facing the same issue, could you tell me what is the error?",facing issue could tell error,issue,negative,neutral,neutral,neutral,neutral,neutral
479098254,"@pmeier when I say ""tweak the LR by hand"", I mean ""write the code by hand"", rather than relying on an opaque class. By having `adjust_learning_rate` close the the rest of the code, in the same file, the user can see and modify immediately the function",say tweak hand mean write code hand rather opaque class close rest code file user see modify immediately function,issue,positive,negative,negative,negative,negative,negative
479039163,With the new reference scripts (which were comitted after I created this PR) you are indeed correct. I will open a new PR in `pytorch/vision`.  ,new reference indeed correct open new,issue,negative,positive,neutral,neutral,positive,positive
479036736,"@soumith Out of curiosity: what purpose does the `adjust_learning_rate` function serve, if we expect the user to adjust the learning rate on his own?",curiosity purpose function serve expect user adjust learning rate,issue,negative,neutral,neutral,neutral,neutral,neutral
477861542,ideally the script needs to be refactored to push everything into a main() function (that's the original problem),ideally script need push everything main function original problem,issue,negative,positive,positive,positive,positive,positive
477861447,"add the following lines to the end of the imports section, right after: `import torchvision.utils as vutils`

```
if __name__ == '__main__':
    torch.multiprocessing.set_start_method('spawn')
```",add following end section right import,issue,negative,positive,positive,positive,positive,positive
477446364,"yes, loss Tensors are always scalars (i.e.e single value Tensors) by default. We use mean reduction by default.",yes loss always single value default use mean reduction default,issue,negative,negative,negative,negative,negative,negative
477412392,No problem at all. And I appreciate you letting me know either way.,problem appreciate know either way,issue,negative,neutral,neutral,neutral,neutral,neutral
477404625,"I really apologize, but I merged https://github.com/pytorch/examples/pull/535 which is the same as yours but came ~10 days after your PR.

Due to the ordering of reading github notifications, I screwed up. I hope you dont mind.",really apologize came day due reading screwed hope dont mind,issue,negative,positive,neutral,neutral,positive,positive
477278431,"so if you are using python2, dont use the distributed launch utility, but use something like environment variable initialization",python dont use distributed launch utility use something like environment variable,issue,negative,neutral,neutral,neutral,neutral,neutral
477278192,"which example are you using?

This can happen if you initialized GPU0 before you forked, so at the time of fork both GPU0 and then later GPU6 will be initialized.",example happen forked time fork later,issue,negative,neutral,neutral,neutral,neutral,neutral
477272285,"it was an explicit choice to not use LR schedulers when we wrote the imagenet example, as this particular user space often wants to tweak learning rates by hand.

Having a level of indirection on how to be able to tweak the LR by hand felt like it would miss the point for those sets of users who will go grappling to other avenues like the forums to find out how to do it.",explicit choice use wrote example particular user space often tweak learning hand level indirection able tweak hand felt like would miss point go grappling like find,issue,positive,positive,positive,positive,positive,positive
477270807,"I think for a base example, adding specialized logging is a bit of an overkill. My main hesitation is to increase the line count of the file over something that's not very important for an example script.

It makes more sense in something like torchvision reference scripts though: https://github.com/pytorch/vision/pull/819",think base example specialized logging bit main hesitation increase line count file something important example script sense something like reference though,issue,positive,negative,neutral,neutral,negative,negative
475517229,@666zz666 Don't you think that there might exist a bug in main.py at line mp.spawn and main_worker accept 3 parameters?,think might exist bug line accept,issue,negative,neutral,neutral,neutral,neutral,neutral
475311621,"@bartolsthoorn thank you for the reply. Iâ€™m pretty new to GAN training, if I have downloaded art images from wiki art and they have different sizes, do I have to somehow preprocess all of them to the same size (eg. 512x512)? What about rectangular images? ",thank reply pretty new gan training art art different size somehow size rectangular,issue,positive,positive,positive,positive,positive,positive
475301332,"DCGAN is quite old. Check the latest papers on GANs and you will find many
large resolution models/examples. You need a dataset with high resolution
images as well (of course).

On Thu, 21 Mar 2019 at 16:56, Chi Nok Enoch K <notifications@github.com>
wrote:

> Not sure if this thread is still active, but did anyone try to generate
> 128x128 images and upscale to 512x512 per @bartolsthoorn
> <https://github.com/bartolsthoorn> 's suggestion?
>
> â€”
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/pytorch/examples/issues/70#issuecomment-475289841>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AAGgWk-LFMMdIzfFfUK6Fg2YBQHumo6yks5vY6vBgaJpZM4MDwky>
> .
>
",quite old check latest find many large resolution need high resolution well course mar chi wrote sure thread still active anyone try generate upscale per suggestion reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
475289841,"Not sure if this thread is still active, but did anyone try to generate 128x128 images and upscale to 512x512 per @bartolsthoorn 's suggestion?",sure thread still active anyone try generate upscale per suggestion,issue,positive,positive,positive,positive,positive,positive
474853790,the same problem that dataloader time is unstable. How did you finally solve it?,problem time unstable finally solve,issue,negative,neutral,neutral,neutral,neutral,neutral
474443521,"`CUDA_VISIBLE_DEVICES=2` makes it so that the only the 3rd GPU (i.e. at index 2) is visible. If you don't have at least three GPUs then no devices will be accessible.

If you want only the first two GPUs visible use `CUDA_VISIBLE_DEVICES=0,1`",index visible least three accessible want first two visible use,issue,negative,positive,positive,positive,positive,positive
474427664,"I exactly have the same problem. Whether or not I set the CUDA_VISIBLE_DEVICES env. var., my both devices (P100) are always idle. I also confirm the test by @drscotthawley that setting the env. var. would disguise the devices, whereas unsetting it would expose the devices.

@colesbury: would you please elaborate what you meant by your answer?",exactly problem whether set always idle also confirm test setting would disguise whereas unsetting would expose would please elaborate meant answer,issue,negative,positive,positive,positive,positive,positive
473273301,"> first resize to a bigger size then crop, some example directly use resize(224)

`transforms.Resize()` resizes the smallest edge to the given value. Thus, if the image is not square (`height != width`) `Resize(224)` would fail to give you an image of size `(224, 224)`.

I think the image is resized at first to make motifs roughly the same size. However, I have no clue on why or how the specific value `256` was chosen.

> inceptionv3 the input size is 299, while others is 224. so the resize parameter counld set to what?

I don't think `inception_v3` can be trained with the current implementation. If you just care about this you could change it to
```python
...
transforms.Resize(299),
transforms.CenterCrop(299),
...
```
For a general solution see #268.

",first resize bigger size crop example directly use resize edge given value thus image square height width resize would fail give image size think image first make roughly size however clue specific value chosen input size resize parameter set think trained current implementation care could change python general solution see,issue,positive,positive,neutral,neutral,positive,positive
471915485,"I **do not remember** now whether I discovered the root cause, resolved it or circumvented it... This seems to be an old story... But I was certainly not using the imagenet dataset.

So I guess it would be wise to close the issue from my end.

Thanks, and sorry!",remember whether discovered root cause resolved old story certainly guess would wise close issue end thanks sorry,issue,positive,positive,positive,positive,positive,positive
471908173,"I have the same issue as like you.
The problem was the files in the Imagenet training/validation files.
Just remove tar files in the each training/validation folder include including folders.

It works for me :)",issue like problem remove tar folder include work,issue,negative,neutral,neutral,neutral,neutral,neutral
471243712,"while you are right, there are syntax errors with your PR (did you compile the code after modifying it?)",right syntax compile code,issue,negative,positive,positive,positive,positive,positive
469963018,"@MakeDirtyCode 
I think this point makes sense.
In the absence of BN, adding bias would be good.
But in the large networks adding/removing  biases in one of the layers wont make much difference. So i think we can omit the bias to reduce number of learning params.",think point sense absence bias would good large one wont make much difference think omit bias reduce number learning,issue,negative,positive,positive,positive,positive,positive
467338488,"parser.add_argument('--save-model', action='store_true', default=False,
                        help='For Saving the current Model')
default rewrite True in parameter",saving current model default rewrite true parameter,issue,negative,positive,positive,positive,positive,positive
466867497,"no real rationale, just empirical success.",real rationale empirical success,issue,positive,positive,positive,positive,positive,positive
466738913,"@rafaelsimonmaia @aakashbehl1994 @QimingChen 
I  m facing issuing in loading  ImageFolder(""/train/"") to read png files in folder train.
> The file structure is
> /train/
             >label1
              >label2
              >label3
Each subfolders have pngs like label1 has mix numbers png's : 1,3,4,8,9....1000
label2 has 118,2,.....3000 label3 folder has 3001..3064 images .

> 
> I failed to load them, leading to errors:
> RuntimeError: Found 0 images in subfolders of: ./data
> Supported image extensions are: .jpg,.JPG,.jpeg,.JPEG,.png,.PNG,.ppm,.PPM,.bmp,.BMP.
Can anyone guide me please",facing issuing loading read folder train file structure label label label like label mix label label folder load leading found image anyone guide please,issue,positive,neutral,neutral,neutral,neutral,neutral
466215881,"@brando90 That was simply there before so I didn't touch it. In this case I believe it's roughly the starting performance of the agent, but this should ideally be set to 0 or documented with a comment.",simply touch case believe roughly starting performance agent ideally set comment,issue,negative,positive,positive,positive,positive,positive
466173612,"@Kaixhin why do you start ur running return with 10?

https://github.com/pytorch/examples/blob/bc8feda9cce9d23e39efbe414a0e14f0a2de7f55/reinforcement_learning/reinforce.py#L79",start ur running return,issue,negative,neutral,neutral,neutral,neutral,neutral
466107529,"@Kaixhin  I think there are still mistakes. e.g.
```
            print('Episode {}\tLast reward: {:.2f}\tAverage reward: {:.2f}'.format(
                  i_episode, ep_reward, running_reward))
```
ep_reward is not an average but a sum of (nondiscoutned) rewards for the current episode, so saying average reward doesn't make sense to me.",think still print reward reward average sum current episode saying average reward make sense,issue,positive,negative,neutral,neutral,negative,negative
466084719,That's a good point @brando90 - you should go ahead and submit a PR to clear up anything like this.,good point go ahead submit clear anything like,issue,positive,positive,positive,positive,positive,positive
465526142,"Thank you so much.
I can confirm matching between batch_size:64 and batch_size:1.
",thank much confirm matching,issue,negative,positive,positive,positive,positive,positive
465511325,"model is in training mode, and there is BatchNorm.

before you do inference, do `model.eval()`",model training mode inference,issue,negative,neutral,neutral,neutral,neutral,neutral
465108459,"while the code is concise and good, we want pytorch/examples to not be overwhelming, which means only a few examples (and probably just 1 or 2 on a topic).
So I think you can keep it as a separate project, and add it to a list like https://github.com/ritchieng/the-incredible-pytorch",code concise good want overwhelming probably topic think keep separate project add list like,issue,positive,positive,positive,positive,positive,positive
465027154,"To get it to work I changed the final conv layer of the Generator to: `nn.ConvTranspose2d(    ngf,      nc, kernel_size=1, stride=1, padding=0, bias=False)`

and the final conv layer of the Discriminator to: `nn.Conv2d(ndf * 8, 1, 2, 2, 0, bias=False)`.

For reference, I added code that fixed this [here](https://github.com/csinva/pytorch_gan_pretrained/blob/master/cifar10/dcgan.py)",get work final layer generator final layer discriminator reference added code fixed,issue,negative,positive,neutral,neutral,positive,positive
463039575,"I tried to make my code work, thanks a lot!!",tried make code work thanks lot,issue,negative,positive,positive,positive,positive,positive
462653145,@juyoungk what pytorch version are you on? `print(torch.__version__)`. Make sure you're on the latest version,version print make sure latest version,issue,negative,positive,positive,positive,positive,positive
461691033,"Can also confirm that  @QimingChen, solution works. ",also confirm solution work,issue,negative,neutral,neutral,neutral,neutral,neutral
460063589,">           the wrong installation of Pytorch, need installation with Cuda.

@irazakharchenko  your meaning is that we should install CUDA to solve the problem???",wrong installation need installation meaning install solve problem,issue,negative,negative,negative,negative,negative,negative
458827913,"What difference would it bring to the training if I simply use:
```
    dl = DataLoader(...,  sampler=sampler)
    diter = iter(dl)
    for it in range(max_iter):
        try:
            im, lb = next(diter)
            if not im.size()[0]==batchsize: continue
        except StopIteration:
            diter = iter(dl)
            im, lb = next(diter)
```
Without setting the epoch, will the training be misled to unhealthy states?",difference would bring training simply use diter iter range try next diter continue except diter iter next diter without setting epoch training misled unhealthy,issue,negative,negative,neutral,neutral,negative,negative
458808573,"it's for determinism across runs, i.e. same samples are fetched to workers across multiple runs even across distributed training.",determinism across fetched across multiple even across distributed training,issue,negative,neutral,neutral,neutral,neutral,neutral
455911644,"capitalizing input just looks weird. none of the other variables will be capitalized.

In the context of just this example, `images` might make sense.",input weird none context example might make sense,issue,negative,negative,negative,negative,negative,negative
455911200,"we are explicitly not planning to change `input`, as it's the best word that describes it. `predictor` doesn't even come close.

Removing the `sys` package sounds good.",explicitly change input best word predictor even come close removing package good,issue,positive,positive,positive,positive,positive,positive
454180516,"> Hi, I finally figured out a solution. Maybe you can try this:
> CUDA_VISIBLE_DEVICES=0,1 python3 main.py -j 0 -a resnet50 --lr 0.01 --epochs 20 --dist-url 'tcp://192.168.100.1:9999' --gpu-number 2 --dist-backend 'nccl' --multiprocessing-distributed ~/imagenet/
> 
> The trick part is: CUDA_VISIBLE_DEVICES=0 for 1GPU
> CUDA_VISIBLE_DEVICES=0,1 for 2GPUs
> 
> Please let me know if this solution can help you.
> 
> Good luck

This answer cannot work for me, is there any other solution? Because the error is ""AttributeError: module 'torch.multiprocessing' has no attribute 'spawn'"", I guess maybe the python version problem? I am using pytorch1.0 and python3.7",hi finally figured solution maybe try python trick part please let know solution help good luck answer work solution error module attribute guess maybe python version problem python,issue,positive,positive,positive,positive,positive,positive
453916209,"For some reason, the added missing rank argument doesn't show in the README.md",reason added missing rank argument show,issue,negative,negative,negative,negative,negative,negative
452580340,@npmhung yes this is intentional to not be thread-safe. This method is called Hogwild training.,yes intentional method training,issue,negative,neutral,neutral,neutral,neutral,neutral
452559079,"I dont think we need this. It's also a simple example, so one can take and modify it :)",dont think need also simple example one take modify,issue,negative,neutral,neutral,neutral,neutral,neutral
452303102,"~~Did anyone figure this out?~~

Edit: just read that MP only works with v1.0",anyone figure edit read work,issue,negative,neutral,neutral,neutral,neutral,neutral
451376854,@bionicles   i think it is explanatory except this https://github.com/pytorch/examples/blob/e0929a4253f9ae6ccdde24e787788a9955fdfe1c/dcgan/main.py#L232  might cause trouble,think explanatory except might cause trouble,issue,negative,negative,negative,negative,negative,negative
450605730,"Do you have multiple GPUs on your OSX machine? :O :O :O 

Right now, we dont support CUDA or multi-GPU or a few of the distributed backends (such as gloo) on OSX, but if you are inclined, compile on OSX from source and it should have `torch.distributed.deprecated` working (i.e. the old distributed backend).",multiple machine right dont support distributed compile source working old distributed,issue,negative,positive,positive,positive,positive,positive
450600334,"Thanks @soumith . I tried that indeed, but I was just wondering if it was documented anywhere, looked all over the place (e.g. maybe another backend would work?). Any tips on using MacOS anyway? It's mostly for convenience, dev testing.
[I am assuming the author of this issue was on MacOS too, so i guess the issue can be closed]
I also noticed TCP has been deprecated, but the docs seem full with it. Am i correct in thinking the docs need updating? I'd be glad to help out.",thanks tried indeed wondering anywhere place maybe another would work anyway mostly convenience dev testing assuming author issue guess issue closed also seem full correct thinking need glad help,issue,positive,positive,positive,positive,positive,positive
450600239,the OSX binary of PyTorch does not ship with distributed support. You can check this with: `torch.distributed.is_available()` and it returns `False`,binary ship distributed support check false,issue,negative,negative,negative,negative,negative,negative
450557163,"I have a similar problem. The code runs well on 2 GPU, but when i run the code on 4 gpu, it freezes at the begining. Then i upgrade my pytorch from version 0.3.1 to 0.4.1, it can run for a few iteration but it stalls again and the process is sleeping. I degrade pytorch to 0.3.1 and compare to the code  last successful running on 4 GPU. The reason is that I use a mediate model( mediate_out = modelA(input), out = modelB(mediate_out),  and after merging the two models,  it works.",similar problem code well run code upgrade version run iteration process sleeping degrade compare code last successful running reason use mediate model input two work,issue,negative,positive,positive,positive,positive,positive
450537446,"Hi, your question does not have any context.
I am not sure what you are asking about, but if you want to all-reduce the gradients, then yes this line should work.

Please use https://discuss.pytorch.org for questions, and any further follow-up",hi question context sure want yes line work please use,issue,positive,positive,positive,positive,positive,positive
450537179,"This seems like an issue with your cygwin environment.
If you are using Windows, try using PyTorch for Windows directly rather than cygwin.",like issue environment try directly rather,issue,negative,positive,neutral,neutral,positive,positive
450537020,"The numbers even for the small models were not for WikiText2, but for PTB, so removed them as well.

Closed via https://github.com/pytorch/examples/commit/97e3e13d3628b7016dc315ad64119663955c5075 and https://github.com/pytorch/examples/commit/e0929a4253f9ae6ccdde24e787788a9955fdfe1c",even small removed well closed via,issue,negative,negative,negative,negative,negative,negative
450536995,"After looking at the `git blame`, I've removed the perplexity numbers from the README entirely, as they were all the numbers for Penn-TreeBank and not WikiText2.

Closed via https://github.com/pytorch/examples/commit/97e3e13d3628b7016dc315ad64119663955c5075 and https://github.com/pytorch/examples/commit/e0929a4253f9ae6ccdde24e787788a9955fdfe1c",looking git blame removed perplexity entirely closed via,issue,negative,negative,neutral,neutral,negative,negative
450536895,"About reported perplexities on the larger models, please see https://github.com/pytorch/examples/commit/97e3e13d3628b7016dc315ad64119663955c5075

As @HolyLow pointed out in another thread, the given perplexity numbers for the larger models were on Penn_Treebank and not on WikiText2, so removed those references from README for now",please see pointed another thread given perplexity removed,issue,negative,neutral,neutral,neutral,neutral,neutral
450536776,"@HolyLow you are right, the README needs to be updated with baselines on WT2. For now, I am removing the perplexity values in the README",right need removing perplexity,issue,negative,positive,positive,positive,positive,positive
450276556,It seems from [https://github.com/pytorch/examples/issues/281](url) that the perplexity in the README is based on PTB dataset while now the default provided dataset is WT2. Maybe the README should be updated.,perplexity based default provided maybe,issue,negative,neutral,neutral,neutral,neutral,neutral
450276026,"As discussed in [https://github.com/pytorch/examples/issues/415](url), many (including myself) fail to reproduce the claimed perplexity as reported in the README. I really want to know what I've done wrong. @soumith @Smerity ",many fail reproduce perplexity really want know done wrong,issue,negative,negative,neutral,neutral,negative,negative
450274535,"For

python main.py --emsize 1500 --nhid 1500 --dropout 0.65 --epochs 40 --tied

I also get a test perplexity of 83.62. Have no idea why the reported performance is much better than mine.",python dropout tied also get test perplexity idea performance much better mine,issue,negative,positive,positive,positive,positive,positive
449967378,"Had kind of the same issue with the same error but my padding was correct. With my issue I was flattening incorrectly and was able to resolve by verifying the size of x before flattening the tensor and corrected my x.view and the fc1 layer accordingly.

```
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # convolutional layer
        self.conv1 = nn.Conv2d(3, 6, 3, padding=1)
        self.conv2 = nn.Conv2d(6, 16, 3, padding=1)
        self.conv3 = nn.Conv2d(16, 32, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        ->self.fc1 = nn.Linear(32 * 4 * 4, 120)
        self.fc2 = nn.Linear(120, 10)

    def forward(self, x):
        # add sequence of convolutional and max pooling layers
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.pool(F.relu(self.conv3(x)))
        ->print(x.size())
        ->x = x.view(-1, 32 * 4 * 4)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        
        return x
```",kind issue error padding correct issue flattening incorrectly able resolve size flattening tensor corrected layer accordingly class net self super net self convolutional layer forward self add sequence convolutional print return,issue,positive,positive,positive,positive,positive,positive
449950610,"thanks @QimingChen, it worked for me after looking at your explanation. ",thanks worked looking explanation,issue,negative,positive,positive,positive,positive,positive
449727283,"For

`python main.py --emsize 1500 --nhid 1500 --dropout 0.65 --epochs 40 --tied`

I also get a test perplexity of 84.12. 

I observed similar discrepancies for the other parameter settings.
",python dropout tied also get test perplexity similar parameter,issue,negative,neutral,neutral,neutral,neutral,neutral
449564775,"Yes, you can do that @xjdeng, you simply have to ensure that the output of your FC layer has the ratio you want from the final output. That is one way. So in your case, the dense layer output should be (batch_size, channels, 4, 1) or some multiple of that. If your network then consists of transposed convolutions that double in size each layer you would need 5 transposed conv layers to get images of size (batch_size, channels, 128, 32)",yes simply ensure output layer ratio want final output one way case dense layer output multiple network double size layer would need get size,issue,positive,neutral,neutral,neutral,neutral,neutral
449125718,"@soumith 
locally i coudnt give a try due to unavailability of the gpu .
Anyway thanks for the response .
Will keep these points in mind for future PRs.
Once Again Thanks.",locally give try due unavailability anyway thanks response keep mind future thanks,issue,positive,positive,neutral,neutral,positive,positive
449124963,"Thanks @surgan12 

I dont like the fact that it complicates the examples/imagenet/main.py with new concepts such as multiprocessing.Manager. The examples are aimed to be simple / easy to understand.

Also, I see that there's a global `best_acc1 = 0` that should be accessible, so I dont think your PR fixes the underlying problem. Did you verify locally that it fixes the issue?

",thanks dont like fact new simple easy understand also see global accessible dont think underlying problem verify locally issue,issue,positive,positive,positive,positive,positive,positive
449109827,"@soumith could you tell me if this is something working ?
Thanks !!",could tell something working thanks,issue,negative,positive,positive,positive,positive,positive
449036401,Is there any way to implement a DCGAN that generates rectangular images (ie 128x32) in Pytorch? Nearly every example I've seen works with square images.  ,way implement rectangular ie nearly every example seen work square,issue,negative,positive,neutral,neutral,positive,positive
448861533,"Likewise, for 

```python main.py --emsize 1500 --nhid 1500 --dropout 0.65 --epochs 40 --tied```

I get test perplexity 84.17 instead of 72.3 in README. Has anyone been able to close the gap to the reported perplexities?",likewise python dropout tied get test perplexity instead anyone able close gap,issue,negative,positive,positive,positive,positive,positive
448691706,you can change the kernel size = 2 for final layer of the discriminator and kernel size = 1 for generator. This will work for imagesize = 32,change kernel size final layer discriminator kernel size generator work,issue,negative,neutral,neutral,neutral,neutral,neutral
448581780,"@FraLotito 
Hi, As far as I know the hidden layer in Word2Vec models doesn't contain any activation function. Hidden layer weights actually represents the Word Vectors trained.Hence, no non-linearity function is added.",hi far know hidden layer contain activation function hidden layer actually word function added,issue,negative,negative,neutral,neutral,negative,negative
448118805,"@soumith , I think the issue is with accessing  best_acc on multiple GPUs acrosss multiples processes , so I sort of made it accessible across all the process .
I am not sure if this is something working here .",think issue multiple sort made accessible across process sure something working,issue,negative,positive,positive,positive,positive,positive
448087582,"@surgan12 i fail to see how this fixes https://github.com/pytorch/examples/issues/476
could you explain?",fail see could explain,issue,negative,negative,negative,negative,negative,negative
447886539,"Looking forward to it. Since the current [example script](https://github.com/pytorch/examples/blob/master/imagenet/main.py) does validation for every process. If I am training on 4 nodes with 4 GPUs on each, there would be 16 validation processing running at the same time, which is not quite ideal. ",looking forward since current example script validation every process training would validation running time quite ideal,issue,positive,positive,positive,positive,positive,positive
447546301,I came across the same error as this. Is this related to the version of pytorch? Thanks.,came across error related version thanks,issue,negative,positive,neutral,neutral,positive,positive
447331556,"I have find it. ""http://docs.pyro.ai/en/0.3.0-release/primitives.html?highlight=random_module"".
But I still don't know the meaning of` infer={""enumerate"": ""parallel""}`
Thank you",find still know meaning enumerate parallel thank,issue,negative,neutral,neutral,neutral,neutral,neutral
447231153,"It may be a problem with the function name:
https://github.com/python-pillow/Pillow/pull/2993",may problem function name,issue,negative,neutral,neutral,neutral,neutral,neutral
446928086,"i set the batch size to something around 500 and num_workers as 16
",set batch size something around,issue,negative,neutral,neutral,neutral,neutral,neutral
446925550,I sort of had the same problem but increasing the batch size and num workers did the trick for me ,sort problem increasing batch size trick,issue,negative,neutral,neutral,neutral,neutral,neutral
446883880,"> did you try increasing the num-workers ?
> maybe something like 16 ?

Yes, I have tried, but it doesn't work.",try increasing maybe something like yes tried work,issue,positive,neutral,neutral,neutral,neutral,neutral
446881679,"did you try increasing the num-workers ?
maybe something like 16 ?",try increasing maybe something like,issue,negative,neutral,neutral,neutral,neutral,neutral
446858444,"ah yes, what I said above is only true if we also `retain_graph=True`. My bad, I stand corrected.",ah yes said true also bad stand corrected,issue,negative,negative,negative,negative,negative,negative
446852900,"@soumith  i have made the changes as asked by you.
Kindly look into it .
Thanks",made kindly look thanks,issue,positive,positive,positive,positive,positive,positive
446555961,"I ran this python code , with batch size 500 , which is way better than 64 or 150 or 4 .
And my code is running successfully without any error and accuracy more than 99% is there ",ran python code batch size way better code running successfully without error accuracy,issue,positive,positive,positive,positive,positive,positive
446150467,"Encountered the same issue, but solved by QimingChen's solution.
Anyway, the logic of detecting a path is a bit strange.",issue solution anyway logic path bit strange,issue,negative,negative,neutral,neutral,negative,negative
446104269,"> I got the above error message with 0.2.0 version.
> 
> When I build pytorch master branch.('0.4.0a0+a3bf06c')
> There was no error message. It works well.
> Thank you.

How do you build?",got error message version build master branch error message work well thank build,issue,negative,neutral,neutral,neutral,neutral,neutral
445704547,"@liu09114 ,

The code is not broken, it operates as explained above.  I see that you gave [this explanation](https://github.com/pytorch/examples/issues/232#issuecomment-395198394) a -1, which doesn't help in this case because it is not clear what you don't like about the implemented behavior and it would help if you gave some detail.

Thanks
Neta",code broken see gave explanation help case clear like behavior would help gave detail thanks,issue,positive,negative,neutral,neutral,negative,negative
445519193,"Thanks @soumith  , it was my first contribution to pytorch and I feel very good about it .
",thanks first contribution feel good,issue,positive,positive,positive,positive,positive,positive
445102737,I see identical behavior with pytorch 0.4.1.post2 ,see identical behavior post,issue,negative,neutral,neutral,neutral,neutral,neutral
443827539,see https://github.com/pytorch/examples/tree/master/imagenet#training . we mention that default learning rate has to be reduced for alexnet,see mention default learning rate reduced,issue,negative,neutral,neutral,neutral,neutral,neutral
443719042,@Robomate can you please explain why this issue was closed? I'm facing the same problem.Thanks!,please explain issue closed facing,issue,negative,negative,neutral,neutral,negative,negative
443456639,"I have this issue as well.

```
wv_arr = torch.Tensor(wv_arr).view(-1, wv_size)
RuntimeError: invalid argument 2: size '[-1 x 300]' is invalid for input with 335552746 elements at /pytorch/aten/src/TH/THStorage.cpp:80
```",issue well invalid argument size invalid input,issue,negative,neutral,neutral,neutral,neutral,neutral
443109947,use https://discuss.pytorch.org as a support resource. There were some discussions on the forums that had a similar question,use support resource similar question,issue,negative,neutral,neutral,neutral,neutral,neutral
441847760,"Hi, I finally figured out a solution. Maybe you can try this:
CUDA_VISIBLE_DEVICES=0,1 python3 main.py -j 0 -a resnet50 --lr 0.01 --epochs 20 --dist-url 'tcp://192.168.100.1:9999' --gpu-number 2 --dist-backend 'nccl' --multiprocessing-distributed ~/imagenet/

The trick part is: CUDA_VISIBLE_DEVICES=0 for 1GPU
CUDA_VISIBLE_DEVICES=0,1 for 2GPUs

Please let me know if this solution can help you.

Good luck",hi finally figured solution maybe try python trick part please let know solution help good luck,issue,positive,positive,positive,positive,positive,positive
441499566,"We usually compare language models using the same evaluation set. There are other requirements for fair comparison such as same training set, same vocabulary, equivalent number of parameters and so on.",usually compare language evaluation set fair comparison training set vocabulary equivalent number,issue,negative,positive,positive,positive,positive,positive
441472574,"I don't thing it is correct to measure the loss as a sum of the losses of every observation, especially if we use the loss to select, among different models, the one that performs better.
By absurd, let's consider two perfectly equal models. If the loss depend on the number of observations len(data), we will end up choosing the model tested with more observations with no reason, because in reality they perform the same.",thing correct measure loss sum every observation especially use loss select among different one better absurd let consider two perfectly equal loss depend number data end choosing model tested reason reality perform,issue,negative,neutral,neutral,neutral,neutral,neutral
441408027,"> Hi, did you figure the reason of this? I have exactly the same question.

Hi, yes. If I remember it right, the _nn.CrossEntropyLoss()_ criterion by default takes the average of losses of minibatch elements. Therefore they are multiplying the loss (mean of losses) by the number of elements in the minibatch.",hi figure reason exactly question hi yes remember right criterion default average therefore multiplying loss mean number,issue,negative,positive,neutral,neutral,positive,positive
441407272,"Hi, did you figure the reason of this? I have exactly the same question.",hi figure reason exactly question,issue,negative,positive,positive,positive,positive,positive
441347294,pillow is already installed. still seeing this error.,pillow already still seeing error,issue,negative,neutral,neutral,neutral,neutral,neutral
441270959,what @plopd  has said is absolutely right. Detaching `fake` from the graph is necessary and will lead to an error if not done so.,said absolutely right fake graph necessary lead error done,issue,negative,negative,neutral,neutral,negative,negative
440984296,"For 
```
python3 main.py --cuda --emsize 650 --nhid 650 --dropout 0.5 --epochs 40 --tied
```
I get a test perplexity of 86.44 as opposed to the 75.96 mentioned in `README.md`.",python dropout tied get test perplexity opposed,issue,negative,neutral,neutral,neutral,neutral,neutral
440405929,"I had the same problem. Sometimes the program seems have deadlock in def train at:
257: for i, (input, target) in enumerate(train_loader):

Sometimes I got output like this:
Use GPU: 1 for training
Use GPU: 2 for training
Use GPU: 0 for training
Use GPU: 3 for training
=> creating model 'resnet50'
=> creating model 'resnet50'
=> creating model 'resnet50'
=> creating model 'resnet50'
Traceback (most recent call last):
  File ""main.py"", line 389, in <module>
    main()
  File ""main.py"", line 106, in main
    mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))
  File ""/home/yiru/.local/lib/python3.5/site-packages/torch/multiprocessing/spawn.py"", line 146, in spawn
    while not spawn_context.join():
  File ""/home/yiru/.local/lib/python3.5/site-packages/torch/multiprocessing/spawn.py"", line 101, in join
    raise Exception(msg)
Exception: 

-- Process 2 terminated with the following error:
Traceback (most recent call last):
  File ""/home/yiru/.local/lib/python3.5/site-packages/torch/multiprocessing/spawn.py"", line 11, in _wrap
    fn(i, *args)
  File ""/home/yiru/src/pytorch_raw/examples/imagenet/main.py"", line 229, in main_worker
    train(train_loader, model, criterion, optimizer, epoch, args)
  File ""/home/yiru/src/pytorch_raw/examples/imagenet/main.py"", line 257, in train
    for i, (input, target) in enumerate(train_loader):
  File ""/home/yiru/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 501, in __iter__
    return _DataLoaderIter(self)
  File ""/home/yiru/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 289, in __init__
    w.start()
  File ""/usr/lib/python3.5/multiprocessing/process.py"", line 103, in start
    'daemonic processes are not allowed to have children'
AssertionError: daemonic processes are not allowed to have children

My command line is:
python3 main.py -a resnet50 --lr 0.01 --dist-url 'tcp://192.168.100.1:1234' --dist-backend 'nccl' --multiprocessing-distributed ~/imagenet/

I have researched for some solutions, but none of them worked for me.
I have tried:
(A) -j 0, this seems to work, but the program randomly results to broken pipe and forces my server to reboot in the following lines:
266: output = model(input)
277: loss.backward()
(B) updated to the latest openCV, no help
(C) pin_memory=False, no help

I really appreciate if anybody can help us.",problem sometimes program deadlock train input target enumerate sometimes got output like use training use training use training use training model model model model recent call last file line module main file line main file line spawn file line join raise exception exception process following error recent call last file line file line train model criterion epoch file line train input target enumerate file line return self file line file line start daemonic command line python none worked tried work program randomly broken pipe server following output model input latest help help really appreciate anybody help u,issue,positive,positive,neutral,neutral,positive,positive
439807404,"## I got the following error:
```
RuntimeError: invalid argument 2: size '[-1 x 1024]' is invalid for input with 11520 elements at 
..\aten\src\TH\THStorage.cpp:80
```

## Full error information:
```
RuntimeError                              Traceback (most recent call last)
<ipython-input-9-6694bd56733d> in <module>()
     21         optimizer.zero_grad()
     22         # forward pass: compute predicted outputs by passing inputs to the model
---> 23         output = model(data)
     24         # calculate the batch loss
     25         loss = criterion(output, target)

~\Anaconda3\lib\site-packages\torch\nn\modules\module.py in __call__(self, *input, **kwargs)
    475             result = self._slow_forward(*input, **kwargs)
    476         else:
--> 477             result = self.forward(*input, **kwargs)
    478         for hook in self._forward_hooks.values():
    479             hook_result = hook(self, input, result)

<ipython-input-7-971843679cb8> in forward(self, x)
     21         x = self.pool(F.relu(self.conv2(x)))
     22         x = self.pool(F.relu(self.conv3(x)))
---> 23         x = x.view(-1, 64*4*4)
     24         x = self.dropout(x)
     25         x = F.relu(self.fc1(x))
```

## My code block:
```
import torch.nn as nn
import torch.nn.functional as F

# define the CNN architecture
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()

        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1) # convolutional layer
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3)
        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2) # max pooling layer
        #self.drop2d = nn.Dropout2d(0.2)
        self.fc1 = nn.Linear(64*4*4, 500) # densely connected layer
        self.fc2 = nn.Linear(500, 10)
        self.dropout = nn.Dropout(0.2)

    def forward(self, x):
        # add sequence of convolutional and max pooling layers
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.pool(F.relu(self.conv3(x)))
        x = x.view(-1, 64*4*4)
        x = self.dropout(x)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

# create a complete CNN
model = Net()
```
## Found the cause: ""parameter padding=1"" was missing in one layer.
Wrong code:
```
    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1) # convolutional layer
    self.conv2 = nn.Conv2d(16, 32, kernel_size=3)
    self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
```
Right code:
```
    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1) # convolutional layer
    self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
    self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
```",got following error invalid argument size invalid input full error information recent call last module forward pas compute passing model output model data calculate batch loss loss criterion output target self input result input else result input hook hook self input result forward self code block import import define architecture class net self super net self convolutional layer layer densely connected layer forward self add sequence convolutional return create complete model net found cause parameter missing one layer wrong code convolutional layer right code convolutional layer,issue,negative,positive,neutral,neutral,positive,positive
439739642,"`exp_` is the in-place version of `exp`. So, I guess they used it there to avoid allocating new memory.",version guess used avoid new memory,issue,negative,positive,positive,positive,positive,positive
439723369,"@nsaphra Yes, it does!! After so long time still the same problem! And no answer in this issue!!",yes long time still problem answer issue,issue,negative,negative,neutral,neutral,negative,negative
439674838,"@curry111  Do you mean accessing cluster node from training code or in general ? 
",curry mean cluster node training code general,issue,negative,negative,negative,negative,negative,negative
439657122,"instead of making `main.py` a one-stop shop for everything, I feel like a `finetune.py` would be a much better idea here. What do you think?",instead making shop everything feel like would much better idea think,issue,positive,positive,positive,positive,positive,positive
439552245,"Add batch size as an argument. Something like  ""-b 15"". That should do it.",add batch size argument something like,issue,negative,neutral,neutral,neutral,neutral,neutral
439281322,"yes, it's possible by modifying the example code. Modify the example as you wish.",yes possible example code modify example wish,issue,positive,neutral,neutral,neutral,neutral,neutral
438565276,Can you elaborate on this? How/where do I make the changes to use the pretrained embeddings that I want to use? ,elaborate make use want use,issue,negative,positive,positive,positive,positive,positive
437717512,"@soumith Is there a solution/workaround for this?

I am getting the below error -
buff = tensor.view(self.batch_size, -1).data.numpy()
RuntimeError: invalid argument 2: size '[64 x -1]' is invalid for input with 10000 elements at /opt/conda/conda-bld/pytorch_1535493744281/work/aten/src/TH/THStorage.cpp:80",getting error buff invalid argument size invalid input,issue,negative,neutral,neutral,neutral,neutral,neutral
437585070,@gautamkmr thank you for asking the question because i have the same issue. I don't have knowledge of parallel or distributed computing and I will use cluster computer(HPC) for my research. I will use Slurm(sbatch). Do you know if that is similar to your issue? I read your modified script. but Is it possible to access individual nodes in the cluster? Is there a way to know their IP address and port number?,thank question issue knowledge parallel distributed use cluster computer research use know similar issue read script possible access individual cluster way know address port number,issue,negative,neutral,neutral,neutral,neutral,neutral
437365631,"Setting the imageSize from 64 to 32 breaks the convolutional layers, because the kernel sizes don't fit anymore!",setting convolutional kernel size fit,issue,negative,positive,positive,positive,positive,positive
437350162,"I came across same problem, too. 

Can not I train Inception_v3 model with 224x224 size? 

Is there only Increasing input size solution?",came across problem train model size increasing input size solution,issue,negative,neutral,neutral,neutral,neutral,neutral
436709320,"Thank you for the link. Do you know how long it takes, roughly, to get access to the ImageNet data? It has been 5 days since I signed up.",thank link know long roughly get access data day since,issue,negative,negative,neutral,neutral,negative,negative
435569919,"I think I have changed it a bit can you take a look again ?
https://github.com/gautamkmr/examples/blob/master/imagenet/DistributedTraing.md ",think bit take look,issue,negative,neutral,neutral,neutral,neutral,neutral
435559603,"no particular reason, a lower feature map size works well on the given dataset.",particular reason lower feature map size work well given,issue,negative,positive,positive,positive,positive,positive
435553800,"hi, thanks for help!  this code is running, but no communication/synchronization among processes.  Is there anything missing in the commit? ",hi thanks help code running among anything missing commit,issue,positive,neutral,neutral,neutral,neutral,neutral
435249089,"i have the same issue.  Even gloo and nccl do not work also.
Error message is:
For NCCL:
=> creating model 'resnet18'
NCCL version 2.3.5+cuda9.0
Traceback (most recent call last):
  File ""main.py"", line 340, in <module>
    main()
  File ""main.py"", line 180, in main
    train(train_loader, model, criterion, optimizer, epoch)
  File ""main.py"", line 228, in train
    loss.backward()
  File ""/home/cchen01/packages-pytorch-distributed/pytorch/lib/python3.7/site-packages/torch/tensor.py"", line 93, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File ""/home/cchen01/packages-pytorch-distributed/pytorch/lib/python3.7/site-packages/torch/autograd/__init__.py"", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
  File ""/home/cchen01/packages-pytorch-distributed/pytorch/lib/python3.7/site-packages/torch/nn/parallel/distributed.py"", line 341, in reduction_fn_nccl
    group=self.nccl_reduction_group_id)
  File ""/home/cchen01/packages-pytorch-distributed/pytorch/lib/python3.7/site-packages/torch/distributed/__init__.py"", line 306, in all_reduce_multigpu
    return torch._C._dist_all_reduce_multigpu(tensor_list, op, group)
RuntimeError: NCCL error in: /home/cchen01/src-pytorch-distributed/Pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp:322, unhandled system error

for gloo:
terminate called after throwing an instance of 'gloo::EnforceNotMet'
  what():  [enforce fail at /home/cchen01/src-pytorch-distributed/Pytorch/third_party/gloo/gloo/transport/ibverbs/pair.cc:462] wc->status == IBV_WC_SUCCESS. 12 vs 0. Memory region send for slot 0: transport retry counter exceeded
Aborted (core dumped)
",issue even work also error message model version recent call last file line module main file line main train model criterion epoch file line train file line backward self gradient file line backward flag file line file line return group error unhandled system error terminate throwing instance enforce fail status memory region send slot transport retry counter aborted core,issue,negative,negative,neutral,neutral,negative,negative
434508334,"@soumith 
I dont think there is any problem with the branch.
it works fine when i run the file. the problem is i just dont know how to use the backward pass in my code...
anyways, could you just give suggestion on how to fix the backward pass?",dont think problem branch work fine run file problem dont know use backward pas code anyways could give suggestion fix backward pas,issue,positive,positive,positive,positive,positive,positive
434504037,@isalirezag it's not meant to be used at all. We dont support that branch (and I'll likely delete it). I dont think it's even in working order.,meant used dont support branch likely delete dont think even working order,issue,negative,neutral,neutral,neutral,neutral,neutral
432697252,"I met the same problem. I solved it by trying a whole day. It is interesting but needs time to figure out.

Let's say I am going to use ImageFolder(""/train/"") to read jpg files in folder train.
The file structure is
/train/
-- 1.jpg
-- 2.jpg
-- 3.jpg

I failed to load them, leading to errors:
RuntimeError: Found 0 images in subfolders of: ./data
Supported image extensions are: .jpg,.JPG,.jpeg,.JPEG,.png,.PNG,.ppm,.PPM,.bmp,.BMP

I read the solution above and tried tens of times. When I changed the structure to
/train/1/
-- 1.jpg
-- 2.jpg
-- 3.jpg
But the read in code is still -- ImageFolder(""/train/""), IT WORKS.

It seems like the program tends to recursively read in files, that is convenient in some cases.

",met problem trying whole day interesting need time figure let say going use read folder train file structure load leading found image read solution tried time structure read code still work like program read convenient,issue,positive,positive,positive,positive,positive,positive
430369104,@u0251077 thanks for pointing this out. I have added the fix in #424.,thanks pointing added fix,issue,negative,positive,positive,positive,positive,positive
429656771,"We already have the word level language modelling example, which is pretty much isomorphic to this one, except it works on words instead of on characters. I'm not sure if we really want to merge this.",already word level language example pretty much isomorphic one except work instead sure really want merge,issue,positive,positive,positive,positive,positive,positive
428673404,"> Are your images placed in a subfolder of the data folder? You should not pass a folder with the image files directly since this is not the structure that is often used (usually the photos are bundled in folders by class name). If your images are directly in data/ try to move everything into data/1/ and run again? Den 18 okt. 2017 4:47 fm skrev ""Gromit Park"" <notifications@github.com>:

I did not quiet understand this. I am facing a similar issue. Could you please help me understand this better?",data folder pas folder image directly since structure often used usually class name directly try move everything run den park quiet understand facing similar issue could please help understand better,issue,positive,positive,neutral,neutral,positive,positive
427455785,"Well, almost. Jensen's inequality states that E_z[g(z)] <= g(E[z]) if g()
is concave; so if g = log f() is concave than you would be correct, but we
don't know.

So in principle the bug could have resulted in either higher or lower
validation loss.

On Thu, Oct 4, 2018 at 10:55 PM Luigi Antelmi <notifications@github.com>
wrote:

> I guess that this is due to the Jensen's inequality E_z[log(f(z))] <
> log(f(E[z])).
> If in the testing you do not sample the log-likelihood term of the ELBO
> you end up with an higher ELBO in the testing with respect to the training.
> Is the reasoning correct?
>
> â€”
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/pytorch/examples/pull/419#issuecomment-427254375>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/ACUc80ZhRRq1LvkW9ySPlG1OMT4-rZsxks5uhvRNgaJpZM4XJWrf>
> .
>
",well almost inequality concave log concave would correct know principle bug could either higher lower validation loss wrote guess due inequality log log testing sample term end higher testing respect training reasoning correct thread reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
427254375,"I guess that this is due to the Jensen's inequality E_z[log(f(z))] < log(f(E[z])).
If in the testing you do not sample the log-likelihood term of the ELBO you end up with an higher ELBO in the testing with respect to the training.
Is the reasoning correct?",guess due inequality log log testing sample term end higher testing respect training reasoning correct,issue,negative,positive,neutral,neutral,positive,positive
425759837,"You're trying to compute things on an empty tensor. `A[:0]` returns all the elements in the first row until the 0-th row, not including this one, this is, the empty tensor.",trying compute empty tensor first row row one empty tensor,issue,negative,positive,neutral,neutral,positive,positive
424171003,"fixed the wording as ""accuracy"", instead via https://github.com/pytorch/examples/commit/8473762c8824e3d546d050f7dfc13e40edf3a416 ",fixed wording accuracy instead via,issue,negative,positive,neutral,neutral,positive,positive
423339981,"Isn't 10% momentum as a default value really big?

Tensorflow's default is 0.99 which would be equivalent to 0.01 in pytorch, or 1% momentum.",momentum default value really big default would equivalent momentum,issue,negative,neutral,neutral,neutral,neutral,neutral
422166778,Then using conv layer and flatten it could be a possible solution for modelling output layer ?,layer flatten could possible solution output layer,issue,negative,neutral,neutral,neutral,neutral,neutral
422156969,"Yes, that is the idea. However, I don't think tanh would be an appropriate choice for the output layer in this case, as we are fitting values in [0,1] rather than [-1,1]. In fact, an error will jump out if an negative input is feed into torch.nn.functional.binary_cross_entropy().
",yes idea however think tanh would appropriate choice output layer case fitting rather fact error jump negative input feed,issue,negative,positive,positive,positive,positive,positive
421969420,"> because the decoder is implemented by MLP+Sigmoid which can be viewed as a 'Bernoulli distribution'.

Does this mean that in order to model ""continuous"" distribution like Gaussian then we should not use sigmoid as the output layer and  replace it with tanh or even flattened conv layer for example ? ",distribution mean order model continuous distribution like use sigmoid output layer replace tanh even layer example,issue,negative,negative,negative,negative,negative,negative
421952234,"According to the original VAE paper[1], BCE is used because the decoder is implemented by MLP+Sigmoid which can be viewed as a 'Bernoulli distribution'. You can use MSE if you implement a Gaussian decoder. Take the following pseudocode for an example, 
```
mu = MLP(z)
sigma = MLP(z)
reconstruction = Gaussian(mu, sigma)
```


Ref

[1] https://arxiv.org/abs/1312.6114
",according original paper used distribution use implement take following example mu sigma reconstruction mu sigma ref,issue,negative,positive,positive,positive,positive,positive
418573197,"The example didn't save all dict to the model: https://github.com/pytorch/examples/blob/6fd43cd04f4677be005167157c848f0fefe131af/imagenet/main.py#L187

However, after I modified the code to save_checkpoint(model, is_best) to save all the dicts,
I got another error:

Traceback (most recent call last):
  File ""scripts/prepro_mask_feats_hdf5.py"", line 146, in <module>
    main(params)
  File ""scripts/prepro_mask_feats_hdf5.py"", line 75, in main
    net.load_state_dict(torch.load(os.path.join(params['model_root'],params['model']+'.pth')))
  File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 695, in load_state_dict
    state_dict = state_dict.copy()
  File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 532, in __getattr__
    type(self).__name__, name))
AttributeError: 'ResNet' object has no attribute 'copy'",example save model however code model save got another error recent call last file line module main file line main file line file line type self name object attribute,issue,positive,positive,neutral,neutral,positive,positive
418070627,"Usually there is a folder where all the models (1 per epoch are saved). The saving option is not default, so usually you need to tell pytorch to save the weights after each epoch or use a framework like ignite that makes easy that process.",usually folder per epoch saved saving option default usually need tell save epoch use framework like ignite easy process,issue,positive,negative,neutral,neutral,negative,negative
417778004,"This PR implicitly sets the default resize length to 244, when before it was 256.",implicitly default resize length,issue,negative,neutral,neutral,neutral,neutral,neutral
417274972,Is there anyone who solved this problem? I got the similar error.,anyone problem got similar error,issue,negative,neutral,neutral,neutral,neutral,neutral
415745581,The problem is due to the 2G memory limitation of docker for mac,problem due memory limitation docker mac,issue,negative,negative,negative,negative,negative,negative
412752824,"both documentation additions make sense :)
thanks for the suggestions and we welcome a PR (we'll get to it eventually if you dont)",documentation make sense thanks welcome get eventually dont,issue,positive,positive,positive,positive,positive,positive
412587863,"Same issue here. Tried 'python main.py --cuda --tied', got valid ppl 105.97 and test ppl 100.33, while the one shown in readme file is 87.17. Huge gap! ",issue tried tied got valid test one shown file huge gap,issue,negative,positive,positive,positive,positive,positive
412300296,Your dataloaders are taking up the bulk of the running time. It's likely that you are not assigning enough CPUs for the workers. Try e.g. at least 32 CPUs for 8 GPUs and 8 workers.,taking bulk running time likely enough try least,issue,negative,negative,neutral,neutral,negative,negative
409885944,"I meet the same question, how did you solve?thanks",meet question solve thanks,issue,positive,positive,positive,positive,positive,positive
408660109,"Just to anyone seeing the error message 

> TypeError: nll_loss() got an unexpected keyword argument 'reduction'

This is due to the reduction keyword not being supported on 0.4.0 (see https://pytorch.org/docs/0.4.0/nn.html#nll-loss)
So updating to 0.4.1 should solve the error (even if it's not written in the release notes, it's in the docs)
",anyone seeing error message got unexpected argument due reduction see solve error even written release,issue,negative,negative,neutral,neutral,negative,negative
408553185,"Agreed, this had me stumped until I saw this issue. Running `python main.py --cuda --emsize 650 --nhid 650 --dropout 0.5 --epochs 40` (with the default WT2 dataset) yields a test ppl of 94.57 and a valid ppl of 99.83.",agreed saw issue running python dropout default test valid,issue,negative,neutral,neutral,neutral,neutral,neutral
408435393,"Thanks, I have solved the problem by updating code to torch version 0.4.",thanks problem code torch version,issue,negative,positive,positive,positive,positive,positive
408309399,"this is done for coherence of example, i.e. to not introduce all concepts at once.",done coherence example introduce,issue,negative,neutral,neutral,neutral,neutral,neutral
406273191,"> But the same run with python 2.7 environment and same versions or torch and torchvision gives Prec@1 76.012 Prec@5 92.934 .

Because of CuDNN, the runs might be a little non-deterministic.
if you want determinism at the expense of speed, you can set:
```
torch.backends.cudnn.deterministic=True
torch.backends.cudnn.benchmark=False
```",run python environment torch might little want determinism expense speed set,issue,negative,negative,negative,negative,negative,negative
406271253,"I was using existing dataset path on our cluster. To isolate the problem, I prepared val folder from scratch using tar file downloaded from imagenet site. Now I get the same result as yours.

The run which produced same results was in python 3.6 environment. But the same run with python 2.7 environment and same versions or torch and torchvision gives  `Prec@1 76.012 Prec@5 92.934` . 

Anyways, thanks for the quick response @soumith . ",path cluster isolate problem prepared folder scratch tar file site get result run produced python environment run python environment torch anyways thanks quick response,issue,negative,positive,positive,positive,positive,positive
405967110,"I just re-ran now with torch 0.4.0 and torchvision 0.2.1.
My result is: ` * Prec@1 76.130 Prec@5 92.862`

Did you resize / downsample the validation images before-hand from their original resolution?",torch result resize validation original resolution,issue,negative,positive,positive,positive,positive,positive
405846159,"I reran it again and got the same results ! Following is the config:
torch = 0.4.0
torchvision = 0.2.1
OS = CentOS Linux release 7.4.1708 (Core)

I am using code at commit -> 98b1fce0d2e55c79b59e98ec28ebbc2f58da38f7 .
",got following torch o release core code commit,issue,negative,neutral,neutral,neutral,neutral,neutral
405781345,"I verified this quite recently, so I'm surprised by your result. What version of PyTorch and torchvision are you using? and on what OS?",quite recently result version o,issue,negative,neutral,neutral,neutral,neutral,neutral
405004500,"Same problem, gpu 0 usage, only small amount of time has usage",problem usage small amount time usage,issue,negative,negative,negative,negative,negative,negative
403630626,"empirically it didn't matter, we just went with ""before"" probably without any particular reason.",matter went probably without particular reason,issue,negative,positive,positive,positive,positive,positive
403253259,"I remember at some point ` cudnn.deterministic=True` and `cudnn.benchmark=True` did not guarantee deterministic behavior between runs, there was a discussion in one of the pytorch issues. Has this been fixed? (i'm on the phone now, so hard for me to search the issues and look at the code)",remember point guarantee deterministic behavior discussion one fixed phone hard search look code,issue,negative,negative,neutral,neutral,negative,negative
403214993,"cc @ngimel

On Sat, Jul 7, 2018 at 06:56 Jerry Ma <notifications@github.com> wrote:

> Repro
>
>    - Apply #381 <https://github.com/pytorch/examples/pull/381> on this
>    repo
>    - cd to the ImageNet folder
>    - Run python main.py --arch resnet18 --seed 0 --gpu 0
>    /path/to/imagenet/ on a multi-GPU machine, once with
>    CUDA_VISIBLE_DEVICES=0 and once with CUDA_VISIBLE_DEVICES=1.
>
> Environment
>
>    - PyTorch master
>    - CUDA 9.0
>    - Driver 384.81
>    - Ubuntu 16.04
>
> Expected behavior
>
> The two runs have the same output.
> Actual behavior
>
> The two runs have the same output when you run them one after the other
> (e.g. GPU 0 first, then Ctrl-C, then GPU 1). But when you run them at the
> same time, you get different output.
> Suspicion
>
> This is a driver bug. I dunno how PyTorch would be able to bypass
> CUDA_VISIBLE_DEVICES-based GPU segregation.
>
> cc @shubho <https://github.com/shubho> @SsnL <https://github.com/SsnL>
> @soumith <https://github.com/soumith> @ailzhang
> <https://github.com/ailzhang>
>
> â€”
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/pytorch/examples/issues/382>, or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AFaWZUyHLSDhEOhVdlhHmZn4JuiVkOrpks5uEJPJgaJpZM4VGUVT>
> .
>
",sat jerry wrote apply folder run python arch seed machine environment master driver behavior two output actual behavior two output run one first run time get different output suspicion driver bug would able bypass segregation reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
403123645,"Me neither :) Soumith's traveling, but he should be able to merge when he's near laptop.",neither traveling able merge near,issue,negative,positive,positive,positive,positive,positive
403123402,"Thanks for reviewing :) . I don't seem to have write access to this repo, so could you merge?",thanks seem write access could merge,issue,negative,positive,positive,positive,positive,positive
402141102,"the wrong installation of Pytorch, need installation with Cuda.",wrong installation need installation,issue,negative,negative,negative,negative,negative,negative
401657142,@soumith Should I change this in `model_zoo.py` or move it somewhere around?,change move somewhere around,issue,negative,neutral,neutral,neutral,neutral,neutral
401656896,"about (2), I can re-host the model after it's loaded via `load_lua` and then saved back to disk in Python pickle format",model loaded via saved back disk python pickle format,issue,negative,neutral,neutral,neutral,neutral,neutral
401654510,"So this issue can be split into 2 parts:
1. Cross-platform file download utilities. Unfortunately, the `download_url` is in `pytorch/vision`, should I move that to `pytorch/pytorch`, @soumith ?
2. Probably cross-platform `load_lua`. I'm sure how to do this. You may have to specify `long_size=8` by yourself.",issue split file unfortunately move probably sure may specify,issue,negative,neutral,neutral,neutral,neutral,neutral
401608907,cc: @peterjc123 can you issue to make sure we use cross-platform download command. We can probably even use torch.utils.download_url (or something like that which we have),issue make sure use command probably even use something like,issue,positive,positive,positive,positive,positive,positive
401579280,"@soumith This is not true. Detaching `fake` from the graph is necessary to avoid forward-passing the noise through `G` when we actually update the generator. If we do not detach, then, although `fake` is not needed for gradient update of `D,` it will still be added to the computational graph and as a consequence of `backward` pass which clears all the variables in the graph (`retain_graph=False` by default), `fake` won't be available when G is updated.",true fake graph necessary avoid noise actually update generator detach although fake gradient update still added computational graph consequence backward pas graph default fake wo available,issue,negative,negative,negative,negative,negative,negative
400189044,@donghwicha  can you please share what is the solution? I am also in a similar situation and wanted to know what works? Thanks in advance.,please share solution also similar situation know work thanks advance,issue,positive,positive,neutral,neutral,positive,positive
398427898,"As a workaround, you need to use easydict instead of argparse for https://github.com/pytorch/examples/blob/master/mnist/main.py
Please refer following sites.
-  https://stackoverflow.com/questions/48796169/how-to-fix-ipykernel-launcher-py-error-unrecognized-arguments-in-jupyter
-  https://qiita.com/LittleWat/items/6e56857e1f97c842b261",need use instead please refer following,issue,negative,neutral,neutral,neutral,neutral,neutral
397124635,"Each item is a word, no matter a sentence is on a row or col, we can get the same result.
If you debug it, you will find it's tricky on reshaping, but the final shapes are correct. That's my understanding, hopefully it can help.",item word matter sentence row col get result find tricky final correct understanding hopefully help,issue,positive,neutral,neutral,neutral,neutral,neutral
396469600,"I see. But this assumes uniform distribution on the first word, which isn't what a language model is right? Shouldn't the first input always be <start> and the first word should be sampled from the distribution of the corpus?

For instance, it is very unlikely any sentence would start with the word ""unfortunate""

I'm perfectly okay with the answer ""yeah but who cares it's easier this way"", which is what I would've done too, technically bit incorrect but who cares. Is that the case?",see uniform distribution first word language model right first input always start first word distribution corpus instance unlikely sentence would start word unfortunate perfectly answer yeah easier way would done technically bit incorrect case,issue,positive,positive,positive,positive,positive,positive
396091878,"This should not cause problems, since `optimizerG.step()` only update the parameters of the generator network.",cause since update generator network,issue,negative,neutral,neutral,neutral,neutral,neutral
396082395,Does this mean `0.1` in PyTorch's `BatchNorm2d` is same as `1 - 0.1 = 0.9` in TF/caffe? So if a TF model for example uses 0.2 I should use 0.8 here in PyTorch?,mean model example use,issue,negative,negative,negative,negative,negative,negative
395331130,@happsky see [issue 4884](https://github.com/pytorch/pytorch/issues/4884). Increasing input size or adapting the network architecture seems to fix this as Kernel size in later layers can get too large for the corresponding feature map. ,see issue increasing input size network architecture fix kernel size later get large corresponding feature map,issue,negative,positive,positive,positive,positive,positive
395198394,"The code saves the current model if the current epoch produced a model that has a lower validation loss than the best model seen so far.  So in the way this particular example is coded (and you can surely change it), you end up with only one saved model, which captures the state of the best model seen during the entire training session (""best"" is as measured by comparing validation loss; at the end of each training epoch).

Cheers,
Neta",code current model current epoch produced model lower validation loss best model seen far way particular example surely change end one saved model state best model seen entire training session best measured validation loss end training epoch,issue,positive,positive,positive,positive,positive,positive
395102675,"Yes, checkout the[ PyTorch 0.4 migration guide](https://pytorch.org/2018/04/22/0_4_0-migration-guide.html) and search for torch.device.

Cheers,
Neta",yes migration guide search,issue,negative,neutral,neutral,neutral,neutral,neutral
395101262,"This ```input``` tensor is used to sample the dictionary - to randomly choose the first word in the input sequence.
The next time ```input``` is used, it is already after it was set to the output of the RNN:
```
output, hidden = model(input, hidden)
word_weights = output.squeeze().div(args.temperature).exp().cpu()
word_idx = torch.multinomial(word_weights, 1)[0]
input.fill_(word_idx)
```

I hope that helps,
Neta",input tensor used sample dictionary randomly choose first word input sequence next time input used already set output output hidden model input hidden hope,issue,negative,negative,negative,negative,negative,negative
394707118,"Found out that the crop augmentation range 8% to 100% originates from the Inception paper. 

""Going deeper with convolutions"", Christian Szegedy et. al.
https://arxiv.org/pdf/1409.4842.pdf

Quoting from the paper:
"".., one prescription that was verified to work very well after the competition includes sampling
of various sized patches of the image whose size is distributed evenly between 8% and 100% of the
image area and whose aspect ratio is chosen randomly between 3/4 and 4/3. ""

I did find that using the range 50% to 100% as I mentioned above speeds up training, but I don't know whether it is best for accuracy. I'll close this issue. 
",found crop augmentation range inception paper going al paper one prescription work well competition sampling various sized image whose size distributed evenly image area whose aspect ratio chosen randomly find range training know whether best accuracy close issue,issue,positive,positive,positive,positive,positive,positive
394591451,"- when you run the mnist code you will also meet the problem ï¼š

```
  device = torch.device(""cuda"" if use_cuda else ""cpu"")
AttributeError: 'module' object has no attribute 'device'
```

- it also is the pytorch version ??",run code also meet problem device else object attribute also version,issue,negative,neutral,neutral,neutral,neutral,neutral
394415407,"@Zekodon please read the warnings, we have provided a workaround right there.",please read provided right,issue,negative,positive,positive,positive,positive,positive
393389735,I got this same issue when running the translate.py step. Did you determine the cause?,got issue running step determine cause,issue,negative,neutral,neutral,neutral,neutral,neutral
393307179,"Running just once with ""python main.py --cuda --emsize 650 --nhid 650 --dropout 0.5 --epochs 40 --tied "" on WT2 I get test ppl of 86.68 and valid ppl of 91.70.",running python dropout tied get test valid,issue,negative,neutral,neutral,neutral,neutral,neutral
393281222,I am seeing the same result.  Data loading dominates training time (similar to as shown in @zcyang's output above).  Is the [pre-processing step](https://github.com/pytorch/examples/blob/master/imagenet/main.py#L116-L126) the bottleneck -- my disk I/O does not appear to be close to max speed?  Is there a way to pipeline that work to be done before GPU training so training does not take prohibitively long?,seeing result data loading training time similar shown output step bottleneck disk appear close speed way pipeline work done training training take prohibitively long,issue,negative,negative,neutral,neutral,negative,negative
392566257,It worked on my side. What is your error code? Maybe I have seen it.,worked side error code maybe seen,issue,negative,neutral,neutral,neutral,neutral,neutral
392467932,"Inception_v3 still does not work after changing input size, any solutions?",still work input size,issue,negative,neutral,neutral,neutral,neutral,neutral
392389632,"`export CUDA_VISIBLE_DEVICES=0; python main.py -a inception_v3  ./cat2dog --batch-size 16 --print-freq 1 --pretrained;`
=> using pre-trained model 'inception_v3'
torch.Size([16, 3, 299, 299])
Traceback (most recent call last):
  File ""main.py"", line 324, in <module>
    main()
  File ""main.py"", line 164, in main
    train(train_loader, model, criterion, optimizer, epoch)
  File ""main.py"", line 207, in train
    prec1, prec5 = accuracy(output, target, topk=(1, 5))
  File ""main.py"", line 312, in accuracy
    _, pred = output.topk(maxk, 1, True, True)
AttributeError: 'tuple' object has no attribute 'topk'",export python model recent call last file line module main file line main train model criterion epoch file line train accuracy output target file line accuracy true true object attribute,issue,positive,positive,positive,positive,positive,positive
392385842,"After updating.
`export CUDA_VISIBLE_DEVICES=0; python main.py -a inception_v3  ./cat2dog --batch-size 16 --print-freq 1 --pretrained;`
=> using pre-trained model 'inception_v3'
Traceback (most recent call last):
  File ""main.py"", line 313, in <module>
    main()
  File ""main.py"", line 157, in main
    train(train_loader, model, criterion, optimizer, epoch)
  File ""main.py"", line 192, in train
    output = model(input)
  File ""/home/hao/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/hao/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py"", line 112, in forward
    return self.module(*inputs[0], **kwargs[0])
  File ""/home/hao/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/hao/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/inception.py"", line 109, in forward
  File ""/home/hao/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/hao/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/inception.py"", line 308, in forward
  File ""/home/hao/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/hao/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/inception.py"", line 325, in forward
  File ""/home/hao/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/hao/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py"", line 301, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: Expected tensor for argument #1 'input' to have the same dimension as tensor for 'result'; but 4 does not equal 2 (while checking arguments for cudnn_convolution)
",export python model recent call last file line module main file line main train model criterion epoch file line train output model input file line result input file line forward return file line result input file line forward file line result input file line forward file line result input file line forward file line result input file line forward tensor argument dimension tensor equal,issue,negative,positive,neutral,neutral,positive,positive
392378288,"Are you using pytorch 0.4?

```python
import torch
print(torch.__version__)
```",python import torch print,issue,negative,neutral,neutral,neutral,neutral,neutral
392338184,"When you run the code specify the output image. 
For example:
`--output-image output/output_image.PNG `
Hope it helps.",run code specify output image example hope,issue,negative,neutral,neutral,neutral,neutral,neutral
390711363,you are giving input that is too small. you should give atleast 64x64,giving input small give,issue,negative,negative,negative,negative,negative,negative
389943741,"Hi, just wanted to chime in that you don't need root to install/compile. You can check out https://www.osc.edu/resources/getting_started/howto/howto_add_python_packages_using_the_conda_package_manager for more information.
",hi chime need root check information,issue,negative,neutral,neutral,neutral,neutral,neutral
388944682,"@peterjc123, @apaszke  - The train() and test() function shared the variables defined in main in original version so to keep changes to minimum I put them as sub functions. Anyway, they are now moved out of main() and accept variables they need from main() as params. PR is updated with this change as requested.",train test function defined main original version keep minimum put sub anyway main accept need main change,issue,positive,positive,positive,positive,positive,positive
388611009,"HI! i get same problem with you ,and i solve the problem by modify the ""neural_style.py"" : 

replace **line 72**:
y = transformer(x.to(device))
as:
x = x.to(device)
y = transformer(x)",hi get problem solve problem modify replace line transformer device device transformer,issue,negative,neutral,neutral,neutral,neutral,neutral
388533065,Why do you wrap all the functions into `main`?  I don't think this is a good idea.,wrap main think good idea,issue,negative,positive,positive,positive,positive,positive
386676355,Is this work? I also meet the same problem. The GPU utilization is almost 0,work also meet problem utilization almost,issue,negative,neutral,neutral,neutral,neutral,neutral
386474005,"Sure, made ONNX export optional.",sure made export optional,issue,negative,positive,positive,positive,positive,positive
385306864,"I mean that I have tried to compute the PSNR on both three channels and y channel, the y channels didn't perform better than it in bicubic, so that it couldn't perform better on three channels. By the way, have you made this project perform better than the bicubic method? What the number of epochs did you set for training to make it perform better than the bicubic?",mean tried compute three channel perform better could perform better three way made project perform better method number set training make perform better,issue,positive,positive,positive,positive,positive,positive
385305309,"Yes, but it still less than the bicubic method, I don't know what is the problem that make this method works worse than bicubic method,  I can't sure whether the problem is in training process, SR process or the model. ",yes still le method know problem make method work worse method ca sure whether problem training process process model,issue,negative,positive,neutral,neutral,positive,positive
385267198,"the example is self-explanatory. You have one powerful GPU and a weak GPU. PyTorch is suggesting that you should exclude the weak GPU using the environment variable `CUDA_VISIBLE_DEVICES`.
For example:

```
CUDA_VISIBLE_DEVICES=1 python main.py # excludes 0
```",example one powerful weak suggesting exclude weak environment variable example python,issue,negative,negative,negative,negative,negative,negative
385236973,"Excuse me, have you solve the problem of low psnr value result ? I tried to transform the 91 images training set into hdf5 file,and trained on it ,but still had 4 dB psnr loss than bicubic method, could you please give me some advises?",excuse solve problem low value result tried transform training set file trained still loss method could please give,issue,negative,negative,neutral,neutral,negative,negative
384497706,"Thank you very much!
May I ask why part of these two models are not in DataParallel:
if args.arch.startswith('alexnet') or args.arch.startswith('vgg'):
            model.features = torch.nn.DataParallel(model.features)
            model.cuda()
",thank much may ask part two,issue,negative,positive,positive,positive,positive,positive
384486261,"@MichaelGuarin0 that is weird, `root` is what is the correct argument.
https://github.com/pytorch/vision/blob/master/torchvision/datasets/lsun.py#L73-L74

Can you update your torchvision to the latest version and re-try?",weird root correct argument update latest version,issue,negative,neutral,neutral,neutral,neutral,neutral
384279511,"we dont need to move input to cuda, DataParallel will do it better.
If input is on CPU, DataParallel will scatter parts of input directly to respective GPUs.
i.e.

[batch split 1] cpu -> cuda0
[batch split 2] cpu -> cuda1",dont need move input better input scatter input directly respective batch split batch split,issue,positive,positive,positive,positive,positive,positive
383965797,The script works. Please check if there are things like firewall that blocks downloading.,script work please check like,issue,positive,neutral,neutral,neutral,neutral,neutral
382928279,"I see we already have the change pr, nice!",see already change nice,issue,negative,positive,positive,positive,positive,positive
381991191,@joekidd The only time the trained means are used without the sampling is during evaluation when you want to reconstruct an input image. The interpretation is that you're recovering the latent sample `z` with the highest probability under the posterior `q(z|x)`. You could still sample `z`'s if you wanted to.,time trained used without sampling evaluation want reconstruct input image interpretation latent sample highest probability posterior could still sample,issue,negative,neutral,neutral,neutral,neutral,neutral
381873594,"@Kaixhin could you explain me please, what was the reason of modifying the reparametrize method in the way you did - now you are simply using trained means and you loose all the stochastic part. Maybe I'm missing something here, but it doesn't look like VAE to me anymore.",could explain please reason method way simply trained loose stochastic part maybe missing something look like,issue,negative,negative,neutral,neutral,negative,negative
381812664,"I did downsampling using the LANCZOS method. But, in [ESPCN](https://arxiv.org/abs/1609.05158),They used the Gaussian blur method. I can not be sure this is the reason, but the PSNR value is too low. In the Set5 dataset, I found that the psnr value is lower than the bicubic method.",method used blur method sure reason value low set found value lower method,issue,negative,positive,positive,positive,positive,positive
381126850,"yes, LANCZOS is better and PSNR result wich 
For data augmentation which aims to different scenarios, even better to use random choice of resize method.",yes better result data augmentation different even better use random choice resize method,issue,positive,positive,positive,positive,positive,positive
379547550,"Same problem.  
Is there anything like a `keras.callbacks.ModelCheckpoint` that saves only the best model after each epoch? Or we just have to write a `for` loop and compare it ourselves.",problem anything like best model epoch write loop compare,issue,positive,positive,positive,positive,positive,positive
379060776,Sure that makes sense - I will probably move things over to https://github.com/pytorch/benchmark,sure sense probably move,issue,negative,positive,positive,positive,positive,positive
379059393,Can we keep the perf testing code a for of the examples? Those files are meant as demos/starter code for projects and this flag isn't useful anywhere outside this single use case,keep testing code meant code flag useful anywhere outside single use case,issue,negative,positive,neutral,neutral,positive,positive
378880615,"This is expected. The reason is because in the official MNIST dataset, all the data is stored in memory.
You can reduce the loading time in your case by increasing the number of threads you use to load the data.",reason official data memory reduce loading time case increasing number use load data,issue,negative,neutral,neutral,neutral,neutral,neutral
378783807,"I'm having the same issue. Error also occurs when loading from same directory as model. 

Any suggestions on resolving?",issue error also loading directory model,issue,negative,neutral,neutral,neutral,neutral,neutral
378021136,"@xiaopengguo1406 This is indeed interesting. The WassersetinGAN implementation for instance does something similar to what you suggest (it was added in this pull request: https://github.com/martinarjovsky/WassersteinGAN/pull/35):

https://github.com/martinarjovsky/WassersteinGAN/blob/f81eafd2aa41e93698f203732f8f395abc70be02/main.py#L223-L225

It seems like something similar may make sense here in the case of this DCGAN example as well.",indeed interesting implementation instance something similar suggest added pull request like something similar may make sense case example well,issue,positive,positive,positive,positive,positive,positive
373914086,"self.rnn=nn.LSTM(128,128,2)

RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().",module part single contiguous chunk memory need compacted every call possibly increasing memory usage compact call,issue,negative,negative,neutral,neutral,negative,negative
373170716,Can you explain why the temporal encoding parameters include batch size as a dimension? It's pretty strange in deep learning to have explicit batch-dependence like that other than through batchnorm.,explain temporal include batch size dimension pretty strange deep learning explicit like,issue,positive,positive,neutral,neutral,positive,positive
372849902,"@csarofeen, dist_fp16/imagenet  doesn't work either. With the gloo or tcp backends, I get:
AttributeError: module 'torch.distributed' has no attribute '_backend
With the nccl backend, I get:
RuntimeError: _Map_base::at
Which might be what you get when you don't correctly have nccl installed. In any event it's a confusing error message.

BTW, is there a tutorial for correctly compiling pytorch with nccl? Am I supposed to use nccl 2?

@soumith Will the next pytorch release be compiled with nccl?
Thanks",work either get module attribute get might get correctly event error message tutorial correctly supposed use next release thanks,issue,negative,positive,neutral,neutral,positive,positive
371122892,"Hi @MLnick, yes they are freely available to use. The only license which applies is [BSD 3-Clause](https://github.com/pytorch/examples/blob/master/LICENSE) as the code is a part of pytorch/examples repository. This is true for the pre-trained weights also. ",hi yes freely available use license code part repository true also,issue,positive,positive,positive,positive,positive,positive
371045753,"@abhiskk hey there, was just wondering if there is an explicit license on the pre-trained weights you have at https://github.com/pytorch/examples/tree/master/fast_neural_style#models? (link to dropbox `https://www.dropbox.com/s/lrvwfehqdcxoza8/saved_models.zip?dl=0`). Are they freely available to use?",hey wondering explicit license link freely available use,issue,negative,positive,positive,positive,positive,positive
370438429,thanks for pointing the bug. it looks like we just have to add `.long()`. I've fixed it in https://github.com/pytorch/examples/commit/c66593f1699ece14a4a2f4d314f1afb03c6793d9,thanks pointing bug like add fixed,issue,positive,positive,positive,positive,positive,positive
370369744,"All operations involving trainable variables should be defined within the constructor, ""\_\_init\_\_"". That's how Pytorch somehow attaches parameters to the "".parameters()"" generator. Note, however, that simple operations which do not issue gradient such as views, reshapes, activations can be defined in the ""forward()"" method.",trainable defined within constructor somehow generator note however simple issue gradient defined forward method,issue,negative,neutral,neutral,neutral,neutral,neutral
370303056,"Not quite, it looks like it was caused by the fact that .eq returns a ByteTensor and not a Float or LongTensor. Then, summing over this ByteTensor will give the answer mod 256.

A simpler correction might just be changing line 103 to 
`correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()`

Sorry about changing to accuracy instead of error. ",quite like fact float give answer simpler correction might line correct sorry accuracy instead error,issue,negative,negative,negative,negative,negative,negative
370260086,"Was this caused by broadcasting? I really can't spot the error in the original code. I think accuracy is a more standard metric when running scripts on MNIST, so let's stick to that",really ca spot error original code think accuracy standard metric running let stick,issue,negative,positive,positive,positive,positive,positive
370259200,"The bug is in the equality counter (correct). If you print out a concatenated version of the current pred and target variables, you'll see that they align more often than correct says that they do.

I changed accuracy to error rate, which are equivalent.",bug equality counter correct print version current target see align often correct accuracy error rate equivalent,issue,negative,neutral,neutral,neutral,neutral,neutral
370258015,"What was the bug? It looks like the patch just changes accuracy to error rate, but they are equivalent",bug like patch accuracy error rate equivalent,issue,negative,neutral,neutral,neutral,neutral,neutral
370201480,"Looking into this more, the problem is with `torch.nn.parallel.replicate()` It only replicates an fp16 model to the first gpu. The other gpus don't get the correct weights.

As a partial workaround, you can keep your model an fp32 model but then run:

```
def new_replicate(self, module, device_ids):
                    replicas = torch.nn.parallel.replicate(module,device_ids)
                    replicas = [convert_to_half(r) for r in replicas]
                    return replicas

torch.nn.DataParallel.replicate=new_replicate
```
(where convert_to_half converts everything in the model to half except for batch norm layers)

Though this breaks eval mode.

Seems like a bug with pytorch that should get fixed.",looking problem model first get correct partial keep model model run self module module return everything model half except batch norm though mode like bug get fixed,issue,negative,positive,neutral,neutral,positive,positive
368293260,"I'm not just throwing .half on my models and inputs. I'm using the procedure outlined in http://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html as you coded in https://github.com/csarofeen/examples/tree/fp16_examples_cuDNN-ATen/imagenet . This works, but only for a single GPU. The only problems happen when I use 2 gpus AND fp16.

Unfortunately I can't use NCCL because I don't have the ability to recompile and install pytorch (no root). For what it's worth, scaling isn't a problem. When I do run both GPUs the speed is about 2x as fast, it just doesn't converge.

I just wish I knew what was going on. Like what mathematically is happening such that it diverges. And why would reducing the learning rate prevent it from diverging, but result in poor model performance? While the same code works perfectly on a single GPU or on two gpus with fp32. Something definitely seems like a bug.",throwing procedure outlined work single happen use unfortunately ca use ability recompile install root worth scaling problem run speed fast converge wish knew going like mathematically happening would reducing learning rate prevent diverging result poor model performance code work perfectly single two something definitely like bug,issue,positive,negative,neutral,neutral,negative,negative
368259536,"As the default branch and all the links I've been posting, the correct branch is dist_fp16",default branch link posting correct branch,issue,negative,neutral,neutral,neutral,neutral,neutral
368255418,"Training in mixed-precision is a bit more complicated than throwing a .half() on the model and input. That's why I wrote https://github.com/csarofeen/examples/tree/dist_fp16/imagenet

Mixed precision training can work in DataParallel but I do not have it in my example. Mainly because it doesn't have good scaling on multi-gpu. My recommendation is get NCCL 2.1.2, build PyTorch from source with it, and use my example as written. If you need to modify it try to understand what exactly is being done in terms of a master_parameter copy in fp32, and try to be consistent with the example. If you're coming to GTC next month, I will a few mixed-precision presentations including a short tutorial that I will give on pytorch.",training bit complicated throwing model input wrote mixed precision training work example mainly good scaling recommendation get build source use example written need modify try understand exactly done copy try consistent example coming next month short tutorial give,issue,negative,positive,neutral,neutral,positive,positive
368252528,"Also, can we clarify which version of your imagenet example we are talking about? I was basing my question on the branch ""fp16_examples_cuDNN_ATen""

Thanks again.",also clarify version example talking question branch thanks,issue,negative,positive,positive,positive,positive,positive
368252401,"So are you saying that fp16 training doesn't work on multiple GPU's with `torch.nn.DataParallel()` it only works with `torch.nn.parallel.DistributedDataParallel()`?

I know that I don't need the loss scale. I just put it because I was messing with the parameters to try to get convergence on 2 gpus. On 1 gpu, it converges without it. In other words, the loss scale is not actually related to my problem.",saying training work multiple work know need loss scale put messing try get convergence without loss scale actually related problem,issue,negative,neutral,neutral,neutral,neutral,neutral
368250717,gloo is very slow with fp16 communication you need NCCL to get good perf. The point of `python -m multiproc` is to fill world-size and rank automatically. Distributed is also intended for single computer multi-gpu runs as well. Loss scale for resnet is not needed for final convergence. We've converged it many times without any loss scaling.,slow communication need get good point python fill rank automatically distributed also intended single computer well loss scale final convergence many time without loss scaling,issue,negative,positive,neutral,neutral,positive,positive
368250493,It also occurs to me that dist-backend has no effect because world_size is not changed from the default of 1. I'm not trying to train on multiple separate computers. Things aren't working with a single computer that has 2 gpus in it.,also effect default trying train multiple separate working single computer,issue,negative,negative,neutral,neutral,negative,negative
368248282,"The learning rate and the scale factor were attempts to make training on both GPU's with fp16 stable, and this works somewhat.

What is dist-backend? What is the difference between nccl and the default (gloo)? Why doesn't gloo work with fp16? Are you saying I need to recompile pytorch to use nccl?  I don't have root privileges on the machine I'm running on, so I'm not sure if I can do that.

Thanks,
Trevor",learning rate scale factor make training stable work somewhat difference default work saying need recompile use root machine running sure thanks,issue,positive,positive,positive,positive,positive,positive
368247453,"Also, where did you get loss-scale of 256? This example doesn't need any loss scale, it's only included as a demonstration of using loss scale when needed.",also get example need loss scale included demonstration loss scale,issue,negative,neutral,neutral,neutral,neutral,neutral
368247279,"For nccl distributed you need to build from source with nccl version 2.1.2 installed locally. You'll also need to make sure the build picks up this version of nccl. In fact when you don't run it as the README specifies, it does not run multi-gpu.",distributed need build source version locally also need make sure build version fact run run,issue,negative,positive,positive,positive,positive,positive
368246922,"When I run this, it works: (16 bit, 1 gpu)
`CUDA_VISIBLE_DEVICES=0 python3 main.py /data/ilsvrc -a resnet50 -b 128 --fp16 --loss-scale=256 --lr=.01`

This works too: (32 bit, 2 gpus)
`CUDA_VISIBLE_DEVICES=0,1 python3 main.py /data/ilsvrc -a resnet50 -b 128 --lr=.01`

This is broken: (16 bit, 2 gpus)
`CUDA_VISIBLE_DEVICES=0,1 python3 imain.py /data/ilsvrc -a resnet50 -b 128 --fp16 --loss-scale=256 --lr=.01`

It converges much more slowly and I get high loss. If I run the same 3 examples with a higher learning rate, again the first two work, but the last one now diverges.

My understanding is that --dist-backend is not functional for fp16. It's not what I want either, my GPU's are both local, so I want `DataParallel`

Thanks!",run work bit python work bit python broken bit python much slowly get high loss run higher learning rate first two work last one understanding functional want either local want thanks,issue,negative,positive,neutral,neutral,positive,positive
368245877,Could you please walk me through what exactly you're running? The example I have up is https://github.com/csarofeen/examples/tree/dist_fp16/imagenet I would recommend running with `python -m multiproc main.py -a resnet50 -b 256 -j 5 -p 10 --fp16 --dist-backend nccl`,could please walk exactly running example would recommend running python,issue,positive,positive,positive,positive,positive,positive
367797939,"Would this break PRs that comes before https://github.com/pytorch/pytorch/pull/5225? One thing we can do is to change the MNIST links in https://github.com/pytorch/pytorch/blob/master/.jenkins/perf_test/test_gpu_speed_mnist.sh and https://github.com/pytorch/pytorch/blob/master/.jenkins/perf_test/test_cpu_speed_mnist.sh to point to colesbury:perftests in the PyTorch PR, and then after it is merged we can merge this PR to master.",would break come one thing change link point merge master,issue,negative,neutral,neutral,neutral,neutral,neutral
366850646,"I tried to adapt your code to run Pong and it runs into severe memory issues. With cuda, I encountered cuda runtime error: out of memory by the 1200 episodes. Without cuda, the Linux PC run out of RAM (32G) at around the same time.

In a Pong Game, where the agent plays up to 1500 steps with the game AI, the simple 2-layer NN with softmax seems to be replicated 1500 times in the computational graph. Is there a more memory efficient implementation of both REINFORCE and ACTOR-CRITIC?

I documented the issue on PyTorch:

https://github.com/pytorch/pytorch/issues/5353",tried adapt code run pong severe memory error memory without run ram around time pong game agent game ai simple replicated time computational graph memory efficient implementation reinforce issue,issue,negative,negative,negative,negative,negative,negative
365373240,"natural images dont have significantly different distributions on random crop vs center crop. they are different (because of how we take pictures, center usually has more information of interest), but not significantly.

in computer vision, it's become standard practice across the industry to do a center crop as a first-check and for more refined results, do a 5-crop (center + {left/right/top/bottom} corner)/ 10-crop (5-crop + hflip) / 144-crop (more dense cropping).

the examples reflect this common practice.",natural dont significantly different random crop center crop different take center usually information interest significantly computer vision become standard practice across industry center crop refined center corner dense reflect common practice,issue,positive,negative,neutral,neutral,negative,negative
365235567,This is an unfortunate effect of using pickle as the core of serialization for pytorch. Does this error also occur if you load from within the same directory that contains your model code?,unfortunate effect pickle core serialization error also occur load within directory model code,issue,negative,negative,negative,negative,negative,negative
364741677,"I got the similar error, can you elaborate a little bit more on ""You need to calculate the input size of this layer according to the input size of the net.""",got similar error elaborate little bit need calculate input size layer according input size net,issue,negative,positive,neutral,neutral,positive,positive
363741000,right! thanks for the response. I got confused a bit,right thanks response got confused bit,issue,negative,positive,neutral,neutral,positive,positive
363652436,`loss_function` is formulated correctly for gradient descent - minimise the reconstruction error and minimise the KL divergence between the distributions.,correctly gradient descent reconstruction error divergence,issue,negative,neutral,neutral,neutral,neutral,neutral
362479303,Orthogonal weight initialisation and hence this example should be fixed by https://github.com/pytorch/pytorch/pull/2734.,orthogonal weight hence example fixed,issue,negative,positive,neutral,neutral,positive,positive
362478759,"> This example illustrates how to use the efficient sub-pixel convolution layer

The code states that this is an example of how to use the layer, not a replica of the model from the associated paper.

The amount of padding is chosen to [make sure the output size is the same as the input size](https://stackoverflow.com/questions/37674306/what-is-the-difference-between-same-and-valid-padding-in-tf-nn-max-pool-of-t).",example use efficient convolution layer code example use layer replica model associated paper amount padding chosen make sure output size input size,issue,positive,positive,positive,positive,positive,positive
361782372,We probably should start a branch with updated scripts as people are starting to compile from source by themselves.,probably start branch people starting compile source,issue,negative,neutral,neutral,neutral,neutral,neutral
361028183,it is saved to the current working directory.,saved current working directory,issue,negative,neutral,neutral,neutral,neutral,neutral
361023295,"where did you specify th folder where to save best model and checkpoints ?

l run it as follow : 

python main.py --resume='/main_resnet18'


",specify th folder save best model run follow python,issue,positive,positive,positive,positive,positive,positive
360994961,"i think it is not beneficial to do extreme color augmentation, then network has to allocate some capacity to correct more and more weird coloring.",think beneficial extreme color augmentation network allocate capacity correct weird coloring,issue,negative,negative,negative,negative,negative,negative
360992440,"@soumith Hi, after removing color augmentation part, mobilenet as well as resnet can be successfully reproduced in my side(resnet18 69.41 in epoch 69). I'm so curious about that this step will hurt the training process.....",hi removing color augmentation part well successfully side epoch curious step hurt training process,issue,negative,positive,positive,positive,positive,positive
360365533,"@soumith BTW, in principle, this step is very important since it's commonly used in many famous architectures(ResNet, Inception, DenseNet....). Did u use color augmentation in training your released deep models as ResNet152 or DenseNet161?",principle step important since commonly used many famous inception use color augmentation training deep,issue,negative,positive,positive,positive,positive,positive
360365269,@soumith I think so.... I am also re-training the network without color augmentation now. ,think also network without color augmentation,issue,negative,neutral,neutral,neutral,neutral,neutral
360364687,maybe your color augmentation tricks are decreasing accuracy...?,maybe color augmentation decreasing accuracy,issue,negative,neutral,neutral,neutral,neutral,neutral
360362592,@soumith Do u use color augmentation tricks? This is the only difference.,use color augmentation difference,issue,negative,neutral,neutral,neutral,neutral,neutral
360359808,"my resnet18 run finished 75 epochs. I trained with 0.3.0.post4.
I used 2 P100 GPUs.

It reached validation  * Prec@1 69.436 Prec@5 89.002

Tomorrow I will kick-off a run with pytorch-master",run finished trained post used validation tomorrow run,issue,negative,neutral,neutral,neutral,neutral,neutral
360078597,You need to calculate the input size of this layer according to the input size of the net.,need calculate input size layer according input size net,issue,negative,neutral,neutral,neutral,neutral,neutral
359823813,"Thx for your kind help. The pytorch version is 0.4.0a0+017893e, and is installed from source. I use 4 P100 cards each with 64 images(totally 256 images/minibatch) . I add color augmentation in original script(transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5)) as ResNet paper said. Other settings as optimizar, learning policy are all identical to the script. The best model achieves 67.86% in ImageNet val, while released model achieves 69.65%.",kind help version source use totally add color augmentation original script paper said learning policy identical script best model model,issue,positive,positive,positive,positive,positive,positive
359819714,"I shall kick off a Resnet-18 run today to verify this.
To double-check, what PyTorch version are you using and how did you install it?",shall kick run today verify version install,issue,negative,neutral,neutral,neutral,neutral,neutral
358961937,A belated thank you for the explanation @t-vi / Tom - makes total sense,belated thank explanation total sense,issue,negative,neutral,neutral,neutral,neutral,neutral
358880673,@Cadene  Thanks for the tip! I already found solution. I'm closing this thread.,thanks tip already found solution thread,issue,positive,positive,positive,positive,positive,positive
358844285,"Hi SsnL,

Thank you for your response.
Maybe you are right, because when I run the script again, the problem has not appeared.
",hi thank response maybe right run script problem,issue,negative,positive,positive,positive,positive,positive
358321487,"@SsnL 
as you mentioned,I try to open the 'n09332890_20644.JPEG' by python.To my surprise,the picture damaged! so there are few images that are damaged in the Imagenet Train Dataset. 
 Thanks for your reply! ",try open surprise picture train thanks reply,issue,positive,positive,neutral,neutral,positive,positive
358010183,It could be IO problem. Could you add some more debug prints for validation set data loading?,could io problem could add validation set data loading,issue,negative,neutral,neutral,neutral,neutral,neutral
358009664,I'm confused. Do you want to resize to 256 or not? Do you want to still do random 224 crops?,confused want resize want still random,issue,negative,negative,negative,negative,negative,negative
358009305,Also we should update the doc on `no_grad` before next release.,also update doc next release,issue,negative,neutral,neutral,neutral,neutral,neutral
358008976,"If you look at the trace, the problem is that Pillow is having trouble loading the `/mnt/yp/muzidoctument/ilsvrc/train/n09332890/n09332890_20644.JPEG` image. Could you try and see if you can manually open it in python? Also check your Pillow install for any issues and updates.",look trace problem pillow trouble loading image could try see manually open python also check pillow install,issue,negative,negative,neutral,neutral,negative,negative
358006154,"> x in [seq_length, batch_size, embedding_size] ...=> x is [batch_size, embedding_size] .. i.e. x is the input to a RNN layer

I'm not sure where you get that impression. `[seq_length, batch_size, embedding_size]` is just an input format. It really doesn't imply anything. Feeding data as `[batch_size, seq_length, embedding_size]` will achieve same equivalent result. Library functions do what they promise in the doc. Whether it slices the first dimension to get input is not implied anywhere.

> which means words are not fed one by one, but instead are grouped in batches... 

Batch dimension is across different sequence. For a single sequence, the tokens are fed one by one.

Lastly, this is not a bug. You've already posted on the forum, which is the better place to discuss these.",input layer sure get impression input format really imply anything feeding data achieve equivalent result library promise doc whether first dimension get input anywhere fed one one instead grouped batch dimension across different sequence single sequence fed one one lastly bug already posted forum better place discus,issue,positive,positive,positive,positive,positive,positive
357947099,"@dugu9sword thanks for the reply. when the first dimension is sequence_length, this means input is fed like: x in [seq_length, batch_size, embedding_size] ...=> x is [batch_size, embedding_size] .. i.e. x is the input to a RNN layer ...which means words are not fed one by one, but instead are grouped in batches... the time dimension you mention is the same as seq_length right? Clearly, I am missing some small but significant thing here..",thanks reply first dimension input fed like input layer fed one one instead grouped time dimension mention right clearly missing small significant thing,issue,positive,positive,positive,positive,positive,positive
357939831,"Here is an example:
```
# supposing input is (batch, seq, feature)
rnn = torch.nn.RNN(~, batch_first=True)
output=rnn(input)
```",example supposing input batch feature input,issue,negative,neutral,neutral,neutral,neutral,neutral
357939197,"In most cases, the first dimension of an input is 'batch'. However in a recurrent neural networks, the input are fed into the cell one by one along the ""time"" dimension. Thus by default the first dimension of an input to fed into RNN will be ""seq_len"".
However, you can specify whether the first dimension of the input is ""batch"" by setting `batch_first`: if True, the input and output tensors are provided as (batch, seq, feature).",first dimension input however recurrent neural input fed cell one one along time dimension thus default first dimension input fed however specify whether first dimension input batch setting true input output provided batch feature,issue,negative,positive,positive,positive,positive,positive
357933754,"@dugu9sword I am not getting how input: (seq_len, batch, input_size) will work, are we feeding sequqnces in batches? 

That is not the purpose of RNNs right? RNNs should iterate over individual words of a sentence rather than a group of words ie. a batch?

Could you explain a bit here? Thank you.
",getting input batch work feeding purpose right iterate individual sentence rather group ie batch could explain bit thank,issue,negative,positive,positive,positive,positive,positive
357822000,"Having similar issue, gets stuck at epoch 0, running in a docker container on a P2 Amazon linux AMI with cuda 8. ",similar issue stuck epoch running docker container ami,issue,negative,neutral,neutral,neutral,neutral,neutral
356087393,"The PR is still open for this to be reviewed and merged.

@soumith @apaszke Would you mind taking a look at this? How should this proceed?",still open would mind taking look proceed,issue,negative,neutral,neutral,neutral,neutral,neutral
355433322,The default learning rate for batch size one should be lowered to 0.001 to make the network learn... That was suggest to me in the pytorch forum by ptrblck .,default learning rate batch size one make network learn suggest forum,issue,negative,neutral,neutral,neutral,neutral,neutral
355126062,"@rohun-tripathi no I am still struggling to find the exact problem. For me, it also gets stuck when using docker but works fine on my local machine. Additionally, I found that with nvidia-docker2 it works fine, but gets stuck using nvidia-docker1. So this can also be something related to nvidia-docker. 
Which version of nvidia-docker are you using? ",still struggling find exact problem also stuck docker work fine local machine additionally found work fine stuck also something related version,issue,negative,positive,positive,positive,positive,positive
355124026,"@iqbalu Any solution to this? I am getting this problem when the input data size is huge and the num_workers>0
It happens when using docker in Ubuntu systems",solution getting problem input data size huge docker,issue,negative,positive,positive,positive,positive,positive
354528773,I'm wondering what is the current status of this example?,wondering current status example,issue,negative,neutral,neutral,neutral,neutral,neutral
354328199,"I have the same confusion. It seems that due to RNN's default batch second setting, the example codes suffer from some implicit problems. **The word language model does have the same shape problem.**

Can anyone give us an answer?",confusion due default batch second setting example suffer implicit word language model shape problem anyone give u answer,issue,negative,negative,neutral,neutral,negative,negative
354052807,Is there any other solution to solve this error when we need to run pytorch 0.2 installed with conda?,solution solve error need run,issue,negative,neutral,neutral,neutral,neutral,neutral
353773816,Yes somebody should solve this error. Getting when installing pytorch=0.2 with conda.,yes somebody solve error getting,issue,negative,neutral,neutral,neutral,neutral,neutral
353406981,"`import torch
import torch.autograd
import torch.nn
import torch.multiprocessing
import torch.utils
import torch.legacy.nn
import torch.legacy.optim

xp = torch.load(r""D:\SDS\1_MachineLearning\amr-eager-master\LDC2015E86\reentrancies.dat"")

Traceback (most recent call last):

  File ""<ipython-input-10-c4a3fa896893>"", line 9, in <module>
    xp = torch.load(r""D:\SDS\1_MachineLearning\amr-eager-master\LDC2015E86\reentrancies.dat"")

  File ""D:\Anaconda3\envs\amr-eager\lib\site-packages\torch\serialization.py"", line 261, in load
    return _load(f, map_location, pickle_module)

  File ""D:\Anaconda3\envs\amr-eager\lib\site-packages\torch\serialization.py"", line 399, in _load
    magic_number = pickle_module.load(f)

UnpicklingError: invalid load key, ''.`

I'm loading a model available [here](http://kinloch.inf.ed.ac.uk/public/direct/amreager/LDC2015E86.tar.gz). ",import torch import import import import import import recent call last file line module file line load return file line invalid load key loading model available,issue,negative,positive,neutral,neutral,positive,positive
353062133,"@soumith  May I know the reason behind why it was done by hand instead of using an optimizer? Trying to get the insight, since I am going to use it in a project.

",may know reason behind done hand instead trying get insight since going use project,issue,negative,negative,negative,negative,negative,negative
352728743,"I am still confused about how we should set the weight between two losses. 

Anyway, thank you very much!",still confused set weight two anyway thank much,issue,negative,negative,neutral,neutral,negative,negative
351458715,can someone please explain why this issue was closed? I'm facing the same problem,someone please explain issue closed facing problem,issue,negative,negative,neutral,neutral,negative,negative
350074355,"On a Windows machine, you may need to wrap your multiprocessing code inside:
`if __name__ == '__main__'`",machine may need wrap code inside,issue,negative,neutral,neutral,neutral,neutral,neutral
349865863,"Same issue sometimes occured on my Ubuntu 16.04, when training other networks,
the training process just got stuck at Epoch: [0].
",issue sometimes training training process got stuck epoch,issue,negative,neutral,neutral,neutral,neutral,neutral
349813304,"Hit me this week. On Ubuntu 16 machine everything works fine, but in a docker container, it freezes randomly. Once it also completed successfully.  ",hit week machine everything work fine docker container randomly also successfully,issue,positive,positive,positive,positive,positive,positive
348385356,"fixed, I just need build vision package from source",fixed need build vision package source,issue,negative,positive,neutral,neutral,positive,positive
346094786,"yes, see https://github.com/pytorch/examples/pull/249 for the fix. We'll merge this PR once v0.3.0 is released.",yes see fix merge,issue,negative,neutral,neutral,neutral,neutral,neutral
345500353,"@barnabytprowe, it really depends on the nature of what you are learning. Here, the tanh would have required the input to the tanh to get extremely large/small to reproduce values at +/- 1 or very close. This - that you need extremely large values - is a bad idea in general.
On the other hand, if you know that values close to +/-1 are extremely improbable, you might go with tanh at the end.
(we might move to the [discussion forums](https://discuss.pytorch.org/), I'm tom there)
",really nature learning tanh would input tanh get extremely reproduce close need extremely large bad idea general hand know close extremely improbable might go tanh end might move discussion,issue,negative,negative,neutral,neutral,negative,negative
345384615,"A quick question @t-vi , as someone interested to learn - was the tanh nonlinearity in the output a bad idea in this specific example, or is it one _in general_?",quick question someone interested learn tanh output bad idea specific example one,issue,negative,negative,neutral,neutral,negative,negative
344544059,"I got the above error message with 0.2.0 version.

When I build pytorch master branch.('0.4.0a0+a3bf06c')
There was no error message. It works well.
Thank you.",got error message version build master branch error message work well thank,issue,negative,neutral,neutral,neutral,neutral,neutral
344478702,"We have a similar problem with training locking up on a CentOS system with 4 Pascal Titan Xs in an Ubuntu docker container. We can exec into the docker container, but can't kill the process. 

We have not seen this on systems using Ubuntu 16.",similar problem training locking system docker container docker container ca kill process seen,issue,negative,neutral,neutral,neutral,neutral,neutral
344113358,"@colesbury well spotted, and thanks for tidying this up more generally too! Out of curiosity, are there any reasons one might want to accumulate losses in an array to then sum over rather than accumulating them on the fly using `+=`?",well spotted thanks generally curiosity one might want accumulate array sum rather fly,issue,positive,positive,positive,positive,positive,positive
344111823,There was a bug in the `reinforce.py` example. It referred to `reward` instead of `r`.,bug example reward instead,issue,positive,neutral,neutral,neutral,neutral,neutral
344095799,"So, I reckon there is a bug in the `master` version of reinforce too. I've run a ton of REINFORCE algos lately, and sure there's high variance, but I've never seen the reward suddenly flatline to zero, and stay at exactly 0, forever.

But the one time I ported my model to `master` reinforce, and left it running overnight, I came back in the morning, and it had been flatlined pretty much the last 7 hours or so :P died at around 1am or so.

Hence, I left `master` alone for now, and am keeping on v0.2, so I don't have to be the one to diagnose/fix this bug :)  (sometimes I am energetic and fix stuff, but sometimes I try to stay focused on my actual intended end task :) ). But I do think there's something not quite right; and that correlates wth zou3519 noticing something odd plausibly.

(Edit: if I was going to go further than this, I think I would poke around and see if we have any tests that compare the current existing REINFORCE output wtih the new version; and if there isnt such a test, then write some somewhere, and see what happens. It's possible that my own flat-lining experience was elsewhere though. It was at the same time as porting to cuda 9, running on v100, and upgrading from v0.2 to `master`, so there's a lot of things that could have changed/broken...)",reckon bug master version reinforce run ton reinforce lately sure high variance never seen reward suddenly zero stay exactly forever one time ported model master reinforce left running overnight came back morning pretty much last around hence left master alone keeping one bug sometimes energetic fix stuff sometimes try stay actual intended end task think something quite right something odd plausibly edit going go think would poke around see compare current reinforce output new version test write somewhere see possible experience elsewhere though time running master lot could,issue,negative,positive,positive,positive,positive,positive
344066053,"@zou3519 I'm tempted to say that this is just the instability of pure REINFORCE. I checked the old `reinforce.py` under 0.2 - works fine - and `actor_critic.py` under 0.3 - works fine, so I can replicate your findings. I've tried tuning some hyperparameters, network architecture, the random seed etc., and can improve or worsen stability, but still haven't found a setting that learns properly. Perhaps someone who isn't familiar with RL can spot a difference between `reinforce.py` and `actor_critic.py` that I'm missing, but I didn't spot any discrepancies (apart from learning rate, which is something that would be tuned separately anyway).",say instability pure reinforce checked old work fine work fine replicate tried tuning network architecture random seed improve worsen stability still found setting properly perhaps someone familiar spot difference missing spot apart learning rate something would tuned separately anyway,issue,negative,positive,positive,positive,positive,positive
344031704,"@Kaixhin regarding `reinforce.py`:

I ran the model under pytorch v0.3.0 and it doesn't seem to be training anymore:
![image](https://user-images.githubusercontent.com/5652049/32745028-7d83b478-c87f-11e7-9b3d-cf9c1294aee9.png)
I don't know much about RL, do you know what could be causing this?",regarding ran model seem training image know much know could causing,issue,negative,positive,positive,positive,positive,positive
343146633,thanks a lot for investigating and fixing it Thomas!,thanks lot investigating fixing,issue,negative,positive,positive,positive,positive,positive
343077561,"For reference, I think that ending the network with a nonlinearity is a bad idea. My suggestion is to add a linear layer (PR #250 ).",reference think ending network bad idea suggestion add linear layer,issue,negative,negative,negative,negative,negative,negative
343058095,"You don't want the nonlinearity at the final output.
You could either add linear layer or so, e.g. https://gist.github.com/t-vi/886eed2dda02319e5d111799eb2446be ,  or use the unmodulated output (c_t) for the last layer.",want final output could either add linear layer use unmodulated output last layer,issue,negative,neutral,neutral,neutral,neutral,neutral
342956614,"@barnabytprowe thanks for pointing out the variability in the learning rate.  Looking at the history, it seems that @boathit and @soumith changed the learning rate to 0.8.  

@boathit or @soumith , can you comment on this?  Were you actually able to get viable output with a learning rate of 0.8? 
Because when I run this example as is, I get the following plot instead of the one displayed for this example: https://discuss.pytorch.org/uploads/default/original/2X/2/221e3389d4d569afff9d073c08b36a1b1e0604b6.jpg

",thanks pointing variability learning rate looking history learning rate comment actually able get viable output learning rate run example get following plot instead one displayed example,issue,negative,positive,positive,positive,positive,positive
341628242,"According to [torch.nn.BCELoss](http://pytorch.org/docs/master/nn.html?highlight=binary_cross_entropy#torch.nn.BCELoss) doc,  and [its cpu implementation source code ](https://github.com/pytorch/pytorch/blob/a64560c22eb3882bc6b9ab15d12e1d832f3735fc/torch/legacy/nn/BCECriterion.py#L20-L54), if `size_average=True`, BCE loss is averaged over both batch and image pixel dimension. Because when computing `- log(input) * target - log(1 - input) * (1 - target)`, both `input` and `target` are resized into one dimension via `view(-1)`, and the outcome is divided by the one dimension size, which is `batch_size * image pixel dimension`.",according doc implementation source code loss batch image dimension log input target log input target input target one dimension via view outcome divided one dimension size image dimension,issue,negative,neutral,neutral,neutral,neutral,neutral
341252828,"I think this discussion is tangentially relevant: https://discuss.pytorch.org/t/lstm-time-sequence-generation/1916

As is this: https://discuss.pytorch.org/t/lstm-time-sequence-generation/1916",think discussion tangentially relevant,issue,negative,positive,positive,positive,positive,positive
341247904,"Thanks @plang85 , have posted on #243, will move my involvement in the discussion there",thanks plang posted move involvement discussion,issue,negative,positive,positive,positive,positive,positive
341247405,"Thanks for linking this issue @plang85 .

I had been having trouble with this example and had put some results on #215.  I had to reduce the learning rate `lr` significantly to achieve a ""good"" solution.

I ran three cases spanning the decade from `lr` = 0.01 up to `lr` = 0.10.

These are below (gzipped avi and stdout output from the code):
lr = 0.01:
[lr_0.01_epochs_300_predicts.avi.gz](https://github.com/pytorch/examples/files/1435747/lr_0.01_epochs_300_predicts.avi.gz)
[lr_0.01_epochs_300_predicts.stdout.txt.gz](https://github.com/pytorch/examples/files/1435748/lr_0.01_epochs_300_predicts.stdout.txt.gz)

lr = 0.03:
[lr_0.03_epochs_300_predicts.avi.gz](https://github.com/pytorch/examples/files/1435749/lr_0.03_epochs_300_predicts.avi.gz)
[lr_0.03_epochs_300_predicts.stdout.txt.gz](https://github.com/pytorch/examples/files/1435750/lr_0.03_epochs_300_predicts.stdout.txt.gz)

lr = 0.10:
[lr_0.10_epochs_300_predicts.avi.gz](https://github.com/pytorch/examples/files/1435751/lr_0.10_epochs_300_predicts.avi.gz)
[lr_0.10_epochs_300_predicts.stdout.txt.gz](https://github.com/pytorch/examples/files/1435752/lr_0.10_epochs_300_predicts.stdout.txt.gz)

The videos are 10 steps / epochs per second, 300 epochs / 30 seconds total. If anyone would rather have the videos in a .zip format let me know.

As you can see, only the `lr` = 0.01 case converges to a solution which repeats in the way illustrated in the example. Both others find a different stable minimum in the cost function, in the `lr` = 0.10 case with a lower final cost than the ""better"" case that repeats stably.

I think
- when training, better fits to the training data can often be models that don't repeat well
- the slower learning rate helps the `lr` = 0.1 model move stably to ""good"" repeating solution for this one example configuration of random initial weights
- results will probably vary significantly if we use a different random initialisation of the weights

So I don't know too much what to read into this. But it might be good to change the learning rate in the example to 0.01 and set the number of learning steps / epochs to 150. The main issue is the code then takes a while to run, over 2 hours on my (old) laptop.",thanks linking issue plang trouble example put reduce learning rate significantly achieve good solution ran three decade output code per second total anyone would rather format let know see case solution way example find different stable minimum cost function case lower final cost better case stably think training better training data often repeat well learning rate model move stably good solution one example configuration random initial probably vary significantly use different random know much read might good change learning rate example set number learning main issue code run old,issue,positive,positive,positive,positive,positive,positive
340688535,"Yes, I'm partly agree with you, but with a small correction, the algorithm implemented should be an offline 
 version A2C(Advantage Actor Critic).",yes partly agree small correction algorithm version advantage actor critic,issue,positive,negative,negative,negative,negative,negative
340425803,"@BestSonny The code works great. However, it seems that it doesn't work with multi-GPU with nn.DataParallel. ",code work great however work,issue,positive,positive,positive,positive,positive,positive
339806223,thank you so much! the comments look really thoughtful and well done.,thank much look really thoughtful well done,issue,positive,positive,positive,positive,positive,positive
339417068,"In it's current state the example is broken, so I think this deserves to be an open issue, pls correct me if I'm wrong",current state example broken think open issue correct wrong,issue,negative,negative,negative,negative,negative,negative
339414911,"backing out #222 makes the example converge again, trying to make sense of this",backing example converge trying make sense,issue,negative,neutral,neutral,neutral,neutral,neutral
339414039,Just saw that this is still being duscussed in a previously closed issue #215 ,saw still previously closed issue,issue,negative,negative,neutral,neutral,negative,negative
338615731,"Great, thanks for getting back to me @mlpanda ... I think that means this issue can be closed.",great thanks getting back think issue closed,issue,positive,positive,positive,positive,positive,positive
338512007,"@berkersonmez 

> I wonder how the error goes down with lr=0.1, while it gets seemingly worse results after step 7 according to the pdfs

I think the answer is that the LSTM model isn't being trained with the primary objective of making a nice, stable repeating sine wave.  It is trained to predict a single numerical value accurately based on an input sequence of prior numerical values.  It's kind of a different problem.

Models that predict the next value well on average in your data don't necessarily have to repeat nicely when recurrent multi-value predictions are made.  I think you might hope that the best solutions would repeat nicely, but it's not guaranteed in practice because you might not have enough data.

The task in this example is actually quite tricky.

By the way, sometimes you see smaller oscillations being preferred to larger ones in the recurrent predictions.  I think that when you see that change between epochs you are actually seeing the impact of a change in the weights to the model forget gates.  These control whether or not to retain values from a fair way back in the input sequence.  Pretty cool.",wonder error go seemingly worse step according think answer model trained primary objective making nice stable sine wave trained predict single numerical value accurately based input sequence prior numerical kind different problem predict next value well average data necessarily repeat nicely recurrent made think might hope best would repeat nicely practice might enough data task example actually quite tricky way sometimes see smaller preferred recurrent think see change actually seeing impact change model forget control whether retain fair way back input sequence pretty cool,issue,positive,positive,positive,positive,positive,positive
338510919,"OK so I jacked the number of steps up to 300 and ran three cases spanning the decade from `lr` = 0.01 up to `lr` = 0.10.

These are below (gzipped avi and stdout output from the code):
`lr` = 0.01:
[lr_0.01_epochs_300_predicts.avi.gz](https://github.com/pytorch/examples/files/1405345/lr_0.01_epochs_300_predicts.avi.gz)
[lr_0.01_epochs_300_predicts.stdout.txt](https://github.com/pytorch/examples/files/1405346/lr_0.01_epochs_300_predicts.stdout.txt)

`lr` = 0.03:
[lr_0.03_epochs_300_predicts.avi.gz](https://github.com/pytorch/examples/files/1405347/lr_0.03_epochs_300_predicts.avi.gz)
[lr_0.03_epochs_300_predicts.stdout.txt](https://github.com/pytorch/examples/files/1405348/lr_0.03_epochs_300_predicts.stdout.txt)

`lr` = 0.10:
[lr_0.10_epochs_300_predicts.avi.gz](https://github.com/pytorch/examples/files/1405349/lr_0.10_epochs_300_predicts.avi.gz)
[lr_0.10_epochs_300_predicts.stdout.txt](https://github.com/pytorch/examples/files/1405350/lr_0.10_epochs_300_predicts.stdout.txt)

The videos are 10 steps / epochs per second, 300 epochs / 30 seconds total.  If anyone would rather have the videos in a .zip format let me know.

As you can see, only the `lr` = 0.01 case converges to a solution which repeats in the way we want.  Both others find a different stable minimum in the cost function, in the `lr=0.10` case with a _lower_ final cost than the ""better"" case that repeats stably.

I think
* when training, better fits to the training data can often be models that don't repeat well
* the slower learning rate helps the `lr` = 0.1 model move stably to ""good"" repeating solution for this _one example configuration_ of random initial weights
* results will probably vary significantly if we use a different random initialisation of the weights

So I don't know too much what to read into this.  But it might be good to change the learning rate in the example to 0.01 and set the number of learning steps / epochs to 150.  The main issue is the code then takes a while to run, over 2 hours on my (old) laptop.

I have a fork where I ran these tests - would you prefer I move this discussion to a Pull Request @soumith ?",number ran three decade output code per second total anyone would rather format let know see case solution way want find different stable minimum cost function case final cost better case stably think training better training data often repeat well learning rate model move stably good solution example random initial probably vary significantly use different random know much read might good change learning rate example set number learning main issue code run old fork ran would prefer move discussion pull request,issue,positive,positive,positive,positive,positive,positive
338502067,"Yeah I am getting the exact results as @barnabytprowe.
I wonder how the error goes down with lr=0.1, while it gets seemingly worse results after step 7 according to the pdfs :confused:",yeah getting exact wonder error go seemingly worse step according confused,issue,negative,negative,negative,negative,negative,negative
338432251,"OK so I ended up doing 100 steps / epochs, but it still doesn't converge to a solution like the given example.  I can provide the images as fairly low quality jpgs (to come under 10MB) here: [lr_0.03_epochs_100_predicts.tar.gz](https://github.com/pytorch/examples/files/1404457/lr_0.03_epochs_100_predicts.tar.gz)
I also made a movie (gzipped avi): [lr_0.03_epochs_100_predicts.avi.gz](https://github.com/pytorch/examples/files/1404463/lr_0.03_epochs_100_predicts.avi.gz)

It strikes me that this is actually a fairly finely balanced thing to ask the NN to achieve: it is trained to predict the next point only, not a probable arc of points.  A lot of decisions can be made to support the first aim that do not necessarily support the second.  Thus solutions which achieve attractive long future predictions such as that illustrated in the documentation to this example do not necessarily persist under further training.

Is it asking too much, or am I missing something that ought to make me more optimistic?



",ended still converge solution like given example provide fairly low quality come also made movie actually fairly finely balanced thing ask achieve trained predict next point probable arc lot made support first aim necessarily support second thus achieve attractive long future documentation example necessarily persist training much missing something ought make optimistic,issue,positive,positive,positive,positive,positive,positive
338384080,"For `lr=0.03` things seem to be very slowly moving in the right direction, but 15 steps is insufficient.  I'm going to increase the number of steps to 30 and try again: [lr_0.03_predicts.tar.gz](https://github.com/pytorch/examples/files/1404033/lr_0.03_predicts.tar.gz)
",seem slowly moving right direction insufficient going increase number try,issue,negative,negative,neutral,neutral,negative,negative
338382368,"For `lr=0.1` I again get similar problems as @nnmm found for 0.7 above (but it's an interesting sequence, this is a tricky optimum to find I think): [lr_0.1_predicts.tar.gz](https://github.com/pytorch/examples/files/1404024/lr_0.1_predicts.tar.gz)

",get similar found interesting sequence tricky optimum find think,issue,negative,positive,positive,positive,positive,positive
338381114,"Hey @barnabytprowe, my difference was not the addition of a third layer (stupid of me to add that), but that they fed the cell state, c_t, into the next layer previously:

h_t2, c_t2 = self.lstm2(c_t, (h_t2, c_t2))

Instead that should be (as per my comment):

h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))

And yes, they have fixed that lately. As mentioned in #222 it works for the sine example either way, but it's correct to feed the hidden state, h_t, to the next layer.  ",hey difference addition third layer stupid add fed cell state next layer previously instead per comment yes fixed lately work sine example either way correct feed hidden state next layer,issue,negative,negative,negative,negative,negative,negative
338380454,"I'm also having problems with the example.  As is (lr=0.8) my final plot is:
[predict14.pdf](https://github.com/pytorch/examples/files/1403986/predict14.pdf)
![p14](https://user-images.githubusercontent.com/1454961/31850781-82e62bf8-b650-11e7-9d12-771df9e8c54a.jpg)
and things go wrong from step 1-2:
[predict1.pdf](https://github.com/pytorch/examples/files/1403987/predict1.pdf)
![p1](https://user-images.githubusercontent.com/1454961/31850799-d4d35dc8-b650-11e7-853b-9083925ebc43.jpg)

[predict2.pdf](https://github.com/pytorch/examples/files/1403988/predict2.pdf) (looks just like the final plot)

Here's the loss function info on stdout:
```
STEP:  0
loss: 0.537086230955
loss: 0.519054139134
loss: 0.334312643402
loss: 0.245408088178
loss: 0.241630530524
loss: 0.236310649411
loss: 0.227821229553
loss: 0.212925355116
loss: 0.18457783161
loss: 0.134251495296
loss: 0.0847459995036
loss: 0.0574250318391
loss: 0.0512263487439
loss: 0.0473003375388
loss: 0.039150461162
loss: 0.0352442710805
loss: 0.0323013329392
loss: 0.0247076271594
loss: 0.0190413202173
loss: 0.0149685327246
test loss: 0.0122880477308
STEP:  1
loss: 0.0123111050907
loss: 0.00789734904096
loss: 0.00671599497279
loss: 0.00353805780334
loss: 0.0032202868875
loss: 0.00275785860848
loss: 0.00255092406484
loss: 0.00235055785575
loss: 0.00218358551007
loss: 0.00186726224563
loss: 0.00158670291557
loss: 0.00253018196482
loss: 0.00124947355053
loss: 0.0012916389302
loss: 0.00115403097923
loss: 0.00113527961888
loss: 0.00111034928821
loss: 0.00110101619576
loss: 0.0010856783608
loss: 0.00107512001803
test loss: 0.00151556788033
STEP:  2
loss: 0.00106017296497
loss: 0.00101216329951
loss: 0.195053268076
loss: 1.49997814199
test loss: 1.50362019943
STEP:  3
loss: 1.49997814199
test loss: 1.50362019943
STEP:  4
loss: 1.49997814199
test loss: 1.50362019943
STEP:  5
loss: 1.49997814199
test loss: 1.50362019943
STEP:  6
loss: 1.49997814199
test loss: 1.50362019943
STEP:  7
loss: 1.49997814199
test loss: 1.50362019943
STEP:  8
loss: 1.49997814199
test loss: 1.50362019943
STEP:  9
loss: 1.49997814199
test loss: 1.50362019943
STEP:  10
loss: 1.49997814199
test loss: 1.50362019943
STEP:  11
loss: 1.49997814199
test loss: 1.50362019943
STEP:  12
loss: 1.49997814199
test loss: 1.50362019943
STEP:  13
loss: 1.49997814199
test loss: 1.50362019943
STEP:  14
loss: 1.49997814199
test loss: 1.50362019943
```",also example final plot go wrong step like final plot loss function step loss loss loss loss loss loss loss loss loss loss loss loss loss loss loss loss loss loss loss loss test loss step loss loss loss loss loss loss loss loss loss loss loss loss loss loss loss loss loss loss loss loss test loss step loss loss loss loss test loss step loss test loss step loss test loss step loss test loss step loss test loss step loss test loss step loss test loss step loss test loss step loss test loss step loss test loss step loss test loss step loss test loss step loss test loss,issue,negative,negative,negative,negative,negative,negative
338380130,"Hi guys - I'm just exploring known issues here because I'm having some trouble with the LSTM example along the lines of #215, but is this particular issue not resolved by the merge in #222? (@dtolpin might be able to comment.)

The only difference seems to be the extra _third_ layer you are advocating @mlpanda...",hi exploring known trouble example along particular issue resolved merge might able comment difference extra layer,issue,negative,positive,positive,positive,positive,positive
338241050,"Still happens for me with 0.8. Lower values at least allow the optimizer to function correctly (i. e. perform several steps and monotonically decrease the loss), but the result is still wrong. This is the final plot with lr=0.7:

![predict14](https://user-images.githubusercontent.com/1933253/31829117-a4b2e95e-b5bc-11e7-9f37-5f90904555fc.png)

",still lower least allow function correctly perform several monotonically decrease loss result still wrong final plot predict,issue,negative,negative,negative,negative,negative,negative
337501275,"Hi, @bartolsthoorn.

It works~!
I fixed It.
 
Thanks a lot!
 ",hi fixed thanks lot,issue,negative,positive,positive,positive,positive,positive
337488192,"Are your images placed in a subfolder of the data folder? You should not
pass a folder with the image files directly since this is not the structure
that is often used (usually the photos are bundled in folders by class
name). If your images are directly in data/ try to move everything into
data/1/ and run again?

Den 18 okt. 2017 4:47 fm skrev ""Gromit Park"" <notifications@github.com>:

> Also, I've already try absolute path.
>
> Like this:
> :~/examples/dcgan$ python3 main.py --dataset folder --dataroot
> '/home/kraken/examples/dcgan/data'
>
> But, met same error message
>
> What's wrong with me?
>
> Thanks.
>
> â€”
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/pytorch/examples/issues/236#issuecomment-337444785>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AAGgWu8Q6w3dr92qwgESILR3JT-3LuDSks5stWbTgaJpZM4P8EPU>
> .
>
",data folder pas folder image directly since structure often used usually class name directly try move everything run den park also already try absolute path like python folder met error message wrong thanks reply directly view mute thread,issue,negative,negative,neutral,neutral,negative,negative
337444785,"Also, I've already try absolute path.

Like this: 
:~/examples/dcgan$ python3 main.py --dataset folder --dataroot '/home/kraken/examples/dcgan/data' 

But, met same error message

What's wrong with me?

Thanks.",also already try absolute path like python folder met error message wrong thanks,issue,negative,negative,neutral,neutral,negative,negative
337443809,"Of course, I've already done such that.

What's wrong with me?

If the filenames must have a certain format?

Thanks.    

 ",course already done wrong must certain format thanks,issue,negative,negative,neutral,neutral,negative,negative
337442217,Try running the script with the current working directory inside dcgan.,try running script current working directory inside,issue,negative,neutral,neutral,neutral,neutral,neutral
337097818,"@zym1010 

Have you found a solution to this, except for the `pin_memory=False` setting? It doesn't work for me.",found solution except setting work,issue,negative,neutral,neutral,neutral,neutral,neutral
336734662,"No problem @apaszke . I thought the Linux kernel might be simpler and a bit more concrete than the Penn data, particularly with the addition of sample.py to show results. But I understand your point. ",problem thought kernel might simpler bit concrete data particularly addition show understand point,issue,negative,positive,positive,positive,positive,positive
336734334,"Thanks for the PR, but I think that's too much. This repo is meant to contain simple examples that show people how to write certain classes of models, without having a complete collection of relevant datasets.",thanks think much meant contain simple show people write certain class without complete collection relevant,issue,positive,positive,positive,positive,positive,positive
336127264,"Sorry forgot about this. Actually, I found that the problem has to do with calculating the ppl score for the batches early in the first epoch. If the loss starts out at or above ``700``, this will cause an ``OverflowError`` when calling ``math.exp`` that aborts the entire program.

Putting a ``try-except`` block around calculating the perplexity score (``math.exp(cur_loss)``) solved the issue. Let me know if you'd like me to fire up a PR for it.",sorry forgot actually found problem calculating score early first epoch loss cause calling entire program block around calculating perplexity score issue let know like fire,issue,negative,negative,neutral,neutral,negative,negative
335999144,This commit makes sense! I haven't found a single good example of usage of torchtext in a sequence prediction problem. Why wasn't it merged into the example? @jekbradbury ,commit sense found single good example usage sequence prediction problem example,issue,negative,positive,positive,positive,positive,positive
335848913,"I think we'll leave the defaults as they are. We're aware they won't work for all kinds of models, but that's how it is with hyperparameters.",think leave aware wo work,issue,negative,positive,positive,positive,positive,positive
335848714,"I'd add gradient clipping. Exploding gradients are common in RNNs, and would happen even with a smaller LR (unless you actually make it extremely small)",add gradient clipping common would happen even smaller unless actually make extremely small,issue,negative,negative,negative,negative,negative,negative
335576880,"That should work too. I just wanted to point out in this issue that, as it stands, the loss computation is incorrect (albeit, mildly).
",work point issue loss computation incorrect albeit mildly,issue,negative,positive,positive,positive,positive,positive
334380518,"@Smerity @keskarnitish wouldn't it be more straightforward to just use a criterion with `size_average=False`, then keep track of total loss and divide at the end by the # of examples seen? Or am i missing something...",would straightforward use criterion keep track total loss divide end seen missing something,issue,negative,positive,neutral,neutral,positive,positive
334343375,"I am also getting a similar kind of error...with python 3.5

Traceback (most recent call last):
  File ""train.py"", line 137, in <module>
    model=train_model(net,criterion,optimizer,torch.optim.lr_scheduler)
  File ""train.py"", line 79, in train_model
    outputs = model.forward(inputs)
  File ""/data/acp15maj/Action_recog_exp1/scripts/C3D_model.py"", line 59, in forward
    h = h.view(-1,8192)
  File ""/home/acp15maj/.conda/envs/pytorch/lib/python3.5/site-packages/torch/autograd/variable.py"", line 510, in view
    return View.apply(self, sizes)
  File ""/home/acp15maj/.conda/envs/pytorch/lib/python3.5/site-packages/torch/autograd/_functions/tensor.py"", line 96, in forward
    result = i.view(*sizes)
RuntimeError: invalid argument 2: size '[-1 x 8192]' is invalid for input of with 45056 elements at /opt/conda/conda-bld/pytorch_1503968623488/work/torch/lib/TH/THStorage.c:37
",also getting similar kind error python recent call last file line module net criterion file line file line forward file line view return self size file line forward result size invalid argument size invalid input,issue,negative,positive,positive,positive,positive,positive
334185371,"Still, Variable overhead is fairly small and we're aggressively reducing it. Removing one addition from Variable-land will have absolutely no effect on the run time.",still variable overhead fairly small aggressively reducing removing one addition absolutely effect run time,issue,negative,negative,neutral,neutral,negative,negative
334152640,"Exactly, it doesn't really make a difference, so at this point you'd probably have to ask the original authors.

Sorry, just assumed that it was added for a reason like backpropagation, but in this case yes you could just start working with tensors instead of Variables.",exactly really make difference point probably ask original sorry assumed added reason like case yes could start working instead,issue,positive,positive,neutral,neutral,positive,positive
334135567,"> The discriminator is trained to recognize real data as real and fake data as fake.

Yes, I see that. In fact, the discriminator is trained using the gradient from two losses: 
- `loss(discriminator(real_images), label_for_real_images)` 
- `loss(discriminator(generated_images), label_for_fake_images)`.

> It is possible to do what you suggested, but summing more accurately matches the discriminator update step in the literature

I went back to check the paper and I see what you mean, the code reflects what the authors do:
```1 / #batch * (loss_on_#batch_real_images + loss_on_#batch_fakes)``` 
I'm actually surprised because I'd have intuitively done this:
```1 / (2 * #batch) * (loss_on_#batch_real_images + loss_on_#batch_fakes)```
Do you happen to have an explanation for their choice or some resource to point me to?<br/>
Anyway, the result is basically the same, the only difference is that the computed gradient is doubled, but with an adaptive learning rate this does not affect training.

> Extracting `.data` and creating a new Variable breaks the computation graph, so you wouldn't be able to backpropagate through the networks

Yes, I understand how the graph works. However, at [line 239](https://github.com/pytorch/examples/blob/aa7adf08482be9136dac7ea65a188df89a8b1c7a/dcgan/main.py#L239) the two losses have already been computed and the two gradients deriving from their backprop have already been accumulated in the weights. At that point, we don't need the sum of the losses to be part of the graph, in fact we never call `errD.backward()`, and we are only interested in its value. Anyway, I'm positive that creating a new `Variable` does not add too much overhead compared to a `Tensor`, even if I don't know the implementation details.",discriminator trained recognize real data real fake data fake yes see fact discriminator trained gradient two loss discriminator loss discriminator possible accurately discriminator update step literature went back check paper see mean code batch actually intuitively done batch happen explanation choice resource point anyway result basically difference gradient doubled adaptive learning rate affect training new variable computation graph would able yes understand graph work however line two already two already point need sum part graph fact never call interested value anyway positive new variable add much overhead tensor even know implementation,issue,negative,positive,neutral,neutral,positive,positive
334007166,"> Why do we sum the losses instead of taking the mean?

It is possible to do what you suggested, but summing more accurately matches the discriminator update step in the literature (see Algorithm 1 in [the original paper](https://arxiv.org/pdf/1406.2661.pdf)). The discriminator is trained to recognise real data as real *and* fake data as fake. 

> Why do we operate on the Variables

Extracting `.data` and creating a new Variable breaks the computation graph, so you wouldn't be able to backpropagate through the networks. The general advice is [not to over-optimise for memory](http://pytorch.org/docs/master/notes/autograd.html#in-place-operations-on-variables). You may want to [read this](https://github.com/jcjohnson/pytorch-examples) for a reasonable introduction to the graph in PyTorch.",sum instead taking mean possible accurately discriminator update step literature see algorithm original paper discriminator trained real data real fake data fake operate new variable computation graph would able general advice memory may want read reasonable introduction graph,issue,negative,positive,neutral,neutral,positive,positive
334004618,"Hi @soumith, thanks for the dcgan example!

I have a couple of questions regarding line [239](https://github.com/pytorch/examples/blob/aa7adf08482be9136dac7ea65a188df89a8b1c7a/dcgan/main.py#L239):
```
errD = errD_real + errD_fake
```

- **Why do we sum the losses instead of taking the mean?** 
  If we were to feed to the discriminator a batch with 50 real images and 50 fakes, with the corresponding labels, we'd get a loss that is the mean of the individual losses. 
  In our case, we are feeding the network the two batches separately, computing the losses separately, backpropagating separately (but without zeroing the gradients, so that they accumulate), and then running an optimization step, but then we are summing the two losses instead of averaging
- **Why do we operate on the Variables**
  The total loss is simply computed to print an aggregate of the discriminator loss, could we save some memory by using `errD = (errD_real.data + errD_fake.data).mean()`, instead of creating a new Variable in the graph?",hi thanks example couple regarding line sum instead taking mean feed discriminator batch real corresponding get loss mean individual case feeding network two separately separately separately without accumulate running optimization step two instead operate total loss simply print aggregate discriminator loss could save memory instead new variable graph,issue,negative,negative,neutral,neutral,negative,negative
333412034,"Multivariate doesn't mean matrix, it just means vector. For example a 2D gaussian has 2 values, not 4. The VAE code therefore generates a 20D mean at the moment.

Theoretically the covariance could be a matrix (as off-diagonal terms would correspond to relationships between the different dimensions). However this is more difficult to deal with, so VAEs usually use a diagonal covariance matrix, hence the variance is also 20D.",mean matrix vector example code therefore mean moment theoretically covariance could matrix would correspond different however difficult deal usually use diagonal covariance matrix hence variance also,issue,negative,negative,negative,negative,negative,negative
333383408,This has actually been fixed in #213 so I think this can be closed cc @soumith ,actually fixed think closed,issue,negative,neutral,neutral,neutral,neutral,neutral
333250064,"Imagenet example shows how to use built-in optimizer on a master copy of parameters. It is hacky, but I don't know how palatable it would be to hide all this in the built-in optimizers. ",example use master copy hacky know palatable would hide,issue,negative,neutral,neutral,neutral,neutral,neutral
333247691,"The master copy/update strategy is essentially mandatory for FP16, so there should probably be an ""official"" way to do it that's compatible with builtin optimizers?",master strategy essentially mandatory probably official way compatible,issue,negative,neutral,neutral,neutral,neutral,neutral
332240268,"Ok, I pushed this just in case it was a typo.

I suppose the default has been tested so should work. Currently I only have access to my laptop (mac), which might be a fringe case. In this case, running ``RNN_RELU`` with defaults arguments produces ``nan`` losses immediately. Running ``RNN_TANH`` with defaults causes training to plateau at a loss of roughly 10 (> 1000 in ppl). The losses I'm referring to are the printouts so the actual loss used by ``torch`` in the backprop step might not suffer from overflow, but regardless on my machine the loss is unstable and doesn't improve as expected.

Being examples, it might be an idea to lower the learning rate a bit to ensure training is stable, but that's of course up to you. If you're happy with the current default, feel free to close this PR.",case typo suppose default tested work currently access mac might fringe case case running nan immediately running training plateau loss roughly actual loss used torch step might suffer overflow regardless machine loss unstable improve might idea lower learning rate bit ensure training stable course happy current default feel free close,issue,negative,positive,positive,positive,positive,positive
332213281,"the default setting is border-line fastest learning rate we can use, i believe.",default setting learning rate use believe,issue,negative,neutral,neutral,neutral,neutral,neutral
332085553,change the print frequency bigger and wait for a longer time,change print frequency bigger wait longer time,issue,negative,neutral,neutral,neutral,neutral,neutral
331607735,"@fmassa ,  thanks for your reference. I have one question. how to fullfil finetune in imagenet/main.py ?
set pretrained to be true just like the below code?
parser.add_argument('--pretrained', default='true', dest='pretrained', action='store_true',
                    help='use pre-trained model')  ",thanks reference one question set true like code model,issue,positive,positive,positive,positive,positive,positive
331361808,Found wrong help messages. Fixed.,found wrong help fixed,issue,negative,negative,negative,negative,negative,negative
331054491,"@soumith 
Hi Soumith, after changing the learning rate to 0.8, both the training loss and test loss go down steadily and we can reproduce the original result. A Pull Request has been made.",hi learning rate training loss test loss go steadily reproduce original result pull request made,issue,negative,positive,positive,positive,positive,positive
331043968,"could someone send a Pull Request to fix it, if they are confident that it's the right fix.
Thank you very much!",could someone send pull request fix confident right fix thank much,issue,positive,positive,positive,positive,positive,positive
330713939,It seems that this is caused by a too large learning rate in the optimizer. Setting the `LBFGS` learning rate to a smaller value like `0.8` can address the problem.,large learning rate setting learning rate smaller value like address problem,issue,negative,positive,positive,positive,positive,positive
330437309,"hey sanyam,

our metric for adding something to examples is:
- is it a baseline / standard across the community
- does it showcase good practices in situations where the other baselines dont
- is it something that folks will commonly come searching for

Based on these metrics, i dont think ARC is a good example to add.

Sorry for the delay in answering, I was on a long vacation and quite a few notifications slipped through the cracks in that time.

I think it'd be really nice to have ARC in something like: https://github.com/ritchieng/the-incredible-pytorch

We're thinking of blessing one of these lists soon and officially maintaining them.",hey metric something standard across community showcase good dont something commonly come searching based metric dont think arc good example add sorry delay long vacation quite slipped time think really nice arc something like thinking blessing one soon officially,issue,positive,positive,positive,positive,positive,positive
330141729,@sjagter Indeed ! I should have tried this instead of switching first to Adam.,indeed tried instead switching first,issue,negative,positive,positive,positive,positive,positive
328496168,"I got it working by using a smaller learning rate of 0.1 (default looks like 1.0).

Line 52: `optimizer = optim.LBFGS(seq.parameters(), lr=0.1)`

[predict14.pdf](https://github.com/pytorch/examples/files/1292293/predict14.pdf)
",got working smaller learning rate default like line,issue,negative,neutral,neutral,neutral,neutral,neutral
326789552,"I am not sure what it was like when you opened the issue, but if you try
`import gym`
`env = gym.make('CartPole-v0')`
`print(env.spec.reward_threshold)` with the latest gym in Python3.6, then you will get exactly ""195.0"".",sure like issue try import gym print latest gym get exactly,issue,positive,positive,positive,positive,positive,positive
325718845,"Deleted the vector file and allowed the script to redownload it, but got the same error.",vector file script got error,issue,negative,neutral,neutral,neutral,neutral,neutral
325511182,Can't replicate this on Python 2.7 or 3 -- maybe the vector file is corrupted?,ca replicate python maybe vector file corrupted,issue,negative,neutral,neutral,neutral,neutral,neutral
325356700,"@Pierre-Bartet I'm seeing the same results.  Were you able to get this working?  I tried backing out the last commit in the train file to see if it worked, but I still get the same results.  ",seeing able get working tried backing last commit train file see worked still get,issue,negative,positive,positive,positive,positive,positive
323782668,"Parameter juggling comes from recipes for fp16 convergence - one needs master copy of parameters in fp32, and apply updates in fp32.",parameter juggling come convergence one need master copy apply,issue,negative,neutral,neutral,neutral,neutral,neutral
323458305,Had a mistake which broke distributed. Fixed now.,mistake broke distributed fixed,issue,negative,positive,neutral,neutral,positive,positive
322812077,This may be okay for gloo across processes but will not work if we support nccl as backend.,may across work support,issue,negative,neutral,neutral,neutral,neutral,neutral
322382337,"I'm sorry, I dont know. We dont have an idea about Windows.",sorry dont know dont idea,issue,negative,negative,negative,negative,negative,negative
322046823,"I agree that this should be changed to:

        for i, input_t in enumerate(input.chunk(input.size(1), dim=1)):
            h_t, c_t = self.lstm1(input_t, (h_t, c_t))
            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))
            h_t3, c_t3 = self.lstm3(h_t2, (h_t3, c_t3))
            outputs += [h_t3]

        for i in range(future):# if we should predict the future
            h_t, c_t = self.lstm1(h_t3, (h_t, c_t))
            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))
            h_t3, c_t3 = self.lstm3(h_t2, (h_t3, c_t3))
            outputs += [h_t3]",agree enumerate range future predict future,issue,negative,neutral,neutral,neutral,neutral,neutral
321742402,"Can not reproduce this problem on 0.2.0, do you change anything? get_batch should return variable with shape (32,4) and (4,1)",reproduce problem change anything return variable shape,issue,negative,neutral,neutral,neutral,neutral,neutral
319673086,"I just ran this with `python main.py` (GPU) and `python main.py --no-cuda` (CPU), and don't have any issues, despite the batch size being 64. If you still have this error now, please try uninstalling and reinstalling torch and torchvision.",ran python python despite batch size still error please try torch,issue,negative,neutral,neutral,neutral,neutral,neutral
319668521,"@Kaixhin  Does this modification work with standard pytorch release download from conda?

```
  File ""~/examples/super_resolution/model.py"", line 27, in _initialize_weights
    init.orthogonal(self.conv1.weight, init.gain('relu'))
AttributeError: module 'torch.nn.init' has no attribute 'gain'
```",modification work standard release file line module attribute,issue,negative,neutral,neutral,neutral,neutral,neutral
317276438,"Are you talking about the use of `normalize_batch` within `neural_style.py`? Looks like this is applied before feeding images into a pretrained VGG net, which was trained on ImageNet. Whether or not normalisation is applied to images fed into the transformer network is a completely separate matter.",talking use within like applied feeding net trained whether applied fed transformer network completely separate matter,issue,negative,positive,neutral,neutral,positive,positive
317243640,"I quickly drew up the computation graph to show why this already as efficient as possible. First, D takes a real image and calculates one forward/backward pass on that (D is trained to classify this as real). Then, D takes a fake image (which is ""detached"" from G to prevent gradients going back in to G) and calculates one forward/backward pass on that (D is trained to classify this as fake). Then we update the D using these gradients. So whether or not you add these first two losses, they come from completely different forward passes, so as Soumith said, there's completely different backward passes to be made and there is no computation to be saved.

Finally, we take the fake image (still attached to G) and calculates one forward/backward pass on that (D is trained to classify this as *real*). Only G is updated with these gradients.

![gan](https://user-images.githubusercontent.com/991891/28498631-90c38a70-6f99-11e7-84ed-d2925da0f09e.png)",quickly drew computation graph show already efficient possible first real image one pas trained real fake image detached prevent going back one pas trained fake update whether add first two come completely different forward said completely different backward made computation saved finally take fake image still attached one pas trained real gan,issue,negative,negative,neutral,neutral,negative,negative
316450005,@log26 I didn't see such a problem. Can you print out your log or model?,log see problem print log model,issue,negative,neutral,neutral,neutral,neutral,neutral
316253377,"@BestSonny  I use resnet50 to do classification task, and add center loss after avgpool layer (the dimension of features is 2048).  When the network backward, the memory requires more than 12G even I set the batch size to 1.  Is it reasonable? Or maybe I do something wrong?",use classification task add center loss layer dimension network backward memory even set batch size reasonable maybe something wrong,issue,negative,negative,negative,negative,negative,negative
315954405,"I add center loss to resnet50. Although I set the batch size to 1, it's still out of memory when backward. Is it reasonable? ",add center loss although set batch size still memory backward reasonable,issue,negative,positive,neutral,neutral,positive,positive
314301695,@exiaxu this is because of different preprocessing needed for the pytorch pretrained vgg. I have a version of the algorithm [here](https://github.com/abhiskk/fast-neural-style) which uses the pretrained caffe vgg and has the same `args.content_loss` and `args.style_loss` scales as the one in original fast-neural-style code.,different version algorithm scale one original code,issue,negative,positive,positive,positive,positive,positive
314299793,Can you explain why the hyperparameter `args.content_loss` and `args.style_loss` is so huge while in [Justin Johnson's code](https://github.com/jcjohnson/fast-neural-style/blob/master/train.lua) these parameter is much smaller ?,explain huge code parameter much smaller,issue,negative,positive,positive,positive,positive,positive
313899307,"I install pytorch from conda, can not reproduce your problem.

```
(pytorch) âœ  reinforcement_learning git:(master) python actor_critic.py
[2017-07-09 13:09:50,068] Making new env: CartPole-v0
Episode 10      Last length:   132      Average length: 14.91
Episode 20      Last length:    21      Average length: 15.53
Episode 30      Last length:    50      Average length: 16.97
Episode 40      Last length:   199      Average length: 21.94
Episode 50      Last length:    26      Average length: 25.18
Episode 60      Last length:    35      Average length: 26.79
Episode 70      Last length:    43      Average length: 28.10
```",install reproduce problem git master python making new episode last length average length episode last length average length episode last length average length episode last length average length episode last length average length episode last length average length episode last length average length,issue,negative,negative,neutral,neutral,negative,negative
313828905,"Problem Solved: Actually, it is caused by the PIL (the Python Imaging Library). Try to reinstall PIL via `pip install pillow`.",problem actually python library try reinstall via pip install pillow,issue,negative,neutral,neutral,neutral,neutral,neutral
311338482,"Seems like I have one solution to this problem, after the discriminator, the output size changed with the input, when you using loss calculate the input and the target, your label size not changed, so you can let the label size changed with your input size, or you should let the discriminator output the fix size no matter what size data you input . 

FYI 
```
size_feature = self.D_A(x_A).size()
real_tensor.data.resize_(size_feature).fill_(real_label)
fake_tensor.data.resize_(size_feature).fill_(fake_label)
l_d_A_real, l_d_A_fake = bce(self.D_A(x_A), real_tensor), bce(self.D_A(x_BA), fake_tensor)

```

like your x_A had size [batch_size, 3, 64, 64], after the D , size_feature will be [batch_size, 1],  real_label size [batch_size], 
But when your input x_A size changed ,like [batch_size, 3, 128, 128], after the D, the output size will be [batch_size, 25], when you calculating the loss between [batch_size, 25] and label [batch_size,] occurred the error. 

",like one solution problem discriminator output size input loss calculate input target label size let label size input size let discriminator output fix size matter size data input like size size input size like output size calculating loss label error,issue,negative,neutral,neutral,neutral,neutral,neutral
310789122,"Hi @abhiskk, thanks for the suggestion. It seems the png image was the culprit. I could successfully run the example on a jpg file. Even like this, my GPU seems too small to run the example (out of memory), but that is a different problem :) (without CUDA, it works fine)
Thanks for your help!",hi thanks suggestion image culprit could successfully run example file even like small run example memory different problem without work fine thanks help,issue,positive,positive,positive,positive,positive,positive
309644366,"Oh, just found that there're tutorial in the official page",oh found tutorial official page,issue,negative,neutral,neutral,neutral,neutral,neutral
309586398,"Hey @fakufaku , I just tested the code again and it works for me, can you install pytorch using `conda` following the instructions from the [pytorch site](http://pytorch.org/). I think there might be some issue using the CUDA version of code on OSX, can you try running the code with the option `--cuda 0`. Also can you test with the content image that is present in the repository `/images/content-images/amber.jpg`, it's unlikely but there might be an issue with loading the PNG file format which you are using.",hey tested code work install following site think might issue version code try running code option also test content image present repository unlikely might issue loading file format,issue,negative,negative,negative,negative,negative,negative
309259704,"Done.
```
[2017-06-18 14:15:18,993] Making new env: CartPole-v0
Episode 0	Last length:    23	Average length: 2.30
Episode 10	Last length:    13	Average length: 31.30
Episode 20	Last length:    31	Average length: 17.40
Episode 30	Last length:    28	Average length: 18.70
Episode 40	Last length:    13	Average length: 17.90
Episode 50	Last length:    10	Average length: 13.70
Episode 60	Last length:    13	Average length: 16.00
Episode 70	Last length:    15	Average length: 19.70
Episode 80	Last length:    11	Average length: 15.00
Episode 90	Last length:    10	Average length: 14.30
Episode 100	Last length:    13	Average length: 12.90
Episode 110	Last length:    11	Average length: 13.60
Episode 120	Last length:    27	Average length: 14.70
Episode 130	Last length:    17	Average length: 29.50
Episode 140	Last length:    21	Average length: 31.10
Episode 150	Last length:    64	Average length: 45.80
Episode 160	Last length:   142	Average length: 97.60
Episode 170	Last length:   196	Average length: 163.60
Episode 180	Last length:   144	Average length: 184.80
Episode 190	Last length:   199	Average length: 196.70
Episode 200	Last length:   199	Average length: 194.70
Episode 210	Last length:   171	Average length: 193.30
Episode 220	Last length:   199	Average length: 198.10
```",done making new episode last length average length episode last length average length episode last length average length episode last length average length episode last length average length episode last length average length episode last length average length episode last length average length episode last length average length episode last length average length episode last length average length episode last length average length episode last length average length episode last length average length episode last length average length episode last length average length episode last length average length episode last length average length episode last length average length episode last length average length episode last length average length episode last length average length episode last length average length,issue,negative,negative,neutral,neutral,negative,negative
309228505,"Uh.. plz don't merge right now, let me try to get a better default parameters.",merge right let try get better default,issue,negative,positive,positive,positive,positive,positive
309054968,isn't the loss of real and the fake loss both losses of the discriminator ? how does  the 2nd graph (generator )come into play here?,loss real fake loss discriminator graph generator come play,issue,negative,negative,negative,negative,negative,negative
309037630,"No, this will still call backward of the two separate graphs separately.",still call backward two separate separately,issue,negative,neutral,neutral,neutral,neutral,neutral
308872765,You gave a path to a `.jpg` file as a model checkpoint.,gave path file model,issue,negative,neutral,neutral,neutral,neutral,neutral
307652809,@sanyam5 Some time ago I wrote an example of character-level RHN: https://gist.github.com/a-rodin/d4f2ab5d7eb9d9887b26f28144e4ffdf.,time ago wrote example,issue,negative,neutral,neutral,neutral,neutral,neutral
307408445,"@mratsim 
I use your code to finetune the densenet121 on my own dataset, and I can validate it works. Thanks for sharing the code!
By the way, what the meaning of ` It must be parameterized for all DenseNets`?",use code validate work thanks code way meaning must,issue,positive,positive,positive,positive,positive,positive
307066471,"@mratsim Thanks for the code. I have tested it, and that was so good. However, I can't finetune 'SqueezeNet 1.0' model. would you please help me. In fact, my code for fine tuning the squeezenet1_0, is as follows:

```
model_conv = torchvision.models.squeezenet1_0(pretrained=True)
mod = list(model_conv.classifier.children())
mod.pop()
mod.append(torch.nn.Linear(1000, 7))
new_classifier = torch.nn.Sequential(*mod)
print( list(list(new_classifier.children())[1].parameters()) )
model_conv.classifier = new_classifier
for p in model_conv.features.parameters():
    p.requires_grad = False

```",thanks code tested good however ca model would please help fact code fine tuning list print list list false,issue,positive,positive,positive,positive,positive,positive
306924837,"Thanks @soumith , happy to make a contribution!",thanks happy make contribution,issue,positive,positive,positive,positive,positive,positive
306747409,"Because `nn.DataParallel` accepts inputs that are on CPU as well. It's faster this way, because it can copy only the needed parts to each GPU.

Note that we're using GitHub issues for bug reports only, and all questions should be posted on [our forums](discuss.pytorch.org).",well faster way copy note bug posted,issue,negative,neutral,neutral,neutral,neutral,neutral
306565010,"Hmm thanks for your reply, I tried `state.cuda()` and `reward.cuda()` and also this:

`autograd.backward(policy.saved_actions.cuda(), [None for _ in policy.saved_actions.cuda()])`

But resulted in the same error message.  And taken from this [DQN tutorial](http://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html), I also used the below with the same results:

```
FloatTensor = torch.cuda.FloatTensor if args.cuda else torch.FloatTensor
LongTensor = torch.cuda.LongTensor if args.cuda else torch.LongTensor
ByteTensor = torch.cuda.ByteTensor if args.cuda else torch.ByteTensor
Tensor = FloatTensor
```

Interestingly all the DQN tutorial did was `model.cuda()` with the above and it worked there.",thanks reply tried also none error message taken tutorial also used else else else tensor interestingly tutorial worked,issue,positive,positive,positive,positive,positive,positive
306374376,this has been (unfortunately) succeeded by https://github.com/pytorch/examples/pull/129 which is now merged into master. Thanks a lot for the PR.,unfortunately master thanks lot,issue,negative,negative,negative,negative,negative,negative
306287926,"@mratsim Thanks for the code, and I will test it on my custom dataset tomorrow.",thanks code test custom tomorrow,issue,negative,positive,positive,positive,positive,positive
305950890,"@panovr @sanealytics:

Here is my fine-tuning code for DenseNet-121. It must be parameterized for all DenseNets. Also it only gave me `nan` on the dataset I tried to use it on so it needs testing on MNIST / CIFAR-10 I think.

```python
class DenseNet121(nn.Module):
    def __init__(self, num_classes):
        super(DenseNet121, self).__init__()
        
        original_model = models.densenet121(pretrained=True)
        
        # Everything except the last linear layer
        self.features = nn.Sequential(*list(original_model.children())[:-1])
        
        # Get number of features of last layer
        num_feats = original_model.classifier.in_features
        
        # Plug our classifier
        self.classifier = nn.Sequential(
        nn.Linear(num_feats, num_classes)
        )

        # Init of last layer
        for m in self.classifier:
            kaiming_normal(m.weight)
            
        # Freeze weights
        # for p in self.features.parameters():
        #     p.requires_grad = False

    def forward(self, x):
        f = self.features(x)
        out = F.relu(f, inplace=True)
        out = F.avg_pool2d(out, kernel_size=7).view(f.size(0), -1)
        out = self.classifier(out)
        return out
```",code must also gave nan tried use need testing think python class self super self everything except last linear layer list get number last layer plug classifier last layer freeze false forward self return,issue,positive,negative,neutral,neutral,negative,negative
305941960,@sanealytics Thanks! May you also add finetune for densenet?,thanks may also add,issue,negative,positive,positive,positive,positive,positive
305819062,"I have tested on multiple machines and the same case happens. I also tested the disk read and write speed, they are normal.",tested multiple case also tested disk read write speed normal,issue,negative,positive,neutral,neutral,positive,positive
305798313,"for alexnet, it takes close to 1000 mini-batches for loss to start going down.",close loss start going,issue,negative,neutral,neutral,neutral,neutral,neutral
304968207,"> Is it possible to use the existing torch.nn modules and implement RHNs?

Yes.

> Would it make sense to have RHN as a separate module in torch.nn?

No, just like we dont do the same for ResNets.",possible use implement yes would make sense separate module like dont,issue,positive,neutral,neutral,neutral,neutral,neutral
304921813,@saurabhRTR May you test the inception_v3 model finetune? I still can't fintune the inception_v3 model.,may test model still ca model,issue,negative,neutral,neutral,neutral,neutral,neutral
304557698,"When finetuning with inception_v3 model, there is an error:

> python main.py -a inception_v3 -b 16 --lr 0.01 --pretrained data
> => using pre-trained model 'inception_v3'
> Traceback (most recent call last):
> File ""main.py"", line 352, in 
> main()
> File ""main.py"", line 194, in main
> train(train_loader, model, criterion, optimizer, epoch)
> File ""main.py"", line 231, in train
> output = model(input_var)
> File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 206, in call
> result = self.forward(*input, **kwargs)
> File ""/usr/local/lib/python2.7/dist-packages/torch/nn/parallel/data_parallel.py"", line 59, in forward
> return self.module(*inputs[0], **kwargs[0])
> File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 206, in call
> result = self.forward(*input, **kwargs)
> File ""main.py"", line 104, in forward
> f = self.features(x)
> File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 206, in call
> result = self.forward(*input, **kwargs)
> File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/container.py"", line 64, in forward
> input = module(input)
> File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 206, in call
> result = self.forward(*input, **kwargs)
> File ""/usr/local/lib/python2.7/dist-packages/torchvision/models/inception.py"", line 311, in forward
> x = self.fc(x)
> File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 206, in call
> result = self.forward(*input, **kwargs)
> File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/linear.py"", line 54, in forward
> return self._backend.Linear()(input, self.weight, self.bias)
> File ""/usr/local/lib/python2.7/dist-packages/torch/nn/_functions/linear.py"", line 10, in forward
> output.addmm_(0, 1, input, weight.t())
> RuntimeError: size mismatch at /b/wheel/pytorch-src/torch/lib/THC/generic/THCTensorMathBlas.cu:243",model error python data model recent call last file line main file line main train model criterion epoch file line train output model file line call result input file line forward return file line call result input file line forward file line call result input file line forward input module input file line call result input file line forward file line call result input file line forward return input file line forward input size mismatch,issue,negative,positive,neutral,neutral,positive,positive
302937068,"@sanealytics It would be good to have -predict option to make predictions on test datasets. Currently, I see an option for evaluating using validation dataset. Option to predict on test dataset and right output back to CSV file containing test images and probability for each class would be great functionality.",would good option make test currently see option validation option predict test right output back file test probability class would great functionality,issue,positive,positive,positive,positive,positive,positive
302900303,"so has anyone figured out why? I'm experiencing a similar issue, as in #148",anyone figured similar issue,issue,negative,neutral,neutral,neutral,neutral,neutral
301582177,"I had a similar error. I checked my input shape and it was `1x3x112x96`. It threw the `RuntimeError: expected 3D tensor` right before my fully connected layer. When I changed my batch size to 2, input shape was `2x3x112x96` it worked without any errors. 

In my case, the `squeeze` before the fully connected layer was causing the error. When the batch size was 1, the output shape after the squeeze was `[132]` instead of `[1,132]`. Once I corrected that, it worked fine.",similar error checked input shape threw tensor right fully connected layer batch size input shape worked without case squeeze fully connected layer causing error batch size output shape squeeze instead corrected worked fine,issue,negative,positive,positive,positive,positive,positive
299019639,"Hi @fmassa, I have incorporated all your review comments and I have updated the code which now uses the inbuilt `InstanceNorm2d` code. Regarding the `content-weight` and `style-weight` I have commented above.",hi incorporated review code inbuilt code regarding,issue,negative,neutral,neutral,neutral,neutral,neutral
298727643,"@liorshk I didn't do the post-process testing yet. It takes a long time to train the network. Instead, I can find a time to run the MNIST experiment as in the original paper. ",testing yet long time train network instead find time run experiment original paper,issue,negative,positive,positive,positive,positive,positive
298725028,@BestSonny Splitting to train and test is good but actually in the original paper they tested on pairs of images (LFW dataset) by checking the distance between the feature vectors (the output before the last layer). Can you report your results?,splitting train test good actually original paper tested distance feature output last layer report,issue,positive,positive,positive,positive,positive,positive
298722218,"@fmassa  Thanks for the precious comments and I learn a lot from them. I made some modifications according to your feedback. 

@liorshk  I created a split function for splitting the training and the testing. https://github.com/BestSonny/examples/blob/7bcf8e1c83fdfcff1d97a1d2303276ca2d933d4a/center_loss/data.py#L51-#L65",thanks precious learn lot made according feedback split function splitting training testing,issue,positive,positive,positive,positive,positive,positive
298470772,"Thanks for the review @fmassa, I will have to retrain the 4 example models to use the latest `InstanceNorm2d` code. I will update the models and get back to you once I have finished training them.",thanks review retrain example use latest code update get back finished training,issue,negative,positive,positive,positive,positive,positive
298271567,"I tried with only a few batches in one single epoch, (takes ~10 secs) and it can't exit as before. Can you post your settings and the script to run a nvprof.",tried one single epoch ca exit post script run,issue,negative,negative,neutral,neutral,negative,negative
298250128,"Thatâ€™s way too much profile data for nvprof to handle easily; you should run with a couple of iterations, not a couple of epochs.",way much profile data handle easily run couple couple,issue,negative,positive,positive,positive,positive,positive
296972696,@wangg12 The accuracy of `alexnet` is still low. It sounds a good solution i will try it.,accuracy still low good solution try,issue,negative,positive,positive,positive,positive,positive
296971708,"@Foristkirito Do you get better results with alexnet on cifar-10 now?

Also IMO, alexnet is not suitable for cifar10 though. The architecture is for bigger images but 32x32 cifar-10 images.

Besides, if you do not use pre-trained weights, you should be careful with the learning rate and the weights initialization (The random behavior can be fixed by torch.manual_seed(seed)).",get better also suitable though architecture bigger besides use careful learning rate random behavior fixed seed,issue,positive,positive,neutral,neutral,positive,positive
296969292,"@wangg12 the problem is directories. It's necessary to maintain the project directory structure and put it right at your home directory. I do not understand why but it does not work if you put `imagenet` at your home directory. However, `resnet` always works fine i will spend some time to figure our the real problem.",problem necessary maintain project directory structure put right home directory understand work put home directory however always work fine spend time figure real problem,issue,negative,positive,positive,positive,positive,positive
296956427,@wangg12 I figure it out. I made a mistake. Problem solved thank you guy.,figure made mistake problem thank guy,issue,negative,neutral,neutral,neutral,neutral,neutral
296903622,"@wangg12 of course, I just use the [modified code](https://github.com/Foristkirito/examples/blob/master/imagenet/main.py) by me.   
with command `python main.py -a alexnet -j 6 --resume ./alexnet_cp --epochs 90 -b 256 ./data`. I think the problem is that the loss is too large to over flow.  
I also ran with `--pretrained`, loss is ok, but after 90 epochs, performance nearly did not change. shown as below:
```text
 * Prec@1 10.000 Prec@5 50.000
Epoch: [89][0/196]      Time 1.455 (1.455)      Data 1.072 (1.072)      Loss 2.3118 (2.3118)    Prec@1 9.375 (9.375)    Prec@5 49.219 (49.219)
Epoch: [89][10/196]     Time 0.406 (0.507)      Data 0.001 (0.100)      Loss 2.3118 (2.3118)    Prec@1 10.547 (10.085)  Prec@5 51.172 (50.604)
Epoch: [89][20/196]     Time 0.408 (0.460)      Data 0.001 (0.053)      Loss 2.3118 (2.3118)    Prec@1 8.594 (10.212)   Prec@5 51.172 (50.930)
Epoch: [89][30/196]     Time 0.401 (0.444)      Data 0.001 (0.036)      Loss 2.3119 (2.3118)    Prec@1 8.594 (9.929)    Prec@5 44.922 (50.441)
Epoch: [89][40/196]     Time 0.004 (0.436)      Data 0.001 (0.027)      Loss 2.3118 (2.3118)    Prec@1 10.156 (9.861)   Prec@5 55.859 (50.210)
Epoch: [89][50/196]     Time 0.410 (0.431)      Data 0.001 (0.022)      Loss 2.3119 (2.3118)    Prec@1 10.547 (9.934)   Prec@5 49.609 (50.444)
Epoch: [89][60/196]     Time 0.415 (0.428)      Data 0.001 (0.019)      Loss 2.3117 (2.3118)    Prec@1 11.719 (10.028)  Prec@5 55.078 (50.506)
Epoch: [89][70/196]     Time 0.407 (0.426)      Data 0.001 (0.016)      Loss 2.3119 (2.3118)    Prec@1 8.594 (10.030)   Prec@5 49.219 (50.539)
Epoch: [89][80/196]     Time 0.393 (0.428)      Data 0.001 (0.014)      Loss 2.3118 (2.3118)    Prec@1 6.641 (9.968)    Prec@5 50.781 (50.236)
Epoch: [89][90/196]     Time 0.392 (0.426)      Data 0.001 (0.013)      Loss 2.3119 (2.3118)    Prec@1 8.984 (10.045)   Prec@5 49.219 (50.206)
Epoch: [89][100/196]    Time 0.591 (0.425)      Data 0.001 (0.011)      Loss 2.3118 (2.3118)    Prec@1 10.156 (9.998)   Prec@5 50.000 (50.085)
Epoch: [89][110/196]    Time 0.399 (0.423)      Data 0.001 (0.011)      Loss 2.3118 (2.3118)    Prec@1 13.672 (10.015)  Prec@5 51.953 (50.070)
Epoch: [89][120/196]    Time 0.395 (0.422)      Data 0.001 (0.010)      Loss 2.3119 (2.3118)    Prec@1 8.203 (9.985)    Prec@5 48.438 (49.913)
Epoch: [89][130/196]    Time 0.389 (0.422)      Data 0.001 (0.009)      Loss 2.3118 (2.3118)    Prec@1 10.938 (9.951)   Prec@5 50.781 (49.860)
Epoch: [89][140/196]    Time 0.404 (0.421)      Data 0.001 (0.008)      Loss 2.3119 (2.3118)    Prec@1 8.984 (9.912)    Prec@5 50.000 (49.986)
Epoch: [89][150/196]    Time 0.397 (0.421)      Data 0.001 (0.008)      Loss 2.3119 (2.3118)    Prec@1 7.422 (9.910)    Prec@5 49.609 (50.000)
Epoch: [89][160/196]    Time 0.408 (0.419)      Data 0.001 (0.008)      Loss 2.3119 (2.3118)    Prec@1 9.766 (9.899)    Prec@5 47.266 (49.939)
Epoch: [89][170/196]    Time 0.399 (0.419)      Data 0.001 (0.007)      Loss 2.3119 (2.3118)    Prec@1 12.109 (9.875)   Prec@5 48.438 (49.836)
Epoch: [89][180/196]    Time 0.405 (0.419)      Data 0.001 (0.007)      Loss 2.3119 (2.3118)    Prec@1 11.328 (9.874)   Prec@5 48.828 (49.767)
Epoch: [89][190/196]    Time 0.398 (0.419)      Data 0.000 (0.006)      Loss 2.3119 (2.3118)    Prec@1 8.203 (9.835)    Prec@5 49.219 (49.691)
Test: [0/40]    Time 0.845 (0.845)      Loss 2.3119 (2.3119)    Prec@1 8.984 (8.984)    Prec@5 46.875 (46.875)
Test: [10/40]   Time 0.155 (0.264)      Loss 2.3118 (2.3118)    Prec@1 10.547 (10.298)  Prec@5 54.688 (49.503)
Test: [20/40]   Time 0.168 (0.214)      Loss 2.3118 (2.3118)    Prec@1 10.938 (10.305)  Prec@5 53.125 (50.186)
Test: [30/40]   Time 0.338 (0.203)      Loss 2.3117 (2.3118)    Prec@1 9.766 (10.131)   Prec@5 57.812 (50.101)
 * Prec@1 10.000 Prec@5 50.000
```",course use code command python resume think problem loss large flow also ran loss performance nearly change shown text epoch time data loss epoch time data loss epoch time data loss epoch time data loss epoch time data loss epoch time data loss epoch time data loss epoch time data loss epoch time data loss epoch time data loss epoch time data loss epoch time data loss epoch time data loss epoch time data loss epoch time data loss epoch time data loss epoch time data loss epoch time data loss epoch time data loss epoch time data loss test time loss test time loss test time loss test time loss,issue,negative,positive,positive,positive,positive,positive
296895803,@Foristkirito Could you provide a small snippet to reproduce your bug?,could provide small snippet reproduce bug,issue,negative,negative,negative,negative,negative,negative
296455069,"another run from CentOS (previously, all those CentOS runs were done using Maxwell Titan X; this one was done using Pascal).

~~~
Epoch: [0][4960/5005]   Time 0.111 (0.368)      Data 0.001 (0.274)      Loss 6.0431 (6.6413)    Prec@1 2.344 (0.549)       Prec@5 8.203 (2.242)
Epoch: [0][4980/5005]   Time 0.100 (0.367)      Data 0.000 (0.274)      Loss 5.9902 (6.6386)    Prec@1 1.562 (0.556)       Prec@5 5.469 (2.266)
Epoch: [0][5000/5005]   Time 0.100 (0.368)      Data 0.000 (0.275)      Loss 6.1395 (6.6359)    Prec@1 2.734 (0.563)       Prec@5 6.641 (2.289)
Test: [0/196]   Time 7.893 (7.893)      Loss 5.1316 (5.1316)    Prec@1 7.031 (7.031)    Prec@5 30.078 (30.078)
^CTraceback (most recent call last):
  File ""main.py"", line 292, in <module>
Process Process-37:
Process Process-23:
Process Process-24:
Process Process-39:
Process Process-25:
Process Process-36:
Process Process-40:
Process Process-41:
Process Process-42:
Process Process-43:
Process Process-35:
Process Process-44:
Process Process-38:
    main()
  File ""main.py"", line 137, in main
    prec1 = validate(val_loader, model, criterion)
  File ""main.py"", line 210, in validate
    for i, (input, target) in enumerate(val_loader):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 168, in __next__
    idx, batch = self.data_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/queue.py"", line 164, in get
    self.not_empty.wait()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/threading.py"", line 293, in wait
Traceback (most recent call last):
Traceback (most recent call last):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
Traceback (most recent call last):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
Traceback (most recent call last):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
Traceback (most recent call last):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
Traceback (most recent call last):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
KeyboardInterrupt
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
KeyboardInterrupt
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
KeyboardInterrupt
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
Traceback (most recent call last):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
Traceback (most recent call last):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
KeyboardInterrupt
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
KeyboardInterrupt
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
KeyboardInterrupt
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
KeyboardInterrupt
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
KeyboardInterrupt
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
KeyboardInterrupt
Traceback (most recent call last):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
Traceback (most recent call last):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 343, in get
    res = self._reader.recv_bytes()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/connection.py"", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/connection.py"", line 407, in _recv_bytes
    buf = self._recv(4)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/connection.py"", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
    waiter.acquire()
KeyboardInterrupt
~~~",another run previously done maxwell one done epoch time data loss epoch time data loss epoch time data loss test time loss recent call last file line module process process process process process process process process process process process process process main file line main validate model criterion file line validate input target enumerate file line batch file line get file line wait recent call last recent call last file line recent call last file line run file line file line get recent call last file line return file line recent call last file line file line run recent call last file line run file line file line file line get file line get file line return file line file line file line return file line recent call last recent call last recent call last file line run file line run file line run file line file line file line file line get file line get file line recent call last file line get file line return recent call last file line return file line run file line file line file line return file line file line run file line run file line get file line file line file line file line file line get file line return file line get file line return file line return file line run file line run file line file line file line get file line get file line return file line return recent call last file line file line run file line file line get file line return recent call last file line file line run file line file line get file line file line file line chunk read handle,issue,negative,positive,neutral,neutral,positive,positive
296418686,"another run on CentOS.

~~~
Epoch: [0][4980/5005]   Time 0.159 (1.079)      Data 0.000 (0.902)      Loss 5.9452 (6.6232)    Prec@1 4.297 (0.639)    Prec@5 8.984 (2.517)
Epoch: [0][5000/5005]   Time 0.157 (1.076)      Data 0.000 (0.899)      Loss 5.9369 (6.6202)    Prec@1 1.172 (0.646)    Prec@5 8.984 (2.546)
^CTraceback (most recent call last):
  File ""main.py"", line 292, in <module>
Process Process-31:
Process Process-42:
Process Process-33:
Process Process-25:
Process Process-26:
Process Process-44:
Process Process-41:
Process Process-43:
Process Process-39:
Process Process-29:
Process Process-32:
Process Process-27:
Process Process-34:
Process Process-35:
Process Process-30:
Process Process-36:
Process Process-37:
Process Process-40:
Process Process-28:
Process Process-38:
    main()
  File ""main.py"", line 137, in main
    prec1 = validate(val_loader, model, criterion)
  File ""main.py"", line 210, in validate
    for i, (input, target) in enumerate(val_loader):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 168, in __next__
    idx, batch = self.data_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/queue.py"", line 164, in get
    self.not_empty.wait()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/threading.py"", line 293, in wait
Traceback (most recent call last):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
Traceback (most recent call last):
Traceback (most recent call last):
KeyboardInterrupt
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
KeyboardInterrupt
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
Traceback (most recent call last):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
Traceback (most recent call last):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
Traceback (most recent call last):
Traceback (most recent call last):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
Traceback (most recent call last):
Traceback (most recent call last):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
KeyboardInterrupt
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
KeyboardInterrupt
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
KeyboardInterrupt
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
KeyboardInterrupt
KeyboardInterrupt
KeyboardInterrupt
KeyboardInterrupt
KeyboardInterrupt
Traceback (most recent call last):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
KeyboardInterrupt
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
KeyboardInterrupt
KeyboardInterrupt
Traceback (most recent call last):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
Traceback (most recent call last):
Traceback (most recent call last):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
KeyboardInterrupt
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 343, in get
    res = self._reader.recv_bytes()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/connection.py"", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/connection.py"", line 407, in _recv_bytes
    buf = self._recv(4)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/connection.py"", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
    waiter.acquire()
KeyboardInterrupt
~~~",another run epoch time data loss epoch time data loss recent call last file line module process process process process process process process process process process process process process process process process process process process process main file line main validate model criterion file line validate input target enumerate file line batch file line get file line wait recent call last file line file line run file line file line get file line return recent call last recent call last file line file line file line run file line run file line file line file line get file line get file line return file line return recent call last recent call last recent call last file line file line run recent call last file line file line file line file line get recent call last file line run file line run recent call last recent call last file line return recent call last recent call last file line file line file line get file line get file line file line return file line return file line run file line file line file line file line file line file line file line run file line file line get file line return file line run file line run file line run file line file line file line run file line get file line get file line get file line return file line return file line file line file line get file line get file line return file line return file line return recent call last file line file line run file line file line get file line return recent call last recent call last recent call last recent call last file line file line file line run file line file line file line file line run file line run file line get file line run file line return file line file line file line file line get file line get file line return file line get file line return file line return recent call last file line file line run file line recent call last recent call last file line file line run file line file line get file line file line return file line run file line file line get file line file line file line chunk read handle,issue,negative,positive,neutral,neutral,positive,positive
296410424,"another run on Ubuntu.

~~~
Epoch: [0][5000/5005]   Time 0.340 (0.339)      Data 0.000 (0.001)      Loss 5.9935 (6.6413)    Prec@1 1.953 (0.572)    Prec@5 10.156 (2.335)
^C^CProcess Process-44:
Process Process-42:
Process Process-38:
Traceback (most recent call last):
  File ""main.py"", line 289, in <module>
Process Process-41:
Process Process-32:
Traceback (most recent call last):
Process Process-39:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
    main()
  File ""main.py"", line 134, in main
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
Process Process-33:
Process Process-24:
    prec1 = validate(val_loader, model, criterion)
  File ""main.py"", line 207, in validate
Traceback (most recent call last):
Process Process-27:
Process Process-40:
Process Process-28:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
    for i, (input, target) in enumerate(val_loader):
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 168, in __next__
KeyboardInterrupt
    idx, batch = self.data_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/queue.py"", line 164, in get
Process Process-35:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
Process Process-26:
Process Process-25:
KeyboardInterrupt
Process Process-34:
Process Process-29:
    self.not_empty.wait()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/threading.py"", line 293, in wait
Process Process-36:
Traceback (most recent call last):
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
    waiter.acquire()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
Traceback (most recent call last):
KeyboardInterrupt
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
Process Process-43:
Process Process-31:
Traceback (most recent call last):
Process Process-37:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
Traceback (most recent call last):
Process Process-30:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
KeyboardInterrupt
Traceback (most recent call last):
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
KeyboardInterrupt
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
Traceback (most recent call last):
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
Traceback (most recent call last):
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
Traceback (most recent call last):
Traceback (most recent call last):
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
KeyboardInterrupt
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
Traceback (most recent call last):
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
Traceback (most recent call last):
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
Traceback (most recent call last):
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
~~~",another run epoch time data loss process process recent call last file line module process process recent call last process file line file line run file line main file line main file line get process process validate model criterion file line validate recent call last process process process file line file line run file line file line get file line return input target enumerate file line batch file line get process file line return process process process process file line wait process recent call last file line file line run file line file line get recent call last file line return file line file line run file line file line get file line return process process recent call last process file line file line run file line file line get file line return recent call last process file line file line run recent call last file line file line get file line return file line file line run file line file line get file line return recent call last file line file line run file line file line get file line return recent call last file line file line run file line file line get file line return recent call last recent call last file line file line file line run file line file line get file line run file line return file line file line get file line return recent call last file line file line run file line file line get file line return recent call last file line file line run file line file line get file line return recent call last file line file line run file line file line get file line return,issue,negative,positive,neutral,neutral,positive,positive
296408867,"another run on Ubuntu.

~~~
Epoch: [0][4980/5005]   Time 0.340 (0.338)      Data 0.000 (0.002)      Loss 6.0021 (6.6848)    Prec@1 2.734 (0.486)    Prec@5 7.031 (2.024)
Epoch: [0][5000/5005]   Time 0.335 (0.338)      Data 0.000 (0.002)      Loss 5.9103 (6.6820)    Prec@1 2.734 (0.493)    Prec@5 9.375 (2.046)
^B1^CProcess Process-44:
Process Process-43:
Process Process-42:
Process Process-41:
Process Process-39:
Process Process-37:
Process Process-28:
Process Process-34:
Process Process-40:
Process Process-32:
Process Process-35:
Process Process-38:
Process Process-33:
Process Process-31:
Traceback (most recent call last):
Process Process-24:
  File ""main.py"", line 289, in <module>
Process Process-30:
Process Process-36:
    main()
  File ""main.py"", line 134, in main
    prec1 = validate(val_loader, model, criterion)
  File ""main.py"", line 207, in validate
Process Process-25:
    for i, (input, target) in enumerate(val_loader):
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 168, in __next__
Process Process-29:
Process Process-26:
Process Process-27:
    idx, batch = self.data_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/queue.py"", line 164, in get
    self.not_empty.wait()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/threading.py"", line 293, in wait
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
Traceback (most recent call last):
Traceback (most recent call last):
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
KeyboardInterrupt
KeyboardInterrupt
KeyboardInterrupt
KeyboardInterrupt
Traceback (most recent call last):
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
Traceback (most recent call last):
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
    waiter.acquire()
KeyboardInterrupt
Traceback (most recent call last):
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 343, in get
    res = self._reader.recv_bytes()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/connection.py"", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/connection.py"", line 407, in _recv_bytes
    buf = self._recv(4)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/connection.py"", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
~~~",another run epoch time data loss epoch time data loss process process process process process process process process process process process process process recent call last process file line module process process main file line main validate model criterion file line validate process input target enumerate file line process process process batch file line get file line wait recent call last recent call last recent call last recent call last recent call last file line file line file line file line run file line run file line file line run file line file line get file line file line recent call last recent call last file line run file line file line return file line get file line run file line file line get file line return file line get file line file line return file line return file line get file line file line file line return file line run file line run file line file line file line get file line get file line return file line return recent call last file line file line run file line file line get file line return recent call last file line file line run file line file line get file line return recent call last file line file line run file line file line get file line file line file line chunk read handle,issue,negative,positive,neutral,neutral,positive,positive
296406366,"another run on CentOS 6 gave the following.

~~~
Epoch: [0][5000/5005]   Time 0.157 (0.645)      Data 0.000 (0.483)      Loss 5.8995 (6.6278)    Prec@1 3.906 (0.611)    Prec@5 8.984 (2.423)
C^CTraceback (most recent call last):
  File ""main.py"", line 292, in <module>
Process Process-26:
Process Process-31:
Process Process-25:
Process Process-33:
Process Process-28:
Process Process-38:
Process Process-36:
Process Process-30:
Process Process-34:
Process Process-24:
Process Process-37:
Process Process-40:
Process Process-32:
Process Process-35:
Process Process-29:
Process Process-27:
Process Process-39:
    main()
  File ""main.py"", line 137, in main
    prec1 = validate(val_loader, model, criterion)
  File ""main.py"", line 210, in validate
    for i, (input, target) in enumerate(val_loader):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 168, in __next__
    idx, batch = self.data_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/queue.py"", line 164, in get
    self.not_empty.wait()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/threading.py"", line 293, in wait
    waiter.acquire()
KeyboardInterrupt
~~~",another run gave following epoch time data loss recent call last file line module process process process process process process process process process process process process process process process process process main file line main validate model criterion file line validate input target enumerate file line batch file line get file line wait,issue,negative,positive,neutral,neutral,positive,positive
296403796,"similar issues happen on a CentOS 6 machine

~~~

Test: [0/196]   Time 5.107 (5.107)      Loss 5.4296 (5.4296)    Prec@1 5.469 (5.469)    Prec@5 20.703 (20.703)
^CTraceback (most recent call last):
  File ""main.py"", line 292, in <module>
Process Process-35:
Process Process-38:
Process Process-40:
Process Process-33:
Process Process-39:
Process Process-34:
Process Process-21:
Process Process-36:
Process Process-22:
Process Process-27:
Process Process-30:
Process Process-29:
Process Process-31:
Process Process-26:
Process Process-37:
Process Process-28:
Process Process-32:
    main()
  File ""main.py"", line 137, in main
    prec1 = validate(val_loader, model, criterion)
  File ""main.py"", line 210, in validate
    for i, (input, target) in enumerate(val_loader):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 168, in __next__
    idx, batch = self.data_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/queue.py"", line 164, in get
    self.not_empty.wait()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/threading.py"", line 293, in wait
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
Traceback (most recent call last):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
Traceback (most recent call last):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
Traceback (most recent call last):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
Traceback (most recent call last):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
Traceback (most recent call last):
KeyboardInterrupt
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
Traceback (most recent call last):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
Traceback (most recent call last):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
Traceback (most recent call last):
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
KeyboardInterrupt
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
KeyboardInterrupt
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
KeyboardInterrupt
KeyboardInterrupt
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
Traceback (most recent call last):
    waiter.acquire()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
KeyboardInterrupt
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
KeyboardInterrupt
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
KeyboardInterrupt
KeyboardInterrupt
KeyboardInterrupt
~~~",similar happen machine test time loss recent call last file line module process process process process process process process process process process process process process process process process process main file line main validate model criterion file line validate input target enumerate file line batch file line get file line wait recent call last recent call last recent call last file line file line run recent call last file line file line get file line file line file line return file line run recent call last file line file line run file line get recent call last recent call last recent call last recent call last file line file line return file line recent call last file line recent call last file line run file line get file line recent call last file line run file line file line file line file line run file line return file line file line file line file line file line run recent call last file line run file line get file line file line recent call last file line run file line run file line recent call last file line file line run file line get file line get file line return file line run file line file line file line return file line file line get file line return file line return file line get file line get file line get file line file line get file line return file line return file line run file line file line return file line file line file line recent call last file line return file line get file line run file line run file line file line return file line file line get file line get file line return file line return,issue,negative,positive,neutral,neutral,positive,positive
296400046,"@apaszke @soumith  this is the output after I ctrl+c the program (on a Ubuntu 14.04 machine, with Titan Black and 64GB RAM). Is it anyway related to pytorch/pytorch#1120?

```
Epoch: [0][5000/5005]   Time 0.339 (0.340)      Data 0.000 (0.001)      Loss 5.6525 (6.5535)    Prec@1 3.125 (0.760)    Prec@5 12.891 (2.980)
^CProcess Process-40:
Process Process-38:
Process Process-39:
Process Process-35:
Process Process-34:
Process Process-24:
Process Process-26:
Process Process-36:
Process Process-37:
Process Process-33:
Process Process-22:
Traceback (most recent call last):
  File ""main.py"", line 289, in <module>
Process Process-27:
Process Process-29:
Process Process-30:
    main()
  File ""main.py"", line 134, in main
Process Process-25:
Process Process-28:
    prec1 = validate(val_loader, model, criterion)
  File ""main.py"", line 207, in validate
    for i, (input, target) in enumerate(val_loader):
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 168, in __next__
Process Process-32:
Process Process-31:
Process Process-23:
Traceback (most recent call last):
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
Traceback (most recent call last):
Traceback (most recent call last):
KeyboardInterrupt
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
KeyboardInterrupt
    idx, batch = self.data_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/queue.py"", line 164, in get
KeyboardInterrupt
Traceback (most recent call last):
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
    self.not_empty.wait()
KeyboardInterrupt
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/threading.py"", line 293, in wait
Traceback (most recent call last):
Traceback (most recent call last):
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
Traceback (most recent call last):
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
KeyboardInterrupt
KeyboardInterrupt
Traceback (most recent call last):
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
Traceback (most recent call last):
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
KeyboardInterrupt
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
Traceback (most recent call last):
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
Traceback (most recent call last):
Traceback (most recent call last):
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
KeyboardInterrupt
    waiter.acquire()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
KeyboardInterrupt
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
KeyboardInterrupt
Traceback (most recent call last):
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
Traceback (most recent call last):
KeyboardInterrupt
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 343, in get
    res = self._reader.recv_bytes()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/connection.py"", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/connection.py"", line 407, in _recv_bytes
    buf = self._recv(4)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/connection.py"", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Traceback (most recent call last):
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 28, in _worker_loop
    r = index_queue.get()
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/queues.py"", line 342, in get
    with self._rlock:
  File ""/home/yimengzh_everyday/miniconda2/envs/pytorch_openblas/lib/python3.5/multiprocessing/synchronize.py"", line 96, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
```",output program machine black ram anyway related epoch time data loss process process process process process process process process process process recent call last file line module process process process main file line main process process validate model criterion file line validate input target enumerate file line process process process recent call last file line file line run file line file line get file line return recent call last recent call last recent call last recent call last recent call last file line file line file line file line run file line run file line run file line file line file line file line file line file line run file line run file line get file line get file line get file line file line return file line file line return file line return file line get file line get file line return file line return batch file line get recent call last file line file line run file line file line get file line return file line wait recent call last recent call last file line file line file line run file line run recent call last file line file line file line get file line get file line return file line return file line file line run file line file line get file line return recent call last file line file line run recent call last file line file line get file line return file line file line run file line file line get file line return recent call last file line file line run recent call last recent call last file line file line get file line return file line file line file line run file line run file line file line file line get file line get file line return file line return recent call last file line file line run file line file line get file line return recent call last file line file line run file line file line get file line file line file line chunk read handle recent call last file line file line run file line file line get file line return,issue,negative,positive,neutral,neutral,positive,positive
296395437,"@apaszke I tested the same script on another machine with Ubuntu 14.04. This time, the training speed is rock solid. So I think problem is with CentOS 6.",tested script another machine time training speed rock solid think problem,issue,negative,neutral,neutral,neutral,neutral,neutral
296026446,"@apaszke I retried, and seems by checking `free -m`, the reason of slowing down is that too much data got cached in mem (the python programs don't take much memory), and maybe the it takes time for the OS to make room for newly read images. I'm using a CentOS 6 cluster system. Maybe the caching mechanism is poor there?",free reason much data got mem python take much memory maybe time o make room newly read cluster system maybe mechanism poor,issue,negative,positive,positive,positive,positive,positive
296009064,"@apaszke thanks. I think maybe AlexNet is the culprit, as the timing becomes more stable when using VGG.",thanks think maybe culprit timing becomes stable,issue,positive,positive,positive,positive,positive,positive
295909943,"@apaszke BTW, can you have a look at <https://discuss.pytorch.org/t/how-to-prefetch-data-when-processing-with-gpu/548/4>? I think when I'm using Caffe, memory is not that a big issue.",look think memory big issue,issue,negative,neutral,neutral,neutral,neutral,neutral
295892830,"@apaszke  thanks. but seems that the code keeps assuming more and and more memory... is that expected? I mean on ImageNet dataset. When using Caffe with LMDB, it consumes all the memory, but training isn't slowed down.",thanks code assuming memory mean memory training,issue,negative,negative,neutral,neutral,negative,negative
295876542,If that's all of your memory then the system might be swapping and the performance loss is expected. To fix it try with fewer workers or buy more RAM ğŸ˜‰ ,memory system might swapping performance loss fix try buy ram,issue,negative,neutral,neutral,neutral,neutral,neutral
295874825,"It's not expected, but I don't think it's a library bug. It might be a system configuration/hardware issue. Did you try running with fewer/more workers? AlexNet is so fast that it might be hard to keep up with data loading just because of limited disk bandwidth.",think library bug might system issue try running fast might hard keep data loading limited disk,issue,negative,negative,neutral,neutral,negative,negative
295654649,"@taion no, never released anywhere. you can find the latest version of this code here https://gist.github.com/szagoruyko/f1c70fef94daa4cea4bff497dc9c9a7b",never anywhere find latest version code,issue,negative,positive,positive,positive,positive,positive
295544156,"@szagoruyko Was this version of the code released anywhere? The top hit I can see on Google for Wide ResNets in PyTorch is https://github.com/xternalz/WideResNet-pytorch, which looks to be different.",version code anywhere top hit see wide different,issue,negative,positive,positive,positive,positive,positive
295307584,"Thanks for the review @fmassa. I have incorporated the changes you suggested in the code except the 255 comment. I have discussed that issue above.

Thanks",thanks review incorporated code except comment issue thanks,issue,positive,positive,positive,positive,positive,positive
294503550,"@FuriouslyCurious There is a thing to keep in mind when dealing with images in PyTorch: you have to feed into the model Tensor of size `batch_size x num_channels x height x width`. Assume that your image has a correct order of `num_channels x height x width`, use have to do e.g.  `aImage = aImage.unsqueeze(0)` to add an additional dimension for minibatch (of course, with size 1), your tensor now is in the size `1 x num_channels x height x width`, which is good to feed into the model.",thing keep mind dealing feed model tensor size height width assume image correct order height width use add additional dimension course size tensor size height width good feed model,issue,negative,positive,positive,positive,positive,positive
294500984,I usually kill [defunct] process by killing its parent process. This thread might help you: https://askubuntu.com/questions/201303/what-is-a-defunct-process-and-why-doesnt-it-get-killed,usually kill defunct process killing parent process thread might help,issue,negative,negative,negative,negative,negative,negative
294335077,it is because they have very big fully connected layers at the end that are not suitable to DataParallel,big fully connected end suitable,issue,negative,positive,positive,positive,positive,positive
294167936,"My mistake was that I though PyTorch was installed (I did with pip setting config) because it said all requirements were fullfilled, but it was not installed in reality. Anaconda install instead worked and fixed it. Thanks in any case!",mistake though pip setting said reality anaconda install instead worked fixed thanks case,issue,negative,positive,positive,positive,positive,positive
294157666,"For installing pytorch, follow the instructions in http://pytorch.org/
Running `pip install -r requirements.txt` won't install pytorch.",follow running pip install wo install,issue,negative,neutral,neutral,neutral,neutral,neutral
294155568,"Yes I did. And I get the same Error for import torch.
My version of pip: 9.01, 
Python: 2.7.6 (default, Oct 26 2016, 20:30:19) 
[GCC 4.8.4] on linux2
>>> import numpy
>>> numpy.version.version
'1.12.1'
>>> import scipy
>>> scipy.version.version
'0.13.3'
",yes get error import torch version pip python default import import,issue,negative,neutral,neutral,neutral,neutral,neutral
294146962,Did you try running `pip install -r requirements.txt` from the `reinforcement_learning` folder? It will install the requirements (including gym).,try running pip install folder install gym,issue,negative,neutral,neutral,neutral,neutral,neutral
294110228,"Yes, it's the score function estimator a.k.a. vanilla policy gradient a.k.a. REINFORCE. BTW, the forums (discuss.pytorch.org) are the best place for questions; GitHub issues are reserved for bug reports.",yes score function estimator vanilla policy gradient reinforce best place reserved bug,issue,positive,positive,positive,positive,positive,positive
293697963,"But it causes overfitting and therefore doesn't really validate the learning process. I think the example should be trained on some other dataset and tested on LFW.
The other option would be to split LFW to train/test sets.
Either way the number of classes in the last layer should be changed based on the dataset.",therefore really validate learning process think example trained tested option would split either way number class last layer based,issue,negative,positive,neutral,neutral,positive,positive
293680684,"@liorshk Because in the LFW dataset, each class only has several pictures. This demo code only presents a brief description on how to train and validate a classification network with the center loss.
",class several code brief description train validate classification network center loss,issue,negative,negative,neutral,neutral,negative,negative
293591101,"Yes, there's no need anymore and the loop should be removed.",yes need loop removed,issue,negative,neutral,neutral,neutral,neutral,neutral
293420574,"I think it would be cool to have the `PhotoTour` as a triplet `Sampler` instance. I mean something similar to the ones here: [sampler.py](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/sampler.py) You can pass those to a DataLoader. It could for example be called `TripletSampler`. Don't know if it this makes sense.
Anyway, great work :+1: ",think would cool triplet sampler instance mean something similar pas could example know sense anyway great work,issue,positive,positive,positive,positive,positive,positive
293400318,"hey @soumith,  @fmassa I tried using the pre-trained pytorch VGG models for this implementation. I can get good results, but the results look slightly better if I use the Caffe pretrained VGG. The colors are slightly more vivid when using the Caffe VGG as compared to pytorch VGG. Currently I am sticking to using pytorch VGG because the results are almost as good as the Caffe VGG and the code comes out cleaner with pytorch VGG.

Below is a comparison of results:

**pytorch VGG**
![chicago-test](https://cloud.githubusercontent.com/assets/3365035/25006473/02b75666-202b-11e7-800b-b08620c22511.jpg)


**Caffe VGG**
![chicago-benchmark](https://cloud.githubusercontent.com/assets/3365035/24961531/12a9408a-1f67-11e7-92d6-fe3629a6e960.jpg)

**content image**
![chicago](https://cloud.githubusercontent.com/assets/3365035/24961715/b3caf7d8-1f67-11e7-8180-45223b44a7d9.jpg)

**style image**
![mosaic](https://cloud.githubusercontent.com/assets/3365035/24961737/c19a7758-1f67-11e7-8fdb-0fa259882bd2.jpg)


",hey tried implementation get good look slightly better use color slightly vivid currently sticking almost good code come cleaner comparison content image style image mosaic,issue,positive,positive,positive,positive,positive,positive
293361466,"@jekbradbury Great explanation, thanks a lot. By the way, [This line](https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py#L66) centralize the rewards, just to confirm if I understand it right that it is additional to the original algorithm ?

By the way, I have left `running_reward` unchanged, i.e. `10`. ",great explanation thanks lot way line centralize confirm understand right additional original algorithm way left unchanged,issue,positive,positive,positive,positive,positive,positive
293355738,"Slide 17 describes policy gradient with a scalar baseline that's updated with least-squares; `actor_critic.py` uses a value function baseline with an infinite time horizon, as described on slides 20-22. The terminology is definitely confusing -- many people would say policy gradient with a value function baseline is not ""true"" actor-critic, but I believe it's exactly the algorithm referred to as ""advantage actor-critic"" in the A3C paper...",slide policy gradient scalar value function infinite time horizon terminology definitely many people would say policy gradient value function true believe exactly algorithm advantage paper,issue,positive,positive,positive,positive,positive,positive
293349648,"@jekbradbury Thanks for the info ! I am a bit confused if `actor_critic.py` actually implements the Vanilla Policy Gradient algorithm, e.g. in [John Schulman's lecture note ](http://rll.berkeley.edu/deeprlcourse/docs/lec2.pdf) slide 17 ?",thanks bit confused actually vanilla policy gradient algorithm lecture note slide,issue,negative,negative,neutral,neutral,negative,negative
293348963,`actor_critic.py` uses an advantage estimate (subtracts the output of a value network) to reduce variance. And I believe `running_reward` should not be initialized to zero but rather to approximately the typical early value of the reward.,advantage estimate output value network reduce variance believe zero rather approximately typical early value reward,issue,positive,negative,neutral,neutral,negative,negative
293348621,"@soumith  By the way, I am quite curious to see an additional example of `Vanilla Policy Gradient`, with advantage estimate (subtract baseline to reduce variance). ",way quite curious see additional example vanilla policy gradient advantage estimate subtract reduce variance,issue,positive,negative,neutral,neutral,negative,negative
293347637,"@soumith Default value of `random seed` was not consistent with helper message, `saved_actions` are not used and `running_reward` should be initialized to be 0, if I got it correctly. ",default value random seed consistent helper message used got correctly,issue,positive,negative,negative,negative,negative,negative
293307147,"@FuriouslyCurious, would you mind posting the output of `print(InputImg.shape)`?
I believe you want to feed the network a batch of one image â‡’ so your input `Variable` is 4D, and dimension zero is `1`, so that indexing with `[0]` will give you the expected 3D `Tensor`.",would mind posting output print believe want feed network batch one image input variable dimension zero indexing give tensor,issue,negative,neutral,neutral,neutral,neutral,neutral
292977637,"you are correct. in v0.1.9 we introduced allocating gradients lazily, and hence you see this.",correct lazily hence see,issue,negative,negative,negative,negative,negative,negative
292870076,"Guess you mean ""Data Parallelization"".

Like Line 72 examples/imagenet/main.py, please explicitly use:

    model = torch.nn.DataParallel(model).cuda()

Once you use DataParallel(model) as model, you can run your command as 

    CUDA_VISIBLE_DEVICES=4,5,6,7 python main.py [options]

to use last 4 GPU out of total 8 cards.",guess mean data parallelization like line please explicitly use model model use model model run command python use last total,issue,positive,negative,negative,negative,negative,negative
291731299,"i'm just saying that batch statistics are of better quality, so we should rather use those.
I'm not sure it leads to convergence problems though.",saying batch statistic better quality rather use sure convergence though,issue,positive,positive,positive,positive,positive,positive
291037250,"Ah, good catch, I should have included this sanity check upon adding tied weights. LGTM.",ah good catch included sanity check upon tied,issue,negative,positive,positive,positive,positive,positive
290918910,A TF WIP Mask R-CNN effort was started at https://github.com/CharlesShang/FastMaskRCNN. Actually there is still no public reference implementation of the paper so we will see what kind of accurancy can be reproduced.,mask effort actually still public reference implementation paper see kind,issue,positive,positive,positive,positive,positive,positive
290917639,"Missed detach when implementing dcgan in pytorch, and it gives me this error:

```
RuntimeError: Trying to backward through the graph second time, but the buffers have already been freed. Please specify retain_variables=True when calling backward for the first time.
```",detach error trying backward graph second time already freed please specify calling backward first time,issue,negative,positive,positive,positive,positive,positive
290192637,"But my lr appears to go down to zero, and stay there after epoch 14 (for my particular settings):
```
| epoch  14 |  2000/ 2323 batches | lr 1.00 | ms/batch 132.84 | loss  3.93 | ppl    51.05
| epoch  14 |  2200/ 2323 batches | lr 1.00 | ms/batch 130.91 | loss  3.87 | ppl    47.75
-----------------------------------------------------------------------------------------
| end of epoch  14 | time: 326.76s | valid loss  4.84 | valid ppl   126.41
-----------------------------------------------------------------------------------------
epoch  15 |   200/ 2323 batches | lr 0.00 | ms/batch 133.49 | loss  4.12 | ppl    61.81
| epoch  15 |   400/ 2323 batches | lr 0.00 | ms/batch 131.63 | loss  4.27 | ppl    71.50
| epoch  15 |   600/ 2323 batches | lr 0.00 | ms/batch 132.10 | loss  4.15 | ppl    63.44
| epoch  15 |   800/ 2323 batches | lr 0.00 | ms/batch 132.22 | loss  4.12 | ppl    61.75
-----------------------------------------------------------------------------------------
| end of epoch  15 | time: 315.69s | valid loss  4.84 | valid ppl   126.41
-----------------------------------------------------------------------------------------
| epoch  16 |   200/ 2323 batches | lr 0.00 | ms/batch 131.21 | loss  4.12 | ppl    61.81
| epoch  16 |   400/ 2323 batches | lr 0.00 | ms/batch 134.99 | loss  4.27 | ppl    71.50
```",go zero stay epoch particular epoch loss epoch loss end epoch time valid loss valid epoch loss epoch loss epoch loss epoch loss end epoch time valid loss valid epoch loss epoch loss,issue,negative,positive,positive,positive,positive,positive
290191605,args.lr is of type float. float / int in python is float right?,type float float python float right,issue,negative,positive,positive,positive,positive,positive
290191190,":) You are too quick for me to even complete writing up my issue:
https://github.com/pytorch/examples/blob/master/word_language_model/main.py#L177",quick even complete writing issue,issue,negative,positive,positive,positive,positive,positive
289772254,ah yes of course. thanks for fixing it.,ah yes course thanks fixing,issue,positive,positive,positive,positive,positive,positive
289632132,"i'm going to track and investigate this here: https://github.com/pytorch/pytorch/issues/1120
Thanks for the detailed repro.

Please dont double-post issues.",going track investigate thanks detailed please dont,issue,positive,positive,positive,positive,positive,positive
289487000,"@apaszke 

Hi, I also implemented a Fast Neural Style in pytorch. Most of the implementation is similar to the implementation present in this pr. I made a few changes like replacing batch-normalization with instance-normalization which gives better results.

repo: https://github.com/darkstar112358/fast-neural-style

I can work on adding fast-neural-style to pytorch/examples if you need it.",hi also fast neural style implementation similar implementation present made like better work need,issue,positive,positive,positive,positive,positive,positive
289312572,"Very helpful script. Although I noticed one problem for resnets. 

It works fine for resnet18 and 34 because their last conv layers are 512 in depth. However, for resnet50, 101, and 152 this should be increased to 2048. The offending line is:
`nn.Linear(512, num_classes)`",helpful script although one problem work fine last depth however line,issue,negative,positive,positive,positive,positive,positive
289310679,"Next time you need help, please use: https://discuss.pytorch.org/",next time need help please use,issue,positive,neutral,neutral,neutral,neutral,neutral
289201597,@KaimingHe In the plan of releasing Mask r-cnn there will be also a faster-rcnn pytorch implementation merged in this repository?,plan mask also implementation repository,issue,negative,neutral,neutral,neutral,neutral,neutral
288892834,"@bhack I haven't, and I didn't have the time to finish this properly.
Given that there are already a number of pytorch implementations of object detection algorithms in pytorch, I'll close this one for the time being.
If I find some time to finish this up with a simple interface, I'll send a new PR.",time finish properly given already number object detection close one time find time finish simple interface send new,issue,negative,positive,neutral,neutral,positive,positive
288886992,"Yes, you can use fill_from_vocab=True. (It will make the training a little slower, though, because the optimizer has to use sparse tensors to make use of the fact that most of the word vectors don't need to be updated at each optimization step; I'm not actually sure if it does or doesn't yet. @ebetica?)",yes use make training little though use sparse make use fact word need optimization step actually sure yet,issue,positive,positive,positive,positive,positive,positive
288874881,"I found the error. During training, the inputs vocabulary was built using the train,dev,test set but during testing, I was only building it using the train set. This is a good way of doing it to evaluate the model since we are building the vocabulary with words we know we will face.

But instead of just storing the word vectors that appear in my train/dev/test set, what if I wanted to build up a vocabulary with all the words that are in the glove.42b.300d.txt? Although that will take up more memory, I think that will be a good option to have since it will be useful to have for deploying a product that may contain many words that don't appear in the train/dev/test set but there are embeddings for them in the glove..txt file. Is there anyway of doing that now in TorchText?",found error training vocabulary built train dev test set testing building train set good way evaluate model since building vocabulary know face instead word appear set build vocabulary although take memory think good option since useful product may contain many appear set glove file anyway,issue,positive,positive,positive,positive,positive,positive
288832119,"Instead of saving just the vector, I think we need to save the entire Vocab object when we are training. Still looking into it.",instead saving vector think need save entire object training still looking,issue,negative,neutral,neutral,neutral,neutral,neutral
288787693,"If you do answers.build_vocab(train), the answer vocab mapping should be the same as it was during training. Iâ€™ll try to figure out whatâ€™s wrong today (and also weâ€™ll soon have the ability to serialize torchtext Fields, which should make your use case easier)",train answer training try figure wrong today also soon ability serialize make use case easier,issue,negative,negative,negative,negative,negative,negative
288781061,"I think I found the issue:

```python
$ python -i test.py
## Building vocab on TEST data
>>> inputs.build_vocab(test)
>>> inputs.vocab.vectors = torch.load(args.vector_cache)
>>> len(inputs.vocab)
381
>>> answers.build_vocab(test)
>>> len(answers.vocab)
4
>>> vars(answers.vocab)
{'freqs': Counter({'neutral': 35, 'entailment': 33, 'contradiction': 32}), 'unk_init': 'random', 'stoi': defaultdict(<function Vocab.__init__.<locals>.<lambda> at 0x7f1b886bbb70>, {'neutral': 1, 'entailment': 2, 'contradiction': 3}), 'itos': ['<unk>', 'neutral', 'entailment', 'contradiction']}


## Building vocab on TRAIN data
>>> inputs.build_vocab(train)
>>> inputs.vocab.vectors = torch.load(args.vector_cache)
>>> len(inputs.vocab)
56220
>>> answers.build_vocab(train)
>>> len(answers.vocab)
4
>>> vars(answers.vocab)
{'freqs': Counter({'entailment': 183416, 'contradiction': 183187, 'neutral': 182764}), 'unk_init': 'random', 'stoi': defaultdict(<function Vocab.__init__.<locals>.<lambda> at 0x7f1b9971ea60>, {'entailment': 1, 'contradiction': 2, 'neutral': 3}), 'itos': ['<unk>', 'entailment', 'contradiction', 'neutral']}
```

The problem is the vocabulary mapping of the answers labels. It's different for TEST and TRAIN  data. 
TEST answers vocab mapping -     {'neutral': 1, 'entailment': 2, 'contradiction': 3}
TRAIN answers vocab mapping -   {'entailment': 1, 'contradiction': 2, 'neutral': 3}

How do I get the labels from the test data but ensure the mapping that the model used during training? Is there any nice way of doing that in TorchText? ",think found issue python python building test data test test counter function lambda building train data train train counter function lambda problem vocabulary different test train data test train get test data ensure model used training nice way,issue,negative,positive,positive,positive,positive,positive
288706369,"I added this:
```python
train = data.TabularDataset( path=[path/to/training/data], format='json', fields={'sentence1': ('premise', inputs),
                                                                    'sentence2': ('hypothesis', inputs),
                                                                    'gold_label': ('label', answers)},
                                                            filter_pred=lambda ex: ex.label != '-' )


inputs.build_vocab(train)
inputs.vocab.vectors = torch.load([path/to/vector/cache])
answers.build_vocab(train)
```

These are the results:
```bash
$ python -i test.py
train accuracy - 48.000000
>>> len(answers.vocab)
4
>>> len(inputs.vocab)
56220
```

The test data provided here is a sample of 100 instances from the training data and the model gets much better on the training set so sth must be wrong. I am directly using the vector cache and the model from the resume snapshot that was saved during training. Thoughts?",added python train ex train train bash python train accuracy test data provided sample training data model much better training set must wrong directly vector cache model resume snapshot saved training,issue,negative,positive,neutral,neutral,positive,positive
288614571,You have to use the same vocabulary you used during training. The easiest way to do that is to load the train set in your script and call inputs.build_vocab(train),use vocabulary used training easiest way load train set script call train,issue,negative,neutral,neutral,neutral,neutral,neutral
288603101,"Well, it may not be the case. Since what we want are better generative results on `eval` mode rather than based on batch statistics, we should also treat the same for the input of D, right? Technically, I didn't see a problem there. But did you mean it may lead to some converge problems?",well may case since want better generative mode rather based batch statistic also treat input right technically see problem mean may lead converge,issue,positive,positive,positive,positive,positive,positive
288593550,"your change in line 257 seems fine.
But your changes in other places seem inappropriate because the learning is very stochastic, and it is better to rely on batch statistics than running_mean / running_std in those places..",change line fine seem inappropriate learning stochastic better rely batch statistic,issue,positive,positive,positive,positive,positive,positive
288446893,"@soumith Yes. Maybe it won't influence the final result much. But it could corrupt BatchNorm statistics technically. And as a tutorial sample for new users to learn, it is good to remind them of using `train` and `eval` mode. ",yes maybe wo influence final result much could corrupt statistic technically tutorial sample new learn good remind train mode,issue,positive,positive,neutral,neutral,positive,positive
288444802,"this is not really necessary. are you doing this in an attempt to save computation, or to get BatchNorm statistics right?",really necessary attempt save computation get statistic right,issue,positive,positive,positive,positive,positive,positive
288324608,I seem to have used the earlier version of this file which fixed the above error.,seem used version file fixed error,issue,negative,positive,neutral,neutral,positive,positive
288316176,@soumith I have created another pull request: https://github.com/pytorch/examples/pull/118 . Please check it. Thanks.,another pull request please check thanks,issue,positive,positive,positive,positive,positive,positive
288302656,"actually, sorry i didn't notice. Yes, your example will be helpful. can you open a PR without the OpenNMT modification?
Thanks.",actually sorry notice yes example helpful open without modification thanks,issue,negative,negative,neutral,neutral,negative,negative
288293514,"Dear @soumith, I'm sorry for modified a small bug in OpenNMT while submit it with a new example. This is a new example to show how to do time sequence prediction with pytorch and it have no relation with OpenNMT. Can I submit this new example again without the modification of OpenNMT?",dear sorry small bug submit new example new example show time sequence prediction relation submit new example without modification,issue,negative,negative,neutral,neutral,negative,negative
288138968,"OpenNMT has been moved to https://github.com/OpenNMT/OpenNMT-py and will be continuing development there by the OpenNMT team and other community members.

Please send pull requests and issues over there.

Thank you.",development team community please send pull thank,issue,positive,neutral,neutral,neutral,neutral,neutral
288138900,"this was fixed.
Also

OpenNMT has been moved to https://github.com/OpenNMT/OpenNMT-py and will be continuing development there by the OpenNMT team and other community members.

Please send pull requests and issues over there.

Thank you.",fixed also development team community please send pull thank,issue,positive,positive,neutral,neutral,positive,positive
288138817,"this was fixed.

OpenNMT has been moved to https://github.com/OpenNMT/OpenNMT-py and will be continuing development there by the OpenNMT team and other community members.

Please send pull requests and issues over there.

Thank you.",fixed development team community please send pull thank,issue,positive,positive,neutral,neutral,positive,positive
287914786,"you are correct. it is done for speed, not correctness. The computation of gradients wrt the weights of netG can be fully avoided in the backward pass if the graph is detached where it is.",correct done speed correctness computation fully backward pas graph detached,issue,negative,neutral,neutral,neutral,neutral,neutral
287794945,"I've just ran the VAE example with both v0.1.10 and against master. Both of them seem to run without error both on CUDA and on CPU.

What is your pytorch version?",ran example master seem run without error version,issue,negative,neutral,neutral,neutral,neutral,neutral
287631083,"@magsol If you happen to run into difficulties training the 512x512 images, you could always scale down the images first to say 64^2 and see if it would even get you the results you'd want, then later scale up? 
Thanks for checking back! :)",happen run training could always scale first say see would even get want later scale thanks back,issue,negative,positive,positive,positive,positive,positive
287578659,"Ha! Isn't all of practical ML just tricks :) Haven't had a chance
yet--behind on my teaching. Hoping to get back to this within the next week
though, and will absolutely update when I do.
On Tue, Mar 14, 2017 at 04:40 Lukas Mosser <notifications@github.com> wrote:

> Any luck with the above tricks heuristics?
>
> â€”
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/pytorch/examples/issues/70#issuecomment-286369535>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AAIQ-V7JYksn7-m-IfnFvafXVdcqfQtSks5rlmCYgaJpZM4MDwky>
> .
>
-- 
iPhone'd
",ha practical chance yet behind teaching get back within next week though absolutely update tue mar mosser wrote luck reply directly view mute thread,issue,positive,negative,neutral,neutral,negative,negative
287383079,"Ok thanks! I thought `Scale` was for converting from [0, 255] to [0, 1]. But I see that that happens in `transforms.ToTensor()`.",thanks thought scale converting see,issue,negative,positive,positive,positive,positive,positive
287353953,@cbasavaraj The `Scale` transform resizes the image such that it's smallest size is 256.,scale transform image size,issue,negative,neutral,neutral,neutral,neutral,neutral
287346155,"Thanks, but maybe my question was not clear. Both are normalised, but train is not scaled. ",thanks maybe question clear train scaled,issue,positive,positive,positive,positive,positive,positive
286591562,"there, i've merged it. if you have new fixes, do send them in a  new pr.
Thanks a lot for your work Bryan!",new send new thanks lot work,issue,negative,positive,positive,positive,positive,positive
286215139,"Indeed, I should have cited both, thanks for fixing. LGTM, suggest merge.",indeed thanks fixing suggest merge,issue,negative,positive,positive,positive,positive,positive
286122064,i'll let @Smerity take a call here. I'm not familiar with relevant literature. Thanks.,let take call familiar relevant literature thanks,issue,negative,positive,positive,positive,positive,positive
285589277,"1. I don't know how to modify it, is there any way to block the gradient flow at a sentence beginning?
2. I recommend you to read the paper ""Efficient GPU-based Training of Recurrent Neural Network Language Models Using Spliced Sentence Bunch"".",know modify way block gradient flow sentence beginning recommend read paper efficient training recurrent neural network language sentence bunch,issue,negative,neutral,neutral,neutral,neutral,neutral
285553674,"I think this is good to go. It would probably be a good idea if at least one person looked over the final draft. Data parallelism for the generator always seemed at least as good as model parallelism, so I left out the model parallel option for now. If anyone is curious about how I went about it, let me know.",think good go would probably good idea least one person final draft data parallelism generator always least good model parallelism left model parallel option anyone curious went let know,issue,positive,positive,positive,positive,positive,positive
285454315,"1. Yes, that's true, but if you think it's unfair to compare that with other methods, you can modify it. It's meant to be an example and I think it makes perfect sense to keep the hidden state from the previous sentence, as it will likely increase the quality of generated text.

2. I don't understand the suggestion, it seems that you've just increased the batch twice, so it's not exactly equivalent. Of course larger batches lead to increased parallelism.

Please use issues for problem reports and feature requests. If you want to discuss anything or have a question, post on [our forums](discuss.pytorch.org).",yes true think unfair compare modify meant example think perfect sense keep hidden state previous sentence likely increase quality text understand suggestion batch twice exactly equivalent course lead parallelism please use problem feature want discus anything question post,issue,positive,positive,positive,positive,positive,positive
285255615,"Ran a bunch of experiments today testing out the model parallelism for the generator. It seems helpful only in the high batch size, large softmax, 4 or more GPU range. I'll post some details tomorrow and make sure that users can choose between data parallelism and model parallelism for the generator. Seems like we'll need real multiprocessing to make multi-GPU worth it for smaller datasets.

This change will require changing the saving procedure to using state_dicts, which is really something that should happen anyways, so I'll make that and the series of off-by-one errors I found in the code today my last updates. I think I'll have to do that tomorrow though. One more day should do it!",ran bunch today testing model parallelism generator helpful high batch size large range post tomorrow make sure choose data parallelism model parallelism generator like need real make worth smaller change require saving procedure really something happen anyways make series found code today last think tomorrow though one day,issue,positive,positive,positive,positive,positive,positive
284950763,"Implemented a first attempt at model parallelism for the generator, but it too is not speeding things up. I'm going to take another look tomorrow.",first attempt model parallelism generator speeding going take another look tomorrow,issue,negative,positive,positive,positive,positive,positive
284432802,"Python 3 can handle that, but python 2 cannot. I'll change it to be
compatible with both. Thanks for catching that!
On Mon, Mar 6, 2017 at 4:05 AM zeng <notifications@github.com> wrote:

> [image: image]
> <https://cloud.githubusercontent.com/assets/15373554/23609254/16ed6f94-02a8-11e7-8e84-f3d87ed5dd6c.png>
> what's wrong with the Bottle?
>
> â€”
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/pytorch/examples/issues/87#issuecomment-284378633>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ADNEpTyYP-ZL90Mx1-B3EccBbme5kMkrks5ri_ZtgaJpZM4MMhAc>
> .
>
",python handle python change compatible thanks catching mon mar wrote image image wrong bottle reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
284419188,while this is nice. This is the only example that does explicit optimization rather than use `torch.optim`. I'm inclined to leave it as an illustration.,nice example explicit optimization rather use leave illustration,issue,positive,positive,positive,positive,positive,positive
284241170,"As soon as you remove the [WIP] tag, i'll merge this in.",soon remove tag merge,issue,negative,neutral,neutral,neutral,neutral,neutral
284149863,"Yes, the learning is unstable. There are some new interesting suggestion in the dcgan.torch thread: https://github.com/soumith/dcgan.torch/issues/2#issuecomment-283982237

- Set ndf to ngf/4, this changes the size of the  G and D models in order to balance the training
- Add white noise (this is a trick also mentioned here: [ganhacks](https://github.com/soumith/ganhacks))",yes learning unstable new interesting suggestion thread set size order balance training add white noise trick also,issue,negative,positive,positive,positive,positive,positive
284082777,"Secondary confirmation.

P100, Driver Version: 375.26, cuDNN: 5, CUDA: 8.0

```
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 18.46s | valid loss  4.89 | valid ppl   133.20
-----------------------------------------------------------------------------------------
=========================================================================================
| End of training | test loss  4.86 | test ppl   128.70
=========================================================================================
```

Turning off gradient clipping:
```
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 20.51s | valid loss  4.77 | valid ppl   118.37
-----------------------------------------------------------------------------------------
=========================================================================================
| End of training | test loss  4.74 | test ppl   114.18
=========================================================================================
```

The issue appears caused by the `clip_gradient` function - though I've not yet found the exact source of the issue in that function. Replacing it with the recently added `clip_grad_norm` [(source)](https://github.com/pytorch/pytorch/blob/c238ee368165b5cfd0ff59a5d6479cf6393c719b/torch/nn/utils/clip_grad.py) brings us back towards the reported perplexity of 113 while using gradient clipping.

```
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 16.87s | valid loss  4.78 | valid ppl   119.09
-----------------------------------------------------------------------------------------
=========================================================================================
| End of training | test loss  4.75 | test ppl   115.49
=========================================================================================
```

With a different random seed (42), the perplexity is more in line with the reported 113, so the difference is likely superficial.

```
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 17.03s | valid loss  4.77 | valid ppl   118.16
-----------------------------------------------------------------------------------------
=========================================================================================
| End of training | test loss  4.74 | test ppl   114.46
=========================================================================================
```",secondary confirmation driver version end epoch time valid loss valid end training test loss test turning gradient clipping end epoch time valid loss valid end training test loss test issue function though yet found exact source issue function recently added source u back towards perplexity gradient clipping end epoch time valid loss valid end training test loss test different random seed perplexity line difference likely superficial end epoch time valid loss valid end training test loss test,issue,negative,negative,neutral,neutral,negative,negative
284054627,"After talking with @ngimel, thought I'd summarize some of what we are seeing:

Replacing LSTMCell with nn.LSTM and unrolling manually is indeed slower. This is due to overhead for weight copying. cuDNN is currently only faster when this overhead is amortized over many time steps.

Just to check, I tried replacing the cuDNN encoder with LSTMCell/StackedLSTM that the Decoder uses, and that was also slower, so it looks like the cuDNN encoder/LSTMCell Decoder combination is the sweet spot for the time being.

Running with the cuDNN decoder also caused OOM errors with the same settings that the non-cuDNN decoder could handle. @csarofeen investigated further and has reproduced a simple case here https://github.com/pytorch/pytorch/issues/914.",talking thought summarize seeing unrolling manually indeed due overhead weight currently faster overhead many time check tried also like combination sweet spot time running also could handle simple case,issue,positive,positive,positive,positive,positive,positive
283810466,"i think Sam's suggested is really sufficient. I dont see why we need to make simple code more complicated:
```
unbuffer python main.py
python -u main.py
```
",think sam really sufficient dont see need make simple code complicated python python,issue,negative,negative,neutral,neutral,negative,negative
283798697,"No improvement, though I guess it's a little easier to see that it's not *pure* noise in the fake images. Still looks like static, though.

![fake_samples_epoch_024](https://cloud.githubusercontent.com/assets/135417/23529302/1998032a-ff6b-11e6-9a8a-5c44c073d990.png)
",improvement though guess little easier see pure noise fake still like static though,issue,positive,positive,neutral,neutral,positive,positive
283793146,"I'm going to revert this change. You can get the buffering behavior with either of the following:

```
unbuffer python main.py
python -u main.py
```

https://linux.die.net/man/1/unbuffer
https://docs.python.org/2/using/cmdline.html#cmdoption-u",going revert change get behavior either following python python,issue,negative,neutral,neutral,neutral,neutral,neutral
283786958,"Managed to override the default image loader in torchvision so it properly pulls the images in with grayscale format and changed the `nc = 1`--seems to be running nicely now :) Though the loss functions are still quickly hitting 1 and 0 respectively as before, so I'm not sure that the results of this will be any better than the last one.",override default image loader properly format running nicely though loss still quickly respectively sure better last one,issue,positive,positive,positive,positive,positive,positive
283786944,"This fails on Python 2.7:

```Traceback (most recent call last):
  File ""main.py"", line 290, in <module>
    main()
  File ""main.py"", line 66, in main
    print(""=> creating model '{}'"".format(args.arch), flush=True)
TypeError: 'flush' is an invalid keyword argument for this function```",python recent call last file line module main file line main print model invalid argument function,issue,negative,positive,neutral,neutral,positive,positive
283772625,"@bartolsthoorn I ran dcgan with the following arguments:

`python main.py --cuda --dataset folder --dataroot /images --outf /output`

I tried changing the `nc = 3` value to `nc = 1` since the images are all grayscale, but kept getting `CUDNN_STATUS_BAD_PARAM` errors, so I left the default value unchanged.

Unfortunately after very few training iterations, it looks like the mode collapsed:

<img width=""885"" alt=""screen shot 2017-03-02 at 3 09 59 pm"" src=""https://cloud.githubusercontent.com/assets/135417/23525827/c7b3a5a8-ff5d-11e6-9c2c-3ea8ef4a9e8e.png"">

The images from the 24th epoch look like pure static:

![fake_samples_epoch_024](https://cloud.githubusercontent.com/assets/135417/23525884/05d97b5a-ff5e-11e6-89aa-9bcac87dff6e.png)

The real images, on the other hand, look like this:

![real_samples](https://cloud.githubusercontent.com/assets/135417/23525893/0d5d2e80-ff5e-11e6-865a-6f9d32771ede.png)

Happy to hear any suggestions you may have :) Thank you so much for your help so far! Learning a lot about GANs!",ran following python folder tried value since kept getting left default value unchanged unfortunately training like mode screen shot th epoch look like pure static real hand look like happy hear may thank much help far learning lot,issue,positive,positive,positive,positive,positive,positive
283563020,"@ngimel I've replaced the decoder's list of LSTMCells with a single multi-layer nn.LSTM. In your suggestions, you mentioned replacing each individual LSTMCell with an nn.LSTM, but shouldn't this be even faster? Instead, it appears to be 10% slower, and it runs out of memory. 

I'll slack you the profiler output in case you'd like to take a look.",list single individual even faster instead memory slack profiler output case like take look,issue,negative,negative,neutral,neutral,negative,negative
283177495,I can look at that! The snli example probably needs a bigger update by now. Do you have any other suggestions?,look example probably need bigger update,issue,negative,neutral,neutral,neutral,neutral,neutral
283173841,"Yep, I forgot to change the README.md. https://github.com/pytorch/examples/pull/82 is going to fix it and the one a few lines down for translate.py. ",yep forgot change going fix one,issue,negative,neutral,neutral,neutral,neutral,neutral
283050173,"Thank you for your answer, which is very inspiring to me
I will use this idea in my later experiments
Thanks again.",thank answer inspiring use idea later thanks,issue,positive,positive,positive,positive,positive,positive
283039618,"no, pooling operator performance is fine. this is an explicit choice.",operator performance fine explicit choice,issue,negative,positive,positive,positive,positive,positive
282784262,"@magsol I would suggest to first try your dataset on the standard 64x64. Next you run it on 128x128, with either the extra convolution or pooling layer as listed above. After that you can try 512x512, I am no expert but I have not seen pictures that large generated by a DCGAN. You could also consider generating 128x128 images and then use a separate super-resolution network to reach 512x512.

64x64 and 128x128 are easy to try (the model includes the preprocessing, i.e. the rescaling of the images) and should be easier to generate. Did you already get good results with your data on the 64x64 scale? Please share your experience so far. :smile: ",would suggest first try standard next run either extra convolution layer listed try expert seen large could also consider generating use separate network reach easy try model easier generate already get good data scale please share experience far smile,issue,positive,positive,positive,positive,positive,positive
282779264,"Ahh, thank you for the extra information; that helps immensely, in addition to the intuition for possibly less stable training processes given the larger images.

So @bartolsthoorn for the images I'm using--512x512--I probably should look into the improved GAN paper and associated OpenAI implementation?",thank extra information immensely addition intuition possibly le stable training given probably look gan paper associated implementation,issue,positive,neutral,neutral,neutral,neutral,neutral
282763446,"As @apaszke mentions, the G and D networks are generated with the 64x64 limitations hardcoded. The implementation of the DCGAN here is very similar to the dcgan.torch implementation, and someone else asked about this limitation and got this answer: https://github.com/soumith/dcgan.torch/issues/2#issuecomment-164862299

By following the changes suggested in that comment, you can expand the network for 128x128. So for the generator:
```python
class _netG(nn.Module):
    def __init__(self, ngpu):
        super(_netG, self).__init__()
        self.ngpu = ngpu
        self.main = nn.Sequential(
            # input is Z, going into a convolution
            nn.ConvTranspose2d(     nz, ngf * 16, 4, 1, 0, bias=False),
            nn.BatchNorm2d(ngf * 16),
            nn.ReLU(True),
            # state size. (ngf*16) x 4 x 4
            nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 8),
            nn.ReLU(True),
            # state size. (ngf*8) x 8 x 8
            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 4),
            nn.ReLU(True),
            # state size. (ngf*4) x 16 x 16 
            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 2),
            nn.ReLU(True),
            # state size. (ngf*2) x 32 x 32
            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf),
            nn.ReLU(True),
            # state size. (ngf) x 64 x 64
            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),
            nn.Tanh()
            # state size. (nc) x 128 x 128
        )
```
And for the discriminator:
```python
class _netD(nn.Module):
    def __init__(self, ngpu):
        super(_netD, self).__init__()
        self.ngpu = ngpu
        self.main = nn.Sequential(
            # input is (nc) x 128 x 128
            nn.Conv2d(nc, ndf, 4, stride=2, padding=1, bias=False), 
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (ndf) x 64 x 64
            nn.Conv2d(ndf, ndf * 2, 4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(ndf * 2),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (ndf*2) x 32 x 32
            nn.Conv2d(ndf * 2, ndf * 4, 4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(ndf * 4),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (ndf*4) x 16 x 16 
            nn.Conv2d(ndf * 4, ndf * 8, 4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(ndf * 8),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (ndf*8) x 8 x 8
            nn.Conv2d(ndf * 8, ndf * 16, 4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(ndf * 16),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (ndf*16) x 4 x 4
            nn.Conv2d(ndf * 16, 1, 4, stride=1, padding=0, bias=False),
            nn.Sigmoid()
            # state size. 1
        )
```
However, as you can also see in that thread, it is harder to get a stable game between the generator and discriminator for this larger problem. To avoid this, I think you'll have to take a look at the improvements used here https://github.com/openai/improved-gan (paper: https://arxiv.org/abs/1606.03498). This repository includes a model for 128x128 imagenet generation.",implementation similar implementation someone else limitation got answer following comment expand network generator python class self super self input going convolution true state size true state size true state size true state size true state size state size discriminator python class self super self input state size state size state size state size state size state however also see thread harder get stable game generator discriminator problem avoid think take look used paper repository model generation,issue,positive,positive,positive,positive,positive,positive
282521889,"oh, i kinda didn't do it in hindsight, I should do it.
However that will only save computation. 
D is still frozen because I dont call optimizerD.step() (which is when D's parameters actually get updated)",oh hindsight however save computation still frozen dont call actually get,issue,negative,neutral,neutral,neutral,neutral,neutral
282255023,"I think that an additional improvement would be to rewrite [`memoryEfficientLoss`](https://github.com/pytorch/examples/blob/master/OpenNMT/train.py#L118), so that is first replicates the generator onto all used GPUs (say `n`), and then uses these modules with parallel_apply to process the `n` batches from the splits on each GPU using `parallel_apply`. This could save a bit on replication, because that part is heavily CPU bound. Right now replicate will be called for each batch.",think additional improvement would rewrite first generator onto used say process could save bit replication part heavily bound right replicate batch,issue,positive,positive,positive,positive,positive,positive
282185974,"Also, replacing LSTMCell with a single-step single-layer LSTM (that would get routed to cudnn) might help, especially for small hidden sizes. On the attached profile the small kernels after 2 gemms are pointwise operations that would get fused by cudnn (and reduce load on the CPU). That would help single GPU performance as well. Some massaging of inputs and outputs will be required (LSTMCell expects 2D input, LSTM 3D etc). 
![selection_009](https://cloud.githubusercontent.com/assets/15841449/23287999/af84f210-f9f5-11e6-80cd-5e0acf936f04.png)
",also would get might help especially small hidden size attached profile small pointwise would get fused reduce load would help single performance well input,issue,positive,negative,negative,negative,negative,negative
282182922,"I see. Now that you mention larger batches, etc. I've rerun with a batch size of 512 (everything else left as default), and I'm seeing the multi-gpu code get a bit of a speedup, so for now this looks to be useful only for larger model/batch combinations.

Single GPU:

```
Epoch  1,    10/   19 batches; perplexity: 563542.92; 3993 tokens/s;     33 s elapsed
Train perplexity: 287798
Validation perplexity: 2.91026e+08

Epoch  2,    10/   19 batches; perplexity: 874205.98; 3957 tokens/s;     91 s elapsed
Train perplexity: 504300
Validation perplexity: 57021

Epoch  3,    10/   19 batches; perplexity: 78948.53; 3867 tokens/s;    149 s elapsed
Train perplexity: 58810.5
Validation perplexity: 13508.7

Epoch  4,    10/   19 batches; perplexity: 79398.57; 3973 tokens/s;    208 s elapsed
Train perplexity: 58670.9
Validation perplexity: 12442.1

Epoch  5,    10/   19 batches; perplexity: 16059.98; 4030 tokens/s;    265 s elapsed
Train perplexity: 10363
Validation perplexity: 15048.1
Decaying learning rate to 0.5
```

Two GPUs:

```
Epoch  1,    10/   19 batches; perplexity: 921818.14; 4610 tokens/s;     29 s elapsed
Train perplexity: 692491
Validation perplexity: 50474.8

Epoch  2,    10/   19 batches; perplexity: 741175.54; 5485 tokens/s;     71 s elapsed
Train perplexity: 217865
Validation perplexity: 42705.7

Epoch  3,    10/   19 batches; perplexity: 49493.03; 5504 tokens/s;    115 s elapsed
Train perplexity: 32258.1
Validation perplexity: 68482.7
Decaying learning rate to 0.5

Epoch  4,    10/   19 batches; perplexity: 6975.58; 5166 tokens/s;    157 s elapsed
Train perplexity: 5871.81
Validation perplexity: 6592.61
Decaying learning rate to 0.25

Epoch  5,    10/   19 batches; perplexity: 3146.31; 5716 tokens/s;    200 s elapsed
Train perplexity: 3232.02
Validation perplexity: 2681.03
Decaying learning rate to 0.125
```",see mention rerun batch size everything else left default seeing code get bit useful single epoch perplexity train perplexity validation perplexity epoch perplexity train perplexity validation perplexity epoch perplexity train perplexity validation perplexity epoch perplexity train perplexity validation perplexity epoch perplexity train perplexity validation perplexity learning rate two epoch perplexity train perplexity validation perplexity epoch perplexity train perplexity validation perplexity epoch perplexity train perplexity validation perplexity learning rate epoch perplexity train perplexity validation perplexity learning rate epoch perplexity train perplexity validation perplexity learning rate,issue,negative,positive,neutral,neutral,positive,positive
282176540,"DataParallel at this point does not use multiple threads for backwards, and the code is very CPU bound (as @adamlerer pointed out, kernels are very small). Even threaded forward part remains CPU-bound.  Parameter exchange takes some time, but the biggest problem I see is small kernels. May be running with bigger rnn_size/word_vec_size will have better scaling. ",point use multiple backwards code bound pointed small even threaded forward part remains parameter exchange time biggest problem see small may running bigger better scaling,issue,negative,neutral,neutral,neutral,neutral,neutral
282174605,"Happy to leave this as WIP until we make multi-GPU faster. I think we'll still want to take some of the smaller changes from this branch that fix bugs that are in master without it though. https://github.com/bmccann/examples/tree/onmt_small_fixes will make all the changes we've talked about except batch-first end-to-end. If we decide to roll back the merge, I'll move the other bug fixes in this branch to that one so they can merge sooner.",happy leave make faster think still want take smaller branch fix master without though make except decide roll back merge move bug branch one merge sooner,issue,positive,positive,positive,positive,positive,positive
282163178,"Interesting. Well, if multi-GPU is slower than single-GPU, there's no point in merging this PR until we've figured it out. Have you tried running with the Lua Torch version to see if you get the same behavior?

Some possibilities:
- RNNs are pretty parameter-heavy, so for small batch size maybe you spend all your time communicating parameters
- you're bottlenecked by Python executing the kernels on multiple GPUs since they're so small. I think DataParallel uses multiple threads, but the python parts hold on to the GIL.

cc @colesbury ",interesting well point figured tried running torch version see get behavior pretty small batch size maybe spend time communicating python multiple since small think multiple python hold,issue,positive,positive,neutral,neutral,positive,positive
282157342,It's not used as a function. `print('asdf')` works both in Python 2 and 3. In 2 it just evaluates as a statement + an expression in parenthesis. We can merge it if you import the function.,used function print work python statement expression parenthesis merge import function,issue,negative,neutral,neutral,neutral,neutral,neutral
282157085,"print as a function is already used in master. This PR only adds an argument to the function. To make it compatible, we can have an additional PR to import print_function from `__future__`.",print function already used master argument function make compatible additional import,issue,negative,neutral,neutral,neutral,neutral,neutral
282157037,"@apaszke should we add `from __future__ import print_function`, or your prefer not to rely on that?",add import prefer rely,issue,negative,neutral,neutral,neutral,neutral,neutral
282156262,"This will break in Python 2, as `print` is not a function there ğŸ˜• ",break python print function,issue,negative,neutral,neutral,neutral,neutral,neutral
282141775,"@adamlerer 

With 2 GPUs, each iteration takes longer.

Two GPUs: 

```
python train.py -data data/demo-train.pt -gpus 1 2

Namespace(batch_size=64, brnn=False, brnn_merge='concat', cuda=2, curriculum=False, data='data/demo-train.pt', dropout=0.3, epochs=13, gpus=[1, 2], input_feed=1, layers=2, learning_rate=1.0, learning_rate_decay=0.5, log_interval=50, max_generator_batches=32, max_grad_norm=5, optim='sgd', param_init=0.1, pre_word_vecs_dec=None, pre_word_vecs_enc=None, rnn_size=500, save_model='model', start_decay_at=8, start_epoch=1, train_from=None, word_vec_size=500)
Loading data from 'data/demo-train.pt'
 * vocabulary size. source = 24999; target = 35820
 * number of training sentences. 10000
 * maximum batch size. 64
Building model...
* number of parameters: 58121320
DataParallel (
  (module): NMTModel (
    (encoder): Encoder (
      (word_lut): Embedding(24999, 500, padding_idx=0)
      (rnn): LSTM(500, 500, num_layers=2, dropout=0.3)
    )
    (decoder): Decoder (
      (word_lut): Embedding(35820, 500, padding_idx=0)
      (rnn): StackedLSTM (
        (dropout): Dropout (p = 0.3)
        (layer_0): LSTMCell(1000, 500)
        (layer_1): LSTMCell(500, 500)
      )
      (attn): GlobalAttention (
        (linear_in): Linear (500 -> 500)
        (sm): Softmax ()
        (linear_out): Linear (1000 -> 500)
        (tanh): Tanh ()
      )
      (dropout): Dropout (p = 0.3)
    )
    (generator): DataParallel (
      (module): Sequential (
        (0): Linear (500 -> 35820)
        (1): LogSoftmax ()
      )
    )
  )
  (generator): DataParallel (
    (module): Sequential (
      (0): Linear (500 -> 35820)
      (1): LogSoftmax ()
    )
  )
)

Epoch  1,    50/  156 batches; perplexity: 99443.77; 2330 tokens/s;     31 s elapsed
Epoch  1,   100/  156 batches; perplexity: 8743.52; 2506 tokens/s;     59 s elapsed
Epoch  1,   150/  156 batches; perplexity: 4907.59; 2610 tokens/s;     88 s elapsed
Train perplexity: 15371.7
Validation perplexity: 5809.43

Epoch  2,    50/  156 batches; perplexity: 2800.85; 2472 tokens/s;    129 s elapsed
Epoch  2,   100/  156 batches; perplexity: 2388.12; 2534 tokens/s;    158 s elapsed
Epoch  2,   150/  156 batches; perplexity: 1901.31; 2541 tokens/s;    186 s elapsed
Train perplexity: 2335.83
Validation perplexity: 1914.02

Epoch  3,    50/  156 batches; perplexity: 1649.94; 2582 tokens/s;    227 s elapsed
Epoch  3,   100/  156 batches; perplexity: 1572.27; 2448 tokens/s;    256 s elapsed
Epoch  3,   150/  156 batches; perplexity: 1505.26; 2554 tokens/s;    284 s elapsed
Train perplexity: 1572.15
Validation perplexity: 1667.94

Epoch  4,    50/  156 batches; perplexity: 1271.02; 2473 tokens/s;    326 s elapsed
Epoch  4,   100/  156 batches; perplexity: 1146.34; 2594 tokens/s;    355 s elapsed
Epoch  4,   150/  156 batches; perplexity: 1108.89; 2457 tokens/s;    383 s elapsed
Train perplexity: 1174.73
Validation perplexity: 1533.18

Epoch  5,    50/  156 batches; perplexity: 957.82; 2437 tokens/s;    423 s elapsed
Epoch  5,   100/  156 batches; perplexity: 913.88; 2571 tokens/s;    452 s elapsed
Epoch  5,   150/  156 batches; perplexity: 900.18; 2509 tokens/s;    481 s elapsed
Train perplexity: 920.71
Validation perplexity: 1240.65

Epoch  6,    50/  156 batches; perplexity: 744.90; 2371 tokens/s;    522 s elapsed
Epoch  6,   100/  156 batches; perplexity: 759.57; 2599 tokens/s;    551 s elapsed
Epoch  6,   150/  156 batches; perplexity: 699.02; 2568 tokens/s;    580 s elapsed
Train perplexity: 731.5
Validation perplexity: 1199.55

Epoch  7,    50/  156 batches; perplexity: 633.01; 2521 tokens/s;    623 s elapsed
Epoch  7,   100/  156 batches; perplexity: 597.26; 2441 tokens/s;    651 s elapsed
Epoch  7,   150/  156 batches; perplexity: 589.11; 2554 tokens/s;    678 s elapsed
Train perplexity: 606.887
Validation perplexity: 1018.89

Epoch  8,    50/  156 batches; perplexity: 526.82; 2580 tokens/s;    721 s elapsed
Epoch  8,   100/  156 batches; perplexity: 509.78; 2551 tokens/s;    751 s elapsed
Epoch  8,   150/  156 batches; perplexity: 479.35; 2469 tokens/s;    777 s elapsed
Train perplexity: 505.752
Validation perplexity: 998.23
Decaying learning rate to 0.5

Epoch  9,    50/  156 batches; perplexity: 364.82; 2491 tokens/s;    818 s elapsed
Epoch  9,   100/  156 batches; perplexity: 362.30; 2465 tokens/s;    847 s elapsed
Epoch  9,   150/  156 batches; perplexity: 372.00; 2555 tokens/s;    875 s elapsed
Train perplexity: 369.812
Validation perplexity: 925.486
Decaying learning rate to 0.25

Epoch 10,    50/  156 batches; perplexity: 308.08; 2526 tokens/s;    916 s elapsed
Epoch 10,   100/  156 batches; perplexity: 317.01; 2501 tokens/s;    946 s elapsed
Epoch 10,   150/  156 batches; perplexity: 293.38; 2506 tokens/s;    974 s elapsed
Train perplexity: 306.679
Validation perplexity: 937.862
Decaying learning rate to 0.125

Epoch 11,    50/  156 batches; perplexity: 258.26; 2411 tokens/s;   1014 s elapsed
Epoch 11,   100/  156 batches; perplexity: 274.06; 2442 tokens/s;   1043 s elapsed
Epoch 11,   150/  156 batches; perplexity: 292.96; 2661 tokens/s;   1072 s elapsed
Train perplexity: 275.771
Validation perplexity: 949.411
Decaying learning rate to 0.0625

Epoch 12,    50/  156 batches; perplexity: 259.58; 2490 tokens/s;   1113 s elapsed
Epoch 12,   100/  156 batches; perplexity: 271.06; 2560 tokens/s;   1142 s elapsed
Epoch 12,   150/  156 batches; perplexity: 251.97; 2485 tokens/s;   1170 s elapsed
Train perplexity: 260.238
Validation perplexity: 991.659
Decaying learning rate to 0.03125

Epoch 13,    50/  156 batches; perplexity: 252.53; 2556 tokens/s;   1212 s elapsed
Epoch 13,   100/  156 batches; perplexity: 249.92; 2534 tokens/s;   1240 s elapsed
Epoch 13,   150/  156 batches; perplexity: 251.81; 2482 tokens/s;   1268 s elapsed
Train perplexity: 252.564
Validation perplexity: 992.94
Decaying learning rate to 0.015625

```

Single-GPU:

```
python train.py -data data/demo-train.pt -gpus 0

Namespace(batch_size=64, brnn=False, brnn_merge='concat', cuda=1, curriculum=False, data='data/demo-train.pt', dropout=0.3, epochs=13, gpus=[0], input_feed=1, layers=2, learning_rate=1.0, learning_rate_decay=0.5, log_interval=50, max_generator_batches=32, max_grad_norm=5, optim='sgd', param_init=0.1, pre_word_vecs_dec=None, pre_word_vecs_enc=None, rnn_size=500, save_model='model', start_decay_at=8, start_epoch=1, train_from=None, word_vec_size=500)
Loading data from 'data/demo-train.pt'
 * vocabulary size. source = 24999; target = 35820
 * number of training sentences. 10000
 * maximum batch size. 64
Building model...
* number of parameters: 58121320
NMTModel (
  (encoder): Encoder (
    (word_lut): Embedding(24999, 500, padding_idx=0)
    (rnn): LSTM(500, 500, num_layers=2, dropout=0.3)
  )
  (decoder): Decoder (
    (word_lut): Embedding(35820, 500, padding_idx=0)
    (rnn): StackedLSTM (
      (dropout): Dropout (p = 0.3)
      (layer_0): LSTMCell(1000, 500)
      (layer_1): LSTMCell(500, 500)
    )
    (attn): GlobalAttention (
      (linear_in): Linear (500 -> 500)
      (sm): Softmax ()
      (linear_out): Linear (1000 -> 500)
      (tanh): Tanh ()
    )
    (dropout): Dropout (p = 0.3)
  )
  (generator): Sequential (
    (0): Linear (500 -> 35820)
    (1): LogSoftmax ()
  )
)

Epoch  1,    50/  156 batches; perplexity: 169375.25; 2719 tokens/s;     27 s elapsed
Epoch  1,   100/  156 batches; perplexity: 28906.83; 2715 tokens/s;     52 s elapsed
Epoch  1,   150/  156 batches; perplexity: 6529.42; 2678 tokens/s;     81 s elapsed
Train perplexity: 29324.4
Validation perplexity: 4475.37

Epoch  2,    50/  156 batches; perplexity: 3370.68; 2676 tokens/s;    115 s elapsed
Epoch  2,   100/  156 batches; perplexity: 2475.44; 2683 tokens/s;    143 s elapsed
Epoch  2,   150/  156 batches; perplexity: 2033.16; 2641 tokens/s;    171 s elapsed
Train perplexity: 2535.36
Validation perplexity: 2359.5

Epoch  3,    50/  156 batches; perplexity: 1736.63; 2562 tokens/s;    206 s elapsed
Epoch  3,   100/  156 batches; perplexity: 1571.85; 2665 tokens/s;    233 s elapsed
Epoch  3,   150/  156 batches; perplexity: 1509.00; 2670 tokens/s;    261 s elapsed
Train perplexity: 1596.73
Validation perplexity: 2986.71
Decaying learning rate to 0.5

Epoch  4,    50/  156 batches; perplexity: 1039.26; 2798 tokens/s;    297 s elapsed
Epoch  4,   100/  156 batches; perplexity: 944.99; 2734 tokens/s;    324 s elapsed
Epoch  4,   150/  156 batches; perplexity: 899.23; 2490 tokens/s;    351 s elapsed
Train perplexity: 958.626
Validation perplexity: 1149.34
Decaying learning rate to 0.25

Epoch  5,    50/  156 batches; perplexity: 742.53; 2636 tokens/s;    387 s elapsed
Epoch  5,   100/  156 batches; perplexity: 740.75; 2692 tokens/s;    414 s elapsed
Epoch  5,   150/  156 batches; perplexity: 710.58; 2634 tokens/s;    442 s elapsed
Train perplexity: 729.089
Validation perplexity: 1069.12
Decaying learning rate to 0.125

Epoch  6,    50/  156 batches; perplexity: 644.06; 2604 tokens/s;    477 s elapsed
Epoch  6,   100/  156 batches; perplexity: 643.12; 2740 tokens/s;    504 s elapsed
Epoch  6,   150/  156 batches; perplexity: 615.39; 2592 tokens/s;    532 s elapsed
Train perplexity: 634.366
Validation perplexity: 987.2
Decaying learning rate to 0.0625

Epoch  7,    50/  156 batches; perplexity: 593.35; 2695 tokens/s;    570 s elapsed
Epoch  7,   100/  156 batches; perplexity: 588.14; 2720 tokens/s;    597 s elapsed
Epoch  7,   150/  156 batches; perplexity: 579.01; 2497 tokens/s;    623 s elapsed
Train perplexity: 586.226
Validation perplexity: 973.5
Decaying learning rate to 0.03125

Epoch  8,    50/  156 batches; perplexity: 571.16; 2561 tokens/s;    661 s elapsed
Epoch  8,   100/  156 batches; perplexity: 581.59; 2772 tokens/s;    689 s elapsed
Epoch  8,   150/  156 batches; perplexity: 537.52; 2625 tokens/s;    715 s elapsed
Train perplexity: 563.508
Validation perplexity: 963.563
Decaying learning rate to 0.015625

Epoch  9,    50/  156 batches; perplexity: 554.87; 2569 tokens/s;    752 s elapsed
Epoch  9,   100/  156 batches; perplexity: 551.34; 2536 tokens/s;    779 s elapsed
Epoch  9,   150/  156 batches; perplexity: 549.24; 2738 tokens/s;    806 s elapsed
Train perplexity: 552.116
Validation perplexity: 958.603
Decaying learning rate to 0.0078125

Epoch 10,    50/  156 batches; perplexity: 549.09; 2621 tokens/s;    843 s elapsed
Epoch 10,   100/  156 batches; perplexity: 537.11; 2669 tokens/s;    869 s elapsed
Epoch 10,   150/  156 batches; perplexity: 555.24; 2702 tokens/s;    897 s elapsed
Train perplexity: 546.821
Validation perplexity: 957.038
Decaying learning rate to 0.00390625

Epoch 11,    50/  156 batches; perplexity: 542.42; 2625 tokens/s;    934 s elapsed
Epoch 11,   100/  156 batches; perplexity: 545.51; 2672 tokens/s;    961 s elapsed
Epoch 11,   150/  156 batches; perplexity: 539.52; 2588 tokens/s;    987 s elapsed
Train perplexity: 544.143
Validation perplexity: 955.48
Decaying learning rate to 0.00195312

Epoch 12,    50/  156 batches; perplexity: 551.75; 2643 tokens/s;   1024 s elapsed
Epoch 12,   100/  156 batches; perplexity: 528.27; 2656 tokens/s;   1051 s elapsed
Epoch 12,   150/  156 batches; perplexity: 547.05; 2620 tokens/s;   1078 s elapsed
Train perplexity: 542.386
Validation perplexity: 954.578
Decaying learning rate to 0.000976562

Epoch 13,    50/  156 batches; perplexity: 551.38; 2505 tokens/s;   1115 s elapsed
Epoch 13,   100/  156 batches; perplexity: 553.35; 2663 tokens/s;   1143 s elapsed
Epoch 13,   150/  156 batches; perplexity: 525.45; 2646 tokens/s;   1171 s elapsed
Train perplexity: 542.348
Validation perplexity: 954.621
Decaying learning rate to 0.000488281
```

",iteration longer two python loading data vocabulary size source target number training maximum batch size building model number module dropout dropout linear linear tanh tanh dropout dropout generator module sequential linear generator module sequential linear epoch perplexity epoch perplexity epoch perplexity train perplexity validation perplexity epoch perplexity epoch perplexity epoch perplexity train perplexity validation perplexity epoch perplexity epoch perplexity epoch perplexity train perplexity validation perplexity epoch perplexity epoch perplexity epoch perplexity train perplexity validation perplexity epoch perplexity epoch perplexity epoch perplexity train perplexity validation perplexity epoch perplexity epoch perplexity epoch perplexity train perplexity validation perplexity epoch perplexity epoch perplexity epoch perplexity train perplexity validation perplexity epoch perplexity epoch perplexity epoch perplexity train perplexity validation perplexity learning rate epoch perplexity epoch perplexity epoch perplexity train perplexity validation perplexity learning rate epoch perplexity epoch perplexity epoch perplexity train perplexity validation perplexity learning rate epoch perplexity epoch perplexity epoch perplexity train perplexity validation perplexity learning rate epoch perplexity epoch perplexity epoch perplexity train perplexity validation perplexity learning rate epoch perplexity epoch perplexity epoch perplexity train perplexity validation perplexity learning rate python loading data vocabulary size source target number training maximum batch size building model number dropout dropout linear linear tanh tanh dropout dropout generator sequential linear epoch perplexity epoch perplexity epoch perplexity train perplexity validation perplexity epoch perplexity epoch perplexity epoch perplexity train perplexity validation perplexity epoch perplexity epoch perplexity epoch perplexity train perplexity validation perplexity learning rate epoch perplexity epoch perplexity epoch perplexity train perplexity validation perplexity learning rate epoch perplexity epoch perplexity epoch perplexity train perplexity validation perplexity learning rate epoch perplexity epoch perplexity epoch perplexity train perplexity validation perplexity learning rate epoch perplexity epoch perplexity epoch perplexity train perplexity validation perplexity learning rate epoch perplexity epoch perplexity epoch perplexity train perplexity validation perplexity learning rate epoch perplexity epoch perplexity epoch perplexity train perplexity validation perplexity learning rate epoch perplexity epoch perplexity epoch perplexity train perplexity validation perplexity learning rate epoch perplexity epoch perplexity epoch perplexity train perplexity validation perplexity learning rate epoch perplexity epoch perplexity epoch perplexity train perplexity validation perplexity learning rate epoch perplexity epoch perplexity epoch perplexity train perplexity validation perplexity learning rate,issue,negative,neutral,neutral,neutral,neutral,neutral
282132120,"Fantastic! I am in pytorch slack, but we also have an opennmt gitter.
https://gitter.im/OpenNMT/openmt , maybe we could use that?



On Feb 23, 2017 4:36 PM, ""Bryan McCann"" <notifications@github.com> wrote:

> @pltrdy
> <https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_pltrdy&d=CwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=wnHFZ7D4m-9MRwk-CWlvCGbWEiQX_AvUO2LuMy4Vj7c&m=7T0aGKBlruW28W5xuTe2EzgGmeaa-9KDgCyUE-wy34Y&s=NaaxFvZ3Tqoy2OwaJTWFl2mHZOjVu6PSvIh9KysJNck&e=>
> @srush
> <https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_srush&d=CwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=wnHFZ7D4m-9MRwk-CWlvCGbWEiQX_AvUO2LuMy4Vj7c&m=7T0aGKBlruW28W5xuTe2EzgGmeaa-9KDgCyUE-wy34Y&s=aDn_fzKMn281nAlrJxOXMfbZ-R9Ml8GGmX2A4Jd-s-U&e=>
> PyOpenNMT -- nice! I just put up a multi-gpu PR that got merged and has
> some other fixes if you're going to run translate.py. If I develop further,
> would you mind if I branched off of PyOpenNMT from now on so that I can
> coordinate with you more easily? Also, are you in the PyTorch slack?
>
> â€”
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_pytorch_examples_issues_75-23issuecomment-2D282128909&d=CwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=wnHFZ7D4m-9MRwk-CWlvCGbWEiQX_AvUO2LuMy4Vj7c&m=7T0aGKBlruW28W5xuTe2EzgGmeaa-9KDgCyUE-wy34Y&s=xPx7WnpSndhRhukKpluJvsdW-BFxe86mnnYBzzi4d6w&e=>,
> or mute the thread
> <https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_AACMKrYJxIBUF61nnnrZM4JaVa7eOR3Yks5rffvsgaJpZM4MJyiI&d=CwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=wnHFZ7D4m-9MRwk-CWlvCGbWEiQX_AvUO2LuMy4Vj7c&m=7T0aGKBlruW28W5xuTe2EzgGmeaa-9KDgCyUE-wy34Y&s=CF4ZwoaL4j43MTv-AwUGfvcRjUaWd6_qBWqAQG2xrqU&e=>
> .
>
",fantastic slack also maybe could use wrote nice put got going run develop would mind branched easily also slack reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
282128909,"@pltrdy @srush PyOpenNMT -- nice! I just put up a multi-gpu PR that got merged and has some other fixes if you're going to run translate.py. If I develop further, would you mind if I branched off of PyOpenNMT from now on so that I can coordinate with you more easily? Also, are you in the PyTorch slack?",nice put got going run develop would mind branched easily also slack,issue,positive,positive,positive,positive,positive,positive
282123915,"@ngimel 

I'm using` python train.py -data data/demo-train.pt -gpus 0` for single gpu
and` python train.py -data data/demo-train.pt -gpus 0 1` for multi-gpu

This reminds me that I should have changed the README since I replaced -cuda with -gpus device_list. @soumith, would you prefer to make that change or wait for it in a future PR?",python single python since would prefer make change wait future,issue,negative,negative,neutral,neutral,negative,negative
282120462,"i'm sorry, i merged in https://github.com/pytorch/examples/pull/80 by @bmccann just now. Can you rebase / is the fix still relevant?",sorry rebase fix still relevant,issue,negative,negative,neutral,neutral,negative,negative
281641005,"@apaszke could you revise again this?
Before merging I'd to in integrate PairwiseDistance and both Triplet losses into the main repo.
I guess here? https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/distance.py

Also for the Phototour dataset I'll send a PR to
https://github.com/pytorch/vision/tree/master/torchvision/datasets

However, by the moment I assume that the user has opencv installed since there's a big gap in terms of speed compared to PIL. How do you want to handle this?",could revise integrate triplet main guess also send however moment assume user since big gap speed want handle,issue,negative,positive,neutral,neutral,positive,positive
281218606,"Wow, I didn't know that. That looks much better. Thank you for letting me know.",wow know much better thank know,issue,positive,positive,positive,positive,positive,positive
281217537,"for this basic example, i dont want to add this option in to avoid making this a good practice (it's not).

Instead, you can control the GPU used via the environment variable `CUDA_VISIBLE_DEVICES`.

For example:

`CUDA_VISIBLE_DEVICES=2 python main.py`",basic example dont want add option avoid making good practice instead control used via environment variable example python,issue,negative,positive,positive,positive,positive,positive
280837685,"there are different random seeds per device (for CPU, and for each GPU). This is by design.",different random per device design,issue,negative,negative,negative,negative,negative,negative
280809130,"I apologize if this isnt the place to post this, but has anyone tried training this model on a large dataset? I tried training it on the europarl v7 en->de dataset. The preprocessing works fine but when I start training I get:


> Namespace(batch_size=64, brnn=False, brnn_merge='concat', cuda=True, curriculum=False, >data='data/ep7-s-train.pt', dropout=0.3, epochs=13, input_feed=1, layers=2, learning_rate=1, > >learning_rate_decay=0.5, log_interval=50, max_generator_batches=32, max_grad_norm=5, >optim='sgd', param_init=0.1, pre_word_vecs_dec=None, pre_word_vecs_enc=None, rnn_size=500, >save_model='model', start_decay_at=8, start_epoch=1, train_from=None, word_vec_size=500)
>Loading data from 'data/ep7-s-train.pt'
> * vocabulary size. source = 50004; target = 50002
> * number of training sentences. 20
> * maximum batch size. 64
> Building model...
> * number of parameters: 84820002
NMTModel (
  (encoder): Encoder (
    (word_lut): Embedding(50004, 500, padding_idx=0)
    (rnn): LSTM(500, 500, num_layers=2, dropout=0.3)
  )
  (decoder): Decoder (
    (word_lut): Embedding(50002, 500, padding_idx=0)
    (rnn): StackedLSTM (
      (dropout): Dropout (p = 0.3)
      (layer_0): LSTMCell(1000, 500)
      (layer_1): LSTMCell(500, 500)
    )
    (attn): GlobalAttention (
      (linear_in): Linear (500 -> 500)
      (sm): Softmax ()
      (linear_out): Linear (1000 -> 500)
      (tanh): Tanh ()
    )
    (dropout): Dropout (p = 0.3)
  )
  (generator): Sequential (
    (0): Linear (500 -> 50002)
    (1): LogSoftmax ()
  )
)

>Traceback (most recent call last):
  File ""train.py"", line 280, in <module>
    main()
  File ""train.py"", line 276, in main
    trainModel(model, trainData, validData, dataset, optim)
  File ""train.py"", line 204, in trainModel
    train_loss = trainEpoch(epoch)
  File ""train.py"", line 163, in trainEpoch
    batchOrder = torch.randperm(len(trainData))
RuntimeError: must be strictly positive at /data/users/soumith/miniconda2/conda-bld/pytorch-cuda80-0.1.8_1486041592240/work/torch/lib/TH/generic/THTensorMath.c:1473
",apologize place post anyone tried training model large tried training de work fine start training get loading data vocabulary size source target number training maximum batch size building model number dropout dropout linear linear tanh tanh dropout dropout generator sequential linear recent call last file line module main file line main model file line epoch file line must strictly positive,issue,positive,positive,positive,positive,positive,positive
280777327,"@bmccann no modifications to the code, haven't tried to compute BLEU scores yet. state_dict is the preferred way of checkpointing, it allows you to make changes to model definition and keep the parameters. In case of optim, it also prevents from the state breaking when core PyTorch changes.
@adamlerer I think @apaszke means the text data?",code tried compute yet preferred way make model definition keep case also state breaking core think text data,issue,negative,neutral,neutral,neutral,neutral,neutral
280727928,"The error tells you that the number of inputs to the loss function is different than the number of given targets. It happens in the line 209. The problem is that the generator and discriminator architectures are apparently fixed to the default image size ([see annotations in the model](https://github.com/pytorch/examples/blob/master/dcgan/main.py#L139-L156)). Adding a pooling layer at the end of the discriminator, that squeezes every batch element into a `1x1x1` image would help. I think that appending `nn.MaxPool2d(opt.imageSize // 64)` [after the Sigmoid](https://github.com/pytorch/examples/blob/master/dcgan/main.py#L156) would fix that.",error number loss function different number given line problem generator discriminator apparently fixed default image size see model layer end discriminator every batch element image would help think sigmoid would fix,issue,negative,positive,neutral,neutral,positive,positive
280491704,"Yes, that's the plan. I'm going to upload these models to our so and add
then to the Readme.



On Feb 16, 2017 2:24 PM, ""Adam Paszke"" <notifications@github.com> wrote:

> Can we pack the data and upload it to our s3 instead of checking it in?
>
> â€”
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/pytorch/examples/pull/50#issuecomment-280482164>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AFcCDc8OlrW6CR85I4cpxQFRsZW9XKXoks5rdMy3gaJpZM4LoqXc>
> .
>
",yes plan going add wrote pack data instead reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
280459895,"@szagoruyko, thanks for uploading that model! I had a few questions about the training. Why do you emphasize using checkpointing with .state_dict() and .load_state_dict()? Did you make any other changes to train that model? Have you been able to get BLEU scores?",thanks model training emphasize make train model able get,issue,negative,positive,positive,positive,positive,positive
280435463,"uploaded en-fr model state_dict in hdf5 to AWS (4.85 ppl): https://s3.amazonaws.com/pytorch/h5models/model_enfr_e13_4.85.hkl (320MB)
One thing, checkpointing should be done via `.state_dict()` and `.load_state_dict()` for model and optimizer.",model one thing done via model,issue,negative,neutral,neutral,neutral,neutral,neutral
280104673,"really nice work @adamlerer ! en-fr model on benchmark-1M gets to 12 perplexity after 1 epoch and generates decent predictions. I think we can merge this, just 2 things:
 * math.inf needs a fix in py2 as I pointed above
 * would be nice to have some pretrained models available. I will upload mine after it converges.",really nice work model perplexity epoch decent think merge need fix pointed would nice available mine,issue,positive,positive,positive,positive,positive,positive
279680363,@clcarwin nice work! I added the models to the functional zoo here https://github.com/szagoruyko/functional-zoo/blob/master/fast-neural-style.ipynb,nice work added functional zoo,issue,negative,positive,positive,positive,positive,positive
277907805,"@apaszke @jcjohnson 
Hi, all requested changes have been changed. Please review it again. Thanks.",hi please review thanks,issue,positive,positive,positive,positive,positive,positive
276397006,"@fmassa Yes 3 pairwise distances are for the contribution in the paper, but for the triplet base case just two are needed. I'm trying the base case.",yes pairwise contribution paper triplet base case two trying base case,issue,negative,negative,negative,negative,negative,negative
276394898,"@edgarriba I didn't spend much time looking into it, but I thought that in the lua code there were 3 pairwise distances, while in the Pytorch implementation you only compute 2 pairwise distances. But maybe I'm missing more things",spend much time looking thought code pairwise implementation compute pairwise maybe missing,issue,negative,neutral,neutral,neutral,neutral,neutral
276387534,"@fmassa The code is slightly different since with Pytorch you can easily share the weights of the net (or at least is what I assumed) whereas in the Lua version you had to deal with `ConcatTable` in order to define such architectures.

As you said some of the parts are optimized inside the loss function like the two `PairDistance` functions. In this case basically I took the TripletLoss from Chainer and I adapted to Pytorch routines. Ideally should be the same as the one that I implemented with TensorFlowÂ¹ which btw using it I achieve almost the same results reported to the paper.

What you mention about the min is an extra contribution that we introduced in the paperÂ² in order to improve the loss function.

Â¹ https://github.com/vbalnt/tfeat/blob/master/tensorflow/train/tfeat/loss.py#L13-L35
Â² http://www.iis.ee.ic.ac.uk/%7Evbalnt/shallow_descr/TFeat_paper.pd",code slightly different since easily share net least assumed whereas version deal order define said inside loss function like two case basically took chainer ideally one achieve almost paper mention min extra contribution order improve loss function,issue,positive,positive,positive,positive,positive,positive
276367950,"@edgarriba Hi !
I didn't know about the code you sent, nor have I seen the paper, but I have the impression that there are some differences with your code here and the lua code you sent.
For example, I have the impression that [some parts in here](https://github.com/vbalnt/pnnet/blob/master/train/run.lua#L56-L111) is not present in your code, but maybe that's because you have optimized some operations out in your `TripletLoss`?
The [Min](https://github.com/vbalnt/pnnet/blob/master/train/run.lua#L100) over two distances seems to be absent (except if it's already mixed somehow with the MarginRankingCriterion an present in [here](https://github.com/pytorch/examples/pull/47/files#diff-1357f5fd365db925427f5f0692df5b52R219))?",hi know code sent seen paper impression code code sent example impression present code maybe min two absent except already mixed somehow present,issue,negative,neutral,neutral,neutral,neutral,neutral
276333443,"@apaszke https://github.com/vbalnt/pnnet/blob/master/train/run.lua
notice that you just need to replace [line 166](https://github.com/vbalnt/pnnet/blob/master/train/run.lua#L116) by nn.MarginRankingCriterion in order to have same approach.

@fmassa hi! :smile: Yeah I noticed about this dampening parameter, in Lua seems that takes momentum value by default. Also in case of defining a learning rate decay, by default this policy it's automatically applied. In Pytorch it's not. As for RMSProp I haven't  tried since I want to replicate original codes where we just use SGD with momentum. Actually, it's a very simple code that should work very easily!

I've tried this tricks but no luck. It looks to me that hyperparameters have to be completely different.

**UPDATE**
You can also check TF code here
https://github.com/vbalnt/tfeat/blob/master/tensorflow/train/tfeat/train_tfeat.py",notice need replace line order approach hi smile yeah parameter momentum value default also case learning rate decay default policy automatically applied tried since want replicate original use momentum actually simple code work easily tried luck completely different update also check code,issue,positive,positive,positive,positive,positive,positive
276324123,The only difference I remember between optimizers are different initializations for some parameters. For example [`dampening` in SGD](https://github.com/pytorch/pytorch/issues/6) and the [initialization of the buffers in `RMSProp`](https://github.com/pytorch/pytorch/pull/485),difference remember different example,issue,negative,neutral,neutral,neutral,neutral,neutral
276322130,"Yes, all optimizers were cross-tested with their Lua versions and on toy problem yield the exact same sequence of updates up to 1e-10. It's likely a bug in the implementation somewhere. I'll try to take a look when I'll have a moment",yes toy problem yield exact sequence likely bug implementation somewhere try take look moment,issue,negative,positive,positive,positive,positive,positive
276320166,"@apaszke Are you aware of any difference between optimizers using Lua and Python versions? The same code with Lua is able to achieve a 7% of FPR5 with a LR 0.1, LRD 1e-6 and WD 1e-4. I've tried several LR and the networks always get stuck at ~16% FPR95.

Besides, I've implemented the code in TF which in v0.11 (not in v.12) with MomentumOptimizer M 0.9 and LR 1e-4 I can achieve a 9% FPR95.",aware difference python code able achieve tried several always get stuck besides code achieve,issue,negative,positive,positive,positive,positive,positive
276121047,"Ah, you're right! It got broken by a recent change! Thanks for the report! Please send a PR, if you have a moment. Otherwise I can do it later today.",ah right got broken recent change thanks report please send moment otherwise later today,issue,negative,positive,neutral,neutral,positive,positive
275851909,"These problems will be fixed in next few days. If there's anything else, please feel free to tell me.

1. Add two parameter for train.py ```--style_size``` ```--batch_size```.
2. Fix argparse's description in generate.py.
3. Add some pretrained models.",fixed next day anything else please feel free tell add two parameter fix description add,issue,positive,positive,positive,positive,positive,positive
275685258,"I run this a second time, and it worked. I was orignally using resnet18, but with Alexnet and resnet50 it works fine.",run second time worked work fine,issue,negative,positive,positive,positive,positive,positive
275632127,"@colesbury I'm not saying why this is happening, but basically (and it happens only on multi GPU hardware, for me), at the first batch of the test at the first epoch, suddenly the screen freezes, and the memory is not released. Which, I think due to the pin memory, or at least it has solved the issue to set if to false. It's not clear to me why it happens!",saying happening basically hardware first batch test first epoch suddenly screen memory think due pin memory least issue set false clear,issue,negative,negative,neutral,neutral,negative,negative
275569513,"No -- setting `pin_memory=True` is most useful when using more than one GPU so that the copies to each GPUs can overlap with each other.

@culurciello, what's the traceback when it gets stuck?",setting useful one overlap stuck,issue,negative,positive,positive,positive,positive,positive
275563876,"hi, yes, remove the pin memory when there is a multi gpu",hi yes remove pin memory,issue,negative,neutral,neutral,neutral,neutral,neutral
274820102,@edgarriba no such issues with the LuaTorch version. Training was smooth with both margin and ratio criterions. ,version training smooth margin ratio,issue,negative,positive,positive,positive,positive,positive
274636671,"no, in this example optimization was done by hand",example optimization done hand,issue,positive,neutral,neutral,neutral,neutral,neutral
274592577,"@apaszke I think that the code is somehow functional and pretty close to the original implementation with LuaTorch. However, not sure why the loss get stuck at the value of the margin.

/cc @vbalnt Did you find similar issues during the implementation?",think code somehow functional pretty close original implementation however sure loss get stuck value margin find similar implementation,issue,positive,positive,positive,positive,positive,positive
274057628,"Since you already have a PR, I'll close this issue. Let's keep the discussion in a single place.",since already close issue let keep discussion single place,issue,negative,negative,neutral,neutral,negative,negative
273927145,"Yes, the sampled noise had to be allocated on the GPU too. You can check the diff in my PR to see where was that. Thanks!",yes noise check see thanks,issue,positive,positive,positive,positive,positive,positive
273920505,"Thank you for your answer and the fix ! 
I had tried to pass the data to cuda but this alone had not solved it for me.
With the additional change in the PR it works great :)",thank answer fix tried pas data alone additional change work great,issue,positive,positive,positive,positive,positive,positive
273916669,"It's not a problem with weights, if you look at the order of the arguments it's `(0, 1, input, weight.t())` and it's the 3rd argument that's not on CUDA. I see the bug, the example is missing
```python
if args.cuda:
    data = data.cuda()
```
in the training loop. I'll send a PR in a sec. Thanks for reporting it!",problem look order input argument see bug example missing python data training loop send sec thanks,issue,negative,negative,neutral,neutral,negative,negative
273903581,@soumith @apaszke I opened a PR with a first version. First I want to implement it as single example and check that all is working as expected. After that I'll integrate the loss function using the functional interface and the Phototour dataset into torchvision/datasets. Feel free to review and leave comments in https://github.com/pytorch/examples/pull/47,first version first want implement single example check working integrate loss function functional interface feel free review leave,issue,negative,positive,positive,positive,positive,positive
273872757,"the dcgan and imagenet examples use multiprocessed data loading.

https://github.com/pytorch/examples/blob/master/dcgan/main.py#L80-L81

The num_workers option controls that.",use data loading option,issue,negative,neutral,neutral,neutral,neutral,neutral
273851237,"Or you can implement it as a function, to look like the losses in the functional interface. It's also covered in the linked notes.",implement function look like functional interface also covered linked,issue,negative,neutral,neutral,neutral,neutral,neutral
273787746,"Yeah, but look [at the code](https://github.com/pytorch/examples/pull/42/files#diff-4575643c14f36c8542cc5ed9c56aaa39L69). The value head is trained to learn the value function, but it's no longer used. The code computes a baseline based on the received rewards, not on the computed value estimate",yeah look code value head trained learn value function longer used code based received value estimate,issue,positive,neutral,neutral,neutral,neutral,neutral
273787185,there's two files now. one REINFORCE and another actor-critic (that's what zeming said),two one reinforce another said,issue,negative,neutral,neutral,neutral,neutral,neutral
273704875,Is it still actor_critic after this change? It seems to me that value head isn't used at all now,still change value head used,issue,negative,neutral,neutral,neutral,neutral,neutral
273667107,thank you! fixed it correctly to `torchvision` rather than `pillow` via https://github.com/pytorch/examples/commit/363a9470d60479bce69664d101f02cda6ddf16a9,thank fixed correctly rather pillow via,issue,negative,positive,neutral,neutral,positive,positive
273661155,"Hi @crcrpar, thanks for catching this. I think it's actually missing the [`torchvision`](https://github.com/pytorch/vision) package as a dependency, which in turn depends on pillow.",hi thanks catching think actually missing package dependency turn pillow,issue,negative,positive,positive,positive,positive,positive
273639955,I think we should just delete this line. It was part of a CUDA + hogwild example and isn't necessary here.,think delete line part example necessary,issue,negative,neutral,neutral,neutral,neutral,neutral
273634979,"if you want to implement a triplet loss, with autograd it should be fairly straightforward.

You can implement the loss by subclassing it from `nn.Module` and then defining the forward function that takes in (input, target). There's no difference in implementing a loss or a module.

These notes might help:

http://pytorch.org/docs/notes/extending.html",want implement triplet loss fairly straightforward implement loss forward function input target difference loss module might help,issue,negative,positive,positive,positive,positive,positive
273392992,"Excellent. The revert is a bit of a mess, so I squashed everything into a new branch. If you could rebase your changes onto https://github.com/adamlerer/examples/tree/onmt2 that would be ideal. Thanks again!",excellent revert bit mess everything new branch could rebase onto would ideal thanks,issue,positive,positive,positive,positive,positive,positive
273392311,"Yeah will do! I'll be back at a keyboard in about half and hour.
On Tue, Jan 17, 2017 at 9:11 PM Adam Lerer <notifications@github.com> wrote:

> Great. Bryan, could you submit the PR to
> https://github.com/adamlerer/examples?
>
> Thanks!
>
> On Tue, Jan 17, 2017 at 11:54 PM, jekbradbury <notifications@github.com>
> wrote:
>
> > This PR has all the changes that Bryan's made so far; what's left on our
> > side is replacing the hand-rolled dataset infrastructure here with
> > torchtext when we can replicate the functionality (I'm also doing this
> for
> > the PTB language model example). So we're unlikely to step on each
> other's
> > toes too much, assuming that you're working on the TODOs like feature
> > embeddings and model checkpointing.
> >
> > â€”
> > You are receiving this because you were mentioned.
> > Reply to this email directly, view it on GitHub
> > <https://github.com/pytorch/examples/pull/36#issuecomment-273382743>,
> or mute
> > the thread
> > <
> https://github.com/notifications/unsubscribe-auth/AFcCDYWLYorZiVgFV8KWxvJhyCUHs9urks5rTZsYgaJpZM4LmbyW
> >
> > .
> >
>
> â€”
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/pytorch/examples/pull/36#issuecomment-273384689>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/ADNEpQw_9kUepu7U--kUOr3yUSgcrHxoks5rTZ8GgaJpZM4LmbyW>
> .
>
",yeah back keyboard half hour tue wrote great could submit thanks tue wrote made far left side infrastructure replicate functionality also language model example unlikely step much assuming working like feature model reply directly view mute thread reply directly view mute thread,issue,positive,positive,neutral,neutral,positive,positive
273384689,"Great. Bryan, could you submit the PR to
https://github.com/adamlerer/examples?

Thanks!

On Tue, Jan 17, 2017 at 11:54 PM, jekbradbury <notifications@github.com>
wrote:

> This PR has all the changes that Bryan's made so far; what's left on our
> side is replacing the hand-rolled dataset infrastructure here with
> torchtext when we can replicate the functionality (I'm also doing this for
> the PTB language model example). So we're unlikely to step on each other's
> toes too much, assuming that you're working on the TODOs like feature
> embeddings and model checkpointing.
>
> â€”
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/pytorch/examples/pull/36#issuecomment-273382743>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AFcCDYWLYorZiVgFV8KWxvJhyCUHs9urks5rTZsYgaJpZM4LmbyW>
> .
>
",great could submit thanks tue wrote made far left side infrastructure replicate functionality also language model example unlikely step much assuming working like feature model reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
273382743,"This PR has all the changes that Bryan's made so far; what's left on our side is replacing the hand-rolled dataset infrastructure here with torchtext when we can replicate the functionality (I'm also doing this for the PTB language model example). So we're unlikely to step on each other's toes too much, assuming that you're working on the TODOs like feature embeddings and model checkpointing.",made far left side infrastructure replicate functionality also language model example unlikely step much assuming working like feature model,issue,negative,negative,neutral,neutral,negative,negative
273375650,"@bmccann sorry it looks like this examples was merged into mainline while it was written against an outdated pytorch version, thanks for cleaning up.

I'm actively working on this at the moment, so can we coordinate and merge our changes?",sorry like written outdated version thanks cleaning actively working moment merge,issue,positive,negative,negative,negative,negative,negative
273372274,"Thanks, I'll keep working on fixes on this branch.",thanks keep working branch,issue,negative,positive,positive,positive,positive,positive
273229277,"Right, I haven't thought about it ğŸ˜• We should try to minimize cross-example dependencies to make them easier to maintain and simpler for people to use (e.g. if they want to start their own projects from them). I'll fix that, thanks for the heads up! ",right thought try minimize make easier maintain simpler people use want start fix thanks,issue,positive,positive,positive,positive,positive,positive
273228826,"Note that the VAE example [currently requires `data/mnist`](https://github.com/pytorch/examples/blob/master/VAE/main.py#L20-L21), so this PR will break it. But I think that the example should be adapted to use the mnist from torchvision",note example currently break think example use,issue,negative,neutral,neutral,neutral,neutral,neutral
271187634,This is completely badass. Let us know if we can help in anyway. ,completely let u know help anyway,issue,negative,positive,neutral,neutral,positive,positive
271041969,"Ugh nvm, for some reason I haven't noticed that it's WIP...

Have a nice trip! ğŸ˜ƒ ",ugh reason nice trip,issue,negative,positive,positive,positive,positive,positive
271041724,"@colesbury @apaszke  thanks for your comments. I think the `ConfigParser` might be a nice way of addressing tons of arguments.

I initially wanted to keep the number of files small (as it is an *example* code), so I fused a number of things together, but that's probably a poor design choice.

I will validate that the basic code is working as expected by performing a training/evaluation and then I'll focus on getting a refactoring of this PR.

I'll get back to it on Monday, I've a trip to do in Rio tomorrow :)",thanks think might nice way initially keep number small example code fused number together probably poor design choice validate basic code working focus getting get back trip rio tomorrow,issue,positive,positive,neutral,neutral,positive,positive
270794780,"This looks good. I think the most important thing is to get training (and evaluation) working and then worry more about design and cleaning up the code.

Some thoughts:

- I think copying from Ross's Faster-RCNN repo makes sense where appropriate. Keeping them in separate files, unchanged if possible, would be best
- There's a trade-off between making everything part of the model (nn.Container) and keeping it as part of the training function. For example, FasterRCNN.forward is a bit complicated because it combines training and evaluation logic: it has two possible return types and two possible input types.
- The `forward()` method in modules should take in `Variables` where appropriate; the wrapping of tensors in Variables should happen outside the module
- Try to match PyTorch style (PEP 8) for stuff that's not from RBG's repo. (4 space indent, etc.)
- Use Python argparse to configure options. I like how you're not passing a global cfg around. If you want to support a config file as well, I think you can do so with [ConfigParser](http://stackoverflow.com/questions/3609852/which-is-the-best-way-to-allow-configuration-options-be-overridden-at-the-comman)",good think important thing get training evaluation working worry design cleaning code think ross sense appropriate keeping separate unchanged possible would best making everything part model keeping part training function example bit complicated training evaluation logic two possible return two possible input forward method take appropriate wrapping happen outside module try match style pep stuff space indent use python configure like passing global around want support file well think,issue,positive,positive,positive,positive,positive,positive
262830168,"@ludc About the stochastic modules, it's actually not as simple. It could work, however you'd need to perform the work of `:reinforce` in `backward` and treat `grad_output` as `err`. New `nn` isn't a standalone module, but rather relies heavily on machinery of `torch.autograd`, and that is designed for graphs of differentiable ops (they need to define `forward` and `backward`). You never call these `backward` methods yourself, you only do it once, on a variable, and it bootstraps the whole gradient computation and passes the control to the `ExecutionEngine`.

Some comments to the API you proposed:
* I bound the `Sense` to the `Environment` on purpose, because it has to be 100% compatible with it. There are many backends that provide you with the screen pixels, but every single one will require performing different attribute lookup, so it's logical for me that they need to be logically bound together. Maybe I'm wrong, I hardly have any experience in RL.
* Since to use the `Sense` you need to know exactly what kind of objects it returns, I decided not to force a single point that returns the full value space. For screen you can implement `.size()` and for audio you can implement `.length()`. 
* I think I didn't understand what `Feedback` is because of the name. It sounds as if it's a one time reward, while it actually defines a way to measure how well is the agent performing. It sounds more like a `Task` to me.
* I'm not sure if I like the requirement to pass `env` everywhere. I don't know how often you need to use the same `Task`/`Feedback` in different environments, but if it's not the case, then I think it would be cleaner to pass it once to `__init__`.
* I don't think I understand the difference between `feedback` and `feedback_action`.
* Isn't it better to associate the set of actions with the `World`? It seems more logical to me that it's the world that defines what you can/can't do, and the task can in turn tell you if you're done.

If you're feeling like reimplementing rltorch or sth similar so soon, then go on. I will have to work on some other stuff as well, so I can wait and review the changes if you wish.

@korymath I'll try to take a look at torch-twrl soon and see how it feels.",stochastic actually simple could work however need perform work reinforce backward treat err new module rather heavily machinery designed differentiable need define forward backward never call backward variable whole gradient computation control bound sense environment purpose compatible many provide screen every single one require different attribute logical need logically bound together maybe wrong hardly experience since use sense need know exactly kind decided force single point full value space screen implement audio implement think understand feedback name one time reward actually way measure well agent like task sure like requirement pas everywhere know often need use task feedback different case think would cleaner pas think understand difference feedback better associate set world logical world task turn tell done feeling like similar soon go work stuff well wait review wish try take look soon see,issue,positive,positive,positive,positive,positive,positive
262826830,"Concerning the core classes (not agent/policy here), this is what I have in mind, considering the structure proposed by @apaszke ?

```
class Sense(object):
    def __init__(self, env):
        pass

    #I think that it is important to put the environment here since the same sensor can be used on different copies of an environment
    def observe(self,env):
        raise NonImplementedError

    #A description of the space of the data returned by the sensor
    def sense_space(self):
        raise NonImplementedError

class World(object):
    def __init__(self):
        self.actions = set()

    def take_action(self, action):
        # update environment state. No feedback (see Feedback class)
        pass

    def reset(self,info):
        #reset the environment
        #info: additional information. Can be used for example to generate mazes with different difficulty levels
        pass
    
    def clone(self):
        #clone the environment in its current state
        pass

class Feedback(object):
    def __init__(self,env):
        pass

    def feedback(self,env):
        #return the feedback associated with the env. Here, the feedback is provided when the env state is reached i.e r(s)
        pass

    def feedback_action(self,env,action):
        # return the feedback associated with the state if the action 'action' is taken. Here, the feedback will be computed before modifying the env i.e r(s,a)
        pass

    def finished(self,env):
        #The episode is finished. No more action allowed
        pass

    def possible_actions(self,env):
        #The set of actions that can be applied
        pass


class GymEnvironment( open ai gym class):
    #RLEnvironment is the adapter to openAIGym.

    def __init(self,world,feedback,sense):
        pass

    #See openai gym definition

```",concerning core class mind considering structure class sense object self pas think important put environment since sensor used different environment observe self raise description space data returned sensor self raise class world object self set self action update environment state feedback see feedback class pas reset self reset environment additional information used example generate different difficulty pas clone self clone environment current state pas class feedback object self pas feedback self return feedback associated feedback provided state pas self action return feedback associated state action taken feedback pas finished self episode finished action pas self set applied pas class open ai gym class adapter self world feedback sense pas see gym definition,issue,negative,positive,neutral,neutral,positive,positive
262824231,"Concerning the separation between agent, environment, monitoring code, I totally agree. Concerning enabling a strong (and easy)  connection with openAI Gym, I agree also. 

So, I think that we just have to agree on simple core classes (actually, the way PG or other algorithms will be implemented is a totally separate problem), right ? 

",concerning separation agent environment code totally agree concerning strong easy connection gym agree also think agree simple core class actually way totally separate problem right,issue,positive,positive,positive,positive,positive,positive
262821071,"Looks good, nice discussion guys. 

Thanks for making the connection @soumith. 

The way we did this in https://github.com/twitter/torch-twrl is a little bit different. 

Here we have an agent defined by a learning update, a model and a policy. The agent is completey separate from the environment, and from the monitoring code. This separation allows for building in modular chunks. You can see a nice visualization here: https://blog.twitter.com/2016/reinforcement-learning-for-torch-introducing-torch-twrl 

I have been working on DDPG in pytorch, and will try to model it after the breakdown on the twrl package. Twrl was modelled after the RLLab and torch-rl. 

We should aim to enable capabilities expected by OpenAI Gym as it is the common test bed these days. I have been working on a simple continuous action space example with DDPG.

Not sure if this is helpful, but figured that an easy implementation of a common, popular RL algorithm on OpenAI gym would be the most effective example for RL on pytorch",good nice discussion thanks making connection way little bit different agent defined learning update model policy agent separate environment code separation building modular see nice visualization working try model breakdown package aim enable gym common test bed day working simple continuous action space example sure helpful figured easy implementation common popular algorithm gym would effective example,issue,positive,positive,positive,positive,positive,positive
262818485,"Concerning the dpnn approach, the idea is the following:

You make a model that includes stochastic layers (for example, the MultinomialLayer takes a discrete set of pobabilities as an input and output a one hot vector by sampling following this distribution). Then the gradient can be estimated by providing a ''reward-like'' feedback to this layer before the backward. 

So, imagine you have a loss function, an input x and an output y to predict, you can do:
```

m1=Linear(....)
m2=SoftMax(...)
m3=MultinomialLayer(...)
m4=Linear(...)

a=m1:forward(x)
b=m2:forward(a)
c=m3:forward(b) #Here the sampling is made
d=m4:forward(c)

err=loss:forward(d,y)
m3:reinforce(-err) #Here, the reward is provided to the stochastic modules. It will allow to compute the derivative of the term log P(sample|input) * reward that will be backpropagated
delta=loss:backward(d,y)
delta=m4:backward(....)
delta=m3:backward(....)
delta=m2:backward(....)
delta=m1:backward(....)

```

When using with reinforcement, you directly have a reward (no loss), so you start your backpropagation with a empty delta, but the idea remains the same.  (see https://github.com/Element-Research/dpnn#nn.Reinforce)

Basically, the goal is to include stochastic modules into the computation graph, and this can be done by adding a reinforce method for these modules (but other ways can be imagined I suppose)


",concerning approach idea following make model stochastic example discrete set input output one hot vector sampling following distribution gradient providing feedback layer backward imagine loss function input output predict forward forward forward sampling made forward forward reinforce reward provided stochastic allow compute derivative term log reward backward backward backward backward backward reinforcement directly reward loss start empty delta idea remains see basically goal include stochastic computation graph done reinforce method way suppose,issue,positive,positive,neutral,neutral,positive,positive
262816152,"OK, some differences between what you propose and rltorch:

* the Feedback class in rltorch defines the task to solve:
** It can be a reward function (but different tasks can be defined on the same environment)
** It can be an output to predict (what I call predictive reinforcement learning) e.g a category, a value, a structure if you are doing structured output prediction with reinforcement learning
** It can also contain teacher feedback (actions to take)

I think it is interesting to have a separate class for defining the task to solve,. Here is the code I imagine:

```
  `class Feedback(object): 
    def __init__(self, env):
        self.env = env

    def feedback(self,env):
        raise NonImplementedError`

    def finished(self,env):
        raise NonImplementedError`
```

Concerning the finished method, I think that one clever thing would be to define a method that list all authorized actions (or authorized domains for continuous RL). If this method returns and empty set, then the episode is finished. Other types of feedback can be defined (see https://github.com/ludc/rltorch/tree/master/torch/environments/classiclearning/classification)

The second point concerns the definition of the agent (or policy). Since the agent can receive different types of feedback (at different time steps in the process), here is what I imagine:


``` 
 def new_episode(self,information):
 #informations contains additionnal information for the intialization of the agent

def observe(self,observation):

def feedback(self,feedback):
#can be called whenever during the life of the agent, and multiple times

def sample(self):
#Sample an action

def end_episode(self,feedback):

def reset(self):
```

This definition is very general, and then can be instantiated for particular 'gradient-based' agent (what you propose in your previous post)

Concerning the RLTrainer, I have no preference....

* In Environment: a reset method (to sample an initial state) would be nice
* In Sense: being able to get information about the nature of the data provided by the sensor (the size of the tensor for example)

I can write all these classes in 'real' python during the week end, but it will be almost the same thing that the core classes of rltorch.

 How do you want to proceed ? 



",propose feedback class task solve reward function different defined environment output predict call predictive reinforcement learning category value structure structured output prediction reinforcement learning also contain teacher feedback take think interesting separate class task solve code imagine class feedback object self feedback self raise finished self raise concerning finished method think one clever thing would define method list authorized authorized continuous method empty set episode finished feedback defined see second point definition agent policy since agent receive different feedback different time process imagine self information information agent observe self observation feedback self feedback whenever life agent multiple time sample self sample action self feedback reset self definition general particular agent propose previous post concerning preference environment reset method sample initial state would nice sense able get information nature data provided sensor size tensor example write class python week end almost thing core class want proceed,issue,positive,positive,positive,positive,positive,positive
262807696,"Hey,

So I've been also playing with some RL lately and I started decomposing the code into different chunks and thinking how would they fit into pytorch. I think right now I ended up with something quite closely following the design of rltorch. However, if this framework works well in all/most RL cases, we could go a step further and also add define an `RLTrainer` class.

Right now, we have only a basic `torch.utils.trainer.Trainer` class, but it's definitely too general for this. I like the decomposition of the problem into World, Sensor and Policy. I only don't understand what Feedback is for (is it a generalization of a reward? do you every give non-scalar rewards?). I'd propose that we start posting code samples. My initial design is the following:

```python
class Sense(object):
    def __init__(self, env):
        self.env = env

    def observe(self):
        raise NonImplementedError


class Environment(object):
    def __init__(self):
        self.actions = set()

    def take_action(self, action):
        # update environment state
        return feedback

    # Optionally a number of senses you can use to observe the world


class Agent(object):

    def __init__(self, env):
        # initialize internal models
        # gather senses from env
        pass

    def forward(self):
        # use envs to observe the environment state
        # create input for the internal models
        # predict action / choose random one
        pass

    def backward(self, feedback):
        # generate gradients for internal models
        # can use experience replay instead of feedback
        pass

    def new_session(self):
        # clear any saved state e.g. screen history
        pass

# I'm omitting all the plugin-calling code for clarity
class RLTrainer(Trainer):

    def __init__(self, optimizer, env, agent, session_length=1000):
        self.optimizer = optimizer
        self.env = env
        self.agent = agent

    def train(self):
        self.agent.new_session()
        for i in range(session_length):
            action = self.agent.forward()
            feedback = self.env.take_action(action)
            optimizer.zero_grad()
            agent.backward(feedback)
            optimizer.step()
```

An example implementation:

```python
class GameEnvironment(Environment):
    def __init__(self):
        self.actions = set(MOVE_FORWARD, MOVE_BACKWARD, SHOOT)
        self.game = ... # initialize the game engine

    def take_action(self, action):
        self.game.update(action)

    def screen_buffer(self):
        return _ScreenBuffer(self)

    class _ScreenBuffer(Sense):
        def __init__(self, env):
            self.env = env

        def observe(self):
            return torch.FloatTensor(self.env.game.screen_buffer)

        def size(self):
            return torch.Size([1, 3, 100, 100])

class DQNAgent(Agent):
    def __init__(self, env):
        self.actions = env.actions
        self.screen_buffer = env.screen_buffer()
        self.dqn = DQN(self.screen_buffer.size(), len(self.actions))
        self.last_screens = ...
        self.replay_memory = ...

    def forward(self):
        self.remember_frame(self.screen_buffer.observe()) # updates self.last_frames

        if self._should_pick_random():
            action_idx = random() * self.num_actions
        else:
            action_idx = self._predict()
            
        self.last_action = self.actions[action_idx]
        return self.last_action

    def _should_pick_random():
        return random() > threshold

    def _predict(self):
        output = self.dqn.forward(self.last_frames)
        return output.max(1)[1]

    def backward(self, feedback):
        self.replay_memory.store(
                self.last_frames,
                self.last_action,
                feedback,
                self.screen_buffer.observe()
        )
        # sample from replay_memory and do backward on the dqn

```

Also, could you please point me to the code of `dpnn` you're referring to? I haven't done any RL with Lua Torch, so I don't know what was the design.",hey also lately code different thinking would fit think right ended something quite closely following design however framework work well could go step also add define class right basic class definitely general like decomposition problem world sensor policy understand feedback generalization reward every give propose start posting code initial design following python class sense object self observe self raise class environment object self set self action update environment state return feedback optionally number use observe world class agent object self initialize internal gather pas forward self use observe environment state create input internal predict action choose random one pas backward self feedback generate internal use experience replay instead feedback pas self clear saved state screen history pas code clarity class trainer self agent agent train self range action feedback action feedback example implementation python class environment self set shoot initialize game engine self action action self return self class sense self observe self return size self return class agent self forward self random else return return random threshold self output return backward self feedback feedback sample backward also could please point code done torch know design,issue,positive,negative,neutral,neutral,negative,negative
262788846,"Hello, 

I just discovered pytorch yesterday, so I still have to go indepth, but my first intention was to evaluate if I can recode the rltorch package with pytorch. Basically, rltorch is very simple but more general than openAI Gym since it allows one to decompose any problem in environment, sensor, feedback and policy and thus can be also used for other problems (like supervised classification with RL, etc...). Then, making a openAI Gym wrapper is very easy. In that case, one way to test the platform is to reimplement one of the environments (e.g cartpole) and to compare the implemented environment with the openAI Gym one. I think that this could be done in a few days.

The second point concerns the implementation of policy gradient algorithms. I think that the 'dpnn' mechanism is very nice. I suppose it can be implemented in pytorch by extending the 'Container' and/or 'Module' classes to incorporate a 'reinforce' method (but I have to check to be sure)

In term of algorithms, I would like to start with: policy gradient, recurrent policy gradient, predictive policy (see rltorch), ucbpolicy (for online learning), and imitation policy (supervised)

 What do you think ? 

",hello discovered yesterday still go first intention evaluate recode package basically simple general gym since one decompose problem environment sensor feedback policy thus also used like classification making gym wrapper easy case one way test platform one compare environment gym one think could done day second point implementation policy gradient think mechanism nice suppose extending class incorporate method check sure term would like start policy gradient recurrent policy gradient predictive policy see learning imitation policy think,issue,positive,positive,positive,positive,positive,positive
261618186,"If you're doing inference only, you might need to set `volatile=True`. Otherwise you often end up with reference cycles which aren't collected immediately. (This is something we should fix in PyTorch)
",inference might need set otherwise often end reference collected immediately something fix,issue,negative,neutral,neutral,neutral,neutral,neutral
260516712,"100% agree. I think the OMP_NUM_THREADS=1 might not be necessary anymore. There used to be a bug where the multi-process data loader would spawn lots of OpenMP threads, but I think that's fixed now (https://github.com/pytorch/pytorch/issues/81 and https://github.com/pytorch/pytorch/issues/82)
",agree think might necessary used bug data loader would spawn lot think fixed,issue,negative,positive,neutral,neutral,positive,positive
254111790,"I updated this to use the torch RNN library (the monolithic one, that uses cudnn). It's within 5% of the speed of the lua-torch version under the standard parameters. Ready to merge.

P.S. Should I also include the version that builds an RNN from scratch? It might be instructive for people who want to do something different than what's supported by the monolith.
",use torch library monolithic one within speed version standard ready merge also include version scratch might instructive people want something different monolith,issue,negative,positive,neutral,neutral,positive,positive
251819743,"My latest commit fixes @colesbury and @apaszke comments, and some other bugs. I also fixed a few discrepancies with the torch model (remove biases, fix lr) so it now reaches the same perplexity as the torch model (~114).

Requires https://github.com/pytorch/pytorch/pull/106
",latest commit also fixed torch model remove fix perplexity torch model,issue,negative,positive,positive,positive,positive,positive
251761987,"FYI the perplexity isn't as good for this model as an ""equivalent"" rnnlib3 model, I'm looking at why now - probably has to do with different initialization.
",perplexity good model equivalent model looking probably different,issue,negative,positive,positive,positive,positive,positive
