id,original_comment,processed_comment,source,sentiment_VADER,sentiment_textblob,sentiment_pattern,sentiment_bert,sentiment_spacy,max_voted_sentiment
2014856940,@hxgqh  you need to install [annlite](https://pypi.org/project/annlite/) package. I had the same issue and resolved it by installing `annlite`.,need install package issue resolved,issue,negative,neutral,neutral,neutral,neutral,neutral
1893011966,What's the 'codecov' problem? Do you think that needs a unit test to cover?,problem think need unit test cover,issue,negative,neutral,neutral,neutral,neutral,neutral
1891319831,All the torch models have been migrated and tested. The onnx models will be migrated later due to my network.,torch tested later due network,issue,negative,negative,neutral,neutral,negative,negative
1872487504,"> You may learn how to start a search flow with [this documentation](https://clip-as-service.jina.ai/user-guides/retriever/), and then you may use `clip_client` to do the searching

Follow this document, it will fail:

```log
ERROR  indexer/rep-0@73 fail to load file dependency         [12/30/23 08:53:42]
ERROR  indexer/rep-0@73 FileNotFoundError('can not find
       annlite.executor') during 'WorkerRuntime'
       initialization
        add ""--quiet-error"" to suppress the exception
       details
       Traceback (most recent call last):
         File
       ""/usr/local/lib/python3.8/dist-packages/jina/orchest…
       line 89, in run
           runtime = AsyncNewLoopRuntime(
         File
       ""/usr/local/lib/python3.8/dist-packages/jina/serve/r…
       line 81, in __init__
           self._loop.run_until_complete(self.async_setup())
         File ""/usr/lib/python3.8/asyncio/base_events.py"",
       line 616, in run_until_complete
           return future.result()
         File
       ""/usr/local/lib/python3.8/dist-packages/jina/serve/r…
       line 255, in async_setup
           self.server = self._get_server()
         File
       ""/usr/local/lib/python3.8/dist-packages/jina/serve/r…
       line 181, in _get_server
           return GRPCServer(
         File
       ""/usr/local/lib/python3.8/dist-packages/jina/serve/r…
       line 30, in __init__
           super().__init__(**kwargs)
         File
       ""/usr/local/lib/python3.8/dist-packages/jina/serve/r…
       line 54, in __init__
           self._request_handler = req_handler or
       self._get_request_handler()
         File
       ""/usr/local/lib/python3.8/dist-packages/jina/serve/r…
       line 79, in _get_request_handler
           return self.req_handler_cls(
         File
       ""/usr/local/lib/python3.8/dist-packages/jina/serve/r…
       line 127, in __init__
           self._load_executor(
         File
       ""/usr/local/lib/python3.8/dist-packages/jina/serve/r…
       line 324, in _load_executor
           self._executor: BaseExecutor =
       BaseExecutor.load_config(
         File
       ""/usr/local/lib/python3.8/dist-packages/jina/jaml/__…
       line 742, in load_config
           load_py_modules(
         File
       ""/usr/local/lib/python3.8/dist-packages/jina/jaml/he…
       line 285, in load_py_modules
           PathImporter.add_modules(*mod)
         File
       ""/usr/local/lib/python3.8/dist-packages/jina/importe…
       line 161, in add_modules
           _path_import(complete_path(m))
         File
       ""/usr/local/lib/python3.8/dist-packages/jina/jaml/he…
       line 229, in complete_path
           raise FileNotFoundError(f'can not find {path}')
       FileNotFoundError: can not find annlite.executor
INFO   gateway/rep-0@74 start server bound to 0.0.0.0:51000  [12/30/23 08:53:42]
INFO   encoder/rep-0@72 start server bound to 0.0.0.0:58717  [12/30/23 08:53:50]
ERROR  Flow@ 1 Flow is aborted due to ['indexer'] can not be [12/30/23 08:53:51]
       started.
UserWarning: `docs` annotation must be a type hint, got DocumentArray instead, you should maybe remove the string annotation. Default valueDocumentArray will be used instead. (raised from /usr/local/lib/python3.8/dist-packages/jina/serve/executors/__init__.py:119)
Traceback (most recent call last):
  File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.8/dist-packages/clip_server/__main__.py"", line 25, in <module>
    with f:
  File ""/usr/local/lib/python3.8/dist-packages/jina/orchestrate/orchestrator.py"", line 14, in __enter__
    return self.start()
  File ""/usr/local/lib/python3.8/dist-packages/jina/orchestrate/flow/builder.py"", line 33, in arg_wrapper
    return func(self, *args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/jina/orchestrate/flow/base.py"", line 1832, in start
    self._wait_until_all_ready()
  File ""/usr/local/lib/python3.8/dist-packages/jina/orchestrate/flow/base.py"", line 1975, in _wait_until_all_ready
    raise RuntimeFailToStart
jina.excepts.RuntimeFailToStart
```",may learn start search flow documentation may use searching follow document fail log error fail load file dependency error find add suppress exception recent call last file line run file line file line return file line file line return file line super file line file line return file line file line file line file line file line file line raise find path find start server bound start server bound error flow flow aborted due annotation must type hint got instead maybe remove string annotation default used instead raised recent call last file line return code none file line code file line module file line return file line return self file line start file line raise,issue,negative,negative,neutral,neutral,negative,negative
1871912218,"## [Codecov](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/945?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
Attention: `4 lines` in your changes are missing coverage. Please review.
> Comparison is base [(`ca2b25b`)](https://app.codecov.io/gh/jina-ai/clip-as-service/commit/ca2b25b7564bc9b18ae38b93f0134e1f9aa0cee7?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) 70.73% compared to head [(`3ac7d2b`)](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/945?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) 71.71%.
> Report is 1 commits behind head on main.

| [Files](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/945?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Patch % | Lines |
|---|---|---|
| [server/clip\_server/model/clip\_onnx.py](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/945?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfb25ueC5weQ==) | 81.81% | [2 Missing :warning: ](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/945?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) |
| [server/clip\_server/model/pretrained\_models.py](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/945?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3ByZXRyYWluZWRfbW9kZWxzLnB5) | 84.61% | [2 Missing :warning: ](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/945?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) |

<details><summary>Additional details and impacted files</summary>


```diff
@@            Coverage Diff             @@
##             main     #945      +/-   ##
==========================================
+ Coverage   70.73%   71.71%   +0.98%     
==========================================
  Files          23       23              
  Lines        1565     1584      +19     
==========================================
+ Hits         1107     1136      +29     
+ Misses        458      448      -10     
```

| [Flag](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/945/flags?src=pr&el=flags&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [cas](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/945/flags?src=pr&el=flag&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | `71.71% <84.61%> (+0.98%)` | :arrow_up: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.


</details>

[:umbrella: View full report in Codecov by Sentry](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/945?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).   
:loudspeaker: Have feedback on the report? [Share it here](https://about.codecov.io/codecov-pr-comment-feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report attention missing coverage please review comparison base head report behind head main patch missing warning missing warning summary additional impacted coverage main coverage flag coverage carried forward coverage wo shown click find umbrella view full report sentry feedback report share,issue,negative,negative,negative,negative,negative,negative
1871893971,This PR exceeds the recommended size of 1000 lines. Please make sure you are NOT addressing multiple issues with one PR. Note this PR might be rejected due to its size.,size please make sure multiple one note might due size,issue,positive,positive,positive,positive,positive,positive
1871855025,"## [Codecov](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/944?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
Attention: `203 lines` in your changes are missing coverage. Please review.
> Comparison is base [(`ca2b25b`)](https://app.codecov.io/gh/jina-ai/clip-as-service/commit/ca2b25b7564bc9b18ae38b93f0134e1f9aa0cee7?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) 70.73% compared to head [(`a7d724e`)](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/944?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) 26.77%.
> Report is 1 commits behind head on main.

> :exclamation: Current head a7d724e differs from pull request most recent head 2bb2fe7. Consider uploading reports for the commit 2bb2fe7 to get more accurate results

| [Files](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/944?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Patch % | Lines |
|---|---|---|
| [client/clip\_client/client.py](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/944?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L2NsaWVudC5weQ==) | 25.25% | [74 Missing :warning: ](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/944?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) |
| [server/clip\_server/executors/clip\_onnx.py](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/944?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX29ubngucHk=) | 14.28% | [24 Missing :warning: ](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/944?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) |
| [server/clip\_server/executors/clip\_tensorrt.py](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/944?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RlbnNvcnJ0LnB5) | 0.00% | [22 Missing :warning: ](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/944?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) |
| [server/clip\_server/executors/clip\_torch.py](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/944?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RvcmNoLnB5) | 19.04% | [17 Missing :warning: ](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/944?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) |
| [server/clip\_server/executors/helper.py](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/944?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9oZWxwZXIucHk=) | 25.00% | [12 Missing :warning: ](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/944?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) |
| [server/clip\_server/model/model.py](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/944?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL21vZGVsLnB5) | 0.00% | [11 Missing :warning: ](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/944?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) |
| [server/clip\_server/model/openclip\_model.py](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/944?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL29wZW5jbGlwX21vZGVsLnB5) | 0.00% | [11 Missing :warning: ](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/944?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) |
| [server/clip\_server/model/pretrained\_models.py](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/944?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3ByZXRyYWluZWRfbW9kZWxzLnB5) | 16.66% | [10 Missing :warning: ](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/944?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) |
| [server/clip\_server/model/clip\_onnx.py](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/944?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfb25ueC5weQ==) | 10.00% | [9 Missing :warning: ](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/944?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) |
| [server/clip\_server/\_\_main\_\_.py](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/944?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL19fbWFpbl9fLnB5) | 0.00% | [5 Missing :warning: ](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/944?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) |
| ... and [4 more](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/944?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | |

<details><summary>Additional details and impacted files</summary>


```diff
@@             Coverage Diff             @@
##             main     #944       +/-   ##
===========================================
- Coverage   70.73%   26.77%   -43.96%     
===========================================
  Files          23       23               
  Lines        1565     1576       +11     
===========================================
- Hits         1107      422      -685     
- Misses        458     1154      +696     
```

| [Flag](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/944/flags?src=pr&el=flags&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [cas](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/944/flags?src=pr&el=flag&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | `26.77% <23.10%> (-43.96%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.


</details>

[:umbrella: View full report in Codecov by Sentry](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/944?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).   
:loudspeaker: Have feedback on the report? [Share it here](https://about.codecov.io/codecov-pr-comment-feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report attention missing coverage please review comparison base head ade report behind head main exclamation current head ade pull request recent head consider commit get accurate patch missing warning missing warning missing warning missing warning missing warning missing warning missing warning missing warning missing warning missing warning summary additional impacted coverage main coverage flag coverage carried forward coverage wo shown click find umbrella view full report sentry feedback report share,issue,negative,negative,negative,negative,negative,negative
1871839517,"Thanks for your contribution :heart:
:broken_heart: Unfortunately, this PR has one or more **bad commit messages**, it can not be merged. To fix this problem, please refer to:
- [Commit Message Guideline for the First Time Contributor](https://github.com/jina-ai/jina/issues/553)
- [Contributing Guideline](https://github.com/jina-ai/jina/blob/master/CONTRIBUTING.md)
This message will be deleted automatically when the commit messages get fixed.",thanks contribution heart unfortunately one bad commit fix problem please refer commit message guideline first time contributor guideline message automatically commit get fixed,issue,negative,negative,negative,negative,negative,negative
1868738715,"> > > > > > > Hello, I recently noticed that often the bucket from which you can download the models gets blocked and opens after a while as you can see here #931 and [here](https://github.com/immich-app/immich/issues/4117#issue-1899687244).
> > > > > > > Today the block of the bucket was removed, so I decided to download all the models to my computer in order to have a backup of them when needed, however I thought how I can give back a contribution to the community and to this great project, I want to upload all the clip_onnx model files to the Telegram channel So even if the bucket is blocked there will still be an additional download source for all the models that appear in the file openclip_model.py
> > > > > > > Of course, if I do it, I will link to the original page on GitHub, a link to the website and whatever you tell me is necessary
> > > > > > > **I wanted to ask if this is something that can be done or if it might violate any rights?**
> > > > > > > Thanks!
> > > > > > 
> > > > > > 
> > > > > > @aviv926 It would better if you close this issue if it's solved?
> > > > > 
> > > > > 
> > > > > Is there a copyright issue with what I want to do?
> > > > 
> > > > 
> > > > you can talk with @JoanFM or @samsja , hope they will help you to get the answer of your question.
> > > 
> > > 
> > > Do you mean waiting for their answer here or should I contact them?
> > 
> > 
> > You should send message on the discord server of Jina.
> 
> I'll try again, I tried before I opened the discussion here, but I didn't get any response in Discord. thanks for help.

just the mention the maintainer in the message to get responses from them.
Thank you :)",hello recently often bucket blocked see today block bucket removed decided computer order backup however thought give back contribution community great project want model telegram channel even bucket blocked still additional source appear file course link original page link whatever tell necessary ask something done might violate thanks would better close issue copyright issue want talk hope help get answer question mean waiting answer contact send message discord server jina try tried discussion get response discord thanks help mention maintainer message get thank,issue,positive,positive,positive,positive,positive,positive
1868597909,"> > > > > > Hello, I recently noticed that often the bucket from which you can download the models gets blocked and opens after a while as you can see here #931 and [here](https://github.com/immich-app/immich/issues/4117#issue-1899687244).
> > > > > > Today the block of the bucket was removed, so I decided to download all the models to my computer in order to have a backup of them when needed, however I thought how I can give back a contribution to the community and to this great project, I want to upload all the clip_onnx model files to the Telegram channel So even if the bucket is blocked there will still be an additional download source for all the models that appear in the file openclip_model.py
> > > > > > Of course, if I do it, I will link to the original page on GitHub, a link to the website and whatever you tell me is necessary
> > > > > > **I wanted to ask if this is something that can be done or if it might violate any rights?**
> > > > > > Thanks!
> > > > > 
> > > > > 
> > > > > @aviv926 It would better if you close this issue if it's solved?
> > > > 
> > > > 
> > > > Is there a copyright issue with what I want to do?
> > > 
> > > 
> > > you can talk with @JoanFM or @samsja , hope they will help you to get the answer of your question.
> > 
> > 
> > Do you mean waiting for their answer here or should I contact them?
> 
> You should send message on the discord server of Jina.

I'll try again, I tried before I opened the discussion here, but I didn't get any response in Discord.
thanks for help.",hello recently often bucket blocked see today block bucket removed decided computer order backup however thought give back contribution community great project want model telegram channel even bucket blocked still additional source appear file course link original page link whatever tell necessary ask something done might violate thanks would better close issue copyright issue want talk hope help get answer question mean waiting answer contact send message discord server jina try tried discussion get response discord thanks help,issue,positive,positive,positive,positive,positive,positive
1868473094,"> > > > > Hello, I recently noticed that often the bucket from which you can download the models gets blocked and opens after a while as you can see here #931 and [here](https://github.com/immich-app/immich/issues/4117#issue-1899687244).
> > > > > Today the block of the bucket was removed, so I decided to download all the models to my computer in order to have a backup of them when needed, however I thought how I can give back a contribution to the community and to this great project, I want to upload all the clip_onnx model files to the Telegram channel So even if the bucket is blocked there will still be an additional download source for all the models that appear in the file openclip_model.py
> > > > > Of course, if I do it, I will link to the original page on GitHub, a link to the website and whatever you tell me is necessary
> > > > > **I wanted to ask if this is something that can be done or if it might violate any rights?**
> > > > > Thanks!
> > > > 
> > > > 
> > > > @aviv926 It would better if you close this issue if it's solved?
> > > 
> > > 
> > > Is there a copyright issue with what I want to do?
> > 
> > 
> > you can talk with @JoanFM or @samsja , hope they will help you to get the answer of your question.
> 
> Do you mean waiting for their answer here or should I contact them?

You should send message on the discord server of Jina.",hello recently often bucket blocked see today block bucket removed decided computer order backup however thought give back contribution community great project want model telegram channel even bucket blocked still additional source appear file course link original page link whatever tell necessary ask something done might violate thanks would better close issue copyright issue want talk hope help get answer question mean waiting answer contact send message discord server jina,issue,positive,positive,positive,positive,positive,positive
1868346588,"> > > > Hello, I recently noticed that often the bucket from which you can download the models gets blocked and opens after a while as you can see here #931 and [here](https://github.com/immich-app/immich/issues/4117#issue-1899687244).
> > > > Today the block of the bucket was removed, so I decided to download all the models to my computer in order to have a backup of them when needed, however I thought how I can give back a contribution to the community and to this great project, I want to upload all the clip_onnx model files to the Telegram channel So even if the bucket is blocked there will still be an additional download source for all the models that appear in the file openclip_model.py
> > > > Of course, if I do it, I will link to the original page on GitHub, a link to the website and whatever you tell me is necessary
> > > > **I wanted to ask if this is something that can be done or if it might violate any rights?**
> > > > Thanks!
> > > 
> > > 
> > > @aviv926 It would better if you close this issue if it's solved?
> > 
> > 
> > Is there a copyright issue with what I want to do?
> 
> you can talk with @JoanFM or @samsja , hope they will help you to get the answer of your question.

Do you mean waiting for their answer here or should I contact them?",hello recently often bucket blocked see today block bucket removed decided computer order backup however thought give back contribution community great project want model telegram channel even bucket blocked still additional source appear file course link original page link whatever tell necessary ask something done might violate thanks would better close issue copyright issue want talk hope help get answer question mean waiting answer contact,issue,positive,positive,positive,positive,positive,positive
1868324714,"> > > Hello, I recently noticed that often the bucket from which you can download the models gets blocked and opens after a while as you can see here #931 and [here](https://github.com/immich-app/immich/issues/4117#issue-1899687244).
> > > Today the block of the bucket was removed, so I decided to download all the models to my computer in order to have a backup of them when needed, however I thought how I can give back a contribution to the community and to this great project, I want to upload all the clip_onnx model files to the Telegram channel So even if the bucket is blocked there will still be an additional download source for all the models that appear in the file openclip_model.py
> > > Of course, if I do it, I will link to the original page on GitHub, a link to the website and whatever you tell me is necessary
> > > **I wanted to ask if this is something that can be done or if it might violate any rights?**
> > > Thanks!
> > 
> > 
> > @aviv926 It would better if you close this issue if it's solved?
> 
> Is there a copyright issue with what I want to do?

you can talk with @JoanFM or @samsja , hope they will help you to get the answer of your question.",hello recently often bucket blocked see today block bucket removed decided computer order backup however thought give back contribution community great project want model telegram channel even bucket blocked still additional source appear file course link original page link whatever tell necessary ask something done might violate thanks would better close issue copyright issue want talk hope help get answer question,issue,positive,positive,positive,positive,positive,positive
1868323100,"> > Hello, I recently noticed that often the bucket from which you can download the models gets blocked and opens after a while as you can see here #931 and [here](https://github.com/immich-app/immich/issues/4117#issue-1899687244).
> > 
> > Today the block of the bucket was removed, so I decided to download all the models to my computer in order to have a backup of them when needed, however I thought how I can give back a contribution to the community and to this great project, I want to upload all the clip_onnx model files to the Telegram channel So even if the bucket is blocked there will still be an additional download source for all the models that appear in the file openclip_model.py
> > 
> > Of course, if I do it, I will link to the original page on GitHub, a link to the website and whatever you tell me is necessary
> > 
> > **I wanted to ask if this is something that can be done or if it might violate any rights?**
> > 
> > Thanks!
> 
> @aviv926 It would better if you close this issue if it's solved?

Is there a copyright issue with what I want to do?",hello recently often bucket blocked see today block bucket removed decided computer order backup however thought give back contribution community great project want model telegram channel even bucket blocked still additional source appear file course link original page link whatever tell necessary ask something done might violate thanks would better close issue copyright issue want,issue,positive,positive,positive,positive,positive,positive
1867901606,"> Hello, I recently noticed that often the bucket from which you can download the models gets blocked and opens after a while as you can see here #931 and [here](https://github.com/immich-app/immich/issues/4117#issue-1899687244).
> 
> Today the block of the bucket was removed, so I decided to download all the models to my computer in order to have a backup of them when needed, however I thought how I can give back a contribution to the community and to this great project, I want to upload all the clip_onnx model files to the Telegram channel So even if the bucket is blocked there will still be an additional download source for all the models that appear in the file openclip_model.py
> 
> Of course, if I do it, I will link to the original page on GitHub, a link to the website and whatever you tell me is necessary
> 
> **I wanted to ask if this is something that can be done or if it might violate any rights?**
> 
> Thanks!

@aviv926 It would better if you close this issue if it's solved?",hello recently often bucket blocked see today block bucket removed decided computer order backup however thought give back contribution community great project want model telegram channel even bucket blocked still additional source appear file course link original page link whatever tell necessary ask something done might violate thanks would better close issue,issue,negative,positive,positive,positive,positive,positive
1863028436,"## [Codecov](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/942?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
All modified and coverable lines are covered by tests :white_check_mark:
> Comparison is base [(`c7e84a4`)](https://app.codecov.io/gh/jina-ai/clip-as-service/commit/c7e84a49a585edfae5fa26b91d302c1ed793f725?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) 65.62% compared to head [(`1c17697`)](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/942?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) 71.62%.
> Report is 1 commits behind head on main.


<details><summary>Additional details and impacted files</summary>


```diff
@@            Coverage Diff             @@
##             main     #942      +/-   ##
==========================================
+ Coverage   65.62%   71.62%   +6.00%     
==========================================
  Files          23       23              
  Lines        1565     1565              
==========================================
+ Hits         1027     1121      +94     
+ Misses        538      444      -94     
```

| [Flag](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/942/flags?src=pr&el=flags&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [cas](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/942/flags?src=pr&el=flag&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | `71.62% <ø> (+6.00%)` | :arrow_up: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.


</details>

[:umbrella: View full report in Codecov by Sentry](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/942?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).   
:loudspeaker: Have feedback on the report? [Share it here](https://about.codecov.io/codecov-pr-comment-feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report covered comparison base head report behind head main summary additional impacted coverage main coverage flag coverage carried forward coverage wo shown click find umbrella view full report sentry feedback report share,issue,negative,negative,negative,negative,negative,negative
1851159935,I ran into the same ERROR on both my local machine and Colab. Have you solve it and how?,ran error local machine solve,issue,negative,neutral,neutral,neutral,neutral,neutral
1813717967,"To convert a PyTorch model to ONNX, you can follow the step-by-step guide provided by PyTorch. Check out their tutorial for converting a super-resolution model as an example: [PyTorch to ONNX Conversion](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html).",convert model follow guide provided check tutorial converting model example conversion,issue,negative,neutral,neutral,neutral,neutral,neutral
1807419402,"hi, You can refer to this link. it works for me。

https://github.com/jina-ai/clip-as-service/issues/937#issuecomment-1791025746",hi refer link work,issue,negative,neutral,neutral,neutral,neutral,neutral
1803975720,"**1.**

* 使用 docker.io/jinaai/clip-server:0.8.2
* Use docker.io/jinaai/clip-server:0.8.2

**2.**

* 需要把如上镜像作为基础，再重新打一个镜像出来即可
* You need to use the above image as the base and then build a new image.

**3.**

* Dockerfile 添加 RUN pip install -i  cn_clip，其实已经支持 https://github.com/jina-ai/clip-as-service/blob/main/server/clip_server/model/cnclip_model.py#L10
* Add RUN pip install -i  cn_clip to the Dockerfile. cn_clip is already supported: https://github.com/jina-ai/clip-as-service/blob/main/server/clip_server/model/cnclip_model.py#L10

**4.**

* 配置中添加 CN-CLIP/ViT-B-16
* Add CN-CLIP/ViT-B-16 to the configuration.
",use need use image base build new image run pip install add run pip install already add configuration,issue,negative,negative,negative,negative,negative,negative
1791025746,"Hi,

It seems that downgrading open-clip-torch allow to run clip_server
```
pip install open-clip-torch==2.22.0
```

Best Regards,
Michel.
",hi allow run pip install best,issue,positive,positive,positive,positive,positive,positive
1790309914,"justin@DESKTOP-ROS689L:~$ pip show clip_server
Name: clip-server
Version: 0.8.2
Summary: Embed images and sentences into fixed-length vectors via CLIP
Home-page: https://github.com/jina-ai/clip-as-service
Author: Jina AI
Author-email: hello@jina.ai
License: Apache 2.0
Location: /usr/local/src/python37/lib/python3.7/site-packages
Requires: docarray, ftfy, jina, open-clip-torch, prometheus-client, regex, torch, torchvision
Required-by:",pip show name version summary embed via clip author jina ai hello license apache location jina torch,issue,negative,neutral,neutral,neutral,neutral,neutral
1790307473,"justin@DESKTOP-ROS689L:~$ python3.7 -m jina -vf
- jina 3.22.4
- docarray 0.21.1
- jcloud 0.3
- jina-hubble-sdk 0.39.0
- jina-proto 0.1.27
- protobuf 4.24.4
- proto-backend upb
- grpcio 1.47.5
- pyyaml 6.0.1
- python 3.7.4
- platform Linux
- platform-release 4.4.0-22621-Microsoft
- platform-version #2506-Microsoft Fri Jan 01 08:00:00 PST 2016
- architecture x86_64
- processor x86_64
- uid 238300918521988
- session-id b6b67638-795c-11ee-b03d-d8bbc1992084
- uptime 2023-11-02T16:49:20.291171
- ci-vendor (unset)
- internal False
* JINA_DEFAULT_HOST (unset)
* JINA_DEFAULT_TIMEOUT_CTRL (unset)
* JINA_DEPLOYMENT_NAME (unset)
* JINA_DISABLE_UVLOOP (unset)
* JINA_EARLY_STOP (unset)
* JINA_FULL_CLI (unset)
* JINA_GATEWAY_IMAGE (unset)
* JINA_GRPC_RECV_BYTES (unset)
* JINA_GRPC_SEND_BYTES (unset)
* JINA_HUB_NO_IMAGE_REBUILD (unset)
* JINA_LOG_CONFIG (unset)
* JINA_LOG_LEVEL (unset)
* JINA_LOG_NO_COLOR (unset)
* JINA_MP_START_METHOD (unset)
* JINA_OPTOUT_TELEMETRY (unset)
* JINA_RANDOM_PORT_MAX (unset)
* JINA_RANDOM_PORT_MIN (unset)
* JINA_LOCKS_ROOT (unset)
* JINA_K8S_ACCESS_MODES (unset)
* JINA_K8S_STORAGE_CLASS_NAME (unset)
* JINA_K8S_STORAGE_CAPACITY (unset)
* JINA_STREAMER_ARGS (unset)",python jina jina python platform pst architecture processor unset internal false unset unset unset unset unset unset unset unset unset unset unset unset unset unset unset unset unset unset unset unset unset unset,issue,negative,negative,negative,negative,negative,negative
1790306757,"I have the error too:
justin@DESKTOP-ROS689L:~$ python3.7 -m clip_server
ERROR  clip_t/rep-0@14397 fail to load file dependency                                                                               [11/02/23 16:47:37]
ERROR  clip_t/rep-0@14397 FileNotFoundError('can not find clip_server.executors.clip_torch') during 'WorkerRuntime' initialization
        add ""--quiet-error"" to suppress the exception details
       Traceback (most recent call last):
         File ""/usr/local/src/python37/lib/python3.7/site-packages/jina/serve/executors/run.py"", line 144, in run
           signal_handlers_installed_event=is_signal_handlers_installed
         File ""/usr/local/src/python37/lib/python3.7/site-packages/jina/serve/runtimes/asyncio.py"", line 92, in __init__
           self._loop.run_until_complete(self.async_setup())
         File ""/usr/local/src/python37/lib/python3.7/asyncio/base_events.py"", line 579, in run_until_complete
           return future.result()
         File ""/usr/local/src/python37/lib/python3.7/site-packages/jina/serve/runtimes/asyncio.py"", line 309, in async_setup
           self.server = self._get_server()
         File ""/usr/local/src/python37/lib/python3.7/site-packages/jina/serve/runtimes/asyncio.py"", line 221, in _get_server
           proxy=getattr(self.args, 'proxy', None),
         File ""/usr/local/src/python37/lib/python3.7/site-packages/jina/serve/runtimes/servers/grpc.py"", line 34, in __init__
           super().__init__(**kwargs)
         File ""/usr/local/src/python37/lib/python3.7/site-packages/jina/serve/runtimes/servers/__init__.py"", line 63, in __init__
           ] = (req_handler or self._get_request_handler())
         File ""/usr/local/src/python37/lib/python3.7/site-packages/jina/serve/runtimes/servers/__init__.py"", line 100, in
       _get_request_handler
           works_as_load_balancer=self.works_as_load_balancer,
         File ""/usr/local/src/python37/lib/python3.7/site-packages/jina/serve/runtimes/worker/request_handling.py"", line 142, in
       __init__
           meter_provider=meter_provider,
         File ""/usr/local/src/python37/lib/python3.7/site-packages/jina/serve/runtimes/worker/request_handling.py"", line 392, in
       _load_executor
           extra_search_paths=self.args.extra_search_paths,
         File ""/usr/local/src/python37/lib/python3.7/site-packages/jina/jaml/__init__.py"", line 746, in load_config
           else _extra_search_paths,
         File ""/usr/local/src/python37/lib/python3.7/site-packages/jina/jaml/helper.py"", line 285, in load_py_modules
           PathImporter.add_modules(*mod)
         File ""/usr/local/src/python37/lib/python3.7/site-packages/jina/importer.py"", line 182, in add_modules
           _path_import(complete_path(m))
         File ""/usr/local/src/python37/lib/python3.7/site-packages/jina/jaml/helper.py"", line 229, in complete_path
           raise FileNotFoundError(f'can not find {path}')
       FileNotFoundError: can not find clip_server.executors.clip_torch
ERROR  Flow@14373 An exception occurred:                                                                                             [11/02/23 16:47:37]
ERROR  Flow@14373 Flow is aborted due to ['clip_t', 'gateway'] can not be started.
E1102 16:47:37.327415100   14398 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {""created"":""@1698914857.327366700"",""description"":""Protocol not available"",""errno"":92,""file"":""src/core/lib/iomgr/socket_utils_common_posix.cc"",""file_line"":202,""os_error"":""Protocol not available"",""syscall"":""getsockopt(SO_REUSEPORT)""}
INFO   gateway/rep-0@14398 start server bound to 0.0.0.0:51000                                                                       [11/02/23 16:47:37]
Traceback (most recent call last):
  File ""/usr/local/src/python37/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/local/src/python37/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/usr/local/src/python37/lib/python3.7/site-packages/clip_server/__main__.py"", line 25, in <module>
    with f:
  File ""/usr/local/src/python37/lib/python3.7/site-packages/jina/orchestrate/orchestrator.py"", line 14, in __enter__
    return self.start()
  File ""/usr/local/src/python37/lib/python3.7/site-packages/jina/orchestrate/flow/builder.py"", line 33, in arg_wrapper
    return func(self, *args, **kwargs)
  File ""/usr/local/src/python37/lib/python3.7/site-packages/jina/orchestrate/flow/base.py"", line 1846, in start
    self._wait_until_all_ready()
  File ""/usr/local/src/python37/lib/python3.7/site-packages/jina/orchestrate/flow/base.py"", line 2012, in _wait_until_all_ready
    raise RuntimeFailToStart
jina.excepts.RuntimeFailToStart
",error python error fail load file dependency error find add suppress exception recent call last file line run file line file line return file line file line none file line super file line file line file line file line file line else file line file line file line raise find path find error flow exception error flow flow aborted due check description protocol available file protocol available start server bound recent call last file line file line code file line module file line return file line return self file line start file line raise,issue,negative,positive,neutral,neutral,positive,positive
1772428428,"Having the same issue:
 
Failed to download https://clip-as-service.s3.us-east-2.amazonaws.com/models-436c69702d61732d53657276696365/onnx/ViT-H-14-laion2b-s32b-b79k/textual.onnx with <HTTPError 403: 'Forbidden'> at the 0th 
attempt
Failed to download https://clip-as-service.s3.us-east-2.amazonaws.com/models-436c69702d61732d53657276696365/onnx/ViT-H-14-laion2b-s32b-b79k/textual.onnx with <HTTPError 403: 'Forbidden'> at the 1th 
attempt
Failed to download https://clip-as-service.s3.us-east-2.amazonaws.com/models-436c69702d61732d53657276696365/onnx/ViT-H-14-laion2b-s32b-b79k/textual.onnx with <HTTPError 403: 'Forbidden'> at the 2th 
attempt",issue th attempt th attempt th attempt,issue,negative,neutral,neutral,neutral,neutral,neutral
1772108919,"Good can you also remove https://clip-as-service.jina.ai/hosting/by-jina/ , https://clip-as-service.jina.ai/ and maybe other place in the docs? 🙂 阿里嘎多",good also remove maybe place,issue,negative,positive,positive,positive,positive,positive
1756136755,"## [Codecov](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/934?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
All modified lines are covered by tests :white_check_mark:
> Comparison is base [(`c7e84a4`)](https://app.codecov.io/gh/jina-ai/clip-as-service/commit/c7e84a49a585edfae5fa26b91d302c1ed793f725?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) 65.62% compared to head [(`7a6eb1b`)](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/934?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) 61.46%.

<details><summary>Additional details and impacted files</summary>


```diff
@@            Coverage Diff             @@
##             main     #934      +/-   ##
==========================================
- Coverage   65.62%   61.46%   -4.16%     
==========================================
  Files          23       23              
  Lines        1565     1565              
==========================================
- Hits         1027      962      -65     
- Misses        538      603      +65     
```

| [Flag](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/934/flags?src=pr&el=flags&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [cas](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/934/flags?src=pr&el=flag&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | `61.46% <ø> (-4.16%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

[see 9 files with indirect coverage changes](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/934/indirect-changes?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)


</details>

[:umbrella: View full report in Codecov by Sentry](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/934?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).   
:loudspeaker: Have feedback on the report? [Share it here](https://about.codecov.io/codecov-pr-comment-feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report covered comparison base head summary additional impacted coverage main coverage flag coverage carried forward coverage wo shown click find see indirect coverage umbrella view full report sentry feedback report share,issue,negative,negative,neutral,neutral,negative,negative
1724888590,"I have similar issue but seems related to some permission issue because I see some `403` when downloading the model

```
DEBUG  gateway/rep-0@32033 starting warmup task for deployment clip_o                                         
DEBUG  gateway/rep-0@32033 server bound to 0.0.0.0:51000 started                                              
DEBUG  gateway/rep-0@32033 GRPC server setup successful                                                       
objc[32033]: +[__NSTimeZone initialize] may have been in progress in another thread when fork() was called.
objc[32033]: +[__NSTimeZone initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.
DEBUG  gateway/rep-0@32023 ready and listening                                             [09/18/23 23:12:13]
DEBUG  gateway-replica-set@32023 ReplicaSet started successfully                                              
DEBUG  gateway@32023 Deployment started successfully                                       [09/18/23 23:12:13]
UserWarning: `docs` annotation must be a type hint, got DocumentArray instead, you should maybe remove the string annotation. Default valueDocumentArray will be used instead. (raised from /opt/homebrew/lib/python3.11/site-packages/jina-3.20.3-py3.11-macosx-13-arm64.egg/jina/serve/executors/__init__.py:265)
Failed to download 
https://clip-as-service.s3.us-east-2.amazonaws.com/models-436c69702d61732d53657276696365/onnx/ViT-B-32/textual
.onnx with <HTTPError 403: 'Forbidden'> at the 0th attempt
Failed to download 
https://clip-as-service.s3.us-east-2.amazonaws.com/models-436c69702d61732d53657276696365/onnx/ViT-B-32/textual
.onnx with <HTTPError 403: 'Forbidden'> at the 1th attempt
Failed to download 
https://clip-as-service.s3.us-east-2.amazonaws.com/models-436c69702d61732d53657276696365/onnx/ViT-B-32/textual
.onnx with <HTTPError 403: 'Forbidden'> at the 2th attempt
  textual.onnx 0.0% • 0/100 bytes • ? • -:--:--
                                               
CRITI… clip_o/rep-0@32032 can not load the executor from {""jtype"": ""CLIPEncoder"", ""metas"": [09/18/23 23:12:15]
       {""py_modules"": [""clip_server.executors.clip_onnx""]}}                                                   
ERROR  clip_o/rep-0@32032 RuntimeError('Failed to download                                                    
       https://clip-as-service.s3.us-east-2.amazonaws.com/models-436c69702d61732d53657276…                    
       within retry limit 3') during 'WorkerRuntime' initialization                                           
        add ""--quiet-error"" to suppress the exception details                                                 
       Traceback (most recent call last):                                                                     
         File                                                                                                 
       ""/opt/homebrew/lib/python3.11/site-packages/jina-3.20.3-py3.11-macosx-13-arm64.egg…                    
       line 140, in run                                                                                       
           runtime = AsyncNewLoopRuntime(                                                                     
                     ^^^^^^^^^^^^^^^^^^^^                                                                     
         File                                                                                                 
       ""/opt/homebrew/lib/python3.11/site-packages/jina-3.20.3-py3.11-macosx-13-arm64.egg…                    
       line 90, in __init__                                                                                   
           self._loop.run_until_complete(self.async_setup())                                                  
         File                                                                                                 
       ""/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11…                    
       line 653, in run_until_complete                                                                        
           return future.result()                                                                             
                  ^^^^^^^^^^^^^^^                                                                             
         File                                                                                                 
       ""/opt/homebrew/lib/python3.11/site-packages/jina-3.20.3-py3.11-macosx-13-arm64.egg…                    
       line 270, in async_setup                                                                               
           self.server = self._get_server()                                                                   
                         ^^^^^^^^^^^^^^^^^^                                                                   
         File                                                                                                 
       ""/opt/homebrew/lib/python3.11/site-packages/jina-3.20.3-py3.11-macosx-13-arm64.egg…                    
       line 192, in _get_server                                                                               
           return GRPCServer(                                                                                 
                  ^^^^^^^^^^^                                                                                 
         File                                                                                                 
       ""/opt/homebrew/lib/python3.11/site-packages/jina-3.20.3-py3.11-macosx-13-arm64.egg…                    
       line 34, in __init__                                                                                   
           super().__init__(**kwargs)                                                                         
         File                                                                                                 
       ""/opt/homebrew/lib/python3.11/site-packages/jina-3.20.3-py3.11-macosx-13-arm64.egg…                    
       line 56, in __init__                                                                                   
           self._request_handler = req_handler or self._get_request_handler()                                 
                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^                                 
         File                                                                                                 
       ""/opt/homebrew/lib/python3.11/site-packages/jina-3.20.3-py3.11-macosx-13-arm64.egg…                    
       line 81, in _get_request_handler                                                                       
           return self.req_handler_cls(                                                                       
                  ^^^^^^^^^^^^^^^^^^^^^                                                                       
         File                                                                                                 
       ""/opt/homebrew/lib/python3.11/site-packages/jina-3.20.3-py3.11-macosx-13-arm64.egg…                    
       line 139, in __init__                                                                                  
           self._load_executor(                                                                               
         File                                                                                                 
       ""/opt/homebrew/lib/python3.11/site-packages/jina-3.20.3-py3.11-macosx-13-arm64.egg…                    
       line 354, in _load_executor                                                                            
           self._executor: BaseExecutor = BaseExecutor.load_config(                                           
                                          ^^^^^^^^^^^^^^^^^^^^^^^^^                                           
         File                                                                                                 
       ""/opt/homebrew/lib/python3.11/site-packages/jina-3.20.3-py3.11-macosx-13-arm64.egg…                    
       line 792, in load_config                                                                               
           obj = JAML.load(tag_yml, substitute=False, runtime_args=runtime_args)                              
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                              
         File                                                                                                 
       ""/opt/homebrew/lib/python3.11/site-packages/jina-3.20.3-py3.11-macosx-13-arm64.egg…                    
       line 174, in load                                                                                      
           r = yaml.load(stream, Loader=get_jina_loader_with_runtime(runtime_args))                           
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                           
         File ""/opt/homebrew/lib/python3.11/site-packages/yaml/__init__.py"", line 81, in                      
       load                                                                                                   
           return loader.get_single_data()                                                                    
                  ^^^^^^^^^^^^^^^^^^^^^^^^                                                                    
         File ""/opt/homebrew/lib/python3.11/site-packages/yaml/constructor.py"", line 51,                      
       in get_single_data                                                                                     
           return self.construct_document(node)                                                               
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                               
         File ""/opt/homebrew/lib/python3.11/site-packages/yaml/constructor.py"", line 55,                      
       in construct_document                                                                                  
           data = self.construct_object(node)                                                                 
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                 
         File ""/opt/homebrew/lib/python3.11/site-packages/yaml/constructor.py"", line 100,                     
       in construct_object                                                                                    
           data = constructor(self, node)                                                                     
                  ^^^^^^^^^^^^^^^^^^^^^^^                                                                     
         File                                                                                                 
       ""/opt/homebrew/lib/python3.11/site-packages/jina-3.20.3-py3.11-macosx-13-arm64.egg…                    
       line 582, in _from_yaml                                                                                
           return get_parser(cls, version=data.get('version', None)).parse(                                   
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                   
         File                                                                                                 
       ""/opt/homebrew/lib/python3.11/site-packages/jina-3.20.3-py3.11-macosx-13-arm64.egg…                    
       line 46, in parse                                                                                      
           obj = cls(                                                                                         
                 ^^^^                                                                                         
         File                                         
```",similar issue related permission issue see model starting task deployment server bound server setup successful initialize may progress another thread fork initialize may progress another thread fork safely call ignore fork child process instead set ready listening successfully gateway deployment successfully annotation must type hint got instead maybe remove string annotation default used instead raised th attempt th attempt th attempt load executor error within retry limit add suppress exception recent call last file line run file line file line return file line file line return file line super file line file line return file line file line file line file line load stream file line load return file line return node file line data node file line data constructor self node file line return none file line parse file,issue,positive,positive,positive,positive,positive,positive
1724885074,"Same issue for me.

RuntimeError: Failed to download
https://clip-as-service.s3.us-east-2.amazonaws.com/models-436c69702d61732d53657276696365/onnx/ViT-B-32/textual.onnx within retry limit 3
...
RuntimeError: Failed to download
https://clip-as-service.s3.us-east-2.amazonaws.com/models-436c69702d61732d53657276696365/onnx/ViT-B-32/visual.onnx within retry limit 3
...",issue within retry limit within retry limit,issue,negative,neutral,neutral,neutral,neutral,neutral
1724315933,"> I downloaded all models but I cannot find the needed textual.onnx file in the models 

you can try to convert it to onnx with https://gist.github.com/patharanordev/cd68e942499b6c0f2c259667f07b223f

and I want to try download the onnx files from https://huggingface.co/Marqo/onnx-open_clip-ViT-L-14/tree/main",find file try convert want try,issue,negative,neutral,neutral,neutral,neutral,neutral
1724268427,"> Downloading models and placing them directly in the cache directory works to bypass the download.
> 
> OpenAI models: https://github.com/openai/CLIP/blob/a1d071733d7111c9c014f024669f959182114e33/clip/clip.py#L30

I downloaded all models but I cannot find the needed textual.onnx file in the models",directly cache directory work bypass find file,issue,negative,positive,neutral,neutral,positive,positive
1724121703,"Downloading models and placing them directly in the cache directory works to bypass the download.

OpenAI models: https://github.com/openai/CLIP/blob/a1d071733d7111c9c014f024669f959182114e33/clip/clip.py#L30",directly cache directory work bypass,issue,negative,positive,neutral,neutral,positive,positive
1723522816,"This is affecting production services, would very much appreciate any response.",affecting production would much appreciate response,issue,negative,positive,positive,positive,positive,positive
1723493503,"Having the same issue. Looks like a policy issue on the bucket.

Example: https://clip-as-service.s3.us-east-2.amazonaws.com/models/torch/ViT-L-14.pt",issue like policy issue bucket example,issue,negative,neutral,neutral,neutral,neutral,neutral
1708835929,"## [Codecov](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/930?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
Patch coverage has no change and project coverage change: **`-5.31%`** :warning:
> Comparison is base [(`c7e84a4`)](https://app.codecov.io/gh/jina-ai/clip-as-service/commit/c7e84a49a585edfae5fa26b91d302c1ed793f725?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) 65.62% compared to head [(`50d5429`)](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/930?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) 60.31%.

<details><summary>Additional details and impacted files</summary>


```diff
@@            Coverage Diff             @@
##             main     #930      +/-   ##
==========================================
- Coverage   65.62%   60.31%   -5.31%     
==========================================
  Files          23       23              
  Lines        1565     1565              
==========================================
- Hits         1027      944      -83     
- Misses        538      621      +83     
```

| [Flag](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/930/flags?src=pr&el=flags&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [cas](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/930/flags?src=pr&el=flag&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | `60.31% <ø> (-5.31%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

[see 7 files with indirect coverage changes](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/930/indirect-changes?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)


</details>

[:umbrella: View full report in Codecov by Sentry](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/930?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).   
:loudspeaker: Have feedback on the report? [Share it here](https://about.codecov.io/codecov-pr-comment-feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report patch coverage change project coverage change warning comparison base head summary additional impacted coverage main coverage flag coverage carried forward coverage wo shown click find see indirect coverage umbrella view full report sentry feedback report share,issue,negative,negative,neutral,neutral,negative,negative
1702579135,"> 

i didn't find it in README.md，could you supply a latest example in another language?",find supply latest example another language,issue,negative,positive,positive,positive,positive,positive
1679958469,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`.

If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",wo notify release get touch new version available rather skip next major minor version let know ignore major version ignore minor version change mind resolve,issue,negative,positive,neutral,neutral,positive,positive
1675307021,"## [Codecov](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/927?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#927](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/927?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (f9ca192) into [main](https://app.codecov.io/gh/jina-ai/clip-as-service/commit/c7e84a49a585edfae5fa26b91d302c1ed793f725?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (c7e84a4) will **increase** coverage by `6.00%`.
> The diff coverage is `n/a`.

```diff
@@            Coverage Diff             @@
##             main     #927      +/-   ##
==========================================
+ Coverage   65.62%   71.62%   +6.00%     
==========================================
  Files          23       23              
  Lines        1565     1565              
==========================================
+ Hits         1027     1121      +94     
+ Misses        538      444      -94     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `71.62% <ø> (+6.00%)` | :arrow_up: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

[see 4 files with indirect coverage changes](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/927/indirect-changes?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)

",report main increase coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find see indirect coverage,issue,negative,positive,positive,positive,positive,positive
1657453384,"> Thanks for the answer! I reported it also to fix for new users.
> 
> docker run -p 51009:51000 -v $HOME/.cache:/home/cas/.cache --gpus all jinaai/clip-server:master-onnx onnx-flow.yml
> 
> the encodes extracted from above server and when you remove --gpus all parameter (CPU) encodes are different. How can I avoid it?

What do you mean the encodes are different? Can you show some examples?
The fp16 will lose some precisions by natural, some slight differences would be expected",thanks answer also fix new docker run extracted server remove parameter different avoid mean different show lose natural slight would,issue,negative,negative,neutral,neutral,negative,negative
1657450568,"Yes the base image from Nvidia with cuda == 11.6.0 was deleted, please use 11.6.2 instead

> Problem seems the cuda image version.
> change it to `ARG CUDA_VERSION=11.6.2` that did work for me... kind regards

",yes base image please use instead problem image version change work kind,issue,positive,negative,negative,negative,negative,negative
1656586166,"Hi pasa, i'am just guessing because i just started using this project. I think there could be several reasons for that:

-  maybe the order of your input data is changing
-  maybe cpu mode does some different preprocessing on your input(fp16 vs fp32).  

but i wonder if you even get the same results on 2 different instances both running with gpu.

kind regards",hi guessing project think could several maybe order input data maybe mode different input wonder even get different running kind,issue,positive,positive,positive,positive,positive,positive
1656306058,"Thanks for the answer! I reported it also to fix for new users.

docker run -p 51009:51000 -v $HOME/.cache:/home/cas/.cache --gpus all jinaai/clip-server:master-onnx onnx-flow.yml

the encodes extracted from above server and when you remove --gpus all parameter (CPU) encodes are different. How can I avoid it?",thanks answer also fix new docker run extracted server remove parameter different avoid,issue,negative,positive,positive,positive,positive,positive
1655870848,"Problem seems the cuda image version. 
change it to `ARG CUDA_VERSION=11.6.2` that did work for me... kind regards",problem image version change work kind,issue,negative,positive,positive,positive,positive,positive
1655489115,"@ZiniuYu  Hello again! 

Do you know how can get ""clip-server:latest-onnx""'s dockerfile?  Pulling repo and build & run is not latest and producing different encoding values.",hello know get build run latest different,issue,negative,positive,positive,positive,positive,positive
1651907630,"found the problem, the docs are using the wrong label in the sample:

print(c.rank(da)['@m', ['**text**', 'scores__clip_score__value']])
print(c.rank(da)['@m', ['**uri**', 'scores__clip_score__value']]) 

**text** --> **uri**

",found problem wrong label sample print da text print da text,issue,negative,negative,negative,negative,negative,negative
1651726450,"Hi,

i have the same issue using the sample from the docs:
_""Given sentence, rank images""_
https://clip-as-service.jina.ai/user-guides/client/#rank-api

**code**:


```
from docarray import Document
from client.clip_client import Client

c = Client('grpc://172.17.0.2:51000')
# c.profile()

d = Document(
    text='a photo of a table',
    matches=[
        Document(uri='imagexyzsdfdsf'),
        Document(uri='imagexyz45435'),
        Document(uri='imagexyz68768'),
        Document(uri='imagexyz7865756'),
    ],
)
r = c.rank([d])
```
**replace image urls

**result**:

`[['', '', '', ''], [0.9542718529701233, 0.0419040322303772, 0.002422966528683901, 0.0014011985622346401]]`

",hi issue sample given sentence rank code import document import client client document photo table document document document document replace image result,issue,negative,negative,negative,negative,negative,negative
1641316398,"If you prefer to self-host Clip models, you can refer to the documentation at https://clip-as-service.jina.ai/user-guides/server/. With this approach, there's no need for a Jina AI Cloud account.

Alternatively, you can utilize our online Clip API by following the instructions at https://clip-as-service.jina.ai/hosting/by-jina/ and https://jina.readme.io/docs/inference. With our hosting service, we handle all the necessary work for you.",prefer clip refer documentation approach need jina ai cloud account alternatively utilize clip following hosting service handle necessary work,issue,negative,neutral,neutral,neutral,neutral,neutral
1640045369,"> Hello, please read our documentation https://clip-as-service.jina.ai/
@ZiniuYu From the doc, I have to use  [Jina AI Cloud](https://cloud.jina.ai/) account？
",hello please read documentation doc use jina ai cloud,issue,negative,neutral,neutral,neutral,neutral,neutral
1640025410,"Hello, please read our documentation https://clip-as-service.jina.ai/",hello please read documentation,issue,negative,neutral,neutral,neutral,neutral,neutral
1639721600,"Unfortunately, we currently do not have access to Windows devices for testing purposes. While we investigate this issue further, we recommend trying the following troubleshooting steps:

1. Clear the model cache: Clearing the model cache can help resolve any potential caching conflicts. It normally stores at `~/.cache`

2. Configure the VPN: If you are using a VPN, ensure that it is properly configured and not causing any network-related issues. Double-check the VPN settings and try reconnecting or using a different VPN server if possible.

3. Use a Linux device or Docker image: If you have access to a Linux device or a Docker image, try running the application on that platform. This can help determine if the issue is specific to Windows or if it's a more general problem.

4. Utilize the SaaS CLIP service via [inference.jina.ai](https://cloud.jina.ai/user/inference): As an alternative, you can leverage our SaaS CLIP service for inference by visiting [inference.jina.ai](https://cloud.jina.ai/user/inference). This allows you to perform inference tasks without relying on local setups.

We appreciate your patience while we work on resolving this issue. If you have any further questions or concerns, please don't hesitate to reach out to us.",unfortunately currently access testing investigate issue recommend trying following clear model cache clearing model cache help resolve potential normally configure ensure properly causing try different server possible use device docker image access device docker image try running application platform help determine issue specific general problem utilize clip service via alternative leverage clip service inference visiting perform inference without local appreciate patience work issue please hesitate reach u,issue,positive,positive,neutral,neutral,positive,positive
1639513612,"Hi, thank you for your reply! here is my `jina -vf` output after set `JINA_LOG_LEVEL=debug`. By restart the server, I find the problem is GRPC not ready (see the output below), then I try to install grpcio and grpcio-tools from conda, but it's still don't work, what should I do next?
```
(jina) istar@DESKTOP:~$ jina -vf
DeprecationWarning: pkg_resources is deprecated as an API (raised from /home/istar/anaconda3/envs/jina/lib/python3.8/site-packages/pkg_resources/__init__.py:121)
DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages (raised from /home/istar/anaconda3/envs/jina/lib/python3.8/site-packages/pkg_resources/__init__.py:2870)
DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.logging')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages (raised from /home/istar/anaconda3/envs/jina/lib/python3.8/site-packages/pkg_resources/__init__.py:2870)
DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages (raised from /home/istar/anaconda3/envs/jina/lib/python3.8/site-packages/pkg_resources/__init__.py:2349)
DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages (raised from /home/istar/anaconda3/envs/jina/lib/python3.8/site-packages/pkg_resources/__init__.py:2870)
DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages (raised from /home/istar/anaconda3/envs/jina/lib/python3.8/site-packages/pkg_resources/__init__.py:2870)
- jina 3.19.0
- docarray 0.21.1
- jcloud 0.2.12
- jina-hubble-sdk 0.39.0
- jina-proto 0.1.27
- protobuf 3.20.3
- proto-backend cpp
- grpcio 1.47.5
- pyyaml 6.0
- python 3.8.17
- platform Linux
- platform-release 5.15.90.1-microsoft-standard-WSL2
- platform-version #1 SMP Fri Jan 27 02:56:13 UTC 2023
- architecture x86_64
- processor x86_64
- uid 91769515676
- session-id 62682054-2529-11ee-ab13-00155de3ae9c
- uptime 2023-07-18T13:10:16.805832
- ci-vendor (unset)
- internal False
* JINA_DEFAULT_HOST (unset)
* JINA_DEFAULT_TIMEOUT_CTRL (unset)
* JINA_DEPLOYMENT_NAME (unset)
* JINA_DISABLE_UVLOOP (unset)
* JINA_EARLY_STOP (unset)
* JINA_FULL_CLI (unset)
* JINA_GATEWAY_IMAGE (unset)
* JINA_GRPC_RECV_BYTES (unset)
* JINA_GRPC_SEND_BYTES (unset)
* JINA_HUB_NO_IMAGE_REBUILD (unset)
* JINA_LOG_CONFIG (unset)
* JINA_LOG_LEVEL debug
* JINA_LOG_NO_COLOR (unset)
* JINA_MP_START_METHOD (unset)
* JINA_OPTOUT_TELEMETRY (unset)
* JINA_RANDOM_PORT_MAX (unset)
* JINA_RANDOM_PORT_MIN (unset)
* JINA_LOCKS_ROOT (unset)
* JINA_K8S_ACCESS_MODES (unset)
* JINA_K8S_STORAGE_CLASS_NAME (unset)
* JINA_K8S_STORAGE_CAPACITY (unset)
* JINA_STREAMER_ARGS (unset)
```

```
(jina) istar@DESKTOP:~$ python -m clip_server
DEBUG  clip_t/rep-0@484 Setting signal handlers                                            [07/18/23 13:23:05]
DEBUG  clip_t/rep-0@484 Signal handlers already set
DEBUG  clip_t-replica-set@480 Waiting for ReplicaSet to start successfully                 [07/18/23 13:23:05]
DEBUG  gateway/rep-0@485 Setting signal handlers                                           [07/18/23 13:23:05]
⠋ Waiting ... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0/2 -:--:--DEBUG  gateway/rep-0@485 Signal handlers already set
DEBUG  gateway-replica-set@480 Waiting for ReplicaSet to start successfully                [07/18/23 13:23:05]
⠋ Waiting ... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0/2 -:--:--DEBUG  gateway/rep-0@485 adding connection for deployment clip_t/heads/0 to                [07/18/23 13:23:05]
       grpc://0.0.0.0:60420
DEBUG  gateway/rep-0@485 create_connection connection for clip_t to grpc://0.0.0.0:60420
DEBUG  gateway/rep-0@485 create_connection connection for clip_t to grpc://0.0.0.0:60420
DEBUG  gateway/rep-0@485 connection for deployment clip_t/heads/0 to grpc://0.0.0.0:60420
       added
DEBUG  gateway/rep-0@485 Setting up GRPC server
INFO   gateway/rep-0@485 start server bound to 0.0.0.0:51000
DEBUG  gateway/rep-0@485 Running GatewayRuntime warmup
DEBUG  gateway/rep-0@485 starting warmup task for deployment clip_t
DEBUG  gateway/rep-0@485 server bound to 0.0.0.0:51000 started
DEBUG  gateway/rep-0@485 GRPC server setup successful
DEBUG  gateway/rep-0@480 ready and listening                                               [07/18/23 13:23:06]
DEBUG  gateway-replica-set@480 ReplicaSet started successfully                             [07/18/23 13:23:06]
DEBUG  gateway@480 Deployment started successfully                                         [07/18/23 13:23:06]
⠹ Waiting clip_t gateway... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0/2 0:00:00UserWarning: `docs` annotation must be a type hint, got DocumentArray instead, you should maybe remove the string annotation. Default valueDocumentArray will be used instead. (raised from /home/istar/anaconda3/envs/jina/lib/python3.8/site-packages/jina/serve/executors/__init__.py:131)
⠙ Waiting clip_t... ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 1/2 0:00:00DeprecationWarning: Importing from timm.models.layers is deprecated, please import via timm.layers (raised from /home/istar/anaconda3/envs/jina/lib/python3.8/site-packages/timm/models/layers/__init__.py:49)
⠴ Waiting clip_t... ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 1/2 0:00:10DEBUG  clip_t/rep-0@484 <clip_server.executors.clip_torch.CLIPEncoder object at            [07/18/23 13:23:16]
       0x7fd25709aee0> is successfully loaded!
DEBUG  clip_t/rep-0@484 Setting up GRPC server
INFO   clip_t/rep-0@484 start server bound to 0.0.0.0:60420
DEBUG  clip_t/rep-0@484 server bound to 0.0.0.0:60420 started
DEBUG  clip_t/rep-0@484 GRPC server setup successful
DEBUG  clip_t/rep-0@480 Checking readiness to 0.0.0.0:60420 with protocol GRPC             [07/18/23 13:23:16]
⠙ Waiting clip_t... ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 1/2 0:00:10DEBUG  clip_t/rep-0@484 recv _status request
DEBUG  gateway/rep-0@485 completed warmup task in 10.483339071273804s.                     [07/18/23 13:23:16]
DEBUG  clip_t/rep-0@480 Exception: <AioRpcError of RPC that terminated with:               [07/18/23 13:23:17]
               status = StatusCode.DEADLINE_EXCEEDED
               details = ""Deadline Exceeded""
               debug_error_string =
       ""{""created"":""@1689657797.204551546"",""description"":""Deadline
       Exceeded"",""file"":""src/core/ext/filters/deadline/deadline_filter.cc"",""file_line"":81…
       >
DEBUG  clip_t/rep-0@480 Server on 0.0.0.0:60420 with protocol GRPC is not yet ready
DEBUG  clip_t/rep-0@480 Checking readiness to 0.0.0.0:60420 with protocol GRPC
DEBUG  clip_t/rep-0@480 Exception: <AioRpcError of RPC that terminated with:               [07/18/23 13:23:18]
               status = StatusCode.DEADLINE_EXCEEDED
               details = ""Deadline Exceeded""
               debug_error_string =
       ""{""created"":""@1689657798.313920705"",""description"":""Deadline
       Exceeded"",""file"":""src/core/ext/filters/deadline/deadline_filter.cc"",""file_line"":81…
       >
DEBUG  clip_t/rep-0@480 Server on 0.0.0.0:60420 with protocol GRPC is not yet ready
... (repeat the GRPC not ready message)
```",hi thank reply jina output set restart server find problem ready see output try install still work next jina jina raised call implicit pep preferred see raised call implicit pep preferred see raised call implicit pep preferred see raised call implicit pep preferred see raised call implicit pep preferred see raised jina python platform architecture processor unset internal false unset unset unset unset unset unset unset unset unset unset unset unset unset unset unset unset unset unset unset unset unset jina python setting signal signal already set waiting start successfully setting signal waiting signal already set waiting start successfully waiting connection deployment connection connection connection deployment added setting server start server bound running starting task deployment server bound server setup successful ready listening successfully gateway deployment successfully waiting gateway annotation must type hint got instead maybe remove string annotation default used instead raised waiting please import via raised waiting object successfully loaded setting server start server bound server bound server setup successful readiness protocol waiting request task exception status deadline description deadline file server protocol yet ready readiness protocol exception status deadline description deadline file server protocol yet ready repeat ready message,issue,positive,positive,positive,positive,positive,positive
1639277266,"Hi, what's your output of `jina -vf`?

Can you set `JINA_LOG_LEVEL=debug` and start the server again?",hi output jina set start server,issue,negative,neutral,neutral,neutral,neutral,neutral
1628052856,"Hi, how do you start the flow? You can get the instructions from our docs https://clip-as-service.jina.ai/user-guides/server/#yaml-config

Please let me know if you have any further questions.",hi start flow get please let know,issue,negative,neutral,neutral,neutral,neutral,neutral
1628049947,"Hi, could you please provide a minimum code snippet and a data sample that demonstrate how you obtained the result shown in your screenshot?",hi could please provide minimum code snippet data sample demonstrate result shown,issue,negative,neutral,neutral,neutral,neutral,neutral
1619784140,"You may learn how to start a search flow with [this documentation](https://clip-as-service.jina.ai/user-guides/retriever/), and then you may use `clip_client` to do the searching",may learn start search flow documentation may use searching,issue,negative,neutral,neutral,neutral,neutral,neutral
1619397725,"Like this? I always feel that something is not right.

from clip_client import Client
from docarray import Document

c = Client('grpc://0.0.0.0:51000')  

da = [
    Document(text='she smiled, with pain'),
    Document(uri='kkk.JPG'),
    Document(uri='kkk.JPG').load_uri_to_image_tensor(),
    Document(blob=open('kkk.JPG', 'rb').read()),
    Document(uri='https://clip-as-service.jina.ai/_static/favicon.png'),
    Document(
        uri='data:image/gif;base64,R0lGODlhEAAQAMQAAORHHOVSKudfOulrSOp3WOyDZu6QdvCchPGolfO0o/XBs/fNwfjZ0frl3/zy7////wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACH5BAkAABAALAAAAAAQABAAAAVVICSOZGlCQAosJ6mu7fiyZeKqNKToQGDsM8hBADgUXoGAiqhSvp5QAnQKGIgUhwFUYLCVDFCrKUE1lBavAViFIDlTImbKC5Gm2hB0SlBCBMQiB0UjIQA7'
    ),
]


r = c.encode(da)

r = c.index(da)
# print(r.summary())   

result = c.search(['smile'], limit=2)

print(result['@m', ['text', 'scores__cosine']])

",like always feel something right import client import document client da document pain document document document document document base da da print result print result,issue,negative,negative,negative,negative,negative,negative
1619389414,"Hi, as the tip suggests, do you have a flow with encoder and indexer?",hi tip flow indexer,issue,negative,neutral,neutral,neutral,neutral,neutral
1589006414,"## [Codecov](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/918?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#918](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/918?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (2b4eaaa) into [main](https://app.codecov.io/gh/jina-ai/clip-as-service/commit/280b925e16ab5605a124d412f66ff56caa492553?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (280b925) will **increase** coverage by `0.89%`.
> The diff coverage is `100.00%`.

```diff
@@            Coverage Diff             @@
##             main     #918      +/-   ##
==========================================
+ Coverage   70.71%   71.61%   +0.89%     
==========================================
  Files          23       23              
  Lines        1564     1564              
==========================================
+ Hits         1106     1120      +14     
+ Misses        458      444      -14     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `71.61% <100.00%> (+0.89%)` | :arrow_up: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/918?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [client/clip\_client/\_\_init\_\_.py](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/918?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/\_\_init\_\_.py](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/918?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |

... and [1 file with indirect coverage changes](https://app.codecov.io/gh/jina-ai/clip-as-service/pull/918/indirect-changes?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)

",report main increase coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage file indirect coverage,issue,negative,positive,positive,positive,positive,positive
1569428553,"tbh, i'm not sure I fully got your points. Anyway, we already integrated this embedding product into our Jina Cloud and make it accessible public https://cloud.jina.ai/user/inference.  It works smoothly and efficiently. ",sure fully got anyway already product jina cloud make accessible public work smoothly efficiently,issue,positive,positive,positive,positive,positive,positive
1539340221,"Thank you for your interest in clip-as-service and for reaching out with your question.

To answer your question, we want to clarify that clip-as-service aims to support as many models as possible, including the openai/clip-vit-large-patch14-336 model. However, due to the complex compatibility issues between TensorRT and other packages, such as hardware and CUDA version requirements, we are currently unable to support TensorRT for this model.

We appreciate your feedback and interest in TensorRT support, but at this moment, we don't have any concrete plans to add it in the near future due to our current capacity. However, we encourage you to continue exploring and experimenting with different solutions, and if you find a way to make TensorRT work with this model, please feel free to share it with the community so that others can benefit from your findings.

Thank you for your support and contribution to the open source community.",thank interest reaching question answer question want clarify support many possible model however due complex compatibility hardware version currently unable support model appreciate feedback interest support moment concrete add near future due current capacity however encourage continue exploring different find way make work model please feel free share community benefit thank support contribution open source community,issue,positive,positive,neutral,neutral,positive,positive
1519771514,"We are actually working on this model, it will be available on [Inference-API](https://cloud.jina.ai/user/inference)",actually working model available,issue,negative,positive,positive,positive,positive,positive
1514121299,"Hi @joyjayng ,

Thank you for your interest in clip-as-service and for reaching out to us with your question. We appreciate your feedback and are glad to know that you are interested in using the latest ViT CLIP model (ViT-G/14 on LAION-2B) with our project.

Unfortunately, we currently may not have the capacity to prioritize support for this specific model. However, we encourage you to contribute to our project by submitting a pull request with the necessary changes to add support for this model. We welcome contributions from the community, and we will be happy to review and merge your changes if they meet our standards and requirements.

In case you need guidance on how to add support for new models or any other aspect of our project, please feel free to reach out to us, and we'll do our best to help you.

We look forward to your contribution.",hi thank interest reaching u question appreciate feedback glad know interested latest clip model project unfortunately currently may capacity support specific model however encourage contribute project pull request necessary add support model welcome community happy review merge meet case need guidance add support new aspect project please feel free reach u best help look forward contribution,issue,positive,positive,positive,positive,positive,positive
1514077770,"## [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/911?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#911](https://codecov.io/gh/jina-ai/clip-as-service/pull/911?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (33c300d) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/35733a0ba7fe6d9ae64d2d4d657d6ded2df3a6d1?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (35733a0) will **decrease** coverage by `11.13%`.
> The diff coverage is `n/a`.

```diff
@@             Coverage Diff             @@
##             main     #911       +/-   ##
===========================================
- Coverage   82.73%   71.61%   -11.13%     
===========================================
  Files          23       23               
  Lines        1564     1564               
===========================================
- Hits         1294     1120      -174     
- Misses        270      444      +174     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `71.61% <ø> (-11.13%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

[see 4 files with indirect coverage changes](https://codecov.io/gh/jina-ai/clip-as-service/pull/911/indirect-changes?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)

Help us with your feedback. Take ten seconds to tell us [how you rate us](https://about.codecov.io/nps?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Have a feature suggestion? [Share it here.](https://app.codecov.io/gh/feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
",report main decrease coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find see indirect coverage help u feedback take ten tell u rate u feature suggestion share,issue,positive,positive,positive,positive,positive,positive
1507858469,"Sorry there is something not clear in my previous response. What I meant is that we use fp16 with onnx on gpu (which you may observe the difference), and fp32 with onnx on cpu.

Are you running in cpu mode or gpu mode? Could you please share the scripts/data you found the discrepancy?",sorry something clear previous response meant use may observe difference running mode mode could please share found discrepancy,issue,positive,negative,negative,negative,negative,negative
1507855731,You should use `PIP_TAG=onnx` in the Dockerfile to install extra onnx dependencies. ref: [here](https://github.com/jina-ai/clip-as-service/blob/1888ef65f20a94b38f318696e663d447c7cb1dc6/Dockerfiles/server.Dockerfile#L33) and [here](https://github.com/jina-ai/clip-as-service/blob/main/server/setup.py#L53),use install extra ref,issue,negative,neutral,neutral,neutral,neutral,neutral
1507254105,"after built image with :
```

git clone https://github.com/jina-ai/clip-as-service.git
docker build . -f Dockerfiles/server.Dockerfile  --build-arg GROUP_ID=$(id -g ${USER}) --build-arg USER_ID=$(id -u ${USER}) -t jinaai/clip-server
```

and run with both : 

`cat server/clip_server/onnx-flow.yml | docker run -i -p 51009:51000 -v $HOME/.cache:/home/cas/.cache --gpus all clip/clon:latest -i`

and 

`docker run -p 51009:51000 -v $HOME/.cache:/home/cas/.cache --gpus all clip/clon:latest onnx-flow.yml`

They throw the same error: 

```
ERROR  clip_o/rep-0@24 fail to load file dependency          [04/13/23 16:22:41]
ERROR  clip_o/rep-0@24 FileNotFoundError('can not find       [04/13/23 16:22:41]
       clip_server.executors.clip_onnx') during <class                          
       'jina.serve.runtimes.worker.WorkerRuntime'>                              
       initialization                                                           
        add ""--quiet-error"" to suppress the exception                           
       details                                                                  
       Traceback (most recent call last):                                       
         File                                                                   
       ""/usr/local/lib/python3.8/dist-packages/jina/orchest…                    
       line 79, in run                                                          
           runtime = runtime_cls(                                               
         File                                                                   
       ""/usr/local/lib/python3.8/dist-packages/jina/serve/r…                    
       line 41, in __init__                                                     
           super().__init__(args, **kwargs)                                     
         File                                                                   
       ""/usr/local/lib/python3.8/dist-packages/jina/serve/r…                    
       line 78, in __init__                                                     
           self._loop.run_until_complete(self.async_setup())                    
         File ""/usr/lib/python3.8/asyncio/base_events.py"",                      
       line 616, in run_until_complete                                          
           return future.result()                                               
         File                                                                   
       ""/usr/local/lib/python3.8/dist-packages/jina/serve/r…                    
       line 106, in async_setup                                                 
           self._request_handler = WorkerRequestHandler(                        
         File                                                                   
       ""/usr/local/lib/python3.8/dist-packages/jina/serve/r…                    
       line 53, in __init__                                                     
           self._load_executor(                                                 
         File                                                                   
       ""/usr/local/lib/python3.8/dist-packages/jina/serve/r…                    
       line 203, in _load_executor                                              
           self._executor: BaseExecutor =                                       
       BaseExecutor.load_config(                                                
         File                                                                   
       ""/usr/local/lib/python3.8/dist-packages/jina/jaml/__…                    
       line 742, in load_config                                                 
           load_py_modules(                                                     
         File                                                                   
       ""/usr/local/lib/python3.8/dist-packages/jina/jaml/he…                    
       line 285, in load_py_modules                                             
           PathImporter.add_modules(*mod)                                       
         File                                                                   
       ""/usr/local/lib/python3.8/dist-packages/jina/importe…                    
       line 161, in add_modules                                                 
           _path_import(complete_path(m))                                       
         File                                                                   
       ""/usr/local/lib/python3.8/dist-packages/jina/jaml/he…                    
       line 229, in complete_path                                               
           raise FileNotFoundError(f'can not find {path}')                      
       FileNotFoundError: can not find                                          
       clip_server.executors.clip_onnx                                          
ERROR  Flow@ 1 Flow is aborted due to ['clip_o'] can not be  [04/13/23 16:22:41]
       started.                                                                 
Traceback (most recent call last):
  File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.8/dist-packages/clip_server/__main__.py"", line 25, in <module>
    with f:
  File ""/usr/local/lib/python3.8/dist-packages/jina/orchestrate/orchestrator.py"", line 14, in __enter__
    return self.start()
  File ""/usr/local/lib/python3.8/dist-packages/jina/orchestrate/flow/builder.py"", line 33, in arg_wrapper
    return func(self, *args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/jina/orchestrate/flow/base.py"", line 1788, in start
    self._wait_until_all_ready()
  File ""/usr/local/lib/python3.8/dist-packages/jina/orchestrate/flow/base.py"", line 1927, in _wait_until_all_ready
    raise RuntimeFailToStart
jina.excepts.RuntimeFailToStart
```",built image git clone docker build id user id user run cat docker run latest docker run latest throw error error fail load file dependency error find class add suppress exception recent call last file line run file line super file line file line return file line file line file line file line file line file line file line raise find path find error flow flow aborted due recent call last file line return code none file line code file line module file line return file line return self file line start file line raise,issue,negative,positive,neutral,neutral,positive,positive
1505040775,"## [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/910?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#910](https://codecov.io/gh/jina-ai/clip-as-service/pull/910?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (164765e) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/1888ef65f20a94b38f318696e663d447c7cb1dc6?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (1888ef6) will **decrease** coverage by `11.13%`.
> The diff coverage is `87.50%`.

```diff
@@             Coverage Diff             @@
##             main     #910       +/-   ##
===========================================
- Coverage   82.73%   71.61%   -11.13%     
===========================================
  Files          23       23               
  Lines        1564     1564               
===========================================
- Hits         1294     1120      -174     
- Misses        270      444      +174     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `71.61% <87.50%> (-11.13%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/910?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/executors/clip\_tensorrt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/910?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RlbnNvcnJ0LnB5) | `0.00% <0.00%> (-94.60%)` | :arrow_down: |
| [server/clip\_server/executors/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/910?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX29ubngucHk=) | `86.81% <100.00%> (ø)` | |
| [server/clip\_server/executors/clip\_torch.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/910?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RvcmNoLnB5) | `87.77% <100.00%> (ø)` | |
| [server/clip\_server/executors/helper.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/910?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9oZWxwZXIucHk=) | `97.18% <100.00%> (ø)` | |

... and [3 files with indirect coverage changes](https://codecov.io/gh/jina-ai/clip-as-service/pull/910/indirect-changes?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)

Help us with your feedback. Take ten seconds to tell us [how you rate us](https://about.codecov.io/nps?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Have a feature suggestion? [Share it here.](https://app.codecov.io/gh/feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
",report main decrease coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage indirect coverage help u feedback take ten tell u rate u feature suggestion share,issue,positive,positive,positive,positive,positive,positive
1504978627,"Hi @geekchen007,

Thank you for bringing this issue to our attention. To better understand the problem, could you please let us know what device you are using to run the model? Our pytorch model uses mixed-precision, which means it utilizes both float32 and float16. However, we only use float16 on the GPU, which may result in some precision loss.

Looking forward to hearing back from you.",hi thank issue attention better understand problem could please let u know device run model model float float however use float may result precision loss looking forward hearing back,issue,negative,positive,positive,positive,positive,positive
1504940598,"TODO: 
- [ ] Change the pre-processing implementation in CLIP Executor to address the above bug",change implementation clip executor address bug,issue,negative,neutral,neutral,neutral,neutral,neutral
1504925059,@numb3r3 please finish the proposed implementation in the description. ,please finish implementation description,issue,negative,neutral,neutral,neutral,neutral,neutral
1499955324,"Nice, I will close this issue now. And please feel free to open a new issue when you have new questions. ",nice close issue please feel free open new issue new,issue,positive,positive,positive,positive,positive,positive
1499912931,"> 
Thank you for your reply. I am just an undergraduate completing my graduation project. I hope to find metaphors in text by obtaining dynamic word vectors.
I have now completed my requirements through bert-as-service, thank you very much!",thank reply undergraduate graduation project hope find text dynamic word thank much,issue,positive,positive,neutral,neutral,positive,positive
1499911375,"@mrjackbo Thank you for pointing that out. Previously, we conducted a simple experiment to show that the `_transform_ndarray` wouldn't harm the downstream task (including retrieval, and zero-shot classification tasks). Thus, we made the conclusion that the embeddings from the same `transform` operation would be acceptable. 

However, based on your question:
> Are the text embeddings now slightly misaligned to the image embeddings?

I think you are right, we did not consider this use case. We should use the `_transform_blob` that potentially improve the text-image retrieval quality. 


",thank pointing previously simple experiment show would harm downstream task retrieval classification thus made conclusion transform operation would acceptable however based question text slightly image think right consider use case use potentially improve retrieval quality,issue,negative,positive,neutral,neutral,positive,positive
1499904780,"@MarrytheToilet In the current implementation of CLIP model, the word-level embeddings are not returned. To support your case, we need to refactor `encode_text(...)` API to return full sequence of embeddings, rather than the eos_token embedding only. May I know what's the downstream tasks you are working on that needs word-level embeddings? ",current implementation clip model returned support case need return full sequence rather may know downstream working need,issue,negative,positive,positive,positive,positive,positive
1497084481,Similar issue (Running for more than half an hour) in google colab. Can anybody help resolve the same?,similar issue running half hour anybody help resolve,issue,positive,negative,neutral,neutral,negative,negative
1495796313,@numb3r3 the other reason why I wanted to build is because I needed to fork the code and add debug statements. Even on `JINA_LOG_LEVEL=DEBUG` I was getting errors from the `indexer` stating that a `Doc.embedding` was `None`. I was trying to figure out which piece of data it was etc.,reason build fork code add even getting indexer none trying figure piece data,issue,negative,neutral,neutral,neutral,neutral,neutral
1495402186,"@themantalope For your purpose, you can directly use the containerized  executor from the hub `jinahub+docker://CLIPTorchEncoder` (https://cloud.jina.ai/executor/gzpbl8jh):

```
jtype: Flow
executors:
  - uses: jinahub+docker://CLIPTorchEncoder
```

In the above example, the executor will be a docker container in a flow. 

But you are right, the document is not clear and misleading. Thank you for pointing that out. ",purpose directly use executor hub flow example executor docker container flow right document clear misleading thank pointing,issue,negative,positive,positive,positive,positive,positive
1495380921,"Hi @numb3r3 

Yeah that's fine. I wanted to use it as an executor in a `jina flow`. The documentation wasn't clear that was the correct way to locally build for the `flow` use case.",hi yeah fine use executor jina flow documentation clear correct way locally build flow use case,issue,positive,positive,positive,positive,positive,positive
1495337460,"Ok, I see. You want to build `CUDA` image. May I ask you to add the building CLI in our document instead of README? ",see want build image may ask add building document instead,issue,negative,neutral,neutral,neutral,neutral,neutral
1495250267,"Yep it works, as long as you are in the server directory. Don't need to copy the `Dockerfiles`. Seems like the `server` dir is the correct build context 

```bash
cd server
docker build . -f ../Dockerfiles/cuda.Dockerfile -t clip-as-service-gpu:latest
```
I put in a [PR](https://github.com/jina-ai/clip-as-service/pull/903) with updated readme.",yep work long server directory need copy like server correct build context bash server docker build latest put,issue,positive,positive,positive,positive,positive,positive
1486543036,Running nvidia-smi the vRAM looks empty so I don't think it is actually building the engines.,running empty think actually building,issue,negative,negative,neutral,neutral,negative,negative
1485069462,"Hi @ZiniuYu , 

apologies for the delay in the answer.
The error occurs after a certain period of time running and I am sending batches of combinations of texts and images to clip as a service.

Running latest version of jina: 3.14.2.

Let's take the scenario where we're running out of computing resource. Should there be a recovery mechanism inside of the container to recover from that state? Via a retry mechanism or other?

At the moment, when this occurs, the container ends up in a state where it lost connectivity and doesn't recover from the state even if no more resources are consumed.

If you want to try to replicate, try a local setup on your machine and send a big batch of records like images to encode to get your container in an ""resource exhausted state"".",hi delay answer error certain period time running sending clip service running latest version jina let take scenario running resource recovery mechanism inside container recover state via retry mechanism moment container state lost connectivity recover state even want try replicate try local setup machine send big batch like encode get container resource exhausted state,issue,negative,positive,neutral,neutral,positive,positive
1479379549,"## [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/900?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#900](https://codecov.io/gh/jina-ai/clip-as-service/pull/900?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (7a286c4) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/cce3b05a1cfa23db129e8a7077e75e75f5da73c6?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (cce3b05) will **decrease** coverage by `11.13%`.
> The diff coverage is `n/a`.

> :exclamation: Current head 7a286c4 differs from pull request most recent head 08c4766. Consider uploading reports for the commit 08c4766 to get more accurate results

```diff
@@             Coverage Diff             @@
##             main     #900       +/-   ##
===========================================
- Coverage   82.73%   71.61%   -11.13%     
===========================================
  Files          23       23               
  Lines        1564     1564               
===========================================
- Hits         1294     1120      -174     
- Misses        270      444      +174     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `71.61% <ø> (-11.13%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

[see 4 files with indirect coverage changes](https://codecov.io/gh/jina-ai/clip-as-service/pull/900/indirect-changes?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)

Help us with your feedback. Take ten seconds to tell us [how you rate us](https://about.codecov.io/nps?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Have a feature suggestion? [Share it here.](https://app.codecov.io/gh/feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
",report main decrease coverage coverage exclamation current head pull request recent head consider commit get accurate coverage main coverage flag coverage carried forward coverage wo shown click find see indirect coverage help u feedback take ten tell u rate u feature suggestion share,issue,positive,positive,positive,positive,positive,positive
1477212015,We need to have a decision here. is it still necessary to keep this demo page alive. ,need decision still necessary keep page alive,issue,negative,positive,neutral,neutral,positive,positive
1455387582,"Great news! It looks like you were able to fix the issue on your own. We appreciate your initiative and effort in resolving this problem. As a result, we're going to close this ticket. If you run into any more issues or have any other feedback, please don't hesitate to let us know. Thank you for your contribution!",great news like able fix issue appreciate initiative effort problem result going close ticket run feedback please hesitate let u know thank contribution,issue,positive,positive,positive,positive,positive,positive
1455382257,"你好，


&nbsp; 可能是WIN11下LINUX子系统不够稳定导致的，我更换成VM环境,重新装LINUX，目前问题已经解决了。






娜美智能
***@***.***



&nbsp;




------------------&nbsp;原始邮件&nbsp;------------------
发件人:                                                                                                                        ""jina-ai/clip-as-service""                                                                                    ***@***.***&gt;;
发送时间:&nbsp;2023年3月2日(星期四) 下午3:19
***@***.***&gt;;
***@***.******@***.***&gt;;
主题:&nbsp;Re: [jina-ai/clip-as-service] 基于WIN11的LINUX子系统安装CLIP报错 (Issue #895)





 
@billhuang6277 can you paste the full error trace? Thanks
 
—
Reply to this email directly, view it on GitHub, or unsubscribe.
You are receiving this because you were mentioned.Message ID: ***@***.***&gt;",issue paste full error trace thanks reply directly view id,issue,negative,positive,positive,positive,positive,positive
1453083552,"@ZiniuYu From the example bar, I would suggest changing `Recv` to `Progress`. ",example bar would suggest progress,issue,negative,neutral,neutral,neutral,neutral,neutral
1452970633,Could you give an example of the new progress bar after this PR? ,could give example new progress bar,issue,negative,positive,positive,positive,positive,positive
1452286415,"@billhuang6277 
I created a fresh python 3.10 env in win11 WSL ubuntu20, but was not able to reproduce your problem.
Could you provide more details? For example, what's your output of `jina -vf` and what's your `clip_server` version? Anything that may help us to reproduce the problem is welcomed.
Can you please also try again with `export JINA_LOG_LEVEL=DEBUG`?

```
(py310) ziniu@SurfaceBook:/mnt/c/Users/ziniu$ export JINA_LOG_LEVEL=debug
(py310) ziniu@SurfaceBook:/mnt/c/Users/ziniu$ python -m clip_server
DeprecationWarning: There is no current event loop (raised from /home/ziniu/miniconda3/envs/py310/lib/python3.10/site-packages/jina/orchestrate/flow/base.py:1892)
⠙ Waiting clip_t gateway... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0/2 0:00:00DEBUG  gateway/rep-0@76 adding connection for deployment clip_t/heads/0 to grpc://0.0.0.0:56397      [03/03/23 01:22:43]
DEBUG  gateway/rep-0@76 start server bound to 0.0.0.0:51000
DEBUG  gateway/rep-0@76 Running GatewayRuntime warmup
DEBUG  gateway/rep-0@76 starting warmup task for deployment clip_t
DEBUG  gateway/rep-0@65 ready and listening                                                          [03/03/23 01:22:43]⠼ Waiting clip_t... ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 1/2 0:00:05UserWarning: `docs` annotation must be a type hint, got DocumentArray instead, you should maybe remove the string annotation. Default valueDocumentArray will be used instead. (raised from /home/ziniu/miniconda3/envs/py310/lib/python3.10/site-packages/jina/serve/executors/__init__.py:119)  ViT-B-32.pt 25.9% • 91.5/354.0 MB • 204.8 kB/s • 0:21:22
                                                          DEBUG  gateway/rep-0@76 completed warmup task in 300.112263202  ViT-B-32.pt 100.0% • 354.0/354.0 MB • 252.7 kB/s • 0:00:00

⠋ Waiting clip_t... ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 1/2 0:19:45DEBUG  clip_t/rep-0@75 <clip_server.executors.clip_torch.CLIPEncoder object at 0x7f4aa9d61300> is    [03/03/23 01:42:29]
       successfully loaded!
DEBUG  clip_t/rep-0@75 start listening on 0.0.0.0:56397
DEBUG  clip_t/rep-0@75 run grpc server forever
DEBUG  clip_t/rep-0@65 ready and listening                                                           [03/03/23 01:42:29]
────────────────────────────────────────────── 🎉 Flow is ready to serve! ──────────────────────────────────────────────
╭────────────── 🔗 Endpoint ───────────────╮
│  ⛓      Protocol                   GRPC  │
│  🏠        Local          0.0.0.0:51000  │
│  🔒      Private    172.18.64.108:51000  │
╰──────────────────────────────────────────╯
DEBUG  Flow@65 2 Deployments (i.e. 2 Pods) are running in this Flow                                  [03/03/23 01:42:31]
Do you love Open Source? Help us get better and be heard in just 1 minute and 30 seconds 💖Your feedback will help us
build better features for Jina, your loved open-source project 🎉
Take the Jina user survey!
```",fresh python win able reproduce problem could provide example output jina version anything may help u reproduce problem please also try export export python current event loop raised waiting gateway connection deployment start server bound running starting task deployment ready listening waiting annotation must type hint got instead maybe remove string annotation default used instead raised task waiting object successfully loaded start listening run server forever ready listening flow ready serve link protocol house local locked private flow running flow love open source help u get better minute feedback help u build better jina project take jina user survey,issue,positive,positive,positive,positive,positive,positive
1451683505,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/897?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#897](https://codecov.io/gh/jina-ai/clip-as-service/pull/897?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (4125016) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/d70f238220f76593fb9b14e43e50f9a9d2cecd8a?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (d70f238) will **decrease** coverage by `11.10%`.
> The diff coverage is `100.00%`.

```diff
@@             Coverage Diff             @@
##             main     #897       +/-   ##
===========================================
- Coverage   82.70%   71.61%   -11.10%     
===========================================
  Files          23       23               
  Lines        1567     1564        -3     
===========================================
- Hits         1296     1120      -176     
- Misses        271      444      +173     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `71.61% <100.00%> (-11.10%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/897?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [client/clip\_client/client.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/897?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L2NsaWVudC5weQ==) | `87.57% <100.00%> (+0.18%)` | :arrow_up: |
| [server/clip\_server/executors/clip\_tensorrt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/897?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RlbnNvcnJ0LnB5) | `0.00% <0.00%> (-94.60%)` | :arrow_down: |
| [server/clip\_server/model/clip\_trt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/897?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfdHJ0LnB5) | `0.00% <0.00%> (-85.72%)` | :arrow_down: |
| [server/clip\_server/model/trt\_utils.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/897?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3RydF91dGlscy5weQ==) | `0.00% <0.00%> (-83.52%)` | :arrow_down: |
| [server/clip\_server/model/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/897?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfb25ueC5weQ==) | `87.30% <0.00%> (+22.22%)` | :arrow_up: |

Help us with your feedback. Take ten seconds to tell us [how you rate us](https://about.codecov.io/nps?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Have a feature suggestion? [Share it here.](https://app.codecov.io/gh/feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
",report main decrease coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage help u feedback take ten tell u rate u feature suggestion share,issue,positive,positive,positive,positive,positive,positive
1451404106,@billhuang6277 can you paste the full error trace? Thanks,paste full error trace thanks,issue,negative,positive,positive,positive,positive,positive
1451210241,"The flow running behinds the demo is deleted, let me redeploy it",flow running let redeploy,issue,negative,neutral,neutral,neutral,neutral,neutral
1449847407,Thanks for pointing it out. I guess this is due to the upstream issue. We need some time to reproduce this issue. ,thanks pointing guess due upstream issue need time reproduce issue,issue,negative,positive,neutral,neutral,positive,positive
1441157049,"Hi @vincetrep 

What's your jina version (`jina -vf`)? Can you set the env `JINA_LOG_LEVEL=debug` and see if it prints any more info?

One possible reason is that you are running out of computing resource. We recently fixed a issue in Jina Core that affects the health check latency when a flow is stressed by load in k8s, could you please upgrade to the latest jina and try again?

Could you also provide more details about how the communication stops if possible? Like it stops when you are sending large requests or just stops when the flow is idle, and etc. Anything that may help us debug or reproduce is welcomed.",hi jina version jina set see one possible reason running resource recently fixed issue jina core health check latency flow load could please upgrade latest jina try could also provide communication possible like sending large flow idle anything may help u reproduce,issue,positive,positive,positive,positive,positive,positive
1440724902,"Hello, 

I am running with similar issues on different setups.

I am running clip as a service as well in GRPC mode. The clip-server is running in a docker container.

I have seen this issue on my development environment where the communication stops at some point. This time, it stopped at restart which I have not seen as often.

On my other environments running on kubernetes, every time this has happened I had to redeploy the containers to make them functional again. Do you have any clues as to what could be the source of this problem? Could it be related with management of sockets/communication channels? Is is possible that when peppering the service with too many queued requests it runs out of connections? Let me know how I can help with the troubleshooting.

Here is an error log upon restarting the container on Docker Desktop running on Windows:

Task exception was never retrieved

future: <Task finished name='Task-13' coro=<GatewayRequestHandler.handle_request.<locals>.gather_endpoints() done, defined at /usr/local/lib/python3.9/site-packages/jina/serve/runtimes/gateway/request_handling.py:54> exception=failed to connect to all addresses |Gateway: Communication error with deployment at address(es) {'0.0.0.0:64294'}. Head or worker(s) may be down.>

Traceback (most recent call last):

  File ""/usr/local/lib/python3.9/site-packages/jina/serve/networking.py"", line 1068, in task_wrapper

    return await connection.send_discover_endpoint(

  File ""/usr/local/lib/python3.9/site-packages/jina/serve/networking.py"", line 377, in send_discover_endpoint

    await self._init_stubs()

  File ""/usr/local/lib/python3.9/site-packages/jina/serve/networking.py"", line 353, in _init_stubs

    available_services = await GrpcConnectionPool.get_available_services(

  File ""/usr/local/lib/python3.9/site-packages/jina/serve/networking.py"", line 1390, in get_available_services

    async for res in response:

  File ""/usr/local/lib/python3.9/site-packages/grpc/aio/_call.py"", line 326, in _fetch_stream_responses

    await self._raise_for_status()

  File ""/usr/local/lib/python3.9/site-packages/grpc/aio/_call.py"", line 236, in _raise_for_status

    raise _create_rpc_error(await self.initial_metadata(), await

grpc.aio._call.AioRpcError: <AioRpcError of RPC that terminated with:

status = StatusCode.UNAVAILABLE

details = ""failed to connect to all addresses""

debug_error_string = ""{""created"":""@1677095795.838017200"",""description"":""Failed to pick subchannel"",""file"":""src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3260,""referenced_errors"":[{""created"":""@1677095795.838016400"",""description"":""failed to connect to all addresses"",""file"":""src/core/lib/transport/error_utils.cc"",""file_line"":167,""grpc_status"":14}]}""

>


During handling of the above exception, another exception occurred:


Traceback (most recent call last):

  File ""/usr/local/lib/python3.9/site-packages/jina/serve/runtimes/gateway/request_handling.py"", line 68, in gather_endpoints

ERROR  gateway/rep-0/GatewayRuntime@22 Error while getting   [02/22/23 19:56:35]

       responses from deployments: failed to connect to all                     

       addresses |Gateway: Communication error with                             

       deployment clip_t at address(es) {'0.0.0.0:64294'}.                      

       Head or worker(s) may be down.                                           

ERROR  gateway/rep-0/GatewayRuntime@22 Error while getting                      

       responses from deployments: failed to connect to all                     

       addresses |Gateway: Communication error with                             

       deployment clip_t at address(es) {'0.0.0.0:64294'}.                      

       Head or worker(s) may be down.                                           

ERROR  gateway/rep-0/GatewayRuntime@22 Error while getting   [02/22/23 19:56:36]

       responses from deployments: failed to connect to all                     

       addresses |Gateway: Communication error with                             

       deployment clip_t at address(es) {'0.0.0.0:64294'}.                      

       Head or worker(s) may be down.                                           

ERROR  gateway/rep-0/GatewayRuntime@22 Error while getting                      

       responses from deployments: failed to connect to all                     

       addresses |Gateway: Communication error with                             

       deployment clip_t at address(es) {'0.0.0.0:64294'}.                      

    raise err

  File ""/usr/local/lib/python3.9/site-packages/jina/serve/runtimes/gateway/request_handling.py"", line 60, in gather_endpoints

    endpoints = await asyncio.gather(*tasks_to_get_endpoints)

  File ""/usr/local/lib/python3.9/site-packages/jina/serve/networking.py"", line 1082, in task_wrapper

    raise error

jina.excepts.InternalNetworkError: failed to connect to all addresses |Gateway: Communication error with deployment at address(es) {'0.0.0.0:64294'}. Head or worker(s) may be down.

       Head or worker(s) may be down.                                           

ERROR  gateway/rep-0/GatewayRuntime@22 Error while getting   [02/22/23 19:56:38]

       responses from deployments: failed to connect to all                     

       addresses |Gateway: Communication error with                             

       deployment clip_t at address(es) {'0.0.0.0:64294'}.                      

       Head or worker(s) may be down.                                           

ERROR  gateway/rep-0/GatewayRuntime@22 Error while getting                      

       responses from deployments: failed to connect to all                     

       addresses |Gateway: Communication error with                             

       deployment clip_t at address(es) {'0.0.0.0:64294'}.                      

       Head or worker(s) may be down.                                           

ERROR  gateway/rep-0/GatewayRuntime@22 Error while getting                      

       responses from deployments: failed to connect to all                     

       addresses |Gateway: Communication error with                             

       deployment clip_t at address(es) {'0.0.0.0:64294'}.                      

       Head or worker(s) may be down.                                           

ERROR  gateway/rep-0/GatewayRuntime@22 Error while getting   [02/22/23 19:56:41]

       responses from deployments: failed to connect to all                     

       addresses |Gateway: Communication error with                             

       deployment clip_t at address(es) {'0.0.0.0:64294'}.                      

       Head or worker(s) may be down.                                           

ERROR  gateway/rep-0/GatewayRuntime@22 Error while getting   [02/22/23 19:56:43]

       responses from deployments: failed to connect to all                     

       addresses |Gateway: Communication error with                             

       deployment clip_t at address(es) {'0.0.0.0:64294'}.                      

       Head or worker(s) may be down.                                           

ERROR  gateway/rep-0/GatewayRuntime@22 Error while getting   [02/22/23 19:56:45]

       responses from deployments: failed to connect to all                     

       addresses |Gateway: Communication error with                             

       deployment clip_t at address(es) {'0.0.0.0:64294'}.                      

       Head or worker(s) may be down.                                           

ERROR  gateway/rep-0/GatewayRuntime@22 Error while getting   [02/22/23 19:56:50]

       responses from deployments: failed to connect to all                     

       addresses |Gateway: Communication error with                             

       deployment clip_t at address(es) {'0.0.0.0:64294'}.                      

       Head or worker(s) may be down.                                           

ERROR  gateway/rep-0/GatewayRuntime@22 Error while getting   [02/22/23 19:56:53]

       responses from deployments: failed to connect to all                     

       addresses |Gateway: Communication error with                             

       deployment clip_t at address(es) {'0.0.0.0:64294'}.                      

       Head or worker(s) may be down.                                           

────────────────────────── 🎉 Flow is ready to serve! ──────────────────────────

╭────────────── 🔗 Endpoint ───────────────╮

│  ⛓      Protocol                   GRPC  │

│  🏠        Local           0.0.0.0:9100  │

│  🔒      Private        172.24.0.5:9100  │

│  🌍       Public    23.233.181.148:9100  │

╰──────────────────────────────────────────╯

╭──────── 💎 Prometheus extension ─────────╮

│  🔦             clip_t         ...:9091  │

│  🔦            gateway         ...:9090  │

╰──────────────────────────────────────────╯


ERROR  gateway/rep-0/GatewayRuntime@22 Error while getting   [02/22/23 19:57:00]

       responses from deployments: failed to connect to all                     

       addresses |Gateway: Communication error with                             

       deployment clip_t at address(es) {'0.0.0.0:64294'}.                      

       Head or worker(s) may be down.                                           

ERROR  gateway/rep-0/GatewayRuntime@22 Error while getting   [02/22/23 19:57:01]

       responses from deployments: failed to connect to all                     

       addresses |Gateway: Communication error with                             

       deployment clip_t at address(es) {'0.0.0.0:64294'}.                      

       Head or worker(s) may be down.                 ",hello running similar different running clip service well mode running docker container seen issue development environment communication point time stopped restart seen often running every time redeploy make functional could source problem could related management possible service many let know help error log upon container docker running task exception never future task finished done defined connect communication error deployment address e head worker may recent call last file line return await file line await file line await file line response file line await file line raise await await status connect description pick file description connect file handling exception another exception recent call last file line error error getting connect communication error deployment address e head worker may error error getting connect communication error deployment address e head worker may error error getting connect communication error deployment address e head worker may error error getting connect communication error deployment address e raise err file line await file line raise error connect communication error deployment address e head worker may head worker may error error getting connect communication error deployment address e head worker may error error getting connect communication error deployment address e head worker may error error getting connect communication error deployment address e head worker may error error getting connect communication error deployment address e head worker may error error getting connect communication error deployment address e head worker may error error getting connect communication error deployment address e head worker may error error getting connect communication error deployment address e head worker may error error getting connect communication error deployment address e head worker may flow ready serve link protocol house local locked private public extension flashlight flashlight gateway error error getting connect communication error deployment address e head worker may error error getting connect communication error deployment address e head worker may,issue,negative,positive,neutral,neutral,positive,positive
1440419198,"Hi @pasa13142

The issue is fixed and the pr is merged. Please pull the latest `jinaai/clip-server:master-onnx` docker image and try again. 

Notice that we also updated the instructions on how to serve the docker container [here](https://clip-as-service.jina.ai/user-guides/server/#run). The correct way to run a onnx docker container using the default config is like 
`docker run -p 51009:51000 -v $HOME/.cache:/home/cas/.cache --gpus all jinaai/clip-server:master-onnx onnx-flow.yml` (If you don't add `onnx-flow.yml` it will use the default yaml which runs in pytorch mode)
And to use a custom config is the same
`cat my.yml | docker run -i -p 51009:51000 -v $HOME/.cache:/home/cas/.cache --gpus all jinaai/clip-server:master-onnx -i`

Thanks for reporting this issue 🍻",hi issue fixed please pull latest docker image try notice also serve docker container correct way run docker container default like docker run add use default mode use custom cat docker run thanks issue,issue,positive,positive,positive,positive,positive,positive
1436616482,"@pasa13142 There is a issue with the clip_server onnx docker image, we are fixing it and will let you know for any progress. Sorry for the inconvenience! You might use clip_server torch image or [serve with source code](https://clip-as-service.jina.ai/user-guides/server/#start-a-onnx-backed-server) to unblock your work",issue docker image fixing let know progress sorry inconvenience might use torch image serve source code unblock work,issue,negative,negative,negative,negative,negative,negative
1436392607,"Didnt work again. Throws full of a page of this error as same as i explained methods above. Default docker image run command doesnt throw this deprecationWarning message.

```
DeprecationWarning: tostring() is deprecated. Use tobytes() instead. (raised from /usr/local/lib/python3.8/dist-packages/onnxconverter_common/float16.py:95)
DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead (raised from /usr/local/lib/python3.8/dist-packages/onnxconverter_common/float16.py:91)
```",didnt work full page error default docker image run command doesnt throw message use instead raised binary mode surprisingly use instead raised,issue,negative,positive,positive,positive,positive,positive
1436305669,"Yes, both local and private work!",yes local private work,issue,negative,neutral,neutral,neutral,neutral,neutral
1436296911,"Could you try this:
```
docker run -i -p 51009:51000 -v $HOME/.cache:/home/cas/.cache -v $PWD:/cas/ --gpus all jinaai/clip-server:master-onnx onnx-flow-custom.yml
```",could try docker run,issue,negative,neutral,neutral,neutral,neutral,neutral
1435149526,You can convert the OWL-ViT weights to onnx using `torch.export` and then supply the path manually in the jina service (in case no one picks this up).,convert supply path manually jina service case one,issue,negative,neutral,neutral,neutral,neutral,neutral
1434607473,"> your python and system config problem, out of scope.



> I have installed it but still got this error. Can anyone help?
> 
> ModuleNotFoundError Traceback (most recent call last) in 1 # import sys ----> 2 from bert_serving.client import BertClient 3 client = BertClient() 4 vectors = client.encode(['dog'],['cat'],['man'])
> 
> ModuleNotFoundError: No module named 'bert_serving'

got the same problem",python system problem scope still got error anyone help recent call last import import client module got problem,issue,negative,neutral,neutral,neutral,neutral,neutral
1421888921,"To persistent the embeddings, you can append a new executor for this purpose. As an example, you can refer to https://clip-as-service.jina.ai/user-guides/retriever/ If you have any questions, feel free to reach out to us. ",persistent append new executor purpose example refer feel free reach u,issue,positive,positive,positive,positive,positive,positive
1379961991,"@Hippopotamus0308 lint failed, please check your commit messages (including pr title) 🙇 ",hippopotamus lint please check commit title,issue,positive,neutral,neutral,neutral,neutral,neutral
1378471639,"Hi @learningpro

Thank you for your inquiry. At this time, `clip_server` does not support Python 3.11 due to some upstream dependencies that are not fully supported in Python 3.11. We are currently investigating potential solutions and updates to the software, so please check back in the future for updates. Thank you for your patience.",hi thank inquiry time support python due upstream fully python currently investigating potential please check back future thank patience,issue,positive,negative,neutral,neutral,negative,negative
1376953318,"Please use replicate executor and replicate GPUs, following this: https://docs.jina.ai/concepts/flow/scale-out/",please use replicate executor replicate following,issue,negative,neutral,neutral,neutral,neutral,neutral
1376947737,"Hi, please refer [here](https://clip-as-service.jina.ai/user-guides/server/#serve-on-multiple-gpus) for multiple GPU serving 🍻 ",hi please refer multiple serving,issue,negative,neutral,neutral,neutral,neutral,neutral
1370573103,"The communication among gateway, and (external-)executors are all grpc-based protocol. ",communication among gateway protocol,issue,negative,neutral,neutral,neutral,neutral,neutral
1370572163,"basically, we only support grpc-based external service. Thus, the right approach is to start external service ahead:
```
# the flow uses default `grpc` protocol
with Flow(port='12345').add(....) as external_flow:
    # specify the external service host and port
    with Flow().add(host='0.0.0.0', port=external_flow.port, external=True) as f:
        f.post(...)
    
``` ",basically support external service thus right approach start external service ahead flow default protocol flow specify external service host port flow,issue,negative,positive,neutral,neutral,positive,positive
1370569770,"from jina import Flow,Document

# replica_hosts, replica_ports = ['localhost', '91.198.174.192'], ['12345', '12346']
# Flow().add(host=replica_hosts, port=replica_ports, external=True)


# alternative syntax
def main():
    f = Flow().add(host=[' http://192.168.0.243:8036', ' http://192.168.0.243:8038'], external=True)
    print(f.is_flow_ready())

if __name__ == '__main__':
    main()",jina import flow document flow alternative syntax main flow print main,issue,negative,positive,positive,positive,positive,positive
1366345965,"Hi, please view https://clip-as-service.jina.ai/user-guides/server/#model-support for list of models we supported.",hi please view list,issue,negative,neutral,neutral,neutral,neutral,neutral
1365748402,"This is what I did.


from docarray import DocumentArray, Document
from clip_client import Client
import numpy as np

doc_array = DocumentArray.pull('amazon-berkeley-objects-dataset', show_progress=True)
clt = Client('grpcs://api.clip.jina.ai:2096', credential={'Authorization': 'insert-authorization-code'})
encoded_docarray = clt.encode(doc_array, show_progress=True)",import document import client import client,issue,negative,neutral,neutral,neutral,neutral,neutral
1365585064,"So, please try to make it work with `cuda-11.6.0`, rather than freeze the cuda version. ",please try make work rather freeze version,issue,negative,neutral,neutral,neutral,neutral,neutral
1365584671,"> I just tested both Torch and ONNX executors, they are all fine

Then, I doubled if this issue comes from cuda? And I would not recommend pinning the cuda version. Our work should work on all stable cuda versions. ",tested torch fine doubled issue come would recommend pinning version work work stable,issue,positive,positive,positive,positive,positive,positive
1365572152,"> @ZiniuYu Does the hub executor also suffer from this issue?

I just tested both Torch and ONNX executors, they are all fine",hub executor also suffer issue tested torch fine,issue,negative,positive,positive,positive,positive,positive
1365067716,@ZiniuYu Does the hub executor also suffer from this issue?,hub executor also suffer issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1365059301,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/882?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#882](https://codecov.io/gh/jina-ai/clip-as-service/pull/882?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (a3d8796) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/0b293ec834e80f7335aa625d683904594373a607?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (0b293ec) will **decrease** coverage by `11.27%`.
> The diff coverage is `100.00%`.

```diff
@@             Coverage Diff             @@
##             main     #882       +/-   ##
===========================================
- Coverage   83.06%   71.78%   -11.28%     
===========================================
  Files          22       22               
  Lines        1529     1531        +2     
===========================================
- Hits         1270     1099      -171     
- Misses        259      432      +173     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `71.78% <100.00%> (-11.28%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/882?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/model/model.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/882/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL21vZGVsLnB5) | `75.37% <100.00%> (+0.18%)` | :arrow_up: |
| [server/clip\_server/executors/clip\_tensorrt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/882/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RlbnNvcnJ0LnB5) | `0.00% <0.00%> (-94.60%)` | :arrow_down: |
| [server/clip\_server/model/clip\_trt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/882/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfdHJ0LnB5) | `0.00% <0.00%> (-85.72%)` | :arrow_down: |
| [server/clip\_server/model/trt\_utils.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/882/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3RydF91dGlscy5weQ==) | `0.00% <0.00%> (-83.52%)` | :arrow_down: |
| [server/clip\_server/executors/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/882/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX29ubngucHk=) | `87.91% <0.00%> (+1.09%)` | :arrow_up: |
| [server/clip\_server/model/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/882/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfb25ueC5weQ==) | `87.30% <0.00%> (+22.22%)` | :arrow_up: |

Help us with your feedback. Take ten seconds to tell us [how you rate us](https://about.codecov.io/nps?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Have a feature suggestion? [Share it here.](https://app.codecov.io/gh/feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
",report ad main decrease coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage help u feedback take ten tell u rate u feature suggestion share,issue,positive,positive,positive,positive,positive,positive
1365054771,please rebuild the image and reply to the communities.,please rebuild image reply,issue,negative,neutral,neutral,neutral,neutral,neutral
1362909580,"now a server side issue happens:
```
    raise _create_rpc_error(await self.initial_metadata(), await
grpc.aio._call.AioRpcError: <AioRpcError of RPC that terminated with:
	status = StatusCode.UNAVAILABLE
	details = ""HTTP Balancer service in fail-fast |Gateway: Communication error with deployment clip_t at address(es) {'clip-t.grpc-flow.svc:8080'}. Head or worker(s) may be down.""
	debug_error_string = ""{""created"":""@1671719158.543427000"",""description"":""Error received from peer ipv4:52.58.81.82:2096"",""file"":""src/core/lib/surface/call.cc"",""file_line"":967,""grpc_message"":""HTTP Balancer service in fail-fast |Gateway: Communication error with deployment clip_t at address(es) {'clip-t.grpc-flow.svc:8080'}. Head or worker(s) may be down."",""grpc_status"":14}""
```",server side issue raise await await status balancer service communication error deployment address e head worker may description error received peer file balancer service communication error deployment address e head worker may,issue,negative,neutral,neutral,neutral,neutral,neutral
1361082423,"Hi, what's your reproducible script? Please make sure the input is valid and complete.",hi reproducible script please make sure input valid complete,issue,positive,positive,positive,positive,positive,positive
1361080635,"Hi, please make sure your input is valid. `Document(uri=xxx)` takes a local file path, a remote url starts with http or https or data URI scheme.",hi please make sure input valid document local file path remote data scheme,issue,positive,positive,positive,positive,positive,positive
1356800530,"Thanks for your feedback, it fills a few cells in the matrix :) I may do
some benchmarks later I don't find the answer. I think TensorRT is the
fastest but need to check



Le sam. 17 déc. 2022, 11:44, Jie Fu ***@***.***> a écrit :

> For model comparison, we have a benchmark for this:
> https://clip-as-service.jina.ai/user-guides/benchmark/
> Also we are now working on different inference frameworks such as
> AITemplate / dynamo to find the suitable one for CLIP-as-service.
> Unfortunately we are not able to test the metrics on different hardwares
> since: we don't have sufficient hardware to test it and also some framework
> doesn't work well on specific hardware (AITemplate onV100).
>
> Does this answer your question?
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/jina-ai/clip-as-service/issues/877#issuecomment-1356061863>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADE64VLHDL43NYBMKNBKDSLWNVK37ANCNFSM6AAAAAATA3X27Q>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>
",thanks feedback matrix may later find answer think need check sam fu model comparison also working different inference dynamo find suitable one unfortunately able test metric different since sufficient hardware test also framework work well specific hardware answer question reply directly view id,issue,negative,positive,positive,positive,positive,positive
1356061863,"Hello Simon, 

Thank you for your interest in CLIP-as-service. We currently do not have the exact comparison you are looking for, however we do have benchmarks of the different models. You can find them [here](https://clip-as-service.jina.ai/user-guides/benchmark).

We are in the process of evaluating various inference frameworks, such as AITemplate/dynamo, to determine the most suitable one for each model. Unfortunately, we do not have enough hardware to test the metrics on different hardwares, and some frameworks do not perform well on certain hardware (AITemplate on V100).",hello thank interest currently exact comparison looking however different find process various inference determine suitable one model unfortunately enough hardware test metric different perform well certain hardware,issue,positive,positive,positive,positive,positive,positive
1352659214,Please fix open_clip version to v2.7.0 for now. I will take a look a the latset 2.8,please fix version take look,issue,negative,neutral,neutral,neutral,neutral,neutral
1343941794,"To give more precision, have tested this docker image (https://hub.docker.com/r/jinaai/clip-server) in both comment. In the first try (https://github.com/jina-ai/clip-as-service/issues/873#issuecomment-1341230474) I have used the image with the tag ```0.8.1``` and in the second comment (https://github.com/jina-ai/clip-as-service/issues/873#issuecomment-1342808181) I used the tag ```master```. The config and the command to start the container that I used are :

```
cat clip_config.yml | CUDA_VISIBLE_DEVICES=1 docker run -i   -p 51000:51000 -v $HOME/jina/.cache:/home/cas/.cache --gpus all jinaai/clip-server:master -i
```
And in ```clip_config.yml``` :

```
jtype: Flow
version: '1'
with:
  port: 51000
executors:
  - name: clip_t
    uses:
      jtype: CLIPEncoder
      with:
        name: ViT-L-14-336::openai
      metas:
        py_modules:
          - clip_server.executors.clip_torch
 ```",give precision tested docker image comment first try used image tag second comment used tag master command start container used cat docker run master flow version port name name,issue,negative,positive,positive,positive,positive,positive
1342808181,"I have tested with the new master image, I got this issue : 
```
jina.excepts.BadServer: request_id: ""98fc908ccd2944fa8221d5fed3f420f6""
status {
  code: ERROR
  description: ""RuntimeError(\'CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasGemmStridedBatchedExFix(
handle, opa, opb, m, n, k, (void*)(&falpha), a, CUDA_R_16F, lda, stridea, b, CUDA_R_16F, ldb, strideb, 
(void*)(&fbeta), c, CUDA_R_16F, ldc, stridec, num_batches, CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP)`\')""
  exception {
    name: ""RuntimeError""
    args: ""CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasGemmStridedBatchedExFix( handle, opa, opb, 
m, n, k, (void*)(&falpha), a, CUDA_R_16F, lda, stridea, b, CUDA_R_16F, ldb, strideb, (void*)(&fbeta), c, 
CUDA_R_16F, ldc, stridec, num_batches, CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP)`""
    stacks: ""Traceback (most recent call last):\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/jina/serve/runtimes/worker/__init__.py\"", line 222, in
process_data\n    result = await self._request_handler.handle(\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/jina/serve/runtimes/worker/request_handling.py\"", line
291, in handle\n    return_data = await self._executor.__acall__(\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/jina/serve/executors/__init__.py\"", line 354, in 
__acall__\n    return await self.__acall_endpoint__(__default_endpoint__, **kwargs)\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/jina/serve/executors/__init__.py\"", line 401, in 
__acall_endpoint__\n    return await exec_func(\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/jina/serve/executors/__init__.py\"", line 366, in 
exec_func\n    return await func(self, tracing_context=tracing_context, **kwargs)\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/jina/serve/executors/decorators.py\"", line 173, in 
arg_wrapper\n    return await fn(executor_instance, *args, **kwargs)\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/clip_server/executors/clip_torch.py\"", line 194, in 
encode\n    self._model.encode_image(**batch_data)\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/clip_server/model/openclip_model.py\"", line 64, in 
encode_image\n    return self._model.encode_image(pixel_values)\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/open_clip/model.py\"", line 182, in encode_image\n    
features = self.visual(image)\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\"", line 1190, in 
_call_impl\n    return forward_call(*input, **kwargs)\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/clip_server/model/model.py\"", line 88, in forward\n   
return super().forward(x)\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/open_clip/transformer.py\"", line 314, in forward\n    
x = self.transformer(x)\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\"", line 1190, in 
_call_impl\n    return forward_call(*input, **kwargs)\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/open_clip/transformer.py\"", line 230, in forward\n    
x = r(x, attn_mask=attn_mask)\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\"", line 1190, in 
_call_impl\n    return forward_call(*input, **kwargs)\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/open_clip/transformer.py\"", line 154, in forward\n    
x = x + self.ls_1(self.attention(self.ln_1(x), attn_mask=attn_mask))\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/open_clip/transformer.py\"", line 151, in attention\n  
return self.attn(x, x, x, need_weights=False, attn_mask=attn_mask)[0]\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\"", line 1190, in 
_call_impl\n    return forward_call(*input, **kwargs)\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/torch/nn/modules/activation.py\"", line 1167, in 
forward\n    attn_output, attn_output_weights = F.multi_head_attention_forward(\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\"", line 5160, in 
multi_head_attention_forward\n    attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))\n""
    stacks: ""RuntimeError: CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasGemmStridedBatchedExFix( 
handle, opa, opb, m, n, k, (void*)(&falpha), a, CUDA_R_16F, lda, stridea, b, CUDA_R_16F, ldb, strideb, 
(void*)(&fbeta), c, CUDA_R_16F, ldc, stridec, num_batches, CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP)`\n""
    executor: ""CLIPEncoder""
  }
}
exec_endpoint: ""/encode""
target_executor: """"
```
Here is the output of my nvidia-smi :
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-PCIE...  On   | 00000000:18:00.0 Off |                    0 |
| N/A   35C    P0    37W / 250W |   2125MiB / 32768MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-PCIE...  On   | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0    26W / 250W |      4MiB / 32768MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-PCIE...  On   | 00000000:86:00.0 Off |                    0 |
| N/A   52C    P0    65W / 250W |   3515MiB / 16384MiB |     65%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A    683662      C   python                           2121MiB |
+-----------------------------------------------------------------------------+

```",tested new master image got issue status code error description error calling handle void void exception name error calling handle void void recent call last file line result await file line await file line return await file line return await file line return await self file line return await file line file line return file line image file line return input file line return super file line file line return input file line file line return input file line file line return file line return input file line file line error calling handle void void executor output driver version version name volatile fan temp compute mig mib mib default mib mib default mib mib default type process name memory id id usage python mib,issue,negative,positive,positive,positive,positive,positive
1342731700,"Glad to see it works 🍻
You can also give the main branch another try! The problem you met should be fixed now.",glad see work also give main branch another try problem met fixed,issue,negative,positive,positive,positive,positive,positive
1342724092,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/876?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#876](https://codecov.io/gh/jina-ai/clip-as-service/pull/876?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (78fa087) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/1fe3a5a01123dcfea8a7981fc5aea212d42c1299?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (1fe3a5a) will **decrease** coverage by `11.37%`.
> The diff coverage is `100.00%`.

```diff
@@             Coverage Diff             @@
##             main     #876       +/-   ##
===========================================
- Coverage   83.12%   71.74%   -11.38%     
===========================================
  Files          22       22               
  Lines        1529     1529               
===========================================
- Hits         1271     1097      -174     
- Misses        258      432      +174     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `71.74% <100.00%> (-11.38%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/876?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/executors/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/876/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX29ubngucHk=) | `87.91% <100.00%> (+1.09%)` | :arrow_up: |
| [server/clip\_server/model/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/876/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfb25ueC5weQ==) | `87.30% <100.00%> (+20.63%)` | :arrow_up: |
| [server/clip\_server/executors/clip\_tensorrt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/876/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RlbnNvcnJ0LnB5) | `0.00% <0.00%> (-94.60%)` | :arrow_down: |
| [server/clip\_server/model/clip\_trt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/876/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfdHJ0LnB5) | `0.00% <0.00%> (-85.72%)` | :arrow_down: |
| [server/clip\_server/model/trt\_utils.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/876/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3RydF91dGlscy5weQ==) | `0.00% <0.00%> (-83.52%)` | :arrow_down: |

Help us with your feedback. Take ten seconds to tell us [how you rate us](https://about.codecov.io/nps?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Have a feature suggestion? [Share it here.](https://app.codecov.io/gh/feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
",report fa main decrease coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage help u feedback take ten tell u rate u feature suggestion share,issue,positive,positive,positive,positive,positive,positive
1342694239,"Hi @ZiniuYu, thanks it's working with this version, I was using a wrong version in my last comment",hi thanks working version wrong version last comment,issue,negative,negative,neutral,neutral,negative,negative
1342280697,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/875?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#875](https://codecov.io/gh/jina-ai/clip-as-service/pull/875?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (32a1dd8) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/0b293ec834e80f7335aa625d683904594373a607?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (0b293ec) will **decrease** coverage by `14.22%`.
> The diff coverage is `0.00%`.

```diff
@@             Coverage Diff             @@
##             main     #875       +/-   ##
===========================================
- Coverage   83.06%   68.83%   -14.23%     
===========================================
  Files          22       22               
  Lines        1529     1534        +5     
===========================================
- Hits         1270     1056      -214     
- Misses        259      478      +219     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `68.83% <0.00%> (-14.23%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/875?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/executors/clip\_tensorrt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/875/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RlbnNvcnJ0LnB5) | `0.00% <0.00%> (-94.60%)` | :arrow_down: |
| [server/clip\_server/model/clip\_trt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/875/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfdHJ0LnB5) | `0.00% <0.00%> (-85.72%)` | :arrow_down: |
| [server/clip\_server/model/trt\_utils.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/875/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3RydF91dGlscy5weQ==) | `0.00% <0.00%> (-83.52%)` | :arrow_down: |
| [server/clip\_server/model/clip.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/875/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXAucHk=) | `68.75% <0.00%> (-18.75%)` | :arrow_down: |
| [server/clip\_server/model/pretrained\_models.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/875/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3ByZXRyYWluZWRfbW9kZWxzLnB5) | `84.12% <0.00%> (-14.29%)` | :arrow_down: |
| [client/clip\_client/client.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/875/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L2NsaWVudC5weQ==) | `79.87% <0.00%> (-7.51%)` | :arrow_down: |
| [server/clip\_server/model/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/875/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfb25ueC5weQ==) | `82.53% <0.00%> (+17.46%)` | :arrow_up: |

Help us with your feedback. Take ten seconds to tell us [how you rate us](https://about.codecov.io/nps?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Have a feature suggestion? [Share it here.](https://app.codecov.io/gh/feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
",report add main decrease coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage help u feedback take ten tell u rate u feature suggestion share,issue,positive,positive,positive,positive,positive,positive
1341928954,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/874?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#874](https://codecov.io/gh/jina-ai/clip-as-service/pull/874?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (e90c78d) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/fd16e5abef94e274572d40912f12baeffece8696?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (fd16e5a) will **not change** coverage.
> The diff coverage is `n/a`.

```diff
@@           Coverage Diff           @@
##             main     #874   +/-   ##
=======================================
  Coverage   81.04%   81.04%           
=======================================
  Files          22       22           
  Lines        1498     1498           
=======================================
  Hits         1214     1214           
  Misses        284      284           
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `81.04% <ø> (ø)` | |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.


Help us with your feedback. Take ten seconds to tell us [how you rate us](https://about.codecov.io/nps?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Have a feature suggestion? [Share it here.](https://app.codecov.io/gh/feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
",report main change coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find help u feedback take ten tell u rate u feature suggestion share,issue,positive,positive,positive,positive,positive,positive
1341230474,"Thanks @ZiniuYu, but when I try this version 0.8.1 from dockerhub I have this issue :
```
jina.excepts.BadServer: request_id: ""3732bde0aa46429da0be6f4638c50b08""
status {
  code: ERROR
  description: ""RuntimeError(\'CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasGemmStridedBatchedExFix(
handle, opa, opb, m, n, k, (void*)(&falpha), a, CUDA_R_16F, lda, stridea, b, CUDA_R_16F, ldb, strideb, 
(void*)(&fbeta), c, CUDA_R_16F, ldc, stridec, num_batches, CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP)`\')""
  exception {
    name: ""RuntimeError""
    args: ""CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasGemmStridedBatchedExFix( handle, opa, opb, 
m, n, k, (void*)(&falpha), a, CUDA_R_16F, lda, stridea, b, CUDA_R_16F, ldb, strideb, (void*)(&fbeta), c, 
CUDA_R_16F, ldc, stridec, num_batches, CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP)`""
    stacks: ""Traceback (most recent call last):\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/jina/serve/runtimes/worker/__init__.py\"", line 219, in
process_data\n    result = await self._data_request_handler.handle(\n""
    stacks: ""  File 
\""/usr/local/lib/python3.8/dist-packages/jina/serve/runtimes/request_handlers/data_request_handler.py\"", line 228, 
in handle\n    return_data = await self._executor.__acall__(\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/jina/serve/executors/__init__.py\"", line 329, in 
__acall__\n    return await self.__acall_endpoint__(__default_endpoint__, **kwargs)\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/jina/serve/executors/__init__.py\"", line 378, in 
__acall_endpoint__\n    return await exec_func(\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/jina/serve/executors/__init__.py\"", line 339, in 
exec_func\n    return await func(self, tracing_context=tracing_context, **kwargs)\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/jina/serve/executors/decorators.py\"", line 153, in 
arg_wrapper\n    return await fn(executor_instance, *args, **kwargs)\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/clip_server/executors/clip_torch.py\"", line 140, in 
encode\n    self._model.encode_image(**batch_data)\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/clip_server/model/openclip_model.py\"", line 51, in 
encode_image\n    return self._model.encode_image(pixel_values)\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/clip_server/model/model.py\"", line 591, in 
encode_image\n    return self.visual(image.type(self.dtype))\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\"", line 1190, in 
_call_impl\n    return forward_call(*input, **kwargs)\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/clip_server/model/model.py\"", line 428, in forward\n  
x = self.transformer(x)\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\"", line 1190, in 
_call_impl\n    return forward_call(*input, **kwargs)\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/clip_server/model/model.py\"", line 353, in forward\n  
x = r(x, attn_mask=attn_mask)\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\"", line 1190, in 
_call_impl\n    return forward_call(*input, **kwargs)\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/clip_server/model/model.py\"", line 322, in forward\n  
x = x + self.ln_attn(self.attention(self.ln_1(x), attn_mask=attn_mask))\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/clip_server/model/model.py\"", line 317, in attention\n
return self.attn(x, x, x, need_weights=False, attn_mask=attn_mask)[0]\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\"", line 1190, in 
_call_impl\n    return forward_call(*input, **kwargs)\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/torch/nn/modules/activation.py\"", line 1167, in 
forward\n    attn_output, attn_output_weights = F.multi_head_attention_forward(\n""
    stacks: ""  File \""/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\"", line 5160, in 
multi_head_attention_forward\n    attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))\n""
    stacks: ""RuntimeError: CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasGemmStridedBatchedExFix( 
handle, opa, opb, m, n, k, (void*)(&falpha), a, CUDA_R_16F, lda, stridea, b, CUDA_R_16F, ldb, strideb, 
(void*)(&fbeta), c, CUDA_R_16F, ldc, stridec, num_batches, CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP)`\n""
    executor: ""CLIPEncoder""
  }
}
exec_endpoint: ""/encode""
target_executor: """"
```",thanks try version issue status code error description error calling handle void void exception name error calling handle void void recent call last file line result await file line await file line return await file line return await file line return await self file line return await file line file line return file line return file line return input file line file line return input file line file line return input file line file line return file line return input file line file line error calling handle void void executor,issue,negative,positive,neutral,neutral,positive,positive
1341198440,"@TopTea1 You can use our pre-build docker image to get around with the error like this:
```
jtype: Flow
with:
  port: 51000
executors:
  - name: clip_t
    uses: jinahub+docker://CLIPTorchEncoder/0.8.1
    uses_with:
      name: ViT-L-14-336::openai
```",use docker image get around error like flow port name name,issue,negative,neutral,neutral,neutral,neutral,neutral
1341191358,"Hi @TopTea1 , thanks for reporting this!

This is a known issue and we are fixing it.",hi thanks known issue fixing,issue,negative,positive,positive,positive,positive,positive
1340354960,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/872?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#872](https://codecov.io/gh/jina-ai/clip-as-service/pull/872?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (e38c854) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/1eebdd7f489abb8e694226d5c5c29b011eab229a?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (1eebdd7) will **increase** coverage by `10.34%`.
> The diff coverage is `100.00%`.

```diff
@@             Coverage Diff             @@
##             main     #872       +/-   ##
===========================================
+ Coverage   70.69%   81.04%   +10.34%     
===========================================
  Files          22       22               
  Lines        1498     1498               
===========================================
+ Hits         1059     1214      +155     
+ Misses        439      284      -155     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `81.04% <100.00%> (+10.34%)` | :arrow_up: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/872?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/model/model.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/872/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL21vZGVsLnB5) | `75.09% <100.00%> (ø)` | |
| [server/clip\_server/model/trt\_utils.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/872/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3RydF91dGlscy5weQ==) | `56.04% <0.00%> (+56.04%)` | :arrow_up: |
| [server/clip\_server/model/clip\_trt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/872/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfdHJ0LnB5) | `69.38% <0.00%> (+69.38%)` | :arrow_up: |
| [server/clip\_server/executors/clip\_tensorrt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/872/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RlbnNvcnJ0LnB5) | `94.59% <0.00%> (+94.59%)` | :arrow_up: |

Help us with your feedback. Take ten seconds to tell us [how you rate us](https://about.codecov.io/nps?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Have a feature suggestion? [Share it here.](https://app.codecov.io/gh/feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
",report main increase coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage help u feedback take ten tell u rate u feature suggestion share,issue,positive,positive,positive,positive,positive,positive
1334001953,"> can you fix the commit message checking error by removing the dot? It complains: `subject may not end with full stop` @zawabest

done",fix commit message error removing dot subject may end full stop done,issue,negative,positive,neutral,neutral,positive,positive
1333543331,Since rust-tokenizer is not pip installable. I would close this PR atm. ,since pip would close,issue,negative,neutral,neutral,neutral,neutral,neutral
1333537659,can you fix the commit message checking error by removing the dot? It complains: `subject may not end with full stop` @zawabest ,fix commit message error removing dot subject may end full stop,issue,negative,positive,neutral,neutral,positive,positive
1326973592,"Can you successfully import this?  @flamz3d 
`import importlib` and then `importlib.import_module('clip_server.executors.clip_torch')`
Here is my output for this:
<img width=""926"" alt=""Screen Shot 2022-11-25 at 11 01 38 AM"" src=""https://user-images.githubusercontent.com/16580382/203891906-209a75b9-c55b-40b8-bee2-504562b039eb.png"">

We found that there is a compatibility issue in python3.7 for `torchvision`. For now, the quick solution is to downgrade the `torchvision` to 0.13.0 (the default version is 0.14.0 if you install clip-server using pip install) and everything should be fine.",successfully import import output screen shot found compatibility issue python quick solution downgrade default version install pip install everything fine,issue,positive,positive,positive,positive,positive,positive
1326142820,What's the jina version you use? You can paste the output of `jina -vf` here @flamz3d ,jina version use paste output jina,issue,negative,neutral,neutral,neutral,neutral,neutral
1326139680,"Hello @flamz3d ,

thanks for opening the ticket, I am going to move to `clip-as-a-service` repository",hello thanks opening ticket going move repository,issue,negative,positive,positive,positive,positive,positive
1325003984,"To reproduce, I created a clean py3.7 environment via conda on PRETTY_NAME=""Debian GNU/Linux 10 (buster)"" , then ran 

```
pip install clip-server
python -m clip_server
``` 

output of pip show

```
Name: pip
Version: 22.3.1
Summary: The PyPA recommended tool for installing Python packages.
Home-page: https://pip.pypa.io/
Author: The pip developers
Author-email: distutils-sig@python.org
License: MIT
Location: /opt/conda/envs/jina/lib/python3.7/site-packages
Requires: 
Required-by: 
---
Name: clip-server
Version: 0.8.1
Summary: Embed images and sentences into fixed-length vectors via CLIP
Home-page: https://github.com/jina-ai/clip-as-service
Author: Jina AI
Author-email: hello@jina.ai
License: Apache 2.0
Location: /opt/conda/envs/jina/lib/python3.7/site-packages
Requires: ftfy, jina, open-clip-torch, prometheus-client, regex, torch, torchvision
Required-by:
```

output of python -m clip_server

```
⠋ Waiting ... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0/0 -:--:--ERROR  clip_t/rep-0@2172 fail to load file dependency                                       [11/23/22 12:45:01]
ERROR  clip_t/rep-0@2172 FileNotFoundError('can not find clip_server.executors.clip_torch') [11/23/22 12:45:01]
       during <class 'jina.serve.runtimes.worker.WorkerRuntime'> initialization                                
        add ""--quiet-error"" to suppress the exception details                                                  
       Traceback (most recent call last):                                                                      
         File                                                                                                  
       ""/opt/conda/envs/jina/lib/python3.7/site-packages/jina/orchestrate/pods/__init__.py…                    
       line 75, in run                                                                                         
           args=args,                                                                                          
         File                                                                                                  
       ""/opt/conda/envs/jina/lib/python3.7/site-packages/jina/serve/runtimes/worker/__init…                    
       line 36, in __init__                                                                                    
           super().__init__(args, **kwargs)                                                                    
         File                                                                                                  
       ""/opt/conda/envs/jina/lib/python3.7/site-packages/jina/serve/runtimes/asyncio.py"",                      
       line 80, in __init__                                                                                    
           self._loop.run_until_complete(self.async_setup())                                                   
         File ""/opt/conda/envs/jina/lib/python3.7/asyncio/base_events.py"", line 587, in                        
       run_until_complete                                                                                      
           return future.result()                                                                              
         File                                                                                                  
       ""/opt/conda/envs/jina/lib/python3.7/site-packages/jina/serve/runtimes/worker/__init…                    
       line 106, in async_setup                                                                                
           self.meter_provider,                                                                                
         File                                                                                                  
       ""/opt/conda/envs/jina/lib/python3.7/site-packages/jina/serve/runtimes/request_handl…                    
       line 52, in __init__                                                                                    
           meter_provider=meter_provider,                                                                      
         File                                                                                                  
       ""/opt/conda/envs/jina/lib/python3.7/site-packages/jina/serve/runtimes/request_handl…                    
       line 155, in _load_executor                                                                             
           extra_search_paths=self.args.extra_search_paths,                                                    
         File ""/opt/conda/envs/jina/lib/python3.7/site-packages/jina/jaml/__init__.py"",                        
       line 734, in load_config                                                                                
           else _extra_search_paths,                                                                           
         File ""/opt/conda/envs/jina/lib/python3.7/site-packages/jina/jaml/helper.py"", line                     
       285, in load_py_modules                                                                                 
           PathImporter.add_modules(*mod)                                                                      
         File ""/opt/conda/envs/jina/lib/python3.7/site-packages/jina/importer.py"", line                        
       159, in add_modules                                                                                     
           _path_import(complete_path(m))                                                                      
         File ""/opt/conda/envs/jina/lib/python3.7/site-packages/jina/jaml/helper.py"", line                     
       229, in complete_path                                                                                   
           raise FileNotFoundError(f'can not find {path}')                                                     
       FileNotFoundError: can not find clip_server.executors.clip_torch                                        
ERROR  Flow@2164 Flow is aborted due to ['clip_t'] can not be started.                      [11/23/22 12:45:02]
Traceback (most recent call last):
  File ""/opt/conda/envs/jina/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/opt/conda/envs/jina/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/opt/conda/envs/jina/lib/python3.7/site-packages/clip_server/__main__.py"", line 25, in <module>
    with f:
  File ""/opt/conda/envs/jina/lib/python3.7/site-packages/jina/orchestrate/flow/base.py"", line 1556, in __enter__
    return self.start()
  File ""/opt/conda/envs/jina/lib/python3.7/site-packages/jina/orchestrate/flow/builder.py"", line 33, in arg_wrapper
    return func(self, *args, **kwargs)
  File ""/opt/conda/envs/jina/lib/python3.7/site-packages/jina/orchestrate/flow/base.py"", line 1620, in start
    self._wait_until_all_ready()
  File ""/opt/conda/envs/jina/lib/python3.7/site-packages/jina/orchestrate/flow/base.py"", line 1719, in _wait_until_all_ready
    raise RuntimeFailToStart
jina.excepts.RuntimeFailToStart
```


",reproduce clean environment via buster ran pip install python output pip show name pip version summary tool python author pip license location name version summary embed via clip author jina ai hello license apache location jina torch output python waiting error fail load file dependency error find class add suppress exception recent call last file line run file line super file line file line return file line file line file line file line else file line file line file line raise find path find error flow flow aborted due recent call last file line file line code file line module file line return file line return self file line start file line raise,issue,negative,positive,neutral,neutral,positive,positive
1324560038,@flamz3d Could you provide more context for your problem? what's the output of `jina -vf` on your side? ,could provide context problem output jina side,issue,negative,neutral,neutral,neutral,neutral,neutral
1323793322,"I get a similar problem on linux Debian

```
       FileNotFoundError: can not find clip_server.executors.clip_torch                                                          
ERROR  Flow@18076 Flow is aborted due to ['clip_t'] can not be started. 
```",get similar problem find error flow flow aborted due,issue,negative,negative,neutral,neutral,negative,negative
1323337819,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/864?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#864](https://codecov.io/gh/jina-ai/clip-as-service/pull/864?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (6661484) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/e4717a35f850e6a2cd8b4d8b4c994fad30fd5c72?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (e4717a3) will **not change** coverage.
> The diff coverage is `100.00%`.

```diff
@@           Coverage Diff           @@
##             main     #864   +/-   ##
=======================================
  Coverage   80.28%   80.28%           
=======================================
  Files          22       22           
  Lines        1633     1633           
=======================================
  Hits         1311     1311           
  Misses        322      322           
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `80.28% <100.00%> (ø)` | |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/864?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [client/clip\_client/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/864/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/864/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |

Help us with your feedback. Take ten seconds to tell us [how you rate us](https://about.codecov.io/nps?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Have a feature suggestion? [Share it here.](https://app.codecov.io/gh/feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
",report main ea change coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage help u feedback take ten tell u rate u feature suggestion share,issue,positive,positive,positive,positive,positive,positive
1322323653,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/862?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#862](https://codecov.io/gh/jina-ai/clip-as-service/pull/862?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (2b52a93) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/e4717a35f850e6a2cd8b4d8b4c994fad30fd5c72?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (e4717a3) will **decrease** coverage by `2.02%`.
> The diff coverage is `100.00%`.

```diff
@@            Coverage Diff             @@
##             main     #862      +/-   ##
==========================================
- Coverage   80.28%   78.26%   -2.03%     
==========================================
  Files          22       22              
  Lines        1633     1633              
==========================================
- Hits         1311     1278      -33     
- Misses        322      355      +33     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `78.26% <100.00%> (-2.03%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/862?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [client/clip\_client/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/862/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/862/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/model/trt\_utils.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/862/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3RydF91dGlscy5weQ==) | `56.04% <0.00%> (-27.48%)` | :arrow_down: |
| [server/clip\_server/model/clip\_trt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/862/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfdHJ0LnB5) | `69.38% <0.00%> (-16.33%)` | :arrow_down: |

Help us with your feedback. Take ten seconds to tell us [how you rate us](https://about.codecov.io/nps?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Have a feature suggestion? [Share it here.](https://app.codecov.io/gh/feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
",report ba main ea decrease coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage help u feedback take ten tell u rate u feature suggestion share,issue,positive,positive,positive,positive,positive,positive
1319528384,Wait until the new models uploaded to pass the ci test,wait new pas test,issue,negative,positive,positive,positive,positive,positive
1316961241,I will fix the failing pr after upgrading the jina version to 3.11.1.,fix failing jina version,issue,negative,neutral,neutral,neutral,neutral,neutral
1316709149,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/860?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#860](https://codecov.io/gh/jina-ai/clip-as-service/pull/860?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (04c130a) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/e4717a35f850e6a2cd8b4d8b4c994fad30fd5c72?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (e4717a3) will **increase** coverage by `0.10%`.
> The diff coverage is `91.74%`.

```diff
@@            Coverage Diff             @@
##             main     #860      +/-   ##
==========================================
+ Coverage   80.28%   80.38%   +0.10%     
==========================================
  Files          22       22              
  Lines        1633     1448     -185     
==========================================
- Hits         1311     1164     -147     
+ Misses        322      284      -38     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `80.38% <91.74%> (+0.10%)` | :arrow_up: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/860?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/model/flash\_attention.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/860/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2ZsYXNoX2F0dGVudGlvbi5weQ==) | `22.22% <ø> (+2.22%)` | :arrow_up: |
| [server/clip\_server/model/pretrained\_models.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/860/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3ByZXRyYWluZWRfbW9kZWxzLnB5) | `98.41% <ø> (ø)` | |
| [server/clip\_server/model/model.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/860/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL21vZGVsLnB5) | `75.09% <91.42%> (+4.98%)` | :arrow_up: |
| [client/clip\_client/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/860/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/860/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/model/openclip\_model.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/860/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL29wZW5jbGlwX21vZGVsLnB5) | `93.10% <100.00%> (+3.44%)` | :arrow_up: |
| [server/clip\_server/model/trt\_utils.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/860/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3RydF91dGlscy5weQ==) | `56.04% <0.00%> (-27.48%)` | :arrow_down: |
| [server/clip\_server/model/clip\_trt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/860/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfdHJ0LnB5) | `69.38% <0.00%> (-16.33%)` | :arrow_down: |

Help us with your feedback. Take ten seconds to tell us [how you rate us](https://about.codecov.io/nps?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Have a feature suggestion? [Share it here.](https://app.codecov.io/gh/feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
",report ca main ea increase coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage help u feedback take ten tell u rate u feature suggestion share,issue,positive,positive,positive,positive,positive,positive
1316665682,"To transfer the code, we merge into  a new subbranch first #860 .",transfer code merge new subbranch first,issue,negative,positive,positive,positive,positive,positive
1316313343,@OrangeSodahub Please make sure the changes will not change the embedding result for each model. ,please make sure change result model,issue,positive,positive,positive,positive,positive,positive
1315048066,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/859?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#859](https://codecov.io/gh/jina-ai/clip-as-service/pull/859?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (9b0a267) into [bump-openclip-v25](https://codecov.io/gh/jina-ai/clip-as-service/commit/e4717a35f850e6a2cd8b4d8b4c994fad30fd5c72?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (e4717a3) will **decrease** coverage by `39.56%`.
> The diff coverage is `56.49%`.

> :exclamation: Current head 9b0a267 differs from pull request most recent head 50528b3. Consider uploading reports for the commit 50528b3 to get more accurate results

```diff
@@                  Coverage Diff                   @@
##           bump-openclip-v25     #859       +/-   ##
======================================================
- Coverage              80.28%   40.71%   -39.57%     
======================================================
  Files                     22       25        +3     
  Lines                   1633     1820      +187     
======================================================
- Hits                    1311      741      -570     
- Misses                   322     1079      +757     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `40.71% <56.49%> (-39.57%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/859?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/model/pretrained\_models.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/859/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3ByZXRyYWluZWRfbW9kZWxzLnB5) | `77.77% <ø> (-20.64%)` | :arrow_down: |
| [server/clip\_server/model/utils.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/859/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3V0aWxzLnB5) | `41.66% <41.66%> (ø)` | |
| [server/clip\_server/model/transformer.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/859/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3RyYW5zZm9ybWVyLnB5) | `50.99% <50.99%> (ø)` | |
| [server/clip\_server/model/model.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/859/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL21vZGVsLnB5) | `53.35% <57.00%> (-16.76%)` | :arrow_down: |
| [server/clip\_server/model/modified\_resnet.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/859/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL21vZGlmaWVkX3Jlc25ldC5weQ==) | `69.91% <69.91%> (ø)` | |
| [server/clip\_server/model/mclip\_model.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/859/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL21jbGlwX21vZGVsLnB5) | `82.05% <75.00%> (-1.29%)` | :arrow_down: |
| [server/clip\_server/model/simple\_tokenizer.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/859/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3NpbXBsZV90b2tlbml6ZXIucHk=) | `0.00% <0.00%> (-94.90%)` | :arrow_down: |
| [server/clip\_server/executors/clip\_tensorrt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/859/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RlbnNvcnJ0LnB5) | `0.00% <0.00%> (-93.11%)` | :arrow_down: |
| [server/clip\_server/model/clip\_trt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/859/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfdHJ0LnB5) | `0.00% <0.00%> (-85.72%)` | :arrow_down: |
| [server/clip\_server/model/trt\_utils.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/859/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3RydF91dGlscy5weQ==) | `0.00% <0.00%> (-83.52%)` | :arrow_down: |
| ... and [12 more](https://codecov.io/gh/jina-ai/clip-as-service/pull/859/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | |

Help us with your feedback. Take ten seconds to tell us [how you rate us](https://about.codecov.io/nps?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Have a feature suggestion? [Share it here.](https://app.codecov.io/gh/feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
",report ba ea decrease coverage coverage exclamation current head ba pull request recent head consider commit get accurate coverage coverage flag coverage carried forward coverage wo shown click find impacted coverage help u feedback take ten tell u rate u feature suggestion share,issue,positive,positive,positive,positive,positive,positive
1313745164,@numb3r3 Noted! Thanks. Do keep me updated if you have any progress.,noted thanks keep progress,issue,positive,positive,positive,positive,positive,positive
1313547831,"@kaushikb11 so far, we cannot reproduce your error on our side (exactly the same envs, including M1 Pro, jina, docarray, pytorch version). We guess this is an upstream issue related to `pytorch` installation. We just need more time to verify this issue. Of course, any more feedbacks are welcome. I believe our community would also face this problem.  ",far reproduce error side exactly pro jina version guess upstream issue related installation need time verify issue course welcome believe community would also face problem,issue,negative,positive,positive,positive,positive,positive
1313205125,"Traceback when I run the client
```bash
DEBUG  GRPCClient@9945 connected to 0.0.0.0:51000                                                                                            [11/14/22 12:50:55]
Traceback (most recent call last):
  File ""/Users/kaushikbokka/apps/search-app/venv/lib/python3.8/site-packages/jina/clients/helper.py"", line 47, in _arg_wrapper
    return func(*args, **kwargs)
  File ""/Users/kaushikbokka/apps/search-app/venv/lib/python3.8/site-packages/clip_client/client.py"", line 153, in _gather_result
    results[r[:, 'id']][:, attribute] = r[:, attribute]
  File ""/Users/kaushikbokka/apps/search-app/venv/lib/python3.8/site-packages/docarray/array/mixins/getitem.py"", line 102, in __getitem__
    elif isinstance(index[0], bool):
IndexError: list index out of range

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""check.py"", line 9, in <module>
    r = client.encode([""She is in pain"", ""what's pain""])
  File ""/Users/kaushikbokka/apps/search-app/venv/lib/python3.8/site-packages/clip_client/client.py"", line 295, in encode
    self._client.post(
  File ""/Users/kaushikbokka/apps/search-app/venv/lib/python3.8/site-packages/jina/clients/mixin.py"", line 271, in post
    return run_async(
  File ""/Users/kaushikbokka/apps/search-app/venv/lib/python3.8/site-packages/jina/helper.py"", line 1334, in run_async
    return asyncio.run(func(*args, **kwargs))
  File ""/opt/homebrew/Cellar/python@3.8/3.8.15/Frameworks/Python.framework/Versions/3.8/lib/python3.8/asyncio/runners.py"", line 44, in run
    return loop.run_until_complete(main)
  File ""/opt/homebrew/Cellar/python@3.8/3.8.15/Frameworks/Python.framework/Versions/3.8/lib/python3.8/asyncio/base_events.py"", line 616, in run_until_complete
    return future.result()
  File ""/Users/kaushikbokka/apps/search-app/venv/lib/python3.8/site-packages/jina/clients/mixin.py"", line 262, in _get_results
    async for resp in c._get_results(*args, **kwargs):
  File ""/Users/kaushikbokka/apps/search-app/venv/lib/python3.8/site-packages/jina/clients/base/grpc.py"", line 131, in _get_results
    callback_exec(
  File ""/Users/kaushikbokka/apps/search-app/venv/lib/python3.8/site-packages/jina/clients/helper.py"", line 83, in callback_exec
    _safe_callback(on_done, continue_on_error, logger)(response)
  File ""/Users/kaushikbokka/apps/search-app/venv/lib/python3.8/site-packages/jina/clients/helper.py"", line 49, in _arg_wrapper
    err_msg = f'uncaught exception in callback {func.__name__}(): {ex!r}'
AttributeError: 'functools.partial' object has no attribute '__name__'
```

Server side
```bash
python3 -m clip_server                                                                                            search-app 12:49:45
⠋ Waiting ... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0/0 -:--:--DEBUG  gateway/rep-0/GatewayRuntime@9744 adding connection for deployment clip_t/heads/0 to grpc://0.0.0.0:65282                             [11/14/22 12:49:53]
DEBUG  gateway/rep-0/GatewayRuntime@9744 start server bound to 0.0.0.0:51000                                                                                    
DEBUG  gateway/rep-0@9729 ready and listening                                                                                                [11/14/22 12:49:53]
⠼ Waiting clip_t... ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 2/3 0:00:03DEBUG  clip_t/rep-0@9743 <clip_server.executors.clip_torch.CLIPEncoder object at 0x13f4d1dc0> is successfully loaded!                        [11/14/22 12:49:57]
DEBUG  clip_t/rep-0@9743 start listening on 0.0.0.0:65282                                                                                                       
DEBUG  clip_t/rep-0@9729 ready and listening                                                                                                 [11/14/22 12:49:57]
────────────────────────────────────────────────────────────────── 🎉 Flow is ready to serve! ──────────────────────────────────────────────────────────────────
╭────────────── 🔗 Endpoint ───────────────╮
│  ⛓      Protocol                   GRPC  │
│  🏠        Local          0.0.0.0:51000  │
│  🔒      Private    10.191.62.138:51000  │
│  🌍       Public             None:51000  │
╰──────────────────────────────────────────╯
DEBUG  Flow@9729 2 Deployments (i.e. 2 Pods) are running in this Flow                                                                        [11/14/22 12:49:57]
DEBUG  clip_t/rep-0@9743 got an endpoint discovery request                                                                                   [11/14/22 12:50:22]
DEBUG  clip_t/rep-0@9743 recv DataRequest at /encode with id: 4a0fa5aa31ca493e9f316474cb5909a7                                                                  
DEBUG  clip_t/rep-0@9743 recv DataRequest at /encode with id: 1842d6550a384061bea12795770d5cf9                                               [11/14/22 12:50:25]
DEBUG  clip_t/rep-0@9743 recv DataRequest at /encode with id: 766e528658bf4b6f81b0f5ce96631d7c                                               [11/14/22 12:50:55]
DEBUG  gateway/rep-0/GatewayRuntime@9744 GRPC call failed with code StatusCode.UNAVAILABLE, retry attempt 1/3. Trying next replica, if       [11/14/22 12:50:55]
       available.                                                                                                                                               
DEBUG  gateway/rep-0/GatewayRuntime@9744 GRPC call failed with code StatusCode.UNAVAILABLE, retry attempt 2/3. Trying next replica, if                          
       available.                                                                                                                                               
DEBUG  gateway/rep-0/GatewayRuntime@9744 GRPC call failed with code StatusCode.UNAVAILABLE, retry attempt 3/3. Trying next replica, if                          
       available.                                                                                                                                               
DEBUG  gateway/rep-0/GatewayRuntime@9744 GRPC call failed, retries exhausted                                                                                    
DEBUG  gateway/rep-0/GatewayRuntime@9744 resetting connection to 0.0.0.0:65282                                                                                  
ERROR  gateway/rep-0/GatewayRuntime@9744 Error while getting responses from deployments: failed to connect to all addresses |Gateway:                           
       Communication error with deployment clip_t at address(es) {'0.0.0.0:65282'}. Head or worker(s) may be down.  
```

",run client bash connected recent call last file line return file line attribute attribute file line index bool list index range handling exception another exception recent call last file line module pain pain file line encode file line post return file line return file line run return main file line return file line resp file line file line logger response file line exception ex object attribute server side bash python waiting connection deployment start server bound ready listening waiting object successfully loaded start listening ready listening flow ready serve link protocol house local locked private public none flow running flow got discovery request id id id call code retry attempt trying next replica available call code retry attempt trying next replica available call code retry attempt trying next replica available call exhausted connection error error getting connect communication error deployment address e head worker may,issue,negative,positive,positive,positive,positive,positive
1313202068,"@kaushikb11  The environment looks legit.
Can you also please rerun everything with `export JINA_LOG_LEVEL=DEBUG` and paste the output here?",environment legit also please rerun everything export paste output,issue,negative,neutral,neutral,neutral,neutral,neutral
1313171832,Let me know if I could help you with anything else. fyi: The system is Mac M1 Pro,let know could help anything else system mac pro,issue,negative,neutral,neutral,neutral,neutral,neutral
1313171170,"> What's the output for r = client.encode(['she smiled, with pain']) and r = client.encode(['she smiled, with pain', 'what is pain?'])? I am wondering why they had different behaviors

@jemmyshin I have no idea. The first returned an embedding. ",output pain pain pain wondering different idea first returned,issue,negative,positive,positive,positive,positive,positive
1313170292,"@numb3r3 PyTorch versions
```bash
pip3 freeze | grep torch                                                                                         
open-clip-torch==2.4.1
torch==1.13.0
torchmetrics==0.10.2
torchvision==0.14.0
```",bash pip freeze torch,issue,negative,neutral,neutral,neutral,neutral,neutral
1313169772,"@ZiniuYu 
Here you go
```bash
jina -vf                                                                                                          search-app 19:02:13
- jina 3.11.0
- docarray 0.18.1
- jcloud 0.0.36
- jina-hubble-sdk 0.22.2
- jina-proto 0.1.13
- protobuf 3.20.3
- proto-backend python
- grpcio 1.47.2
- pyyaml 6.0
- python 3.8.15
- platform Darwin
- platform-release 22.1.0
- platform-version Darwin Kernel Version 22.1.0: Sun Oct 9 20:15:09 PDT 2022; root:xnu-8792.41.9~2/RELEASE_ARM64_T6000
- architecture arm64
- processor arm
- uid 55969664184872
- session-id 94442b94-63e7-11ed-ad67-32e773f3b228
- uptime 2022-11-14T12:12:58.912197
- ci-vendor (unset)
- internal False
* JINA_DEFAULT_HOST (unset)
* JINA_DEFAULT_TIMEOUT_CTRL (unset)
* JINA_DEPLOYMENT_NAME (unset)
* JINA_DISABLE_UVLOOP (unset)
* JINA_EARLY_STOP (unset)
* JINA_FULL_CLI (unset)
* JINA_GATEWAY_IMAGE (unset)
* JINA_GRPC_RECV_BYTES (unset)
* JINA_GRPC_SEND_BYTES (unset)
* JINA_HUB_NO_IMAGE_REBUILD (unset)
* JINA_LOG_CONFIG (unset)
* JINA_LOG_LEVEL (unset)
* JINA_LOG_NO_COLOR (unset)
* JINA_MP_START_METHOD (unset)
* JINA_OPTOUT_TELEMETRY (unset)
* JINA_RANDOM_PORT_MAX (unset)
* JINA_RANDOM_PORT_MIN (unset)
* JINA_LOCKS_ROOT (unset)
* JINA_K8S_ACCESS_MODES (unset)
* JINA_K8S_STORAGE_CLASS_NAME (unset)
* JINA_K8S_STORAGE_CAPACITY (unset)
```",go bash jina jina python python platform kernel version sun root architecture arm processor arm unset internal false unset unset unset unset unset unset unset unset unset unset unset unset unset unset unset unset unset unset unset unset unset,issue,negative,negative,negative,negative,negative,negative
1313043181,"Hi @kaushikb11 , 
What's your output of `jina -vf`?
Can you also try `export JINA_LOG_LEVEL=DEBUG` and rerun your code?",hi output jina also try export rerun code,issue,negative,neutral,neutral,neutral,neutral,neutral
1312959867,"What's the output for `r = client.encode(['she smiled, with pain'])` and `r = client.encode(['she smiled, with pain', 'what is pain?'])`? I am wondering why they had different behaviors. @kaushikb11 ",output pain pain pain wondering different,issue,negative,neutral,neutral,neutral,neutral,neutral
1312446730,"yes, it worked the initial time with
```python
r = client.encode(['she smiled, with pain'])
```
but not with two strings of text
```python
r = client.encode(['she smiled, with pain', 'what is pain?'])
```
It failed with a single image as well
```python
r = client.encode(['https://clip-as-service.jina.ai/_static/favicon.png'])
```",yes worked initial time python pain two text python pain pain single image well python,issue,negative,negative,neutral,neutral,negative,negative
1312446199,"@kaushikb11, have u tried sending first only text and later only images?",tried sending first text later,issue,negative,positive,positive,positive,positive,positive
1312395178,"Client code
```python
from clip_client import Client

client = Client('grpc://0.0.0.0:51000')

r = client.encode(['she smiled, with pain', 'https://clip-as-service.jina.ai/_static/favicon.png'])

print(r)
```",client code python import client client client pain print,issue,negative,neutral,neutral,neutral,neutral,neutral
1312394736,"```bash
❯ python3 -m clip_server                                       search-app 12:27:07
────────────────────────────────────────────────────────────────── 🎉 Flow is ready to serve! ──────────────────────────────────────────────────────────────────
╭────────────── 🔗 Endpoint ───────────────╮
│  ⛓      Protocol                   GRPC  │
│  🏠        Local          0.0.0.0:51000  │
│  🔒      Private     192.168.1.47:51000  │
│  🌍       Public             None:51000  │
╰──────────────────────────────────────────╯
ERROR  gateway/rep-0/GatewayRuntime@5462 Error while getting responses from deployments: failed to connect to all addresses |Gateway:        [11/12/22 12:31:35]
       Communication error with deployment clip_t at address(es) {'0.0.0.0:59354'}. Head or worker(s) may be down.                                              
ERROR  gateway/rep-0/GatewayRuntime@5462 Error while getting responses from deployments: failed to connect to all addresses |Gateway:        [11/12/22 12:31:39]
       Communication error with deployment clip_t at address(es) {'0.0.0.0:59354'}. Head or worker(s) may be down.                                              
ERROR  gateway/rep-0/GatewayRuntime@5462 Error while getting responses from deployments: failed to connect to all addresses |Gateway:        [11/12/22 12:32:15]
       Communication error with deployment clip_t at address(es) {'0.0.0.0:59354'}. Head or worker(s) may be down.                                              
ERROR  gateway/rep-0/GatewayRuntime@5462 Error while getting responses from deployments: failed to connect to all addresses |Gateway:        [11/12/22 12:32:21]
       Communication error with deployment clip_t at address(es) {'0.0.0.0:59354'}. Head or worker(s) may be down.                                              
ERROR  gateway/rep-0/GatewayRuntime@5462 Error while getting responses from deployments: failed to connect to all addresses |Gateway:        [11/12/22 12:32:23]
       Communication error with deployment clip_t at address(es) {'0.0.0.0:59354'}. Head or worker(s) may be down.                                              
ERROR  gateway/rep-0/GatewayRuntime@5462 Error while getting responses from deployments: failed to connect to all addresses |Gateway:        [11/12/22 12:35:01]
       Communication error with deployment clip_t at address(es) {'0.0.0.0:59354'}. Head or worker(s) may be down. 
```",bash python flow ready serve link protocol house local locked private public none error error getting connect communication error deployment address e head worker may error error getting connect communication error deployment address e head worker may error error getting connect communication error deployment address e head worker may error error getting connect communication error deployment address e head worker may error error getting connect communication error deployment address e head worker may error error getting connect communication error deployment address e head worker may,issue,positive,positive,neutral,neutral,positive,positive
1312394523,"@numb3r3 @ZiniuYu facing the same issue. 
Machine: `Mac M1 Pro`

Command
```python
python -m clip_server
```",facing issue machine mac pro command python python,issue,negative,neutral,neutral,neutral,neutral,neutral
1311240054,"good catch, this is one tech trick we use to utilize multi-threads for data preprocessing. But I don't think this `map_batch` is the key reason. Most of the performance would come from the advances of `Jina` and `docarray`. Especially, `docarray` is a data structure with good performance in transit.  ",good catch one tech trick use utilize data think key reason performance would come jina especially data structure good performance transit,issue,positive,positive,positive,positive,positive,positive
1311236788,"Yeah，i want to learn the key reason of how to be so fast.
Is the map_batch make you fast?
https://github.com/jina-ai/clip-as-service/blob/710862279bdef342983bd7944f413d8ee54f9603/server/clip_server/executors/clip_torch.py#L147-L163
![image](https://user-images.githubusercontent.com/26429138/201251095-6af55e61-c16a-45db-a21b-4801eb4bed5c.png)
",want learn key reason fast make fast image,issue,negative,positive,positive,positive,positive,positive
1311212598,"Hi @wqh17101 , 

CLIP-as-service is a modern, easy-to-use, fast, and elastic inference service for CLIP models. 

It has a minimalist design on both the client and server sides, and provides an intuitive and consistent API for image and sentence embedding. It also supports async clients and can easily switch between gRPC, HTTP, and WebSocket protocols with TLS and compression. The APIs are carefully designed to handle large inputs into smaller batches with lower memory footprint. You can scale the service easily by setting replicas and sharding in one line. 

It also integrates smoothly with the [Jina](https://jina.ai) ecosystem, allowing you to build cross-modal and multi-modal solutions quickly and easily. Jina handles the infrastructure complexity, making advanced solution engineering and cloud-native technologies accessible to every developer. You can also access free CPU/GPU hosting via [Jina Cloud](https://cloud.jina.ai).

You can easily build many interesting applications based on CLIP-as-service's 4 main functionalities: encoding, ranking, indexing and searching. You can take a look at some examples provided by us: [Text & Image Embedding](https://clip-as-service.jina.ai/playground/embedding/), [Visual Reasoning](https://clip-as-service.jina.ai/playground/reasoning/) and [Text & Image Searching](https://clip-as-service.jina.ai/playground/searching/). Learn more about CLIP-as-service from our [docs](https://clip-as-service.jina.ai/).

We also have an always online clip service for you to encode and rank contents. You can focus on your core logic and leave the infrastructure to us. Try it out [here](https://console.clip.jina.ai/get_started) 🚀",hi modern fast elastic inference service clip design client server side intuitive consistent image sentence also easily switch compression carefully designed handle large smaller lower memory footprint scale service easily setting one line also smoothly jina ecosystem build quickly easily jina infrastructure complexity making advanced solution engineering accessible every developer also access free hosting via jina cloud easily build many interesting based main indexing searching take look provided u text image visual reasoning text image searching learn also always clip service encode rank content focus core logic leave infrastructure u try rocket,issue,positive,positive,positive,positive,positive,positive
1309847754,"I will first close this PR, and reopen it when we have better idea to support mps device.",first close reopen better idea support device,issue,positive,positive,positive,positive,positive,positive
1308392394,"> @OrangeSodahub Please also present the performance under full-precision inference in the `Openclip-flashattn` repo

Flash attention does not support fp32 see: https://github.com/HazyResearch/flash-attention#alpha-release-01",please also present performance inference flash attention support see,issue,positive,neutral,neutral,neutral,neutral,neutral
1308375047,@OrangeSodahub Please also present the performance under full-precision inference in the `Openclip-flashattn` repo,please also present performance inference,issue,negative,neutral,neutral,neutral,neutral,neutral
1308269743,"> And what's more, please confirm that the flash_attention can load the pre-trained parameters, and result the consistency output with `nn.MultiHeadAttention`

I've confirmed the results' correctness in this [repo](https://github.com/OrangeSodahub/Openclip-flashattn). There exists max errors up to 1E-3 (<=0.003) level.",please confirm load result consistency output confirmed correctness level,issue,negative,positive,positive,positive,positive,positive
1308261673,"And what's more, please confirm that the flash_attention can load the pre-trained parameters, and result the consistency output with `nn.MultiHeadAttention`",please confirm load result consistency output,issue,negative,neutral,neutral,neutral,neutral,neutral
1308121716,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/851?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#851](https://codecov.io/gh/jina-ai/clip-as-service/pull/851?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (ab2ffa8) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/4fcbf68a883cb3143e47738df4c8044dfec2a131?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (4fcbf68) will **decrease** coverage by `10.77%`.
> The diff coverage is `n/a`.

> :exclamation: Current head ab2ffa8 differs from pull request most recent head ff308f0. Consider uploading reports for the commit ff308f0 to get more accurate results

```diff
@@             Coverage Diff             @@
##             main     #851       +/-   ##
===========================================
- Coverage   81.39%   70.61%   -10.78%     
===========================================
  Files          21       21               
  Lines        1596     1596               
===========================================
- Hits         1299     1127      -172     
- Misses        297      469      +172     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `70.61% <ø> (-10.78%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/851?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/executors/clip\_tensorrt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/851/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RlbnNvcnJ0LnB5) | `0.00% <0.00%> (-93.11%)` | :arrow_down: |
| [server/clip\_server/model/clip\_trt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/851/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfdHJ0LnB5) | `0.00% <0.00%> (-85.72%)` | :arrow_down: |
| [server/clip\_server/model/trt\_utils.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/851/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3RydF91dGlscy5weQ==) | `0.00% <0.00%> (-83.52%)` | :arrow_down: |

Help us with your feedback. Take ten seconds to tell us [how you rate us](https://about.codecov.io/nps?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Have a feature suggestion? [Share it here.](https://app.codecov.io/gh/feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
",report main decrease coverage coverage exclamation current head pull request recent head consider commit get accurate coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage help u feedback take ten tell u rate u feature suggestion share,issue,positive,positive,positive,positive,positive,positive
1299529414,"@learningpro Could you provide more details on this problem? Like the YAML file you use, steps to reproduce, etc. Thanks!",could provide problem like file use reproduce thanks,issue,negative,positive,positive,positive,positive,positive
1298464007,"> Hi： i creat a folder named ""tmp"" and follow constructions to install and run,but got run: ""tensorflow.python.framework.errors_impl.NotFoundError: /chinese_L-12_H-768_A-12/bert_config.json; No such file or directory"" i think my folder routine is correct,so i dont know whats wrong

请问你解决了吗",creat folder follow install run got run file directory think folder routine correct dont know whats wrong,issue,negative,negative,negative,negative,negative,negative
1298416742,"#自动回复邮件#automatic response#Auto-réponse
 
您好，我已收到您的邮件，我将在看到后尽快给您回复，谢谢！祝您生活愉快！Hello, I have received your e-mail. I will reply as soon as possible, thank you! Best wishes!",automatic response received reply soon possible thank best,issue,positive,positive,positive,positive,positive,positive
1296447403,@learningpro How do you start the server? via local CLI `python -m clip_server` or k8s?,start server via local python,issue,negative,neutral,neutral,neutral,neutral,neutral
1295260232,"Because my Docker cluster cannot be connected to the Internet, I downloaded ViT-B-32.pt from the local image and then uploaded it to the Docker cluster. However, the cluster container cannot find the option to continue downloading ViT, but the relative location in the cluster contains ViT


My problem is solved. The main reason is that the program cannot obtain the model from the root path with the model because of the change of the environment variable ""root""",docker cluster connected local image docker cluster however cluster container find option continue relative location cluster problem main reason program obtain model root path model change environment variable root,issue,negative,positive,neutral,neutral,positive,positive
1294431476,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/847?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#847](https://codecov.io/gh/jina-ai/clip-as-service/pull/847?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (da0d377) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/baf94b5f70b9c18cfe2c0fea3e284fe30e4ca093?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (baf94b5) will **not change** coverage.
> The diff coverage is `n/a`.

```diff
@@           Coverage Diff           @@
##             main     #847   +/-   ##
=======================================
  Coverage   81.58%   81.58%           
=======================================
  Files          21       21           
  Lines        1575     1575           
=======================================
  Hits         1285     1285           
  Misses        290      290           
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `81.58% <ø> (ø)` | |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.


Help us with your feedback. Take ten seconds to tell us [how you rate us](https://about.codecov.io/nps?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Have a feature suggestion? [Share it here.](https://app.codecov.io/gh/feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
",report dad main change coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find help u feedback take ten tell u rate u feature suggestion share,issue,positive,positive,positive,positive,positive,positive
1294364157,"> I have left a lot of comments on the client.md file. Please make a new ticket https://github.com/orgs/jina-ai/projects/34/views/1 to read and revise the entire page instead of just the changes. It need copyediting badly.

@scott-martens thanks for the suggestions!! I saw that a draft ticket has already been added, so I won't add a duplicated one",left lot file please make new ticket read revise entire page instead need badly thanks saw draft ticket already added wo add one,issue,negative,positive,neutral,neutral,positive,positive
1293510148,I have left a lot of comments on the client.md file. Please make a new ticket https://github.com/orgs/jina-ai/projects/34/views/1 to read and revise the entire page instead of just the changes. It need copyediting badly.,left lot file please make new ticket read revise entire page instead need badly,issue,negative,negative,negative,negative,negative,negative
1287822781,"
When I convert clip_server.executors.clip_torch to executors\clip_torch.py, I am prompted ""ImportError: DLL load failed while importing _imaging: the specified module cannot be found."" So pip uninstall pillow and pip install pillow ,   'Downloading https://pypi.tuna.tsinghua.edu.cn/packages/19/3f/b4d4bcf05dbcbe07f2e9613a8f4180c297395e73a91d8ad22c32c6624f8c/Pillow-9.2.0-cp39-cp39-win_amd64.whl (3.3 MB)‘

The program is ready to run",convert load module found pip pillow pip install pillow program ready run,issue,negative,positive,positive,positive,positive,positive
1283956413,"A few days later, I still haven't solved this problem. The following is all the error information

[ 20:37:15 ] D:\..\..\Scripts ❯ python -m clip_server
⠇ Waiting clip_t gateway... ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/3 0:00:03ERROR  clip_t/rep-0@35540 fail to load file dependency                                               [10/19/22 20:37:28]
ERROR  clip_t/rep-0@35540 FileNotFoundError('can not find clip_server.executors.clip_torch') during  [10/19/22 20:37:28]
       <class 'jina.serve.runtimes.worker.WorkerRuntime'> initialization
        add ""--quiet-error"" to suppress the exception details
       Traceback (most recent call last):
         File ""D:\IDE\Anaconda3\lib\site-packages\jina\orchestrate\pods\__init__.py"", line 74, in
       run
           runtime = runtime_cls(
         File ""D:\IDE\Anaconda3\lib\site-packages\jina\serve\runtimes\worker\__init__.py"", line 33,
       in __init__
           super().__init__(args, **kwargs)
         File ""D:\IDE\Anaconda3\lib\site-packages\jina\serve\runtimes\asyncio.py"", line 70, in
       __init__
           self._loop.run_until_complete(self.async_setup())
         File ""D:\IDE\Anaconda3\lib\asyncio\base_events.py"", line 647, in run_until_complete
           return future.result()
         File ""D:\IDE\Anaconda3\lib\site-packages\jina\serve\runtimes\worker\__init__.py"", line 81,
       in async_setup
           self._data_request_handler = DataRequestHandler(
         File
       ""D:\IDE\Anaconda3\lib\site-packages\jina\serve\runtimes\request_handlers\data_request_handle…
       line 44, in __init__
           self._load_executor(metrics_registry)
         File
       ""D:\IDE\Anaconda3\lib\site-packages\jina\serve\runtimes\request_handlers\data_request_handle…
       line 94, in _load_executor
           self._executor: BaseExecutor = BaseExecutor.load_config(
         File ""D:\IDE\Anaconda3\lib\site-packages\jina\jaml\__init__.py"", line 730, in load_config
           load_py_modules(
         File ""D:\IDE\Anaconda3\lib\site-packages\jina\jaml\helper.py"", line 285, in load_py_modules
           PathImporter.add_modules(*mod)
         File ""D:\IDE\Anaconda3\lib\site-packages\jina\importer.py"", line 159, in add_modules
           _path_import(complete_path(m))
         File ""D:\IDE\Anaconda3\lib\site-packages\jina\jaml\helper.py"", line 229, in complete_path
           raise FileNotFoundError(f'can not find {path}')
       FileNotFoundError: can not find clip_server.executors.clip_torch
ERROR  Flow@41968 Flow is aborted due to ['clip_t'] can not be started.                              [10/19/22 20:37:30]
WARNI… gateway/rep-0@41968 Pod was forced to close after 1 second. Graceful closing is not available [10/19/22 20:37:31]
       on Windows.
Traceback (most recent call last):
  File ""D:\IDE\Anaconda3\lib\runpy.py"", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""D:\IDE\Anaconda3\lib\runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""D:\IDE\Anaconda3\lib\site-packages\clip_server\__main__.py"", line 25, in <module>
    with f:
  File ""D:\IDE\Anaconda3\lib\site-packages\jina\orchestrate\flow\base.py"", line 1474, in __enter__
    return self.start()
  File ""D:\IDE\Anaconda3\lib\site-packages\jina\orchestrate\flow\builder.py"", line 33, in arg_wrapper
    return func(self, *args, **kwargs)
  File ""D:\IDE\Anaconda3\lib\site-packages\jina\orchestrate\flow\base.py"", line 1538, in start
    self._wait_until_all_ready()
  File ""D:\IDE\Anaconda3\lib\site-packages\jina\orchestrate\flow\base.py"", line 1637, in _wait_until_all_ready
    raise RuntimeFailToStart
jina.excepts.RuntimeFailToStart


When using the latest clip-server, because my sys.path is disk D, there will be a ""DLL load failed while importing win32api:"" error. The solution is as follows: https://blog.csdn.net/ljr_123/article/details/104693372",day later still problem following error information python waiting gateway fail load file dependency error find class add suppress exception recent call last file line run file line super file line file line return file line file line file line file line file line file line file line raise find path find error flow flow aborted due pod forced close second graceful available recent call last file line return code none file line code file line module file line return file line return self file line start file line raise latest disk load error solution,issue,negative,positive,neutral,neutral,positive,positive
1277021086,"@alaeddine-13 The granted user is not necessary a jina registered user. For example, I created a jina account and create a TOKEN. Then I can share this TOKEN with anybody (e.g., my teammate). Generall, IMO, the authentication token works for the consumer. `jina auth login` works for the producer. So, at this moment, I would not merge them to make the logic confusing. ",user necessary jina registered user example jina account create token share token anybody teammate generall authentication token work consumer jina login work producer moment would merge make logic,issue,positive,neutral,neutral,neutral,neutral,neutral
1275868476,"The output singularities of the parallel run of ""complete_path"" are

”torch-flow.yml ['D:\\Program_Data\\Anaconda3\\lib\\site-packages\\clip_server'] True
{""jtype"": ""CLIPEncoder"", ""metas"": {""py_modules"": [""clip_server.executors.clip_torch""]}} None True 
clip_server.executors.clip_torch None True“。

Is there a problem with my path?",output parallel run true none true none true problem path,issue,positive,positive,positive,positive,positive,positive
1275758346,Done. See first comment.,done see first comment,issue,negative,positive,positive,positive,positive,positive
1274037373,"@161424 Thanks for your information, that helps us a lot. We will look into it. ",thanks information u lot look,issue,negative,positive,positive,positive,positive,positive
1273535197,"At first, it was jina/jmal/helper.py 227 line parameters of _ p.  When Parallel operation run _p, the first two paths are normal as absolute path of the torch-flow. yml, and the last ones are None.  This is the initial error. Take me _ P is specified as torch-flow.yml path, the input parameter spec of 126 lines of modules in the Jina/Importer is null, and an error is reported. Spec_ name and absolute_paths point to the absolute paths of torch-flow. yml and absolute path of the torch-flow. yml , but the output of the spec is None. It's beyond my ability to get here",first line parallel operation run first two normal absolute path last none initial error take path input parameter spec null error name point absolute absolute path output spec none beyond ability get,issue,negative,positive,positive,positive,positive,positive
1273185935,"jina -vf

- jina 3.9.1
- docarray 0.16.5
- jcloud 0.0.35
- jina-hubble-sdk 0.16.2
- jina-proto 0.1.13
- protobuf 3.20.0
- proto-backend cpp
- grpcio 1.47.0
- pyyaml 6.0
- python 3.9.13
- platform Windows
- platform-release 10
- platform-version 10.0.22621
- architecture AMD64
- processor AMD64 Family 25 Model 80 Stepping 0, AuthenticAMD
- uid 224391396904208
- session-id 0c3097e5-4890-11ed-b66e-cc153180cd10
- uptime 2022-10-10T19:38:21.850875
- ci-vendor (unset)
- internal False
* JINA_DEFAULT_HOST (unset)
* JINA_DEFAULT_TIMEOUT_CTRL (unset)
* JINA_DEPLOYMENT_NAME (unset)
* JINA_DISABLE_UVLOOP (unset)
* JINA_EARLY_STOP (unset)
* JINA_FULL_CLI (unset)
* JINA_GATEWAY_IMAGE (unset)
* JINA_GRPC_RECV_BYTES (unset)
* JINA_GRPC_SEND_BYTES (unset)
* JINA_HUB_NO_IMAGE_REBUILD (unset)
* JINA_LOG_CONFIG (unset)
* JINA_LOG_LEVEL (unset)
* JINA_LOG_NO_COLOR (unset)
* JINA_MP_START_METHOD (unset)
* JINA_OPTOUT_TELEMETRY (unset)
* JINA_RANDOM_PORT_MAX (unset)
* JINA_RANDOM_PORT_MIN (unset)",jina jina python platform architecture processor family model stepping ce unset internal false unset unset unset unset unset unset unset unset unset unset unset unset unset unset unset unset unset,issue,negative,negative,negative,negative,negative,negative
1273182778,@161424 Could you provide more context for your problem? Are you working on windows? What's the output of `jina -vf`?,could provide context problem working output jina,issue,negative,neutral,neutral,neutral,neutral,neutral
1273141420,"I have a similar problem

 python -m clip_server
⠇ Waiting clip_t gateway... ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/3 0:00:02ERROR  clip_t/rep-0@12624 fail to load file dependency                                               [10/10/22 16:31:37]
⠙ Waiting clip_t gateway... ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/3 0:00:02ERROR  clip_t/rep-1@25224 fail to load file dependency                                               [10/10/22 16:31:38]
ERROR  clip_t/rep-0@12624 FileNotFoundError('can not find clip_server.executors.clip_torch') during  [10/10/22 16:31:37]       <class 'jina.serve.runtimes.worker.WorkerRuntime'> initialization
        add ""--quiet-error"" to suppress the exception details

pip show clip_server
Name: clip-server
Version: 0.7.0
Summary: Embed images and sentences into fixed-length vectors via CLIP
Home-page: https://github.com/jina-ai/clip-as-service
Author: Jina AI
Author-email: hello@jina.ai
License: Apache 2.0
Location: d:\program_data\anaconda3\lib\site-packages
Requires: ftfy, jina, open-clip-torch, prometheus-client, regex, torch, torchvision
Required-by:

I think it should be a software compatibility problem, but I don't know which one. Because it can work normally before",similar problem python waiting gateway fail load file dependency waiting gateway fail load file dependency error find class add suppress exception pip show name version summary embed via clip author jina ai hello license apache location jina torch think compatibility problem know one work normally,issue,negative,negative,negative,negative,negative,negative
1272860638,"Thanks for your contribution :heart:
:broken_heart: Unfortunately, this PR has one ore more **bad commit messages**, it can not be merged. To fix this problem, please refer to:
- [Commit Message Guideline for the First Time Contributor](https://github.com/jina-ai/jina/issues/553)
- [Contributing Guideline](https://github.com/jina-ai/jina/blob/master/CONTRIBUTING.md)

Note, other CI tests will *not* *start* until the commit messages get fixed.

This message will be deleted automatically when the commit messages get fixed.",thanks contribution heart unfortunately one ore bad commit fix problem please refer commit message guideline first time contributor guideline note start commit get fixed message automatically commit get fixed,issue,positive,negative,neutral,neutral,negative,negative
1272859859,"> Support ViT-L/14, ViT-H/14 and ViT-g/14 trained on laion-2b

we also have the new ViT-B-32::laion2B-s34B-b79K model ",support trained also new model,issue,negative,positive,positive,positive,positive,positive
1272805586,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/839?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#839](https://codecov.io/gh/jina-ai/clip-as-service/pull/839?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (4bc1f4d) into [clip-benchmark](https://codecov.io/gh/jina-ai/clip-as-service/commit/cc0e98c1bb132d88694d6aaee36bf4b225b373c7?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (cc0e98c) will **not change** coverage.
> The diff coverage is `n/a`.

```diff
@@               Coverage Diff               @@
##           clip-benchmark     #839   +/-   ##
===============================================
  Coverage           81.58%   81.58%           
===============================================
  Files                  21       21           
  Lines                1575     1575           
===============================================
  Hits                 1285     1285           
  Misses                290      290           
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `81.58% <ø> (ø)` | |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.


Help us with your feedback. Take ten seconds to tell us [how you rate us](https://about.codecov.io/nps?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Have a feature suggestion? [Share it here.](https://app.codecov.io/gh/feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
",report change coverage coverage coverage coverage flag coverage carried forward coverage wo shown click find help u feedback take ten tell u rate u feature suggestion share,issue,positive,neutral,neutral,neutral,neutral,neutral
1272800866,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/838?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#838](https://codecov.io/gh/jina-ai/clip-as-service/pull/838?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (bcc501b) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/2ba8a4fe71f26faa5e92d62df04edb616389f6bd?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (2ba8a4f) will **decrease** coverage by `2.79%`.
> The diff coverage is `n/a`.

> :exclamation: Current head bcc501b differs from pull request most recent head c6928a4. Consider uploading reports for the commit c6928a4 to get more accurate results

```diff
@@            Coverage Diff             @@
##             main     #838      +/-   ##
==========================================
- Coverage   84.38%   81.58%   -2.80%     
==========================================
  Files          21       21              
  Lines        1575     1575              
==========================================
- Hits         1329     1285      -44     
- Misses        246      290      +44     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `81.58% <ø> (-2.80%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/838?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/model/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/838/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfb25ueC5weQ==) | `72.72% <ø> (-5.46%)` | :arrow_down: |
| [server/clip\_server/model/pretrained\_models.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/838/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3ByZXRyYWluZWRfbW9kZWxzLnB5) | `98.41% <ø> (ø)` | |
| [server/clip\_server/model/model.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/838/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL21vZGVsLnB5) | `69.85% <0.00%> (-9.12%)` | :arrow_down: |
| [server/clip\_server/executors/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/838/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX29ubngucHk=) | `81.94% <0.00%> (-2.78%)` | :arrow_down: |

Help us with your feedback. Take ten seconds to tell us [how you rate us](https://about.codecov.io/nps?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Have a feature suggestion? [Share it here.](https://app.codecov.io/gh/feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
",report main decrease coverage coverage exclamation current head pull request recent head ca consider commit ca get accurate coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage help u feedback take ten tell u rate u feature suggestion share,issue,positive,positive,positive,positive,positive,positive
1263040920,"@ZiniuYu  And what's more, I would suggest normalizing the model name underneath. So that we do not need to update the s3 link if we have the same trouble in future. ",would suggest model name underneath need update link trouble future,issue,negative,negative,neutral,neutral,negative,negative
1262469916,"Hi @big-thousand 

Notice that if you do not specify [`timeout_ready`](https://docs.jina.ai/fundamentals/gateway/yaml-spec/?highlight=timeout_ready) in the YAML file, it will adapt the default value of 10 min.
Several possible reasons may cause you to get timeout errors:

- Internet connection: Some large models take longer to download, or you may have a poor connection to those model resources. You can re-deploy the server after the download is finished.
- Computation resource: Larger models take longer to load, especially on less powerful machines. It is recommended to use a GPU machine to host the server, for a shorter loading speed and stronger encoding power.

Anyway, you can increase the timeout limit by setting the [`timeout_ready`](https://docs.jina.ai/fundamentals/gateway/yaml-spec/?highlight=timeout_ready) in the YAML file.
Or you could try to use a smaller [model](https://clip-as-service.jina.ai/user-guides/server/#model-support).
For example, to have a longer timeout at 30 min, you can use:
```
jtype: Flow
version: '1'
with:
  port: 51000
executors:
  - name: clip_t
    uses:
      jtype: CLIPEncoder
      timeout_ready: 1800000
      with:
        name: M-CLIP/XLM-Roberta-Large-Vit-L-14
      metas:
        py_modules:
          - clip_server.executors.clip_torch
```

",hi notice specify file adapt default value min several possible may cause get connection large take longer may poor connection model server finished computation resource take longer load especially le powerful use machine host server shorter loading speed power anyway increase limit setting file could try use smaller model example longer min use flow version port name name,issue,positive,positive,neutral,neutral,positive,positive
1258879331,"@NOT-HAL9000 I'm glad to say now, ONNX M-CLIP models have been enabled in the latest version. Please play it around, and leave your comments. ",glad say latest version please play around leave,issue,positive,positive,positive,positive,positive,positive
1258357692,"hello,

greetings to all... i tried to activate BERT and this is what i've got : 

(base) C:\Users\r>conda activate bert_server_test

(bert_server_test) C:\Users\r>bert-serving-start -model_dir E:\uncased_L-12_H-768_A-12\uncased_L-12_H-768_A-12\ -num_worker=1 -cpu
2022-09-27 00:08:50.944272: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2022-09-27 00:08:50.944453: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
C:\Users\r\.conda\envs\bert_server_test\lib\site-packages\bert_serving\server\helper.py:176: UserWarning: Tensorflow 2.4.0 is not tested! It may or may not work. Feel free to submit an issue at https://github.com/hanxiao/bert-as-service/issues/
  'Feel free to submit an issue at https://github.com/hanxiao/bert-as-service/issues/' % tf.__version__)
usage: C:\Users\r\.conda\envs\bert_server_test\Scripts\bert-serving-start -model_dir E:\uncased_L-12_H-768_A-12\uncased_L-12_H-768_A-12\ -num_worker=1 -cpu
                 ARG   VALUE
__________________________________________________
           ckpt_name = bert_model.ckpt
         config_name = bert_config.json
                cors = *
                 cpu = True
          device_map = []
       do_lower_case = True
  fixed_embed_length = False
                fp16 = False
 gpu_memory_fraction = 0.5
       graph_tmp_dir = None
    http_max_connect = 10
           http_port = None
        mask_cls_sep = False
      max_batch_size = 256
         max_seq_len = 25
           model_dir = E:\uncased_L-12_H-768_A-12\uncased_L-12_H-768_A-12\
no_position_embeddings = False
    no_special_token = False
          num_worker = 1
       pooling_layer = [-2]
    pooling_strategy = REDUCE_MEAN
                port = 5555
            port_out = 5556
       prefetch_size = 10
 priority_batch_size = 16
show_tokens_to_client = False
     tuned_model_dir = None
             verbose = False
                 xla = False

I:←[35mVENTILATOR←[0m:freeze, optimize and export graph, could take a while...
2022-09-27 00:08:53.953735: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2022-09-27 00:08:53.953890: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
C:\Users\r\.conda\envs\bert_server_test\lib\site-packages\bert_serving\server\helper.py:176: UserWarning: Tensorflow 2.4.0 is not tested! It may or may not work. Feel free to submit an issue at https://github.com/hanxiao/bert-as-service/issues/
  'Feel free to submit an issue at https://github.com/hanxiao/bert-as-service/issues/' % tf.__version__)
E:←[36mGRAPHOPT←[0m:fail to optimize the graph!
Traceback (most recent call last):
  File ""C:\Users\r\.conda\envs\bert_server_test\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Users\r\.conda\envs\bert_server_test\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\r\.conda\envs\bert_server_test\Scripts\bert-serving-start.exe\__main__.py"", line 7, in <module>
  File ""C:\Users\r\.conda\envs\bert_server_test\lib\site-packages\bert_serving\server\cli\__init__.py"", line 4, in main
    with BertServer(get_run_args()) as server:
  File ""C:\Users\r\.conda\envs\bert_server_test\lib\site-packages\bert_serving\server\__init__.py"", line 71, in __init__
    self.graph_path, self.bert_config = pool.apply(optimize_graph, (self.args,))
TypeError: 'NoneType' object is not iterable

anyone knows how to solve this issue? thank you in advance..
",hello tried activate got base activate could load dynamic library found ignore set machine tested may may work feel free submit issue free submit issue usage value true true false false none none false false false port false none verbose false false freeze optimize export graph could take could load dynamic library found ignore set machine tested may may work feel free submit issue free submit issue fail optimize graph recent call last file line file line code file line module file line main server file line object iterable anyone solve issue thank advance,issue,positive,negative,neutral,neutral,negative,negative
1258263369,I want to hold this PR. Because I cannot see the big benefits of this PR. ,want hold see big,issue,negative,neutral,neutral,neutral,neutral,neutral
1255877445,@ZiniuYu Please polish the PR's title. `4 new clip models` is meaningless. Please highlights the contribution in the title,please polish title new clip meaningless please contribution title,issue,negative,negative,negative,negative,negative,negative
1255006806,"I just want to get word vectors now, but it may be that I have selected too many features, which leads to the saved file being a little large. Let me try to reduce it from other aspects first, and then try to modify the dimension",want get word may selected many saved file little large let try reduce first try modify dimension,issue,negative,positive,positive,positive,positive,positive
1254986621,"In general, you can simply use an unsupervised method, PCA to reduce the dimension. But PCA would degrade the accuracy a bit. BTW, what's your use case to have lower-dimension embeddings?",general simply use unsupervised method reduce dimension would degrade accuracy bit use case,issue,negative,positive,neutral,neutral,positive,positive
1253175763,You rock!  I appreciate the effort. Maybe we need to put your tricks into our docs. That will help the community to fix the same issue quickly. ,rock appreciate effort maybe need put help community fix issue quickly,issue,positive,positive,positive,positive,positive,positive
1252382939,"no, so basically to make it works one need to:

- install onnxruntime
- install onnxruntime-gpu
- uninstall onnxruntime

and then it works ... ",basically make work one need install install work,issue,negative,neutral,neutral,neutral,neutral,neutral
1250615741,"It's hard to offer the exact timeline for supporting ONNX M-CLIP models. However, It's hopefully we can release it by end of this month. ",hard offer exact supporting however hopefully release end month,issue,positive,positive,neutral,neutral,positive,positive
1250613152,"@ZiniuYu to guarantee the safe conflicts resolve, let's use this PR instead.  https://github.com/jina-ai/clip-as-service/pull/824",guarantee safe resolve let use instead,issue,positive,positive,positive,positive,positive,positive
1248279499,"Hi @161424 , it looks like your PyTorch is not successfully installed. Which platform are you using? Maybe you can try to uninstall and reinstall PyTorch or install `clip_server` in a new Python environment. Please refer [here](https://clip-as-service.jina.ai/#install) for installing instructions. 

If you only want to get embeddings for images or texts, you can try out our new service at [console.clip.jina.ai](https://console.clip.jina.ai/). It's free and simple to use 😇",hi like successfully platform maybe try reinstall install new python environment please refer want get try new service free simple use,issue,positive,positive,positive,positive,positive,positive
1248000441,"> For ONNX it is possible, and it will be supported in future releases

Thanks! Is there a roadmap with a rough ETA for the ONYX runtime for M-CLIP models?",possible future thanks rough eta onyx,issue,negative,positive,neutral,neutral,positive,positive
1247874344,"Hi, thank you for asking! 

For ONNX it is possible, and it will be supported in future releases. Unfortunately, we have not yet planned to support TensorRT for M-CLIP models very soon. You may refer to other online resources on converting PyTorch models to TensorRT.",hi thank possible future unfortunately yet support soon may refer converting,issue,negative,negative,negative,negative,negative,negative
1244920532,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/819?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#819](https://codecov.io/gh/jina-ai/clip-as-service/pull/819?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (f859b9b) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/fa7e577606d68e65a0e7952048c64d2b3a28e231?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (fa7e577) will **decrease** coverage by `10.91%`.
> The diff coverage is `n/a`.

> :exclamation: Current head f859b9b differs from pull request most recent head 9b14f49. Consider uploading reports for the commit 9b14f49 to get more accurate results

```diff
@@             Coverage Diff             @@
##             main     #819       +/-   ##
===========================================
- Coverage   84.30%   73.38%   -10.92%     
===========================================
  Files          21       21               
  Lines        1548     1548               
===========================================
- Hits         1305     1136      -169     
- Misses        243      412      +169     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `73.38% <ø> (-10.92%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/819?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/executors/clip\_tensorrt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/819/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RlbnNvcnJ0LnB5) | `0.00% <0.00%> (-92.73%)` | :arrow_down: |
| [server/clip\_server/model/clip\_trt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/819/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfdHJ0LnB5) | `0.00% <0.00%> (-85.72%)` | :arrow_down: |
| [server/clip\_server/model/trt\_utils.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/819/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3RydF91dGlscy5weQ==) | `0.00% <0.00%> (-83.52%)` | :arrow_down: |

Help us with your feedback. Take ten seconds to tell us [how you rate us](https://about.codecov.io/nps?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Have a feature suggestion? [Share it here.](https://app.codecov.io/gh/feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
",report main fae decrease coverage coverage exclamation current head pull request recent head consider commit get accurate coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage help u feedback take ten tell u rate u feature suggestion share,issue,positive,positive,positive,positive,positive,positive
1241665627,"I'm not sure. Should we put the search in Client doc since it is part of the Client?
We should at least mention the search function in Client intros and link to the search docs",sure put search client doc since part client least mention search function client link search,issue,negative,positive,neutral,neutral,positive,positive
1241559999,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/818?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#818](https://codecov.io/gh/jina-ai/clip-as-service/pull/818?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (626c887) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/4461d2e9ab07c01669237b220cd24cd6f95e30e8?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (4461d2e) will **not change** coverage.
> The diff coverage is `66.66%`.

```diff
@@           Coverage Diff           @@
##             main     #818   +/-   ##
=======================================
  Coverage   83.90%   83.90%           
=======================================
  Files          21       21           
  Lines        1466     1466           
=======================================
  Hits         1230     1230           
  Misses        236      236           
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `83.90% <66.66%> (ø)` | |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/818?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/model/clip\_model.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/818/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfbW9kZWwucHk=) | `87.50% <0.00%> (ø)` | |
| [server/clip\_server/model/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/818/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfb25ueC5weQ==) | `82.22% <ø> (ø)` | |
| [server/clip\_server/model/clip\_trt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/818/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfdHJ0LnB5) | `85.71% <ø> (ø)` | |
| [client/clip\_client/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/818/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/818/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |

Help us with your feedback. Take ten seconds to tell us [how you rate us](https://about.codecov.io/nps?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Have a feature suggestion? [Share it here.](https://app.codecov.io/gh/feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
",report main de change coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage help u feedback take ten tell u rate u feature suggestion share,issue,positive,positive,positive,positive,positive,positive
1241519816,@jemmyshin This PR has been merged to https://github.com/jina-ai/clip-as-service/pull/816. I will close this PR first. let's work on the new PR. ,close first let work new,issue,negative,positive,positive,positive,positive,positive
1240897175,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/817?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#817](https://codecov.io/gh/jina-ai/clip-as-service/pull/817?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (1b0d096) into [inplace-order](https://codecov.io/gh/jina-ai/clip-as-service/commit/5e030e4a2b507f4e5978522fdac82f46cec8bb6e?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (5e030e4) will **decrease** coverage by `0.00%`.
> The diff coverage is `100.00%`.

```diff
@@                Coverage Diff                @@
##           inplace-order     #817      +/-   ##
=================================================
- Coverage          84.67%   84.67%   -0.01%     
=================================================
  Files                 21       21              
  Lines               1573     1566       -7     
=================================================
- Hits                1332     1326       -6     
+ Misses               241      240       -1     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `84.67% <100.00%> (-0.01%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/817?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [client/clip\_client/client.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/817/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L2NsaWVudC5weQ==) | `88.78% <100.00%> (-0.25%)` | :arrow_down: |
| [server/clip\_server/executors/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/817/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX29ubngucHk=) | `84.72% <100.00%> (+0.66%)` | :arrow_up: |
| [server/clip\_server/executors/clip\_tensorrt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/817/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RlbnNvcnJ0LnB5) | `93.10% <100.00%> (+0.37%)` | :arrow_up: |
| [server/clip\_server/executors/clip\_torch.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/817/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RvcmNoLnB5) | `83.33% <100.00%> (+0.79%)` | :arrow_up: |
| [server/clip\_server/executors/helper.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/817/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9oZWxwZXIucHk=) | `97.05% <100.00%> (ø)` | |

Help us with your feedback. Take ten seconds to tell us [how you rate us](https://about.codecov.io/nps?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Have a feature suggestion? [Share it here.](https://app.codecov.io/gh/feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
",report decrease coverage coverage coverage coverage flag coverage carried forward coverage wo shown click find impacted coverage help u feedback take ten tell u rate u feature suggestion share,issue,positive,neutral,neutral,neutral,neutral,neutral
1240278195,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/816?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#816](https://codecov.io/gh/jina-ai/clip-as-service/pull/816?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (0626bd4) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/4461d2e9ab07c01669237b220cd24cd6f95e30e8?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (4461d2e) will **increase** coverage by `0.40%`.
> The diff coverage is `87.77%`.

> :exclamation: Current head 0626bd4 differs from pull request most recent head e1190ae. Consider uploading reports for the commit e1190ae to get more accurate results

```diff
@@            Coverage Diff             @@
##             main     #816      +/-   ##
==========================================
+ Coverage   83.90%   84.30%   +0.40%     
==========================================
  Files          21       21              
  Lines        1466     1548      +82     
==========================================
+ Hits         1230     1305      +75     
- Misses        236      243       +7     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `84.30% <87.77%> (+0.40%)` | :arrow_up: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/816?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/model/clip\_model.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/816/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfbW9kZWwucHk=) | `87.50% <0.00%> (ø)` | |
| [server/clip\_server/model/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/816/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfb25ueC5weQ==) | `88.88% <ø> (+6.66%)` | :arrow_up: |
| [server/clip\_server/model/clip\_trt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/816/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfdHJ0LnB5) | `85.71% <ø> (ø)` | |
| [client/clip\_client/client.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/816/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L2NsaWVudC5weQ==) | `87.45% <88.50%> (+0.12%)` | :arrow_up: |
| [client/clip\_client/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/816/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/816/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/executors/clip\_torch.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/816/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RvcmNoLnB5) | `82.53% <0.00%> (-3.18%)` | :arrow_down: |
| [server/clip\_server/executors/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/816/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX29ubngucHk=) | `84.05% <0.00%> (+2.89%)` | :arrow_up: |

Help us with your feedback. Take ten seconds to tell us [how you rate us](https://about.codecov.io/nps?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Have a feature suggestion? [Share it here.](https://app.codecov.io/gh/feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
",report main de increase coverage coverage exclamation current head pull request recent head consider commit get accurate coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage help u feedback take ten tell u rate u feature suggestion share,issue,positive,positive,positive,positive,positive,positive
1233668711,Let's first hold this PR until we have a clear vision of the UX. ,let first hold clear vision,issue,positive,positive,positive,positive,positive,positive
1232735417,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/814?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#814](https://codecov.io/gh/jina-ai/clip-as-service/pull/814?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (a8b223b) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/4461d2e9ab07c01669237b220cd24cd6f95e30e8?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (4461d2e) will **decrease** coverage by `0.03%`.
> The diff coverage is `66.66%`.

```diff
@@            Coverage Diff             @@
##             main     #814      +/-   ##
==========================================
- Coverage   83.90%   83.86%   -0.04%     
==========================================
  Files          21       21              
  Lines        1466     1469       +3     
==========================================
+ Hits         1230     1232       +2     
- Misses        236      237       +1     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `83.86% <66.66%> (-0.04%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/814?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/model/clip\_model.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/814/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfbW9kZWwucHk=) | `87.50% <0.00%> (ø)` | |
| [server/clip\_server/model/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/814/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfb25ueC5weQ==) | `82.22% <ø> (ø)` | |
| [server/clip\_server/model/clip\_trt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/814/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfdHJ0LnB5) | `85.71% <ø> (ø)` | |
| [client/clip\_client/client.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/814/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L2NsaWVudC5weQ==) | `87.05% <80.00%> (-0.28%)` | :arrow_down: |

Help us with your feedback. Take ten seconds to tell us [how you rate us](https://about.codecov.io/nps?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Have a feature suggestion? [Share it here.](https://app.codecov.io/gh/feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
",report abb main de decrease coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage help u feedback take ten tell u rate u feature suggestion share,issue,positive,positive,positive,positive,positive,positive
1232673011,"@numb3r3 I tried to use `f''` strs but python doesn't allow to put backslashes in it, so I have to `'{}'.format()` instead",tried use python allow put instead,issue,negative,neutral,neutral,neutral,neutral,neutral
1232656697,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/813?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#813](https://codecov.io/gh/jina-ai/clip-as-service/pull/813?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (216b2be) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/ea239685bff56372aeadaeb3050f5c2ccc37175f?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (ea23968) will **increase** coverage by `2.25%`.
> The diff coverage is `100.00%`.

```diff
@@            Coverage Diff             @@
##             main     #813      +/-   ##
==========================================
+ Coverage   81.65%   83.90%   +2.25%     
==========================================
  Files          21       21              
  Lines        1466     1466              
==========================================
+ Hits         1197     1230      +33     
+ Misses        269      236      -33     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `83.90% <100.00%> (+2.25%)` | :arrow_up: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/813?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [client/clip\_client/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/813/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/813/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/model/clip\_trt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/813/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfdHJ0LnB5) | `85.71% <0.00%> (+16.32%)` | :arrow_up: |
| [server/clip\_server/model/trt\_utils.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/813/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3RydF91dGlscy5weQ==) | `83.51% <0.00%> (+27.47%)` | :arrow_up: |

Help us with your feedback. Take ten seconds to tell us [how you rate us](https://about.codecov.io/nps?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Have a feature suggestion? [Share it here.](https://app.codecov.io/gh/feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
",report main ea increase coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage help u feedback take ten tell u rate u feature suggestion share,issue,positive,positive,positive,positive,positive,positive
1232461715,"we will close this issue for now. If you have some findings to share, you are welcome to share with community. Thanks!",close issue share welcome share community thanks,issue,positive,positive,positive,positive,positive,positive
1232461295,"we will close this issue for now. If you have some news findings to share, you are welcome to create a new ticket Thanks!",close issue news share welcome create new ticket thanks,issue,positive,positive,positive,positive,positive,positive
1232388080,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/812?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#812](https://codecov.io/gh/jina-ai/clip-as-service/pull/812?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (78a75a7) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/ea239685bff56372aeadaeb3050f5c2ccc37175f?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (ea23968) will **increase** coverage by `2.25%`.
> The diff coverage is `66.66%`.

```diff
@@            Coverage Diff             @@
##             main     #812      +/-   ##
==========================================
+ Coverage   81.65%   83.90%   +2.25%     
==========================================
  Files          21       21              
  Lines        1466     1466              
==========================================
+ Hits         1197     1230      +33     
+ Misses        269      236      -33     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `83.90% <66.66%> (+2.25%)` | :arrow_up: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/812?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/model/clip\_model.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/812/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfbW9kZWwucHk=) | `87.50% <0.00%> (ø)` | |
| [server/clip\_server/model/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/812/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfb25ueC5weQ==) | `82.22% <ø> (ø)` | |
| [server/clip\_server/model/clip\_trt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/812/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfdHJ0LnB5) | `85.71% <ø> (+16.32%)` | :arrow_up: |
| [client/clip\_client/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/812/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/812/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/model/trt\_utils.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/812/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3RydF91dGlscy5weQ==) | `83.51% <0.00%> (+27.47%)` | :arrow_up: |

Help us with your feedback. Take ten seconds to tell us [how you rate us](https://about.codecov.io/nps?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Have a feature suggestion? [Share it here.](https://app.codecov.io/gh/feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
",report aa main ea increase coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage help u feedback take ten tell u rate u feature suggestion share,issue,positive,positive,positive,positive,positive,positive
1220734385,"Hi, it is a pretty old library and I stopped using it. You should look into HuggingFace BERT, which should cover most use cases.",hi pretty old library stopped look cover use,issue,negative,positive,positive,positive,positive,positive
1220712690,"hlw, I run bertclient() at google colab !!! but it also takes infinite time. can i have the answer please!
",run also infinite time answer please,issue,negative,neutral,neutral,neutral,neutral,neutral
1216203107,"Hi @RaiAmanRai . In general, any image/text (any orders and combinations) encoding-related task can use the CLIP model and can be fine-tuned with a custom dataset. CLIP-as-service supports the use of custom pre-trained models in ONNX runtime. [See the instruction here.](https://clip-as-service.jina.ai/user-guides/finetuner/) You can also take a look at [the basic logic and introduction from OpenAI](https://openai.com/blog/clip/). 

Remember to join our community Slack channel to talk to our engineers and share thoughts with other people ✌️  
🔗 [slack.jina.ai](http://slack.jina.ai/) ",hi general task use clip model custom use custom see instruction also take look basic logic introduction remember join community slack channel talk share people link,issue,positive,positive,neutral,neutral,positive,positive
1210702728,Cheers and thanks for quick replies. Will surely share if I find anything unusual.,thanks quick surely share find anything unusual,issue,positive,positive,positive,positive,positive,positive
1210698318,"Hi @RaiAmanRai , thank you for asking! As you have mentioned, there are some unsupported operation issues and other things such as size limits, hardware compatibilities that need to be addressed before we can fully support those models. We are investigating those blockers and will support them in the future releases if possible. You are more than welcome to share any observations or solutions! Cheers 🍻",hi thank unsupported operation size hardware need fully support investigating support future possible welcome share,issue,positive,positive,positive,positive,positive,positive
1210664191,"Figured it out, thanks for the quick response @ZiniuYu ",figured thanks quick response,issue,negative,positive,positive,positive,positive,positive
1210297856,"Hi @RaiAmanRai , you may take a look [here](https://clip-as-service.jina.ai/user-guides/server/#clip-model-config) and [here](https://docs.jina.ai/fundamentals/flow/yaml-spec/)
For example,
```{yaml}
jtype: Flow
version: '1'
with:
  port: 51000
executors:
  - name: clip_t
    uses:
      jtype: CLIPEncoder
      metas:
        py_modules:
          - clip_server.executors.clip_torch
    uses_with:
      name: 'RN101::openai'
```",hi may take look example flow version port name name,issue,negative,neutral,neutral,neutral,neutral,neutral
1210188306,"below image is from https://hub.jina.ai/executor/gzpbl8jh

<img width=""835"" alt=""Screenshot 2022-08-10 at 07 42 58"" src=""https://user-images.githubusercontent.com/492616/183824400-148b5d05-6a75-4d28-9d40-eeb33634f81a.png"">

here even old name is wrong.",image even old name wrong,issue,negative,negative,negative,negative,negative,negative
1210089747,"Hi @delgermurun , it is a naming convention. We used to only support OpenAI models and they called it `ViT-L/14@336px` (left slash). After we switch to OpenCLIP models, we changed to `ViT-L-14-336::openai`. We kept the old names for backward compatibility. But I do think we need to clarify this or have higher fault tolerance on those names. Thanks for pointing this out!",hi naming convention used support left slash switch kept old backward compatibility think need clarify higher fault tolerance thanks pointing,issue,negative,positive,positive,positive,positive,positive
1209209447,You could try to use better-designed prompts/texts to improve the results. This is in fact a common classification problem. Feel free to test it on the [Playground](https://clip-as-service.jina.ai/playground/reasoning/) 🤩,could try use improve fact common classification problem feel free test playground,issue,positive,positive,neutral,neutral,positive,positive
1209178571,"@ZiniuYu thanks for your quick reply. As you mentioned  `.scores['clip_score_cosine']` , I did come across them but couldn't figure out any way to make sense out of it.

So, when dealing with probablities, if in case there are multiple texts that could relate to the given picture, the overall probablities get distributed over all input texts. Now if I try to apply a threshold say 0.5 to the results, inorder to be a bit more sure about the prediction, the text with the highest probablity couldn't make the cut, because the probablity was distributed over three closely related texts say 0.3, 0.3,0.3 and 0.1 (for the rest of the classes).

And so, I am looking for a way to get around it.",thanks quick reply come across could figure way make sense dealing case multiple could relate given picture overall get distributed input try apply threshold say bit sure prediction text highest could make cut distributed three closely related say rest class looking way get around,issue,negative,positive,positive,positive,positive,positive
1209159878,"Hi @RaiAmanRai , welcome to CLIP-as-service! Softmax scores should be able to do classifications. However, you can still use `.scores['clip_score_cosine']` to get raw scores. Are there any special reasons for getting them?",hi welcome able however still use get raw special getting,issue,positive,positive,positive,positive,positive,positive
1207681424,Thanks @Vision0220  for pointing it out. Fixed in #790. Pleas try it again.  ,thanks vision pointing fixed try,issue,positive,positive,positive,positive,positive,positive
1206163239,"> I would like to evaluate the perf loss of this. Why dont we have the client say which doc Ids of a batch failed to be encoded?

@JoanFM I added this logic in the first several commits, but the error always occurred at preprocessing steps, which means that we can never reach to the end to generate some report like stuff saying which docs are failed. The client failed even before it start to post docs to the sever and encode. We need to refactor the client logic a bit.",would like evaluate loss dont client say doc batch added logic first several error always never reach end generate report like stuff saying client even start post sever encode need client logic bit,issue,negative,positive,positive,positive,positive,positive
1206030186,I tried to add warning messages to the client at the result gathering steps at first but changed to (image) preprocessing steps since in this way the process can proceed normally and no repeated warning will be printed,tried add warning client result gathering first image since way process proceed normally repeated warning printed,issue,negative,positive,positive,positive,positive,positive
1204899510,"> @ZiniuYu BTW, do we need also to update the client side?

@numb3r3  client happened to handle text first so I think no need to change",need also update client side client handle text first think need change,issue,negative,positive,positive,positive,positive,positive
1204892073,"@ZiniuYu BTW, do we need also to update the client side?",need also update client side,issue,negative,neutral,neutral,neutral,neutral,neutral
1204790555,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/795?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#795](https://codecov.io/gh/jina-ai/clip-as-service/pull/795?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (6220a2a) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/7c6708fa8a592b5ce306f1ab2f1af1504148484a?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (7c6708f) will **increase** coverage by `0.04%`.
> The diff coverage is `100.00%`.

```diff
@@            Coverage Diff             @@
##             main     #795      +/-   ##
==========================================
+ Coverage   83.75%   83.79%   +0.04%     
==========================================
  Files          21       21              
  Lines        1440     1438       -2     
==========================================
- Hits         1206     1205       -1     
+ Misses        234      233       -1     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `83.79% <100.00%> (+0.04%)` | :arrow_up: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/795?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/executors/helper.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/795/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9oZWxwZXIucHk=) | `96.96% <100.00%> (+1.38%)` | :arrow_up: |

Help us with your feedback. Take ten seconds to tell us [how you rate us](https://about.codecov.io/nps?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Have a feature suggestion? [Share it here.](https://app.codecov.io/gh/feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
",report aa main increase coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage help u feedback take ten tell u rate u feature suggestion share,issue,positive,positive,positive,positive,positive,positive
1202084171,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/792?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#792](https://codecov.io/gh/jina-ai/clip-as-service/pull/792?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (df46102) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/8681b88eb3a7806c1286eaefff3bd8a8ab28ff03?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (8681b88) will **decrease** coverage by `2.28%`.
> The diff coverage is `100.00%`.

```diff
@@            Coverage Diff             @@
##             main     #792      +/-   ##
==========================================
- Coverage   84.27%   81.98%   -2.29%     
==========================================
  Files          21       21              
  Lines        1418     1421       +3     
==========================================
- Hits         1195     1165      -30     
- Misses        223      256      +33     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `81.98% <100.00%> (-2.29%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/792?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/model/mclip\_model.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/792/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL21jbGlwX21vZGVsLnB5) | `84.21% <100.00%> (+1.35%)` | :arrow_up: |
| [server/clip\_server/model/trt\_utils.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/792/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3RydF91dGlscy5weQ==) | `56.04% <0.00%> (-27.48%)` | :arrow_down: |
| [server/clip\_server/model/clip\_trt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/792/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfdHJ0LnB5) | `69.38% <0.00%> (-16.33%)` | :arrow_down: |

Help us with your feedback. Take ten seconds to tell us [how you rate us](https://about.codecov.io/nps?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report main decrease coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage help u feedback take ten tell u rate u,issue,negative,positive,positive,positive,positive,positive
1200955550,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/791?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#791](https://codecov.io/gh/jina-ai/clip-as-service/pull/791?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (c3c82a4) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/8b8082a939f67f7ea01cc9f55ebce9c5368ebe1a?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (8b8082a) will **decrease** coverage by `2.77%`.
> The diff coverage is `65.51%`.

```diff
@@            Coverage Diff             @@
##             main     #791      +/-   ##
==========================================
- Coverage   84.30%   81.52%   -2.78%     
==========================================
  Files          21       21              
  Lines        1421     1440      +19     
==========================================
- Hits         1198     1174      -24     
- Misses        223      266      +43     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `81.52% <65.51%> (-2.78%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/791?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/executors/clip\_torch.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/791/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RvcmNoLnB5) | `82.53% <55.55%> (-5.18%)` | :arrow_down: |
| [server/clip\_server/executors/clip\_tensorrt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/791/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RlbnNvcnJ0LnB5) | `92.72% <63.63%> (-7.28%)` | :arrow_down: |
| [server/clip\_server/executors/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/791/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX29ubngucHk=) | `84.05% <77.77%> (-1.66%)` | :arrow_down: |
| [server/clip\_server/model/trt\_utils.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/791/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3RydF91dGlscy5weQ==) | `56.04% <0.00%> (-27.48%)` | :arrow_down: |
| [server/clip\_server/model/clip\_trt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/791/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfdHJ0LnB5) | `69.38% <0.00%> (-16.33%)` | :arrow_down: |

Help us with your feedback. Take ten seconds to tell us [how you rate us](https://about.codecov.io/nps?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report main ba decrease coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage help u feedback take ten tell u rate u,issue,negative,positive,positive,positive,positive,positive
1200487812,"Figured it out:
jtype: Flow
version: '1'
with:
  port: 51000
  protocol: http
  prefetch: 10
executors:
  - name: clip_t
    replicas: 1
    uses:
      jtype: CLIPEncoder
      with:
        name: ViT-L/14
      metas:
        py_modules:
          - executors/clip_torch.py",figured flow version port protocol name name,issue,negative,neutral,neutral,neutral,neutral,neutral
1199019665,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/784?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#784](https://codecov.io/gh/jina-ai/clip-as-service/pull/784?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (58d5eb4) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/8b8082a939f67f7ea01cc9f55ebce9c5368ebe1a?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (8b8082a) will **decrease** coverage by `11.68%`.
> The diff coverage is `n/a`.

```diff
@@             Coverage Diff             @@
##             main     #784       +/-   ##
===========================================
- Coverage   84.30%   72.62%   -11.69%     
===========================================
  Files          21       21               
  Lines        1421     1421               
===========================================
- Hits         1198     1032      -166     
- Misses        223      389      +166     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `72.62% <ø> (-11.69%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/784?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/executors/clip\_tensorrt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/784/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RlbnNvcnJ0LnB5) | `0.00% <0.00%> (-100.00%)` | :arrow_down: |
| [server/clip\_server/model/clip\_trt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/784/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfdHJ0LnB5) | `0.00% <0.00%> (-85.72%)` | :arrow_down: |
| [server/clip\_server/model/trt\_utils.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/784/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3RydF91dGlscy5weQ==) | `0.00% <0.00%> (-83.52%)` | :arrow_down: |

Help us with your feedback. Take ten seconds to tell us [how you rate us](https://about.codecov.io/nps?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report deb main ba decrease coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage help u feedback take ten tell u rate u,issue,negative,positive,positive,positive,positive,positive
1198861990,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/788?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#788](https://codecov.io/gh/jina-ai/clip-as-service/pull/788?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (e9fbf06) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/1db43b485b0fe368eb3949ddc052b5dd8002c279?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (1db43b4) will **decrease** coverage by `15.74%`.
> The diff coverage is `n/a`.

```diff
@@             Coverage Diff             @@
##             main     #788       +/-   ##
===========================================
- Coverage   85.89%   70.14%   -15.75%     
===========================================
  Files          21       20        -1     
  Lines        1134     1055       -79     
===========================================
- Hits          974      740      -234     
- Misses        160      315      +155     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `70.14% <ø> (-15.75%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/788?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/executors/clip\_tensorrt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/788/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RlbnNvcnJ0LnB5) | `0.00% <0.00%> (-100.00%)` | :arrow_down: |
| [server/clip\_server/model/clip\_trt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/788/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfdHJ0LnB5) | `0.00% <0.00%> (-85.72%)` | :arrow_down: |
| [server/clip\_server/model/trt\_utils.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/788/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3RydF91dGlscy5weQ==) | `0.00% <0.00%> (-83.52%)` | :arrow_down: |
| [server/clip\_server/executors/helper.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/788/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9oZWxwZXIucHk=) | `95.58% <0.00%> (-1.48%)` | :arrow_down: |
| [client/clip\_client/client.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/788/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L2NsaWVudC5weQ==) | `85.78% <0.00%> (-1.02%)` | :arrow_down: |
| [server/clip\_server/model/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/788/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfb25ueC5weQ==) | `88.88% <0.00%> (+6.66%)` | :arrow_up: |

------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/788?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/788?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [c458dd6...e9fbf06](https://codecov.io/gh/jina-ai/clip-as-service/pull/788?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report main decrease coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage continue review full report legend click learn absolute relative impact affected missing data powered last update read comment,issue,negative,positive,neutral,neutral,positive,positive
1195101741,"According to this PR, I feel the model table contains some redundant information. Let's first hold this PR until we have an idea how to proceed. ",according feel model table redundant information let first hold idea proceed,issue,negative,positive,neutral,neutral,positive,positive
1194999490,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/785?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#785](https://codecov.io/gh/jina-ai/clip-as-service/pull/785?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (6bbfe74) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/f043b4d934a9454b5db32e7ea7331307506a1a6f?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (f043b4d) will **increase** coverage by `0.20%`.
> The diff coverage is `81.15%`.

```diff
@@            Coverage Diff             @@
##             main     #785      +/-   ##
==========================================
+ Coverage   85.72%   85.92%   +0.20%     
==========================================
  Files          21       21              
  Lines        1121     1137      +16     
==========================================
+ Hits          961      977      +16     
  Misses        160      160              
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `85.92% <81.15%> (+0.20%)` | :arrow_up: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/785?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/executors/clip\_torch.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/785/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RvcmNoLnB5) | `87.93% <ø> (ø)` | |
| [server/clip\_server/model/mclip\_model.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/785/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL21jbGlwX21vZGVsLnB5) | `83.33% <0.00%> (ø)` | |
| [server/clip\_server/model/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/785/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfb25ueC5weQ==) | `82.22% <78.94%> (-6.02%)` | :arrow_down: |
| [server/clip\_server/model/clip\_trt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/785/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfdHJ0LnB5) | `85.71% <78.94%> (-6.40%)` | :arrow_down: |
| [server/clip\_server/model/openclip\_model.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/785/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL29wZW5jbGlwX21vZGVsLnB5) | `83.33% <81.25%> (+17.47%)` | :arrow_up: |
| [server/clip\_server/model/clip\_model.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/785/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfbW9kZWwucHk=) | `87.50% <91.66%> (+3.28%)` | :arrow_up: |
| [server/clip\_server/executors/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/785/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX29ubngucHk=) | `85.93% <100.00%> (ø)` | |
| [server/clip\_server/executors/clip\_tensorrt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/785/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RlbnNvcnJ0LnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/model/clip.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/785/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXAucHk=) | `92.10% <0.00%> (-1.32%)` | :arrow_down: |

------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/785?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/785?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [8bd8389...6bbfe74](https://codecov.io/gh/jina-ai/clip-as-service/pull/785?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report main increase coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage continue review full report legend click learn absolute relative impact affected missing data powered last update read comment,issue,negative,positive,neutral,neutral,positive,positive
1192334952,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/779?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#779](https://codecov.io/gh/jina-ai/clip-as-service/pull/779?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (93771e1) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/fa62d8e93baf2579b2934cc0ed8daca12c144d7d?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (fa62d8e) will **increase** coverage by `0.02%`.
> The diff coverage is `100.00%`.

```diff
@@            Coverage Diff             @@
##             main     #779      +/-   ##
==========================================
+ Coverage   86.61%   86.64%   +0.02%     
==========================================
  Files          21       21              
  Lines        1106     1108       +2     
==========================================
+ Hits          958      960       +2     
  Misses        148      148              
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `86.64% <100.00%> (+0.02%)` | :arrow_up: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/779?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [client/clip\_client/client.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/779/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L2NsaWVudC5weQ==) | `86.80% <100.00%> (+0.13%)` | :arrow_up: |

------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/779?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/779?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [fa62d8e...93771e1](https://codecov.io/gh/jina-ai/clip-as-service/pull/779?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report main fade increase coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage continue review full report legend click learn absolute relative impact affected missing data powered last update fade read comment,issue,negative,positive,neutral,neutral,positive,positive
1192317922,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/778?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#778](https://codecov.io/gh/jina-ai/clip-as-service/pull/778?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (4f9c8f1) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/32b11cd64bb76bca5075fbcbc84b9334952c236c?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (32b11cd) will **increase** coverage by `0.90%`.
> The diff coverage is `62.79%`.

> :exclamation: Current head 4f9c8f1 differs from pull request most recent head ac7acb7. Consider uploading reports for the commit ac7acb7 to get more accurate results

```diff
@@            Coverage Diff             @@
##             main     #778      +/-   ##
==========================================
+ Coverage   81.91%   82.81%   +0.90%     
==========================================
  Files          17       21       +4     
  Lines        1255     1135     -120     
==========================================
- Hits         1028      940      -88     
+ Misses        227      195      -32     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `82.81% <62.79%> (+0.90%)` | :arrow_up: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/778?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [client/clip\_client/client.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/778/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L2NsaWVudC5weQ==) | `82.14% <62.79%> (-4.53%)` | :arrow_down: |
| [server/clip\_server/model/trt\_utils.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/778/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3RydF91dGlscy5weQ==) | `56.04% <0.00%> (-27.48%)` | :arrow_down: |
| [server/clip\_server/model/clip\_trt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/778/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfdHJ0LnB5) | `71.05% <0.00%> (-21.06%)` | :arrow_down: |
| [server/clip\_server/model/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/778/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfb25ueC5weQ==) | `88.23% <0.00%> (-8.74%)` | :arrow_down: |
| [server/clip\_server/executors/helper.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/778/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9oZWxwZXIucHk=) | `97.05% <0.00%> (-2.95%)` | :arrow_down: |
| [server/clip\_server/executors/clip\_tensorrt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/778/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RlbnNvcnJ0LnB5) | `100.00% <0.00%> (ø)` | |
| [server/clip\_server/model/model.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/778/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL21vZGVsLnB5) | | |
| [server/clip\_server/model/openclip\_model.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/778/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL29wZW5jbGlwX21vZGVsLnB5) | `89.28% <0.00%> (ø)` | |
| [server/clip\_server/model/mclip\_model.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/778/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL21jbGlwX21vZGVsLnB5) | `83.33% <0.00%> (ø)` | |
| [server/clip\_server/model/clip\_model.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/778/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfbW9kZWwucHk=) | `84.21% <0.00%> (ø)` | |
| ... and [5 more](https://codecov.io/gh/jina-ai/clip-as-service/pull/778/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | |

Help us with your feedback. Take ten seconds to tell us [how you rate us](https://about.codecov.io/nps?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Have a feature suggestion? [Share it here.](https://app.codecov.io/gh/feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
",report main increase coverage coverage exclamation current head pull request recent head consider commit get accurate coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage help u feedback take ten tell u rate u feature suggestion share,issue,positive,positive,positive,positive,positive,positive
1192159666,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/777?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#777](https://codecov.io/gh/jina-ai/clip-as-service/pull/777?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (32434c2) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/fa62d8e93baf2579b2934cc0ed8daca12c144d7d?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (fa62d8e) will **decrease** coverage by `0.85%`.
> The diff coverage is `62.50%`.

> :exclamation: Current head 32434c2 differs from pull request most recent head f09eadb. Consider uploading reports for the commit f09eadb to get more accurate results

```diff
@@            Coverage Diff             @@
##             main     #777      +/-   ##
==========================================
- Coverage   86.61%   85.76%   -0.86%     
==========================================
  Files          21       21              
  Lines        1106     1124      +18     
==========================================
+ Hits          958      964       +6     
- Misses        148      160      +12     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `85.76% <62.50%> (-0.86%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/777?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [client/clip\_client/client.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/777/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L2NsaWVudC5weQ==) | `82.15% <62.50%> (-4.51%)` | :arrow_down: |

------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/777?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/777?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [fa62d8e...f09eadb](https://codecov.io/gh/jina-ai/clip-as-service/pull/777?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report main fade decrease coverage coverage exclamation current head pull request recent head consider commit get accurate coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage continue review full report legend click learn absolute relative impact affected missing data powered last update fade read comment,issue,negative,positive,positive,positive,positive,positive
1192097391,"> Do I need to train my CLIP model again with FineTuner?

That depends on your domain. The pre-trained CLIP model is trained with a large-scale corpus in the general domain. So, usually, we can archive decent results without fine-tuning. Of course, fine-tuning can leads to a better one always. 

> How I need to prepare data ? Right now, I have a dataframe with one column as ""image path"" and other column as. ""caption"" for those particular images. If you can share some examples or notebooks to utilize my work directly into the finetuner that would be great help.

Yes, you need convert your data into `docarray` format. checkout this page: https://finetuner.jina.ai/walkthrough/create-training-data/ tab 3

> I can see, it needs my data need to be uploaded to the cloud and training has to be done at cloud. But i don't have permission to share the data on any outside server. Is it possible to use Finetuner on my machine directly instead of uploading the data to cloud and doing the training and evaluation there?

as for now, yes the only way is to upload it to cloud. While fine-tuning does not require “all” data, constructing several thousands piece of  training data would be enough.
",need train clip model domain clip model trained corpus general domain usually archive decent without course better one always need prepare data right one column image path column caption particular share utilize work directly would great help yes need convert data format page tab see need data need cloud training done cloud permission share data outside server possible use machine directly instead data cloud training evaluation yes way cloud require data several piece training data would enough,issue,positive,positive,positive,positive,positive,positive
1190046661,"> > how I can evaluate it on my already trained CLIP model?
> > You can evaluate your model with the downstream task ., e.g., classification, retrieval, based on your dataset.
> 
> And `finetuner` is free to use. Please play it around, and give your comments!

I have few more questions:
1. Do I need to train my CLIP model again with FineTuner?
2. How I need to prepare data ? Right now, I have a dataframe with one column as ""image path"" and other column as. ""caption"" for those particular images. If you can share some examples or notebooks to utilize my work directly into the finetuner that would be great help.
3. I can see, it needs my data need to be uploaded to the cloud and training has to be done at cloud. But i don't have permission to share the data on any outside server. Is it possible to use Finetuner on my machine directly instead of uploading the data to cloud and doing the training and evaluation there?

Thanks",evaluate already trained clip model evaluate model downstream task classification retrieval based free use please play around give need train clip model need prepare data right one column image path column caption particular share utilize work directly would great help see need data need cloud training done cloud permission share data outside server possible use machine directly instead data cloud training evaluation thanks,issue,positive,positive,positive,positive,positive,positive
1190036745,"> how I can evaluate it on my already trained CLIP model?
You can evaluate your model with the downstream task ., e.g., classification, retrieval, based on your dataset.

And `finetuner` is free to use. Please play it around, and give your comments!
",evaluate already trained clip model evaluate model downstream task classification retrieval based free use please play around give,issue,positive,positive,positive,positive,positive,positive
1189881012,"> > Can CLIP can help to generate embedding for products and club into one fixed length vector that represents the product embedding?
> 
> the answer is yes. CLIP can produce text embedding and image embedding with text and image input respectively. As a consequence, for each product in your case, two embeddings will be generated for every modality. Then, you can simply use `concat` or `add` operator to convert the resulting two embeddings into one fixed-length vector. BTW, you can also direclty use the modality embedding separately for downstream application, and aggregate the predictions/results with different weights at the end.
> 
> > then once I train the CLIP model using image and text, how I can evaluate the efficiency and accuracy of the embeddings learned.
> 
> To train and evaluate CLIP models with custom data, I suggest you can try this tool `finetuner`. This tool enables users to finetune and evaluate their embedding model with fewer efforts. For more details, you can check this doc https://finetuner.jina.ai/tasks/text-to-image/

@numb3r3 Thanks for answering. Just wanted to understand how I can evaluate it on my already trained CLIP model? 
Also, is finetuner chargeable or its free to use?",clip help generate club one fixed length vector product answer yes clip produce text image text image input respectively consequence product case two every modality simply use add operator convert resulting two one vector also use modality separately downstream application aggregate different end train clip model image text evaluate efficiency accuracy learned train evaluate clip custom data suggest try tool tool evaluate model check doc thanks understand evaluate already trained clip model also chargeable free use,issue,positive,positive,positive,positive,positive,positive
1189814370,"> Can CLIP can help to generate embedding for products and club into one fixed length vector that represents the product embedding? 

the answer is yes. CLIP can produce text embedding and image embedding with text and image input respectively. As a consequence, for each product in your case, two embeddings will be generated for every modality. Then, you can simply use `concat` or `add` operator to convert the resulting two embeddings into one fixed-length vector. BTW, you can also direclty use the modality embedding separately for downstream application, and aggregate the predictions/results with different weights at the end. 

> then once I train the CLIP model using image and text, how I can evaluate the efficiency and accuracy of the embeddings learned.

To train and evaluate CLIP models with custom data, I suggest you can try this tool `finetuner`. This tool enables users to finetune and evaluate their embedding model with fewer efforts. For more details, you can check this doc https://finetuner.jina.ai/tasks/text-to-image/
",clip help generate club one fixed length vector product answer yes clip produce text image text image input respectively consequence product case two every modality simply use add operator convert resulting two one vector also use modality separately downstream application aggregate different end train clip model image text evaluate efficiency accuracy learned train evaluate clip custom data suggest try tool tool evaluate model check doc,issue,positive,positive,neutral,neutral,positive,positive
1188828115,"i guess we need to tweak the words a bit, I'll invite @violenil  to take a look",guess need tweak bit invite take look,issue,negative,neutral,neutral,neutral,neutral,neutral
1188506256,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/772?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#772](https://codecov.io/gh/jina-ai/clip-as-service/pull/772?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (955130d) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/2b78b12e3aa527b386eac4ee7eed74e580eadbf6?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (2b78b12) will **decrease** coverage by `60.01%`.
> The diff coverage is `n/a`.

> :exclamation: Current head 955130d differs from pull request most recent head e30d286. Consider uploading reports for the commit e30d286 to get more accurate results

```diff
@@             Coverage Diff             @@
##             main     #772       +/-   ##
===========================================
- Coverage   82.09%   22.08%   -60.02%     
===========================================
  Files          17       14        -3     
  Lines        1240     1028      -212     
===========================================
- Hits         1018      227      -791     
- Misses        222      801      +579     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `22.08% <ø> (-60.02%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/772?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/executors/clip\_tensorrt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/772/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RlbnNvcnJ0LnB5) | `0.00% <0.00%> (-100.00%)` | :arrow_down: |
| [server/clip\_server/model/clip\_trt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/772/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfdHJ0LnB5) | `0.00% <0.00%> (-92.11%)` | :arrow_down: |
| [server/clip\_server/executors/clip\_torch.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/772/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RvcmNoLnB5) | `0.00% <0.00%> (-87.04%)` | :arrow_down: |
| [server/clip\_server/executors/clip\_hg.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/772/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX2hnLnB5) | `0.00% <0.00%> (-86.08%)` | :arrow_down: |
| [server/clip\_server/model/trt\_utils.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/772/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3RydF91dGlscy5weQ==) | `0.00% <0.00%> (-83.52%)` | :arrow_down: |
| [server/clip\_server/executors/helper.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/772/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9oZWxwZXIucHk=) | `16.92% <0.00%> (-83.08%)` | :arrow_down: |
| [server/clip\_server/model/simple\_tokenizer.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/772/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3NpbXBsZV90b2tlbml6ZXIucHk=) | `43.87% <0.00%> (-51.03%)` | :arrow_down: |
| [server/clip\_server/executors/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/772/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX29ubngucHk=) | `37.09% <0.00%> (-48.39%)` | :arrow_down: |
| [server/clip\_server/model/model.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/772/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL21vZGVsLnB5) | `15.83% <0.00%> (-46.34%)` | :arrow_down: |
| [server/clip\_server/model/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/772/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfb25ueC5weQ==) | `54.54% <0.00%> (-42.43%)` | :arrow_down: |
| ... and [4 more](https://codecov.io/gh/jina-ai/clip-as-service/pull/772/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | |

------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/772?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/772?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [2b78b12...e30d286](https://codecov.io/gh/jina-ai/clip-as-service/pull/772?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report main decrease coverage coverage exclamation current head pull request recent head consider commit get accurate coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage continue review full report legend click learn absolute relative impact affected missing data powered last update read comment,issue,negative,positive,positive,positive,positive,positive
1188499377,"> docs failing. possible reason: [ryanfox/sphinx-markdown-tables#36](https://github.com/ryanfox/sphinx-markdown-tables/issues/36)

I will fix this issue in main since it breaks our doc building, please rebase your PR then. ",failing possible reason fix issue main since doc building please rebase,issue,negative,positive,neutral,neutral,positive,positive
1185386180,We have a better idea to support this feature. I will close this PR now. ,better idea support feature close,issue,positive,positive,positive,positive,positive,positive
1184180213,"@numb3r3 Thanks. I will try to play with `async` for deep learning model to see the performance boost :D. Anyway, if jina can support both `batching` and `async`  then we can maximize the utilize of GPU. ",thanks try play deep learning model see performance boost anyway jina support maximize utilize,issue,positive,positive,neutral,neutral,positive,positive
1183968125,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/767?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#767](https://codecov.io/gh/jina-ai/clip-as-service/pull/767?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (73c5a04) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/ee7da10d1f56a130e6f9a85d5fb3518b80e5df0d?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (ee7da10) will **decrease** coverage by `12.20%`.
> The diff coverage is `91.11%`.

```diff
@@             Coverage Diff             @@
##             main     #767       +/-   ##
===========================================
- Coverage   79.03%   66.82%   -12.21%     
===========================================
  Files          17       18        +1     
  Lines        1240     1272       +32     
===========================================
- Hits          980      850      -130     
- Misses        260      422      +162     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `66.82% <91.11%> (-12.21%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/767?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/executors/clip\_torch.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/767/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RvcmNoLnB5) | `75.75% <84.00%> (-11.28%)` | :arrow_down: |
| [server/clip\_server/model/mclip.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/767/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL21jbGlwLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/model/trt\_utils.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/767/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3RydF91dGlscy5weQ==) | `56.04% <0.00%> (-27.48%)` | :arrow_down: |
| [server/clip\_server/executors/clip\_hg.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/767/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX2hnLnB5) | `60.75% <0.00%> (-25.32%)` | :arrow_down: |
| [server/clip\_server/model/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/767/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfb25ueC5weQ==) | `72.72% <0.00%> (-24.25%)` | :arrow_down: |
| [server/clip\_server/model/clip\_trt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/767/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfdHJ0LnB5) | `71.05% <0.00%> (-21.06%)` | :arrow_down: |
| [client/clip\_client/client.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/767/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L2NsaWVudC5weQ==) | `71.11% <0.00%> (-17.23%)` | :arrow_down: |
| [server/clip\_server/model/model.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/767/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL21vZGVsLnB5) | `49.42% <0.00%> (-12.75%)` | :arrow_down: |
| [server/clip\_server/executors/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/767/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX29ubngucHk=) | `74.19% <0.00%> (-11.30%)` | :arrow_down: |
| ... and [3 more](https://codecov.io/gh/jina-ai/clip-as-service/pull/767/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | |

------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/767?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/767?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [ee7da10...73c5a04](https://codecov.io/gh/jina-ai/clip-as-service/pull/767?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report ca main decrease coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage continue review full report legend click learn absolute relative impact affected missing data powered last update ca read comment,issue,negative,positive,neutral,neutral,positive,positive
1183863934,"yes, `async` works well with deep learning model. we perform some experiments to confirm `async` can benefit the pytorch/tensorflow. And feel free to use replicas, cause they can share to use the single GPU. ",yes work well deep learning model perform confirm benefit feel free use cause share use single,issue,positive,positive,positive,positive,positive,positive
1183105908,"@numb3r3 In our case, we start with 1 worker (1 replica) so that our server process requests sequentially, that why we need micro batching. I see in your code, you use `async` for `encode` function but I'm not sure if deep learning model such as pytorch, tensorflow can run `async` ?  ",case start worker replica server process sequentially need micro see code use encode function sure deep learning model run,issue,negative,positive,positive,positive,positive,positive
1182982747,"Yes, I agree that simple scheduling should be a good starting point. We would seriously think about how to proceed. Regarding your use case, I cannot see the bottleneck, maybe I got a misunderstanding. 
",yes agree simple good starting point would seriously think proceed regarding use case see bottleneck maybe got misunderstanding,issue,positive,positive,neutral,neutral,positive,positive
1182793863,"@numb3r3 Thanks for replying to me quickly.

> The biggest challenge here is to develop a scheduling algorithm to balance the maximum throughput and latency

In this aspect, I think we just need options such as `max_letency` and `max_batch_size` then let users tune those parameters to balance their latency and throughput. We can begin with a simple scheduling algorithm assuming that the number of requests sent to the server is stable.

> BTW, may I know the target throughput you expect in your project? Maybe we have another way to meet your case.

In my project, I have a model that only uses 5gb GPU Vram with batch_size <= 8. I also try to simulate a situation where the number of requests is much larger than it should be in reality to check if my GPU Vram increases or not. With batch_size = 8 our model only requires 1.5-2 times inference time with batch_size = 1 without increasing GPU Vram.
",thanks quickly biggest challenge develop algorithm balance maximum throughput latency aspect think need let tune balance latency throughput begin simple algorithm assuming number sent server stable may know target throughput expect project maybe another way meet case project model also try simulate situation number much reality check model time inference time without increasing,issue,positive,positive,positive,positive,positive,positive
1182737864,"Thanks a lot for your interest!

That's a pity, **micro batching** has not been supported yet. We also agree that clip-as-service will also benefit from micro-batching. The biggest challenge here is to develop a scheduling algorithm to balance the maximum throughput and latency. We need to dig into this algorithm to archive decent performance. 

BTW, may I know the target throughput you expect in your project?   Maybe we have another way to meet your case. ",thanks lot interest pity micro yet also agree also benefit biggest challenge develop algorithm balance maximum throughput latency need dig algorithm archive decent performance may know target throughput expect project maybe another way meet case,issue,positive,positive,neutral,neutral,positive,positive
1179981469,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/764?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#764](https://codecov.io/gh/jina-ai/clip-as-service/pull/764?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (bee786f) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/ed1b92d1896cc0c12733b51bd1bd83040676f505?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (ed1b92d) will **not change** coverage.
> The diff coverage is `n/a`.

```diff
@@           Coverage Diff           @@
##             main     #764   +/-   ##
=======================================
  Coverage   78.31%   78.31%           
=======================================
  Files          17       17           
  Lines        1213     1213           
=======================================
  Hits          950      950           
  Misses        263      263           
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `78.31% <ø> (ø)` | |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.


------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/764?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/764?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [ed1b92d...bee786f](https://codecov.io/gh/jina-ai/clip-as-service/pull/764?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report beef main change coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find continue review full report legend click learn absolute relative impact affected missing data powered last update beef read comment,issue,negative,positive,neutral,neutral,positive,positive
1175686736,"@sjcotto maybe the metric `search_params = {""metric_type"": ""L2""}` chosen here does not fit with clip model. Usually, we use `cosine` as the distance metric, cause CLIP is trained with cosine distance. ",maybe metric chosen fit clip model usually use cosine distance metric cause clip trained cosine distance,issue,negative,positive,neutral,neutral,positive,positive
1175661247,"maybe the problem is related to narray vs list? i notice that in order to store in milvus i need to transform to list `([vector.tolist()[0]])`

",maybe problem related list notice order store need transform list,issue,negative,neutral,neutral,neutral,neutral,neutral
1175127682,"Hi, thk for quick reply. 

not receiving an error but just random search results, i'm trying the same examples in the jina fashion example for no luck

do you know if there is something odd in the code above?",hi quick reply error random search trying jina fashion example luck know something odd code,issue,negative,negative,negative,negative,negative,negative
1175109980,"Hi @sjcotto , could you provide more details like the error messages or the unexpected behaviors?",hi could provide like error unexpected,issue,negative,positive,neutral,neutral,positive,positive
1175048810,"Thanks. 

I was trying to use the clip-as-service only to understand. 

For some reason the image to image search works perfectly but he text to image don't, i need to do more research for sure. 

the save to milvus

```
    c = Client(_CLIP_AS_SERVICE)
    
    d = Document(uri=item.uri,tags=item.data)
    da = DocumentArray()
    da.append(d)

    r = await c.aencode( da )
    vector = r.embeddings

    data = [
        [item.id],
        [vector.tolist()[0]],
    ]

    collection = Collection(item.index)
    collection.load()    
    collection.insert(data)
```    

the search

```
c = Client(_CLIP_AS_SERVICE)
    d = Document()

    if item_input.text is not None:
        d.text=item_input.text
    if item_input.uri is not None:
        d.uri=item_input.uri

    da = DocumentArray()
    da.append(d)

    r = await c.aencode( da )
    vector = r.embeddings
    print(vector)

    collection = Collection(item_input.index) 
    collection.load()
 
    search_params = {""metric_type"": ""L2""}
    results = collection.search(vector, anns_field=_VECTOR_FIELD_NAME, param=search_params, limit=5 )
    

tests
![image](https://user-images.githubusercontent.com/3880470/177336179-deb993f5-41dd-42c8-883b-6daa79990c84.png)

i did many tests putting the text as literal what is in image and looks the replies are random, i'm using the same dataset used in jina fashion example

![image](https://user-images.githubusercontent.com/3880470/177336319-45145aed-ba75-438e-a11a-f38f44f0c497.png)
```

",thanks trying use understand reason image image search work perfectly text image need research sure save client document da await da vector data collection collection data search client document none none da await da vector print vector collection collection vector image many text literal image random used jina fashion example image,issue,positive,positive,positive,positive,positive,positive
1174543938,"Hi @sjcotto ! There are different ways to generate vector embeddings:

1. You can manually assign vector embedding [here](https://docarray.jina.ai/fundamentals/document/embedding/). 
2. You can use Clip-as-Service to generate vector embedding using Clip models [here](https://clip-as-service.jina.ai/user-guides/client/#encoding). 
3. You can use different encoders from [Jina Hub](https://hub.jina.ai). 

You may then follow the example you found to create a search application.🚀 Remember to take a look at our latest product [Jina Now](https://now.jina.ai) to build neural search in just one command execution 👀👀👀",hi different way generate vector manually assign vector use generate vector clip use different jina hub may follow example found create search application rocket remember take look latest product jina build neural search one command execution,issue,negative,positive,positive,positive,positive,positive
1174463520,"thanks a lot!

just to confirm, doing that it will index text (json fields tags) + image into the vector and doing so the search will improve?",thanks lot confirm index text image vector search improve,issue,positive,positive,positive,positive,positive,positive
1173896634,"Hello @sjcotto , glad to see you here! You can utilize the capability of [DocArray](https://docarray.jina.ai/) to store such information. For example, you can store product name, category to different tags of a document and imageurl to uri of that document ([refer](https://docarray.jina.ai/fundamentals/document/)). You can also create a more complex structure by putting things in the chunks or matches ([refer](https://docarray.jina.ai/fundamentals/document/nested/)). Just do some experiments and feel the power of DocArray!",hello glad see utilize capability store information example store product name category different document document refer also create complex structure refer feel power,issue,positive,positive,neutral,neutral,positive,positive
1170684694,"@morgoth95 very nice, looking forward the next nebullvm release. ",nice looking forward next release,issue,negative,positive,positive,positive,positive,positive
1169230102,"@numb3r3 Hi, I hope all is fine on the Jina side. I took a look at the failed tests and I may have identified the source of the problem. Unfortunately it will require some work on our end, but we think it will be fixed with the next nebullvm release (it will happen in about a week ;)). I will get back to you as soon as the problem is fixed :)",hi hope fine jina side took look may source problem unfortunately require work end think fixed next release happen week get back soon problem fixed,issue,negative,positive,neutral,neutral,positive,positive
1166922106,"@JoanFM The examples are [here](https://github.com/jina-ai/clip-as-service#visual-reasoning).

@bitcoinmeetups Good catch👍 Those results indeed are not perfect because we are using general models in the examples (i.e. they are not fine-tuned for given inputs). You can also test visual reasoning for other images [here](https://clip-as-service.jina.ai/playground/reasoning/).",good catch indeed perfect general given also test visual reasoning,issue,positive,positive,positive,positive,positive,positive
1166915578,"We show random, non-cherry picked, predictions of zero-shot CLIP classifiers on examples. ",show random picked clip,issue,negative,negative,negative,negative,negative,negative
1166910680,"The examples are from the Readme.md file on the front page of the repository. That's why I'm wondering, why use examples where the AI is mostly incorrect?",file front page repository wondering use ai mostly incorrect,issue,negative,positive,positive,positive,positive,positive
1166909898,"Hello @bitcoinmeetups ,

Can you share here the examples that you are referring to? What exactly are the images and texts passed to the CAS?

Also please take into accound that all the models are not 100% perfect.",hello share exactly also please take perfect,issue,positive,positive,positive,positive,positive,positive
1160443199,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/758?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#758](https://codecov.io/gh/jina-ai/clip-as-service/pull/758?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (cf7f70c) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/03541dd765849ec453b501c83dbf4071b317bce1?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (03541dd) will **decrease** coverage by `0.33%`.
> The diff coverage is `28.57%`.

```diff
@@            Coverage Diff             @@
##             main     #758      +/-   ##
==========================================
- Coverage   80.96%   80.62%   -0.34%     
==========================================
  Files          17       17              
  Lines        1208     1213       +5     
==========================================
  Hits          978      978              
- Misses        230      235       +5     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `80.62% <28.57%> (-0.34%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/758?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/\_\_main\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/758/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL19fbWFpbl9fLnB5) | `0.00% <0.00%> (ø)` | |
| [client/clip\_client/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/758/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/758/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |

------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/758?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/758?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [4a298d4...cf7f70c](https://codecov.io/gh/jina-ai/clip-as-service/pull/758?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report main decrease coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage continue review full report legend click learn absolute relative impact affected missing data powered last update ad read comment,issue,negative,positive,neutral,neutral,positive,positive
1156480787,"


> looks like a test failing

That's weird. Fixed by reran the failed one. Anyway, learned a lot of lessons from this task🥹",like test failing weird fixed one anyway learned lot task,issue,negative,negative,negative,negative,negative,negative
1156075865,Remember to add a link to ONNX runtime and vice versa,remember add link vice,issue,negative,neutral,neutral,neutral,neutral,neutral
1156013706,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/754?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#754](https://codecov.io/gh/jina-ai/clip-as-service/pull/754?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (97595d9) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/9d872f2e20e53e988a6d13dff191a42fa6e7e0d2?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (9d872f2) will **not change** coverage.
> The diff coverage is `n/a`.

```diff
@@           Coverage Diff           @@
##             main     #754   +/-   ##
=======================================
  Coverage   81.78%   81.78%           
=======================================
  Files          17       17           
  Lines        1208     1208           
=======================================
  Hits          988      988           
  Misses        220      220           
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `81.78% <ø> (ø)` | |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.


------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/754?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/754?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [4c4e74b...97595d9](https://codecov.io/gh/jina-ai/clip-as-service/pull/754?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report main change coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find continue review full report legend click learn absolute relative impact affected missing data powered last update read comment,issue,negative,positive,neutral,neutral,positive,positive
1156002880,"@ZiniuYu please fix the commit message issue. 
```
Error: You have commit messages with errors
⧗   input: docs: RAM usage
✖   subject must not be sentence-case, start-case, pascal-case, upper-case [subject-case]
✖   found 1 problems, 0 warnings
```",please fix commit message issue error commit input ram usage multiply subject must multiply found,issue,negative,negative,neutral,neutral,negative,negative
1153865296,Please fix this https://jinaai.slack.com/archives/C038JR9U6F6/p1655123876925349 as well. Explain all questions I mentioned in very details,please fix well explain,issue,positive,neutral,neutral,neutral,neutral,neutral
1153396461,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/751?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#751](https://codecov.io/gh/jina-ai/clip-as-service/pull/751?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (d2d7be2) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/4d069a84ac0414059acce322f00815bf0cd12536?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (4d069a8) will **not change** coverage.
> The diff coverage is `n/a`.

```diff
@@           Coverage Diff           @@
##             main     #751   +/-   ##
=======================================
  Coverage   81.78%   81.78%           
=======================================
  Files          17       17           
  Lines        1208     1208           
=======================================
  Hits          988      988           
  Misses        220      220           
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `81.78% <ø> (ø)` | |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.


------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/751?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/751?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [96923f1...d2d7be2](https://codecov.io/gh/jina-ai/clip-as-service/pull/751?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report main da change coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find continue review full report legend click learn absolute relative impact affected missing data powered last update read comment,issue,negative,positive,neutral,neutral,positive,positive
1152102045,"> traversal path should not be handled in CAS, user should first traverse and then send via client

The executors in the CAS will then be uploaded to the hub. So I think the `traversal_paths` support is necessary.  And of course, if the user uses CAS client, the recommended way is to traverse first (the `traversal_paths` will not be mentioned in CAS doc). 

",traversal path handled user first traverse send via client hub think support necessary course user client way traverse first doc,issue,negative,positive,positive,positive,positive,positive
1152095203,"traversal path should not be handled in CAS, user should first traverse and then send via client",traversal path handled user first traverse send via client,issue,negative,positive,positive,positive,positive,positive
1151958404,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/748?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#748](https://codecov.io/gh/jina-ai/clip-as-service/pull/748?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (eca7708) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/752202f8b730d0ab8785a703fc719dacfbe2993b?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (752202f) will **increase** coverage by `0.27%`.
> The diff coverage is `96.96%`.

```diff
@@            Coverage Diff             @@
##             main     #748      +/-   ##
==========================================
+ Coverage   81.49%   81.76%   +0.27%     
==========================================
  Files          17       17              
  Lines        1232     1212      -20     
==========================================
- Hits         1004      991      -13     
+ Misses        228      221       -7     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `81.76% <96.96%> (+0.27%)` | :arrow_up: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/748?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [client/clip\_client/client.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/748/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L2NsaWVudC5weQ==) | `88.04% <88.88%> (-0.29%)` | :arrow_down: |
| [server/clip\_server/executors/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/748/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX29ubngucHk=) | `85.48% <90.90%> (+0.76%)` | :arrow_up: |
| [client/clip\_client/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/748/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/748/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/executors/clip\_hg.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/748/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX2hnLnB5) | `86.07% <100.00%> (+0.54%)` | :arrow_up: |
| [server/clip\_server/executors/clip\_tensorrt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/748/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RlbnNvcnJ0LnB5) | `100.00% <100.00%> (+7.01%)` | :arrow_up: |
| [server/clip\_server/executors/clip\_torch.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/748/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RvcmNoLnB5) | `87.03% <100.00%> (+1.09%)` | :arrow_up: |
| [server/clip\_server/executors/helper.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/748/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9oZWxwZXIucHk=) | `100.00% <100.00%> (ø)` | |

------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/748?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/748?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [52a8b0a...eca7708](https://codecov.io/gh/jina-ai/clip-as-service/pull/748?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report main increase coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage continue review full report legend click learn absolute relative impact affected missing data powered last update aba read comment,issue,negative,positive,neutral,neutral,positive,positive
1151876268,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/747?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#747](https://codecov.io/gh/jina-ai/clip-as-service/pull/747?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (9fe4c10) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/752202f8b730d0ab8785a703fc719dacfbe2993b?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (752202f) will **decrease** coverage by `13.39%`.
> The diff coverage is `100.00%`.

```diff
@@             Coverage Diff             @@
##             main     #747       +/-   ##
===========================================
- Coverage   81.49%   68.10%   -13.40%     
===========================================
  Files          17       17               
  Lines        1232     1232               
===========================================
- Hits         1004      839      -165     
- Misses        228      393      +165     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `68.10% <100.00%> (-13.40%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/747?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [client/clip\_client/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/747/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/747/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/executors/clip\_tensorrt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/747/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RlbnNvcnJ0LnB5) | `0.00% <0.00%> (-92.99%)` | :arrow_down: |
| [server/clip\_server/model/clip\_trt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/747/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfdHJ0LnB5) | `0.00% <0.00%> (-92.31%)` | :arrow_down: |
| [server/clip\_server/model/trt\_utils.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/747/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3RydF91dGlscy5weQ==) | `0.00% <0.00%> (-83.52%)` | :arrow_down: |

------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/747?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/747?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [52a8b0a...9fe4c10](https://codecov.io/gh/jina-ai/clip-as-service/pull/747?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report main decrease coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage continue review full report legend click learn absolute relative impact affected missing data powered last update aba read comment,issue,negative,positive,neutral,neutral,positive,positive
1151127840,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/745?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#745](https://codecov.io/gh/jina-ai/clip-as-service/pull/745?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (77e0a55) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/5eb5d7e8ed6f924c6560bb850edb08bbd809ff09?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (5eb5d7e) will **decrease** coverage by `13.39%`.
> The diff coverage is `77.77%`.

```diff
@@             Coverage Diff             @@
##             main     #745       +/-   ##
===========================================
- Coverage   81.49%   68.10%   -13.40%     
===========================================
  Files          17       17               
  Lines        1232     1232               
===========================================
- Hits         1004      839      -165     
- Misses        228      393      +165     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `68.10% <77.77%> (-13.40%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/745?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/executors/clip\_tensorrt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/745/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RlbnNvcnJ0LnB5) | `0.00% <0.00%> (-92.99%)` | :arrow_down: |
| [client/clip\_client/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/745/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/745/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/executors/clip\_hg.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/745/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX2hnLnB5) | `85.52% <100.00%> (ø)` | |
| [server/clip\_server/executors/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/745/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX29ubngucHk=) | `84.72% <100.00%> (ø)` | |
| [server/clip\_server/executors/clip\_torch.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/745/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RvcmNoLnB5) | `85.93% <100.00%> (ø)` | |
| [server/clip\_server/model/clip\_trt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/745/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfdHJ0LnB5) | `0.00% <0.00%> (-92.31%)` | :arrow_down: |
| [server/clip\_server/model/trt\_utils.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/745/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3RydF91dGlscy5weQ==) | `0.00% <0.00%> (-83.52%)` | :arrow_down: |

------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/745?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/745?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [06097f2...77e0a55](https://codecov.io/gh/jina-ai/clip-as-service/pull/745?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report ea main decrease coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage continue review full report legend click learn absolute relative impact affected missing data powered last update ea read comment,issue,negative,positive,neutral,neutral,positive,positive
1148147735,"@hanxiao This PR is mainly developed for the community. If the users who already use (or finetune) the clip model from huggingface, then this PR can enable them to easily enjoy the power of jina. ",mainly community already use clip model enable easily enjoy power jina,issue,positive,positive,positive,positive,positive,positive
1146772424,"what to fix can you elaborate? incomplete issue from the maintainer is often very negative to the community. Consider when I raise an issue in Jina: `fix broken jina` without any explanation on what to fix. This gives the community user a bad impression: ""oh so Jina is broken and we better not use it"", ""the maintainer said so so it must be true""

So please elaborate this ticket otherwise I will close it in the next few days. Thanks",fix elaborate incomplete issue maintainer often negative community consider raise issue jina fix broken jina without explanation fix community user bad impression oh jina broken better use maintainer said must true please elaborate ticket otherwise close next day thanks,issue,negative,positive,neutral,neutral,positive,positive
1146772335,"looks like a simple problem but 10 days no action point, so close it.

I encourage people to solve easy problem in prompt, rather than raising an issue, especially you are the maintainer of the repo.",like simple problem day action point close encourage people solve easy problem prompt rather raising issue especially maintainer,issue,positive,positive,positive,positive,positive,positive
1146243380,"Probably a bit late to this, but I've gotten success using the same process too.  ",probably bit late gotten success process,issue,positive,neutral,neutral,neutral,neutral,neutral
1144880114,"> @morgoth95 Hi, I hope you are well. Did you get a chance to continue this PR to pass the unit test?

Hi Felix, I'm working on the tests. A couple of days and I'll close the open tasks. Looking forward to the integration!",hi hope well get chance continue pas unit test hi working couple day close open looking forward integration,issue,positive,neutral,neutral,neutral,neutral,neutral
1143102467,"@morgoth95 Hi, I hope you are well. Did you get a chance to continue this PR to pass the unit test?",hi hope well get chance continue pas unit test,issue,positive,neutral,neutral,neutral,neutral,neutral
1143090905,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/739?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#739](https://codecov.io/gh/jina-ai/clip-as-service/pull/739?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (9a4145e) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/5e06667ac9afef335b98b72a58ba0d28985d9b18?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (5e06667) will **decrease** coverage by `14.64%`.
> The diff coverage is `100.00%`.

```diff
@@             Coverage Diff             @@
##             main     #739       +/-   ##
===========================================
- Coverage   81.16%   66.52%   -14.65%     
===========================================
  Files          16       16               
  Lines        1168     1168               
===========================================
- Hits          948      777      -171     
- Misses        220      391      +171     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `66.52% <100.00%> (-14.65%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/739?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [client/clip\_client/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/739/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/739/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/executors/clip\_tensorrt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/739/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RlbnNvcnJ0LnB5) | `0.00% <0.00%> (-93.45%)` | :arrow_down: |
| [server/clip\_server/model/clip\_trt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/739/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfdHJ0LnB5) | `0.00% <0.00%> (-92.31%)` | :arrow_down: |
| [server/clip\_server/model/trt\_utils.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/739/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3RydF91dGlscy5weQ==) | `0.00% <0.00%> (-83.52%)` | :arrow_down: |
| [server/clip\_server/executors/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/739/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX29ubngucHk=) | `82.89% <0.00%> (-2.64%)` | :arrow_down: |

------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/739?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/739?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [4b88e99...9a4145e](https://codecov.io/gh/jina-ai/clip-as-service/pull/739?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report ae main decrease coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage continue review full report legend click learn absolute relative impact affected missing data powered last update ae read comment,issue,negative,positive,neutral,neutral,positive,positive
1142261165,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/737?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#737](https://codecov.io/gh/jina-ai/clip-as-service/pull/737?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (2b217a2) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/bb8c4ce01de76d2be63444a517e90b530422110e?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (bb8c4ce) will **decrease** coverage by `5.73%`.
> The diff coverage is `n/a`.

```diff
@@            Coverage Diff             @@
##             main     #737      +/-   ##
==========================================
- Coverage   66.69%   60.95%   -5.74%     
==========================================
  Files          16       16              
  Lines        1171     1168       -3     
==========================================
- Hits          781      712      -69     
- Misses        390      456      +66     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `60.95% <ø> (-5.74%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/737?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/executors/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/737/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX29ubngucHk=) | `80.26% <ø> (-5.46%)` | :arrow_down: |
| [server/clip\_server/executors/clip\_tensorrt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/737/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RlbnNvcnJ0LnB5) | `0.00% <ø> (ø)` | |
| [server/clip\_server/executors/clip\_torch.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/737/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RvcmNoLnB5) | `80.88% <ø> (-6.08%)` | :arrow_down: |
| [server/clip\_server/executors/helper.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/737/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9oZWxwZXIucHk=) | `64.40% <0.00%> (-33.90%)` | :arrow_down: |
| [client/clip\_client/client.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/737/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L2NsaWVudC5weQ==) | `68.88% <0.00%> (-18.34%)` | :arrow_down: |
| [server/clip\_server/model/clip.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/737/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXAucHk=) | `82.85% <0.00%> (-3.58%)` | :arrow_down: |
| [server/clip\_server/model/simple\_tokenizer.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/737/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3NpbXBsZV90b2tlbml6ZXIucHk=) | `93.87% <0.00%> (-1.03%)` | :arrow_down: |

------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/737?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/737?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [bb8c4ce...2b217a2](https://codecov.io/gh/jina-ai/clip-as-service/pull/737?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report ba main decrease coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage continue review full report legend click learn absolute relative impact affected missing data powered last update ba read comment,issue,negative,positive,neutral,neutral,positive,positive
1141735056,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/735?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#735](https://codecov.io/gh/jina-ai/clip-as-service/pull/735?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (be4dfa5) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/60a986a07921b2374fdc64dccde7e1ec1e728cdb?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (60a986a) will **increase** coverage by `0.05%`.
> The diff coverage is `100.00%`.

```diff
@@            Coverage Diff             @@
##             main     #735      +/-   ##
==========================================
+ Coverage   80.86%   80.91%   +0.05%     
==========================================
  Files          16       16              
  Lines        1155     1153       -2     
==========================================
- Hits          934      933       -1     
+ Misses        221      220       -1     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `80.91% <100.00%> (+0.05%)` | :arrow_up: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/735?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [client/clip\_client/client.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/735/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L2NsaWVudC5weQ==) | `87.22% <ø> (+0.40%)` | :arrow_up: |
| [client/clip\_client/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/735/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/735/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |

------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/735?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/735?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [b523c62...be4dfa5](https://codecov.io/gh/jina-ai/clip-as-service/pull/735?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report main aa increase coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage continue review full report legend click learn absolute relative impact affected missing data powered last update read comment,issue,negative,positive,neutral,neutral,positive,positive
1141667159,"> I stumbled on this issue almost an hour ago, and sadly none of the solutions above worked for me.
> 
> So I started diagnosing the problem on my own and here is the fix I found, as well as the explanation to it (hopefully it will help other people in my situation). blush
> 
> It really was the root cause in my case, since undoing the fix caused the server to fallback to CPU, and redoing it re-enabled GPU usage by the server each time.
> ### TL;DR
> 
> `sudo nvidia-smi -pm 1`
> ### Low-level stuff
> 
> The NVIDIA kernel driver has (among many other options) two modes: persistent and non-persistent.
> 
> Roughly speaking this comes down to a trade-off: **non-persistent mode** means the GPU will try to remain **idle as long as possible** (to save battery etc.) and it will fire up on connections (loading the driver application, triggering ECC scrubbing and so on), which can take many seconds.
> 
> On the other hand, the **persistent mode** will trigger it all once and keep the driver application loaded and ready for connections (at the expense of greater battery usage). [cf. [driver-persistence](https://docs.nvidia.com/deploy/driver-persistence/index.html)]
> 
> This also explains the strangely high volatile GPU-Util in ""nvidia-smi"" (for me it was consistently 90-99%) in non-persistent mode, even with no other programs using the GPU (!): the program `nvidia-smi` itself is querying the GPU, hence ""activating"" it and causing the spikes aforementioned. By the way, this was the event that caught my attention and led to this solution. It's funny how this reminded me of the [observer effect](https://en.wikipedia.org/wiki/Observer_effect_(physics)) in physics: the observation itself is changing the phenomenon, but I digress... [reference: [this discussion](https://devtalk.nvidia.com/default/topic/539632/k20-with-high-utilization-but-no-compute-processes-/)]
> 
> So I thought: doesn't the BERT server check for GPU availability before allocation? If it does, and if the GPU is idle (i.e. if the driver is configured not to persist), it will trigger the driver application to wake up and the GPU to spike like crazy (for some seconds), and by doing this, **the GPU makes itself unavailable** to clients demanding a **minimum level of availability**. And indeed, [this line of code](https://github.com/hanxiao/bert-as-service/blob/9e8824e830bb25fd16304c804244a64629a79b23/server/bert_serving/server/__init__.py#L196) requires the GPU to have a maximum load of 90% (as well as a maximum memory usage of 90%). So in the event of temporary unavailability, it will return an empty list (I tested), even when all else seems good (GPU is there, CUDA driver configured, tensorflow-gpu installed etc.).
> 
> So the command line in the TL;DR above, which sets persistence mode in the GPUs, was the only fix for me, since it will cause the GPU to be much more stable, and thus bypassing the ""spikes on connection"" that makes it so busy it cannot connect.
> 
> Hope it helps!

Thanks! That works for me
",issue almost hour ago sadly none worked problem fix found well explanation hopefully help people situation blush really root cause case since undoing fix server fallback usage server time stuff kernel driver among many two persistent roughly speaking come mode try remain idle long possible save battery fire loading driver application take many hand persistent mode trigger keep driver application loaded ready expense greater battery usage also strangely high volatile consistently mode even program querying hence causing way event caught attention led solution funny observer effect physic observation phenomenon digress reference discussion thought server check availability allocation idle driver persist trigger driver application wake spike like crazy unavailable demanding minimum level availability indeed line code maximum load well maximum memory usage event temporary unavailability return empty list tested even else good driver command line persistence mode fix since cause much stable thus connection busy connect hope thanks work,issue,positive,positive,positive,positive,positive,positive
1141174498,"@numb3r3 thanks for prompt response.

definitely, will keep checking the progress on tensorRT.

Thanks!!

",thanks prompt response definitely keep progress thanks,issue,positive,positive,positive,positive,positive,positive
1140912082,"@Jitendradubey Ok, the performance of `tensorrt` is very sensitive to the hardware. But the major reason why tensorrt is slower than pytorch is that tensorrt inference is actually using fp32 (but pytorch is using fp16). In our experiment on the `NVIDIA GeForce RTX 3090`, tensorrt fp32 is much faster than pytorch fp16. Anyway, the fp16 inference on TensorRT will be supported later. The blocker here is fp16 tensorrt inference produces inconsistent output with fp32. We are still struggling with this issue. Please keep an eye on our progress. ",performance sensitive hardware major reason inference actually experiment much faster anyway inference later blocker inference inconsistent output still struggling issue please keep eye progress,issue,negative,positive,neutral,neutral,positive,positive
1140854247,"Thanks @numb3r3 !!

Able to run Onnxruntime-gpu on GPU machine.

Reinstalled the onnx,onnxruntime, onnxruntime-gpu then installed only onnxruntime-gpu
it is started working.

now, we can close this issue, thanks again!!

",thanks able run machine working close issue thanks,issue,positive,positive,positive,positive,positive,positive
1140628357,"@Jitendradubey appreciate your time. From the logging, we can see that the installation of onnxruntie-gpu failed again. According to https://github.com/microsoft/onnxruntime/issues/7748#issuecomment-844358537, we can try the following steps: 
>  I uninstalled onnx as well this time and just installed onnxruntime-gpu from sctrach and that made it work, 
",appreciate time logging see installation according try following uninstalled well time made work,issue,positive,neutral,neutral,neutral,neutral,neutral
1140303113,"> @houshangwu感谢您指出。某些层/运算符会导致 FP16 上的输出不一致。为了解决这个问题，混合精度方法可以工作。请密切关注我们的进展，我们正在努力支持混合精度推理。

Thank you for your reply. I will continue to follow your progress.",thank reply continue follow progress,issue,positive,neutral,neutral,neutral,neutral,neutral
1140242853,"@numb3r3 thanks!!
using pytorch cuda inference via `python -m clip_server torch-flow.yml?` 
Yes using the same, check the below image
![model1](https://user-images.githubusercontent.com/7260090/170823527-a7429b91-b996-48b7-92e7-b868c54b8ea2.PNG)

for this question - Just want to clarify what's the normal CUDA inference in your test? 
took 214 second of normal CUDA inference timings, check in below too
![model2](https://user-images.githubusercontent.com/7260090/170823552-f402cb00-4d23-4804-93d4-9d00f1553952.PNG)
# code which is i'm refering 
![code](https://user-images.githubusercontent.com/7260090/170823595-d9d21bff-a4bd-48e8-858a-12dc1823a3a4.PNG)

If you compare with ONNX CUDA inference it took around 222 second for 1000 iteration image results, but here only 214 seconds time took to get the results.

Hope this is useful to you.

Thanks
",thanks inference via python yes check image model question want clarify normal inference test took second normal inference check model code code compare inference took around second iteration image time took get hope useful thanks,issue,positive,positive,positive,positive,positive,positive
1140239161,"@numb3r3 Thank you for quick response.

Now, I have changed the versioning of 'onnxruntime-gpu'  from 1.11 to 1.10,  but again not able to use GPU, check the below installed packages  
![again_error](https://user-images.githubusercontent.com/7260090/170822509-95f695a9-b9b8-47b0-bd56-16f41b595111.PNG)

And running again and again , still showing CPU utilizations, check the below image.
![Model resullt](https://user-images.githubusercontent.com/7260090/170822641-facda19f-9853-44d6-a80a-901ab5e63c37.PNG)

Please let me know if any other specific things were missing,

Thanks in Advance.


",thank quick response able use check running still showing check image model please let know specific missing thanks advance,issue,positive,positive,positive,positive,positive,positive
1140225244,"Based on the logging, I guess this is due to that the latest `onnxruntime-gpu` is not compatible with cuda 11.4.  Can you try to re-install `onnxruntime-gpu` with correct version by following https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements",based logging guess due latest compatible try correct version following,issue,negative,positive,positive,positive,positive,positive
1140224016,"@Jitendradubey 
> Using above code to get the inference result for 1000 iteration, around 222 second of time, which is very slow comparative normal CUDA inferences using CLIP model.

Just want to clarify what's the *normal CUDA inference* in your test? using pytorch cuda inference via `python -m clip_server torch-flow.yml`? 

BTW, could you please provide the full logging? ",code get inference result iteration around second time slow comparative normal clip model want clarify normal inference test inference via python could please provide full logging,issue,negative,positive,neutral,neutral,positive,positive
1138097350,"hlw, I run bertclient() at google colab !!! but it also takes infinite time.",run also infinite time,issue,negative,neutral,neutral,neutral,neutral,neutral
1135761848,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/729?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#729](https://codecov.io/gh/jina-ai/clip-as-service/pull/729?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (a384c44) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/0054b47cf12043b0bf493424ec22defa9448a9be?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (0054b47) will **not change** coverage.
> The diff coverage is `100.00%`.

```diff
@@           Coverage Diff           @@
##             main     #729   +/-   ##
=======================================
  Coverage   80.56%   80.56%           
=======================================
  Files          16       16           
  Lines        1137     1137           
=======================================
  Hits          916      916           
  Misses        221      221           
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `80.56% <100.00%> (ø)` | |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/729?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [client/clip\_client/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/729/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/729/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |

------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/729?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/729?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [7323d99...a384c44](https://codecov.io/gh/jina-ai/clip-as-service/pull/729?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report main change coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage continue review full report legend click learn absolute relative impact affected missing data powered last update read comment,issue,negative,positive,neutral,neutral,positive,positive
1135403044,"Actually, the `DA_NO_RICH_HANDLER` is introduced in the dependency of package`docarray` and not documented yet. TBH, it is much trickier to involve this env variable  `DA_NO_RICH_HANDLER`.  Hope we will have an elegant way to control the traceback handling later. 

Anyway, thanks for your reporting, and hope this issue can help others who have the same concerns as you. ",actually dependency package yet much involve variable hope elegant way control handling later anyway thanks hope issue help,issue,positive,positive,positive,positive,positive,positive
1134617527,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/726?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#726](https://codecov.io/gh/jina-ai/clip-as-service/pull/726?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (ca50323) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/42ef75b185369c1ffd64f8ea5a7b0fca8c8656b2?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (42ef75b) will **increase** coverage by `5.91%`.
> The diff coverage is `73.52%`.

```diff
@@            Coverage Diff             @@
##             main     #726      +/-   ##
==========================================
+ Coverage   61.10%   67.02%   +5.91%     
==========================================
  Files          16       16              
  Lines        1103     1119      +16     
==========================================
+ Hits          674      750      +76     
+ Misses        429      369      -60     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `67.02% <73.52%> (+5.91%)` | :arrow_up: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/726?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/executors/clip\_tensorrt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/726/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RlbnNvcnJ0LnB5) | `0.00% <0.00%> (ø)` | |
| [server/clip\_server/executors/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/726/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX29ubngucHk=) | `84.74% <100.00%> (+5.11%)` | :arrow_up: |
| [server/clip\_server/executors/clip\_torch.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/726/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RvcmNoLnB5) | `86.27% <100.00%> (+5.83%)` | :arrow_up: |
| [server/clip\_server/executors/helper.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/726/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9oZWxwZXIucHk=) | `98.30% <100.00%> (+37.96%)` | :arrow_up: |
| [server/clip\_server/model/simple\_tokenizer.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/726/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3NpbXBsZV90b2tlbml6ZXIucHk=) | `94.89% <0.00%> (+1.02%)` | :arrow_up: |
| [server/clip\_server/model/clip.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/726/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXAucHk=) | `86.42% <0.00%> (+3.57%)` | :arrow_up: |
| [client/clip\_client/client.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/726/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L2NsaWVudC5weQ==) | `86.81% <0.00%> (+18.13%)` | :arrow_up: |

------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/726?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/726?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [42ef75b...ca50323](https://codecov.io/gh/jina-ai/clip-as-service/pull/726?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report ca main increase coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage continue review full report legend click learn absolute relative impact affected missing data powered last update ca read comment,issue,negative,positive,neutral,neutral,positive,positive
1130311555,you can set environment variable `DA_NO_RICH_HANDLER` before the process starts to disable rich handler,set environment variable process disable rich handler,issue,positive,positive,positive,positive,positive,positive
1129641534,"> please format your code via:
> 
> ```
> $ pre-commit run --all
> ```

Solved in https://github.com/jina-ai/clip-as-service/pull/697/commits/932cfe6de4bc1b9c836e2ceeefec593879a3a556",please format code via run,issue,negative,neutral,neutral,neutral,neutral,neutral
1129535419,"@houshangwu Thanks for pointing it out. Some layers/operators result in inconsistent output on FP16. To fix this problem, the mix-precision approach could work. Please keep an eye on our progress, we are working on mix-precision inference support. ",thanks pointing result inconsistent output fix problem approach could work please keep eye progress working inference support,issue,positive,positive,positive,positive,positive,positive
1128564980,"@morgoth95 please remember to install pre-commit to format your code before commits:
```
$ pre-commit install
$ git commit -am ""fix: xxxx""
```",please remember install format code install git commit fix,issue,positive,neutral,neutral,neutral,neutral,neutral
1127157722,@morgoth95 Please rebase this PR so that it does not show unwanted commits,please rebase show unwanted,issue,negative,neutral,neutral,neutral,neutral,neutral
1126048738,"@numb3r3 

> FYI, this PR has one ore more **bad commit messages**,
> 
> ```
> 
> ⧗   input: docs: Fix typo in docs
> ✖   subject must not be sentence-case, start-case, pascal-case, upper-case [subject-case]
> ```
 it should have been fixed now! Let me know if further changes are needed
",one ore bad commit input fix typo multiply subject must fixed let know,issue,negative,negative,negative,negative,negative,negative
1125832895,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/722?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#722](https://codecov.io/gh/jina-ai/clip-as-service/pull/722?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (4ac32fc) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/3804367632cccdc1f7fa1f5fc998f3530e1ee05c?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (3804367) will **decrease** coverage by `13.70%`.
> The diff coverage is `100.00%`.

```diff
@@             Coverage Diff             @@
##             main     #722       +/-   ##
===========================================
- Coverage   80.67%   66.96%   -13.71%     
===========================================
  Files          16       16               
  Lines        1102     1102               
===========================================
- Hits          889      738      -151     
- Misses        213      364      +151     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `66.96% <100.00%> (-13.71%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/722?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [client/clip\_client/client.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/722/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L2NsaWVudC5weQ==) | `86.81% <100.00%> (ø)` | |
| [server/clip\_server/executors/clip\_tensorrt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/722/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RlbnNvcnJ0LnB5) | `0.00% <0.00%> (-100.00%)` | :arrow_down: |
| [server/clip\_server/model/clip\_trt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/722/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfdHJ0LnB5) | `0.00% <0.00%> (-92.31%)` | :arrow_down: |
| [server/clip\_server/model/trt\_utils.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/722/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3RydF91dGlscy5weQ==) | `0.00% <0.00%> (-83.52%)` | :arrow_down: |

------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/722?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/722?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [0da311e...4ac32fc](https://codecov.io/gh/jina-ai/clip-as-service/pull/722?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report main decrease coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage continue review full report legend click learn absolute relative impact affected missing data powered last update dae read comment,issue,negative,positive,neutral,neutral,positive,positive
1124465706,"FYI, this PR has one ore more **bad commit messages**, 

```

⧗   input: docs: Fix typo in docs
✖   subject must not be sentence-case, start-case, pascal-case, upper-case [subject-case]
``` ",one ore bad commit input fix typo multiply subject must,issue,negative,negative,negative,negative,negative,negative
1123405021,"@morgoth95 Congrats. `nebullvm v0.3.0` comes yesterday, very cool.  And there are still some bad commit messages in this PR, to fix them please refer to https://github.com/jina-ai/jina/issues/553",come yesterday cool still bad commit fix please refer,issue,negative,negative,negative,negative,negative,negative
1123207534,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/712?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#712](https://codecov.io/gh/jina-ai/clip-as-service/pull/712?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (a00661d) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/bb520d14b6c5172fce9a971b51c4125b60418119?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (bb520d1) will **not change** coverage.
> The diff coverage is `100.00%`.

```diff
@@           Coverage Diff           @@
##             main     #712   +/-   ##
=======================================
  Coverage   81.24%   81.24%           
=======================================
  Files          16       16           
  Lines        1189     1189           
=======================================
  Hits          966      966           
  Misses        223      223           
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `81.24% <100.00%> (ø)` | |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/712?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/executors/clip\_tensorrt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/712/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RlbnNvcnJ0LnB5) | `95.89% <ø> (ø)` | |
| [client/clip\_client/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/712/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/712/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |

------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/712?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/712?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [72d69c7...a00661d](https://codecov.io/gh/jina-ai/clip-as-service/pull/712?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report ad main change coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage continue review full report legend click learn absolute relative impact affected missing data powered last update ad read comment,issue,negative,positive,neutral,neutral,positive,positive
1122177494,"> @morgoth95 how is going now? Just wondering do we have the chance to merge this PR this week?

Sorry for the delay,  we were working on the latest release of nebullvm! I should have fixed the issues you pointed out! I think we can try to accelerate the process and merge the PR this week.",going wondering chance merge week sorry delay working latest release fixed pointed think try accelerate process merge week,issue,negative,positive,neutral,neutral,positive,positive
1120702805,@morgoth95  how is going now? Just wondering do we have the chance to merge this PR this week? ,going wondering chance merge week,issue,negative,neutral,neutral,neutral,neutral,neutral
1118104548,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/708?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#708](https://codecov.io/gh/jina-ai/clip-as-service/pull/708?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (5bdfb2d) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/f66b145be9a19b64e6404665fce8ff5c14b9552b?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (f66b145) will **increase** coverage by `0.20%`.
> The diff coverage is `100.00%`.

```diff
@@            Coverage Diff             @@
##             main     #708      +/-   ##
==========================================
+ Coverage   81.03%   81.24%   +0.20%     
==========================================
  Files          16       16              
  Lines        1176     1189      +13     
==========================================
+ Hits          953      966      +13     
  Misses        223      223              
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `81.24% <100.00%> (+0.20%)` | :arrow_up: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/708?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [client/clip\_client/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/708/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/708/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/executors/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/708/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX29ubngucHk=) | `86.51% <100.00%> (-0.30%)` | :arrow_down: |
| [server/clip\_server/executors/clip\_torch.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/708/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RvcmNoLnB5) | `88.09% <100.00%> (+1.08%)` | :arrow_up: |
| [server/clip\_server/executors/clip\_trt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/708/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RydC5weQ==) | `95.89% <100.00%> (+0.11%)` | :arrow_up: |
| [server/clip\_server/executors/helper.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/708/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9oZWxwZXIucHk=) | `97.29% <100.00%> (+0.52%)` | :arrow_up: |

------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/708?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/708?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [706fa62...5bdfb2d](https://codecov.io/gh/jina-ai/clip-as-service/pull/708?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report main increase coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage continue review full report legend click learn absolute relative impact affected missing data powered last update fa read comment,issue,negative,positive,neutral,neutral,positive,positive
1116003379,"> Summary of the sync between @numb3r3 and me:
> 
> We think about three possible interfaces for exposing the metrics from an Executor:
> 
>     * Accessing the raw prometheus client.
>       
>       * Con: user need to define the summary by themself.
>       * Pro: they can leverage the full power of the prometheus client. This solution will stay any way because the two others will rely on it
> 
>     * Having a decorator to monitor private method
> 
>     * Having a pytorch-like interface to do the monitoring: ex: `self.summary.log('text_docs', len(da), ...)`
>       
>       * Pro: easier to use as we just need to add one line of code
>       * Con: we have to teach a new monitoring interface to the user
> 
> 
> I will create a draft feature for each of the proposition so that we can try it out with @numb3r3 and decide what we want to ship at the end

- [x] raw prometheus (this pr)
- [x] decorator ( https://github.com/jina-ai/clip-as-service/pull/704)
- [x] log (https://github.com/jina-ai/clip-as-service/pull/705)

@numb3r3 @JoanFM ",summary sync think three possible metric executor raw client con user need define summary pro leverage full power client solution stay way two rely decorator monitor private method interface ex da pro easier use need add one line code con teach new interface user create draft feature proposition try decide want ship end raw decorator log,issue,positive,positive,neutral,neutral,positive,positive
1114366553,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/702?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#702](https://codecov.io/gh/jina-ai/clip-as-service/pull/702?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (3a21d49) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/835eb13fcb84d126b6a35e18b2fe0ef9d8b835b7?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (835eb13) will **decrease** coverage by `15.30%`.
> The diff coverage is `100.00%`.

```diff
@@             Coverage Diff             @@
##             main     #702       +/-   ##
===========================================
- Coverage   81.24%   65.93%   -15.31%     
===========================================
  Files          16       16               
  Lines        1189     1189               
===========================================
- Hits          966      784      -182     
- Misses        223      405      +182     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `65.93% <100.00%> (-15.31%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/702?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [client/clip\_client/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/702/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/702/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/executors/clip\_trt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/702/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RydC5weQ==) | `0.00% <0.00%> (-95.90%)` | :arrow_down: |
| [server/clip\_server/model/clip\_trt.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/702/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfdHJ0LnB5) | `0.00% <0.00%> (-92.31%)` | :arrow_down: |
| [server/clip\_server/model/trt\_utils.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/702/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL3RydF91dGlscy5weQ==) | `0.00% <0.00%> (-83.52%)` | :arrow_down: |

------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/702?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/702?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [099e221...3a21d49](https://codecov.io/gh/jina-ai/clip-as-service/pull/702?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report ad main decrease coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage continue review full report legend click learn absolute relative impact affected missing data powered last update ad read comment,issue,negative,positive,neutral,neutral,positive,positive
1114035816,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/700?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#700](https://codecov.io/gh/jina-ai/clip-as-service/pull/700?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (aa81230) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/8ac2e9bb68b96d1421f7e2ae6b01cec95aad3183?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (8ac2e9b) will **not change** coverage.
> The diff coverage is `75.00%`.

```diff
@@           Coverage Diff           @@
##             main     #700   +/-   ##
=======================================
  Coverage   77.94%   77.94%           
=======================================
  Files          12       12           
  Lines         934      934           
=======================================
  Hits          728      728           
  Misses        206      206           
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `77.94% <75.00%> (ø)` | |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/700?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/helper.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/700/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2hlbHBlci5weQ==) | `42.85% <33.33%> (ø)` | |
| [client/clip\_client/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/700/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [client/clip\_client/helper.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/700/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L2hlbHBlci5weQ==) | `84.61% <100.00%> (ø)` | |
| [server/clip\_server/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/700/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |

------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/700?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/700?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [d2c2c87...aa81230](https://codecov.io/gh/jina-ai/clip-as-service/pull/700?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report aa main change coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage continue review full report legend click learn absolute relative impact affected missing data powered last update aa read comment,issue,negative,positive,neutral,neutral,positive,positive
1113938045,"@MrBeeMovie for now, please consider use `.encode(iterable)` instead of `iter(.encode(...))` as I mentioned in the comment above",please consider use iterable instead iter comment,issue,negative,neutral,neutral,neutral,neutral,neutral
1111826501,"> I solved it by downloading `tensorflow-gpu==1.15.0`.

Hi can I ask how the others versions chosen? I set python=3.6.13, tensorflow-gpu=1.15.0, bert-serving-client/server=1.10.0, but still got [TypeError: 'NoneType' object is not iterable] error (tensorflow-gpu works well btw). Thanks!!",hi ask chosen set still got object iterable error work well thanks,issue,negative,positive,positive,positive,positive,positive
1111250552,"> hi @morgoth95 is it possible to open PR modification permission to us? we could take care of it on the styling and minors things. Reference here: https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/allowing-changes-to-a-pull-request-branch-created-from-a-fork

Hello @hanxiao! Actually, I already activated the option ""Allow edits by maintainers"" when I created the PR 😄 In theory all of you with writing access to clip-as-a-service should be able to directly push new commits to my branch 😉 ",hi possible open modification permission u could take care styling reference hello actually already option allow theory writing access able directly push new branch,issue,positive,positive,positive,positive,positive,positive
1111234153,hi @morgoth95 is it possible to open PR modification permission to us? we could take care of it on the styling and minors things. Reference here: https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/allowing-changes-to-a-pull-request-branch-created-from-a-fork,hi possible open modification permission u could take care styling reference,issue,negative,neutral,neutral,neutral,neutral,neutral
1110927180,"BTW., please also use `pre-commit` to format your codes
```
$ pre-commit install
$ git commit -am ""feat: nebullm backend""
```",please also use format install git commit feat,issue,positive,neutral,neutral,neutral,neutral,neutral
1110926080,awsome job. The current PR cannot pass CI checking. Please follow this guidelines https://github.com/jina-ai/jina/blob/master/CONTRIBUTING.md ,job current pas please follow,issue,negative,neutral,neutral,neutral,neutral,neutral
1108649359,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/695?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#695](https://codecov.io/gh/jina-ai/clip-as-service/pull/695?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (6975f5e) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/0ebc4c0363aa182d2992bbc11bbcf676aeaf69df?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (0ebc4c0) will **decrease** coverage by `23.60%`.
> The diff coverage is `82.75%`.

```diff
@@             Coverage Diff             @@
##             main     #695       +/-   ##
===========================================
- Coverage   80.84%   57.24%   -23.61%     
===========================================
  Files          11       11               
  Lines         830      870       +40     
===========================================
- Hits          671      498      -173     
- Misses        159      372      +213     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `57.24% <82.75%> (-23.61%)` | :arrow_down: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/695?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/executors/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/695/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX29ubngucHk=) | `0.00% <0.00%> (-88.74%)` | :arrow_down: |
| [client/clip\_client/client.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/695/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L2NsaWVudC5weQ==) | `49.71% <86.48%> (-36.32%)` | :arrow_down: |
| [server/clip\_server/executors/clip\_torch.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/695/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RvcmNoLnB5) | `70.65% <87.50%> (-20.26%)` | :arrow_down: |
| [client/clip\_client/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/695/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/695/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/model/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/695/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXBfb25ueC5weQ==) | `0.00% <0.00%> (-96.00%)` | :arrow_down: |
| [server/clip\_server/model/clip.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/695/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXAucHk=) | `57.55% <0.00%> (-28.78%)` | :arrow_down: |

------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/695?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/695?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [5e1dd60...6975f5e](https://codecov.io/gh/jina-ai/clip-as-service/pull/695?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report fe main decrease coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage continue review full report legend click learn absolute relative impact affected missing data powered last update fe read comment,issue,negative,positive,neutral,neutral,positive,positive
1107817631,"> tbh, the jina will not support `python < 3.7`. `python 2.7` supporting is not in our plan.

Thanks for your reply!",jina support python python supporting plan thanks reply,issue,positive,positive,positive,positive,positive,positive
1107808540,"tbh, the jina will not support `python < 3.7`.  `python 2.7` supporting is not in our plan. ",jina support python python supporting plan,issue,positive,positive,positive,positive,positive,positive
1107808282,"Of course, the easiest solution is to upgrade your working environment. But I guess it is not donable for some special reasons. Is it possible to use docker container? ",course easiest solution upgrade working environment guess donable special possible use docker container,issue,positive,positive,positive,positive,positive,positive
1107806263,"> 

Many problems. My work is based on:

python 2.7
tensorflow 1.15 or 1.13
onnx < 1.7.0 
(if onnx >1.7.0, an error will be ""Failed to build onnx
ERROR: Could not build wheels for onnx which use PEP 517 and cannot be installed directly"" )

And when I extract the .onnx file of the clip text encoder, I have to use:

pytorch > 1.7.1
(clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn(""PyTorch version 1.7.1 or higher is recommended"")
/218019043/software/anaconda3/envs/dassl/lib/python3.7/site-packages/torch/onnx/symbolic_opset9.py:2378: UserWarning: Exporting aten::index operator of advanced indexing in opset 9 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.
  ""If indices include negative values, the exported graph will produce incorrect results.""))
===============================================================

When the .onnx file (gotten by pytorch > 1.7.1) is used in ""python 2.7 && tensorflow 1.13 && onnx <1.7.0"", an error will be ""*** ValidationError: Your model ir_version is higher than the checker's.""

",many work based python error build error could build use pep directly extract file clip text use version higher version higher operator advanced indexing combination multiple reshape transpose gather index include negative graph produce incorrect index include negative graph produce incorrect file gotten used python error model higher checker,issue,negative,positive,positive,positive,positive,positive
1107806042,"> 

thank u for the response， I just found that no public address doesnt affect my task. only local access is enough.",thank found public address doesnt affect task local access enough,issue,negative,neutral,neutral,neutral,neutral,neutral
1107805956,"> Public IP lookup depends on third-party service, which may timeout due to your network condition. This, however, should not affect how you use the service. If you need the public IP to use the service, you will have to look up by yourself, e.g. google/baidu ""what is my IP address""

thank u for the response， I just found that no public address doesnt affect my task. only local access is enough.",public service may due network condition however affect use service need public use service look address thank found public address doesnt affect task local access enough,issue,negative,negative,neutral,neutral,negative,negative
1107804826,"> @LioHaunt maybe my response is too late. The reason behind is that the model downloading procedure is interrupted the first time. Then, the downloading cannot be resumed and the local model file cannot be loaded.
> 
> Anyway, this problem has been addressed and released in `v0.2.2`.

yes，I do interrupted the model downloading procedure>_<, and i deleted all the file related then the problem solved. anyway, thank you for your response^_^.

",maybe response late reason behind model procedure interrupted first time local model file loaded anyway problem interrupted model procedure file related problem anyway thank,issue,negative,negative,neutral,neutral,negative,negative
1107792715,"some more detailed explanation about this problem. https://github.com/jina-ai/clip-as-service/issues/692#issuecomment-1107791827 
> Public IP lookup depends on third-party service, which may timeout due to your network condition. This, however, should not affect how you use the service. If you need the public IP to use the service, you will have to look up by yourself, e.g. google/baidu ""what is my IP address""",detailed explanation problem public service may due network condition however affect use service need public use service look address,issue,negative,positive,neutral,neutral,positive,positive
1107792621,"Because my work is based on tensorflow 1.13 platform, and I want to add the clipt text part with current ckpt to compute the loss. I have tried the onnx, but failed.",work based platform want add clipt text part current compute loss tried,issue,negative,neutral,neutral,neutral,neutral,neutral
1107791980,"i think @username123062 was talking about the old bert-as-service, please consider use the actively maintained CLIP-as-service",think talking old please consider use actively,issue,positive,negative,neutral,neutral,negative,negative
1107791827,"Public IP lookup depends on third-party service, which may timeout due to your network condition. This, however, should not affect how you use the service. If you need the public IP to use the service, you will have to look up by yourself, e.g. google/baidu ""what is my IP address""",public service may due network condition however affect use service need public use service look address,issue,negative,negative,neutral,neutral,negative,negative
1107785103,"That's a good question. My concern is are there any benefits to supporting tensorflow? For inference efficiency, we can use `onnx` and `tensorrt` (`tensorrt` backend is on the road) on CPU and GPU respectively. What's more, almost all of the pre-trained models are pytorch. ",good question concern supporting inference efficiency use road respectively almost,issue,positive,positive,positive,positive,positive,positive
1107780386,"@LioHaunt  maybe my response is too late. The reason behind is that the model downloading procedure is interrupted the first time. Then, the downloading cannot be resumed and the local model file cannot be loaded. 

Anyway, this problem has been addressed and released in `v0.2.2`. ",maybe response late reason behind model procedure interrupted first time local model file loaded anyway problem,issue,negative,negative,negative,negative,negative,negative
1107778549," windows supporting problem seems to have been addressed in the latest version (`jina>=3.3.4`, `clip-server>=0.2.1`). I will close this issue. ",supporting problem latest version jina close issue,issue,negative,positive,positive,positive,positive,positive
1107777809,"well, this is maybe caused by the network problem. BTW, does the started service can handle requests? ",well maybe network problem service handle,issue,negative,neutral,neutral,neutral,neutral,neutral
1105485788,"> @ShangwenWang @JosephFATucker windows supported in latest `jina` and `clip-as-service`:
> 
> ```
> pip install --upgrade jina>=3.3.4 clip-server>=0.2.1
> ```

Thanks, the update works and I was able to run 
```
python -m clip_server
```
with no errors.
",latest jina pip install upgrade jina thanks update work able run python,issue,negative,positive,positive,positive,positive,positive
1105403361,"@ShangwenWang @JosephFATucker windows supported in latest `jina` and `clip-as-service`:
```
pip install --upgrade jina>=3.3.4 clip-server>=0.2.1
```",latest jina pip install upgrade jina,issue,negative,positive,positive,positive,positive,positive
1104642717,I'm looking into it. this is related `spawn` context. ,looking related spawn context,issue,negative,neutral,neutral,neutral,neutral,neutral
1104060546,"@numb3r3 could you take a look with the core team, I'm afraid that `complete_path` function from Jina core is not really Windows-friendly, i.e. it doesn't work.",could take look core team afraid function jina core really work,issue,negative,negative,negative,negative,negative,negative
1104055840,"Thanks for trying. Hint. Avoid `for` loop. Instead, prepare the data in advance:

```python
import time

from clip_client import Client

c = Client('grpc://10.190.153.14:51000')

num = 1_000

da = ['COCO_val2014_000000558911.jpg' for _ in range(num)]

start = time.perf_counter()
r = c.encode(da, show_progress=True)
end = time.perf_counter()

print(f'cost:{(end - start) / num}')

```

Try it, should be much faster.",thanks trying hint avoid loop instead prepare data advance python import time import client client da range start da end print end start try much faster,issue,negative,positive,positive,positive,positive,positive
1103551499,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/685?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#685](https://codecov.io/gh/jina-ai/clip-as-service/pull/685?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (92db8bb) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/65ad956dab7e4f011568ac14357d636312dd4f5e?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (65ad956) will **increase** coverage by `0.14%`.
> The diff coverage is `100.00%`.

```diff
@@            Coverage Diff             @@
##             main     #685      +/-   ##
==========================================
+ Coverage   78.74%   78.88%   +0.14%     
==========================================
  Files          11       11              
  Lines         795      791       -4     
==========================================
- Hits          626      624       -2     
+ Misses        169      167       -2     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `78.88% <100.00%> (+0.14%)` | :arrow_up: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/685?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/executors/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/685/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX29ubngucHk=) | `88.57% <100.00%> (+1.07%)` | :arrow_up: |
| [server/clip\_server/executors/clip\_torch.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/685/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RvcmNoLnB5) | `88.33% <100.00%> (+1.23%)` | :arrow_up: |

------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/685?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/685?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [65ad956...92db8bb](https://codecov.io/gh/jina-ai/clip-as-service/pull/685?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report main ad increase coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage continue review full report legend click learn absolute relative impact affected missing data powered last update ad read comment,issue,negative,positive,neutral,neutral,positive,positive
1102706725,"> ah, maybe this is the bug on Windows we did not observe before. Thanks for your report, we will look into it. BTW., what's the clip_server version are you using?
> 
> ```shell
> pip show clip_server
> ```
I'm experiencing this same issue.
My ""pip show clip_server"" displays this:

```shell
Name: clip-server
Version: 0.2.0
Summary: Embed images and sentences into fixed-length vectors via CLIP
Home-page: https://github.com/jina-ai/clip-as-service
Author: Jina AI
Author-email: hello@jina.ai
License: Apache 2.0
Location: c:\users\MyUserNameGoesHere\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\local-packages\python310\site-packages
Requires: docarray, ftfy, jina, regex, torch, torchvision
Required-by:
```",ah maybe bug observe thanks report look version shell pip show issue pip show shell name version summary embed via clip author jina ai hello license apache location jina torch,issue,negative,positive,positive,positive,positive,positive
1101167227,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/684?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#684](https://codecov.io/gh/jina-ai/clip-as-service/pull/684?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (1a65e08) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/10d53eb4c4b84c95412b5f904de705cc01d7ba89?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (10d53eb) will **not change** coverage.
> The diff coverage is `100.00%`.

```diff
@@           Coverage Diff           @@
##             main     #684   +/-   ##
=======================================
  Coverage   74.03%   74.03%           
=======================================
  Files          11       11           
  Lines         778      778           
=======================================
  Hits          576      576           
  Misses        202      202           
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `74.03% <100.00%> (ø)` | |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/684?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [client/clip\_client/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/684/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/684/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |

------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/684?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/684?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [11487ce...1a65e08](https://codecov.io/gh/jina-ai/clip-as-service/pull/684?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report ae main deb change coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage continue review full report legend click learn absolute relative impact affected missing data powered last update ce ae read comment,issue,negative,positive,neutral,neutral,positive,positive
1101080704,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/683?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#683](https://codecov.io/gh/jina-ai/clip-as-service/pull/683?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (f6aa02d) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/10d53eb4c4b84c95412b5f904de705cc01d7ba89?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (10d53eb) will **increase** coverage by `4.70%`.
> The diff coverage is `80.00%`.

```diff
@@            Coverage Diff             @@
##             main     #683      +/-   ##
==========================================
+ Coverage   74.03%   78.74%   +4.70%     
==========================================
  Files          11       11              
  Lines         778      795      +17     
==========================================
+ Hits          576      626      +50     
+ Misses        202      169      -33     
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `78.74% <80.00%> (+4.70%)` | :arrow_up: |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/683?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [server/clip\_server/helper.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/683/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2hlbHBlci5weQ==) | `100.00% <ø> (+20.00%)` | :arrow_up: |
| [server/clip\_server/executors/clip\_torch.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/683/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX3RvcmNoLnB5) | `87.09% <72.72%> (-3.65%)` | :arrow_down: |
| [server/clip\_server/executors/clip\_onnx.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/683/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL2V4ZWN1dG9ycy9jbGlwX29ubngucHk=) | `87.50% <82.35%> (-2.67%)` | :arrow_down: |
| [client/clip\_client/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/683/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/683/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |
| [server/clip\_server/model/clip.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/683/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-c2VydmVyL2NsaXBfc2VydmVyL21vZGVsL2NsaXAucHk=) | `86.17% <0.00%> (+30.89%)` | :arrow_up: |

------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/683?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/683?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [ec3a700...f6aa02d](https://codecov.io/gh/jina-ai/clip-as-service/pull/683?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report main deb increase coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage continue review full report legend click learn absolute relative impact affected missing data powered last update read comment,issue,negative,positive,neutral,neutral,positive,positive
1099104795,"Summary of the sync between @numb3r3  and me:

We think about three possible interfaces for exposing the metrics from an Executor:

* Accessing the raw prometheus client.
  + Con: user need to define the summary by themself. 
  + Pro: they can leverage the full power of the prometheus client. This solution will stay any way because the two others will rely on it
* Having a decorator to monitor private method
* Having a pytorch-like interface to do the monitoring: ex: `self.summary.log('text_docs', len(da), ...)` 
   + Pro: easier to use as we just need to add one line of code
   + Con: we have to teach a new monitoring interface to the user

I will create a draft feature for each of the proposition so that we can try it out with @numb3r3 and decide what we want to ship at the end


",summary sync think three possible metric executor raw client con user need define summary pro leverage full power client solution stay way two rely decorator monitor private method interface ex da pro easier use need add one line code con teach new interface user create draft feature proposition try decide want ship end,issue,positive,positive,neutral,neutral,positive,positive
1098772726,perfect. hope you enjoy the service. Any feedbacks are welcome!,perfect hope enjoy service welcome,issue,positive,positive,positive,positive,positive,positive
1098755575,It's working. I forgot to restart my `clip-client`. Sorry for the trouble. Thanks for fixing it,working forgot restart sorry trouble thanks fixing,issue,negative,negative,negative,negative,negative,negative
1098717402,"Your script works on my side. I guess the upgrading failed. Could you try upgrade via
```
pip install --upgrade clip_server
```",script work side guess could try upgrade via pip install upgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
1098687083,"Yes I've upgraded it. Here is the code to reproduce it
```
from clip_client import Client
from glob import glob
from PIL import Image 
import numpy as np 
from docarray import DocumentArray, Document 

c = Client('grpc://0.0.0.0:51000')
imgs = glob(""path_to_images/*"")
pil_images = [Image.open(img) for img in imgs]
docs_with_tensor = [Document(tensor=np.array(img)) for img in pil_images[:100]]
da= DocumentArray(docs_with_tensor)
r = c.encode(da, batch_size=1, show_progress=True)
```",yes code reproduce import client import import image import import document client document da,issue,negative,neutral,neutral,neutral,neutral,neutral
1098679527,"have you upgraded to `v0.1.14`? If yes, may I get your script to help me to reproduce this error? 
```bash
pip show clip_server
```",yes may get script help reproduce error bash pip show,issue,negative,neutral,neutral,neutral,neutral,neutral
1098673084,"Thanks for your quick response. 

Actually, I now turn to use the old version ""bert_as_service"" and successfully opened the server.
It is sad that my laptop cannot open the clip_server now. But anyway, thanks for your help.",thanks quick response actually turn use old version successfully server sad open anyway thanks help,issue,positive,positive,positive,positive,positive,positive
1098669001,"ah, maybe this is the bug on Windows we did not observe before. Thanks for your report, we will look into it. BTW., what's the clip_server version are you using? 
```bash
pip show clip_server
```",ah maybe bug observe thanks report look version bash pip show,issue,negative,positive,positive,positive,positive,positive
1098653672,"@shubham-goel-zefr thanks for your report, fix is rolled out in `v0.1.14`, you can try it out. ",thanks report fix rolled try,issue,negative,positive,positive,positive,positive,positive
1098550336,"@hanxiao it looks like the tensor `document` doesn't work if the Document doesn't contain a valid `URI`. And it seems to be loading the `URI` into a blob, so I'm not sure if it's using the `tensor` or the `blob` to generate the final embedding. Any help is appreciated.",like tensor document work document contain valid loading blob sure tensor blob generate final help,issue,positive,positive,positive,positive,positive,positive
1094675375,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/679?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#679](https://codecov.io/gh/jina-ai/clip-as-service/pull/679?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (cbe6ba8) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/8b800eea5d40d02417a4368fcc38ac58dc2d651d?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (8b800ee) will **not change** coverage.
> The diff coverage is `n/a`.

```diff
@@           Coverage Diff           @@
##             main     #679   +/-   ##
=======================================
  Coverage   74.02%   74.02%           
=======================================
  Files          11       11           
  Lines         743      743           
=======================================
  Hits          550      550           
  Misses        193      193           
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `74.02% <ø> (ø)` | |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.


------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/679?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/679?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [8b800ee...cbe6ba8](https://codecov.io/gh/jina-ai/clip-as-service/pull/679?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report main bee change coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find continue review full report legend click learn absolute relative impact affected missing data powered last update bee read comment,issue,negative,positive,neutral,neutral,positive,positive
1094479706,"Good catch. According to the doc https://clip-as-service.jina.ai/user-guides/client/#input-as-iterable-of-documents, clip-as-service (CaS) now only accepts `.uri` and `.blob` as the image attribute. 
> each Document must be filled with `.text` or `.uri` or `.blob` attribute. Document filled with `.text` is considered as sentence, Document filled with `.uri` or `.blob` is considered as image.

However, it makes sense to allow `.tensor` as the image input. 
",good catch according doc image attribute document must filled attribute document filled considered sentence document filled considered image however sense allow image input,issue,positive,positive,positive,positive,positive,positive
1086057929,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/671?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#671](https://codecov.io/gh/jina-ai/clip-as-service/pull/671?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (538ac0d) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/065d6a910e44a31718463a51ba0e3c29ca926d1c?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (065d6a9) will **not change** coverage.
> The diff coverage is `100.00%`.

```diff
@@           Coverage Diff           @@
##             main     #671   +/-   ##
=======================================
  Coverage   87.40%   87.40%           
=======================================
  Files           2        2           
  Lines         135      135           
=======================================
  Hits          118      118           
  Misses         17       17           
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `87.40% <100.00%> (ø)` | |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/671?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [client/clip\_client/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/671/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |

------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/671?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/671?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [f0dfc34...538ac0d](https://codecov.io/gh/jina-ai/clip-as-service/pull/671?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report main da change coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage continue review full report legend click learn absolute relative impact affected missing data powered last update read comment,issue,negative,positive,neutral,neutral,positive,positive
1083737671,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/669?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#669](https://codecov.io/gh/jina-ai/clip-as-service/pull/669?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (749926c) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/065d6a910e44a31718463a51ba0e3c29ca926d1c?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (065d6a9) will **not change** coverage.
> The diff coverage is `100.00%`.

> :exclamation: Current head 749926c differs from pull request most recent head a25a32e. Consider uploading reports for the commit a25a32e to get more accurate results

```diff
@@           Coverage Diff           @@
##             main     #669   +/-   ##
=======================================
  Coverage   87.40%   87.40%           
=======================================
  Files           2        2           
  Lines         135      135           
=======================================
  Hits          118      118           
  Misses         17       17           
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `87.40% <100.00%> (ø)` | |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/669?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [client/clip\_client/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/669/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |

------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/669?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/669?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [da3227d...a25a32e](https://codecov.io/gh/jina-ai/clip-as-service/pull/669?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report main da change coverage coverage exclamation current head pull request recent head consider commit get accurate coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage continue review full report legend click learn absolute relative impact affected missing data powered last update dad read comment,issue,negative,positive,positive,positive,positive,positive
1081333923,"For this PR, could you provide more context? I just found it is the first time we use  `Scarf` in our projects. It is a toolchain for maintainers. Do we need to set up other components, e.g., some service, provider?",could provide context found first time use scarf need set service provider,issue,negative,positive,positive,positive,positive,positive
1080178466,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/664?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#664](https://codecov.io/gh/jina-ai/clip-as-service/pull/664?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (26d515e) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/ae4d0bacbfadb57e1cb1de8a5f6adca5426e62d3?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (ae4d0ba) will **not change** coverage.
> The diff coverage is `100.00%`.

```diff
@@           Coverage Diff           @@
##             main     #664   +/-   ##
=======================================
  Coverage   84.00%   84.00%           
=======================================
  Files           2        2           
  Lines         100      100           
=======================================
  Hits           84       84           
  Misses         16       16           
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `84.00% <100.00%> (ø)` | |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/664?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [client/clip\_client/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/664/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |

------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/664?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/664?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [da1dd85...26d515e](https://codecov.io/gh/jina-ai/clip-as-service/pull/664?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report de main change coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage continue review full report legend click learn absolute relative impact affected missing data powered last update de read comment,issue,negative,positive,neutral,neutral,positive,positive
1077873035,"great job on inspecting this.

@samsja btw if core can somehow enable the profiling for any executor that would be awesome! basically auto injecting the similar timing code that @alaeddine-13 wrote into Executor",great job core somehow enable executor would awesome basically auto similar timing code wrote executor,issue,positive,positive,positive,positive,positive,positive
1077412105,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/658?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> Merging [#658](https://codecov.io/gh/jina-ai/clip-as-service/pull/658?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (d7b170d) into [main](https://codecov.io/gh/jina-ai/clip-as-service/commit/dd4cb3c3e142a8b8f702c4eee57a251aab7a10d5?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) (dd4cb3c) will **not change** coverage.
> The diff coverage is `100.00%`.

```diff
@@           Coverage Diff           @@
##             main     #658   +/-   ##
=======================================
  Coverage   84.00%   84.00%           
=======================================
  Files           2        2           
  Lines         100      100           
=======================================
  Hits           84       84           
  Misses         16       16           
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `84.00% <100.00%> (ø)` | |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.

| [Impacted Files](https://codecov.io/gh/jina-ai/clip-as-service/pull/658?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) | Coverage Δ | |
|---|---|---|
| [client/clip\_client/\_\_init\_\_.py](https://codecov.io/gh/jina-ai/clip-as-service/pull/658/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#diff-Y2xpZW50L2NsaXBfY2xpZW50L19faW5pdF9fLnB5) | `100.00% <100.00%> (ø)` | |

------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/658?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/658?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [59b154a...d7b170d](https://codecov.io/gh/jina-ai/clip-as-service/pull/658?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report main change coverage coverage coverage main coverage flag coverage carried forward coverage wo shown click find impacted coverage continue review full report legend click learn absolute relative impact affected missing data powered last update ba read comment,issue,negative,positive,neutral,neutral,positive,positive
1077317640,"# [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/656?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai) Report
> :exclamation: No coverage uploaded for pull request base (`main@e8aa643`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#section-missing-base-commit).
> The diff coverage is `n/a`.

```diff
@@           Coverage Diff           @@
##             main     #656   +/-   ##
=======================================
  Coverage        ?   84.00%           
=======================================
  Files           ?        2           
  Lines           ?      100           
  Branches        ?        0           
=======================================
  Hits            ?       84           
  Misses          ?       16           
  Partials        ?        0           
```

| Flag | Coverage Δ | |
|---|---|---|
| cas | `84.00% <0.00%> (?)` | |

Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai#carryforward-flags-in-the-pull-request-comment) to find out more.


------

[Continue to review full report at Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/656?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/jina-ai/clip-as-service/pull/656?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Last update [e8aa643...ca20ba8](https://codecov.io/gh/jina-ai/clip-as-service/pull/656?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=jina-ai).
",report exclamation coverage pull request base main click learn coverage coverage main coverage flag coverage carried forward coverage wo shown click find continue review full report legend click learn absolute relative impact affected missing data powered last update caba read comment,issue,negative,negative,neutral,neutral,negative,negative
1072557182,"> 


你好？请问在怎样可以找到自己的端口号，并进行更改呢？

> @AshwatKumar 我也遇到了同样的错误。我通过更改端口的程序号解决了这个问题。 这就是我获得参与的工作。我只是按照@geoffreyp7 :)
> 
> 1. 安装一个模块
>    `!pip install -U bert-serving-server[http]`
> 2. 下载预训练模型并解压
>    `!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip`
>    `!unzip uncased_L-12_H-768_A-12.zip`
> 3. 设置端口号
>    `port_num='3399'`
> 4. 在 notebook 中启动服务器
> 
> ```
> get_ipython().system_raw(
>     'bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 -http_port ' + port_num + ' &'
> )
> ```
> 
> 5. 定义这个功能：
> 
> ```
> import json
> import requests
> def get_embeddings(texts):
>     headers = {
>         'content-type':'application/json'
>     }
>     data = {
>         ""id"":123,
>         ""texts"":texts,
>         ""is_tokenized"": False
>     }
>     data = json.dumps(data)
>     r = requests.post(""http://localhost:"" + port_num + ""/encode"", data=data, headers=headers).json()
>     return r['result']
> ```
> 
> 6. 获取嵌入
> 
> ```
> text_list1=['This is a test sentence.']
> text_embeddings1 = get_embeddings(text_list1)
> ```

Hello? How can I find my port number and change it?",pip install notebook import import data id false data data return test sentence hello find port number change,issue,negative,negative,negative,negative,negative,negative
1069229466,"> I tried it a lot and finally this worked. (Worked date: Dec 4, 2020)
> 
> ```python
> %tensorflow_version 1.x
> 
> !pip install bert-serving-client
> !pip install -U bert-serving-server[http]
> 
> !wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip
> !unzip uncased_L-12_H-768_A-12.zip
> !nohup bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 > out.file 2>&1 &
> 
> 
> !ls  # you should see uncased_something_.zip
> 
> 
> from bert_serving.client import BertClient
> bc = BertClient()
> 
> list_text = ['you need to use tensorflow 1', 'tf2 does not work']
> embedded_text = bc.encode(list_text)
> 
> print(embedded_text)
> 
> this gives
> [[-0.32885492 -0.39915255 -0.3393307  ... -0.27655432 -0.13769765
>    0.39554596]
>  [ 0.03333598 -0.6914621   0.0800274  ... -0.46210474 -0.06833883
>    0.30134943]]
> ```

Hello, I ran your code directly and turned on the GPU, but this step has been running. What else can I do to solve this problem?
<img width=""901"" alt=""image"" src=""https://user-images.githubusercontent.com/72616391/158621931-b4e42a89-f437-4f79-9369-27d7b2210e2a.png"">
",tried lot finally worked worked date python pip install pip install see import need use work print hello ran code directly turned step running else solve problem image,issue,negative,positive,neutral,neutral,positive,positive
1069079006,"> I was having the same problem. You need tensorflow 1
> 
> This is how I manage to run it:
> 
> ```
> $ conda create -n bert-as-service python=3.7
> $ conda activate bert-as-service
> $ pip install tensorflow==1.15.5
> $ pip install bert-serving-server
> ```
> 
> and then just run `bert-serving-start`

my tensorflow is 1.11.0 but i still can't solve that ",problem need manage run create activate pip install pip install run still ca solve,issue,negative,neutral,neutral,neutral,neutral,neutral
1068672063,"> 根据@bhishanpdl ，这对我有用
> 
> https://colab.research.google.com/drive/1gwm_sDq9ja5uq-d4b33EaI-HTVJVVAva?usp=sharing
> 
> 2021 年 4 月 26 日 I ran your notebook, but this step has been running all the time, and it has been a long time without results (I turned on the GPU). How long will you run this step?
<img width=""749"" alt=""image"" src=""https://user-images.githubusercontent.com/72616391/158505713-5039253e-2df5-4b40-be4b-044b3a4e57dc.png"">
",ran notebook step running time long time without turned long run step image,issue,negative,negative,neutral,neutral,negative,negative
1019535116,"> For those who are on a CPU: Follow these two steps if you're getting the same error.
> 
> 1. `pip uninstall` _**current_version_of_tf**_
> 2. `pip install tensorflow==1.13.0rc1`

pip install tensorflow==1.13.1 also ok",follow two getting error pip pip install pip install also,issue,negative,neutral,neutral,neutral,neutral,neutral
966775580,"bert的pre-trained的模型生成的原始word embedding/sentence embedding是不适合做cosine similarity的。具体参见以下链接：

https://github.com/UKPLab/sentence-transformers/issues/80

“Cosine-Similarity / Manhatten-Distance / Euclidean-Distance treat all dimension equally, with the same weight. I.e., to get well working sentence embeddings for unsupervised tasks, it is important that all dimensions ""make sense"" and have the ""same scale"".

BERT was not optimized for this, that all output-dimensions must contribute equally. For supervised tasks, the classifier learns which dimension contribute and what the ""scale"" of those are. But for unsupervised tasks, we sadly don't have this luxury.”",treat dimension equally weight get well working sentence unsupervised important make sense scale must contribute equally classifier dimension contribute scale unsupervised sadly luxury,issue,negative,negative,neutral,neutral,negative,negative
955185065,I would like to work on this if not taken up yet @hanxiao. Please help me out with where to get started from,would like work taken yet please help get,issue,positive,neutral,neutral,neutral,neutral,neutral
949494629,I also just came across this issue while running fine-tuned BERT. After every server start I get a different embedding for the same sentence. ,also came across issue running every server start get different sentence,issue,negative,neutral,neutral,neutral,neutral,neutral
947318102,"> > Create a start-bert-as-service.py with the following code
> > ```
> > import sys
> > 
> > from bert_serving.server import BertServer
> > from bert_serving.server.helper import get_run_args
> > 
> > 
> > if __name__ == '__main__':
> >     args = get_run_args()
> >     server = BertServer(args)
> >     server.start()
> >     server.join()
> > ```
> > 
> > 
> >     
> >       
> >     
> > 
> >       
> >     
> > 
> >     
> >   
> > so you can run with the following command
> > python start-bert-as-service.py -model_dir ./tmp/chinese_L-12_H-768_A-12/ -num_worker=1
> 
> @monk1337 Getting this Error using this solution on windows 10 with python 3.6 and tensorflow 2.0.0 . ![image](https://user-images.githubusercontent.com/51500641/68836264-f2d8f580-06df-11ea-93e9-686bfb4fc7a2.png)

 I have the same error.",create following code import import import server run following command python monk getting error solution python image error,issue,negative,neutral,neutral,neutral,neutral,neutral
913924999,"> same problem here

try latest version",problem try latest version,issue,negative,positive,positive,positive,positive,positive
907099103,"I was having the same problem. You need tensorflow 1

This is how I manage to run it:

```
$ conda create -n bert-as-service python=3.7
$ conda activate bert-as-service
$ pip install tensorflow==1.15.5
$ pip install bert-serving-server
```

and then just run `bert-serving-start`",problem need manage run create activate pip install pip install run,issue,negative,neutral,neutral,neutral,neutral,neutral
890836551,"@sxdxfan , @zychen-real 
I beg your pardon for asking, but could you specify how to check if the layer names are identical given the following code?

def get_assignment_map_from_checkpoint(tvars, init_checkpoint):
    """"""Compute the union of the current variables and checkpoint variables.""""""
    assignment_map = {}
    initialized_variable_names = {}

    name_to_variable = collections.OrderedDict()
    for var in tvars:
        name = var.name
        m = re.match(""^(.*):\\d+$"", name)
        if m is not None:
            name = m.group(1)
        name_to_variable[name] = var

    init_vars = tf.train.list_variables(init_checkpoint)

    assignment_map = collections.OrderedDict()
    for x in init_vars:
        (name, var) = (x[0], x[1])
        if name not in name_to_variable:
            continue
        assignment_map[name] = name
        initialized_variable_names[name] = 1
        initialized_variable_names[name + "":0""] = 1

    return (assignment_map, initialized_variable_names)
",beg pardon could specify check layer identical given following code compute union current name name none name name name name continue name name name name return,issue,negative,neutral,neutral,neutral,neutral,neutral
885459775,"> After banging my head against the wall for a while on this issue, was finally able to boot the server using:
> 
> * Python 3.6.16
> * Tensorflow 1.15.0
> * bert-as-service 1.10.0
> * Linux 3.10 (RHEL)
> 
> ```
> $ uname -srv
> Linux 3.10.0-862.2.3.el7.x86_64 #1 SMP Mon Apr 30 12:37:51 EDT 2018
> ```
> 
> from a new directory
> 
> ```shell
> $ conda create -n foo python=3.6
> $ conda activate foo
> $ pip install tensorflow==1.15.0
> $ mkdir bert-params
> $ wget https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip
> $ cd ..
> $ bert-serving-start -model_dir bert-params/cased_L-12_H-768_A-12/
> ```
> 
> FWIW: Using miniconda meaning that a `conda create` does not install a lot of default packages.
> 
> It's possible that either/both of Python 3.6.16 and TF 1.15.0 are required, or that any 3.6.X and 1.15.X would work.
> 
> What I do know is that Python 3.7 with TF 1.15.X does _not_ work.

nice solutiion.thank you",banging head wall issue finally able boot server python mon new directory shell create foo activate foo pip install meaning create install lot default possible python would work know python work nice,issue,positive,positive,positive,positive,positive,positive
884443159,"After banging my head against the wall for a while on this issue, was finally able to boot the server using:

- Python 3.6.16
- Tensorflow 1.15.0
- bert-as-service 1.10.0
- Linux 3.10 (RHEL)

```
$ uname -srv
Linux 3.10.0-862.2.3.el7.x86_64 #1 SMP Mon Apr 30 12:37:51 EDT 2018
```

from a new directory

```bash
$ conda create -n foo python=3.6
$ conda activate foo
$ pip install tensorflow==1.15.0
$ mkdir bert-params
$ wget https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip
$ cd ..
$ bert-serving-start -model_dir bert-params/cased_L-12_H-768_A-12/
```

FWIW: Using miniconda meaning that a `conda create` does not install a lot of default packages.

It's possible that either/both of Python 3.6.16 and TF 1.15.0 are required, or that any 3.6.X and 1.15.X would work.

What I do know is that Python 3.7 with TF 1.15.X does *not* work.",banging head wall issue finally able boot server python mon new directory bash create foo activate foo pip install meaning create install lot default possible python would work know python work,issue,negative,positive,positive,positive,positive,positive
883341455,"I get same error with Python 3.6 and TF 1.15...
This is somehow broken.
---
Edit:
Instead of installing TF 1.15 with pip, I installed Python 3.6 and TF just like dleve123 says below with conda, and it seems to work.",get error python somehow broken edit instead pip python like work,issue,negative,negative,negative,negative,negative,negative
864893701,"@ilham-bintang  Its worked for me

I put the changes like this

from bert_serving.client import BertClient
bc = BertClient(ip='127.0.0.1',port=5555)

Initially the port number I have given was incorrect , that's why it hanged . Now working fine.  ",worked put like import initially port number given incorrect working fine,issue,positive,positive,positive,positive,positive,positive
863848233,I mean the head part that show the config,mean head part show,issue,negative,negative,negative,negative,negative,negative
863780573,"
![Screenshot from 2021-06-18 11-35-31](https://user-images.githubusercontent.com/41164884/122514288-57c99f80-d029-11eb-8b4b-ada1fba92e3c.png)

@ilham-bintang   Here is the output of the bert server running console. ",output server running console,issue,negative,neutral,neutral,neutral,neutral,neutral
863506006,"Hi @pradeepdev-1995

Can you show me the output from your bert server? The IP address and port should be shown there",hi show output server address port shown,issue,negative,neutral,neutral,neutral,neutral,neutral
863225088,"@ilham-bintang 
For me also the given code snippet is seems like hanging.

from bert_serving.client import BertClient
bc = BertClient(ip='127.0.0.1')

The versions I am using are bert-serving-client==1.10.0
bert-serving-server==1.10.0.  And running the code on ubuntu cpu machine.

Please help to solve this



",also given code snippet like hanging import running code machine please help solve,issue,positive,neutral,neutral,neutral,neutral,neutral
860039245,"Represents [dependency hell](https://en.wikipedia.org/wiki/Dependency_hell).

Also, bert-as-service is too memory-intensive to run on heroku's hobby dynos (up to 1GB RAM).",dependency hell also run hobby ram,issue,negative,neutral,neutral,neutral,neutral,neutral
846884460,"Had the same issue, because I forgot to set the session type to GPU. Don't be as stupid as me.",issue forgot set session type stupid,issue,negative,negative,negative,negative,negative,negative
843146838,"@meilanglang I made it work for me: 
1) tensorflow==1.15.0 (tensorflow-gpu==1.15.0) Make sure to have CUDA = 10.0 installed
2) bert-serving-server==1.9.9
3) rename checkpoint files to: 
      * bert_model.ckpt.data-00000-of-00001 
      * bert_model.ckpt.index
      * bert_model.ckpt.meta
4) set an absolute path to German BERT
5) among the parameters, set the following: max_seq_len=512 (or 128) and max_batch_size=1
6) go to this file of bert-serving-server, in my case it was in the virtual env: `\venv\Lib\site-packages\bert_serving\server\graph.py`. In the lines 61-63, replace the code with 
``` python
input_ids = tf.placeholder(tf.int32, (args.max_batch_size, args.max_seq_len), 'input_ids')
input_mask = tf.placeholder(tf.int32, (args.max_batch_size, args.max_seq_len), 'input_mask')
input_type_ids = tf.placeholder(tf.int32, (args.max_batch_size, args.max_seq_len), 'input_type_ids')
```

For debugging purpose, I used the following script from [https://www.programmersought.com/article/20774372335/](https://www.programmersought.com/article/20774372335/): 

Server: 

```python
from bert_serving.server import BertServer
from bert_serving.server.helper import get_args_parser
def main():
    args = get_args_parser().parse_args(['-model_dir', r'C:\Users\annaz\PycharmProjects\bert-server\bert-base-german-cased',
                                         '-port', '86500',
                                         '-port_out', '86501',
                                         '-max_seq_len', '512',
                                        '-num_worker', '1',
                                         '-mask_cls_sep',
                                         # '-cpu',
                                         '-max_batch_size', '1'
                                         ])
    bs = BertServer(args)
    print(""bert server start...."")
    bs.start()

if __name__ == ""__main__"":
    main()
```


Client: 

```python 
from bert_serving.client import BertClient
bc = BertClient(port=86500, port_out=86501, show_server_config=True, timeout=100000)
vec = bc.encode(['test', 'test 2'])
```

Running the command from the terminal still doesn't work for me, but this client-server code in .py files does it job. ",made work make sure rename set absolute path german among set following go file case virtual replace code python purpose used following script server python import import main print server start main client python import running command terminal still work code job,issue,negative,positive,positive,positive,positive,positive
835959726,"Running into a similar issue trying docker compose.  I tried setting 8000:8125, but no dice - still seeing 'Connection refused'.  Any thoughts? @bigrig2212 @hanxiao  ",running similar issue trying docker compose tried setting dice still seeing,issue,negative,neutral,neutral,neutral,neutral,neutral
832775901,"@weizhenzhao Thanks a lot, I spent almost one day on this .",thanks lot spent almost one day,issue,negative,positive,neutral,neutral,positive,positive
826841391,"I installed the server in a virtual environment with tf 1.14 and the installed the client in tf 2.0 in colab. 
This works fine. I guess its the only work around.

Update me if there are any other solutions.",server virtual environment client work fine guess work around update,issue,negative,positive,positive,positive,positive,positive
824647983,"i referred [to](url) the solution posted in #526 as an alternate method. worked for me in colab. 
I'm guessing colab has some port issues due to which the server isn't able to setup correctly. ",solution posted alternate method worked guessing port due server able setup correctly,issue,negative,positive,positive,positive,positive,positive
824543681,"pip install bert_serving_server
pip install bert_serving_client
",pip install pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
823604870,"I also can't get tf >= 2.0 to work, so this would be awesome.",also ca get work would awesome,issue,positive,positive,positive,positive,positive,positive
822325975,"Hi, probably this caused by wrong IP (in this case 'localhost'), so try remove the localhost and call via `BertClient()` instead.

If this occurs, kindly change the IP to `127.0.0.1`",hi probably wrong case try remove call via instead kindly change,issue,negative,positive,neutral,neutral,positive,positive
821189200,"I had the same issue. It is perplexing that if I open 2 client programs only one of them will get stuck. If I send query1 from client1 and it gets stuck. Then I send query2 from client2, client1 will receive the result and client2 get stuck. Very similar to what 4everlove described here.
[(https://github.com/hanxiao/bert-as-service/issues/387)]",issue perplexing open client one get stuck send query client stuck send query client client receive result client get stuck similar,issue,negative,neutral,neutral,neutral,neutral,neutral
819505645,"@Akila-Ayanthi No I didn't, I suspect the original code from hanxiao:master (last commit August 2020) is not 
regularly maintained and something might have broken due to version issues.
I am using this very good alternative though, to integrate BERT with ElasticSearch https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/semantic-search/semantic_search_quora_elasticsearch.py 
",suspect original code master last commit august regularly something might broken due version good alternative though integrate,issue,negative,positive,neutral,neutral,positive,positive
819474795,"@mmohaveri How do you run BAS from the source code?
",run ba source code,issue,negative,neutral,neutral,neutral,neutral,neutral
819407921,Were you able to run bert-as-a-service from the source code?,able run source code,issue,negative,positive,positive,positive,positive,positive
814791848,"Hi @mmohaveri 
I have tried using your code. However, tf is not able to build the graph correctly, at least for me.
I have downloaded the BER model, I then open a terminal and run the following
`bert-serving-start -model_dir=/d/Users/[...]/uncased_L-12_H-768_A-12 -cpu`

But I then get the following error
```
Traceback (most recent call last):
  File ""D:\Users\[...]\venv38\Scripts\bert-serving-start-script.py"", line 33, in <module>
    sys.exit(load_entry_point('bert-serving-server', 'console_scripts', 'bert-serving-start')())
  File ""d:\users\[...]\bert-as-service\server\bert_serving\server\cli\__init__.py"", line 8, in main
    with BertServer(get_run_args()) as server:
  File ""d:\users\[...]\bert-as-service\server\bert_serving\server\__init__.py"", line 75, in __init__
    self.graph_path, self.bert_config = pool.apply(optimize_graph, (self.args,))
TypeError: cannot unpack non-iterable NoneType object
```

even though the Bert model is there. Any idea what might be going wrong?",hi tried code however able build graph correctly least ber model open terminal run following get following error recent call last file line module file line main server file line unpack object even though model idea might going wrong,issue,negative,negative,neutral,neutral,negative,negative
811677111,"I think maybe the model folder has no permission to be written, when i use ""sudo chmod -R 777 "", it works.",think maybe model folder permission written use work,issue,negative,neutral,neutral,neutral,neutral,neutral
810708126,I have figured it out. In the same way that I found in #293. I forget to make a dir which is called zmq.,figured way found forget make,issue,negative,neutral,neutral,neutral,neutral,neutral
804417915,"I forgot to set polling strategy to NONE , problem solved
",forgot set polling strategy none problem,issue,negative,neutral,neutral,neutral,neutral,neutral
803808944,same problem with me. Did anybody find the solution?,problem anybody find solution,issue,negative,neutral,neutral,neutral,neutral,neutral
781256982,"I found the reason.
It will work when I delete ""-gpu"" from the shell code.
囧 囧 囧 囧 囧 ",found reason work delete shell code,issue,negative,neutral,neutral,neutral,neutral,neutral
779832055,"First, you should check your server ip, you  can use command ifconfig, if your server connect with wifi, your server ip may be 192.168.xxx.xxx, and your client should connect with the same wifi,and enter the server ip when you instance the BertClient 
```=python
from bert_serving.client import BertClient
bc = BertClient(ip='192.168.xxx.xxx')  # ip address of the GPU machine
bc.encode(['First do it', 'then do it right', 'then do it better' 
```
@zephoon",first check server use command server connect server may client connect enter server instance import address machine right better,issue,negative,positive,positive,positive,positive,positive
773259896,"Let me answer this question.

This can be done using the official huggingface tokenizer first, then set `is_tokenized` to True when using `bc.encode(...)`

Assume that you have a list of sentences as ```list_sent``` to encode, where each sentence is a string.

```
from tokenizers import BertWordPieceTokenizer

vocab_filename = ""%s\\vocab.txt"" % model_path
WordPiece = BertWordPieceTokenizer(vocab_filename)
list_sent_tokenised = [WordPiece.encode(sent).tokens[1:-1] for sent in list_sent]
```

After this tokenization, you can then 
```vec,list_tokens_sent = bc.encode(list_sent_tokenised,show_tokens=True,is_tokenized=True)```

This works with the case when ```[MASK]``` is in a sentence, if not doing pre-tokenization, `[MASK]` will be tokenised to `['[', 'mask', ']']`.",let answer question done official first set true assume list encode sentence string import sent sent work case mask sentence mask,issue,positive,positive,positive,positive,positive,positive
770405037,My issue was with the pre-trained model I was trying to use. Check whether you get the same error with the original pre-trained BERT models.,issue model trying use check whether get error original,issue,negative,positive,positive,positive,positive,positive
765809920,"I saw all similar posts and implemented all suggestions such as changing to absolute path, using the right tensorflow version (I have tried both 1.13.0rc1 and 1.14.0rc1).

This works on another machine which has GPU. Can someone please suggest something to fix this?",saw similar absolute path right version tried work another machine someone please suggest something fix,issue,negative,positive,positive,positive,positive,positive
751443018,I reckon that a lot of people meet this problem.,reckon lot people meet problem,issue,negative,neutral,neutral,neutral,neutral,neutral
747814279,"> > Resolved the issue
> > using tensorflow 1.14.0 wont cause this issue.
> 
> @theCuriousHAT hi there, did you just change the version of base image in the Dockerfile to 1.14.0 and it worked?
> I encountered the same problem when using docker. could you please tell me how did you solve this out, thanks~

to anyone who has encountered the same issue. here is the I solved it using docker.
- 1. `git clone https://github.com/hanxiao/bert-as-service.git`
- 2. change the base image version to `1.15.3-py3`, i.e. `FROM tensorflow/tensorflow:1.15.3-py3`
- 3. after building the image, using `docker run -dit -p 5555:5555 -p 5556:5556 -v $PATH_MODEL:/model -t bert-as-service $NUM_WORKER` as mentioned in readme to start up. NOTICE: I removed `--runtime`, because the machine I'm using doesn't have a GPU.

here's a snapshot of the result after a successful starting up↓

![image](https://user-images.githubusercontent.com/7901126/102564080-abf4d500-4115-11eb-8600-6d92d9216dec.png)
",resolved issue wont cause issue hi change version base image worked problem docker could please tell solve anyone issue docker git clone change base image version building image docker run start notice removed machine snapshot result successful starting image,issue,positive,negative,negative,negative,negative,negative
747342974,"> Resolved the issue
> using tensorflow 1.14.0 wont cause this issue.

@theCuriousHAT  hi there, did you just change the version of base image in the Dockerfile to 1.14.0 and it worked? 
I encountered the same problem when using docker. could you please tell me how did you solve this out, thanks~",resolved issue wont cause issue hi change version base image worked problem docker could please tell solve,issue,negative,negative,negative,negative,negative,negative
745493556,"Are use this for a prototype. I haven’t looked at this code for awhile now.
These days my go-to framework is the huggingface libraries.

On Tue, 15 Dec 2020 at 18:09, Ievgen Goichuk <notifications@github.com>
wrote:

> Experiencing the same issue. Have you solved it @boxabirds
> <https://github.com/boxabirds>?
> I use tensorflow/tensorflow:1.15.2-py3 image.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/hanxiao/bert-as-service/issues/411#issuecomment-745468012>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AABD62NEDPOHVKFOTR6SAY3SU6Q4LANCNFSM4ICNKUCA>
> .
>
",use prototype code awhile day framework tue wrote issue use image reply directly view,issue,negative,positive,neutral,neutral,positive,positive
742628657,"I am getting the same problem in windows, although I tried all the methods. 
""TypeError: 'NoneType' object is not iterable"".
Please somebody help me.
",getting problem although tried object iterable please somebody help,issue,negative,neutral,neutral,neutral,neutral,neutral
738933272,"I tried it a lot and finally this worked. (Worked date: Dec 4, 2020)

```python
%tensorflow_version 1.x

!pip install bert-serving-client
!pip install -U bert-serving-server[http]

!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip
!unzip uncased_L-12_H-768_A-12.zip
!nohup bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 > out.file 2>&1 &


!ls  # you should see uncased_something_.zip


from bert_serving.client import BertClient
bc = BertClient()

list_text = ['you need to use tensorflow 1', 'tf2 does not work']
embedded_text = bc.encode(list_text)

print(embedded_text)

this gives
[[-0.32885492 -0.39915255 -0.3393307  ... -0.27655432 -0.13769765
   0.39554596]
 [ 0.03333598 -0.6914621   0.0800274  ... -0.46210474 -0.06833883
   0.30134943]]
```",tried lot finally worked worked date python pip install pip install see import need use work print,issue,negative,neutral,neutral,neutral,neutral,neutral
738710976,"I am running the below command after locating at model_dir
command  bert-serving-start -model_dir uncased_L-2_H-128_A-2\ -cpu 
files present in model_dir :bert_config.json,bert_model.ckpt.data-00000-of-00001,bert_model.ckpt.index,vocab.txt
have tried with providing absolute path till model directory too
system config - 12 gb ram.
let me know how could I resolve this.Thanks in advance

",running command command present tried providing absolute path till model directory system ram let know could resolve advance,issue,negative,positive,neutral,neutral,positive,positive
737901690,"I am still facing the above issue have tried couple of solutions.
Version - tensorflow-cpu = 1.13.1
Bert_serving_client = 1.9.9
Bert_serving_server = 1.9.9

",still facing issue tried couple version,issue,negative,neutral,neutral,neutral,neutral,neutral
732712298,"@Yixan it seems that you have opened the default port before and haven't close it. 

Try to start a service with a new port number.",default port close try start service new port number,issue,negative,positive,positive,positive,positive,positive
729662122,"Solved the problem with: 
 `bc = BertClient(timeout=10000)`
I had the same problem, and it may caused by the default value ""timeout=-1"" in the initial function of BertClient. It caused some issue in socket.",problem problem may default value initial function issue socket,issue,negative,neutral,neutral,neutral,neutral,neutral
727518166,"> > After debugging, I found that when this error occurs, the response format returned by the server is different from the normal response format
> 
> hello, how to solve??

尽可能使用GPU而不是CPU来启动该服务端",found error response format returned server different normal response format hello solve,issue,negative,positive,neutral,neutral,positive,positive
727225337,"I had the same question as the above.

To concatenate the hidden states of last 4 layers, the right way in python for ```-pooling_layer``` is

```
args = get_args_parser().parse_args(['-model_dir', model_path,
                                         '-pooling_strategy', 'NONE',
                                         '-pooling_layer', '-4','-3','-2','-1',
                                         '-max_seq_len', 'None',                                         
                                         '-cpu'])
```

Thanks,
A",question concatenate hidden last right way python thanks,issue,negative,positive,neutral,neutral,positive,positive
723072369,"> Same here guys. Gets completely stuck. I tried dowgrading to 1.9.2 as suggested and it did not work.

hello   how are you deal with it? please help me ",completely stuck tried work hello deal please help,issue,negative,positive,neutral,neutral,positive,positive
720432433,"I have installed it but still got this error. Can anyone help?

---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-66-166c53ca0c78> in <module>
      1 # import sys
----> 2 from bert_serving.client import BertClient
      3 client = BertClient()
      4 vectors = client.encode(['dog'],['cat'],['man'])

ModuleNotFoundError: No module named 'bert_serving'",still got error anyone help recent call last module import import client module,issue,negative,neutral,neutral,neutral,neutral,neutral
720165440,@bevankoopman Yes it did. I recommend using the HuggingFace transformers library to use Bert. It is more up to date with the current releases. ,yes recommend library use date current,issue,positive,neutral,neutral,neutral,neutral,neutral
720164694,"> I specified the -model_dir as the directory that contained the bioBert files and also renamed them made sure the model name and config.json matched the -ckpt_name and -config_name flags. The default of -ckpt_name is 'bert_model.ckpt"" and the default of config_name is ""bert_config.json""

And did that get it working for you?",directory also made sure model name default default get working,issue,negative,positive,positive,positive,positive,positive
719736057,"> I don't think this is the expected use of BERT. BERT is a network trained at the sentence embedding level, thus the representation of more than one sentence should be pretty inaccurate and the computation needed beyond 512 tokens would be huge (remember, the computation isn't linear to the number of tokens).
> 
> There's many strategies for you to try if you want a more accurate paragraph representation, for example:
> 
> * do element wise average / max over the sequence of sentence embeddings that compose your paragraphs (no additional training), thus your resulting paragraph embedding will be of the same number of dims than each sentence embedding.
> * use a paragraph embedding from weighted sentences if you have any extra information about how each sentence matters to your overall paragraph representation (no additional training)
> * use a RNN / CNN downstream layer to get a paragraph embedding, however this requires training on a target label (regression / classification task).

@ironflood The averaging technique sounds interesting! Could you please point me to the results if this has been tried by anyone?",think use network trained sentence level thus representation one sentence pretty inaccurate computation beyond would huge remember computation linear number many try want accurate paragraph representation example element wise average sequence sentence compose additional training thus resulting paragraph number sentence use paragraph weighted extra information sentence overall paragraph representation additional training use downstream layer get paragraph however training target label regression classification task technique interesting could please point tried anyone,issue,positive,positive,positive,positive,positive,positive
714902474,I too faced this problem. Make sure to check if you are giving the correct path to the folder. Give the absolute path just to be sure. This solved the problem for me. ,faced problem make sure check giving correct path folder give absolute path sure problem,issue,negative,positive,positive,positive,positive,positive
713525074,Have you solved this problem? I also meet this problem,problem also meet problem,issue,negative,neutral,neutral,neutral,neutral,neutral
706966847,"> Just use absolute path

Thanks bro",use absolute path thanks,issue,negative,positive,positive,positive,positive,positive
706747999,"Hey, Use an exclamation mark in the beginning as it is a terminal command.
!bert-serving-start -model_dir = Users/HP/Downloads/uncased_L-12-H-768-A-12/uncased_L-12-H-768-A-12 -num_worker= 1 (this worked for me, was getting the same error initially)",hey use exclamation mark beginning terminal command worked getting error initially,issue,negative,neutral,neutral,neutral,neutral,neutral
700491634,"I have the same issue ,too.can someone solve this problem?
",issue someone solve problem,issue,negative,neutral,neutral,neutral,neutral,neutral
700076066,Still not fixed in pip install -U,still fixed pip install,issue,negative,positive,neutral,neutral,positive,positive
697144542,"When I run this on Noteboks, bert-serving-start -model_dir = Users/HP/Downloads/uncased_L-12-H-768-A-12/uncased_L-12-H-768-A-12 -num_worker= 1
I got an error Invalid syntax",run got error invalid syntax,issue,negative,neutral,neutral,neutral,neutral,neutral
695863854,"


> 2020-09-02 10:03:25.990101: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
> c:\programdata\anaconda3\lib\site-packages\bert_serving\server\helper.py:175: UserWarning: Tensorflow 2.3.0 is not tested! It may or may not work. Feel free to submit an issue at
> https://github.com/hanxiao/bert-as-service/issues/
> warnings.warn('Tensorflow %s is not tested! It may or may not work. '
> E:?[36mGRAPHOPT?[0m:fail to optimize the graph!
> Traceback (most recent call last):
> File ""c:\programdata\anaconda3\lib\runpy.py"", line 194, in _run_module_as_main return _run_code(code, main_globals, None, File ""c:\programdata\anaconda3\lib\runpy.py"", line 87, in _run_code exec(code, run_globals) File ""C:\ProgramData\Anaconda3\Scripts\bert-serving-start.exe__main__.py"", line 7, in File ""c:\programdata\anaconda3\lib\site-packages\bert_serving\server\cli__init__.py"", line 4, in main with BertServer(get_run_args()) as server: File ""c:\programdata\anaconda3\lib\site-packages\bert_serving\server__init__.py"", line 71, in **init**
> self.graph_path, self.bert_config = pool.apply(optimize_graph, (self.args,))
> TypeError: cannot unpack non-iterable NoneType object
> 
> 请问这个问题怎么解决

使用Tensorflow1的版本就没问题，目前我是这样解决的，没有去修改代码",ignore set machine tested may may work feel free submit issue tested may may work fail optimize graph recent call last file line return code none file line code file line file line main server file line unpack object,issue,negative,positive,neutral,neutral,positive,positive
689478838,"Same same same,  it runs on GPU, but CPU is capped out with minor memory consumption on GPU(~97MB).",capped minor memory consumption,issue,negative,negative,neutral,neutral,negative,negative
685520899,"TypeError: cannot unpack non-iterable NoneType object. I m getting  same error message everytime I run the Bert service using CPU, can anyone explain",unpack object getting error message run service anyone explain,issue,negative,neutral,neutral,neutral,neutral,neutral
685518606,"I have a similar issue when running Bert cased model, I m running it on CPU 


(base) C:\Users\Ishwar>bert-serving-start -model_dir ""C:\Bert Models\cased_L-12_H-768_A-12\cased_L-12_H-768_A-12\"" -cpu -max_batch_size 16 -num_worker=1
usage: C:\Users\Ishwar\Anaconda3\Scripts\bert-serving-start -model_dir C:\Bert Models\cased_L-12_H-768_A-12\cased_L-12_H-768_A-12"" -cpu -max_batch_size 16 -num_worker=1
                 ARG   VALUE
__________________________________________________
           ckpt_name = bert_model.ckpt
         config_name = bert_config.json
                cors = *
                 cpu = False
          device_map = []
       do_lower_case = True
  fixed_embed_length = False
                fp16 = False
 gpu_memory_fraction = 0.5
       graph_tmp_dir = None
    http_max_connect = 10
           http_port = None
        mask_cls_sep = False
      max_batch_size = 256
         max_seq_len = 25
           model_dir = C:\Bert Models\cased_L-12_H-768_A-12\cased_L-12_H-768_A-12"" -cpu -max_batch_size 16 -num_worker=1
no_position_embeddings = False
    no_special_token = False
          num_worker = 1
       pooling_layer = [-2]
    pooling_strategy = REDUCE_MEAN
                port = 5555
            port_out = 5556
       prefetch_size = 10
 priority_batch_size = 16
show_tokens_to_client = False
     tuned_model_dir = None
             verbose = False
                 xla = False

I:[35mVENTILATOR[0m:freeze, optimize and export graph, could take a while...
WARNING:tensorflow:From c:\users\ishwar\anaconda3\lib\site-packages\bert_serving\server\helper.py:186: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.

WARNING:tensorflow:From c:\users\ishwar\anaconda3\lib\site-packages\bert_serving\server\helper.py:186: The name tf.logging.ERROR is deprecated. Please use tf.compat.v1.logging.ERROR instead.

I:[36mGRAPHOPT[0m:model config: C:\Bert Models\cased_L-12_H-768_A-12\cased_L-12_H-768_A-12"" -cpu -max_batch_size 16 -num_worker=1\bert_config.json
I:[36mGRAPHOPT[0m:checkpoint: C:\Bert Models\cased_L-12_H-768_A-12\cased_L-12_H-768_A-12"" -cpu -max_batch_size 16 -num_worker=1\bert_model.ckpt
E:[36mGRAPHOPT[0m:fail to optimize the graph!
Traceback (most recent call last):
  File ""c:\users\ishwar\anaconda3\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\users\ishwar\anaconda3\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\Ishwar\Anaconda3\Scripts\bert-serving-start.exe\__main__.py"", line 9, in <module>
  File ""c:\users\ishwar\anaconda3\lib\site-packages\bert_serving\server\cli\__init__.py"", line 4, in main
    with BertServer(get_run_args()) as server:
  File ""c:\users\ishwar\anaconda3\lib\site-packages\bert_serving\server\__init__.py"", line 71, in __init__
    self.graph_path, self.bert_config = pool.apply(optimize_graph, (self.args,))
TypeError: cannot unpack non-iterable NoneType object",similar issue running cased model running base usage value false true false false none none false false false port false none verbose false false freeze optimize export graph could take warning name please use instead warning name please use instead model fail optimize graph recent call last file line file line code file line module file line main server file line unpack object,issue,positive,negative,negative,negative,negative,negative
682955476,"> Got it running using Bert Client and nohup - here is how it worked for me:
> 
> !pip install bert-serving-client
> !pip install -U bert-serving-server[http]
> 
> !wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip
> !unzip uncased_L-12_H-768_A-12.zip
> !nohup bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 > out.file 2>&1 &
> 
> from bert_serving.client import BertClient
> bc = BertClient()
> print (bc.encode(['First do it', 'then do it right', 'then do it better']))

Thanks, it works for me :hearts:",got running client worked pip install pip install import print right better thanks work heart,issue,positive,positive,positive,positive,positive,positive
679454877,I have the same issue but no one can solve this problem,issue one solve problem,issue,negative,neutral,neutral,neutral,neutral,neutral
679127076,"This works for me, so do it line by line. Be careful about the filename you wanna get from github or any other sources.

 
!pip install bert-serving-client
!pip install -U bert-serving-server[http]
!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip
!unzip uncased_L-12_H-768_A-12.zip
 %tensorflow_version 1.x
!nohup bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 > out.file 2>&1 &

from bert_serving.client import BertClient
bc = BertClient()
print (bc.encode(['First do it', 'then do it right', 'then do it better']))",work line line careful wan na get pip install pip install import print right better,issue,positive,positive,positive,positive,positive,positive
678432380,"> Hi,
> I had another problem...
> I installed correctly bert-serving-server and bert-serving client and I downloaded the multilingual model that I uncompressed in the same folder on my code.
> But when i run
> bert-serving-start -model_dir /tmp/multi_cased_L-12_H-768_A-12/ -num_worker=1
> it launches the following error:
> SyntaxError: invalid token.
> Could you help me?
> Thank you so much

Did you get any solution ? Even I am facing the same issue.",hi another problem correctly client multilingual model uncompressed folder code run following error invalid token could help thank much get solution even facing issue,issue,negative,positive,neutral,neutral,positive,positive
677514233,"@DmitryKey 
Thank you for your time and response. I wanted to know am i following the correct method to calculate the score based on above python code.

1. When i type `nonsense` , expectation is to have no answer or if any answer with very low score. In our case the score is 0.7 which is high. How do we handle such case specially for FAQ's in Chatbot.

2.Do you recommend any specific BERT model for FAQ answering in Chatbot?. Kindly suggest( For English and multilingual FAQ's)
",thank time response know following correct method calculate score based python code type nonsense expectation answer answer low score case score high handle case specially recommend specific model kindly suggest multilingual,issue,positive,positive,positive,positive,positive,positive
677374834,"hey @nishithbenhur thanks for your claps on the blog post!

The idea is that if you see issues with score levels, it might mean two things:

1. the query you provided does not have relevant articles (questions in your case) -- was there some question you were hoping to see in response to query ""nonsense"" ? Contrary to that, query ""how do I run in cpu machine"" does bring really relevant query at the top: both because your query is meaningful and because you have a relevant question in the database to match it.

2. you might want to fine tune BERT on your dataset. Originally BERT was trained on Wikipedia, so it might not have some specific to your use case vocabulary and meanings.

Hope this helps.",hey thanks post idea see score might mean two query provided relevant case question see response query nonsense contrary query run machine bring really relevant query top query meaningful relevant question match might want fine tune originally trained might specific use case vocabulary hope,issue,positive,positive,positive,positive,positive,positive
676554703,"@DmitryKey 

Article is very informative , i have also given a clap for the same :)

I am not using any search engine. As of the now the embeddings are calculated and stored in SQL Database.
Please find the code i am using below

>![image](https://user-images.githubusercontent.com/58899985/90668611-f81a3080-e26d-11ea-8fc6-02aea4151ccb.png)

I get below results with a score ranging between 0 and 1

>![image](https://user-images.githubusercontent.com/58899985/90667939-ea17e000-e26c-11ea-9397-d45bb5e66b7f.png)

However the score is not that great when it come to below question as it show a high score of 0.7

>![image](https://user-images.githubusercontent.com/58899985/90668369-9f4a9800-e26d-11ea-964a-01ac86a99c17.png)

Kindly suggestion if the above logic can be improved.
",article informative also given clap search engine calculated please find code image get score ranging image however score great come question show high score image kindly suggestion logic,issue,positive,positive,positive,positive,positive,positive
676300732,"Older issue, but is `tensorflow>=2.2` support planned?

Seems to come up here and there #580 

Downgrade to `tensorflow>=1.10` is a fix but unfortunately not possible on `python 3.8`",older issue support come downgrade fix unfortunately possible python,issue,negative,positive,neutral,neutral,positive,positive
676271860,"Similar issue here, seems to be very tricky dependencies management.

Suggested fix seems generally to be to downgrade to `tensorflow>=1.10`.

However older `tensorflow<=2.0` seems [not to be supported with Python 3.8.](https://www.tensorflow.org/install/pip): 
> Python 3.8 support requires TensorFlow 2.2

Is `tensorflow>=2.2` support planned for `bert-serving-server`?
",similar issue tricky management fix generally downgrade however older python python support support,issue,positive,positive,neutral,neutral,positive,positive
675875406,"@nishithbenhur 
I think you can achieve this by using cosine similarity and simply controlling what to show on the client side. Does this make sense?

Also, I have just published how to use bert-as-service with Solr to implement neural search: https://medium.com/@dmitry.kan/neural-search-with-bert-and-solr-ea5ead060b28
with streamlit demo, where you can see the effect of chosen parameters, like: cosine vs dot product. 

What search engine are you using?",think achieve cosine similarity simply show client side make sense also use implement neural search see effect chosen like cosine dot product search engine,issue,negative,neutral,neutral,neutral,neutral,neutral
675797774,"> no, the problem is consistently reproduced.
> need create dir and export to env
> export ZEROMQ_SOCK_TMP_DIR=/tmp/zmq
> link: `https://sanidem.home.blog/tag/docker/`

<img width=""316"" alt=""屏幕快照 2020-08-19 10 18 15"" src=""https://user-images.githubusercontent.com/69880386/90581098-02e8ad00-e1fd-11ea-93cf-88d28a06f6c6.png"">
Hi
It seems like the link is a private site.
Can u share the solution here?",problem consistently need create export export link hi like link private site share solution,issue,positive,positive,positive,positive,positive,positive
675632614,"@DmitryKey 
Thank you.

basically i want to restrict between 0 and 1 so that i can filter out the results which are below a particular threshold ( ex : Don't display the results to the user if score is less than 0.4).

Hope as you suggested i can achieve the result. 

Please let me know if the above result can be achieved ( filter based on threshold)",thank basically want restrict filter particular threshold ex display user score le hope achieve result please let know result filter based threshold,issue,positive,positive,positive,positive,positive,positive
675458179,"@nishithbenhur here is the code behind cosine calculation: https://github.com/saaay71/solr-vector-scoring/blob/master/src/com/github/saaay71/solr/VectorScoreQuery.java#L71 It is normalized by norms of query and document's vectors => it is bounded [0, 1] value.",code behind cosine calculation query document bounded value,issue,negative,negative,negative,negative,negative,negative
675367890,"@DmitryKey 

Can you please paste here an example code on how to calculate the score when for trained sentences?",please paste example code calculate score trained,issue,negative,neutral,neutral,neutral,neutral,neutral
674815056,"Because tensorflow has no logging
The traceback error: AttributeError: module 'tensorflow' has no attribute 'logging'
So downgrade the version.",logging error module attribute downgrade version,issue,negative,neutral,neutral,neutral,neutral,neutral
674053758,"it is my bad, python2.7 is used in server and it is clearly stated in the documentation.

following dockerfile works


```
FROM ubuntu:18.04
WORKDIR /src
COPY src .
RUN apt-get update && apt-get install -y python3.6 && apt-get install -y python3-pip
RUN pip3 install tensorflow==1.12.0
RUN pip3 install bert-serving-server
RUN pip3 install bert-serving-client
```",bad python used server clearly stated documentation following work copy run update install python install run pip install run pip install run pip install,issue,negative,negative,negative,negative,negative,negative
671108215,"Hi there,
I have a problem with the starting server. It gives actually two errors, one with the ""tf.logging"" and the other one as ""TypeError: cannot unpack non-iterable NoneType object"".
Thanks ",hi problem starting server actually two one one unpack object thanks,issue,negative,positive,neutral,neutral,positive,positive
670591567,"Current version of bert-serving-server on pip is 1.10.0
```
pip3 show bert-serving-server
Name: bert-serving-server
Version: 1.10.0
Summary: Mapping a variable-length sentence to a fixed-length vector using BERT model (Server)
Home-page: https://github.com/hanxiao/bert-as-service
Author: Han Xiao
Author-email: artex.xh@gmail.com
License: MIT
Location: /usr/local/lib/python3.8/dist-packages
Requires: numpy, pyzmq, six, termcolor, GPUtil
Required-by:
```

How to get 1.8.4?",current version pip pip show name version summary sentence vector model server author han license location six get,issue,negative,neutral,neutral,neutral,neutral,neutral
669977769,"Hello, I have used as suggested the absolute path without `\` at the end:
`bert-serving-start -model_dir=%AbsolutePath% -max_seq_len 20 -num_worker=5`
However, I am still getting the same error. I downgraded tensorflow and it also did not work.",hello used absolute path without end however still getting error also work,issue,negative,positive,positive,positive,positive,positive
669781330,"![InkedScreenshot_5_LI](https://user-images.githubusercontent.com/16528112/89507572-f875ee80-d7cc-11ea-977f-b19a6104a57d.jpg)
why it takes long time when get into freeze step?",long time get freeze step,issue,negative,negative,neutral,neutral,negative,negative
667825817,"@nishithbenhur take a look at this plugin, it returns the score in the range [0, 1]: https://github.com/saaay71/solr-vector-scoring",take look score range,issue,negative,neutral,neutral,neutral,neutral,neutral
664425357,"> Just adding a possible solution to help other people. I was able to solve this problem by assigning a different port number to the server.
> 
> `start-bert-as-service.py -model_dir uncased_L-12_H-768_A-12 -num_worker=1 -cpu -max_batch_size 16 -port 8190`
> 
> Hope it helps.

it works for me.",possible solution help people able solve problem different port number server hope work,issue,positive,positive,positive,positive,positive,positive
664024208,"@hanxiao it got resolved when I used the TF v1.15, thanks for your awesome job. But Can expect TF version above 2.0.0 ?",got resolved used thanks awesome job expect version,issue,positive,positive,positive,positive,positive,positive
663980850,"@DmitryKey 

Thank you for your time and quick response.

I have calculated the dot product using bert( training pipeline) and stored it in solr.
However down the line , when ( query pipeline) user asks a question, and computed the dot product , i do not get a score between 0 and 1. Also i want to display the only those documents possessing the score which above a certain threshold ( example above 0.7  only).

Can you please help me on doing this

And apologies over the typing error for Neural Search meetup. It was by mistake. Sorry",thank time quick response calculated dot product training pipeline however line query pipeline user question dot product get score also want display possessing score certain threshold example please help error neural search mistake sorry,issue,positive,positive,neutral,neutral,positive,positive
663974145,"Hi @nishithbenhur 

If you use BERT to compute document vector in preprocessing phase, you can store such vectors in a search engine, like Solr or Elasticsearch.

You have two major paths to go down to utilize BERT:

1. Compute a dot-product instead of traditional TF-IDF to rank the documents with highest dot-product higher. Check specifics of computing a score here: https://opensourceconnections.com/blog/2019/12/18/bert-and-search-relevance-part2-dense-vs-sparse/

2. Use BERT document vectors as a document feature in Learning To Rank model. LTR will take care of ranking the documents for you based on the model you have trained.

I also recommend you to review the practices of applying BERT to search here: https://scott.ai/2020-07-20-cool-way-to-search-text/

Btw, what's the Neural Search meetup -- can you share a link? ",hi use compute document vector phase store search engine like two major go utilize compute instead traditional rank highest higher check score use document document feature learning rank model take care based model trained also recommend review search neural search share link,issue,positive,negative,negative,negative,negative,negative
663967937,"`PS C:\Users\hp\Desktop\bank_chatbot2\bank_chatbot> bert-serving-start -model_dir=C:/Users/hp/Music/Downloads/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12 -num_worker=1
2020-07-26 15:24:40.690930: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2020-07-26 15:24:40.691426: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
c:\users\hp\anaconda3\lib\site-packages\bert_serving\server\helper.py:176: UserWarning: Tensorflow 2.1.1 is not tested! It may or may not work. Feel free to submit an issue at https://github.com/hanxiao/bert-as-service/issues/
  'Feel free to submit an issue at https://github.com/hanxiao/bert-as-service/issues/' % tf.__version__)
usage: C:\Users\hp\Anaconda3\Scripts\bert-serving-start -model_dir=C:/Users/hp/Music/Downloads/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12 -num_worker=1
                 ARG   VALUE
__________________________________________________
           ckpt_name = bert_model.ckpt
         config_name = bert_config.json
                cors = *
                 cpu = False
          device_map = []
       do_lower_case = True
  fixed_embed_length = False
                fp16 = False
 gpu_memory_fraction = 0.5
       graph_tmp_dir = None
    http_max_connect = 10
           http_port = None
        mask_cls_sep = False
      max_batch_size = 256
         max_seq_len = 25
           model_dir = C:/Users/hp/Music/Downloads/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12
no_position_embeddings = False
    no_special_token = False
          num_worker = 1
       pooling_layer = [-2]
    pooling_strategy = REDUCE_MEAN
                port = 5555
            port_out = 5556
       prefetch_size = 10
 priority_batch_size = 16
show_tokens_to_client = False
     tuned_model_dir = None
             verbose = False
                 xla = False

I:VENTILATOR:freeze, optimize and export graph, could take a while...
2020-07-26 15:24:47.307277: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2020-07-26 15:24:47.307992: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
c:\users\hp\anaconda3\lib\site-packages\bert_serving\server\helper.py:176: UserWarning: Tensorflow 2.1.1 is not tested! It may or may not work. Feel free to submit an issue at https://github.com/hanxiao/bert-as-service/issues/
  'Feel free to submit an issue at https://github.com/hanxiao/bert-as-service/issues/' % tf.__version__)
E:GRAPHOPT:fail to optimize the graph!
Traceback (most recent call last):
  File ""c:\users\hp\anaconda3\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\users\hp\anaconda3\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\hp\Anaconda3\Scripts\bert-serving-start.exe\__main__.py"", line 7, in <module>
  File ""c:\users\hp\anaconda3\lib\site-packages\bert_serving\server\cli\__init__.py"", line 4, in main
    with BertServer(get_run_args()) as server:
  File ""c:\users\hp\anaconda3\lib\site-packages\bert_serving\server\__init__.py"", line 71, in __init__
    self.graph_path, self.bert_config = pool.apply(optimize_graph, (self.args,))
TypeError: cannot unpack non-iterable NoneType object
PS C:\Users\hp\Desktop\bank_chatbot2\bank_chatbot> `



@hanxiao 
**Hey can anyone help me with this...**
Like I have also given the absolute path but still the error occurrs",could load dynamic library found ignore set machine tested may may work feel free submit issue free submit issue usage value false true false false none none false false false port false none verbose false false ventilator freeze optimize export graph could take could load dynamic library found ignore set machine tested may may work feel free submit issue free submit issue fail optimize graph recent call last file line file line code file line module file line main server file line unpack object hey anyone help like also given absolute path still error,issue,positive,negative,neutral,neutral,negative,negative
662349728,"> **Prerequisites**
> 
> > Please fill in by replacing `[ ]` with `[x]`.
> 
> * [x ] Are you running the latest `bert-as-service`?
> * [x ] Did you follow [the installation](https://github.com/hanxiao/bert-as-service#install) and [the usage](https://github.com/hanxiao/bert-as-service#usage) instructions in `README.md`?
> * [x ] Did you check the [FAQ list in `README.md`](https://github.com/hanxiao/bert-as-service#speech_balloon-faq)?
> * [ x] Did you perform [a cursory search on existing issues](https://github.com/hanxiao/bert-as-service/issues)?
> 
> **System information**
> 
> > Some of this information can be collected via [this script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh).
> 
> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10
> * TensorFlow installed from (source or binary): I typed this in command window to install:  pip3 install tensorflow-gpu==1.15
> * TensorFlow version: 1.15
> * Python version: 3.5
> * `bert-as-service` version: the latest version;
>   I typed this in command window to install: pip3 install -U bert-serving-server bert-serving-client
> * GPU model and memory:
> * CPU model and memory:
> 
> ### Description
> > Please replace `YOUR_SERVER_ARGS` and `YOUR_CLIENT_ARGS` accordingly. You can also write your own description for reproducing the issue.
> 
> **I'm using this command to start the server:**
> bert-serving-start -model_dir /BERT/uncased_L-24_H-1024_A-16 -num_worker=1
> 
> ```shell
> bert-serving-start YOUR_SERVER_ARGS
> ```
> 
> 2020-06-10 11:03:25.838138: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
> usage: C:\Users\Fanyi Mo\AppData\Local\Programs\Python\Python35\Scripts\bert-serving-start -model_dir /BERT/uncased_L-24_H-1024_A-16 -num_worker=1
> ARG VALUE
> 
> ```
>        ckpt_name = bert_model.ckpt
>      config_name = bert_config.json
>             cors = *
>              cpu = False
>       device_map = []
>    do_lower_case = True
> ```
> 
> fixed_embed_length = False
> fp16 = False
> gpu_memory_fraction = 0.5
> graph_tmp_dir = None
> http_max_connect = 10
> http_port = None
> mask_cls_sep = False
> max_batch_size = 256
> max_seq_len = 25
> model_dir = /BERT/uncased_L-24_H-1024_A-16
> no_position_embeddings = False
> no_special_token = False
> num_worker = 1
> pooling_layer = [-2]
> pooling_strategy = REDUCE_MEAN
> port = 5555
> port_out = 5556
> prefetch_size = 10
> priority_batch_size = 16
> show_tokens_to_client = False
> tuned_model_dir = None
> verbose = False
> xla = False
> 
> **Then this issue shows up:**
> I:�[35mVENTILATOR�[0m:freeze, optimize and export graph, could take a while...
> 2020-06-10 11:03:30.497862: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
> WARNING:tensorflow:From c:\users\fanyi mo\appdata\local\programs\python\python35\lib\site-packages\bert_serving\server\helper.py:186: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.
> 
> WARNING:tensorflow:From c:\users\fanyi mo\appdata\local\programs\python\python35\lib\site-packages\bert_serving\server\helper.py:186: The name tf.logging.ERROR is deprecated. Please use tf.compat.v1.logging.ERROR instead.
> 
> I:�[36mGRAPHOPT�[0m:model config: /BERT/uncased_L-24_H-1024_A-16\bert_config.json
> I:�[36mGRAPHOPT�[0m:checkpoint: /BERT/uncased_L-24_H-1024_A-16\bert_model.ckpt
> E:�[36mGRAPHOPT�[0m:fail to optimize the graph!
> Traceback (most recent call last):
> File ""c:\users\fanyi mo\appdata\local\programs\python\python35\lib\runpy.py"", line 193, in _run_module_as_main ""**main**"", mod_spec) File ""c:\users\fanyi mo\appdata\local\programs\python\python35\lib\runpy.py"", line 85, in _run_code exec(code, run_globals) File ""C:\Users\Fanyi Mo\AppData\Local\Programs\Python\Python35\Scripts\bert-serving-start.exe__main__.py"", line 7, in File ""c:\users\fanyi mo\appdata\local\programs\python\python35\lib\site-packages\bert_serving\server\cli__init__.py"", line 4, in main
> with BertServer(get_run_args()) as server:
> File ""c:\users\fanyi mo\appdata\local\programs\python\python35\lib\site-packages\bert_serving\server__init__.py"", line 71, in **init**
> self.graph_path, self.bert_config = pool.apply(optimize_graph, (self.args,))
> TypeError: 'NoneType' object is not iterable
> ...

perhaps you could check whether your `model_dir`  path is  correct.",please fill running latest follow installation usage check list perform cursory search system information information collected via script o platform distribution win source binary command window install pip install version python version version latest version command window install pip install model memory model memory description please replace accordingly also write description issue command start server shell successfully dynamic library usage value false true false false none none false false false port false none verbose false false issue freeze optimize export graph could take successfully dynamic library warning name please use instead warning name please use instead model fail optimize graph recent call last file line main file line code file line file line main server file line object iterable perhaps could check whether path correct,issue,positive,negative,neutral,neutral,negative,negative
660083492,"I am facing the same problem, i have downgraded the tensorflow version but still facing the problem",facing problem version still facing problem,issue,negative,neutral,neutral,neutral,neutral,neutral
655619882,"I am also getting the same error:

`Traceback (most recent call last):
  File ""/usr/local/activepython-3.6/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/python/tf/lib/python3.6/site-packages/bert_serving/server/__init__.py"", line 115, in run
    self._run()
  File ""/python/tf/lib/python3.6/site-packages/zmq/decorators.py"", line 75, in wrapper
    return func(*args, **kwargs)
  File ""/python/tf/lib/python3.6/site-packages/zmq/decorators.py"", line 75, in wrapper
    return func(*args, **kwargs)
  File ""/python/tf/lib/python3.6/site-packages/zmq/decorators.py"", line 75, in wrapper
    return func(*args, **kwargs)
  File ""/python/tf/lib/python3.6/site-packages/bert_serving/server/zmq_decor.py"", line 27, in wrapper
    return func(*args, **kwargs)
  File ""/python/tf/lib/python3.6/site-packages/bert_serving/server/__init__.py"", line 131, in _run
    addr_front2sink = auto_bind(sink)
  File ""/python/tf/lib/python3.6/site-packages/bert_serving/server/helper.py"", line 203, in auto_bind
    socket.bind('ipc://{}'.format(tmp_dir))
  File ""zmq/backend/cython/socket.pyx"", line 550, in zmq.backend.cython.socket.Socket.bind
  File ""zmq/backend/cython/checkrc.pxd"", line 26, in zmq.backend.cython.checkrc._check_rc
zmq.error.ZMQError: Permission denied`

Does anyone have a solution for the same?",also getting error recent call last file line file line run file line wrapper return file line wrapper return file line wrapper return file line wrapper return file line sink file line file line file line permission anyone solution,issue,negative,neutral,neutral,neutral,neutral,neutral
653077111,"Same here for me, it runs on GPU, but CPU is capped out with minor memory consumption on GPU(~150MB). Have you got any hints?",capped minor memory consumption got,issue,negative,negative,neutral,neutral,negative,negative
652155729,"I also meet the same problem as yours. In the deploy environment, when i use uwsgi+flask,  the global connection ""bc"" can't work, but  if i run the flask project as python app.py, it's OK. 
do you resolve this problem?
Thanks.",also meet problem deploy environment use global connection ca work run flask project python resolve problem thanks,issue,negative,positive,neutral,neutral,positive,positive
651579074,"Getting the same on ubuntu 18.04 with tensorflow 2.2.0. 
I can see this was fixed in #549 but helper.py still has tf.logging. ",getting see fixed still,issue,negative,positive,neutral,neutral,positive,positive
650745215,"I had the same issue. The reason might be the lack of memory due to the running servers on the machine. Here is what I did. I found all the pid(s) by the port and port_out of all the previous servings in command line, then mannually killed them. After this, when I run bert-serving-start, the log ends with 'all set, ready to serve request! '",issue reason might lack memory due running machine found port previous command line run log set ready serve request,issue,negative,negative,neutral,neutral,negative,negative
650722805,"Same here. Tried binding different port_in and port_out, not working. ",tried binding different working,issue,negative,neutral,neutral,neutral,neutral,neutral
647567604,"I also have this issue. Why does the system not allow empty strings? This is a perfectly valid input to the BERT model. 

```
line 403, in _check_input_lst_str
    'all elements in the list must be non-empty string, but element %d is %s' % (idx, repr(s)))

all elements in the list must be non-empty string, but element 0 is ''
```",also issue system allow empty perfectly valid input model line list must string element list must string element,issue,positive,positive,positive,positive,positive,positive
646816713,"I need to second what bb969aa did. I was running on mac and despite looking for all the python processes through the terminal CLI I had to open the activity monitor to finally succeed.
",need second running mac despite looking python terminal open activity monitor finally succeed,issue,negative,neutral,neutral,neutral,neutral,neutral
646467876,"The error is still persisting for me:
I:VENTILATOR:[__i:_ru:136]:start the sink
Process BertSink-2:
Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py"", line 297, in _bootstrap
    self.run()
  File ""/Users/yarkoni/Projects/Teva/bert1/venv/lib/python3.7/site-packages/bert_serving/server/__init__.py"", line 282, in run
    self._run()
  File ""/Users/yarkoni/Projects/Teva/bert1/venv/lib/python3.7/site-packages/zmq/decorators.py"", line 75, in wrapper
    return func(*args, **kwargs)
  File ""/Users/yarkoni/Projects/Teva/bert1/venv/lib/python3.7/site-packages/zmq/decorators.py"", line 75, in wrapper
    return func(*args, **kwargs)
  File ""/Users/yarkoni/Projects/Teva/bert1/venv/lib/python3.7/site-packages/zmq/decorators.py"", line 75, in wrapper
    return func(*args, **kwargs)
  File ""/Users/yarkoni/Projects/Teva/bert1/venv/lib/python3.7/site-packages/bert_serving/server/__init__.py"", line 290, in _run
    sender.bind('tcp://*:%d' % self.port)
  File ""zmq/backend/cython/socket.pyx"", line 550, in zmq.backend.cython.socket.Socket.bind
  File ""zmq/backend/cython/checkrc.pxd"", line 26, in zmq.backend.cython.checkrc._check_rc
zmq.error.ZMQError: Address already in use
",error still persisting ventilator start sink process recent call last file line file line run file line wrapper return file line wrapper return file line wrapper return file line file line file line address already use,issue,negative,neutral,neutral,neutral,neutral,neutral
643931388,"Same issue, trying to run the `run_bert_service.sh` script in this project throws the same error. https://github.com/nghuyong/rasa-faq-bot",issue trying run script project error,issue,negative,neutral,neutral,neutral,neutral,neutral
642762572,I have the same problem and have tried numerous different model paths to no success,problem tried numerous different model success,issue,negative,positive,neutral,neutral,positive,positive
641873170,"![image](https://user-images.githubusercontent.com/2041322/84251185-49d76980-ab0d-11ea-8206-10a40ef82ab1.png)

@HuimengZhang 

Please describe the problem like step-by-step. How to reproduce the problem? Otherwise, it doesn't help anybody by posting ""not working"". You know that but still you choose to simply repeat the meaningless reply. How do you expect *anyone* would take the problem seriously based on **NOTHING** but ""在处理8w条数据的时候，处理到一半就停住了/请问你现在这个问题解决了吗/同样有这样的问题，工程挺好的，但不维护的吗？""

Please note that this is an open-source software, which means you have access to the code and you can fix the problem by yourself. So please don't say ""why is nobody maintaining it?"" Just don't.",image please describe problem like reproduce problem otherwise help anybody posting working know still choose simply repeat meaningless reply expect anyone would take problem seriously based nothing please note access code fix problem please say nobody,issue,negative,negative,negative,negative,negative,negative
641692038,"By the way, I am using mac os system. I think it also works for a Linux distribution.",way mac o system think also work distribution,issue,negative,neutral,neutral,neutral,neutral,neutral
638014531,"> yes, your understanding about contextual embedding is wrong. I'd suggest you first read BERT paper before using this package.

Thank for your anwser.I don't consider the postion embedding.",yes understanding contextual wrong suggest first read paper package thank consider,issue,negative,negative,negative,negative,negative,negative
638013311,"your python and system config problem, out of scope.",python system problem scope,issue,negative,neutral,neutral,neutral,neutral,neutral
638012786,feel free to read the source code and docs,feel free read source code,issue,positive,positive,positive,positive,positive,positive
638012273,"yes, your understanding about contextual embedding is wrong. I'd suggest you first read BERT paper before using this package.",yes understanding contextual wrong suggest first read paper package,issue,negative,negative,negative,negative,negative,negative
637814147,"Observed this same thing. Looks like there is a spike at a specific dimension for any word embedding. The specific dimension at which this large value spike occurs varies with the choice of pre-trained weights. Any information on this phenomenon? Is this an artifact of some processing? 

![image](https://user-images.githubusercontent.com/14856287/83571310-64418f80-a4dc-11ea-9b07-8d5eecaeb4a2.png)
![image](https://user-images.githubusercontent.com/14856287/83571390-820ef480-a4dc-11ea-9d3b-02ae6c885453.png)
![image](https://user-images.githubusercontent.com/14856287/83571425-92bf6a80-a4dc-11ea-82fb-19170de30c73.png)
",thing like spike specific dimension word specific dimension large value spike choice information phenomenon artifact image image image,issue,positive,positive,neutral,neutral,positive,positive
636369411,"As per PyTorch doc [here](https://pytorch.org/docs/master/generated/torch.max.html?highlight=torch%20max#torch.max), I think I follow the max operation wherein it selects the maximum value along each dimension.",per doc think follow operation wherein maximum value along dimension,issue,negative,neutral,neutral,neutral,neutral,neutral
635456844,"Hopefully Bert serving can be compatible with TF-2.0, right now, the workaround for me is to serve the bert model in a different server with TF-1.14",hopefully serving compatible right serve model different server,issue,negative,positive,positive,positive,positive,positive
632987269,"thanks




------------------&nbsp;原始邮件&nbsp;------------------
发件人:&nbsp;""Abhijith Nair""<notifications@github.com&gt;;
发送时间:&nbsp;2020年5月23日(星期六) 中午1:06
收件人:&nbsp;""hanxiao/bert-as-service""<bert-as-service@noreply.github.com&gt;;
抄送:&nbsp;""冬天饮雪水""<347963493@qq.com&gt;;""Mention""<mention@noreply.github.com&gt;;
主题:&nbsp;Re: [hanxiao/bert-as-service] How can i fix this problem please (#546)





 
@SlimenBouras @KwokHooh The issue is due to removal of tf.loggingin Tensorflow 2.x , it should work fine on tensorflow 1.15.
 
If you still want to give it a try on TF 2.x, you can try changing tf.logging to tf.compat.v1.logging in bert_serving/server/helper.py but I feel this alone might not fix the issue because you'd still have to make multiple other changes like tf.placeholders etc. which were all removed in Tensorflow 2.x to support eager execution.
 
—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub, or unsubscribe.",thanks mention mention fix problem please issue due removal work fine still want give try try feel alone might fix issue still make multiple like removed support eager execution reply directly view,issue,positive,positive,positive,positive,positive,positive
632986426,"@SlimenBouras @KwokHooh The issue is due to removal of `tf.logging`in Tensorflow 2.x , it should work fine on tensorflow <= 1.15. 

If you still want to give it a try on TF 2.x, you can try changing `tf.logging` to `tf.compat.v1.logging` in `bert_serving/server/helper.py` but I feel this alone might not fix the issue because you'd still have to make multiple other changes like `tf.placeholders` etc. which were all removed in Tensorflow 2.x to support eager execution.",issue due removal work fine still want give try try feel alone might fix issue still make multiple like removed support eager execution,issue,positive,positive,neutral,neutral,positive,positive
632165358,"check if those cpkt file name, config file name of your files is same as your starting server. I got the same problem when starting a server for Biobert. then I realized i should set cpkt_name=""biobert_model.cpkt"" because my cpkt name is not ""bert_model.cpkt"" as default.",check file name file name starting server got problem starting server set name default,issue,negative,neutral,neutral,neutral,neutral,neutral
631417054,"read the log please, it clearly said tf 2.0 is untested",read log please clearly said untested,issue,positive,positive,positive,positive,positive,positive
631416042,"unrelated, check your system setup",unrelated check system setup,issue,negative,neutral,neutral,neutral,neutral,neutral
631219168,@SlimenBouras select from headers to return and press the tab key. It should be inside the function get_embeddings.,select return press tab key inside function,issue,negative,neutral,neutral,neutral,neutral,neutral
631218353,"> @SlimenBouras This code is working for me
> %tensorflow_version 1.x
> !pip install bert-serving-client
> !pip install -U bert-serving-server[http]
> 
> # Download the zip file
> !wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip
> 
> # unzip that file
> !unzip uncased_L-12_H-768_A-12.zip
> !nohup bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 out.file2>&1&
> from bert_serving.client import BertClient
> get_ipython().system_raw(
> 'bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 -http_port 3333 &'
> )
> import json
> import requests
> def get_embeddings(texts):
> headers = {
> 'content-type':'application/json'
> }
> data = {
> ""id"":123,
> ""texts"":texts,
> ""is_tokenized"": False
> }
> data = json.dumps(data)
> r = requests.post(""http://localhost:3333/encode"", data=data, headers=headers).json()
> return r['result']
> embedd = get_embeddings(['Hi, How are you doing?'])

Thanx bro but i have an error
![image](https://user-images.githubusercontent.com/55366848/82402579-b2727f80-9a54-11ea-83b2-6aa18b3950f9.png)
",code working pip install pip install zip file file import import import data id false data data return error image,issue,negative,negative,negative,negative,negative,negative
631215626,"@SlimenBouras This code is working for me 
%tensorflow_version 1.x
!pip install bert-serving-client
!pip install -U bert-serving-server[http]
# Download the zip file
!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip
# unzip that file
!unzip uncased_L-12_H-768_A-12.zip
!nohup bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 out.file2>&1&
from bert_serving.client import BertClient
get_ipython().system_raw(
    'bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 -http_port 3333 &'
)
import json
import requests
def get_embeddings(texts):
    headers = {
        'content-type':'application/json'
    }
    data = {
        ""id"":123,
        ""texts"":texts,
        ""is_tokenized"": False
    }
    data = json.dumps(data)
    r = requests.post(""http://localhost:3333/encode"", data=data, headers=headers).json()
    return r['result']
embedd = get_embeddings(['Hi, How are you doing?'])",code working pip install pip install zip file file import import import data id false data data return,issue,negative,negative,negative,negative,negative,negative
631212946,"> @SlimenBouras Hi, I am also getting infinite runtime for bc = BertClient().
> My code is following:
> GPU instance
> %tensorflow_version 1.x
> !pip install bert-serving-client
> !pip install -U bert-serving-server[http]
> !wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip
> !unzip uncased_L-12_H-768_A-12.zip
> !nohup bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 out.file2>&1&
> from bert_serving.client import BertClient
> bs = BertClient()
> Kindly help!!

Same problem for me bro",hi also getting infinite code following instance pip install pip install import kindly help problem,issue,negative,positive,positive,positive,positive,positive
630992072,"@SlimenBouras Hi, I am also getting infinite runtime for bc = BertClient().
My code is following:
GPU instance
%tensorflow_version 1.x
!pip install bert-serving-client
!pip install -U bert-serving-server[http]
!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip
!unzip uncased_L-12_H-768_A-12.zip
!nohup bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 out.file2>&1&
from bert_serving.client import BertClient
bs = BertClient()
Kindly help!!",hi also getting infinite code following instance pip install pip install import kindly help,issue,positive,positive,positive,positive,positive,positive
630863873,"> Thank you everyone. I was finally able to fix this issue. I used this GPU and this command to change my tensorflow version.
> %tensorflow_version 1.x

Can u share for us your code please ",thank everyone finally able fix issue used command change version share u code please,issue,positive,positive,positive,positive,positive,positive
630584168,"Well, figured out a solution:
https://github.com/hanxiao/bert-as-service/issues/412

No way it was going to work on cloudrun with a max of 2gigs mem. Needed 52! ",well figured solution way going work mem,issue,positive,neutral,neutral,neutral,neutral,neutral
630581038,"I was having a heck of a time getting this working on GCP. Tried every option they've got. Was also working fine on my local machine (32g ram, 3.5 GHz Intel Core i5, 4 cores). Thanks to the suggestion above, it seems to be working reliably now: 

GCP: Compute Engine: VM 
n1-highmem-8 (8 vCPUs, 52 GB memory)

Wish I could make it work in the various container services... but no luck there. Can't remember if i tried exactly the same config there though... will need to give that a try. Don't think i went up as high as 52gb. ",heck time getting working tried every option got also working fine local machine ram core thanks suggestion working reliably compute engine memory wish could make work various container luck ca remember tried exactly though need give try think went high,issue,positive,positive,positive,positive,positive,positive
630199367,"same issue here. wondering if it's keeping sockets open. looks like the socket list grows pretty quick and doesnt come back down. here's the health check from right before it went down:

> 
{""ckpt_name"":""bert_model.ckpt"",""client"":""1b8cb6cd-3b2f-4ade-80f8-e8eb01298c14"",""config_name"":""bert_config.json"",""cors"":""*"",""cpu"":false,""device_map"":[],""fixed_embed_length"":false,""fp16"":false,""gpu_memory_fraction"":0.5,""graph_tmp_dir"":null,""http_max_connect"":10,""http_port"":8080,""mask_cls_sep"":false,""max_batch_size"":256,""max_seq_len"":25,""model_dir"":""models/uncased_L-12_H-768_A-12"",""num_concurrent_socket"":30,""num_process"":17,""num_worker"":15,""pooling_layer"":[-2],""pooling_strategy"":2,""port"":5555,""port_out"":5556,""prefetch_size"":10,""priority_batch_size"":16,""python_version"":""3.6.3 (default, Jul  9 2019, 08:50:08) \n[GCC 7.3.1 20180303 (Red Hat 7.3.1-5)]"",""pyzmq_version"":""19.0.1"",""server_current_time"":""2020-05-18 13:32:43.888630"",""server_start_time"":""2020-05-18 13:15:37.862617"",""server_version"":""1.8.9"",""show_tokens_to_client"":false,""statistic"":{""avg_last_two_interval"":167.3810899715,""avg_request_per_client"":12.0,""avg_request_per_second"":0.034329692637323224,""avg_size_per_request"":2.0,""max_last_two_interval"":517.0026954719999,""max_request_per_client"":12,""max_request_per_second"":0.10548447649600348,""max_size_per_request"":3,""min_last_two_interval"":9.480067903999952,""min_request_per_client"":12,""min_request_per_second"":0.0019342258923564133,""min_size_per_request"":1,""num_active_client"":0,""num_data_request"":4,""num_max_last_two_interval"":1,""num_max_request_per_client"":1,""num_max_request_per_second"":1,""num_max_size_per_request"":1,""num_min_last_two_interval"":1,""num_min_request_per_client"":1,""num_min_request_per_second"":1,""num_min_size_per_request"":1,""num_sys_request"":8,""num_total_client"":1,""num_total_request"":12,""num_total_seq"":8},""status"":200,""tensorflow_version"":[""1"",""11"",""0""],""tuned_model_dir"":null,""ventilator -> worker"":[""ipc://tmp3fYAAK/socket"",""ipc://tmpG06X52/socket"",""ipc://tmpAZqmBl/socket"",""ipc://tmpYIBL6D/socket"",""ipc://tmp8EubCW/socket"",""ipc://tmpzm5B7e/socket"",""ipc://tmpZmr3Cx/socket"",""ipc://tmpAWuv8P/socket"",""ipc://tmpJWeYD8/socket"",""ipc://tmpPGVr9q/socket"",""ipc://tmpLelWEJ/socket"",""ipc://tmpBTtra2/socket"",""ipc://tmpfmwXFk/socket"",""ipc://tmpFQ0ubD/socket"",""ipc://tmpeKZ3GV/socket"",""ipc://tmp0nSDce/socket"",""ipc://tmpuJCeIw/socket"",""ipc://tmpYDhQdP/socket"",""ipc://tmppWMsJ7/socket"",""ipc://tmpQO75eq/socket"",""ipc://tmpLgnKKI/socket"",""ipc://tmpAKzpg1/socket"",""ipc://tmpDHC5Lj/socket"",""ipc://tmphWYMhC/socket"",""ipc://tmpPjGvNU/socket"",""ipc://tmpTikfjd/socket"",""ipc://tmpYdRZOv/socket"",""ipc://tmpZIfLkO/socket"",""ipc://tmpUovxQ6/socket"",""ipc://tmp8TDkmp/socket""],""ventilator <-> sink"":""ipc://tmpcsCe5r/socket"",""verbose"":false,""worker -> sink"":""ipc://tmpKaNt2G/socket"",""xla"":false,""zmq_version"":""4.3.2""}",issue wondering keeping open like socket list pretty quick doesnt come back health check right went client false false false null default red hat false statistic status null ventilator worker ventilator sink verbose false worker sink false,issue,positive,negative,negative,negative,negative,negative
630022063,"Thank you for sharing the code. I find that my problem is because I use
```
from google.colab import drive
drive.mount('/content/gdrive')
```
to mount my google drive and then start a server.
But I am still confused, if I can not run the server on my drive, what if I want to serve on the fine-tune model of my own? Maybe the only solution is to upload the model and wget it? Although I still don't understand why I can't run the server after mounting my drive...",thank code find problem use import drive mount drive start server still confused run server drive want serve model maybe solution model although still understand ca run server mounting drive,issue,negative,negative,negative,negative,negative,negative
629835513,"I couldnt fixed it but this colab notebook runs bert as a service correctly-
https://colab.research.google.com/drive/1Lg92UAzuqSItBLbG9STprz-iiXIlkltQ?usp=sharing
Thanks to Aashik Saibabu for the code",fixed notebook service thanks code,issue,negative,positive,positive,positive,positive,positive
629738124,"It didn't work to me even without nohup. I have changed to tensorflow 1.x by
```
%tensorflow_version 1.x
```
and I am on a GPU instance.
But when I run the server, it just throws
```
zmq.error.ZMQError: Function not implemented
```
as #543 does.
By the way, I only have this problem on colab, but works well on Windows.
Has anyone met the same problem and worked it out? Appreciate a lot!",work even without instance run server function way problem work well anyone met problem worked appreciate lot,issue,negative,neutral,neutral,neutral,neutral,neutral
629736515,"Hi, I have the same problem. Have you found any solution?
Thank you.",hi problem found solution thank,issue,negative,neutral,neutral,neutral,neutral,neutral
629508867,"Thank you everyone. I was finally able to fix this issue. I used this GPU and this command to change my tensorflow version.
%tensorflow_version 1.x",thank everyone finally able fix issue used command change version,issue,negative,positive,positive,positive,positive,positive
628942557,"> > Still the same problem for me !
> 
> @SlimenBouras Make sure to change run type to GPU before running any command.
> 
> Regards,
> Iknoor

yeah i change it look
![image](https://user-images.githubusercontent.com/55366848/81996406-d2650600-9644-11ea-9dec-18fdf07456c5.png)
",still problem make sure change run type running command yeah change look image,issue,negative,positive,positive,positive,positive,positive
628938831,"> Still the same problem for me !

@SlimenBouras Make sure to change run type to GPU before running any command.

Regards,
Iknoor",still problem make sure change run type running command,issue,negative,positive,positive,positive,positive,positive
628937921,"> @iknoorjobs i have a lot of other stuff in that notebook so I can't share. I would need to create a clean new one. But first let me try to help you. So a few questions:
> 
> * Does it work to you without nohup? You can't interact then in the notebook but you see the log messages
> * Are you running tensorflow 1.x?
> * Are you on a GPU instance?
> 
> Kind regads,
> 
> Dirk

@DHOFM Finally got the problem. GPU instance. XD
Thanks man. :)

Regards, 
Iknoor",lot stuff notebook ca share would need create clean new one first let try help work without ca interact notebook see log running instance kind dirk finally got problem instance thanks man,issue,positive,positive,positive,positive,positive,positive
628742446,"@iknoorjobs i have a lot of other stuff in that notebook so I can't share. I would need to create a clean new one. But first let me try to help you. So a few questions:

- Does it work to you without nohup? You can't interact then in the notebook but you see the log messages

- Are you running tensorflow 1.x?

- Are you on a GPU instance?

Kind regads,

Dirk",lot stuff notebook ca share would need create clean new one first let try help work without ca interact notebook see log running instance kind dirk,issue,positive,positive,positive,positive,positive,positive
628650454,"> iknoorjobs, please look for the spaces and directory address in '!nohup bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 > out.file 2>&1 &'
> 
> It was initially not running for me but it started working for me after I fixed the issues with spaces and directory path.

Hi @GaneshGS @DHOFM 
It is still not working and I am not sure what you are trying to say. I have checked the directories and all seem to be okay. Why don't you share your colab notebook if it's working for you? It'll be helpful to all.
Thanks.",please look directory address initially running working fixed directory path hi still working sure trying say checked seem share notebook working helpful thanks,issue,positive,positive,positive,positive,positive,positive
628056558,"Of course, after many hours of trying I got it working right after posting the issue:
docker run -p 8000:8125 MYID

suppose just a docker newbie error. ID needs to go after port assignment? ",course many trying got working right posting issue docker run suppose docker error id need go port assignment,issue,negative,positive,positive,positive,positive,positive
628055957,"Of course, after many hours of trying I got it working right after posting with:
docker run -p 8000:8125 MYID",course many trying got working right posting docker run,issue,negative,positive,positive,positive,positive,positive
627855395,"I also encountered this problem, do not know how to solve it",also problem know solve,issue,negative,neutral,neutral,neutral,neutral,neutral
627664876,"ooo, i just got the flask message. but still, port is not found...

<img width=""651"" alt=""Screen Shot 2020-05-12 at 5 05 09 PM"" src=""https://user-images.githubusercontent.com/19869387/81757214-fe309200-9472-11ea-9f32-2063131ba612.png"">
",got flask message still port found screen shot,issue,negative,neutral,neutral,neutral,neutral,neutral
627646109,"Same issue here via docker setup. Using: 
bert-serving-server==1.8.9
bert-serving-client==1.8.9
(I did get it working from a non-docker install).

Weird thing is that everything looks good. Health check is running... just can't see the http port.

<img width=""820"" alt=""Screen Shot 2020-05-12 at 4 19 46 PM"" src=""https://user-images.githubusercontent.com/19869387/81755010-721b6c00-946c-11ea-9bfc-4734f52478f5.png"">


<img width=""504"" alt=""Screen Shot 2020-05-12 at 4 04 10 PM"" src=""https://user-images.githubusercontent.com/19869387/81754128-3da6b080-946a-11ea-8912-21bf22b72a89.png"">

I don't see that ""*Serving Flask app"" message though...?
If I sh into the docker container, the port responds... hm.
Tried exposing the port in Dockerfile
Also running with: docker run MYID -p localhost:8125:8125
",issue via docker setup get working install weird thing everything good health check running ca see port screen shot screen shot see serving flask message though sh docker container port tried port also running docker run,issue,negative,positive,neutral,neutral,positive,positive
627006032,"![image](https://user-images.githubusercontent.com/2041322/81619470-f678db00-93e9-11ea-9ce1-2352ee3d6335.png)

@askaydevs You may delete your comment and wish the internet forget what you did. I hope you learn to respect other open source work and be responsible of what you did.",image may delete comment wish forget hope learn respect open source work responsible,issue,positive,positive,neutral,neutral,positive,positive
626872751,"@hanxiao My sincere apologies for that earlier comment. I was just starting with BERT and at that moment there were not enough resources to learn about BERT and also I was an amateur, after giving numerous unsuccessful tries I got frustrated and wrote that; didn't mean any of that. I totally understand you are doing exceptional job to keep things clean and bug free. Again I am sorry @hanxiao.",sincere comment starting moment enough learn also amateur giving numerous unsuccessful got wrote mean totally understand exceptional job keep clean bug free sorry,issue,positive,positive,neutral,neutral,positive,positive
626865633,"> Stop writing shitty answers and give a solution which actually solves the problem. In my case, I don't have any executables with the name bert-serving-start.

@askaydevs You should be really shamed of yourself!",stop writing give solution actually problem case name really shamed,issue,negative,positive,neutral,neutral,positive,positive
626840320,"You should also uninstall tensorfflow-estimator if its version > 2 
pip uninstall tensorflow 
pip uninstall tensorflow-estimator 
and then
pip install tensorflow=1.13.1",also version pip pip pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
626578816,"@sky94520  - example4.py and example5.py use the same data, which can be downloaded from here: https://github.com/thunlp/CAIL/blob/master/README_en.md#23-dataset-download

You will see the same directory structure after decompressing it",sky use data see directory structure decompressing,issue,negative,neutral,neutral,neutral,neutral,neutral
626457202,"same issue here bert client is not starting, the execution kept running but no results
",issue client starting execution kept running,issue,negative,neutral,neutral,neutral,neutral,neutral
626402889,"Hello, did the problem solved eventually. I have followed the steps above. Client is still not starting. Any help is appreciated.",hello problem eventually client still starting help,issue,negative,neutral,neutral,neutral,neutral,neutral
626270242,@dawnduan did not face that problem. How long does it take? Like twice?,face problem long take like twice,issue,negative,negative,neutral,neutral,negative,negative
625891009,"same issue on my server.
I am using a ubuntu server with 4 gpus, and the tensorflow version is 1.15.
My CPU utilization is capped out, and GPU utillization is practically 0.",issue server server version utilization capped practically,issue,negative,neutral,neutral,neutral,neutral,neutral
625870128,"> Hi,
> 
> It might have something to do with the tensorflow version. I had a similar issue, `tf>1.15` is not tested. Did you get a warning saying that version of tensorflow is not supported?

Thank you for your answer
no I didn't get this message, did you get the visualization of the layers or not?",hi might something version similar issue tested get warning saying version thank answer get message get visualization,issue,negative,neutral,neutral,neutral,neutral,neutral
625757828,"Hi,

It might have something to do with the tensorflow version. I had a similar issue, `tf>1.15` is not tested. Did you get a warning saying that version of tensorflow is not supported?",hi might something version similar issue tested get warning saying version,issue,negative,neutral,neutral,neutral,neutral,neutral
625700151,"Resolved the issue
using tensorflow 1.14.0 wont cause this issue.",resolved issue wont cause issue,issue,negative,neutral,neutral,neutral,neutral,neutral
624205791,"> SilmenBouras, Is the problem solved? Is the BertClient() working??? On Tue, May 5, 2020 at 1:07 PM SlimenBouras <notifications@github.com> wrote:
> […](#)
> for first two lines, you used 'pip', change it to '!pip' On Tue, May 5, 2020 at 12:32 PM SlimenBouras ***@***.*** wrote: … <#m_-5811634594184665497_> Hi, Please follow the instructions. 1. uninstall 'tensorflow-gpu==1.12.0' and install 'tensorflow==1.15.0' 2. replace the !nohup command in your code 6 with the one below '!nohup bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 out.file2>&1&' 3. type !ls and check the directory whether you can see unzipped 'uncased_L- 12_H-768_A-12' file in the directory or not. Now, you run it and get back to me. … <#m_8545851393258997135_> On Tue, May 5, 2020 at 11:22 AM SlimenBouras *@*.***> wrote: I have the same problem here with bc=BertClient() [image: image] https://user-images.githubusercontent.com/55366848/81083543-9fa46a80-8eec-11ea-944a-f2a07d58c7e1.png — You are receiving this because you commented. Reply to this email directly, view it on GitHub <#380 <#380> (comment) <#380 (comment) <[#380 (comment)](https://github.com/hanxiao/bert-as-service/issues/380#issuecomment-624120929)>>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AHEJQJX4CBCFQXLSMAQJTG3RQAVNDANCNFSM4HX4JNLQ . -- Dr. Ganesh Sinisetty NLP Developer and Research Scientist Prologue AI Montreal, Quebec, Canada thank you for answering me, i applied your instructions, how long should i wait? [image: image] https://user-images.githubusercontent.com/55366848/81090892-535e2800-8ef6-11ea-8cab-ed0832abb6d6.png — You are receiving this because you commented. Reply to this email directly, view it on GitHub <#380 (comment) <[#380 (comment)](https://github.com/hanxiao/bert-as-service/issues/380#issuecomment-624160983)>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AHEJQJVBHQPNF2SKA336D33RQA5RXANCNFSM4HX4JNLQ . -- Dr. Ganesh Sinisetty NLP Developer and Research Scientist Prologue AI Montreal, Quebec, Canada thanks again can i have your mail ? — You are receiving this because you commented. Reply to this email directly, view it on GitHub <[#380 (comment)](https://github.com/hanxiao/bert-as-service/issues/380#issuecomment-624182650)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AHEJQJVJXWGI443LIOIAPWLRQBBULANCNFSM4HX4JNLQ> .
> -- Dr. Ganesh Sinisetty NLP Developer and Research Scientist Prologue AI Montreal, Quebec, Canada

Still the same problem , i sent you an e-mail",problem working tue may wrote first two used change pip tue may wrote hi please follow install replace command code one type check directory whether see file directory run get back tue may wrote problem image image reply directly view comment comment comment developer research scientist prologue ai canada thank applied long wait image image reply directly view comment comment developer research scientist prologue ai canada thanks mail reply directly view comment developer research scientist prologue ai canada still problem sent,issue,negative,positive,neutral,neutral,positive,positive
624204409,"SilmenBouras,

Is the problem solved? Is the BertClient() working???

On Tue, May 5, 2020 at 1:07 PM SlimenBouras <notifications@github.com>
wrote:

> for first two lines, you used 'pip', change it to '!pip' On Tue, May 5,
> 2020 at 12:32 PM SlimenBouras notifications@github.com wrote:
> … <#m_-5811634594184665497_>
> Hi, Please follow the instructions. 1. uninstall 'tensorflow-gpu==1.12.0'
> and install 'tensorflow==1.15.0' 2. replace the !nohup command in your code
> 6 with the one below '!nohup bert-serving-start
> -model_dir=./uncased_L-12_H-768_A-12 out.file2>&1&' 3. type !ls and check
> the directory whether you can see unzipped 'uncased_L- 12_H-768_A-12' file
> in the directory or not. Now, you run it and get back to me. …
> <#m_8545851393258997135_> On Tue, May 5, 2020 at 11:22 AM SlimenBouras *@*.***>
> wrote: I have the same problem here with bc=BertClient() [image: image]
> https://user-images.githubusercontent.com/55366848/81083543-9fa46a80-8eec-11ea-944a-f2a07d58c7e1.png
> — You are receiving this because you commented. Reply to this email
> directly, view it on GitHub <#380
> <https://github.com/hanxiao/bert-as-service/issues/380> (comment) <#380
> (comment)
> <https://github.com/hanxiao/bert-as-service/issues/380#issuecomment-624120929>>>,
> or unsubscribe
> https://github.com/notifications/unsubscribe-auth/AHEJQJX4CBCFQXLSMAQJTG3RQAVNDANCNFSM4HX4JNLQ
> . -- Dr. Ganesh Sinisetty NLP Developer and Research Scientist Prologue AI
> Montreal, Quebec, Canada thank you for answering me, i applied your
> instructions, how long should i wait? [image: image]
> https://user-images.githubusercontent.com/55366848/81090892-535e2800-8ef6-11ea-8cab-ed0832abb6d6.png
> — You are receiving this because you commented. Reply to this email
> directly, view it on GitHub <#380 (comment)
> <https://github.com/hanxiao/bert-as-service/issues/380#issuecomment-624160983>>,
> or unsubscribe
> https://github.com/notifications/unsubscribe-auth/AHEJQJVBHQPNF2SKA336D33RQA5RXANCNFSM4HX4JNLQ
> .
> -- Dr. Ganesh Sinisetty NLP Developer and Research Scientist Prologue AI
> Montreal, Quebec, Canada
>
> thanks again
> can i have your mail ?
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/hanxiao/bert-as-service/issues/380#issuecomment-624182650>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AHEJQJVJXWGI443LIOIAPWLRQBBULANCNFSM4HX4JNLQ>
> .
>


-- 
Dr. Ganesh Sinisetty
NLP Developer and Research Scientist
Prologue AI
Montreal, Quebec, Canada
",problem working tue may wrote first two used change pip tue may wrote hi please follow install replace command code one type check directory whether see file directory run get back tue may wrote problem image image reply directly view comment comment developer research scientist prologue ai canada thank applied long wait image image reply directly view comment developer research scientist prologue ai canada thanks mail reply directly view developer research scientist prologue ai canada,issue,negative,positive,neutral,neutral,positive,positive
624201482,"> > for first two lines, you used 'pip', change it to '!pip' On Tue, May 5, 2020 at 12:32 PM SlimenBouras [notifications@github.com](mailto:notifications@github.com) wrote:
> > […](#)
> > Hi, Please follow the instructions. 1. uninstall 'tensorflow-gpu==1.12.0' and install 'tensorflow==1.15.0' 2. replace the !nohup command in your code 6 with the one below '!nohup bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 out.file2>&1&' 3. type !ls and check the directory whether you can see unzipped 'uncased_L- 12_H-768_A-12' file in the directory or not. Now, you run it and get back to me. … <#m_8545851393258997135_> On Tue, May 5, 2020 at 11:22 AM SlimenBouras _@_.***> wrote: I have the same problem here with bc=BertClient() [image: image] https://user-images.githubusercontent.com/55366848/81083543-9fa46a80-8eec-11ea-944a-f2a07d58c7e1.png — You are receiving this because you commented. Reply to this email directly, view it on GitHub <#380 (comment) <[#380 (comment)](https://github.com/hanxiao/bert-as-service/issues/380#issuecomment-624120929)>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AHEJQJX4CBCFQXLSMAQJTG3RQAVNDANCNFSM4HX4JNLQ . -- Dr. Ganesh Sinisetty NLP Developer and Research Scientist Prologue AI Montreal, Quebec, Canada thank you for answering me, i applied your instructions, how long should i wait? [image: image] https://user-images.githubusercontent.com/55366848/81090892-535e2800-8ef6-11ea-8cab-ed0832abb6d6.png — You are receiving this because you commented. Reply to this email directly, view it on GitHub <[#380 (comment)](https://github.com/hanxiao/bert-as-service/issues/380#issuecomment-624160983)>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AHEJQJVBHQPNF2SKA336D33RQA5RXANCNFSM4HX4JNLQ .
> > -- Dr. Ganesh Sinisetty NLP Developer and Research Scientist Prologue AI Montreal, Quebec, Canada
> 
> thanks again
> can i have your mail ?

SlimenBouras, please find my e-mail: gane.eflu@gmail.com",first two used change pip tue may wrote hi please follow install replace command code one type check directory whether see file directory run get back tue may wrote problem image image reply directly view comment comment developer research scientist prologue ai canada thank applied long wait image image reply directly view comment developer research scientist prologue ai canada thanks mail please find,issue,positive,positive,positive,positive,positive,positive
624182650,"> for first two lines, you used 'pip', change it to '!pip' On Tue, May 5, 2020 at 12:32 PM SlimenBouras <notifications@github.com> wrote:
> […](#)
> Hi, Please follow the instructions. 1. uninstall 'tensorflow-gpu==1.12.0' and install 'tensorflow==1.15.0' 2. replace the !nohup command in your code 6 with the one below '!nohup bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 out.file2>&1&' 3. type !ls and check the directory whether you can see unzipped 'uncased_L- 12_H-768_A-12' file in the directory or not. Now, you run it and get back to me. … <#m_8545851393258997135_> On Tue, May 5, 2020 at 11:22 AM SlimenBouras *@*.***> wrote: I have the same problem here with bc=BertClient() [image: image] https://user-images.githubusercontent.com/55366848/81083543-9fa46a80-8eec-11ea-944a-f2a07d58c7e1.png — You are receiving this because you commented. Reply to this email directly, view it on GitHub <#380 (comment) <[#380 (comment)](https://github.com/hanxiao/bert-as-service/issues/380#issuecomment-624120929)>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AHEJQJX4CBCFQXLSMAQJTG3RQAVNDANCNFSM4HX4JNLQ . -- Dr. Ganesh Sinisetty NLP Developer and Research Scientist Prologue AI Montreal, Quebec, Canada thank you for answering me, i applied your instructions, how long should i wait? [image: image] <https://user-images.githubusercontent.com/55366848/81090892-535e2800-8ef6-11ea-8cab-ed0832abb6d6.png> — You are receiving this because you commented. Reply to this email directly, view it on GitHub <[#380 (comment)](https://github.com/hanxiao/bert-as-service/issues/380#issuecomment-624160983)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AHEJQJVBHQPNF2SKA336D33RQA5RXANCNFSM4HX4JNLQ> .
> -- Dr. Ganesh Sinisetty NLP Developer and Research Scientist Prologue AI Montreal, Quebec, Canada

thanks again 
can i have your mail ?",first two used change pip tue may wrote hi please follow install replace command code one type check directory whether see file directory run get back tue may wrote problem image image reply directly view comment comment developer research scientist prologue ai canada thank applied long wait image image reply directly view comment developer research scientist prologue ai canada thanks mail,issue,positive,positive,positive,positive,positive,positive
624176325,"for first two lines, you used 'pip', change it to '!pip'

On Tue, May 5, 2020 at 12:32 PM SlimenBouras <notifications@github.com>
wrote:

> Hi, Please follow the instructions. 1. uninstall 'tensorflow-gpu==1.12.0'
> and install 'tensorflow==1.15.0' 2. replace the !nohup command in your code
> 6 with the one below '!nohup bert-serving-start
> -model_dir=./uncased_L-12_H-768_A-12
> out.file2>&1&'
> 3. type !ls and check the directory whether you can see unzipped
> 'uncased_L- 12_H-768_A-12' file in the directory or not. Now, you run it
> and get back to me.
> … <#m_8545851393258997135_>
> On Tue, May 5, 2020 at 11:22 AM SlimenBouras *@*.***> wrote: I have the
> same problem here with bc=BertClient() [image: image]
> https://user-images.githubusercontent.com/55366848/81083543-9fa46a80-8eec-11ea-944a-f2a07d58c7e1.png
> — You are receiving this because you commented. Reply to this email
> directly, view it on GitHub <#380 (comment)
> <https://github.com/hanxiao/bert-as-service/issues/380#issuecomment-624120929>>,
> or unsubscribe
> https://github.com/notifications/unsubscribe-auth/AHEJQJX4CBCFQXLSMAQJTG3RQAVNDANCNFSM4HX4JNLQ
> .
> -- Dr. Ganesh Sinisetty NLP Developer and Research Scientist Prologue AI
> Montreal, Quebec, Canada
>
> thank you for answering me, i applied your instructions, how long should i
> wait?
> [image: image]
> <https://user-images.githubusercontent.com/55366848/81090892-535e2800-8ef6-11ea-8cab-ed0832abb6d6.png>
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/hanxiao/bert-as-service/issues/380#issuecomment-624160983>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AHEJQJVBHQPNF2SKA336D33RQA5RXANCNFSM4HX4JNLQ>
> .
>


-- 
Dr. Ganesh Sinisetty
NLP Developer and Research Scientist
Prologue AI
Montreal, Quebec, Canada
",first two used change tue may wrote hi please follow install replace command code one type check directory whether see file directory run get back tue may wrote problem image image reply directly view comment developer research scientist prologue ai canada thank applied long wait image image reply directly view developer research scientist prologue ai canada,issue,negative,positive,neutral,neutral,positive,positive
624160983,"> Hi, Please follow the instructions. 1. uninstall 'tensorflow-gpu==1.12.0' and install 'tensorflow==1.15.0' 2. replace the !nohup command in your code 6 with the one below '!nohup bert-serving-start -model_dir=./uncased_L-12_H-768_A-12
> out.file2>&1&'
> 3. type !ls and check the directory whether you can see unzipped 'uncased_L- 12_H-768_A-12' file in the directory or not. Now, you run it and get back to me.
> […](#)
> On Tue, May 5, 2020 at 11:22 AM SlimenBouras ***@***.***> wrote: I have the same problem here with bc=BertClient() [image: image] <https://user-images.githubusercontent.com/55366848/81083543-9fa46a80-8eec-11ea-944a-f2a07d58c7e1.png> — You are receiving this because you commented. Reply to this email directly, view it on GitHub <[#380 (comment)](https://github.com/hanxiao/bert-as-service/issues/380#issuecomment-624120929)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AHEJQJX4CBCFQXLSMAQJTG3RQAVNDANCNFSM4HX4JNLQ> .
> -- Dr. Ganesh Sinisetty NLP Developer and Research Scientist Prologue AI Montreal, Quebec, Canada

thank you for answering me, i applied your instructions, how long should i wait?
![image](https://user-images.githubusercontent.com/55366848/81090892-535e2800-8ef6-11ea-8cab-ed0832abb6d6.png)
",hi please follow install replace command code one type check directory whether see file directory run get back tue may wrote problem image image reply directly view comment developer research scientist prologue ai canada thank applied long wait image,issue,negative,positive,neutral,neutral,positive,positive
624151162,"Hi,

Please follow the instructions.

1. uninstall 'tensorflow-gpu==1.12.0' and install 'tensorflow==1.15.0'
2. replace the !nohup command in your code 6 with the one below
   '!nohup bert-serving-start -model_dir=./uncased_L-12_H-768_A-12
>out.file2>&1&'

3. type !ls and check the directory whether you can see unzipped 'uncased_L-
12_H-768_A-12' file in the directory or not.

Now, you run it and get back to me.



On Tue, May 5, 2020 at 11:22 AM SlimenBouras <notifications@github.com>
wrote:

> I have the same problem here with bc=BertClient()
> [image: image]
> <https://user-images.githubusercontent.com/55366848/81083543-9fa46a80-8eec-11ea-944a-f2a07d58c7e1.png>
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/hanxiao/bert-as-service/issues/380#issuecomment-624120929>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AHEJQJX4CBCFQXLSMAQJTG3RQAVNDANCNFSM4HX4JNLQ>
> .
>


-- 
Dr. Ganesh Sinisetty
NLP Developer and Research Scientist
Prologue AI
Montreal, Quebec, Canada
",hi please follow install replace command code one type check directory whether see file directory run get back tue may wrote problem image image reply directly view developer research scientist prologue ai canada,issue,negative,positive,neutral,neutral,positive,positive
624128553,"Hi Team, I am also getting same issue - do you have any update on this?",hi team also getting issue update,issue,negative,neutral,neutral,neutral,neutral,neutral
624117389,"> iknoorjobs, please look for the spaces and directory address in '!nohup bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 > out.file 2>&1 &'
> 
> It was initially not running for me but it started working for me after I fixed the issues with spaces and directory path.

Can you help me please , i have the same problem , i fixed the spaces but dosen't worked for me 
![image](https://user-images.githubusercontent.com/55366848/81083052-ffe6dc80-8eeb-11ea-83e6-a5bcdace246b.png)

",please look directory address initially running working fixed directory path help please problem fixed dose worked image,issue,positive,positive,neutral,neutral,positive,positive
624080871,"iknoorjobs, please look for the spaces and directory address in  '!nohup bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 > out.file 2>&1 &'

It was initially not running for me but it started working for me after I fixed the issues with spaces and directory path.",please look directory address initially running working fixed directory path,issue,negative,positive,neutral,neutral,positive,positive
623928170,"uninstall the latest version and install this version 1.9.8 and try again.

Clear the tf.graph() or kill the background service.
If not restart the machine.",latest version install version try clear kill background service restart machine,issue,negative,positive,positive,positive,positive,positive
622439804,"Hi,
I had another problem...
I installed correctly bert-serving-server and bert-serving client and I downloaded the multilingual model that I uncompressed in the same folder on my code.
But when i run
bert-serving-start -model_dir /tmp/multi_cased_L-12_H-768_A-12/ -num_worker=1
it launches the following error:
SyntaxError: invalid token.
Could you help me?
Thank you so much",hi another problem correctly client multilingual model uncompressed folder code run following error invalid token could help thank much,issue,negative,positive,neutral,neutral,positive,positive
621326484,"> Got it running using Bert Client and nohup - here is how it worked for me:

> !pip install bert-serving-client
> !pip install -U bert-serving-server[HTTP]

> !wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip
> !unzip uncased_L-12_H-768_A-12.zip
> !nohup bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 > out.file 2>&1 &

> from bert_serving.client import BertClient
> bc = BertClient()
> print (bc.encode(['First do it', 'then do it right', 'then do it better']))

@DHOFM This isn't working anymore. When I run the following code, it goes into infinite loop and does not executes. Changing to Tf version 1.x also does not work.

`bc = BertClient()
`
",got running client worked pip install pip install import print right better working run following code go infinite loop version also work,issue,negative,positive,positive,positive,positive,positive
621049037,"After debugging, I found that when this error occurs, the response format returned by the server is different from the normal response format",found error response format returned server different normal response format,issue,negative,positive,neutral,neutral,positive,positive
620958631,"> Hi @Jasperty
> Here is my implementation (https://github.com/duongkstn/albert-vi-as-service), which is same as [bert-as-service](https://github.com/hanxiao/bert-as-service). I added some albert file to server/bert folder. Although this is just for experiment, you can easily start by following:
> 
> * git clone my project
> * cd albert_vi_as_service/server
> * python setup.py install
> * use `albert-vi-serving-start` command as `bert-serving-start`. But it is noted that you have to declare sentence piece model by option: `-spm_model_file`
> * At client side, you use bertClient as normal (`pip install bert-serving-client==1.10.0`) (NOTE: do not use client of my `albert-vi-as-service`)
> 
> Give me a star and feel free to contribute if it's helpful to you <3.
> (I named it 's 'albert-vi' since my purpose is to build a server of [albert_vi](https://github.com/ngoanpv/albert_vi)


**啥是  sentence piece model？？**",hi implementation added file folder although experiment easily start following git clone project python install use command noted declare sentence piece model option client side use normal pip install note use client give star feel free contribute helpful since purpose build server sentence piece,issue,positive,positive,positive,positive,positive,positive
620502938,"Hi @Jasperty 
Here is my implementation (https://github.com/duongkstn/albert-vi-as-service), which is same as [bert-as-service](https://github.com/hanxiao/bert-as-service). I added some albert file to server/bert folder. Although this is just for experiment, you can easily start by following:
- git clone my project
- cd albert_vi_as_service/server
- python setup.py install
- use `albert-vi-serving-start` command as `bert-serving-start`. But it is noted that you have to declare sentence piece model by option: `-spm_model_file`

- At client side, you use bertClient as normal (`pip install bert-serving-client==1.10.0`) (NOTE: do not use client of my `albert-vi-as-service`)

Give me a star and  feel free to contribute if it's helpful to you <3.
(I named it 's 'albert-vi' since my purpose is to build a server of [albert_vi](https://github.com/ngoanpv/albert_vi)",hi implementation added file folder although experiment easily start following git clone project python install use command noted declare sentence piece model option client side use normal pip install note use client give star feel free contribute helpful since purpose build server,issue,positive,positive,positive,positive,positive,positive
619678950,"I can't use TensorFlow 2, because I don't want to change my version of rasa, so I want to solve this with a more appropriate method.",ca use want change version rasa want solve appropriate method,issue,negative,positive,positive,positive,positive,positive
619401063,The problem is with TF 2.0. I tested and it work fine on tf 1.15 but not on tf 2.0,problem tested work fine,issue,negative,positive,positive,positive,positive,positive
618843063,"> For me all works from DHOFM's answer before line ""bc = BertClient()""
> In this part a block loading infinite time without warning throwing or else info.
> Someone in the know about this?

Changing Tensorflow version to 1 worked for me",work answer line part block loading infinite time without warning throwing else someone know version worked,issue,negative,neutral,neutral,neutral,neutral,neutral
616659778,Same problem here. Would be great if someone takes a look,problem would great someone look,issue,negative,positive,positive,positive,positive,positive
615884360,"> Hi, I notice one thing.
> change the _PATH_ '/tmp/english_L-12_H-768_A-12/' to 'tmp/english_L-12_H-768_A-12/' or './tmp/english_L-12_H-768_A-12/' works well for me.
> You might try.

Thank you very much! It works for my code.",hi notice one thing change work well might try thank much work code,issue,positive,positive,positive,positive,positive,positive
614537211,"Hi, 

Even I had a similar question regarding how the output of the sentence embedding is generated. My assumption is that all the embedding of the words in the sentence get averaged out to produce one final embedding. But the problem with this is we would be losing information from individual words. Have you found anything else regarding this? Like what would be a better way to generate sentence embeddings?",hi even similar question regarding output sentence assumption sentence get produce one final problem would losing information individual found anything else regarding like would better way generate sentence,issue,negative,positive,positive,positive,positive,positive
613494727,"First thanks to @swubb for the first two steps.
Then, I followed this [guide](https://www.cyberciti.biz/faq/ubuntu-linux-install-nvidia-driver-latest-proprietary-driver/). Running these commands from command line from ubuntu 19.10. 
`pip uninstall tensorflow`
`pip install tensorflow-gpu`
`sudo apt install nvidia-driver-390`
`sudo reboot`
then everything worked with the option -device_map=0 -num_worker=1 it finds the nvidia geforce gpu and it uses it as desired. Remember to remove -cpu options when running bert server. ",first thanks first two guide running command line pip pip install apt install everything worked option desired remember remove running server,issue,positive,positive,positive,positive,positive,positive
612278890,"I had similar issue... if you have TensorFlow 2 installed, try uninstall that and install the last version of TensorFlow 1.. i.e. `pip install tensorflow==1.15.2`. that fixed it.. till this gets updated to work with v2",similar issue try install last version pip install fixed till work,issue,negative,positive,neutral,neutral,positive,positive
610467752,I've got the same issue. Does anyone know how to fix it?,got issue anyone know fix,issue,negative,neutral,neutral,neutral,neutral,neutral
608065575,thanks for your contribution! well done 👍 ,thanks contribution well done,issue,positive,positive,positive,positive,positive,positive
606633022,"Sorry I found the problem, deepset trained with sequence length 512, so using more than 512 cannot work. This thread can be closed, anyway an exception throwing would be a nice feature :)

",sorry found problem trained sequence length work thread closed anyway exception throwing would nice feature,issue,negative,neutral,neutral,neutral,neutral,neutral
605950872,"I specified the -model_dir as the directory that contained the bioBert files and also renamed them made sure the model name and config.json matched the -ckpt_name and -config_name flags. The default of -ckpt_name is 'bert_model.ckpt"" and the default of config_name is ""bert_config.json""",directory also made sure model name default default,issue,negative,positive,positive,positive,positive,positive
603437813,"#### Facing BertClient TimeOut error with Python 3.7.4

```python
from bert_serving.client import BertClient                                                                                                                                                   
bc = BertClient(ip='127.0.0.1', port=5555, port_out=5556, show_server_config=True, ignore_all_checks=False, timeout=20000)                                                                          

bc.encode(['First do it', 'then do it right', 'then do it better'])                                                                                                                        
```

**Error**
```
TimeoutError: no response from the server (with ""timeout""=20000 ms), please check the following:is the server still online? is the network broken? are ""port"" and ""port_out"" correct? are you encoding a huge amount of data whereas the timeout is too small for that?
```

#### Checked that the BertServer is up and running
```
(base) ➜  docker docker run -p 5555:5555 -p 5556:5556  e98983cd289f
I:VENTILATOR:[__i:__i: 67]:freeze, optimize and export graph, could take a while...
usage: /usr/local/bin/bert-serving-start -model_dir /app/multi_cased_L-12_H-768_A-12 -num_worker=1 -cpu
                 ARG   VALUE
__________________________________________________
           ckpt_name = bert_model.ckpt
         config_name = bert_config.json
                cors = *
                 cpu = True
          device_map = []
       do_lower_case = True
  fixed_embed_length = False
                fp16 = False
 gpu_memory_fraction = 0.5
       graph_tmp_dir = None
    http_max_connect = 10
           http_port = None
        mask_cls_sep = False
      max_batch_size = 256
         max_seq_len = 25
           model_dir = /app/multi_cased_L-12_H-768_A-12
no_position_embeddings = False
    no_special_token = False
          num_worker = 1
       pooling_layer = [-2]
    pooling_strategy = REDUCE_MEAN
                port = 5555
            port_out = 5556
       prefetch_size = 10
 priority_batch_size = 16
show_tokens_to_client = False
     tuned_model_dir = None
             verbose = False
                 xla = False

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_serving/server/helper.py:186: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_serving/server/helper.py:186: The name tf.logging.ERROR is deprecated. Please use tf.compat.v1.logging.ERROR instead.

I:GRAPHOPT:[gra:opt: 53]:model config: /app/multi_cased_L-12_H-768_A-12/bert_config.json
I:GRAPHOPT:[gra:opt: 56]:checkpoint: /app/multi_cased_L-12_H-768_A-12/bert_model.ckpt
I:GRAPHOPT:[gra:opt: 60]:build graph...
I:GRAPHOPT:[gra:opt:132]:load parameters from checkpoint...
I:GRAPHOPT:[gra:opt:136]:optimize...
I:GRAPHOPT:[gra:opt:144]:freeze...
I:GRAPHOPT:[gra:opt:149]:write graph to a tmp file: /tmp/tmprdty0_mo
I:VENTILATOR:[__i:__i: 75]:optimized graph is stored at: /tmp/tmprdty0_mo
I:VENTILATOR:[__i:_ru:129]:bind all sockets
I:VENTILATOR:[__i:_ru:133]:open 8 ventilator-worker sockets
I:VENTILATOR:[__i:_ru:136]:start the sink
I:SINK:[__i:_ru:306]:ready
I:VENTILATOR:[__i:_ge:222]:get devices
I:VENTILATOR:[__i:_ge:255]:device map: 
                worker  0 -> cpu
I:WORKER-0:[__i:_ru:531]:use device cpu, load graph from /tmp/tmprdty0_mo
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_serving/server/helper.py:186: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.

```

#### Ports 5555 & 5556 are also accessible
```
(base) ➜  EKSApps nc -vz 127.0.0.1 5556      
found 0 associations
found 1 connections:
     1: flags=82<CONNECTED,PREFERRED>
        outif lo0
        src 127.0.0.1 port 52875
        dst 127.0.0.1 port 5556
        rank info not available
        TCP aux info available

Connection to 127.0.0.1 port 5556 [tcp/freeciv] succeeded!

(base) ➜  EKSApps nc -vz 127.0.0.1 5555
found 0 associations
found 1 connections:
     1: flags=82<CONNECTED,PREFERRED>
        outif lo0
        src 127.0.0.1 port 52877
        dst 127.0.0.1 port 5555
        rank info not available
        TCP aux info available

Connection to 127.0.0.1 port 5555 [tcp/personal-agent] succeeded!
```

Any thoughts?",facing error python python import right better error response server please check following server still network broken port correct huge amount data whereas small checked running base docker docker run ventilator freeze optimize export graph could take usage value true true false false none none false false false port false none verbose false false warning name please use instead warning name please use instead gra opt model gra opt gra opt build graph gra opt load gra opt optimize gra opt freeze gra opt write graph file ventilator graph ventilator bind ventilator open ventilator start sink sink ready ventilator get ventilator device map worker use device load graph warning name please use instead also accessible base found found connected preferred lo port port rank available available connection port base found found connected preferred lo port port rank available available connection port,issue,positive,negative,negative,negative,negative,negative
602570815,"Hello, I have the same issue. Can you tell me how you resolved this issue?",hello issue tell resolved issue,issue,negative,neutral,neutral,neutral,neutral,neutral
602558294,"if your model's name is bert-base-german-cased
rename like this
bert-base-german-cased.index
bert-base-german-cased.meta
bert-base-german-cased.data-00000-of-00001
bert_config.json
vocab.txt",model name rename like,issue,negative,neutral,neutral,neutral,neutral,neutral
601048122,@shikhar42 you need to initialize your model_dir with the absolute path not the relative path.,need initialize absolute path relative path,issue,negative,positive,neutral,neutral,positive,positive
599407877,I'm also facing the same issue. The client is not able to connect to the server. It looks like the server is not ready.,also facing issue client able connect server like server ready,issue,positive,positive,positive,positive,positive,positive
599390479,I'm having the same issue. Did anyone manage to fix this?,issue anyone manage fix,issue,negative,neutral,neutral,neutral,neutral,neutral
598610003,looks good to me! thanks a lot for your contribution ❤️ ,good thanks lot contribution,issue,positive,positive,positive,positive,positive,positive
598593230,"not sure if your conclusion is valid, please refer to this benchmark

https://github.com/hanxiao/bert-as-service#speed-wrt-pooling_layer",sure conclusion valid please refer,issue,positive,positive,positive,positive,positive,positive
597809984,anyone else facing the same issue?,anyone else facing issue,issue,negative,neutral,neutral,neutral,neutral,neutral
597288059,"@ghunkins 
![image](https://user-images.githubusercontent.com/8600424/76354506-f3c61080-62e8-11ea-929f-d6abb8521d97.png)

Is this what you mean, or am I doing something wrong?
Thanks",image mean something wrong thanks,issue,negative,negative,negative,negative,negative,negative
597283269,"@shikhar42 It defaults to `gpu` config I believe, add the `-cpu` flag and you should be fine. It's failing on some CUDA configuration, so the CPU should bypass that.",believe add flag fine failing configuration bypass,issue,negative,positive,positive,positive,positive,positive
597282699,"@ghunkins thank you for your reply. But when I do that, I get the following error

![image](https://user-images.githubusercontent.com/8600424/76353514-3f77ba80-62e7-11ea-919e-2c55790e3d4d.png)

I was getting this error earlier and when I removed spaces between the path and num_workers, it accepted num_workers as an argument but then it fails with the type error optimizing the graph.",thank reply get following error image getting error removed path accepted argument type error graph,issue,negative,neutral,neutral,neutral,neutral,neutral
597279497,@shikhar42 looks like you're need a space between your path and `num_workers`.,like need space path,issue,negative,neutral,neutral,neutral,neutral,neutral
597275141,"I am still unable to do it. What am I doing wrong?
The path of the folder is:
![image](https://user-images.githubusercontent.com/8600424/76352099-dd1dba80-62e4-11ea-939b-1e7963f0f903.png)

The command I'm running is:

![image](https://user-images.githubusercontent.com/8600424/76352194-050d1e00-62e5-11ea-8bb2-e7a9ad4fdd10.png)

",still unable wrong path folder image command running image,issue,negative,negative,negative,negative,negative,negative
596151552,"This error can be cause by multiple reasons:

(1) Installing the wrong version of TensorFlow; 
You should install TensorFlow >= 1.10 and <=1.15 (**don't install TensorFlow 2.0**)

(2) When trying to start the server using the command:
bert-serving-start -model_dir **cased_L-24_H-1024_A-16/** -num_worker=4

change the path to the _model_dir_ to point to the model directory in your machine.",error cause multiple wrong version install install trying start server command change path point model directory machine,issue,negative,negative,negative,negative,negative,negative
596060916,"I enabled cpu mode, then I got vectors, but gpu mode still doesn't work.",mode got mode still work,issue,negative,neutral,neutral,neutral,neutral,neutral
596055117,"> same problem, I could only get output:
> `I:VENTILATOR:[__i:_ru:180]:new config request req id: 1 client: b'55077a73-76c4-4abd-b554-b6283003f1ed' I:SINK:[__i:_ru:348]:send config client b'55077a73-76c4-4abd-b554-b6283003f1ed' I:VENTILATOR:[__i:_ru:196]:new encode request req id: 2 size: 1 client: b'55077a73-76c4-4abd-b554-b6283003f1ed' I:WORKER-0:[__i:gen:568]:new job socket: 0 size: 1 client: b'55077a73-76c4-4abd-b554-b6283003f1ed#2' I:SINK:[__i:_ru:342]:job register size: 1 job id: b'55077a73-76c4-4abd-b554-b6283003f1ed#2' I:WORKER-0:[__i:_ru:543]:job done size: (1, 768) client: b'a35b5d60-2ebb-4463-a37f-8d805bdb67fa#2' I:SINK:[__i:_ru:334]:collect b'EMBEDDINGS' b'a35b5d60-2ebb-4463-a37f-8d805bdb67fa#2' (E:1/T:0/A:1) I:SINK:[__i:_ru:357]:send back size: 1 job id: b'a35b5d60-2ebb-4463-a37f-8d805bdb67fa#2'`
> then my program is blocked.

if I enter ctrl + c, traceback output is same as @DavidInWuhanChina ",problem could get output ventilator new request id client sink send client ventilator new encode request id size client gen new job socket size client sink job register size job id job done size client sink collect sink send back size job id program blocked enter output,issue,negative,positive,positive,positive,positive,positive
596055023,"same problem, I could only get output:
`
I:VENTILATOR:[__i:_ru:180]:new config request   req id: 1       client: b'55077a73-76c4-4abd-b554-b6283003f1ed'
I:SINK:[__i:_ru:348]:send config        client b'55077a73-76c4-4abd-b554-b6283003f1ed'
I:VENTILATOR:[__i:_ru:196]:new encode request   req id: 2       size: 1 client: b'55077a73-76c4-4abd-b554-b6283003f1ed'
I:WORKER-0:[__i:gen:568]:new job        socket: 0       size: 1 client: b'55077a73-76c4-4abd-b554-b6283003f1ed#2'
I:SINK:[__i:_ru:342]:job register       size: 1 job id: b'55077a73-76c4-4abd-b554-b6283003f1ed#2'
I:WORKER-0:[__i:_ru:543]:job done       size: (1, 768)  client: b'a35b5d60-2ebb-4463-a37f-8d805bdb67fa#2'
I:SINK:[__i:_ru:334]:collect b'EMBEDDINGS' b'a35b5d60-2ebb-4463-a37f-8d805bdb67fa#2' (E:1/T:0/A:1)
I:SINK:[__i:_ru:357]:send back  size: 1 job id: b'a35b5d60-2ebb-4463-a37f-8d805bdb67fa#2'
`
then my program is blocked.",problem could get output ventilator new request id client sink send client ventilator new encode request id size client gen new job socket size client sink job register size job id job done size client sink collect sink send back size job id program blocked,issue,negative,positive,positive,positive,positive,positive
594319435,@wliu-sift Thanks for the response. To where do you add the delay? To the client?,thanks response add delay client,issue,negative,positive,positive,positive,positive,positive
594048805,My workaround is wait for the server to fully booted up and add some delay ~10s. The issue never appears again.,wait server fully booted add delay issue never,issue,negative,neutral,neutral,neutral,neutral,neutral
593890119,I'm having the same issue. Was anybody able to find a solution to this?,issue anybody able find solution,issue,negative,positive,positive,positive,positive,positive
592493206,"Hi there, I got the same confusion. Despite the difference in values, we notice that the calculation time under **None** is quite longer than that under **REDUCE_MEAN**. Just wondering if its the sane in your experiments. Thank you! @bayartsogt-ya",hi got confusion despite difference notice calculation time none quite longer wondering sane thank,issue,negative,neutral,neutral,neutral,neutral,neutral
591535265,"For me all works from DHOFM's answer before line ""bc = BertClient()""
In this part a block loading infinite time without warning throwing or else info.
Someone in the know about this?",work answer line part block loading infinite time without warning throwing else someone know,issue,negative,neutral,neutral,neutral,neutral,neutral
591205246,"I meet that issue to..but I don't want to downgrading,Is there have any other  settlement work for 2.0",meet issue want settlement work,issue,negative,neutral,neutral,neutral,neutral,neutral
590886496,"noop. bert-as-service do support chinese-bert-wwm-ext model.
plz download chinese_wwm_ext_L-12_H-768_A-12.zip from chinese-bert-wwm site.",noop support model site,issue,negative,neutral,neutral,neutral,neutral,neutral
589129781,"Hi @junrong1 ,

I faced the same problem. The problem for me was the tensorflow version, I downgraded it to 1.10 and then it worked.

I hope this can be helpful",hi faced problem problem version worked hope helpful,issue,negative,neutral,neutral,neutral,neutral,neutral
586861612,"> I am also facing same issues, but on one machine with GPU MX130 it runs with tensorflow1.15 but only on CPU not on GPU, but with same software environment its not working another machine with GTX 960M.
> 
> I:�[36mGRAPHOPT�[0m:model config: ./uncased_L-12_H-768_A-12/bert_config.json
> I:�[36mGRAPHOPT�[0m:checkpoint: ./uncased_L-12_H-768_A-12/bert_model.ckpt
> E:�[36mGRAPHOPT�[0m:fail to optimize the graph!
> Traceback (most recent call last):
> File ""d:\anaconda3\envs\gpu_test\lib\runpy.py"", line 193, in _run_module_as_main ""**main**"", mod_spec) File ""d:\anaconda3\envs\gpu_test\lib\runpy.py"", line 85, in _run_code exec(code, run_globals) File ""D:\Anaconda3\envs\gpu_test\Scripts\bert-serving-start.exe__main__.py"", line 7, in File ""d:\anaconda3\envs\gpu_test\lib\site-packages\bert_serving\server\cli__init__.py"", line 4, in main
> with BertServer(get_run_args()) as server:
> File ""d:\anaconda3\envs\gpu_test\lib\site-packages\bert_serving\server__init__.py"", line 71, in **init**
> self.graph_path, self.bert_config = pool.apply(optimize_graph, (self.args,))
> TypeError: 'NoneType' object is not iterable
> 
> ....................................................
> Im meeting same error with cuda 9.0, tf=1.10

Most probably the script cannot find path to your model.
Check whether uncased_L-12_H-768_A-12/ exists or try to provide absolute path to the model directory. ",also facing one machine environment working another machine model fail optimize graph recent call last file line main file line code file line file line main server file line object iterable meeting error probably script find path model check whether try provide absolute path model directory,issue,negative,positive,neutral,neutral,positive,positive
586592475,"can support semantic search engine in ""query - documents""  scene ",support semantic search engine query scene,issue,negative,neutral,neutral,neutral,neutral,neutral
586066864,"@sujithjoseph 
recently, I want to develop a search engine based on vector search. I have an idea that covert query input and document to vector. Their similarity can be measured by cosine or distance between vector of query and vectors of  documents. Can you share  your new idea and thoughts?",recently want develop search engine based vector search idea covert query input document vector similarity measured cosine distance vector query share new idea,issue,negative,positive,neutral,neutral,positive,positive
584486217,"After upgrading to TF2 , I'm getting error for server

```
module 'tensorflow' has no attribute 'logging'
```

I am assuming this is TF 2 issue.",getting error server module attribute assuming issue,issue,negative,neutral,neutral,neutral,neutral,neutral
582387881,"This is old but might be helpful to someone. 

@mullerhai make sure you edit your bert-server address at bindings.js 

The ip address and the context must point to your bert http service 
https://github.com/hanxiao/bert-as-service/blob/0fb1814d6368ff824e90b386a3bf65cab514a067/plugin/dashboard/bindings.js#L6
",old might helpful someone make sure edit address address context must point service,issue,positive,positive,positive,positive,positive,positive
581254059,"Hi @junrong1,

U r on a Windows so u will have to change the forward slash(/) to backward slash(\\)

so ur cmd should be like
`bert-serving-start -model_dir tmp\english_L-12_H-768_A-12`

Hope this solves ur problem
",hi change forward slash backward slash ur like hope ur problem,issue,negative,neutral,neutral,neutral,neutral,neutral
580450461,"i miss understand the documentation , there is only one option not two ! there is AND and not OR . it work correctly now thank you so much for the great project (y)",miss understand documentation one option two work correctly thank much great project,issue,positive,positive,positive,positive,positive,positive
580365323,"hello ,
i'm looking for the same problem , did you find a solution ? if yes, could you share it please? ",hello looking problem find solution yes could share please,issue,positive,neutral,neutral,neutral,neutral,neutral
578455705,"Hello, I have come to a problem. Let me explain so we can decide what to do:

Endpoint /encode is capable of encoding either tokens or sentences. So two data structures can be part of the request, either {id:0, is_tokenized:true,texts:[['string']]} or {id:0, is_tokenized:false, texts['string']}

This is ok, but the issue lies in the OpenAPI generator and Swagger UI. There are two open issues:
- Swagger UI cannot generate examples when you use ""oneOf"" (which is used in polymorphism)
- OpenAPI Generator cannot generate the object structure properly when in request schema there is only ""oneOf""

If I would model the request correctly, so that specification is foolproof then these two issues (that have open PRs) appear.
```
schema:
              oneOf:
                - $ref: '#/components/schemas/EncodeRequestTokenized'
                - $ref: '#/components/schemas/EncodeRequestNotTokenized'
              discriminator:
                propertyName: is_tokenized
                mapping:
                  true: '#/components/schemas/EncodeRequestTokenized'
                  false: '#/components/schemas/EncodeRequestNotTokenized'
              examples:
                EncodeRequestTokenized:
                  value:
                    id: 0
                    is_tokenized: true
                    texts: [['string']]
                EncodeRequestNotTokenized:
                  value:
                    id: 0
                    is_tokenized: false
                    texts: ['string']
```

If I would model it semi-correctly the schema would validate {id:0,is_tokenized:true} as correct, but the issues with generator and Swagger UI are OK.
```
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/EncodeRequest'
...
    EncodeRequest:
...
        is_tokenized:
          type: boolean
          default: false
      discriminator:
        propertyName: is_tokenized
        mapping:
          true: '#/components/schemas/EncodeRequestTokenized'
          false: '#/components/schemas/EncodeRequestNotTokenized'
    EncodeRequestTokenized:
      description: Also need to set the property is_tokenized to true
      allOf:
        - $ref: '#/components/schemas/EncodeRequest'
```
Then there is option to change the API itself to /encodeTokens and /encodeSentences - this seems like best solution, but is breaking change.

What do you think?
Thank you",hello come problem let explain decide capable either two data part request either id true id false issue generator swagger two open swagger generate use used polymorphism generator generate object structure properly request schema would model request correctly specification foolproof two open appear schema ref ref discriminator true false value id true value id false would model schema would validate id true correct generator swagger content schema ref type default false discriminator true false description also need set property true ref option change like best solution breaking change think thank,issue,positive,positive,neutral,neutral,positive,positive
578366232,"I'm also having the same problem, I'm seeing that bert-as-a-service assigns a worker to a GPU, but nvidia-smi register's no workloads

I'm running in GKE so I'm not sure how to set persistence mode there",also problem seeing worker register running sure set persistence mode,issue,negative,positive,positive,positive,positive,positive
577629477,"DHOFM's answer also worked for me, but make sure Colab is using Tensorflow 1.x, because bert-serving-start doesn't currently work with TF 2.1 and `nohup` hides the output of the command failing.",answer also worked make sure currently work output command failing,issue,negative,positive,positive,positive,positive,positive
577409388,"> > I have the same question!Have you solved it?
> 
> I change to another env with
> tensorflow 1.13.1
> bert-serving-client/server 1.9.9
> python3.7
> and it works well😀

thank you so much , finally it work for me !",question change another python work well thank much finally work,issue,positive,neutral,neutral,neutral,neutral,neutral
577001781,"  File ""/home/lgx/nlpleanmy/bert-as-service-master/example/example3.py"", line 26, in <module>
    bc = BertClient(port=int(sys.argv[1]), port_out=int(sys.argv[2]))
IndexError: list index out of range",file line module list index range,issue,negative,neutral,neutral,neutral,neutral,neutral
576989484," ❤️  no problemo, feel free to continue on this thread and I will publish this change in the next release.",feel free continue thread publish change next release,issue,positive,positive,positive,positive,positive,positive
576941780,"Thank you for merging. Today after extended testing, I have found some issues, that I will be resolving soon (I will take a little bit more time testing this time). Namely polymorphism for tokenized and untokenized input is not present in current schema, also there is typo in property name for result(s).
I will be also providing sample Java client for bert as service.",thank today extended testing found soon take little bit time testing time namely polymorphism input present current schema also typo property name result also providing sample client service,issue,negative,negative,neutral,neutral,negative,negative
576704011,❤️ thanks a lot for your contribution! Really appreciated!,thanks lot contribution really,issue,negative,positive,positive,positive,positive,positive
575698318,"still the same problem

Did anyone solve this?",still problem anyone solve,issue,negative,neutral,neutral,neutral,neutral,neutral
574536013,"> > > > > > bert-as-service有几个参数可能影响结果：
> > > > > > -pooling_layer，默认是[-2]，取的是倒数第二层的表示
> > > > > > -pooling_strategy，默认是REDUCE_MEAN
> > > > > > 你核对一下这些配置，和你pytorch模型的配置是否一致
> > > > > 
> > > > > 
> > > > > 谢谢回复，这个倒数第二层 是指transformer 的倒数第二层吗？
> > > > 
> > > > 
> > > > pooling_layer为-1表示最后用来预测MLM和NSP的那一层输出
> > > > bert一共有12个transformer块，pooling_layer为-2表示取的是第11个块的输出，你可以看一下bert源码modeling.py里的all_layer_outputs这个列表的内容。
> > > > 又发现了几个影响输出embedding的参数：
> > > > max_seq_len（太短有可能把你的输入截断了）
> > > > mask_cls_sep（可以选择把[CLS]和[SEP]给mask掉，不参与reduce_mean）
> > > > 另外，看你贴出来的代码，如果按默认配置，bert-as-service出来的是倒数第二个transformer块的词向量平均；但是我不太清楚pytorch那部分的outputs[1]输出的是啥。。。
> > > 
> > > 
> > > 谢谢指导，我得到了相同的向量。。。再请教一下，这里这个-max_seq_len 的默认值是 25， 如果sequence小于这个默认值的话，会padding到长度为25吗? padding 0 还是 1?
> > 
> > 
> > padding添加0；
> > 当句子长度小于25时，padding的策略分两种：
> > 有这个参数：
> > `group2.add_argument('-no_special_token', action='store_true', default=False, help='add [CLS] and [SEP] in every sequence, \ put sequence to the model without [CLS] and [SEP] when True and \ is_tokenized=True in Client')`
> > 这个参数表示是不是在网络中使用[CLS]和[SEP]；如果不用，那么直接把你的句子padding到长度25；如果用，先在句子头尾加这两个标志，如果长度还不够，再padding到25
> > 详见server/bert_serving/server/bert下的extract_features中的convert_lst_to_features函数
> 
> 谢谢您，，，您有用transformers里的bert吗 用torch实现的

没用过torch。。",padding every sequence put sequence model without true client,issue,negative,positive,positive,positive,positive,positive
573330444,The same issues here . Should I run The bert server and client in different colab notebooks and use the ip address of server while declaring the client?,run server client different use address server client,issue,negative,neutral,neutral,neutral,neutral,neutral
573097648,"@NidhiRustagi  
If in any case you are using tensorflow 2, switching to [1.10-1.15] solved the problem for me.
OS: Windows",case switching problem o,issue,negative,neutral,neutral,neutral,neutral,neutral
572529144,"Hi @goodhamgupta 
I downladed the BERT and put it under the  `server/tmp/english_L-12_H-768_A-12` folder, is this path wrong, anywhere else should I put. By the way, I also tried the absolute path for `-model_dir` but still not work with the same problem.",hi put folder path wrong anywhere else put way also tried absolute path still work problem,issue,negative,negative,negative,negative,negative,negative
572414281,"Hi @junrong1 

You have not downloaded the BERT model and stored it in the `/tmp/english_L-12_H-768_A-12` folder, due to which you are facing the above error. Download the models from the links mentioned in the README.md, unzip the downloaded file and try the `bert-serving-start` command again.",hi model folder due facing error link file try command,issue,negative,negative,negative,negative,negative,negative
571210112,same problem. using tf_upgrade_v2 to upgrade code does not solve problem,problem upgrade code solve problem,issue,negative,neutral,neutral,neutral,neutral,neutral
570823918,"@zjwzcn07 You're the best, thank you for saving me with this issue...",best thank saving issue,issue,positive,positive,positive,positive,positive,positive
570154788,"> pytorch要用train.eval()把dropout关掉

################################################
import warnings
warnings.filterwarnings(""ignore"")
import torch
from transformers import BertTokenizer, BertModel

UNCASE = ""./bert-base-chinese""
VOCAB = ""vocab.txt""
tokenizer = BertTokenizer.from_pretrained(UNCASE + ""/"" + VOCAB)
model = BertModel.from_pretrained(UNCASE + ""/test"")
model.eval()
string = '盘踞在山中的妖魔'
# string = ""[CLS]"" + string + ""[SEP]""
tokenized_string = tokenizer.tokenize(string)
tokens_ids = tokenizer.convert_tokens_to_ids(tokenized_string)
tokens_tensor = torch.tensor([tokens_ids])
outputs = model(tokens_tensor)
print(outputs[1])

###############################################
import warnings
warnings.filterwarnings('ignore')
from bert_serving.client import BertClient
bc_ch = BertClient(ip='algo.haetek.com', port=5555, port_out=5556)  # chinese
vec = bc_ch.encode(['盘踞在山中的妖魔'])
print(vec)

################################################
上面分别是pytorch-bert和bert-as-service的生成词向量的方式，，最后出来的词向量不同，我加了eval()啊。请您指教一下哪里出问题了",import ignore import torch import uncase uncase model uncase string string string string model print import import print,issue,negative,neutral,neutral,neutral,neutral,neutral
569523213,"For those who are on a CPU:
Follow these two steps if you're getting the same error.

1. `pip uninstall` ___current_version_of_tf___
2. `pip install tensorflow==1.13.0rc1`",follow two getting error pip pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
569358177,"In my case it seems to be something in the ubuntu anaconda docker image I am using. 
I've tried exactly the same in my mac osx and it worked like a charm using CPU.

",case something anaconda docker image tried exactly mac worked like charm,issue,positive,positive,positive,positive,positive,positive
569200152,"@chikubee @NidhiRustagi @guoyaohua 
I am able to run cased bert but am getting the following error when I try to run bio bert,
TypeError: 'NoneType' object is not iterable
I have tried giving absolute path.
OS: Windows
biobert_v1.1_pubmed",able run cased getting following error try run object iterable tried giving absolute path o,issue,negative,positive,positive,positive,positive,positive
568595941,Thanks for accepting my PR and thank you for the great set of tools for BERT.,thanks thank great set,issue,positive,positive,positive,positive,positive,positive
568310968,"thank you @hanxiao for creating bert-as-service! i've used it for countless projects from grad school to now in production for it's convenience, performance and for letting me focus on the models i'm building :) ",thank used countless grad school production convenience performance focus building,issue,negative,neutral,neutral,neutral,neutral,neutral
567818039,"> @qiunian711
> 
> First, rename:
> `bert-base-german.data-00000-of-00001` to
> `bert-base-german-cased.data-00000-of-00001`
> so that you have `index`, `meta` and `data` files with the same name. The content of the folder should look like this:
> 
> ```
> bert-base-german-cased.index
> bert-base-german-cased.meta
> bert-base-german-cased.data-00000-of-00001
> bert_config.json
> vocab.txt
> ```
> 
> Now, you need to pass an additional parameter to `bert-serving-start`:
> 
> ```
> bert-serving-start -model_dir <folder-name> -ckpt_name bert-base-german-cased
> ```
> 
> This is because Bert expects `ckpt_name = bert_model.ckpt` by default, as shown in the log under
> `ARG VALUE` table.
> 
> Another option would be to rename `index`, `meta` and `data` files to `bert_model.ckpt.index`, etc.

It worked, but in my case, you have to rename 3 files `index`, `meta` and `data` to `bert_model`. If you add an additional param like `-ckpt_name bert-base-german-cased`, it will assume that you have a subfolder inside the pretrained model named `bert-base-german-cased`.

Thanks anyway",first rename index meta data name content folder look like need pas additional parameter default shown log value table another option would rename index meta data worked case rename index meta data add additional param like assume inside model thanks anyway,issue,positive,positive,positive,positive,positive,positive
567817729,"@dslim23 thanks again for your contribution, the new feature is rolled out now at `1.10.0` available on PyPI.",thanks contribution new feature rolled available,issue,negative,positive,positive,positive,positive,positive
567814511,"> @TheJdxfh @mtedmist
> 
> How can I downgrade my TF using anaconda on windows?

First you should delete the current version:
`pip uninstall tensorflow`
Then you can install the specific version you want to. Ex:
`pip install tensorflow==1.13.1`
Then you are good to go",downgrade anaconda first delete current version pip install specific version want ex pip install good go,issue,negative,positive,positive,positive,positive,positive
567813314,"> I agree. It makes this repo pretty useless to use for any classification tasks without adding this.

*""pretty useless""*, seriously? that's heartbreaking 💔 @pierrerappolt ",agree pretty useless use classification without pretty useless seriously heartbreaking,issue,negative,negative,negative,negative,negative,negative
567812286,I agree. It makes this repo pretty useless to use for any classification tasks without adding this.,agree pretty useless use classification without,issue,negative,negative,negative,negative,negative,negative
567639861,"ps listing from inside the container clearly shows ""-show_tokens_to_client"" as CLI option:

```bash
/opt/docker$ docker exec -it bert-as-service /bin/bash
root@2c6ec6089cde:/app# ps axww      
  PID TTY      STAT   TIME COMMAND
    1 pts/0    Ss+    0:00 /bin/sh /app/entrypoint.sh 1
    6 pts/0    Sl+    0:05 /usr/bin/python3 /usr/local/bin/bert-serving-start -model_dir /model -max_seq_len NONE -show_tokens_to_client -http_port 8080 -num_worker=1
```",listing inside container clearly option bash docker root time command none,issue,negative,positive,positive,positive,positive,positive
567638667,"Full output from: http://192.168.1.129:8080/status/server

```json
{
  ""ckpt_name"": ""bert_model.ckpt"",
  ""client"": ""cd350caa-a83e-46c5-82e4-cbe06f9f3a7a"",
  ""config_name"": ""bert_config.json"",
  ""cors"": ""*"",
  ""cpu"": false,
  ""device_map"": [],
  ""do_lower_case"": true,
  ""fixed_embed_length"": false,
  ""fp16"": false,
  ""gpu_memory_fraction"": 0.5,
  ""graph_tmp_dir"": null,
  ""http_max_connect"": 10,
  ""http_port"": 8080,
  ""mask_cls_sep"": false,
  ""max_batch_size"": 256,
  ""max_seq_len"": null,
  ""model_dir"": ""/model"",
  ""no_position_embeddings"": false,
  ""no_special_token"": false,
  ""num_concurrent_socket"": 8,
  ""num_process"": 3,
  ""num_worker"": 1,
  ""pooling_layer"": [
    -2
  ],
  ""pooling_strategy"": 2,
  ""port"": 5555,
  ""port_out"": 5556,
  ""prefetch_size"": 10,
  ""priority_batch_size"": 16,
  ""python_version"": ""3.5.2 (default, Nov 23 2017, 16:37:01) \n[GCC 5.4.0 20160609]"",
  ""pyzmq_version"": ""17.1.2"",
  ""server_current_time"": ""2019-12-19 19:40:43.457178"",
  ""server_start_time"": ""2019-12-19 19:37:52.060314"",
  ""server_version"": ""1.9.9"",
  ""show_tokens_to_client"": true,
  ""statistic"": {
    ""avg_last_two_interval"": 16.154498574964236,
    ""avg_request_per_client"": 2,
    ""avg_request_per_second"": 0.06190226179782333,
    ""avg_size_per_request"": 1,
    ""max_last_two_interval"": 16.154498574964236,
    ""max_request_per_client"": 2,
    ""max_request_per_second"": 0.06190226179782333,
    ""max_size_per_request"": 1,
    ""min_last_two_interval"": 16.154498574964236,
    ""min_request_per_client"": 2,
    ""min_request_per_second"": 0.06190226179782333,
    ""min_size_per_request"": 1,
    ""num_active_client"": 1,
    ""num_data_request"": 1,
    ""num_max_last_two_interval"": 1,
    ""num_max_request_per_client"": 1,
    ""num_max_request_per_second"": 1,
    ""num_max_size_per_request"": 1,
    ""num_min_last_two_interval"": 1,
    ""num_min_request_per_client"": 1,
    ""num_min_request_per_second"": 1,
    ""num_min_size_per_request"": 1,
    ""num_sys_request"": 1,
    ""num_total_client"": 1,
    ""num_total_request"": 2,
    ""num_total_seq"": 1
  },
  ""status"": 200,
  ""tensorflow_version"": [
    ""1"",
    ""12"",
    ""0""
  ],
  ""tuned_model_dir"": null,
  ""ventilator -> worker"": [
    ""ipc://tmpQH1gxr/socket"",
    ""ipc://tmpI4oRMI/socket"",
    ""ipc://tmpCAYt2Z/socket"",
    ""ipc://tmpqZx8hh/socket"",
    ""ipc://tmpgp9Oxy/socket"",
    ""ipc://tmpqRQxNP/socket"",
    ""ipc://tmpuIEi36/socket"",
    ""ipc://tmpM4Kgjo/socket""
  ],
  ""ventilator <-> sink"": ""ipc://tmp05TIha/socket"",
  ""verbose"": false,
  ""worker -> sink"": ""ipc://tmppA4sds/socket"",
  ""xla"": false,
  ""zmq_version"": ""4.2.5""
}
```",full output client false true false false null false null false false port default true statistic status null ventilator worker ventilator sink verbose false worker sink false,issue,positive,negative,negative,negative,negative,negative
567145715,"No GPU isn't really necessary unless you have huge volumes


________________________________
From: JonLevy <notifications@github.com>
Sent: Wednesday, December 18, 2019 9:59 AM
To: hanxiao/bert-as-service
Cc: Erik Chan; Mention
Subject: Re: [hanxiao/bert-as-service] Best cloud services for bert-as-a-service? (#374)


@echan00<https://github.com/echan00> on AWS, are you using GPU accelerated instances? Those are pricy.

—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub<https://github.com/hanxiao/bert-as-service/issues/374?email_source=notifications&email_token=ABP67Q4R3RXB56XSJCVRETLQZJQIVA5CNFSM4HUBQDI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEHG6VYA#issuecomment-567143136>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/ABP67QZJ64VJ7KOLRMJIVULQZJQIVANCNFSM4HUBQDIQ>.
",really necessary unless huge sent mention subject best cloud accelerated reply directly view,issue,positive,positive,positive,positive,positive,positive
567045902,@echan00 Did you end up finding a solution to cloud hosting bert-as-as-service?  I'm looking into options now and having trouble finding recommendations.  It would be nice if it could be setup as an AWS lambda function of sorts,end finding solution cloud hosting looking trouble finding would nice could setup lambda function,issue,negative,positive,positive,positive,positive,positive
565780796,"@akkasi
what error ? And this configure in command line should be `-no_sp… ` not `-no_sp… true`",error configure command line true,issue,negative,positive,positive,positive,positive,positive
565722640,"May I ask you how should be the server configuration when I don't want to get the embedding vectors of ['CLS'] and ['SEP']? 
I tried to set '-no_special_token' to 'True' running a server but it gives me error!!
",may ask server configuration want get tried set running server error,issue,negative,neutral,neutral,neutral,neutral,neutral
565374246,"What if I want to find the most relevant words from the contextualized vector?
Like using Bert as an intent classifier and then use the contextualized vector of the intent to find related words from the vocabulary.",want find relevant vector like intent classifier use vector intent find related vocabulary,issue,negative,positive,positive,positive,positive,positive
565115484,"> Got it running using Bert Client and nohup - here is how it worked for me:
> 
> !pip install bert-serving-client
> !pip install -U bert-serving-server[http]
> 
> !wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip
> !unzip uncased_L-12_H-768_A-12.zip
> !nohup bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 > out.file 2>&1 &
> 
> from bert_serving.client import BertClient
> bc = BertClient()
> print (bc.encode(['First do it', 'then do it right', 'then do it better']))

it worked for me as well , thank you!",got running client worked pip install pip install import print right better worked well thank,issue,positive,positive,positive,positive,positive,positive
564958517,"same 
[EDIT] I tested with a GPU  (under Google Notebook) and it is working now!",edit tested notebook working,issue,negative,neutral,neutral,neutral,neutral,neutral
563446843,"@yolearn 
I also meet this problem. 
How can i downgrade my tf using anaconda environment? ",also meet problem downgrade anaconda environment,issue,negative,neutral,neutral,neutral,neutral,neutral
561458183,"> sorry - downgrading to tensorflow 1.x worked, it was due to 2.0 updates. I tried to upgrade the bert_serving scripts using 2.0 tools, but I wasn't successful as there were some errors.

but with my env with tensorflow 1.14.0 it seems doesn't work....",sorry worked due tried upgrade successful work,issue,positive,positive,neutral,neutral,positive,positive
561457854,"> I have the same question!Have you solved it?

I change to another env with  
tensorflow 1.13.1
bert-serving-client/server 1.9.9
python3.7
and it works well😀 
",question change another python work well,issue,negative,neutral,neutral,neutral,neutral,neutral
560981036,"Thanks. I missed this blog. Thank for you providing a low memory and a scalable service.
",thanks thank providing low memory scalable service,issue,positive,positive,neutral,neutral,positive,positive
560979154,"I'm assuming you want to know the motivation behind `bert-as-service`, please read this blog post:

https://hanxiao.io/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/",assuming want know motivation behind please read post,issue,positive,negative,negative,negative,negative,negative
560471039,"sorry - downgrading to tensorflow 1.x worked, it was due to 2.0 updates. I tried to upgrade the bert_serving scripts using 2.0 tools, but I wasn't successful as there were some errors.",sorry worked due tried upgrade successful,issue,positive,positive,neutral,neutral,positive,positive
560440343,"> I also meet this problem.
> Which do you use, tf2.0 or tf1.0?

@yolearn i was using tf2.0",also meet problem use,issue,negative,neutral,neutral,neutral,neutral,neutral
560435899,"Got it running using Bert Client and nohup - here is how it worked for me:

!pip install bert-serving-client
!pip install -U bert-serving-server[http]

!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip
!unzip uncased_L-12_H-768_A-12.zip
!nohup bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 > out.file 2>&1 &

from bert_serving.client import BertClient
bc = BertClient()
print (bc.encode(['First do it', 'then do it right', 'then do it better']))
",got running client worked pip install pip install import print right better,issue,negative,positive,positive,positive,positive,positive
560435296,"Facing the same issue when I tried to start the server, here is the stacktrace - looks like issue related to tensorflow 2.0 updates. Please let us know if this will be addressed soon.

I:VENTILATOR:[__i:__i: 67]:freeze, optimize and export graph, could take a while...
E:GRAPHOPT:[gra:opt:151]:fail to optimize the graph!
Traceback (most recent call last):
  File ""skc/environments/.virtualenvs/bert/lib/python3.6/site-packages/bert_serving/server/graph.py"", line 41, in optimize_graph
    tf = import_tf(verbose=args.verbose)
  File ""/environments/.virtualenvs/bert/lib/python3.6/site-packages/bert_serving/server/helper.py"", line 186, in import_tf
    tf.logging.set_verbosity(tf.logging.DEBUG if verbose else tf.logging.ERROR)
AttributeError: module 'tensorflow' has no attribute 'logging'
Traceback (most recent call last):
  File ""/environments/.virtualenvs/bert/bin/bert-serving-start"", line 8, in <module>
    sys.exit(main())
  File ""/environments/.virtualenvs/bert/lib/python3.6/site-packages/bert_serving/server/cli/__init__.py"", line 4, in main
    with BertServer(get_run_args()) as server:
  File ""/home/imagen/skc/environments/.virtualenvs/bert/lib/python3.6/site-packages/bert_serving/server/__init__.py"", line 71, in __init__
    self.graph_path, self.bert_config = pool.apply(optimize_graph, (self.args,))
TypeError: 'NoneType' object is not iterable
$ bert-serving-start -model_dir /bert/bert-models/cased_L-12_H-768_A-12/ -num_worker=1
/home/imagen/skc/environments/.virtualenvs/bert/lib/python3.6/site-packages/bert_serving/server/helper.py:176: UserWarning: Tensorflow 2.0.0 is not tested! It may or may not work. Feel free to submit an issue at https://github.com/hanxiao/bert-as-service/issues/
  'Feel free to submit an issue at https://github.com/hanxiao/bert-as-service/issues/' % tf.__version__)",facing issue tried start server like issue related please let u know soon ventilator freeze optimize export graph could take gra opt fail optimize graph recent call last file line file line verbose else module attribute recent call last file line module main file line main server file line object iterable tested may may work feel free submit issue free submit issue,issue,positive,positive,neutral,neutral,positive,positive
560412513,"Hi
I get also the connection refused Error, Changing the port number has no effect. I started successfully Flask Servers with different ports before, but Bert Service fails to start",hi get also connection error port number effect successfully flask different service start,issue,negative,positive,positive,positive,positive,positive
559498050,@hanxiao I'm a bit confused where actually the embedding is calculated within `bertserver`- is it here: https://github.com/hanxiao/bert-as-service/blob/master/server/bert_serving/server/graph.py#L99?,bit confused actually calculated within,issue,negative,negative,negative,negative,negative,negative
559496814,"@hanxiao related to this, a simpler approach that is using `bert-as-service` is here: https://github.com/Hironsan/bertsearch

It would be cool to support ElasticSearch as well directly in `bert-as-service`.",related simpler approach would cool support well directly,issue,positive,positive,positive,positive,positive,positive
559067000,"> @alelasantillan My friend showed me BertHub, which encapsulates BERT into a keras layer. Maybe you can take a look at that. It might be easier.

@4everlove can you provide a link? I was googling unsuccessfully. Thank you again!",friend layer maybe take look might easier provide link unsuccessfully thank,issue,positive,neutral,neutral,neutral,neutral,neutral
558825381,"@alelasantillan My friend showed me BertHub, which encapsulates BERT into a keras layer. Maybe you can take a look at that. It might be easier.",friend layer maybe take look might easier,issue,positive,neutral,neutral,neutral,neutral,neutral
558670763,See https://arxiv.org/abs/1910.14424 for details on how to achieve a top position on e.g MS MARCO with BERT using it as an interaction model meaning you need to encode both query and potential relevant text sentence/passage at the same time and not independently. ,see achieve top position marco interaction model meaning need encode query potential relevant text time independently,issue,negative,positive,positive,positive,positive,positive
558653983,"@jobergum thanks! Very insightful. Though I don't know where this puts my thinking with respect to BERT getting SOTA on semantic text similarity (STS) leaderboards? It's quite a thing for it to say it's an interaction-based model and not a representation model. What do you think about that? Every example I've seen assumes it does have some kind of representation of words and their context, and there's no temporal element I'm aware of. 

Devlin's comment is really interesting, it does challenge some assumptions I personally was making about the utility of these embeddings, and in fact, bert-as-service I think:

> ""I'm not sure what these vectors are, since BERT does not generate meaningful sentence vectors. It seems that this is is doing average pooling over the word tokens to get a sentence vector, but we never suggested that this will generate meaningful sentence representations. And even if they are decent representations when fed into a DNN trained for a downstream task, it doesn't mean that they will be meaningful in terms of cosine distance. (Since cosine distance is a linear space where all dimensions are weighted equally).""",thanks insightful though know thinking respect getting semantic text similarity quite thing say model representation model think every example seen kind representation context temporal element aware comment really interesting challenge personally making utility fact think sure since generate meaningful sentence average word get sentence vector never generate meaningful sentence even decent fed trained downstream task mean meaningful cosine distance since cosine distance linear space weighted equally,issue,positive,positive,positive,positive,positive,positive
558455155,"the other two files look fine. thanks and great job!

regarding the release pipeline, I actually use `./release.sh` for releasing, this includes bumping version, some chore jobs (re-organizing docs), etc. 

right now I will merge it as this Github action file wont affect the release procedure, and people may improve on it in the future.

All in all, thanks for your contribution. ❤️ ",two look fine thanks great job regarding release pipeline actually use bumping version chore right merge action file wont affect release procedure people may improve future thanks contribution,issue,positive,positive,positive,positive,positive,positive
558454261,"wow, thanks for setting up Github action for that! really appreciated!

but I do need some time to check and make sure it works as expected, let me first review the other two files.",wow thanks setting action really need time check make sure work let first review two,issue,positive,positive,positive,positive,positive,positive
558301199,Thanks a lot @4everlove. I will try your way!!!!,thanks lot try way,issue,negative,positive,positive,positive,positive,positive
558300389,"@alelasantillan Basically, you will need to do the tokenization by yourself, either using official BERT's sample code or refer to BERT-as-Service [data generator](https://github.com/hanxiao/bert-as-service/blob/d459f4c098e3f12249ceefada4b9b834eaaf313b/server/bert_serving/server/__init__.py#L549). I then follow BaS to [freeze, optimize and export graph](https://github.com/hanxiao/bert-as-service/blob/d459f4c098e3f12249ceefada4b9b834eaaf313b/server/bert_serving/server/__init__.py#L67). Then as I am using TF Estimator, I would load the graph in my model_fn and then read contextualized word embeddings (my need for BaS, yours might vary):

```python
  # Load freezed BERT graph
  with tf.gfile.GFile(bert_graph_path, 'rb') as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())

  input_map = {'input_ids:0': inp_ids, 'input_mask:0': inp_msk, 'input_type_ids': inp_tid}
  embs = tf.import_graph_def(graph_def, input_map=input_map, return_elements=['final_encodes:0'])[0]
```

For different needs, I recommend reading how the server is written in https://github.com/hanxiao/bert-as-service/blob/d459f4c098e3f12249ceefada4b9b834eaaf313b/server/bert_serving/server/__init__.py

If you are somewhat farmiliar with official BERT codes, it won't be hard to understand it and adapt it to your own needs. Good luck.",basically need either official sample code refer data generator follow ba freeze optimize export graph estimator would load graph read word need ba might vary python load graph different need recommend reading server written somewhat official wo hard understand adapt need good luck,issue,positive,positive,positive,positive,positive,positive
558185646,"Regarding Lambda functions, there are a bunch of limitations that guide users to having relatively small footprint / small start-up / small memory requirements. Even relatively small NLP components like Spacy don't really fit into Lambda without some hackery. So it is probably worth deploying a Fargate (on demand container) or EC2 (always-on) instance where bert-as-service sits patiently waiting for inbound requests. ",regarding lambda bunch guide relatively small footprint small small memory even relatively small like spacy really fit lambda without hackery probably worth demand container instance patiently waiting inbound,issue,negative,negative,neutral,neutral,negative,negative
557976549,"> Downgrading didn't help me. I ended up using BERT directly in my model with some help from Bert-as-Service's code.

@4everlove I am stuck with the same problem, but working on CPU. Can you explain how did you do to use BERT directly in your model? I need to use BERT asap! 
",help ended directly model help code stuck problem working explain use directly model need use,issue,negative,positive,neutral,neutral,positive,positive
557934575,"Same problem here. The bert-as-service is running in one terminal window showing:

I:GRAPHOPT:[gra:opt:128]:load parameters from checkpoint...
I:GRAPHOPT:[gra:opt:132]:optimize...
I:GRAPHOPT:[gra:opt:140]:freeze...
I:GRAPHOPT:[gra:opt:145]:write graph to a tmp file: /tmp/tmpyog8r74t
I:VENTILATOR:[__i:__i: 75]:optimized graph is stored at: /tmp/tmpyog8r74t
I:VENTILATOR:[__i:_ru:129]:bind all sockets
I:VENTILATOR:[__i:_ru:133]:open 8 ventilator-worker sockets
I:VENTILATOR:[__i:_ru:136]:start the sink
I:SINK:[__i:_ru:306]:ready
I:VENTILATOR:[__i:_ge:222]:get devices
W:VENTILATOR:[__i:_ge:246]:no GPU available, fall back to CPU
I:VENTILATOR:[__i:_ge:255]:device map: 
		worker  0 -> cpu
		worker  1 -> cpu
		worker  2 -> cpu
		worker  3 -> cpu
I:WORKER-0:[__i:_ru:531]:use device cpu, load graph from /tmp/tmpyog8r74t
I:WORKER-1:[__i:_ru:531]:use device cpu, load graph from /tmp/tmpyog8r74t
I:WORKER-2:[__i:_ru:531]:use device cpu, load graph from /tmp/tmpyog8r74t
I:WORKER-3:[__i:_ru:531]:use device cpu, load graph from /tmp/tmpyog8r74t

And on another terminal window I run:
python
Python 3.7.3 | packaged by conda-forge | (default, Jul  1 2019, 21:52:21) 
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from bert_serving.client import BertClient
>>> bc = BertClient()

And got stuck there. 
Any idea what's going on?

PS: I don't have a GPU working with CUDA.",problem running one terminal window showing gra opt load gra opt optimize gra opt freeze gra opt write graph file ventilator graph ventilator bind ventilator open ventilator start sink sink ready ventilator get ventilator available fall back ventilator device map worker worker worker worker use device load graph use device load graph use device load graph use device load graph another terminal window run python python default anaconda type help copyright license information import got stuck idea going working,issue,positive,positive,positive,positive,positive,positive
557839853,Downgrading didn't help me. I ended up using BERT directly in my model with some help from Bert-as-Service's code.,help ended directly model help code,issue,positive,positive,neutral,neutral,positive,positive
557785568,"Hi Han, 
I'm still facing the same issue, even though i'm on latest package of BERT.
Could you check if the problem is really resolved on a CPU only machine ?
",hi han still facing issue even though latest package could check problem really resolved machine,issue,negative,positive,positive,positive,positive,positive
556332278,"You need to install tensorflow

pip install tensorflow",need install pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
554599692,"just try      pip install --ignore-installed pyzmq bert-serving-client, and get it",try pip install get,issue,negative,neutral,neutral,neutral,neutral,neutral
554541492,"I fixed this problem by initiating BertClient for every worker in the Multi-process, this way, all BertClients are clones of the original one, and no worker is using the original BertClient at all.",fixed problem every worker way original one worker original,issue,negative,positive,positive,positive,positive,positive
554107061,"> 
> 
> Have you tried using the concurrent one, i.e
> 
> `from bert_serving.client import ConcurrentBertClient as BertClient` ?

Thank you for the reply.

I just tried it, still the same problem",tried concurrent one import thank reply tried still problem,issue,negative,neutral,neutral,neutral,neutral,neutral
554102253,"Have you tried using the concurrent one, i.e

```from bert_serving.client import ConcurrentBertClient as BertClient``` ?",tried concurrent one import,issue,negative,neutral,neutral,neutral,neutral,neutral
553766548,"

> Create a start-bert-as-service.py with the following code
> 
> ```
> import sys
> 
> from bert_serving.server import BertServer
> from bert_serving.server.helper import get_run_args
> 
> 
> if __name__ == '__main__':
>     args = get_run_args()
>     server = BertServer(args)
>     server.start()
>     server.join()
> ```
> 
> so you can run with the following command
> python start-bert-as-service.py -model_dir ./tmp/chinese_L-12_H-768_A-12/ -num_worker=1

@monk1337  Getting this Error using this solution on windows 10 with python 3.6 and tensorflow 2.0.0 .
![image](https://user-images.githubusercontent.com/51500641/68836264-f2d8f580-06df-11ea-93e9-686bfb4fc7a2.png)
",create following code import import import server run following command python monk getting error solution python image,issue,negative,neutral,neutral,neutral,neutral,neutral
552835632,"@AshwatKumar I got a same error. I resolve the issue by changing port number.
This is what I did to get embeddings. I just followed @geoffreyp7 's procedure :)

1. Install a module
`!pip install -U bert-serving-server[http]`

2. Download pre-trained model and unzip it
`!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip`
`!unzip uncased_L-12_H-768_A-12.zip`

3. Set port number
`port_num='3399'`

4. Start the server in the notebook
```
get_ipython().system_raw(
    'bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 -http_port ' + port_num + ' &'
)
```

5. Define this function:
```
import json
import requests
def get_embeddings(texts):
    headers = {
        'content-type':'application/json'
    }
    data = {
        ""id"":123,
        ""texts"":texts,
        ""is_tokenized"": False
    }
    data = json.dumps(data)
    r = requests.post(""http://localhost:"" + port_num + ""/encode"", data=data, headers=headers).json()
    return r['result']
```

6. Get embeddings
```
text_list1=['This is a test sentence.']
text_embeddings1 = get_embeddings(text_list1)
```",got error resolve issue port number get procedure install module pip install model set port number start server notebook define function import import data id false data data return get test sentence,issue,negative,negative,negative,negative,negative,negative
552407766,"I too have the same error. Is there anyway to fix this? Is it because I'm using the latest version of Tensorflow?
I:[35mVENTILATOR[0m:freeze, optimize and export graph, could take a while...
c:\users\avok\appdata\local\continuum\anaconda3\lib\site-packages\bert_serving\server\helper.py:174: UserWarning: Tensorflow 2.0.0 is not tested! It may or may not work. Feel free to submit an issue at https://github.com/hanxiao/bert-as-service/issues/
  'Feel free to submit an issue at https://github.com/hanxiao/bert-as-service/issues/' % tf.__version__)
E:[36mGRAPHOPT[0m:fail to optimize the graph!
Traceback (most recent call last):
  File ""c:\users\avok\appdata\local\continuum\anaconda3\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\users\avok\appdata\local\continuum\anaconda3\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\avok\AppData\Local\Continuum\anaconda3\Scripts\bert-serving-start.exe\__main__.py"", line 9, in <module>
  File ""c:\users\avok\appdata\local\continuum\anaconda3\lib\site-packages\bert_serving\server\cli\__init__.py"", line 4, in main
    with BertServer(get_run_args()) as server:
  File ""c:\users\avok\appdata\local\continuum\anaconda3\lib\site-packages\bert_serving\server\__init__.py"", line 71, in __init__
    self.graph_path, self.bert_config = pool.apply(optimize_graph, (self.args,))
TypeError: cannot unpack non-iterable NoneType object",error anyway fix latest version freeze optimize export graph could take tested may may work feel free submit issue free submit issue fail optimize graph recent call last file line file line code file line module file line main server file line unpack object,issue,positive,positive,positive,positive,positive,positive
552328481,"> While starting bert server, i got this error:
> 
> ![image](https://user-images.githubusercontent.com/10863620/68569033-1f440600-0483-11ea-98c8-d7c107f8f7e3.png)
> 
> Any idea how to overcome this?
this is a usual issue,you can search: start fail or other key word in issues, you can find the way to fix it ,like #467",starting server got error image idea overcome usual issue search start fail key word find way fix like,issue,negative,negative,negative,negative,negative,negative
552326035,"While starting bert server, i got this error:


![image](https://user-images.githubusercontent.com/10863620/68569033-1f440600-0483-11ea-98c8-d7c107f8f7e3.png)

Any idea how to overcome this?",starting server got error image idea overcome,issue,negative,neutral,neutral,neutral,neutral,neutral
552090741,"I am also facing same issues, but on one machine with GPU MX130 it runs with tensorflow1.15 but only on CPU not on GPU, but with same software environment its not working  another machine with GTX 960M. 

I:[36mGRAPHOPT[0m:model config: ./uncased_L-12_H-768_A-12/bert_config.json
I:[36mGRAPHOPT[0m:checkpoint: ./uncased_L-12_H-768_A-12/bert_model.ckpt
E:[36mGRAPHOPT[0m:fail to optimize the graph!
Traceback (most recent call last):
  File ""d:\anaconda3\envs\gpu_test\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""d:\anaconda3\envs\gpu_test\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""D:\Anaconda3\envs\gpu_test\Scripts\bert-serving-start.exe\__main__.py"", line 7, in <module>
  File ""d:\anaconda3\envs\gpu_test\lib\site-packages\bert_serving\server\cli\__init__.py"", line 4, in main
    with BertServer(get_run_args()) as server:
  File ""d:\anaconda3\envs\gpu_test\lib\site-packages\bert_serving\server\__init__.py"", line 71, in __init__
    self.graph_path, self.bert_config = pool.apply(optimize_graph, (self.args,))
TypeError: 'NoneType' object is not iterable

....................................................
Im meeting same error with cuda 9.0, tf=1.10
",also facing one machine environment working another machine model fail optimize graph recent call last file line file line code file line module file line main server file line object iterable meeting error,issue,negative,negative,neutral,neutral,negative,negative
551262792,"@geoffreyp7 - Can you please tell what output are you getting after, running this piece of code 
get_ipython().system_raw(
    'bert-serving-start -model_dir=./cased_L-24_H-1024_A-16 -http_port 3333 &'
)

I am getting error following your methodology.
After calling , text_embeddings = get_embeddings(my_input_list), I am getting this error -

HTTPConnectionPool(host='localhost', port=3333): Max retries exceeded with url: /encode (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fc2daee05c0>: Failed to establish a new connection: [Errno 111] Connection refused',))

Can you tell why it's not being connected to server. I followed exactly what you said. 
Please help, Thanks in advance.",please tell output getting running piece code getting error following methodology calling getting error object establish new connection connection tell connected server exactly said please help thanks advance,issue,positive,positive,positive,positive,positive,positive
548984439,@Nikronic 1.15 works for me. Make sure you extract the model and point to the directory,work make sure extract model point directory,issue,negative,positive,positive,positive,positive,positive
548945743,"@mtedmist I tried all possible way of giving the path and it does not work on tensorflow 1.15>
I have not tried tf 1.10 yet as @TheJdxfh said. I think he is right, because other issues pointing out to tf version and the ones who achieved output, did not used updated tensorflow.",tried possible way giving path work tried yet said think right pointing version output used,issue,negative,positive,positive,positive,positive,positive
548905954,Try giving the absolute path for the model dir,try giving absolute path model,issue,negative,positive,positive,positive,positive,positive
548711397,"> Hi!
> It seems that your model weights are not loaded correctly.
> Please check if layer names are identical for `tvars` and `init_vars` in `get_assignment_map_from_checkpoint` function (`server/bert/modeling.py`)

that's  the problem, thanks ",hi model loaded correctly please check layer identical function problem thanks,issue,negative,positive,positive,positive,positive,positive
548458730,"Hi!
It seems that your model weights are not loaded correctly.
Please check if layer names are identical for `tvars` and `init_vars` in `get_assignment_map_from_checkpoint` function (`server/bert/modeling.py`)",hi model loaded correctly please check layer identical function,issue,negative,neutral,neutral,neutral,neutral,neutral
547832749,"Hi, 
I have the same issue with the fine-tuned BERT. Please answer!



> sorry, can't reproduce.
> 
> ```python
> import numpy.testing
> from bert_serving.client import BertClient
> 
> with BertClient() as bc:
>     numpy.testing.assert_almost_equal(bc.encode(['如何查询社保']), bc.encode(['如何查询社保']))
> 
> # no exception is raised
> ```

This is the wrong way to reproduce. It's different if I run code twice (open server again, create new client), not in the same code with the same running server and client.",hi issue please answer sorry ca reproduce python import import exception raised wrong way reproduce different run code twice open server create new client code running server client,issue,negative,negative,negative,negative,negative,negative
547721646,"@keshavjha123 @shubhangi-tandon @hadisaadat @rcmcabral @craigcitro  I have solved this problem using nohup command, like,
!nohup bert-serving-start -pooling_strategy NONE -max_seq_len 128  -model_dir ./multi_cased_L-12_H-768_A-12 > out.file 2>&1 & ",problem command like none,issue,negative,neutral,neutral,neutral,neutral,neutral
547325601,I downgraded to 1.15 and it did not work. Also I changed source code and removed all loggings and added some breakpoints and found out the model cannot be constructed and code never stops in loggings even before I remove them. ,work also source code removed added found model code never even remove,issue,negative,neutral,neutral,neutral,neutral,neutral
547152633,I had the same issue. Downgraded from TF 2.0 to 1.10 and it worked. Something to do with new logging,issue worked something new logging,issue,negative,positive,positive,positive,positive,positive
545635400,"@shamrockzhao 
I traced the code and found error but I am not familiar with Tensorflow so I could not fix it. 
#469",code found error familiar could fix,issue,negative,positive,positive,positive,positive,positive
545468985,"I have the same issue and it seems related to path but no other mentioned solutions worked. 
Now I need to read the source code and edit it myself.",issue related path worked need read source code edit,issue,negative,neutral,neutral,neutral,neutral,neutral
542949121,Welcome for your contribution. Simply make a pull request and I will approve it. Thanks,welcome contribution simply make pull request approve thanks,issue,positive,positive,positive,positive,positive,positive
541652435,"I am also looking forward to use Tensorflow 2.0 and would like to use it with BAAS.

Since you are testing it right now, what kind of issue are you facing? You have not yet shared any details about that.",also looking forward use would like use since testing right kind issue facing yet,issue,positive,positive,positive,positive,positive,positive
540422710,"Hi,

I have the same issue, but only when trying to embed with a fine tuned model.
I am starting up the BERT server with:
```
bert-serving-start -model_dir=C:/.../multi_cased_L-12_H-768_A-12/ -tuned_model_dir=C:/... -ckpt_name=model.ckpt-6258 -num_worker=1 -max_seq_len=128 -cased_tokenization
```

I am starting up the client with:
```
from bert_serving.client import BertClient
bc = BertClient()
```

When I now use:
```
encoding=bc.encode(['Why does this not work?'])
``` 
This gives: array([[ 5.51824450e-01,  5.25670648e-01,  6.89625621e-01....]

However, when I close the server and start a new one in exactly the same manner and I use:
```
encoding_2=bc.encode(['Why does this not work?'])
```
This gives: array([[-2.78031558e-01, -3.44201356e-01, -1.15424120e+00,...]

This does not happen when using the standard BERT model without fine-tuning. 
I am looking forward to hearing from you if I am doing something wrong.",hi issue trying embed fine tuned model starting server starting client import use work array however close server start new one exactly manner use work array happen standard model without looking forward hearing something wrong,issue,negative,positive,neutral,neutral,positive,positive
539841103,"> > I have tried giving absolute path for the model dir but still giving me same error.
> > What is the fine tuned model? We just download the simple uncase-768 version right?
> 
> No matter which pre-trained model we use, when set absolute path, it works to me.

Thanks for a prompt response. So what can I try now to make it work? Will pasting the screenshot help?",tried giving absolute path model still giving error fine tuned model simple version right matter model use set absolute path work thanks prompt response try make work pasting help,issue,positive,positive,positive,positive,positive,positive
539840058,"> I have tried giving absolute path for the model dir but still giving me same error.
> What is the fine tuned model? We just download the simple uncase-768 version right?

No matter which pre-trained model we use, when set absolute path, it works to me.",tried giving absolute path model still giving error fine tuned model simple version right matter model use set absolute path work,issue,negative,positive,positive,positive,positive,positive
539839038,"Updated the latest version of Tensorflow and have given absolute path still the same error
",latest version given absolute path still error,issue,negative,positive,positive,positive,positive,positive
539838828,"I have tried giving absolute path for the model dir but still giving me same error.
What is the fine tuned model? We just download the simple uncase-768 version right?

",tried giving absolute path model still giving error fine tuned model simple version right,issue,negative,positive,positive,positive,positive,positive
539800811,"you need to turn the `show_token` option on the server side as well, check readme/doc for more details",need turn option server side well check,issue,negative,neutral,neutral,neutral,neutral,neutral
539455383,"> Try giving the absolute path for the model dir and the fine tuned model.

Thanks! It's work.",try giving absolute path model fine tuned model thanks work,issue,positive,positive,positive,positive,positive,positive
538281794,"According to https://arxiv.org/abs/1904.07531 using cosine similarity of the sentence embedding vectors is not great for ranking similarity. Quote from the paper 

>BERT (Rep) uses BERT to represent q and d:
>BERT (Rep)(q,d) = cos(q,d)

>It first uses the last layers’ “[CLS]” embeddings as the query and
>document representations, and then calculates the ranking score via
>their cosine similarity (cos). Thus it is a representation-based ranker


> BERT (Rep) applies BERT on the query and document individually
>and discard these cross sequence interactions, and its performance
>is close to random. BERT is an interaction-based matching model
>and is not suggested to be used as a representation model. 

See also https://github.com/google-research/bert/issues/164#issuecomment-441324222
",according cosine similarity sentence great similarity quote paper rep represent rep co first last query document score via cosine similarity co thus ranker rep query document individually discard cross sequence performance close random matching model used representation model see also,issue,positive,positive,neutral,neutral,positive,positive
537770528,"I think the reason from the sever structure, because, if it was appeared, I try to post same request, it worked. the exception appeared irregular",think reason sever structure try post request worked exception irregular,issue,negative,neutral,neutral,neutral,neutral,neutral
536974353,"> > I also want to KNOW this!
> > @MarvinMogab Do you try to convert the Pytorch model to TF Checkpoint format? I have tried, but failed.
> 
> For me following worked:
> 
> from pytorch_transformers.convert_pytorch_checkpoint_to_tf import main
> 
> main([
> '--model_name', '/home/pbakhtin/models/bert/scibert_fine_tuned_pytorch/',
> '--pytorch_model_path', '/home/pbakhtin/models/bert/scibert_fine_tuned_pytorch/pytorch_model.bin',
> '--tf_cache_dir', '/home/pbakhtin/models/bert/scibert_fine_tuned_pytorch/tf',
> '--cache_dir', '/home/pbakhtin/models/bert/scibert_fine_tuned_pytorch/'
> ])
> 
> The only trick was, due to the fact that I was transforming fine-tuned model, to get TF results, which were hidding in my folder /home/pbakhtin/models/bert/scibert_fine_tuned_pytorch/ . Using ls -a helped to find them.

@pbakhtin Could you elaborate what the individual parameters are, and if I should run it as an individual .py script? Thanks!",also want know try convert model format tried following worked import main main trick due fact transforming model get folder find could elaborate individual run individual script thanks,issue,negative,positive,positive,positive,positive,positive
536458622,"> I try to update the `bert-serving-server` and `bert-serving-client` to version `1.9.8` and it seems to be fine for me.
> I suggest you to update your `bert-as-service`.
> 
> ```python
> # my env: python 3.6, ubuntu 18.04
> pip install -U bert-serving-server bert-serving-client
> ```

ok，i will try it ,thks.",try update version fine suggest update python python pip install try,issue,negative,positive,positive,positive,positive,positive
536396322,"I try to update the `bert-serving-server` and `bert-serving-client` to version `1.9.8` and it seems to be fine for me.
I suggest you to update your `bert-as-service`.

```python
# my env: python 3.6, ubuntu 18.04
pip install -U bert-serving-server bert-serving-client
```",try update version fine suggest update python python pip install,issue,negative,positive,positive,positive,positive,positive
536392180,"> I have the same question. But sometimes it works fine, and I cannot troubleshooting these issue.

yes, and it not happend in my win10 OS, only in unbuntu",question sometimes work fine issue yes win o,issue,positive,positive,positive,positive,positive,positive
536391688,"I have the same question. But sometimes it works fine, and I cannot troubleshooting this issue.",question sometimes work fine issue,issue,negative,positive,positive,positive,positive,positive
536249462,"Thanks for the explanation, I tried ""-show_tokens_to_client"" and it worked.",thanks explanation tried worked,issue,negative,positive,positive,positive,positive,positive
536248994,"it's a flag, it should not be used with true/false. With it then it is true, without it then it is false.

`-show_tokens_to_client` that's it.

please read the doc of `argparse` if you have question about this behavior.",flag used true without false please read doc question behavior,issue,negative,negative,neutral,neutral,negative,negative
533643221,Sorry for the duplicate Issue. Same problem as in #448 ,sorry duplicate issue problem,issue,negative,negative,negative,negative,negative,negative
533611360,"oh, sorry to bring that bug. working on it",oh sorry bring bug working,issue,negative,negative,negative,negative,negative,negative
533602645,"I met this problem too, I think the author missed a 'not' in condition, it should be “if len(tokens_a) > max_seq_length - 2 and **not** (no_special_token and is_tokenized)” ",met problem think author condition,issue,negative,neutral,neutral,neutral,neutral,neutral
533063464,"@qiunian711

First, rename:
`bert-base-german.data-00000-of-00001` to 
`bert-base-german-cased.data-00000-of-00001`
so that you have `index`, `meta` and `data` files with the same name. The content of the folder should look like this:
```
bert-base-german-cased.index
bert-base-german-cased.meta
bert-base-german-cased.data-00000-of-00001
bert_config.json
vocab.txt
```
Now, you need to pass an additional parameter to `bert-serving-start`:
```
bert-serving-start -model_dir <folder-name> -ckpt_name bert-base-german-cased
```
This is because Bert expects `ckpt_name = bert_model.ckpt` by default, as shown in the log under 
`ARG   VALUE` table. 

Another option would be to rename `index`, `meta` and `data` files to `bert_model.ckpt.index`, etc.",first rename index meta data name content folder look like need pas additional parameter default shown log value table another option would rename index meta data,issue,positive,positive,positive,positive,positive,positive
532469650,"I have the same confusion. It seems that:
<1> When I start the server with the command ""bert-serving-start"", **the bertServer is using GPU**. 
<2> But when I use bertClient to encode sentences, **the bertClient is not using GPU**. 
Can anybody explain this? Is this package ""bert-as-service"" designed to run like this?",confusion start server command use encode anybody explain package designed run like,issue,negative,neutral,neutral,neutral,neutral,neutral
532316407,It's much better. I'm really  not good at this😂😂😂,much better really good,issue,positive,positive,positive,positive,positive,positive
531772102,Hi @jihunchoi : do you have any updates on this? I want to use the server in a programmatic way and therefore it would be nice to be able to kill it properly when done.,hi want use server programmatic way therefore would nice able kill properly done,issue,negative,positive,positive,positive,positive,positive
531684648,"The problem is due to http service but not bert service. After changing gunicorn worker-class from `gevent` to `sync`, the problem has been resolved.",problem due service service sync problem resolved,issue,negative,negative,negative,negative,negative,negative
531120367,"I had the same issue, with 8GB machine, after upgraded to 32GB and I'm able to run with 2 workers.

However, when running more than 2 workers on 8 Cores 32GB machine, the issue appears again.

There are heaps of memory available

See the log below

(bert-as-service) sh-4.2$ bert-serving-start -model_dir ./multi_cased_L-12_H-768_A-12 -num_worker=3 -cpu
.............
OMP: Info #250: KMP_AFFINITY: pid 26690 tid 26690 thread 0 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 26687 tid 26753 thread 1 bound to OS proc set 1
OMP: Info #250: KMP_AFFINITY: pid 26690 tid 26776 thread 1 bound to OS proc set 1
OMP: Info #250: KMP_AFFINITY: pid 26684 tid 26751 thread 1 bound to OS proc set 1
terminate called after throwing an instance of 'std::bad_alloc'
  what():  std::bad_alloc
I:WORKER-2:[__i:gen:558]:ready and listening!
I0913 06:26:32.240278 140128425056000 __init__.py:558] ready and listening!
I:WORKER-0:[__i:gen:558]:ready and listening!
I0913 06:26:32.241424 140128156620544 __init__.py:558] ready and listening!",issue machine able run however running machine issue heap memory available see log tid thread bound o set tid thread bound o set tid thread bound o set tid thread bound o set terminate throwing instance gen ready listening ready listening gen ready listening ready listening,issue,positive,positive,positive,positive,positive,positive
530968462,You could try [BERT as a language model](https://github.com/xu-song/bert-as-language-model). As far as i know doesn't BERT as a service support the calculation of the word probability.,could try language model far know service support calculation word probability,issue,negative,positive,neutral,neutral,positive,positive
529029636,"Hi, Im facing the same issue. How were you guys able to run it finally ?
",hi facing issue able run finally,issue,negative,positive,positive,positive,positive,positive
527888581,"@Alexadar Hey, I used the BertPooler and OnlyNSPHead from the pretrained Bert model from pytorch_transformers and put it after the bert-as-a-server output with -pooling-strategy None and -pooling_layer -1 ... that gave me the same results as with the pytorch_transformer class BertForNextSentencePrediction",hey used model put output none gave class,issue,negative,neutral,neutral,neutral,neutral,neutral
527517806,"Got it to work, but I had to run 
`!pip install --upgrade ipykernel`
first, otherwise I couldn't reconnect to runtime (would stay on 'Busy').",got work run pip install upgrade first otherwise could reconnect would stay,issue,negative,positive,positive,positive,positive,positive
526425167,"i solved this at docker option.
I think this problem caused by docker's permission. add privileged docker-option.",docker option think problem docker permission add privileged,issue,negative,neutral,neutral,neutral,neutral,neutral
523411696,"> I stumbled on this issue almost an hour ago, and sadly none of the solutions above worked for me.
> 
> So I started diagnosing the problem on my own and here is the fix I found, as well as the explanation to it (hopefully it will help other people in my situation). blush
> 
> It really was the root cause in my case, since undoing the fix caused the server to fallback to CPU, and redoing it re-enabled GPU usage by the server each time.
> ### TD;LR
> 
> `sudo nvidia-smi -pm 1`
> ### Low-level stuff
> 
> The NVIDIA kernel driver has (among many other options) two modes: persistent and non-persistent.
> 
> Roughly speaking this comes down to a trade-off: **non-persistent mode** means the GPU will try to remain **idle as long as possible** (to save battery etc.) and it will fire up on connections (loading the driver application, triggering ECC scrubbing and so on), which can take many seconds.
> 
> On the other hand, the **persistent mode** will trigger it all once and keep the driver application loaded and ready for connections (at the expense of greater battery usage).
> [cf. [driver-persistence](https://docs.nvidia.com/deploy/driver-persistence/index.html)]
> 
> This also explains the strangely high volatile GPU-Util in ""nvidia-smi"" (for me it was consistently 90-99%) in non-persistent mode, even with no other programs using the GPU (!): the program `nvidia-smi` itself is querying the GPU, hence ""activating"" it and causing the spikes aforementioned. By the way, this was the event that caught my attention and led to this solution.
> It's funny how this reminded me of the [observer effect](https://en.wikipedia.org/wiki/Observer_effect_(physics)) in physics: the observation itself is changing the phenomenon, but I digress...
> [reference: [this discussion](https://devtalk.nvidia.com/default/topic/539632/k20-with-high-utilization-but-no-compute-processes-/)]
> 
> So I thought: doesn't the BERT server check for GPU availability before allocation? If it does, and if the GPU is idle (i.e. if the driver is configured not to persist), it will trigger the driver application to wake up and the GPU to spike like crazy (for some seconds), and by doing this, **the GPU makes itself unavailable** to clients demanding a **minimum level of availability**.
> And indeed, [this line of code](https://github.com/hanxiao/bert-as-service/blob/9e8824e830bb25fd16304c804244a64629a79b23/server/bert_serving/server/__init__.py#L196) requires the GPU to have a maximum load of 90% (as well as a maximum memory usage of 90%). So in the event of temporary unavailability, it will return an empty list (I tested), even when all else seems good (GPU is there, CUDA driver configured, tensorflow-gpu installed etc.).
> 
> So the command line in the TL;DR above, which sets persistence mode in the GPUs, was the only fix for me, since it will cause the GPU to be much more stable, and thus bypassing the ""spikes on connection"" that makes it so busy it cannot connect.
> 
> Hope it helps!


nvidia-smi not showing any running process while training the model

@swubb @hanxiao @shokosanma 
",issue almost hour ago sadly none worked problem fix found well explanation hopefully help people situation blush really root cause case since undoing fix server fallback usage server time stuff kernel driver among many two persistent roughly speaking come mode try remain idle long possible save battery fire loading driver application take many hand persistent mode trigger keep driver application loaded ready expense greater battery usage also strangely high volatile consistently mode even program querying hence causing way event caught attention led solution funny observer effect physic observation phenomenon digress reference discussion thought server check availability allocation idle driver persist trigger driver application wake spike like crazy unavailable demanding minimum level availability indeed line code maximum load well maximum memory usage event temporary unavailability return empty list tested even else good driver command line persistence mode fix since cause much stable thus connection busy connect hope showing running process training model,issue,positive,positive,positive,positive,positive,positive
523410252,"> I had the same issue. Uninstalling tensorflow and installing tensorflow-gpu fixed this for me.

nvidia-smi not showing any running process while training the model

![Screenshot from 2019-08-21 16-03-29](https://user-images.githubusercontent.com/17041720/63427182-2cc49380-c432-11e9-8655-30e743027035.png)
",issue fixed showing running process training model,issue,negative,positive,neutral,neutral,positive,positive
523403263,"nvidia-smi not showing any running process while training the model

![Screenshot from 2019-08-21 16-03-29](https://user-images.githubusercontent.com/17041720/63425735-e3267980-c42e-11e9-9932-7313963627c1.png)
",showing running process training model,issue,negative,neutral,neutral,neutral,neutral,neutral
522907411,"**Bert base**
```shell
 $ ls -al multi_cased_L-12_H-768_A-12
total 684M
-rw-r--r-- 1 hm-kono hm-kono  521 11월 24  2018 bert_config.json
-rw-r--r-- 1 hm-kono hm-kono 682M 11월 24  2018 bert_model.ckpt.data-00000-of-00001
-rw-r--r-- 1 hm-kono hm-kono 8.5K 11월 24  2018 bert_model.ckpt.index
-rw-r--r-- 1 hm-kono hm-kono 888K 11월 24  2018 bert_model.ckpt.meta
-rw-r--r-- 1 hm-kono hm-kono 973K 11월 24  2018 vocab.txt
```

**How I run bert server**
```shell 
$ bert-serving-start -model_dir ../bert/models/multi_cased_L-12_H-768_A-12 -num_worker=1 -show_tokens_to_client
```

**Client**
```python
In [53]: bc.encode(['아버지의 가방'], show_tokens=True)[1]
Out[53]: [['[CLS]', '[UNK]', '[UNK]', '[SEP]']]

In [55]: bc.encode(['你最近怎麼樣'], show_tokens=True)[1]
Out[55]: [['[CLS]', '你', '最', '近', '怎', '麼', '樣', '[SEP]']]
```
Could you have time to look at this? 
Recognizing Chineese text looks fine but not Korean text. ",base shell total run server shell client python could time look text fine text,issue,negative,negative,negative,negative,negative,negative
522840005,"bert-serving-server目前还不支持tensorflow2.0？
TensorFlow version: 2.0.0b1
Python version: 3.7.3
bert-serving-server version: 1.8.0
bert-serving-client  version: 1.9.6

bert-serving-start -model_dir=E:/code/NLP/chinese_L-12_H-768_A-12 -num_worker=2

Traceback (most recent call last):
  File ""g:\python\lib\runpy.py"", line 193, in _run_module_as_main ""__main__"", mod_spec)
  File ""g:\python\lib\runpy.py"", line 85, in _run_code exec(code, run_globals)
  File ""G:\Python\Scripts\bert-serving-start.exe\__main__.py"", line 5, in <module>
  File ""g:\python\lib\site-packages\bert_serving\server\__init__.py"", line 29, in <module>  _tf_ver_ = check_tf_version()
  File ""g:\python\lib\site-packages\bert_serving\server\helper.py"", line 141, in check_tf_version assert int(tf_ver[0]) >= 1 and int(tf_ver[1]) >= 10, 'Tensorflow >=1.10 is required!'
AssertionError: Tensorflow >=1.10 is required!",version python version version version recent call last file line file line code file line module file line module file line assert,issue,negative,neutral,neutral,neutral,neutral,neutral
522394679,"Hi . I used to have the same issue ,but I solved it. I find my model path is wrong and I changed it，Then the problem is solved. I hope it can help you. : )",hi used issue find model path wrong problem hope help,issue,negative,negative,negative,negative,negative,negative
522391054,@Masum06  The dir statement change does not solve the problem. @dhairyadalal  How do you solve the no json file exactly? Can you explain this?,statement change solve problem solve file exactly explain,issue,negative,positive,positive,positive,positive,positive
521118162,"@hanxiao 
It is stated everywhere that the [CLS] token is the correct sentence representation for a fine tuned model.
I fine tuned bert base uncased model for sentiment analysis task.
The eval accuracy is really good.

I aim to interpret the false positives and what lead to them.
I thought finding similar representations in the training set as the test sample, would give me some insight so as to what lead to the prediction.

I've taken [CLS] token representation from the last layer of the fine tuned model.
The sentences I get similar to a false positive, do not look similar but they do share the same sentiment as predicted.
<img width=""1009"" alt=""Screenshot 2019-08-14 at 11 53 32 AM"" src=""https://user-images.githubusercontent.com/25073753/62998798-3a09dd00-be8a-11e9-8330-c65da3fa0cf6.png"">

What is going on here?
Is it really the correct representation.

I would also like to know how this representation is learnt by CLS token, if all the tokens can look at the left and right context and itself., How is it that CLS is capturing the entire sentence's meaning?

Thanks in advance.
",stated everywhere token correct sentence representation fine tuned model fine tuned base uncased model sentiment analysis task accuracy really good aim interpret false lead thought finding similar training set test sample would give insight lead prediction taken token representation last layer fine tuned model get similar false positive look similar share sentiment going really correct representation would also like know representation learnt token look left right context entire sentence meaning thanks advance,issue,positive,positive,neutral,neutral,positive,positive
520808908,"There is a difference in the calculation and hence the scores, 
you can change the normalization in the query to np.sum(query_vec * doc_vecs, axis=1) / (np.linalg.norm(doc_vecs, axis=1) * np.linalg.norm(query_vec))
This query essentially does the same thing as what you are doing, with the only difference in normalization.

Both your exclaimed cases [1,2] can be inferred from either.
I do not understand what you are really trying to mean when you say similar query sentences, I am adding an attachment for your reference.
Hope this helps.
<img width=""1005"" alt=""check2"" src=""https://user-images.githubusercontent.com/25073753/62940594-89e89580-bdf1-11e9-8346-4798cc483c52.png"">
<img width=""1002"" alt=""check3"" src=""https://user-images.githubusercontent.com/25073753/62940726-d0d68b00-bdf1-11e9-9c87-a30128b751d9.png"">
",difference calculation hence change normalization query query essentially thing difference normalization either understand really trying mean say similar query attachment reference hope check check,issue,positive,negative,neutral,neutral,negative,negative
519978030,This also happens to me. @danilo-augusto solution not working for me. The server only uses cpu although it says it uses gpu in the server log,also solution working server although server log,issue,negative,neutral,neutral,neutral,neutral,neutral
519183706,I am having the exact same issue. I followed your suggestion to get the latest version but it still didn't work.,exact issue suggestion get latest version still work,issue,negative,positive,positive,positive,positive,positive
518477286,"Hi,
I have a similar issue but is running on Colab.
>  I:VENTILATOR:[__i:_ru:129]:bind all sockets
I0806 02:49:48.244951 139764003120896 __init__.py:129] bind all sockets
Exception in thread Thread-1:
Traceback (most recent call last):
  File ""/usr/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/usr/local/lib/python3.6/dist-packages/bert_serving/server/__init__.py"", line 115, in run
    self._run()
  File ""/usr/local/lib/python3.6/dist-packages/zmq/decorators.py"", line 75, in wrapper
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/zmq/decorators.py"", line 75, in wrapper
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/zmq/decorators.py"", line 75, in wrapper
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/bert_serving/server/zmq_decor.py"", line 27, in wrapper
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/bert_serving/server/__init__.py"", line 131, in _run
    addr_front2sink = auto_bind(sink)
  File ""/usr/local/lib/python3.6/dist-packages/bert_serving/server/helper.py"", line 197, in auto_bind
    socket.bind('ipc://{}'.format(tmp_dir))
  File ""zmq/backend/cython/socket.pyx"", line 555, in zmq.backend.cython.socket.Socket.bind
  File ""zmq/backend/cython/checkrc.pxd"", line 25, in zmq.backend.cython.checkrc._check_rc
zmq.error.ZMQError: Function not implemented",hi similar issue running ventilator bind bind exception thread recent call last file line file line run file line wrapper return file line wrapper return file line wrapper return file line wrapper return file line sink file line file line file line function,issue,negative,neutral,neutral,neutral,neutral,neutral
516315081,"> Sorry I can't help with this but I couldn't help noticing your memory footprints were quite low compared to what they look like on my local dev machine. For English BERT-large it was startup size of 16GB and stable at around 6-7GB. Is the Chinese model much smaller?

I am using the BERT-Base model which has only 1/3 the parameters. So yes, the Chinese model is much smaller because BERT-Large is not available for Chinese.",sorry ca help could help memory quite low look like local dev machine size stable around model much smaller model yes model much smaller available,issue,positive,negative,neutral,neutral,negative,negative
516044794,Sorry I can't help with this but I couldn't help noticing your memory footprints were quite low compared to what they look like on my local dev machine. For English BERT-large it was startup size of 16GB and stable at around 6-7GB. Is the Chinese model much smaller? ,sorry ca help could help memory quite low look like local dev machine size stable around model much smaller,issue,positive,negative,negative,negative,negative,negative
516042057,"> I have fixed the issue using kaggle/python base image. But I am still not sure what was the root cause of the problem.

Great! How did you fix it? ",fixed issue base image still sure root cause problem great fix,issue,positive,positive,positive,positive,positive,positive
515511802,"Wow man, that looks like exactly what I need!",wow man like exactly need,issue,positive,positive,positive,positive,positive,positive
515446981,"You may want to look at https://github.com/gnes-ai/gnes , which is my latest project. Feel free to check it out",may want look latest project feel free check,issue,positive,positive,positive,positive,positive,positive
515446767,"It's much easier to use GNES to do that https://github.com/gnes-ai/gnes , which is my latest project. Feel free to check it out",much easier use latest project feel free check,issue,positive,positive,positive,positive,positive,positive
515446458,"Hi, you may want to look at my latest project: https://github.com/gnes-ai/gnes",hi may want look latest project,issue,negative,positive,positive,positive,positive,positive
515250669,Using kaggle/python as base image fixed the problem and should work for everyone. But not sure what was the root cause of the issue. It seems zeromq was unable to bind the sockets.,base image fixed problem work everyone sure root cause issue unable bind,issue,negative,negative,negative,negative,negative,negative
515250090,I have fixed the issue using kaggle/python base image. But I am still not sure what was the root cause of the problem. ,fixed issue base image still sure root cause problem,issue,negative,negative,neutral,neutral,negative,negative
514985827,Try giving the absolute path for the model dir and the fine tuned model.,try giving absolute path model fine tuned model,issue,negative,positive,positive,positive,positive,positive
514587264,Same here guys. Gets completely stuck. I tried dowgrading to 1.9.2 as suggested and it did not work.,completely stuck tried work,issue,negative,positive,neutral,neutral,positive,positive
514505674,"Hi, hanxiao!
I read the document of [Client API](https://bert-as-service.readthedocs.io/en/latest/source/client.html#client.BertClient.encode) , but got confused on the parameter` timeout`. 

Is the timeout for `encode` function using the same value of the timeout for `receive operation on the client` that you defined on the class client.BertClient()?

I am using Client version 1.9.2, I can not specify timeout value for only encoding stage:
`bert_clinet.encode([text], timeout=3000)` 
`TypeError: got an unexpected keyword argument 'timeout'`",hi read document client got confused parameter encode function value receive operation client defined class client version specify value stage text got unexpected argument,issue,negative,negative,negative,negative,negative,negative
514479024,"Dear all, bumping this issue -- has anyone found a way to use ""XL-net as a service""?",dear bumping issue anyone found way use service,issue,negative,neutral,neutral,neutral,neutral,neutral
514275846,"I had not find the root reason.
So, I have to start a new terminal with running
```
import tensorflow as tf
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
```

after that, run bert-as-service, and GPU works well.

Maybe you can have a try.",find root reason start new terminal running import sess run work well maybe try,issue,negative,positive,positive,positive,positive,positive
514165970,"Tried to use [the vanilla Docker image](https://github.com/hanxiao/bert-as-service/tree/master/docker) to start only one model, still memory leak.

_However,_ once we removed external IP from the server, i.e. no access from outside internet, the leak was gone.

Therefore my conclusion was that someone was probing the server, probably at port 5555, and failing (because CPU usage was quite low). Yet the memory leak was real, because despite failing to trigger the server to encode, memory usage was increasing. Maybe the leak arose from handling failed requests?",tried use vanilla docker image start one model still memory leak removed external server access outside leak gone therefore conclusion someone server probably port failing usage quite low yet memory leak real despite failing trigger server encode memory usage increasing maybe leak arose handling,issue,negative,positive,neutral,neutral,positive,positive
514090395,"Another approach is called ""further pre-training"" which builds on the horizontal pre-training. Has anyone tried this in this community? [Here's a paper](https://www.scihive.org/paper/1905.05583) that shows favourable ""domain-specific pre-training"" outcomes",another approach horizontal anyone tried community paper,issue,negative,neutral,neutral,neutral,neutral,neutral
514070600,"I removed the http component, and the leak is still occurring. There are now 3 clusters of processes per model.

Changes:
- Instead of `pip install -U bert-serving-server[http]==1.9.6`, I used `pip install -U bert-serving-server==1.9.6`.
- Removed `-http_port 8000`.",removed component leak still per model instead pip install used pip install removed,issue,negative,neutral,neutral,neutral,neutral,neutral
513745917,"> I was trying to do the same thing. The vector I got has values ranging from -3 to 3, while in bert-as-service its around -1 to 16 (most values between -1 and 1, with a outlier 16).
> 
> What I did was get the second to last hidden states, truncate the paddings, do the average, same as the bert-as-service at least I think. Still trying to understand what makes the difference.

I also noticed the performance gap between bert-as service and pytorch-transformer , and read the source codes.However, It's still a problem... Right now,I just use the sulution: finetuning with pytorch and convert torch version of saved model to tf format version to get the better performance.",trying thing vector got ranging around outlier get second last hidden truncate average least think still trying understand difference also performance gap service read source still problem right use convert torch version saved model format version get better performance,issue,negative,positive,neutral,neutral,positive,positive
513674404,"> > Even if open another terminal and run a python script including following code?
> > ```
> > from bert_serving.client import BertClient
> > bc = BertClient()
> > res = bc.encode(['Hello world!'])
> > print(res)
> > ```
> 
> When it runs ""bc=BertClient()"",it stops here.....

I've met this situation.
You may check your RAM/GPU memory occupancy when the program stuck.",even open another terminal run python script following code import world print met situation may check memory occupancy program stuck,issue,negative,neutral,neutral,neutral,neutral,neutral
513623378,"aha, you are totally right! appreciated, thanks a lot 🙇 ",aha totally right thanks lot,issue,negative,positive,positive,positive,positive,positive
513530476,"> Even if open another terminal and run a python script including following code?
> 
> ```
> from bert_serving.client import BertClient
> bc = BertClient()
> res = bc.encode(['Hello world!'])
> print(res)
> ```
When it runs ""bc=BertClient()"",it stops here.....",even open another terminal run python script following code import world print,issue,negative,neutral,neutral,neutral,neutral,neutral
513362558,@boxabirds Hey I have also run into the same problem. Please let me know if you have found a fix or any alternative to this,hey also run problem please let know found fix alternative,issue,negative,neutral,neutral,neutral,neutral,neutral
513324118,"@Kup3a Have you found the solution yet ? I am facing the same issue. The server never seems to start in a docker with a HTTP request, However locally it works fine. 

@hanxiao do you have the fix for the cpu only version. ",found solution yet facing issue server never start docker request however locally work fine fix version,issue,negative,positive,positive,positive,positive,positive
513301827,"I was trying to do the same thing. The vector I got has values ranging from -3 to 3, while in bert-as-service its around -1 to 16 (most values between -1 and 1, with a outlier 16).

What I did was get the second to last hidden states, truncate the paddings, do the average, same as the bert-as-service at least I think. Still trying to understand what makes the difference.",trying thing vector got ranging around outlier get second last hidden truncate average least think still trying understand difference,issue,negative,negative,negative,negative,negative,negative
513271171,"This is the source code about the pooling strategy, it may be help!
![image](https://user-images.githubusercontent.com/27205782/61546341-fa62f780-aa7b-11e9-9301-8518e5ddbd4a.png)
[https://github.com/hanxiao/bert-as-service/blob/master/server/bert_serving/server/graph.py](url)",source code strategy may help image,issue,negative,neutral,neutral,neutral,neutral,neutral
512766183,This problem persists in the latest version(1.9.3) again. When i downgraded to 1.9.2 as suggested by @sauravsahay it is working fine. ,problem latest version working fine,issue,negative,positive,positive,positive,positive,positive
512652107,"> > I also want to KNOW this!
> > @MarvinMogab Do you try to convert the Pytorch model to TF Checkpoint format? I have tried, but failed.
> 
> For me following worked:
> 
> from pytorch_transformers.convert_pytorch_checkpoint_to_tf import main
> 
> main([
> '--model_name', '/home/pbakhtin/models/bert/scibert_fine_tuned_pytorch/',
> '--pytorch_model_path', '/home/pbakhtin/models/bert/scibert_fine_tuned_pytorch/pytorch_model.bin',
> '--tf_cache_dir', '/home/pbakhtin/models/bert/scibert_fine_tuned_pytorch/tf',
> '--cache_dir', '/home/pbakhtin/models/bert/scibert_fine_tuned_pytorch/'
> ])
> 
> The only trick was, due to the fact that I was transforming fine-tuned model, to get TF results, which were hidding in my folder /home/pbakhtin/models/bert/scibert_fine_tuned_pytorch/ . Using ls -a helped to find them.

@pbakhtin  Thanks!!!
I success to transform the pytorch models into tf checkpoints! However, I make some changes when fail to use the `main` function, the code is following:
```
from pytorch_transformers.convert_pytorch_checkpoint_to_tf import convert_pytorch_checkpoint_to_tf

model  = BertModel.from_pretrained(""./data/finetuned_lm"")
convert_pytorch_checkpoint_to_tf(model, ""./data/finetuned_lm_tf"", ""fine_tuned_tf"")
```",also want know try convert model format tried following worked import main main trick due fact transforming model get folder find thanks success transform however make fail use main function code following import model model,issue,negative,positive,neutral,neutral,positive,positive
512574548,"> Is it possible to load a model fine-tuned with [huggingface's](https://github.com/huggingface/pytorch-pretrained-BERT) pytorch implementation?

For me following worked:

from pytorch_transformers.convert_pytorch_checkpoint_to_tf import main

main([
'--model_name', '/home/pbakhtin/models/bert/scibert_fine_tuned_pytorch/',
'--pytorch_model_path', '/home/pbakhtin/models/bert/scibert_fine_tuned_pytorch/pytorch_model.bin',
'--tf_cache_dir', '/home/pbakhtin/models/bert/scibert_fine_tuned_pytorch/tf',
'--cache_dir', '/home/pbakhtin/models/bert/scibert_fine_tuned_pytorch/'
])

The only trick was, due to the fact that I was transforming fine-tuned model, to get TF results, which were hidding in my folder /home/pbakhtin/models/bert/scibert_fine_tuned_pytorch/ . Using ls -a helped to find them.",possible load model implementation following worked import main main trick due fact transforming model get folder find,issue,negative,positive,neutral,neutral,positive,positive
512574232,"> I also want to KNOW this!
> @MarvinMogab Do you try to convert the Pytorch model to TF Checkpoint format? I have tried, but failed.

For me following worked:

from pytorch_transformers.convert_pytorch_checkpoint_to_tf import main

main([
    '--model_name', '/home/pbakhtin/models/bert/scibert_fine_tuned_pytorch/',
    '--pytorch_model_path', '/home/pbakhtin/models/bert/scibert_fine_tuned_pytorch/pytorch_model.bin',
    '--tf_cache_dir', '/home/pbakhtin/models/bert/scibert_fine_tuned_pytorch/tf',
    '--cache_dir', '/home/pbakhtin/models/bert/scibert_fine_tuned_pytorch/'
])

The only trick was, due to the fact that I was transforming fine-tuned model, to get TF results, which were hidding in my folder /home/pbakhtin/models/bert/scibert_fine_tuned_pytorch/ . Using ls -a helped to find them.",also want know try convert model format tried following worked import main main trick due fact transforming model get folder find,issue,negative,positive,neutral,neutral,positive,positive
511236932,OK - solved my problem.  My model was built using 'uncased_L-12_H-768_A-12' pre-trained model but I was trying to start the service using the 'cased_L-12_H-768_A-12.'   ,problem model built model trying start service,issue,negative,neutral,neutral,neutral,neutral,neutral
511211337,"> I had trouble getting a stable running system with RAM as high as 26GB on a default instance out on GCP. Once I upgraded to 52GB, it seemed to run fine.

@kevin-rowney thank you! Is this an inference-only setup or was it also doing fine-tuning? I'm thinking it makes sense to separate inference-only as it seems at least on my local machine to work happily in ""only a few GB"") with any additional fine-tuning. 

Have you had much experience with resource consumption with multiple workers? ",trouble getting stable running system ram high default instance run fine thank setup also thinking sense separate least local machine work happily additional much experience resource consumption multiple,issue,positive,positive,positive,positive,positive,positive
511082386,"I had trouble getting a stable running system with RAM as high as 26GB on a default instance out on GCP.  Once I upgraded to 52GB, it seemed to run fine.",trouble getting stable running system ram high default instance run fine,issue,negative,positive,positive,positive,positive,positive
511068833,"I increased the memory footprint on my instance to 52GB, and it runs fine now.

Sorry for the false alarm.",memory footprint instance fine sorry false alarm,issue,negative,negative,negative,negative,negative,negative
510932524,"I'm seeing a similar issue.  Did a pip install of bert-serving, python 3.6, conda environment.
ValueError: Shape of variable bert/embeddings/LayerNorm/beta:0 ((1024,)) doesn't match with shape of tensor bert/embeddings/LayerNorm/beta ([768]) from checkpoint reader.  Running the MRPC pre-trained model with:
`bert-serving-start -num_worker=1 -model_dir=./bert/wwm_uncased_L-24_H-1024_A-16/ -tuned_model_dir=./sc_mrpc/ -ckpt_name=model.ckpt-343`

I have - tensorflow-gpu-1.14.0 and bert-serving-server-1.9.3 installed.
`
Traceback (most recent call last):
  File ""/home/srinivas/anaconda3/envs/semantic_similarity/bin/bert-serving-start"", line 10, in <module>
    sys.exit(main())
  File ""/home/srinivas/anaconda3/envs/semantic_similarity/lib/python3.6/site-packages/bert_serving/server/cli/__init__.py"", line 4, in main
    with BertServer(get_run_args()) as server:
  File ""/home/srinivas/anaconda3/envs/semantic_similarity/lib/python3.6/site-packages/bert_serving/server/__init__.py"", line 70, in __init__
    self.graph_path, self.bert_config = pool.apply(optimize_graph, (self.args,))
TypeError: 'NoneType' object is not iterable`",seeing similar issue pip install python environment shape variable match shape tensor reader running model recent call last file line module main file line main server file line object iterable,issue,negative,positive,neutral,neutral,positive,positive
510279431,"I'm getting it as well.  The threads appear to report that they are ""ready and listening!"" but the client hangs when I try to connect.    Wondering if it's because I don't have enough RAM on this instance.  Am all the way up to a 13GB RAM instance, so maybe I should try for 16GB....or larger? 

@ice-melt:  How much RAM was on your system when you ran this?",getting well appear report ready listening client try connect wondering enough ram instance way ram instance maybe try much ram system ran,issue,positive,positive,positive,positive,positive,positive
509783273,"> > 5556
> 
> What does port=5555, port_out=5556 mean? Do you have any reference? Thanks in advance!

The server architecture used by `bert-as-a-service` is asynchronous. They are the port numbers for the inbound request and the outbound response back to the client. ",mean reference thanks advance server architecture used asynchronous port inbound request outbound response back client,issue,negative,negative,neutral,neutral,negative,negative
508584864,a simple solution provided in my latest issue #400,simple solution provided latest issue,issue,negative,positive,positive,positive,positive,positive
508447629,"thanks for a detailed report, will check asap",thanks detailed report check,issue,negative,positive,positive,positive,positive,positive
508301412,"@David-webb I must have made it a mistake, I mean I solved my problem by predicting a dir(thousands of long txt) just call `estimator.predict` one time, it saved me a lot of time, and I can't not speed up `estimator.predict`, here is the code I used to solve my own problem:
`def predicts(file_dic):
    if FLAGS.do_predict:
        predict_examples = []
        text_n_len = []
        for (index, file) in enumerate(file_dic):
            with open(file, 'r', encoding='utf-8', errors='ignore') as f:
                text = f.read()
                text = re.sub(r""[^\u4E00-\u9FA5]"", '', text)
                text = re.sub(""转载|首发|喜讯|推荐|编辑|人气|禁闻|大纪元|新唐人|文化大观|翻墙必看|健康医疗|一周大事解读|热点透视"", '', text)

                # if len(text) > 510:
                #     text = text[:128] + text[-382:]
                # n = 1
                # text_n_len.append(n)
                # guid = 'test-%d' % index
                # text_a = tokenization.convert_to_unicode(text)
                # predict_examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label='0'))


                if len(text) > 510:
                    n = len(text)//510
                    t = 0
                    for i in range(n):
                        text_ = text[i*510:(i+1)*510]
                        guid = 'test-%d' % index
                        text_a = tokenization.convert_to_unicode(text_)
                        predict_examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label='0'))
                        t += 1
                        if t > 9:
                            break
                        if len(text[n * 510:]) > 50:
                            guid = 'test-%d' % index
                            text_1 = text[n * 510:]
                            text_a = tokenization.convert_to_unicode(text_1)
                            predict_examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label='0'))
                            t += 1
                    text_n_len.append(t)
                else:
                    n = 1
                    text_n_len.append(n)
                    guid = 'test-%d' % index
                    text_a = tokenization.convert_to_unicode(text)
                    predict_examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label='0'))

        predict_features = convert_examples_to_features(predict_examples, label_list, FLAGS.max_seq_length, tokenizer)
        predict_input_fn = input_fn_builder(
            features=predict_features,
            seq_length=FLAGS.max_seq_length,
            is_training=False,
            drop_remainder=predict_drop_remainder)
        result = estimator.predict(input_fn=predict_input_fn)

        res_dic = {}
        tf.logging.info(""***** Predict results *****"")

        j = 0
        for prediction in result:
            probabilities = prediction[""probabilities""]
            dicts = {}
            for i in range(len(probabilities)):
                dicts[label_list[i]] = probabilities[i]
            dicts = sorted(dicts.items(), key=lambda x: x[1], reverse=True)
            res_dic[j] = [dicts[0][0], dicts[0][1]]
            j += 1
        return res_dic, text_n_len`",must made mistake mean problem long call one time saved lot time ca speed code used solve problem index file enumerate open file text text text text text text text text text index text text text range text index break text index text else index text result predict prediction result prediction range sorted return,issue,negative,negative,negative,negative,negative,negative
508299632,@Biaocsu  you mean you find your way to speed up estimator.predict? can you share it with us?,mean find way speed share u,issue,negative,negative,negative,negative,negative,negative
508014760,"@David-webb  thanks, I think I have solved my problem by other method",thanks think problem method,issue,negative,positive,positive,positive,positive,positive
507987904,"I am facing the exact same issue right now, any idea what the problem is ?",facing exact issue right idea problem,issue,negative,positive,positive,positive,positive,positive
507959603,"I also want to KNOW this! 
@MarvinMogab Do you try to convert the Pytorch model to TF Checkpoint format? I have tried, but failed.",also want know try convert model format tried,issue,negative,neutral,neutral,neutral,neutral,neutral
507438912,"<img width=""951"" alt=""bert-1"" src=""https://user-images.githubusercontent.com/8827900/60468374-41449680-9c0e-11e9-9f20-b3a2f79aff30.PNG"">
<img width=""647"" alt=""bert-2"" src=""https://user-images.githubusercontent.com/8827900/60468375-41449680-9c0e-11e9-9724-7c8e8e09717f.PNG"">

Hi, I'm having a different issue while running the `bert-serving-start` command on a windows 10 machine. As shown, I'm running a command to begin a service with 4 workers and I never get the `""ready and listening!""` message in my output - instead, it hangs after telling me that the 4 workers are ready.

The exact same command works perfectly on a Mac, and I'm at a loss for what to do to fix this on Windows. Things I've tried:
- upgrading `bert-serving-client `and `bert-serving-server`
- changing the path from which I'm loading the pre-trained models (both relative and absolute paths)
- Changing python and tensorflow versions. Currently using: python 3.6.8 and tensorflow 1.13.1",hi different issue running command machine shown running command begin service never get ready listening message output instead telling ready exact command work perfectly mac loss fix tried path loading relative absolute python currently python,issue,negative,positive,positive,positive,positive,positive
507132194,"> 嗯懂了，有两种办法，一种办法是你修改bert-as-service的源码再用pip安装，需要读一下源码，修改你定义的模型以及输入输出的数据格式 另一种是你自己写一个server-client模式的代码，我先大概说一下思路，estimator.predict会不断从 predict_input_fn这个函数里取数据，所以只要predict_input_fn有数据吐就好 因此，你需要一个单独的client.py负责发送数据给 predict_input_fn，然后接收server端算好的数据，这个就是pyzmq做的，你需要稍微了解下pyzmq用法，流程大概下面这样 ``` # server.py # 这里，只要调用了recv()，predict_input_fn就不会结束，会一直等client端发送数据，所以下面的 estimator.predict也不会结束，你不用多次执行 def predict_input_fn(): request_data = pyzmq_obj_s.recv() features = preprocess(request_data) def send_to_client(result): res = preprocess(request_data) pyzmq_obj_s.send(res) # 这里，BERT执行完再返回给client端 for result in estimator.predict(input_fn=predict_input_fn): send_to_client(result) ``` ``` # client.py def send_to_server(data): pyzmq_obj_c.send(data) def wait_res(res): res = pyzmq_obj_c.recv() return preprocess(res) ``` Biaocsu <notifications@github.com> 于2019年7月1日周一 下午1:03写道：
> […](#)
> @nlp4whp <https://github.com/nlp4whp> 我想我还是用中文来说明下我的问题吧。其实我有机器：Ubuntu GPU 20G 内存， 我使用如下代码： `def predicts(file_dic): if FLAGS.do_predict: predict_examples = [] for (index, file) in enumerate(file_dic): with open(file, 'r', encoding='utf-8', errors='ignore') as f: text = f.read() # text = re.sub(r""[^\u4E00-\u9FA5]"", '', text) # text = re.sub(""转载|首发|喜讯|推荐|编辑|人气|禁闻|大纪元|新唐人|文化大观|翻墙必看|健康医疗|一周大事解读|热点透视"", '', text) # if len(text) > 512: # text = text[:128] + text[-382:] guid = 'test-%d' % index text_a = tokenization.convert_to_unicode(text) predict_examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label='0')) predict_features = convert_examples_to_features(predict_examples, label_list, FLAGS.max_seq_length, tokenizer) predict_input_fn = input_fn_builder( features=predict_features, seq_length=FLAGS.max_seq_length, is_training=False, drop_remainder=predict_drop_remainder) result = estimator.predict(input_fn=predict_input_fn) # file_based_convert_examples_to_features(predict_examples, label_list, FLAGS.max_seq_length, tokenizer, predict_file) # predict_input_fn = file_based_input_fn_builder( # input_file=predict_file, # seq_length=FLAGS.max_seq_length, # is_training=False, # drop_remainder=predict_drop_remainder) # result = estimator.predict(input_fn=predict_input_fn) res_dic = {} tf.logging.info(""***** Predict results *****"") j = 0 for prediction in result: probabilities = prediction[""probabilities""] dicts = {} for i in range(len(probabilities)): dicts[label_list[i]] = probabilities[i] dicts = sorted(dicts.items(), key=lambda x: x[1], reverse=True) res_dic[j] = [dicts[0][0], dicts[0][1]] j += 1 return res_dic 将file_dic目录下所有文件处理后，一次性通过estimator.predict 来预测（给出所有文件的结果），这样平均每个文件需要耗时0.4s。但是因为其他原因，我不能将所有数据一次性通过estimator.predict 预测给出，我需要多次调用estimator.predict`（即每个txt文件调用一次），这样就会非常耗时了（平均5s）。你知道使用bert-as-service 可以解决这个问题吗？因为我也相当于刚入门NLP，所以并不知道这个”上游pro“应用到哪里可以起作用。再次感谢您，谢谢 — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#287?email_source=notifications&email_token=AJ5DRTKAB232H6BJSOMRJ5DP5GFZRA5CNFSM4G73YOMKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODY47ZLQ#issuecomment-507116718>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AJ5DRTPKD2NLLNEYNMFZ2NDP5GFZRANCNFSM4G73YOMA> .

谢谢兄弟，多谢",result result result data data return index file enumerate open file text text text text text text text text text index text result result predict prediction result prediction range sorted return reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
507116718,"@nlp4whp 我想我还是用中文来说明下我的问题吧。其实我有机器：Ubuntu GPU 20G 内存， 我使用如下代码：
`def predicts(file_dic):
    if FLAGS.do_predict:

        predict_examples = []
        for (index, file) in enumerate(file_dic):
            with open(file, 'r', encoding='utf-8', errors='ignore') as f:
                text = f.read()
                # text = re.sub(r""[^\u4E00-\u9FA5]"", '', text)
                # text = re.sub(""转载|首发|喜讯|推荐|编辑|人气|禁闻|大纪元|新唐人|文化大观|翻墙必看|健康医疗|一周大事解读|热点透视"", '', text)
                # if len(text) > 512:
                #     text = text[:128] + text[-382:]
                guid = 'test-%d' % index
                text_a = tokenization.convert_to_unicode(text)
                predict_examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label='0'))

        predict_features = convert_examples_to_features(predict_examples, label_list, FLAGS.max_seq_length, tokenizer)
        predict_input_fn = input_fn_builder(
            features=predict_features,
            seq_length=FLAGS.max_seq_length,
            is_training=False,
            drop_remainder=predict_drop_remainder)
        result = estimator.predict(input_fn=predict_input_fn)


        # file_based_convert_examples_to_features(predict_examples, label_list, FLAGS.max_seq_length, tokenizer, predict_file)
        # predict_input_fn = file_based_input_fn_builder(
        #     input_file=predict_file,
        #     seq_length=FLAGS.max_seq_length,
        #     is_training=False,
        #     drop_remainder=predict_drop_remainder)
        # result = estimator.predict(input_fn=predict_input_fn)


        res_dic = {}
        tf.logging.info(""***** Predict results *****"")

        j = 0
        for prediction in result:
            probabilities = prediction[""probabilities""]
            dicts = {}
            for i in range(len(probabilities)):
                dicts[label_list[i]] = probabilities[i]
            dicts = sorted(dicts.items(), key=lambda x: x[1], reverse=True)
            res_dic[j] = [dicts[0][0], dicts[0][1]]
            j += 1
        return res_dic
`
将file_dic目录下所有文件处理后，一次性通过`estimator.predict`来预测（给出所有文件的结果），这样平均每个文件需要耗时0.4s。但是因为其他原因，我不能将所有数据一次性通过`estimator.predict`预测给出，我需要多次调用`estimator.predict`（即每个txt文件调用一次），这样就会非常耗时了（平均5s）。你知道使用bert-as-service 可以解决这个问题吗？因为我也相当于刚入门NLP，所以并不知道这个”上游pro“应用到哪里可以起作用。再次感谢您，谢谢",index file enumerate open file text text text text text text text text text index text result result predict prediction result prediction range sorted return,issue,negative,neutral,neutral,neutral,neutral,neutral
507109284,"> > > @nlpwhp I'm really awkward that I still don't know how to solve my problem. can you give me an example to use this in result predict, I'm fish. thanks bro. hahhahah
> > 
> > 
> > em sorry, I just use this repo, it's convenient .... and I'm not sure what is your specific task.
> > If you try to deploy BERT-based service by some frameworks you familiar with (flask, django, ...) , you can implement one server to start BERT and wait for data by pyzmq and one client for getting request and sending data to server.
> > Or, if you just wanna run your service quickly, using this repo is the good way.
> > btw, if you wanna change service kind, because this repo is based on encoding-task, you can change your model here:
> > https://github.com/hanxiao/bert-as-service/blob/ff5c0afc3e834b1a95da4c883a664a6aab506889/server/bert_serving/server/graph.py#L36
> 
> really really thanks for you patient reply, I just use google-research/bert for text_classification task, and I found it really slow when calling `estimator.predict` (about 5s for each text). I see reply from others that using bert-as-service can solve this problem, but just have no idea of how to or where to add it in the prediction position.

oh, I see, you have no requirements on service. In that case, this repo cannot help you a lot. 

I guess you run your BERT-classification on your personal PC with CPU-device.  
Running on CPU is slow, if you have to, try lower `max_seq_length` like 64 or lower
Running on GPU-device could be faster, as I run  BERT-classification on GTX1080, lower than 50ms",really awkward still know solve problem give example use result predict fish thanks em sorry use convenient sure specific task try deploy service familiar flask implement one server start wait data one client getting request sending data server wan na run service quickly good way wan na change service kind based change model really really thanks patient reply use task found really slow calling text see reply solve problem idea add prediction position oh see service case help lot guess run personal running slow try lower like lower running could faster run lower,issue,positive,positive,neutral,neutral,positive,positive
507106944,"> > @nlpwhp I'm really awkward that I still don't know how to solve my problem. can you give me an example to use this in result predict, I'm fish. thanks bro. hahhahah
> 
> em sorry, I just use this repo, it's convenient .... and I'm not sure what is your specific task.
> 
> If you try to deploy BERT-based service by some frameworks you familiar with (flask, django, ...) , you can implement one server to start BERT and wait for data by pyzmq and one client for getting request and sending data to server.
> 
> Or, if you just wanna run your service quickly, using this repo is the good way.
> 
> btw, if you wanna change service kind, because this repo is based on encoding-task, you can change your model here:
> 
> https://github.com/hanxiao/bert-as-service/blob/ff5c0afc3e834b1a95da4c883a664a6aab506889/server/bert_serving/server/graph.py#L36

really really thanks for you patient reply, I just use google-research/bert for text_classification task, and I found it really slow when calling `estimator.predict` (about 5s for each text). I see reply from others that using bert-as-service can solve this problem, but just have no idea of how to or where to add it in the prediction position.",really awkward still know solve problem give example use result predict fish thanks em sorry use convenient sure specific task try deploy service familiar flask implement one server start wait data one client getting request sending data server wan na run service quickly good way wan na change service kind based change model really really thanks patient reply use task found really slow calling text see reply solve problem idea add prediction position,issue,positive,positive,neutral,neutral,positive,positive
507100761,"> @nlpwhp I'm really awkward that I still don't know how to solve my problem. can you give me an example to use this in result predict, I'm fish. thanks bro. hahhahah

em sorry, I just use this repo, it's convenient .... and I'm not sure what is your specific task. 

If you try to deploy BERT-based service by some frameworks you familiar with (flask, django, ...) , you can implement one server to start BERT and wait for data by pyzmq and one client for getting request and sending data to server.

Or, if you just wanna run your service quickly,  using this repo is the good way. 

btw, if you wanna change service kind, because this repo is based on encoding-task, you can change your model here: 

https://github.com/hanxiao/bert-as-service/blob/ff5c0afc3e834b1a95da4c883a664a6aab506889/server/bert_serving/server/graph.py#L36
 
  
",really awkward still know solve problem give example use result predict fish thanks em sorry use convenient sure specific task try deploy service familiar flask implement one server start wait data one client getting request sending data server wan na run service quickly good way wan na change service kind based change model,issue,positive,positive,neutral,neutral,positive,positive
507090716,"@nlpwhp  I'm really awkward that I still don't know how to solve my problem. can you give me an example to use this in result predict, I'm a fish. thanks bro.   hahhahah",really awkward still know solve problem give example use result predict fish thanks,issue,negative,negative,negative,negative,negative,negative
507006551,"> @nlpwhp can you give me a link of where the author use pyzmq, really appreciate

you can see `_run()` and `input_fn_builder()` in here : 
https://github.com/hanxiao/bert-as-service/blob/ff5c0afc3e834b1a95da4c883a664a6aab506889/server/bert_serving/server/__init__.py#L543



",give link author use really appreciate see,issue,negative,positive,positive,positive,positive,positive
507004718,"@nlpwhp  can you give me a link of where the author use pyzmq, really appreciate",give link author use really appreciate,issue,negative,positive,positive,positive,positive,positive
507003628,"> btw, I'm using estimator.predict() to get result from bert

because of the generator, it has to recover ""context"", all resources about this function.

this func `input_fn()` is a generator ,everythime you run `estimator.predict()`, `input_fn()` has to recover its `context`

you can see in this repo, author uses `pyzmq` to send data to `input_fn()`, in that you don't need run `estimator.predict()` many times



",get result generator recover context function generator run recover context see author send data need run many time,issue,negative,positive,positive,positive,positive,positive
506951797,"I have found a way to get it working in colab:

Install the server using this command (instead of the standard, may have to reset the runtime if the standard server is already installed) to enable http:

    !pip install -U bert-serving-server[http]

Start the server with this code in the notebook (replacing the model directory with your own):

    get_ipython().system_raw(
        'bert-serving-start -model_dir=./cased_L-24_H-1024_A-16 -http_port 3333 &'
    )

Define this function:

    import json
    import requests
    def get_embeddings(texts):
        headers = {
            'content-type':'application/json'
        }
        data = {
            ""id"":123,
            ""texts"":texts,
            ""is_tokenized"": False
        }
        data = json.dumps(data)
        r = requests.post(""http://localhost:3333/encode"", data=data, headers=headers).json()
        return r['result']

You can now get the embeddings like so (you may have to wait a couple of minutes for the server to start up first):

    text_embeddings = get_embeddings(text_list)",found way get working install server command instead standard may reset standard server already enable pip install start server code notebook model directory define function import import data id false data data return get like may wait couple server start first,issue,negative,negative,neutral,neutral,negative,negative
506925529,"Hi, I heard using your project can speed up estimator.predict( ) when using bert for Classification task, but I'm really confused and don't know how to solve this problem, can you give me a demo, really thanks",hi project speed classification task really confused know solve problem give really thanks,issue,negative,negative,neutral,neutral,negative,negative
506924105,"I have the same problem, using estimator.predict( ), it cost me a lot of time, can you tell me how to solve it, thanks",problem cost lot time tell solve thanks,issue,negative,positive,positive,positive,positive,positive
506003944,I had similar issue (server not responding after some time) and upgraded Bert serving server and client to 1.9.2 and that fixed the problem for me.,similar issue server time serving server client fixed problem,issue,negative,positive,neutral,neutral,positive,positive
505729690,"> Please re-open this issue:
> I am having the same problem. Using the directions found on https://bert-as-service.readthedocs.io/en/latest/tutorial/http-server.html I got 'command not found.' I'm running...
> Ubuntu 18.04 LTS,
> Python 3.6.7,
> Flask==1.0.2,
> Flask-Compress==1.4.0,
> Flask-Cors==3.0.7,
> Flask-JSON==0.3.3,
> tensorboard==1.13.1
> tensoflow==1.13.1
> tensorflow-estimator==1.13.0
> bert-serving-client==1.8.9
> bert-serving-server==1.8.9
> 
> 1. The version of bert-serving-*  is a concern because GitHub releases shows 1.8.1 as the most recent version.
> 2. when executing bert-serving-start I receive ""bert-serving-start: command not found""
> 
> Note: I did see the other issue: #194, and suspect it's the same, however it seems as though it's closed as well, and is on the Windows OS. Therefore I'm commenting here.

you may find bert-serving-start in .local/bin this folder,and run it with python",please issue problem found got found running python version concern recent version receive command found note see issue suspect however though closed well o therefore may find folder run python,issue,negative,negative,neutral,neutral,negative,negative
505713000,"> Oops, sorry -- I haven't started the server and was running the script. My bad, closing the issue.

what do you mean""started the server""? i met the same problem",sorry server running script bad issue mean server met problem,issue,negative,negative,negative,negative,negative,negative
505710046,"> Please re-open this issue:
> I am having the same problem. Using the directions found on https://bert-as-service.readthedocs.io/en/latest/tutorial/http-server.html I got 'command not found.' I'm running...
> Ubuntu 18.04 LTS,
> Python 3.6.7,
> Flask==1.0.2,
> Flask-Compress==1.4.0,
> Flask-Cors==3.0.7,
> Flask-JSON==0.3.3,
> tensorboard==1.13.1
> tensoflow==1.13.1
> tensorflow-estimator==1.13.0
> bert-serving-client==1.8.9
> bert-serving-server==1.8.9
> 
> 1. The version of bert-serving-*  is a concern because GitHub releases shows 1.8.1 as the most recent version.
> 2. when executing bert-serving-start I receive ""bert-serving-start: command not found""
> 
> Note: I did see the other issue: #194, and suspect it's the same, however it seems as though it's closed as well, and is on the Windows OS. Therefore I'm commenting here.

try to use sudo pip/pip3 ...",please issue problem found got found running python version concern recent version receive command found note see issue suspect however though closed well o therefore try use,issue,negative,negative,neutral,neutral,negative,negative
505210057,"Bert-as-service is great tool and I also want to use it for sentence prediction. Any tips on how to transform CLS embedding (that we get for a pair of sentences) to probability of sentence ""b"" being continuation of sentence ""a""?

I got as deep as this code, but I don't understand what it does:
_, pooled_output = model.bert(tokens_tensor,segments_tensors, attention_mask=None,
                                 output_all_encoded_layers=False)
seq_relationship_score = model.cls(pooled_output)",great tool also want use sentence prediction transform get pair probability sentence continuation sentence got deep code understand,issue,positive,positive,positive,positive,positive,positive
504654980,"Yes

On Fri, Jun 21, 2019, 3:17 PM Pedram <notifications@github.com> wrote:

> I rectified this by opening up the ports 5555 and 5556 on ec2.
>
> May I ask what type and protocol did you add? You added 5555 as inbound
> and 5556 as outbound ports, right?
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/hanxiao/bert-as-service/issues/382?email_source=notifications&email_token=ALDRXNFBZHK2WKZ6DCULM53P3UZNXA5CNFSM4HYOOLP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODYJPQUA#issuecomment-504559696>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ALDRXNEIGWZX63AT7THWYDLP3UZNXANCNFSM4HYOOLPQ>
> .
>
",yes wrote rectified opening may ask type protocol add added inbound outbound right thread reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
504593260,"I have a similar issue. In my case, two of the workers start and are ready to listen, but the other two never start. And, when I try to interrupt the server it completely freezes and I have to terminate my terminal. Still could not figure it out.

**[UPDATE]** when I changed my model's folder to BERT's smaller model, now server and all workers are started listening and server DOES receive the request.",similar issue case two start ready listen two never start try interrupt server completely terminate terminal still could figure update model folder smaller model server listening server receive request,issue,positive,positive,neutral,neutral,positive,positive
504559696,"> I rectified this by opening up the ports 5555 and 5556 on ec2.

May I ask what type and protocol did you add? You added 5555 as inbound and 5556 as outbound ports, right? I'm asking since I have already opened up these ports, but I still have the same problem.",rectified opening may ask type protocol add added inbound outbound right since already still problem,issue,negative,positive,positive,positive,positive,positive
504542385,"I have the same problem, server is running on my EC2 and is ready to serve requests, but when I send the request from my local, it seems it's just pending and nothing happens. Server *does* receive the request but nothing happens.
[**UPDATE**] problem solved. When I added 5556 also to my **inbound** ports, it's working now. I initially thought that I only need to add 5555 to inbound and 5556 to outbound.








",problem server running ready serve send request local pending nothing server receive request nothing update problem added also inbound working initially thought need add inbound outbound,issue,negative,positive,neutral,neutral,positive,positive
504517537,"OK here is my final congesture:

As previously stated, my WORKER won't make predictions on line 539 unless there are new requests:

```python
for r in estimator.predict(self.input_fn_builder(receivers, tf, sink_token), yield_single_examples=False):
```

And the input_fn is from a tf.data.Dataset.from_generator, which utilizes the features parsed from the incoming message. The logs confirmed that the gen() function received encode requests in time, and worked until the final yield statement. So it got me wondering whether the problem lies in tf.data.Dataset.from_generator. Meaning that it didn't receive the results from generator, or maybe it didn't feed data to tf.Estimator at the time the data arrived. That could explain why when the newer requests come, they act like flushing the old ones into the dataset, then the estimator can finally deal with the embeddings.

Per the tensorflow document about tf.data.Dataset.from_generator:

```
NOTE: If generator depends on mutable global variables or other external state, 
be aware that the runtime may invoke generator multiple times (in order to support 
repeating the Dataset) and at any time between the call to Dataset.from_generator() 
and the production of the first element from the generator. Mutating global variables 
or external state can cause undefined behavior, and we recommend that you explicitly 
cache any external state in generator before calling Dataset.from_generator().
```

I don't know if a generator waiting for socket messages to yield data counts as an external state, but the current behavior surely seems undefined, as some of us have such issues (#300) and others don't.

Is there a way to improve the message feeding into the estimator to guarantee that each request gets timely handling?",final previously stated worker wo make line unless new python incoming message confirmed gen function received encode time worked final yield statement got wondering whether problem meaning receive generator maybe feed data time data could explain come act like flushing old estimator finally deal per document note generator mutable global external state aware may invoke generator multiple time order support time call production first element generator global external state cause undefined behavior recommend explicitly cache external state generator calling know generator waiting socket yield data external state current behavior surely undefined u way improve message feeding estimator guarantee request timely handling,issue,positive,positive,neutral,neutral,positive,positive
504492430,Could you folks check which version of libzmq your machine is using while encountering this issue?,could check version machine issue,issue,negative,neutral,neutral,neutral,neutral,neutral
504414768,"I have the exact same error. Can't figure out how to fix this yet but I'm looking for a solution if someone has one.
Moreover, I know someone that is able to made this work (with the biobert model), but we can't figure out why it's work with him and not with me.

EDIT: I found a solution, it was just, like the error said, a problem with the ckpt name. For me the name must be ""model.ckpt-1000000"" so I used ""-ckpt_name=model.ckpt-1000000"" as an option of bert-serving-start.",exact error ca figure fix yet looking solution someone one moreover know someone able made work model ca figure work edit found solution like error said problem name name must used option,issue,negative,positive,positive,positive,positive,positive
504291383,"I have a similar issue. If I terminate the server, I also have the same exception on threading. As for the sample code to trigger this hang, if you cancel encode by Ctrl+C and rerun it, you can see the SINK sent back the result of first request, but the new encode hangs again.",similar issue terminate server also exception sample code trigger cancel encode rerun see sink sent back result first request new encode,issue,negative,positive,neutral,neutral,positive,positive
504285550,"Is any of the following logs, especially the warning and error from Verbose logs (where the beginnings of lines are marked by >>>), concerning?

```
2019-06-21 00:28:34.255964: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
>>> 2019-06-21 00:28:34.266600: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2019-06-21 00:28:34.266692: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] retrieving CUDA diagnostic information for host: c2162
2019-06-21 00:28:34.266751: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:170] hostname: c2162
2019-06-21 00:28:34.266842: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:194] libcuda reported version is: 396.26.0
2019-06-21 00:28:34.266966: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:198] kernel reported version is: 396.26.0
2019-06-21 00:28:34.267027: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:305] kernel version seems to match DSO: 396.26.0
I:GRAPHOPT:[gra:opt:128]:load parameters from checkpoint...
I:GRAPHOPT:[gra:opt:132]:optimize...
I:GRAPHOPT:[gra:opt:140]:freeze...
I:GRAPHOPT:[gra:opt:145]:write graph to a tmp file: /tmp/tmpb72wv6hu
I:VENTILATOR:[__i:__i: 74]:optimized graph is stored at: /tmp/tmpb72wv6hu
I:VENTILATOR:[__i:_ru:128]:bind all sockets
I:VENTILATOR:[__i:_ru:132]:open 8 ventilator-worker sockets
I:VENTILATOR:[__i:_ru:135]:start the sink
I:SINK:[__i:_ru:305]:ready
I:VENTILATOR:[__i:_ge:221]:get devices
I:VENTILATOR:[__i:_ge:254]:device map:
                worker  0 -> gpu  0
I:WORKER-0:[__i:_ru:531]:use device gpu: 0, load graph from /tmp/tmpb72wv6hu
>>> WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpmefq0346
INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpmefq0346', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {
  key: ""GPU""
  value: 1
}
gpu_options {
  per_process_gpu_memory_fraction: 0.25
  allow_growth: true
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service':
None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ba5583b1390>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_r
eplicas': 0, '_num_worker_replicas': 1}
>>> WARNING:tensorflow:Estimator's model_fn (<function BertWorker.get_estimator.<locals>.model_fn at 0x2ba5583b0d90>) includes params argument, but params are not passed to Estimator.
>>> INFO:tensorflow:Could not find trained model in model_dir: /tmp/tmpmefq0346, running initialization to predict.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Graph was finalized.
```",following especially warning error verbose marked concerning binary use call device diagnostic information host version kernel version kernel version match gra opt load gra opt optimize gra opt freeze gra opt write graph file ventilator graph ventilator bind ventilator open ventilator start sink sink ready ventilator get ventilator device map worker use device load graph warning temporary folder model directory none none key value true none none none none none none object true warning estimator function argument estimator could find trained model running predict calling done calling graph,issue,negative,positive,positive,positive,positive,positive
504271918,Looks like your problem is different from mine. Your WORKER didn't send back numpy array to SINK. It looks strange to me.,like problem different mine worker send back array sink strange,issue,negative,negative,neutral,neutral,negative,negative
504096443,"thanks, let me have a look at your code tomorrow. I remember I fixed such problem before.",thanks let look code tomorrow remember fixed problem,issue,negative,positive,positive,positive,positive,positive
504088661,"After a bit of digging, here's my new findings based on **GPU mode** and your fix code:

If I turned on verbose logging, after the initial encode request

```python
bc.encode(['First do it', 'then do it right', 'then do it better'])
```

I can see the server has generated the tokens and is yielding the data for line 576.

```python
                        yield {
                            'client_id': client_id,
                            'input_ids': [f.input_ids for f in tmp_f],
                            'input_mask': [f.input_mask for f in tmp_f],
                            'input_type_ids': [f.input_type_ids for f in tmp_f]
                        }
```

```
I:SINK:[__i:_ru:341]:job register       size: 3 job id: b'b3cb0264-06fa-4212-9075-2bde36870ccd#2'
I:WORKER-0:[__i:gen:568]:new job        socket: 0       size: 3 client: b'b3cb0264-06fa-4212-9075-2bde36870ccd#2'
D:WORKER-0:[ext:con:115]:tokens: [CLS] first do it [SEP]
D:WORKER-0:[ext:con:116]:input_ids: 101 2034 2079 2009 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
D:WORKER-0:[ext:con:117]:input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
D:WORKER-0:[ext:con:118]:input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
D:WORKER-0:[ext:con:115]:tokens: [CLS] then do it right [SEP]
D:WORKER-0:[ext:con:116]:input_ids: 101 2059 2079 2009 2157 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
D:WORKER-0:[ext:con:117]:input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
D:WORKER-0:[ext:con:118]:input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
D:WORKER-0:[ext:con:115]:tokens: [CLS] then do it better [SEP]
D:WORKER-0:[ext:con:116]:input_ids: 101 2059 2079 2009 2488 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
D:WORKER-0:[ext:con:117]:input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
D:WORKER-0:[ext:con:118]:input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
```

And the server stucks at line 539, where the estimator is trying to output the embeddings from BERT I guess.

```python
for r in estimator.predict(self.input_fn_builder(receivers, tf, sink_token), yield_single_examples=False):
```

After I use ^C and have a new encode request the above line finally gets executed.

Now I don't know if it's because of the mechanism of polling socket requests, as the need of a second request surely looks like a call of flush for streamed data, or some sort of Tensorflow bug as it works OK in CPU mode, but I hope it could shed some light on a potential fix.",bit digging new based mode fix code turned verbose logging initial encode request python right better see server yielding data line python yield sink job register size job id gen new job socket size client con first con con con con right con con con con better con con con server line estimator trying output guess python use new encode request line finally executed know mechanism polling socket need second request surely like call flush data sort bug work mode hope could shed light potential fix,issue,positive,positive,positive,positive,positive,positive
504053768,"Thanks, @paperplanet for the quick response. I have patched the commit and the problem remains.

Here's the minimal reproduction of what I did:

1. start bert

```bash
bert-serving-start -pooling_strategy NONE -pooling_layer -2 -max_seq_len 100 -model_dir bert/uncased_L-12_H-768_A-12/ -device_map 0 -gpu_memory_fraction 0.25
```

2. Launch python CLI and run:

```python
from bert_serving.client import BertClient
bc = BertClient()
bc.encode(['First do it', 'then do it right', 'then do it better'])
```

Now the server shows:

```
I:VENTILATOR:[__i:__i: 66]:freeze, optimize and export graph, could take a while...
I:GRAPHOPT:[gra:opt: 52]:model config: bert/uncased_L-12_H-768_A-12/bert_config.json
I:GRAPHOPT:[gra:opt: 55]:checkpoint: bert/uncased_L-12_H-768_A-12/bert_model.ckpt
I:GRAPHOPT:[gra:opt: 59]:build graph...
I:GRAPHOPT:[gra:opt:128]:load parameters from checkpoint...
I:GRAPHOPT:[gra:opt:132]:optimize...
I:GRAPHOPT:[gra:opt:140]:freeze...
I:GRAPHOPT:[gra:opt:145]:write graph to a tmp file: /tmp/tmpcq0p9umz
I:VENTILATOR:[__i:__i: 74]:optimized graph is stored at: /tmp/tmpcq0p9umz
I:VENTILATOR:[__i:_ru:128]:bind all sockets
I:VENTILATOR:[__i:_ru:132]:open 8 ventilator-worker sockets
I:VENTILATOR:[__i:_ru:135]:start the sink
I:SINK:[__i:_ru:305]:ready
I:VENTILATOR:[__i:_ge:221]:get devices
I:VENTILATOR:[__i:_ge:254]:device map:
                worker  0 -> gpu  0
I:WORKER-0:[__i:_ru:529]:use device gpu: 0, load graph from /tmp/tmpcq0p9umz
I:WORKER-0:[__i:gen:557]:ready and listening!
I:VENTILATOR:[__i:_ru:163]:all set, ready to serve request!
I:VENTILATOR:[__i:_ru:179]:new config request   req id: 1       client: b'a95a1757-3549-49e1-9409-4ffa794a5b68'
I:SINK:[__i:_ru:347]:send config        client b'a95a1757-3549-49e1-9409-4ffa794a5b68'
I:VENTILATOR:[__i:_ru:195]:new encode request   req id: 2       size: 3 client: b'a95a1757-3549-49e1-9409-4ffa794a5b68'
I:WORKER-0:[__i:gen:566]:new job        socket: 0       size: 3 client: b'a95a1757-3549-49e1-9409-4ffa794a5b68#2'
I:SINK:[__i:_ru:341]:job register       size: 3 job id: b'a95a1757-3549-49e1-9409-4ffa794a5b68#2'
```

and hangs there (the Python CLI also shows that line of code hasn't completed running). After I interrupt my Python CLI by ^C and rerun the line:

```python
bc.encode(['First do it', 'then do it right', 'then do it better'])
```

the log now shows the server returns my previous request and hangs again (so does Python CLI) on my new request:

```
I:WORKER-0:[__i:gen:566]:new job        socket: 0       size: 3 client: b'a95a1757-3549-49e1-9409-4ffa794a5b68#3'
I:WORKER-0:[__i:_ru:541]:job done       size: (3, 100, 768)     client: b'a95a1757-3549-49e1-9409-4ffa794a5b68#2'
I:SINK:[__i:_ru:333]:collect b'EMBEDDINGS' b'a95a1757-3549-49e1-9409-4ffa794a5b68#2' (E:3/T:0/A:3)
I:SINK:[__i:_ru:356]:send back  size: 3 job id: b'a95a1757-3549-49e1-9409-4ffa794a5b68#2'
```

As for my production model, after the patch I still need to apply the abovementioned workaround to ``push'' it to continue.

Also I'd like to stress that it works fine in **CPU mode**, which really perplexes me. Hope this toy example can be of more help.
",thanks quick response commit problem remains minimal reproduction start bash none launch python run python import right better server ventilator freeze optimize export graph could take gra opt model gra opt gra opt build graph gra opt load gra opt optimize gra opt freeze gra opt write graph file ventilator graph ventilator bind ventilator open ventilator start sink sink ready ventilator get ventilator device map worker use device load graph gen ready listening ventilator set ready serve request ventilator new request id client sink send client ventilator new encode request id size client gen new job socket size client sink job register size job id python also line code running interrupt python rerun line python right better log server previous request python new request gen new job socket size client job done size client sink collect sink send back size job id production model patch still need apply push continue also like stress work fine mode really hope toy example help,issue,positive,positive,positive,positive,positive,positive
503983393,Try https://github.com/hanxiao/bert-as-service/pull/389 . Maybe fixed your problem.,try maybe fixed problem,issue,negative,positive,neutral,neutral,positive,positive
503914288,"This issue still occurred on Windows Server, when the bert-service was running, I just close the 'Git Bash' command window directly without the 'Ctrl+C' operation.
Finally I solved it through ending the background process named 'Python.exe'  in the Windows Task Manager.",issue still server running close bash command window directly without operation finally ending background process task manager,issue,negative,positive,neutral,neutral,positive,positive
503817697,"when the concurrency goes up , in the bert service commands it will show that error

thanks
weizhen

![image](https://user-images.githubusercontent.com/13706689/59814311-6f40f580-9347-11e9-967a-d259fc967a7f.png)
",concurrency go service show error thanks image,issue,negative,positive,positive,positive,positive,positive
502774045,"For future reference, this means your PATH is not set correctly. 

Refer to this: https://stackoverflow.com/questions/46973667/python-console-scripts-doesnt-work-when-pip-install-user for more details.

If `~/.local/bin` is not on your path, then do this in your `.profile` or something that gets loaded by your shell:
````
export PATH=~/.local/bin:$PATH
````

On a Mac, depending on your Python install, that folder may not exist. Instead the bin folder may sometimes be found here `~/Library/Python/3.7/bin`.",future reference path set correctly refer path something loaded shell export path mac depending python install folder may exist instead bin folder may sometimes found,issue,negative,neutral,neutral,neutral,neutral,neutral
502412983,"ok.i have got the reason,i used the  Pre-trained BIO-BERT Model (https://github.com/search?q=biobert) ,which not in your download list of Pre-trained Model,and then raise the error ,but when  i use the  Pre-trained Model your provide , it did work,  so i want to know whether BIO-BERT Model  is not suit for this job?? @hanxiao ",got reason used model list model raise error use model provide work want know whether model suit job,issue,negative,neutral,neutral,neutral,neutral,neutral
502408934,"thanks a lot ,it didn't work for me ,  i did it according to the readme,",thanks lot work according,issue,negative,positive,positive,positive,positive,positive
502366606,"i have the same problem in ubuntu 18.04,and tensorflow version :1.13, python version 3.7
![image](https://user-images.githubusercontent.com/47856948/59551953-c5aedc80-8fb3-11e9-9116-8e9e36c33751.png)
",problem version python version image,issue,negative,neutral,neutral,neutral,neutral,neutral
502100611,"I tried v1.8.1 and found the same issue; that after a few queries, and a minute or so, the server becomes unresponsive and the old client and new clients never get the responses from the encode function. 

The last logs from the server look like so: 
```
I:SINK:[__i:_ru:312]:job register       size: 1 job id: b'cb057fa0-40e5-46a1-8014-4f7e5f23d218#10'
I:WORKER-0:[__i:gen:492]:new job        socket: 0       size: 1 client: b'cb057fa0-40e5-46a1-8014-4f7e5f23d218#10'
I:WORKER-0:[__i:_ru:468]:job done       size: (1, 768)  client: b'cb057fa0-40e5-46a1-8014-4f7e5f23d218#10'
I:SINK:[__i:_ru:292]:collect b'EMBEDDINGS' b'cb057fa0-40e5-46a1-8014-4f7e5f23d218#10' (E:1/T:0/A:1)
I:SINK:[__i:_ru:301]:send back  size: 1 job id: b'cb057fa0-40e5-46a1-8014-4f7e5f23d218#10'
^[[AI:VENTILATOR:[__i:_ru:164]:new encode request       req id: 11      size: 1 client: b'cb057fa0-40e5-46a1-8014-4
f7e5f23d218'
I:SINK:[__i:_ru:312]:job register       size: 1 job id: b'cb057fa0-40e5-46a1-8014-4f7e5f23d218#11'
I:WORKER-0:[__i:gen:492]:new job        socket: 0       size: 1 client: b'cb057fa0-40e5-46a1-8014-4f7e5f23d218#11'
```

Occasionally, on the client side, instead of just hanging, I see errors like this (the client is running locally using python 3.7, while server is using python 3.5): 

```
Traceback (most recent call last):
  File ""Users/iislucas/index-server/.pyenv/lib/python3.7/site-packages/flask/app.py"", line 2328, in __call__
    return self.wsgi_app(environ, start_response)
  File ""Users/iislucas/index-server/.pyenv/lib/python3.7/site-packages/flask/app.py"", line 2314, in wsgi_app
    response = self.handle_exception(e)
  File ""Users/iislucas/index-server/.pyenv/lib/python3.7/site-packages/flask/app.py"", line 1760, in handle_exception
    reraise(exc_type, exc_value, tb)
  File ""Users/iislucas/index-server/.pyenv/lib/python3.7/site-packages/flask/_compat.py"", line 36, in reraise
    raise value
  File ""Users/iislucas/index-server/.pyenv/lib/python3.7/site-packages/flask/app.py"", line 2311, in wsgi_app
    response = self.full_dispatch_request()
  File ""Users/iislucas/index-server/.pyenv/lib/python3.7/site-packages/flask/app.py"", line 1834, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""Users/iislucas/index-server/.pyenv/lib/python3.7/site-packages/flask/app.py"", line 1737, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""Users/iislucas/index-server/.pyenv/lib/python3.7/site-packages/flask/_compat.py"", line 36, in reraise
    raise value
  File ""Users/iislucas/index-server/.pyenv/lib/python3.7/site-packages/flask/app.py"", line 1832, in full_dispatch_request
    rv = self.dispatch_request()
  File ""Users/iislucas/index-server/.pyenv/lib/python3.7/site-packages/flask/app.py"", line 1818, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""Users/iislucas/index-server/index_server.py"", line 42, in embedding
    embedding = bc.encode([obj['text']])
  File ""Users/iislucas/index-server/.pyenv/lib/python3.7/site-packages/bert_serving/client/__init__.py"", line 202, in arg_wrapper
    return func(self, *args, **kwargs)
  File ""Users/iislucas/index-server/.pyenv/lib/python3.7/site-packages/bert_serving/client/__init__.py"", line 283, in encode
    r = self._recv_ndarray(req_id)
  File ""Users/iislucas/index-server/.pyenv/lib/python3.7/site-packages/bert_serving/client/__init__.py"", line 166, in _recv_ndarray
    request_id, response = self._recv(wait_for_req_id)
  File ""Users/iislucas/index-server/.pyenv/lib/python3.7/site-packages/bert_serving/client/__init__.py"", line 160, in _recv
    raise e
  File ""Users/iislucas/index-server/.pyenv/lib/python3.7/site-packages/bert_serving/client/__init__.py"", line 150, in _recv
    request_id = int(response[-1])
ValueError: invalid literal for int() with base 10: b'{""shape"":[1,768],""dtype"":""float32"",""tokens"":""""}
```

And the corresponding log on the server is: 

```
I:SINK:[__i:_ru:312]:job register       size: 1 job id: b'cb057fa0-40e5-46a1-8014-4f7e5f23d218#12'
I:WORKER-0:[__i:gen:492]:new job        socket: 0       size: 1 client: b'cb057fa0-40e5-46a1-8014-4f7e5f23d218#12'
I:WORKER-0:[__i:_ru:468]:job done       size: (1, 768)  client: b'cb057fa0-40e5-46a1-8014-4f7e5f23d218#11'
I:SINK:[__i:_ru:292]:collect b'EMBEDDINGS' b'cb057fa0-40e5-46a1-8014-4f7e5f23d218#11' (E:1/T:0/A:1)
I:SINK:[__i:_ru:301]:send back  size: 1 job id: b'cb057fa0-40e5-46a1-8014-4f7e5f23d218#11'
```

Are requests somehow getting out of sync? ",tried found issue minute server becomes unresponsive old client new never get encode function last server look like sink job register size job id gen new job socket size client job done size client sink collect sink send back size job id ai ventilator new encode request id size client sink job register size job id gen new job socket size client occasionally client side instead hanging see like client running locally python server python recent call last file line return environ file line response file line reraise file line reraise raise value file line response file line file line reraise file line reraise raise value file line file line return file line file line return self file line encode file line response file line raise file line response invalid literal base shape float corresponding log server sink job register size job id gen new job socket size client job done size client sink collect sink send back size job id somehow getting sync,issue,positive,negative,neutral,neutral,negative,negative
502081933,"I've noticed the same issue, was quite easy to recreate on a google cloud instance; here's the details I was using which may help reproduce: 

* [ x ] Are you running the latest `bert-as-service`? (v 1.9.1)
* [ x ] Did you follow [the installation](https://github.com/hanxiao/bert-as-service#install) and [the usage](https://github.com/hanxiao/bert-as-service#usage) instructions in `README.md`?
* [ x ] Did you check the [FAQ list in `README.md`](https://github.com/hanxiao/bert-as-service#speech_balloon-faq)?
* [ x ] Did you perform [a cursory search on existing issues](https://github.com/hanxiao/bert-as-service/issues)?

**System information**

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
os: Linux
os kernel version: #1 SMP Debian 4.9.168-1+deb9u2 (2019-05-13)
os release version: 4.9.0-9-amd64
os platform: Linux-4.9.0-9-amd64-x86_64-with-debian-9.9
linux distribution: ('debian', '9.9', '')
linux os distribution: ('debian', '9.9', '')

- TensorFlow installed from (source or binary): binary (via pip)
pip installs of: 
numpy (1.16.4)
protobuf (3.8.0)
tensorflow (1.13.1)
tensorflow-estimator (1.13.0)

- TensorFlow version: 1.13.1
- Python version: 3.5
- `bert-as-service` version:  1.9.1
- GPU model and memory:
- CPU model and memory:

```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |
| N/A   37C    P0    38W / 300W |      0MiB / 16130MiB |      3%      Default |
+-------------------------------+----------------------+----------------------+
```

After I start the instance, I send it a few queries, all further client requests hang. I suspect a race condition in the server. ",issue quite easy recreate cloud instance may help reproduce running latest follow installation usage check list perform cursory search system information o platform distribution o o kernel version o release version o platform distribution o distribution source binary binary via pip pip version python version version model memory model memory driver version version name volatile fan temp compute mib mib default start instance send client suspect race condition server,issue,positive,positive,positive,positive,positive,positive
501940269,"I use the following -model_dir ""C:/bert_model/uncased_L-12_H-768_A-12/"", make sure your path has no space too",use following make sure path space,issue,negative,positive,positive,positive,positive,positive
501934492,"Hi . I have the same problem, can  you share me how did you solve it ",hi problem share solve,issue,negative,neutral,neutral,neutral,neutral,neutral
501898651,"Are you trying to load base or large model? I've had this issue when I had insufficient memory on GPU, as a test you can try to do this with -cpu flag to see if that works, provided you have more RAM, like 16 GB+",trying load base large model issue insufficient memory test try flag see work provided ram like,issue,negative,negative,negative,negative,negative,negative
500810884,"Do the predictions you obtain actually make sense after those changes? I tried a bunch of sentences after aforementioned modifications with my fine-tuned sentiment classification model and the predictions seem to be completely random. However, when I load the same model with bert library I get great results, so the model itself is fine for sure. I think maybe the random predictions are due to using placeholders as input_ds, input_mask etc. @hanxiao  do you have any idea what could go wrong?",obtain actually make sense tried bunch sentiment classification model seem completely random however load model library get great model fine sure think maybe random due idea could go wrong,issue,positive,positive,neutral,neutral,positive,positive
500184867,"please read the log
```
tensorflow.python.framework.errors_impl.NotFoundError: ./chinese_L-12_H-768_A-12/bert_config.json; No such file or directory
```",please read log file directory,issue,negative,neutral,neutral,neutral,neutral,neutral
500184683,Wouldn't the [SEP] token equate to your sentence B embedding?,would token equate sentence,issue,negative,neutral,neutral,neutral,neutral,neutral
499599136,"I'm having the same problem, but I can't make the first encoding task go through.
I'm looking to use Colaboratory as a one time thing for this project, just to generate a large amount of sentence embeddings since I have no access to a GPU.

I have tried setting the ignore_all_checks=True parameter when starting the client, but that doesn't work either (it gets stuck when i try to .encode).

Any help would be really appreciated! Thanks.",problem ca make first task go looking use one time thing project generate large amount sentence since access tried setting parameter starting client work either stuck try help would really thanks,issue,negative,positive,positive,positive,positive,positive
499444770,"Hi, I notice one thing.
change the _PATH_ '/tmp/english_L-12_H-768_A-12/' to 'tmp/english_L-12_H-768_A-12/' or './tmp/english_L-12_H-768_A-12/' works well for me.
You might try.",hi notice one thing change work well might try,issue,negative,neutral,neutral,neutral,neutral,neutral
499119712,"Oh thanks!
Besides, if I want to do the task like sentence classification, which pooling strategy is the best?",oh thanks besides want task like sentence classification strategy best,issue,positive,positive,positive,positive,positive,positive
499114907,"the 768 is a dimension from the BERT base model, to have a different length you can choose the larger base model to give you 1024 instead. Bert as a service does not have any parameter to any kind of dimensional reduction on these original dimensions as far I know.",dimension base model different length choose base model give instead service parameter kind dimensional reduction original far know,issue,positive,negative,neutral,neutral,negative,negative
498966584,"@zhp510730568  can you provide more info on the issue. Are you getting an error, if so what is it?",provide issue getting error,issue,negative,neutral,neutral,neutral,neutral,neutral
498797326,"I figured out the mistake I was using reshape and it was giving me wrong vectors, instead used split to get the concatenated layers back.
pooling_layers = np.split(embeddings_4321, 4, axis=0)
further to calculate mean used this: 
mean4321 =  np.mean(pooling_layers, axis=0)",figured mistake reshape giving wrong instead used split get back calculate mean used mean,issue,negative,negative,negative,negative,negative,negative
498784704,I see the same issue. Please respond,see issue please respond,issue,negative,neutral,neutral,neutral,neutral,neutral
498548847,"Hi @sariagoudarzvand as mentioned in your error log, tensorflow can't find your bert_config file. Can you make sure that it is at intended path and rerun again and see if it still throws the error.
`tensorflow.python.framework.errors_impl.NotFoundError: /tmp/english_L-12_H-768_A-12/bert_config.json; No such file or directory
Traceback (most recent call last):`",hi error log ca find file make sure intended path rerun see still error file directory recent call last,issue,negative,positive,positive,positive,positive,positive
498489966,"who know the cause, please give me a help. @craigcitro @abhishekraok @likejazz @GabrielBianconi ",know cause please give help,issue,positive,neutral,neutral,neutral,neutral,neutral
497594591,"thanks for your reply， I have figured out the reason for this, because the client set static IP, as long as the client set dynamic IP, this problem can be solved",thanks figured reason client set static long client set dynamic problem,issue,negative,positive,positive,positive,positive,positive
497373511,"Just adding a possible solution to help other people. I was able to solve this problem by assigning a different port number to the server.

`start-bert-as-service.py -model_dir
 uncased_L-12_H-768_A-12 -num_worker=1 -cpu -max_batch_size 16 -port 8190`

Hope it helps.",possible solution help people able solve problem different port number server hope,issue,positive,positive,positive,positive,positive,positive
497358276,"I am encountering the same error. I've tried it both on Colab (with GPU) and on Jupyter Notebook (Windows 10 with CPU). Both just runs for hours without any error or message. I also tried to make it error by using different machines for server and client and not have it point to any IP address. It just ran for a long time without any error until I had to shut the process down.

Update: As a last ditch effort, I added port and port out even if I'm running the server and client on the same machine. Worked instantly on my local machine. Can't say the same for Colab though since I can't seem to terminate runtimes properly. 

Hope this helps someone else. Still hoping for messages or error traces in the future though.",error tried notebook without error message also tried make error different server client point address ran long time without error shut process update last ditch effort added port port even running server client machine worked instantly local machine ca say though since ca seem terminate properly hope someone else still error future though,issue,negative,negative,neutral,neutral,negative,negative
497304892,"Hi, I'm facing the same problem but I was able to run the bert-serving-start line by using this line

`!bert-serving-start ...`

My current problem is with running the BertClient class as it needs the IP Address of the GPU but I'm not sure if that is possible with colab. Can you do a successful run after using the code above?",hi facing problem able run line line current problem running class need address sure possible successful run code,issue,negative,positive,positive,positive,positive,positive
497131637,"The link mentioned here: 
https://github.com/hanxiao/bert-as-service/tree/server-v2 

Seems to 404 now.  I'm trying to run on CPU only and getting:

```
$ docker run --runtime nvidia -dit -p 5555:5555 -p 5556:5556 -v $PATH_MODEL:/model -t bert-as-service $NUM_WORKER

docker: Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:424: container init caused \""process_linux.go:407: running prestart hook 1 caused \\\""error running hook: exit status 1, stdout: , stderr: exec command: [/usr/bin/nvidia-container-cli --load-kmods configure --ldconfig=@/sbin/ldconfig.real --device=all --compute --utility --require=cuda>=9.0 --pid=15081 /var/lib/docker/overlay2/f2e715d21e9928c4873f6a27873ad6ac9952bb079d5d8921716be3bb073bf5a7/merged]\\\\nnvidia-container-cli: initialization error: cuda error: no cuda-capable device is detected\\\\n\\\""\"""": unknown.
```

Which searches are telling me are related to system failing to find NVIDIA drivers properly.

Is there a CPU-only version that currently works?",link trying run getting docker run docker error response daemon create starting container process container running hook error running hook exit status command configure compute utility error error device unknown telling related system failing find properly version currently work,issue,negative,negative,neutral,neutral,negative,negative
495928600,"Okay, thanks. I now understand that it probably does not make a lot of sense to extract single word embeddings from the Bert model.",thanks understand probably make lot sense extract single word model,issue,negative,positive,neutral,neutral,positive,positive
495435160,"Alright, i see what you mean. Anyway, is there any chance that you will add a server API to return CLS vec after densing in next updated version to support users like me who want to deploy their fine-tuned model with your BAS. In any case, changing source code is the last thing to do for the robustness of our projects. thanks a lot! @hanxiao ",alright see mean anyway chance add server return next version support like want deploy model ba case source code last thing robustness thanks lot,issue,positive,negative,neutral,neutral,negative,negative
495291553,"After reducing the version to 1.8, another problem appeared:
```
I:?[35mVENTILATOR?[0m:freeze, optimize and export graph, could take a while...
I:?[36mGRAPHOPT?[0m:model config: F:\DL\Constant-TL\BERT\chinese_L-12_H-768_A-12\bert_config.json
I:?[36mGRAPHOPT?[0m:checkpoint: F:\DL\Constant-TL\BERT\chinese_L-12_H-768_A-12\bert_model.ckpt
E:?[36mGRAPHOPT?[0m:fail to optimize the graph!
Traceback (most recent call last):
  File ""d:\dl\anaconda3\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""d:\dl\anaconda3\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""D:\DL\anaconda3\Scripts\bert-serving-start.exe\__main__.py"", line 9, in <module>
  File ""d:\dl\anaconda3\lib\site-packages\bert_serving\server\cli\__init__.py"", line 5, in main
    server = BertServer(args)
  File ""d:\dl\anaconda3\lib\site-packages\bert_serving\server\__init__.py"", line 76, in __init__
    raise FileNotFoundError('graph optimization fails and returns empty result')
FileNotFoundError: graph optimization fails and returns empty result
```",reducing version another problem freeze optimize export graph could take model fail optimize graph recent call last file line file line code file line module file line main server file line raise optimization empty result graph optimization empty result,issue,negative,negative,neutral,neutral,negative,negative
495100470,"> The problem done by reducing the version to 1.8

Is the bert-as-server reduced to version 1.8? Can you send me your resolution process? @Jeffyangchina 
Thanks for you help.",problem done reducing version reduced version send resolution process thanks help,issue,negative,positive,positive,positive,positive,positive
495098273,The problem done by reducing the version to 1.8,problem done reducing version,issue,negative,neutral,neutral,neutral,neutral,neutral
495080499,"你也是win10嘛  可以告诉我软件的匹配版本嘛或者你有修改代码吗？
我的启动服务器时会报错：I:?[35mVENTILATOR?[0m:freeze, optimize and export graph, could take a while...
I:?[36mGRAPHOPT?[0m:model config: F:\DL\Constant-TL\BERT\chinese_L-12_H-768_A-12\bert_config.json
I:?[36mGRAPHOPT?[0m:checkpoint: F:\DL\Constant-TL\BERT\chinese_L-12_H-768_A-12\bert_model.ckpt
E:?[36mGRAPHOPT?[0m:fail to optimize the graph!
Traceback (most recent call last):
  File ""d:\dl\anaconda3\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""d:\dl\anaconda3\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""D:\DL\anaconda3\Scripts\bert-serving-start.exe\__main__.py"", line 9, in <module>
  File ""d:\dl\anaconda3\lib\site-packages\bert_serving\server\cli\__init__.py"", line 4, in main
    with BertServer(get_run_args()) as server:
  File ""d:\dl\anaconda3\lib\site-packages\bert_serving\server\__init__.py"", line 70, in __init__
    self.graph_path, self.bert_config = pool.apply(optimize_graph, (self.args,))
TypeError: cannot unpack non-iterable NoneType object",freeze optimize export graph could take model fail optimize graph recent call last file line file line code file line module file line main server file line unpack object,issue,negative,negative,neutral,neutral,negative,negative
495079713,"The same problem met in Windows10, did anyone know the result? I tried version of tensorflow-gpu 1.12 and 1.10  , 1.9.1 version of bert-server ",problem met anyone know result tried version version,issue,negative,neutral,neutral,neutral,neutral,neutral
495077220,"That dense layer is only meaningful after fine-tuning, as in the fine-tuning it is tailored to a specific downstream task.

Surely not all users are interested in the same downstream task, thus `bert-as-service` uses the embedding before that dense layer for the sake of generality.

Think it in another way, the closer an embedding to the downstream output, the more ""biased"" of that embedding; the closer an embedding to the word embedding, the less domain-specific information it contains. So in that sense, there is no ""correct"" or ""wrong"" about using `[CLS]` before/after the dense layer. It's mostly empirical.",dense layer meaningful specific downstream task surely interested downstream task thus dense layer sake generality think another way closer downstream output closer word le information sense correct wrong dense layer mostly empirical,issue,positive,positive,neutral,neutral,positive,positive
494901100,"Hi Han,
I was wondering what dimension sentence embedding should I expect with -pooling_layer -4 -3 -2 -1 and -pooling_strategy=REDUCE_MEAN ? currently I'm getting 4096, instead of 1024 from a large model, would it be alright averaging them after splitting the 4 layers at 1024?",hi han wondering dimension sentence expect currently getting instead large model would alright splitting,issue,negative,positive,positive,positive,positive,positive
494750552,"This works. I did following 

```
mkdir tmp2
export ZEROMQ_SOCK_TMP_DIR=./tmp2/
bert-serving-start -model_dir uncased_L-12_H-768_A-12  -graph_tmp_dir ./tmp2
```

and then 

```
$ ls tmp2/
c46622a0  c46622a2  c46622a4  c46622a6	c46622a8  tmp5n7z43dt
c46622a1  c46622a3  c46622a5  c46622a7	c46622a9
```

All of the above files are socket files except for `tmp5n7z43dt`, which is optimized graph.",work following export ca ca ca ca ca ca ca ca ca ca socket except graph,issue,negative,neutral,neutral,neutral,neutral,neutral
494670517,"I changed the code of source file graph.py as the BERT-src says after install bert-serving-server(use ""pip show --files bert-serving-server"" to get the install path) .  and it's fixed.  But it's not good choice, right? @hanxiao ",code source file install use pip show get install path fixed good choice right,issue,negative,positive,positive,positive,positive,positive
494302919,"That's true. For security concerns, you should not expose this service directly to the web, it's not optimized for security. You can at least containerize this service via docker.

btw, that warning is raised from `flask`, not from `bert-as-service`.
",true security expose service directly web security least service via docker warning raised flask,issue,positive,positive,neutral,neutral,positive,positive
494172673,"It was an issue with my path itself... I needed to end it without the final ""/"":  /Users/USERNAME/model ",issue path end without final,issue,negative,neutral,neutral,neutral,neutral,neutral
494168605,"@wenbotse the original `BERT` repository has an issue about this, and someone provides the suggestion for handling sequences > 512 with ""windowing"": https://github.com/google-research/bert/issues/66",original repository issue someone suggestion handling,issue,negative,positive,positive,positive,positive,positive
494167141,"I understand @wenbotse 's confusion though.  When I read this...

```
maximum length of sequence, longer sequence will be trimmed on the right side. Set it to NONE for dynamically using the longest sequence in a (mini)batch.
```

...it means to me that there is *no* max sequence length, and I *can* submit a sequence larger than 512 and it will still be processed, not truncated.

Although your statement above @hanxiao is true that the `BERT` model was trained on sequences no longer than 512, the original paper that introduced this architecture, ""Attention is All You Need"" (https://arxiv.org/pdf/1706.03762.pdf), claimed that infinite lengths *can* be processed even though the model was only trained on sequences of length 512.

```
We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.
```

In this case, for this project, the description of the `max_seq_len` argument is therefore unclear to me.

I *think* what it is saying is ""by setting to NONE, we will batch your documents to the longest length *as long as* it is still shorter than 512 tokens.""

It would be more clear if there was still a comment indicating that if set to `NONE`, sequences longer than 512 will *still* be truncated.",understand confusion though read maximum length sequence longer sequence right side set none dynamically sequence batch sequence length submit sequence still truncated although statement true model trained longer original paper architecture attention need infinite even though model trained length chose sinusoidal version may allow model extrapolate sequence longer training case project description argument therefore unclear think saying setting none batch length long still shorter would clear still comment set none longer still truncated,issue,negative,positive,positive,positive,positive,positive
493830999,"后来我debug进去了，也一直找不到原因，最后我怀疑内存8G可能少了点，于是又买了条8G内存，插上后，运行ok！！原来是内存不足。
正常运行后是这样的输出：
-------------------------------------------
....
I:VENTILATOR:get devices
W:VENTILATOR:no GPU available, fall back to CPU
I:VENTILATOR:device map: 
		worker  0 -> cpu
I:WORKER-0:use device cpu, load graph from C:\Users\ADMINI~1\AppData\Local\Temp\tmp2pdfg7vm
I:WORKER-0:ready and listening!
I:VENTILATOR:all set, ready to serve request!
I:VENTILATOR:new config request	req id: 1	client: b'f6082f24-5c91-4efb-91b7-e058b786e16e'
I:SINK:send config	client b'f6082f24-5c91-4efb-91b7-e058b786e16e'
I:VENTILATOR:new encode request	req id: 2	size: 3	client: b'f6082f24-5c91-4efb-91b7-e058b786e16e'
I:SINK:job register	size: 3	job id: b'f6082f24-5c91-4efb-91b7-e058b786e16e#2'
I:WORKER-0:new job	socket: 0	size: 3	client: b'f6082f24-5c91-4efb-91b7-e058b786e16e#2'
I:WORKER-0:job done	size: (3, 768)	client: b'f6082f24-5c91-4efb-91b7-e058b786e16e#2'
I:SINK:collect b'EMBEDDINGS' b'f6082f24-5c91-4efb-91b7-e058b786e16e#2' (E:3/T:0/A:3)
I:SINK:send back	size: 3	job id: b'f6082f24-5c91-4efb-91b7-e058b786e16e#2'
I:VENTILATOR:new encode request	req id: 3	size: 3	client: b'f6082f24-5c91-4efb-91b7-e058b786e16e'
I:SINK:job register	size: 3	job id: b'f6082f24-5c91-4efb-91b7-e058b786e16e#3'
I:WORKER-0:new job	socket: 0	size: 3	client: b'f6082f24-5c91-4efb-91b7-e058b786e16e#3'
I:WORKER-0:job done	size: (3, 768)	client: b'f6082f24-5c91-4efb-91b7-e058b786e16e#3'
I:SINK:collect b'EMBEDDINGS' b'f6082f24-5c91-4efb-91b7-e058b786e16e#3' (E:3/T:0/A:3)
I:SINK:send back	size: 3	job id: b'f6082f24-5c91-4efb-91b7-e058b786e16e#3'",ventilator get ventilator available fall back ventilator device map worker use device load graph ready listening ventilator set ready serve request ventilator new request id client sink send client ventilator new encode request id size client sink job register size job id new job socket size client job done size client sink collect sink send back size job id ventilator new encode request id size client sink job register size job id new job socket size client job done size client sink collect sink send back size job id,issue,positive,positive,positive,positive,positive,positive
492990820,"
Hi , Have you got the solution ?  @feiyuchen7 @shilan910 @zysNLP 
I just do as @hanxiao  say, add a  expect num hidden layer in sentence pair classification task when pretraining, but i'm not sure the new Representation would be how much badder than the 768 one .",hi got solution say add expect hidden layer sentence pair classification task pretraining sure new representation would much one,issue,positive,positive,positive,positive,positive,positive
492903718,"the same problem： File ""c:\programdata\anaconda3\lib\site-packages\bert_serving\server\__init__.py"", line 70, in __init__
    self.graph_path, self.bert_config = pool.apply(optimize_graph, (self.args,))
TypeError: cannot unpack non-iterable NoneType object。

possible reason：the model file has been damaged 
the solution is：downlaod the model from the readme, change the model file",file line unpack possible model file solution model change model file,issue,negative,neutral,neutral,neutral,neutral,neutral
492879580,Does anyone how long does it take to run this? Mine stayed in I:GRAPHOPT:[gra:opt:140]:freeze... for nearly one day and have not continued,anyone long take run mine stayed gra opt freeze nearly one day continued,issue,negative,positive,neutral,neutral,positive,positive
492629184,"Create a start-bert-as-service.py with the following code

```
import sys

from bert_serving.server import BertServer
from bert_serving.server.helper import get_run_args


if __name__ == '__main__':
    args = get_run_args()
    server = BertServer(args)
    server.start()
    server.join()
```


so you can run with the following command
python start-bert-as-service.py -model_dir ./tmp/chinese_L-12_H-768_A-12/ -num_worker=1",create following code import import import server run following command python,issue,negative,neutral,neutral,neutral,neutral,neutral
492571603,"I had the same problem in win10, Finally located at two points:
1、""bert-serving-start -model_dir /chinese_L-12_H-768_A-12""，this command should remove ""/"";
2、numpy version should update from 1.15.2 to 1.16.3",problem win finally two command remove version update,issue,negative,positive,positive,positive,positive,positive
492552659,"This turned out to be a local javascript issue. Seems like ""contentType: 'application/json; charset=utf-8'"" needs to be set in addition to ""dataType='json'"". Closing the issue.",turned local issue like need set addition issue,issue,negative,neutral,neutral,neutral,neutral,neutral
491993606,"That seems beyond the scope of this repository, but https://towardsdatascience.com/pre-training-bert-from-scratch-with-cloud-tpu-6e2f71028379 is a quite good description on doing full training of BERT models from your own data.",beyond scope repository quite good description full training data,issue,negative,positive,positive,positive,positive,positive
491821093,"My service calculate the ""get_masked_lm_output()"" that in bert code, it seems cost too much.
",service calculate code cost much,issue,negative,positive,positive,positive,positive,positive
491470900,"Oops, sorry -- I haven't started the server and was running the script. My bad, closing the issue.",sorry server running script bad issue,issue,negative,negative,negative,negative,negative,negative
491170726,"> please help me. I used your method of fine-tuning the model, but reported this error.
> ![KU0)B4``)(}3WKH$C11LD0T](https://user-images.githubusercontent.com/49576595/57019974-8f630b00-6c5a-11e9-9b4a-8c189e7b4036.png)
> ![5ZVM9} T}HA ZO@$YCC)_}J](https://user-images.githubusercontent.com/49576595/57020050-c0dbd680-6c5a-11e9-8932-727306f2c3db.png)
> ![XTW1K$R_5VID 4IW4HC))CR](https://user-images.githubusercontent.com/49576595/57020051-c1746d00-6c5a-11e9-94cf-f813cf96d811.png)
> @hanxiao

hi, it seems your file_path is not correct. ",please help used method model error ha zo hi correct,issue,positive,neutral,neutral,neutral,neutral,neutral
489573686,"@erichare We ended up using the huggingface implementation instead. But the steps above should be relatively easy to implement. Alternatively you can look at the huggingface implementation, which is a much easier way to get started with the different squad and other model heads. ",ended implementation instead relatively easy implement alternatively look implementation much easier way get different squad model,issue,positive,positive,positive,positive,positive,positive
489554022,"i want to find the reason ,but how can i debug the server code in pycharm ? although the server run in cli command line.",want find reason server code although server run command line,issue,negative,neutral,neutral,neutral,neutral,neutral
488367806,"Please re-open this issue:
I am having the same problem. Using the directions found on https://bert-as-service.readthedocs.io/en/latest/tutorial/http-server.html I got 'command not found.'  I'm running...
Ubuntu 18.04 LTS, 
Python 3.6.7, 
Flask==1.0.2, 
Flask-Compress==1.4.0,
Flask-Cors==3.0.7, 
Flask-JSON==0.3.3,
tensorboard==1.13.1
tensoflow==1.13.1
tensorflow-estimator==1.13.0 
bert-serving-client==1.8.9
bert-serving-server==1.8.9

1. The version of bert-serving-*  is a concern because GitHub releases shows 1.8.1 as the most recent version.
2. when executing bert-serving-start I receive ""bert-serving-start: command not found""

Note: I did see the other issue: #194, and suspect it's the same, however it seems as though it's closed as well, and is on the Windows OS. Therefore I'm commenting here.",please issue problem found got found running python version concern recent version receive command found note see issue suspect however though closed well o therefore,issue,negative,negative,neutral,neutral,negative,negative
488286284,"please help me. I used your method of fine-tuning the model, but reported this error.
![KU0)B4``)(}3WKH$C11LD0T](https://user-images.githubusercontent.com/49576595/57019974-8f630b00-6c5a-11e9-9b4a-8c189e7b4036.png)
![5ZVM9} T}HA ZO@$YCC)_}J](https://user-images.githubusercontent.com/49576595/57020050-c0dbd680-6c5a-11e9-8932-727306f2c3db.png)
![XTW1K$R_5VID 4IW4HC))CR](https://user-images.githubusercontent.com/49576595/57020051-c1746d00-6c5a-11e9-94cf-f813cf96d811.png)
@hanxiao ",please help used method model error ha zo,issue,positive,neutral,neutral,neutral,neutral,neutral
487837069,@IDIDIR I tried it. I think it's a problem with old tmp dir,tried think problem old,issue,negative,positive,neutral,neutral,positive,positive
487827417,"no, the problem is consistently reproduced.
need create dir and export to env
export ZEROMQ_SOCK_TMP_DIR=/tmp/zmq
link: ```https://sanidem.home.blog/tag/docker/```",problem consistently need create export export link,issue,negative,positive,positive,positive,positive,positive
487502146,"I use this command, but I can't see tmp* file in /tmp,  Is I use this command and the folder become a hidden folder?",use command ca see file use command folder become hidden folder,issue,negative,negative,negative,negative,negative,negative
487492202,"Geez, that is *some* tmp files. Note, you can set `ZEROMQ_SOCK_TMP_DIR` before starting the server, see:

https://github.com/hanxiao/bert-as-service/#q-after-running-the-server-i-have-several-garbage-tmpxxxx-folders-how-can-i-change-this-behavior-

 ",note set starting server see,issue,negative,neutral,neutral,neutral,neutral,neutral
486480918,Hello~Can someone give me some ideas to optimize the Bert with fp16 and xla?Thanks very much!!!O(∩_∩)O,someone give optimize thanks much,issue,positive,positive,positive,positive,positive,positive
485652226,"btw I realize there is a mistake in my answer:
> When `is_tokenized=True` your input should be whitespace separated, so in this case ss[(j+1):] makes totally sense.

The correct one should be:

When `is_tokenized=True` each of your sentence should be a list of strings (rather than a single string), so in this case `ss[(j+1):]` makes totally sense. See example code at https://bert-as-service.readthedocs.io/en/latest/source/client.html#client.BertClient.encode",realize mistake answer input case totally sense correct one sentence list rather single string case totally sense see example code,issue,negative,negative,neutral,neutral,negative,negative
485651481,"You need to set `-pooling_strategy NONE` on the server side if you want to get token-wise embeddings.

btw, I'm quite confused by your version of `bert-serving-server`. I assume you are showing your local `bert-serving-server` version (1.7.9) whereas you are calling `bc.encode` to a remote `bert-serving-server`, which is 1.8.9. Otherwise, it makes no sense and the client shall throw a version-mismatch exception.",need set none server side want get quite confused version assume showing local version whereas calling remote otherwise sense client shall throw exception,issue,negative,negative,negative,negative,negative,negative
485393084,"> * Sorry but you are wrong. `read_tokenized_examples` wont be called by default, it is only called when `bc.encode(..., is_tokenized=True)`. When `is_tokenized=True` your input should be whitespace separated, so in this case `ss[(j+1):]` makes totally sense.
> * `|||` (wrapped with white space) is a pattern that defines a sentence pair. If you don't wrap whitespace around `|||`, then the pattern fails and the service will consider the input as a single sentence, which explains your case.

Thanks a lot! I understand. : )",sorry wrong wont default input case totally sense wrapped white space pattern sentence pair wrap around pattern service consider input single sentence case thanks lot understand,issue,negative,negative,negative,negative,negative,negative
485379120,"1. Sorry but you are wrong. `read_tokenized_examples` wont be called by default, it is only called when `bc.encode(..., is_tokenized=True)`. When `is_tokenized=True` your input should be whitespace separated, so in this case `ss[(j+1):]` makes totally sense.

2. ` ||| ` (wrapped with white space) is a pattern that defines a sentence pair. If you don't wrap whitespace around `|||`, then the pattern fails and the service will consider the input as a single sentence, which explains your case.",sorry wrong wont default input case totally sense wrapped white space pattern sentence pair wrap around pattern service consider input single sentence case,issue,negative,negative,negative,negative,negative,negative
484690040,Why not return np.zeros whenever input is empty?,return whenever input empty,issue,negative,negative,neutral,neutral,negative,negative
484370364,"Version too old
```bash
pip install -U bert-serving-server bert-serving-client
```",version old bash pip install,issue,negative,positive,neutral,neutral,positive,positive
483150342,"I found the hang arises due to the call of `BertServer.close`, regardless of the version (at least the problem also persists in v1.8.6, v1.8.0, v1.7.0).
Also, `Ctrl-C`-ing the server terminates the process without issue.",found due call regardless version least problem also also server process without issue,issue,negative,negative,negative,negative,negative,negative
483145180,you may want to read the tech details: https://hanxiao.github.io/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ#Serving-with-low-latency,may want read tech,issue,negative,neutral,neutral,neutral,neutral,neutral
483144947,"a better idea would be using zeromq c api and write a c-BertClient, welcome to contribute.",better idea would write welcome contribute,issue,positive,positive,positive,positive,positive,positive
482932401,"Hi, I experienced the same issue.  The symptoms are exactly the same as he described. Pytorch_pretrained_bert is about 100 times faster in my machine than bert as service.  and I am totally sure my GPU is working hard just by the sound of the fan.....

My text is Kaggle Gender pronoun competition,  and it costs 1.5 hour to deal the train data in bert as service, but in pytorch_pretrained_bert it takes only 15s.

I am really curious what happened in bert as service...",hi experienced issue exactly time faster machine service totally sure working hard sound fan text gender pronoun competition hour deal train data service really curious service,issue,positive,positive,positive,positive,positive,positive
482442483,"https://github.com/hanxiao/bert-as-service#using-your-own-tokenizer

, and the least you can do is to spell your question correctly.",least spell question correctly,issue,negative,negative,negative,negative,negative,negative
481584913,"不可以。
连用作语言模型都不行，因为 lm 和 w2v 的 loss 不同：
[can the pre-trained model be used as a language model? · Issue #35 · google-research/bert](https://github.com/google-research/bert/issues/35)",loss model used language model issue,issue,negative,neutral,neutral,neutral,neutral,neutral
480861716,"I find the problem . previously I always using the docker toolbox ,that tool isn't easy for specify the cpu numbers or memory , after I changing to the windows 10 pro system and installing the docker for windows tool . I can easily manage the cpu and the memory .

![image](https://user-images.githubusercontent.com/13706689/55732871-523a9d00-5a4f-11e9-9b64-f0a4fd863732.png)

and when executing the tf.import_graph_def method , task monitor shows that the io grows up to 99%
so that operation really cost the io resource and the memory , in the docker it show be more that 8GB memory for the computer to load that model .

it works
![image](https://user-images.githubusercontent.com/13706689/55733018-a180cd80-5a4f-11e9-8ca6-95c00409c39a.png)

thanks
weizhen 

",find problem previously always docker toolbox tool easy specify memory pro system docker tool easily manage memory image method task monitor io operation really cost io resource memory docker show memory computer load model work image thanks,issue,positive,positive,positive,positive,positive,positive
480738904,I personally can't reproduce this on docker with TF 1.10. I remember some people encountered this issue and solved it by using a specific version of tensorflow.,personally ca reproduce docker remember people issue specific version,issue,negative,neutral,neutral,neutral,neutral,neutral
480724478,"@hanxiao 
Dear hanxiao

I find in the docker which line of code that cause long time and blocking here.
can you take a look?

![image](https://user-images.githubusercontent.com/13706689/55707485-441c5a80-5a16-11e9-8549-c5551492304c.png)

very appreciate for you

Thanks
weizhen",dear find docker line code cause long time blocking take look image appreciate thanks,issue,positive,positive,neutral,neutral,positive,positive
480514164,"Observing similar issue on AWS p3 instance. The service freezes after receiving a request. Apparently the service hangs on some threading issue..

```
I:VENTILATOR:[__i:_ru:216]:terminated!
Traceback (most recent call last):
  File ""/home/hadoop/venv/bin/bert-serving-start"", line 10, in <module>
    sys.exit(main())
  File ""/home/hadoop/venv/local/lib/python3.6/dist-packages/bert_serving/server/cli/__init__.py"", line 5, in main
    server.join()
  File ""/usr/lib64/python3.6/threading.py"", line 1056, in join
    self._wait_for_tstate_lock()
  File ""/usr/lib64/python3.6/threading.py"", line 1072, in _wait_for_tstate_lock
    elif lock.acquire(block, timeout):
KeyboardInterrupt
```

Still haven't figure out why, but apparently this problem is only transient and only happens at start-up time. If the first encoding task goes through then everything else goes through.",observing similar issue instance service request apparently service issue ventilator recent call last file line module main file line main file line join file line block still figure apparently problem transient time first task go everything else go,issue,negative,positive,neutral,neutral,positive,positive
480136679,Thanks a lot! But notice that exposing this service directly to the public internet is not recommended. Securing the incoming traffic (e.g. via encryption) is out of scope of this project.,thanks lot notice service directly public incoming traffic via encryption scope project,issue,negative,positive,neutral,neutral,positive,positive
480135690,"Dear hanxiao

I've made few change with the code , adding some log print , and install from the source
you can see the change in the following picture:
![捕获5](https://user-images.githubusercontent.com/13706689/55601688-a8c88280-5794-11e9-9ee2-95ba3d4b204c.PNG)

and in windows platform I can see the socket ip address and port clearly:
![捕获4](https://user-images.githubusercontent.com/13706689/55601716-c5fd5100-5794-11e9-8893-0477583f41a0.PNG)


but in the docker container , I can't see the socket ip address and the port:
![捕获3](https://user-images.githubusercontent.com/13706689/55601750-e9280080-5794-11e9-8978-8d7f001add48.PNG)

does it matter?

thanks
weizhen",dear made change code log print install source see change following picture platform see socket address port clearly docker container ca see socket address port matter thanks,issue,positive,positive,positive,positive,positive,positive
480035201,"![image](https://user-images.githubusercontent.com/13706689/55583445-7860f400-5754-11e9-8441-9aa57ea76d54.png)

Is there something relative with the zmq connection in the docker?
for example the endpoints between different computers show be exposed out in the docker container?
because I found the problem occurred between just few line codes above , and if these code go through and it show print out  already and listening

Thanks
weizhen


",image something relative connection docker example different show exposed docker container found problem line code go show print already listening thanks,issue,negative,positive,neutral,neutral,positive,positive
479854433,"I know. That's why I used this expression. I have already tested it via bert-serving-client-1.8.9.
Refer to this: https://docs.python.org/3/library/stdtypes.html#str.isdigit",know used expression already tested via refer,issue,negative,neutral,neutral,neutral,neutral,neutral
479733841,"> version too old? please do `pip install -U` first


below is my Dockerfile config , and it build successfully
`
#base image
FROM centos

#MAINTAINER
MAINTAINER weizhen_zhao@163.com

#put rasa_model_output.zip into /usr/local/src and unpack rasa_model_output
ADD rasa_model_output.zip /usr/local/src/

#put chinese_L-12_H-768_A-12.zip into /usr/local/src and unpack chinese_L-12_H-768_A-12
ADD chinese_L-12_H-768_A-12.zip /usr/local/src/


#put Python-3.5.6.tgz into /usr/local/src and unpack Python-3.5.6.tgz
COPY Python-3.5.6.tgz /usr/local/src/


ENV SRC /usr/local/src/
WORKDIR ${SRC} 



#unpack python
RUN tar -xzvf ./Python-3.5.6.tgz
RUN yum -y install zlib zlib-devel gcc ncurses ncurses-devel


#install python
ENV PYTHONDIR /usr/local/src/Python-3.5.6
WORKDIR ${PYTHONDIR}
RUN ./configure --prefix=/usr/local/python-3.5.6
RUN yum -y install make openssl-devel bzip2-devel expat-devel gdbm-devel readline-devel sqlite-devel
RUN echo `pwd`
RUN make && make install

#set soft link for python3 and pip3
ENV root /
WORKDIR ${root} 
RUN ln -s /usr/local/python-3.5.6/bin/python3 /usr/bin/python3
RUN ln -s /usr/local/python-3.5.6/bin/pip3 /usr/bin/pip3

RUN pip3 install --upgrade pip -i https://mirrors.aliyun.com/pypi/simple
RUN pip3 install -U bert-serving-server -i https://mirrors.aliyun.com/pypi/simple
RUN yum -y install unzip


WORKDIR ${SRC}

#unzip and start bert
RUN unzip rasa_model_output.zip
RUN unzip chinese_L-12_H-768_A-12.zip


#starting bert server
WORKDIR ${root}
RUN pip3 install tensorflow -i https://mirrors.aliyun.com/pypi/simple
COPY ./entrypoint.sh /usr/local/src/
RUN chmod +x /usr/local/src/entrypoint.sh
WORKDIR ${SRC}
ENTRYPOINT [""/usr/local/src/entrypoint.sh""]








`

I already used pip3 install -U bert-serving-server to install the component.

Thanks
weizhen",version old please pip install first build successfully base image maintainer maintainer put unpack add put unpack add put unpack copy unpack python run tar run install install python run run install make run echo run make make install set soft link python pip root root run run run pip install upgrade pip run pip install run install start run run starting server root run pip install copy run already used pip install install component thanks,issue,positive,negative,neutral,neutral,negative,negative
479733287,"@A6Matrix version too old, please do `pip install -U` first",version old please pip install first,issue,negative,positive,positive,positive,positive,positive
479561557,"I get this error with a fine-tuned model, but not the original.

EDIT: This was due to not specifying a couple of extra parameters needed: `-tuned_model_dir` and `-ckpt_name`",get error model original edit due couple extra,issue,negative,positive,neutral,neutral,positive,positive
479368716,"> @ZRiddle In fact there is no need to store the fine tuned weights, just add the variables and load the fine tuned model after 'run_classifier'.
> I add a new pooling strategy, and the following codes in graph.py:
> 
> ```
>     if args.pooling_strategy == PoolingStrategy.CLASSIFICATION:
>                 hidden_size = 768
>                 output_weights = tf.get_variable(
>                     ""output_weights"", [args.num_labels, hidden_size],
>                     initializer=tf.truncated_normal_initializer(stddev=0.02))
> 
>                 output_bias = tf.get_variable(
>                     ""output_bias"", [args.num_labels], initializer=tf.zeros_initializer())
> ```
> add the above variables before ""tvars = tf.trainable_variables()""
> 
> then for pooling:
> 
> ```
>      elif args.pooling_strategy == PoolingStrategy.CLASSIFICATION:
>                     pooled = tf.squeeze(encoder_layer[:, 0:1, :], axis=1)
>                     logits = tf.matmul(pooled, output_weights, transpose_b=True)
>                     logits = tf.nn.bias_add(logits, output_bias)
>                     pooled = tf.nn.softmax(logits, axis=-1)
> ```
> don't forget to add the following args:
> num_labels
这句代码  : pooled = tf.squeeze(encoder_layer[:, 0:1, :], axis=1)
需要修改为 ：pooled = model.get_pooled_output()
你觉得呢？",fact need store fine tuned add load fine tuned model add new strategy following add forget add following,issue,negative,positive,positive,positive,positive,positive
478970196,"fyi, this issue is fixed in #305 and the new feature is available since 1.8.9, please do
```
pip install -U bert-serving-server bert-serving-client
```
for the update.

When you start the server with -http_port, you should see something like this
![image](https://user-images.githubusercontent.com/2041322/55401775-5749a900-5584-11e9-82b3-ee0a924bbf59.png)

```
I:VENTILATOR:[__i:_ru:163]:all set, ready to serve request!
```
this means the server is ready.
",issue fixed new feature available since please pip install update start server see something like image ventilator set ready serve request server ready,issue,positive,positive,positive,positive,positive,positive
478964183,"I confirm it's a bug, related to #296 fixing it now",confirm bug related fixing,issue,negative,neutral,neutral,neutral,neutral,neutral
478955126,"same problem here even with 1.8.8

➜  query-embedding curl -X POST http://127.0.0.1:8125/encode \
  -H 'content-type: application/json' \
  -d '{""id"": 123,""texts"": [""hello world""], ""is_tokenized"": false}'
curl: (7) Failed to connect to 127.0.0.1 port 8125: Connection refused

here is my setup:

(query-embedding) ➜  query-embedding pip freeze
absl-py==0.7.1
astor==0.7.1
bert-serving-client==1.8.8
bert-serving-server==1.8.8
Click==7.0
Flask==1.0.2
Flask-Compress==1.4.0
Flask-Cors==3.0.7
Flask-JSON==0.3.3
gast==0.2.2
GPUtil==1.4.0
grpcio==1.19.0
h5py==2.9.0
itsdangerous==1.1.0
Jinja2==2.10
Keras-Applications==1.0.7
Keras-Preprocessing==1.0.9
Markdown==3.1
MarkupSafe==1.1.1
mock==2.0.0
numpy==1.16.2
pbr==5.1.3
protobuf==3.7.1
pyzmq==18.0.1
six==1.12.0
tensorboard==1.13.1
tensorflow==1.13.1
tensorflow-estimator==1.13.0
termcolor==1.1.0
Werkzeug==0.15.1
You are using pip version 10.0.1, however version 19.0.3 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.

(query-embedding) ➜  query-embedding bert-serving-start -model_dir \`pwd\`/uncased_L-12_H-768_A-12/ -num_worker=2 -http_port 8125
usage: /Users/gla/py/annotateimages/query-embedding/bin/bert-serving-start -model_dir /Users/gla/git/query-embedding/uncased_L-12_H-768_A-12/ -num_worker=2 -http_port 8125
                 ARG   VALUE
__________________________________________________
           ckpt_name = bert_model.ckpt
         config_name = bert_config.json
                cors = *
                 cpu = False
          device_map = []
  fixed_embed_length = False
                fp16 = False
 gpu_memory_fraction = 0.5
       graph_tmp_dir = None
    http_max_connect = 10
           http_port = 8125
        mask_cls_sep = False
      max_batch_size = 256
         max_seq_len = 25
           model_dir = /Users/gla/git/query-embedding/uncased_L-12_H-768_A-12/
          num_worker = 2
       pooling_layer = [-2]
    pooling_strategy = REDUCE_MEAN
                port = 5555
            port_out = 5556
       prefetch_size = 10
 priority_batch_size = 16
show_tokens_to_client = False
     tuned_model_dir = None
             verbose = False
                 xla = False

I:VENTILATOR:[__i:__i: 66]:freeze, optimize and export graph, could take a while...
I:GRAPHOPT:[gra:opt: 52]:model config: /Users/gla/git/query-embedding/uncased_L-12_H-768_A-12/bert_config.json
I:GRAPHOPT:[gra:opt: 55]:checkpoint: /Users/gla/git/query-embedding/uncased_L-12_H-768_A-12/bert_model.ckpt
I:GRAPHOPT:[gra:opt: 59]:build graph...

WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
If you depend on functionality not listed there, please file an issue.

I:GRAPHOPT:[gra:opt:128]:load parameters from checkpoint...
I:GRAPHOPT:[gra:opt:132]:optimize...
I:GRAPHOPT:[gra:opt:140]:freeze...
I:GRAPHOPT:[gra:opt:145]:write graph to a tmp file: /var/folders/fp/zb8y8hzn371dwkp2xtgs_8w9tvxd3s/T/tmpdwmqwkgk
I:VENTILATOR:[__i:__i: 74]:optimized graph is stored at: /var/folders/fp/zb8y8hzn371dwkp2xtgs_8w9tvxd3s/T/tmpdwmqwkgk
I:VENTILATOR:[__i:_ru:128]:bind all sockets
I:VENTILATOR:[__i:_ru:132]:open 8 ventilator-worker sockets
I:VENTILATOR:[__i:_ru:135]:start the sink
I:SINK:[__i:_ru:303]:ready
I:VENTILATOR:[__i:_ge:219]:get devices
W:VENTILATOR:[__i:_ge:243]:no GPU available, fall back to CPU
I:VENTILATOR:[__i:_ge:252]:device map: 
		worker  0 -> cpu
		worker  1 -> cpu
I:VENTILATOR:[__i:_ru:151]:start http proxy
I:WORKER-0:[__i:_ru:514]:use device cpu, load graph from /var/folders/fp/zb8y8hzn371dwkp2xtgs_8w9tvxd3s/T/tmpdwmqwkgk
I:WORKER-1:[__i:_ru:514]:use device cpu, load graph from /var/folders/fp/zb8y8hzn371dwkp2xtgs_8w9tvxd3s/T/tmpdwmqwkgk
I:WORKER-0:[__i:gen:542]:ready and listening!
I:WORKER-1:[__i:gen:542]:ready and listening!


",problem even curl post id hello world false curl connect port connection setup pip freeze pip version however version available consider via install upgrade pip command usage value false false false none false port false none verbose false false ventilator freeze optimize export graph could take gra opt model gra opt gra opt build graph warning module included information please see depend functionality listed please file issue gra opt load gra opt optimize gra opt freeze gra opt write graph file ventilator graph ventilator bind ventilator open ventilator start sink sink ready ventilator get ventilator available fall back ventilator device map worker worker ventilator start proxy use device load graph use device load graph gen ready listening gen ready listening,issue,positive,negative,negative,negative,negative,negative
478282237,"> 
> 
> @kramer425 I follow your mathod ,But I got `F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use SSE4.2 instructions, but these aren't available on your machine. Aborted (core dumped)`

Are you able to install that version of tensorflow on your machine without docker? Maybe try to see if there are other base images that will support your cpu.",follow got library use available machine aborted core able install version machine without docker maybe try see base support,issue,negative,positive,neutral,neutral,positive,positive
478254656,"@kramer425 I follow your mathod ,But I got  `F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use SSE4.2 instructions, but these aren't available on your machine.
Aborted (core dumped)`",follow got library use available machine aborted core,issue,negative,positive,positive,positive,positive,positive
478192996,"fyi, this issue if fixed in #299 and the new feature is available since 1.8.8, please do
```bash
pip install -U bert-serving-server bert-serving-client
```
for the update.
",issue fixed new feature available since please bash pip install update,issue,negative,positive,positive,positive,positive,positive
478192367,"aha got it! the error shows up when you start with http, fixing it now",aha got error start fixing,issue,negative,neutral,neutral,neutral,neutral,neutral
478113133,"I'm running into the same error using bert-serving-server[http] (Within or outside Docker).

`pip3 install bert-serving-server[http]`
`bert-serving-start -model_dir $PATH_MODEL -num_worker=1 -cpu -http_port 8125`

Here is a similar trace
```
usage: /usr/local/bin/bert-serving-start -model_dir /Users/****/Desktop/publication/uncased_L-24_H-1024_A-16 -num_worker=1 -cpu -http_port 8125
                 ARG   VALUE
__________________________________________________
           ckpt_name = bert_model.ckpt
         config_name = bert_config.json
                cors = *
                 cpu = True
          device_map = []
  fixed_embed_length = False
                fp16 = False
 gpu_memory_fraction = 0.5
       graph_tmp_dir = None
    http_max_connect = 10
           http_port = 8125
        mask_cls_sep = False
      max_batch_size = 256
         max_seq_len = 25
           model_dir = /Users/*****/Desktop/publication/uncased_L-24_H-1024_A-16
          num_worker = 1
       pooling_layer = [-2]
    pooling_strategy = REDUCE_MEAN
                port = 5555
            port_out = 5556
       prefetch_size = 10
 priority_batch_size = 16
show_tokens_to_client = False
     tuned_model_dir = None
             verbose = False
                 xla = False

I:VENTILATOR:[__i:__i: 66]:freeze, optimize and export graph, could take a while...
I:GRAPHOPT:[gra:opt: 52]:model config: /Users/*****/Desktop/publication/uncased_L-24_H-1024_A-16/bert_config.json
I:GRAPHOPT:[gra:opt: 55]:checkpoint: /Users/*****/Desktop/publication/uncased_L-24_H-1024_A-16/bert_model.ckpt
I:GRAPHOPT:[gra:opt: 59]:build graph...

WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
If you depend on functionality not listed there, please file an issue.

I:GRAPHOPT:[gra:opt:128]:load parameters from checkpoint...
I:GRAPHOPT:[gra:opt:132]:optimize...
I:GRAPHOPT:[gra:opt:140]:freeze...
I:GRAPHOPT:[gra:opt:145]:write graph to a tmp file: /var/folders/lm/s8kzdmsn0nd_3gp9zyffcxd80000gn/T/tmpz7uj9871
I:VENTILATOR:[__i:__i: 74]:optimized graph is stored at: /var/folders/lm/s8kzdmsn0nd_3gp9zyffcxd80000gn/T/tmpz7uj9871
I:VENTILATOR:[__i:_ru:128]:bind all sockets
I:VENTILATOR:[__i:_ru:132]:open 8 ventilator-worker sockets
I:VENTILATOR:[__i:_ru:135]:start the sink
I:SINK:[__i:_ru:303]:ready
I:VENTILATOR:[__i:_ge:219]:get devices
I:VENTILATOR:[__i:_ge:252]:device map: 
		worker  0 -> cpu
I:VENTILATOR:[__i:_ru:151]:start http proxy
I:WORKER-0:[__i:_ru:514]:use device cpu, load graph from /var/folders/lm/s8kzdmsn0nd_3gp9zyffcxd80000gn/T/tmpz7uj9871
I:WORKER-0:[__i:gen:542]:ready and listening!
Exception in thread Thread-1:
Traceback (most recent call last):
  File ""/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/usr/local/lib/python3.6/site-packages/bert_serving/server/__init__.py"", line 114, in run
    self._run()
  File ""/usr/local/lib/python3.6/site-packages/zmq/decorators.py"", line 75, in wrapper
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/zmq/decorators.py"", line 75, in wrapper
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/zmq/decorators.py"", line 75, in wrapper
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/bert_serving/server/zmq_decor.py"", line 27, in wrapper
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/bert_serving/server/__init__.py"", line 160, in _run
    p.is_ready.wait()
AttributeError: 'function' object has no attribute 'wait'
```

Using 1.8.2 worked for me!",running error within outside docker pip install similar trace usage value true false false none false port false none verbose false false ventilator freeze optimize export graph could take gra opt model gra opt gra opt build graph warning module included information please see depend functionality listed please file issue gra opt load gra opt optimize gra opt freeze gra opt write graph file ventilator graph ventilator bind ventilator open ventilator start sink sink ready ventilator get ventilator device map worker ventilator start proxy use device load graph gen ready listening exception thread recent call last file line file line run file line wrapper return file line wrapper return file line wrapper return file line wrapper return file line object attribute worked,issue,positive,negative,negative,negative,negative,negative
478097565,"HI @Accentax @eugenekoh , I was wondering if either of you have made progress in this task and are willing to share some of the code if so?",hi wondering either made progress task willing share code,issue,positive,positive,positive,positive,positive,positive
477957592,Had the same issue. After downgrading to my previously used version 1.8.2 it worked.,issue previously used version worked,issue,negative,negative,negative,negative,negative,negative
477849680,"> ```shell
> pip install -U bert-serving-server bert-serving-client
> ```

(Bert) C:\Users\fad>pip install -U bert-serving-server bert-serving-client
Requirement already up-to-date: bert-serving-server in c:\programdata\anaconda3\envs\bert\lib\site-packages (1.8.7)
Requirement already up-to-date: bert-serving-client in c:\programdata\anaconda3\envs\bert\lib\site-packages (1.8.7)
Requirement already satisfied, skipping upgrade: termcolor>=1.1 in c:\programdata\anaconda3\envs\bert\lib\site-packages (from bert-serving-server) (1.1.0)
Requirement already satisfied, skipping upgrade: numpy in c:\programdata\anaconda3\envs\bert\lib\site-packages (from bert-serving-server) (1.16.2)
Requirement already satisfied, skipping upgrade: six in c:\programdata\anaconda3\envs\bert\lib\site-packages (from bert-serving-server) (1.12.0)
Requirement already satisfied, skipping upgrade: pyzmq>=17.1.0 in c:\programdata\anaconda3\envs\bert\lib\site-packages (from bert-serving-server) (18.0.0)
Requirement already satisfied, skipping upgrade: GPUtil>=1.3.0 in c:\programdata\anaconda3\envs\bert\lib\site-packages (from bert-serving-server) (1.4.0)",shell pip install pip install requirement already requirement already requirement already satisfied skipping upgrade requirement already satisfied skipping upgrade requirement already satisfied skipping upgrade six requirement already satisfied skipping upgrade requirement already satisfied skipping upgrade,issue,positive,positive,positive,positive,positive,positive
477846482,"I did pip install the server. ...heres current update: 

I had made a new anaconda env, I then used the command : pip install --ignore-installed --upgrade PackageURL   to install tensorflow again

I tried to run bert-server-start and got '\bert-serving-start' is not recognized as an internal or external command,operable program or batch file. 

So then I ran it as a python file, and got a new error: 

I:[35mVENTILATOR[0m:freeze, optimize and export graph, could take a while...
I:[36mGRAPHOPT[0m:model config: /tmp/english_L-12_H-768_A-12/bert_config.json
I:[36mGRAPHOPT[0m:checkpoint: /tmp/english_L-12_H-768_A-12/bert_model.ckpt
E:[36mGRAPHOPT[0m:fail to optimize the graph!
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\ProgramData\Anaconda3\envs\Bert\lib\site-packages\bert_serving\server\__init__.py"", line 70, in __init__
    self.graph_path, self.bert_config = pool.apply(optimize_graph, (self.args,))
TypeError: 'NoneType' object is not iterable
>>> server.start()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
NameError: name 'server' is not defined",pip install server current update made new anaconda used command pip install upgrade install tried run got internal external command operable program batch file ran python file got new error freeze optimize export graph could take model fail optimize graph recent call last file line module file line object iterable recent call last file line module name defined,issue,negative,negative,neutral,neutral,negative,negative
477846342,"this doesn't looks like the latest version of bert-as-service. please do 

```bash
pip install -U bert-serving-server bert-serving-client
```
for the update",like latest version please bash pip install update,issue,positive,positive,positive,positive,positive,positive
477832822,"hmm, I can't see how this line breaks the code. Test is fine and I can't reproduce this in my environment.",ca see line code test fine ca reproduce environment,issue,negative,positive,positive,positive,positive,positive
477430499,"use `http_max_connect ` in https://bert-as-service.readthedocs.io/en/latest/source/server.html#server-side-api

That said, this wont happen unless you have 10 *concurrent* http requests. So your problem is not increasing this value, but using it in a correct way.

I notice that your job has size only 1, meaning `batch_size=1`, aka one sentence a time. This is really bad and shouldn't happen unless you are serving real users in production (where the traffic is sparse and out of your control).

Rule of thumb, you should always **send a batch** to the server. A batch can be as large as 1 million sentences, but no worry, just send it to the server. The sever already implements a sophisticated batching scheme to handle nearly all cases.

*Do not* split a batch into a set of single-sentence-batches and send to the server concurrently. This would be over-engineering and won't give you any benefit.",use said wont happen unless concurrent problem increasing value correct way notice job size meaning aka one sentence time really bad happen unless serving real production traffic sparse control rule thumb always send batch server batch large million worry send server sever already sophisticated scheme handle nearly split batch set send server concurrently would wo give benefit,issue,negative,positive,neutral,neutral,positive,positive
477121628,I have solved this problem. It may be due to the bad or unstable Internet.  I didn't do anything and the problem fixed by itself.,problem may due bad unstable anything problem fixed,issue,negative,negative,negative,negative,negative,negative
477086676,"I met the same problem!  
I installed tensorflow-gpu 1.11.0 in anaconda.
After I type the command ""bert-serving-start -model ./chinese_L-12_H-768_A-12 -num_worker=2"", there comes an error:
  File ""zmq/backend/cython/socket.pyx"", line 547, in zmq.backend.cython.socket.Socket.bind
  File ""zmq/backend/cython/checkrc.pxd"", line 25, in zmq.backend.cython.checkrc._check_rc
zmq.error.ZMQError: Operation not permitted



",met problem anaconda type command come error file line file line operation permitted,issue,negative,neutral,neutral,neutral,neutral,neutral
477027858,"Hi Han - Thanks for your great work. Really appreciate it :) 

Though I could install all the requirements,  I am still facing the same as @kvdesai for the following system config:

Ubuntu v16.04 (CPU)
Python v3.5.2
TensorFlow 1.13.1
bert 1.8.6

The server does not go beyond 

`I:WORKER-0:[__i:_ru:492]:use device cpu, load graph from `

Below is my full log:
![Screenshot from 2019-03-27 08-05-12](https://user-images.githubusercontent.com/16071833/55059548-1c68e200-5067-11e9-86d4-ced5c3147c50.png)

Any help on this is greatly appreciated. Thanks :) ",hi han thanks great work really appreciate though could install still facing following system python server go beyond use device load graph full log help greatly thanks,issue,positive,positive,positive,positive,positive,positive
476929935,"I clicked a space on the cmd, and it showed 'ready and listening'. Now it's working. Thanks a lot! :)",space listening working thanks lot,issue,negative,positive,positive,positive,positive,positive
476927387,Any clue from the server log what is failing specifically?,clue server log failing specifically,issue,negative,neutral,neutral,neutral,neutral,neutral
476926732,"worker is not started according to the log. reasons can be tf, nvidia-driver, cuda, etc., please search issue for the solution.
You should see ""ready and listening"" as illustrated in the animation here 
https://github.com/hanxiao/bert-as-service/#3-use-client-to-get-sentence-encodes",worker according log please search issue solution see ready listening animation,issue,positive,positive,positive,positive,positive,positive
476925877,"- there is no `do_lower_case` setting
- make sure you know what model you are using `cased` or `uncased`, see https://github.com/hanxiao/bert-as-service/#1-download-a-pre-trained-bert-model
- you can customize the tokenization behavior by https://github.com/hanxiao/bert-as-service/#using-your-own-tokenizer or monitor its behavior.",setting make sure know model cased uncased see behavior monitor behavior,issue,negative,positive,positive,positive,positive,positive
476925699,"Thanks for your quick response.
```
(py36) E:\BERT>bert-serving-start -model_dir cased_L-24_H-1024_A-16/ -num_worker=4
usage: C:\Users\USER\AppData\Local\conda\conda\envs\py36\Scripts\bert-serving-start -model_dir cased_L-24_H-1024_A-16/ -num_worker=4
                 ARG   VALUE
__________________________________________________
           ckpt_name = bert_model.ckpt
         config_name = bert_config.json
                cors = *
                 cpu = False
          device_map = []
  fixed_embed_length = False
                fp16 = False
 gpu_memory_fraction = 0.5
       graph_tmp_dir = None
    http_max_connect = 10
           http_port = None
        mask_cls_sep = False
      max_batch_size = 256
         max_seq_len = 25
           model_dir = cased_L-24_H-1024_A-16/
          num_worker = 4
       pooling_layer = [-2]
    pooling_strategy = REDUCE_MEAN
                port = 5555
            port_out = 5556
       prefetch_size = 10
 priority_batch_size = 16
show_tokens_to_client = False
     tuned_model_dir = None
             verbose = False
                 xla = False

I:[35mVENTILATOR[0m:freeze, optimize and export graph, could take a while...
I:[36mGRAPHOPT[0m:model config: cased_L-24_H-1024_A-16/bert_config.json
I:[36mGRAPHOPT[0m:checkpoint: cased_L-24_H-1024_A-16/bert_model.ckpt
I:[36mGRAPHOPT[0m:build graph...

WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
If you depend on functionality not listed there, please file an issue.

I:[36mGRAPHOPT[0m:load parameters from checkpoint...
I:[36mGRAPHOPT[0m:optimize...
I:[36mGRAPHOPT[0m:freeze...
I:[36mGRAPHOPT[0m:write graph to a tmp file: C:\Users\USER\AppData\Local\Temp\tmp6r314uky
I:[35mVENTILATOR[0m:optimized graph is stored at: C:\Users\USER\AppData\Local\Temp\tmp6r314uky
I:[35mVENTILATOR[0m:bind all sockets
I:[35mVENTILATOR[0m:open 8 ventilator-worker sockets
I:[35mVENTILATOR[0m:start the sink
I:[32mSINK[0m:ready
I:[35mVENTILATOR[0m:get devices
W:[35mVENTILATOR[0m:only 1 out of 1 GPU(s) is available/free, but ""-num_worker=4""
W:[35mVENTILATOR[0m:multiple workers will be allocated to one GPU, may not scale well and may raise out-of-memory
I:[35mVENTILATOR[0m:device map:
                worker  0 -> gpu  0
                worker  1 -> gpu  0
                worker  2 -> gpu  0
                worker  3 -> gpu  0
I:[33mWORKER-3[0m:use device gpu: 0, load graph from C:\Users\USER\AppData\Local\Temp\tmp6r314uky
I:[33mWORKER-2[0m:use device gpu: 0, load graph from C:\Users\USER\AppData\Local\Temp\tmp6r314uky
I:[33mWORKER-1[0m:use device gpu: 0, load graph from C:\Users\USER\AppData\Local\Temp\tmp6r314uky
I:[33mWORKER-0[0m:use device gpu: 0, load graph from C:\Users\USER\AppData\Local\Temp\tmp6r314uky
I:[35mVENTILATOR[0m:new config request        req id: 1       client: b'3045f5d8-d0c9-4bac-8058-e2c3cd4e4b9c'
I:[32mSINK[0m:send config     client b'3045f5d8-d0c9-4bac-8058-e2c3cd4e4b9c'
I:[35mVENTILATOR[0m:new encode request        req id: 2       size: 3 client: b'3045f5d8-d0c9-4bac-8058-e2c3cd4e4b9c'
I:[32mSINK[0m:job register    size: 3 job id: b'3045f5d8-d0c9-4bac-8058-e2c3cd4e4b9c#2'
```",thanks quick response usage value false false false none none false port false none verbose false false freeze optimize export graph could take model build graph warning module included information please see depend functionality listed please file issue load optimize freeze write graph file graph bind open start sink ready get multiple one may scale well may raise device map worker worker worker worker use device load graph use device load graph use device load graph use device load graph new request id client send client new encode request id size client job register size job id,issue,positive,negative,negative,negative,negative,negative
476924898,could you show the server log?,could show server log,issue,negative,neutral,neutral,neutral,neutral,neutral
476709474,"I had the same error. I had to put:
`bert-serving-start -model_dir tmp/english_L-12_H-768_A-12/ -num_worker=4 `
instead of
`bert-serving-start -model_dir /tmp/english_L-12_H-768_A-12/ -num_worker=4 `
as suggested in the document.",error put instead document,issue,negative,neutral,neutral,neutral,neutral,neutral
476675964,"Pretty sure it's not, but would love that addition",pretty sure would love addition,issue,positive,positive,positive,positive,positive,positive
476519523,"@chenbaicheng Thanks for your reply. Now I found that I can't use cased_L-24_H-1024_A-16 on service, but cased_L-12_H-768_A-12 and chiness_L-12_H-768_A-12 are both ok via CPU. I just wonder that my 16g RAM  is not enough. Thanks.",thanks reply found ca use service via wonder ram enough thanks,issue,positive,positive,positive,positive,positive,positive
476514906,"@yesxiaoyu   only use aliyun cloud server will display this question ,  if  user myself computer even use cpu is not  disappear this problem ， sorry    I have not solution this problem   ",use cloud server display question user computer even use disappear problem sorry solution problem,issue,negative,negative,negative,negative,negative,negative
476437049,"dashboard should run separately. just [download everything here](https://github.com/hanxiao/bert-as-service/tree/master/plugin/dashboard) and open index.html, fill in the ip and port.",dashboard run separately everything open fill port,issue,negative,neutral,neutral,neutral,neutral,neutral
476388558,related - Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence https://github.com/HSLCY/ABSA-BERT-pair,related sentiment analysis via auxiliary sentence,issue,negative,neutral,neutral,neutral,neutral,neutral
476388386,"this is out of scope - https://github.com/hanxiao/bert-as-service/issues/132

you may find some help here - https://github.com/johndpope/ABSA-BERT-pair",scope may find help,issue,negative,neutral,neutral,neutral,neutral,neutral
476330792,Could you please explain what you replaced the path with? I completed my path for the model with /Users/USERNAME/model/ but still get this same error.,could please explain path path model still get error,issue,negative,neutral,neutral,neutral,neutral,neutral
475866037,"I am still having this issue, even after setting prefetch_size to 0 and using latest version of Bert.

Bert 1.8.6
Python 3.6.8
Ubuntu 16.04, cpu only

Below is the server log

--------------------------------

```
$ bert-serving-start -model_dir ./models/cased_L-24_H-1024_A-16 -num_worker=2 -cpu -prefetch_size 0
usage: /home/sphatik/miniconda3/envs/textmining/bin/bert-serving-start -model_dir ./models/cased_L-24_H-1024_A-16 -num_worker=2 -cpu -prefetch_size 0
                 ARG   VALUE
__________________________________________________
           ckpt_name = bert_model.ckpt
         config_name = bert_config.json
                cors = *
                 cpu = True
          device_map = []
  fixed_embed_length = False
                fp16 = False
 gpu_memory_fraction = 0.5
       graph_tmp_dir = None
    http_max_connect = 10
           http_port = None
        mask_cls_sep = False
      max_batch_size = 256
         max_seq_len = 25
           model_dir = ./models/cased_L-24_H-1024_A-16
          num_worker = 2
       pooling_layer = [-2]
    pooling_strategy = REDUCE_MEAN
                port = 5555
            port_out = 5556
       prefetch_size = 0
 priority_batch_size = 16
show_tokens_to_client = False
     tuned_model_dir = None
             verbose = False
                 xla = False

I:VENTILATOR:[__i:__i: 66]:freeze, optimize and export graph, could take a while...
I:GRAPHOPT:[gra:opt: 52]:model config: ./models/cased_L-24_H-1024_A-16/bert_config.json
I:GRAPHOPT:[gra:opt: 55]:checkpoint: ./models/cased_L-24_H-1024_A-16/bert_model.ckpt
I:GRAPHOPT:[gra:opt: 59]:build graph...
I:GRAPHOPT:[gra:opt:128]:load parameters from checkpoint...
I:GRAPHOPT:[gra:opt:132]:optimize...
I:GRAPHOPT:[gra:opt:140]:freeze...
I:GRAPHOPT:[gra:opt:145]:write graph to a tmp file: /tmp/tmpsn_ot94b
I:VENTILATOR:[__i:__i: 74]:optimized graph is stored at: /tmp/tmpsn_ot94b
I:VENTILATOR:[__i:_ru:118]:bind all sockets
I:VENTILATOR:[__i:_ru:122]:open 8 ventilator-worker sockets
I:VENTILATOR:[__i:_ru:125]:start the sink
I:SINK:[__i:_ru:284]:ready
I:VENTILATOR:[__i:_ge:202]:get devices
I:VENTILATOR:[__i:_ge:235]:device map: 
		worker  0 -> cpu
		worker  1 -> cpu
I:WORKER-0:[__i:_ru:492]:use device cpu, load graph from /tmp/tmpsn_ot94b
I:WORKER-1:[__i:_ru:492]:use device cpu, load graph from /tmp/tmpsn_ot94b
I:VENTILATOR:[__i:_ru:176]:new encode request	req id: 3	size: 1	client: b'688a67a7-4a06-4430-9b50-b77d2def7744'
I:SINK:[__i:_ru:331]:job register	size: 1	job id: b'688a67a7-4a06-4430-9b50-b77d2def7744#3'

```

-------------------------------------------------------------------------------------
Client side:

```
>>> from bert_serving.client import BertClient
>>> bc = BertClient()
>>> bc.encode(['hello there friend'])

```",still issue even setting latest version python server log usage value true false false none none false port false none verbose false false ventilator freeze optimize export graph could take gra opt model gra opt gra opt build graph gra opt load gra opt optimize gra opt freeze gra opt write graph file ventilator graph ventilator bind ventilator open ventilator start sink sink ready ventilator get ventilator device map worker worker use device load graph use device load graph ventilator new encode request id size client sink job register size job id client side import friend,issue,positive,negative,negative,negative,negative,negative
475838215,"
I deal it by using following command:
`sudo nvidia-smi -pm 1`

[reference this issue #254](https://github.com/hanxiao/bert-as-service/issues/254#issuecomment-473482746)",deal following command reference issue,issue,negative,neutral,neutral,neutral,neutral,neutral
475668341,I ran into something similar with #279. It was a mis-match of tensorflow version and cuda sdk version. ,ran something similar version version,issue,negative,neutral,neutral,neutral,neutral,neutral
475541906,"The same question occured, when I used cpu. Could you please tell me what's your solution, now.",question used could please tell solution,issue,positive,neutral,neutral,neutral,neutral,neutral
474731846,"1.  nvidia-smi -l 1 Showed its utilized 99% GPU and 100% RAM
2. Thanx!
3. How to calculate nextSentence label from output?",ram calculate label output,issue,negative,neutral,neutral,neutral,neutral,neutral
474711532,"fyi, this issue has been fixed in #286 and the new feature is available since 1.8.6. Please do
```bash
pip install -U bert-serving-server bert-serving-client
```
for the update.

To shutdown a server, please do `bert-serving-terminate --help` and [check README](https://github.com/hanxiao/bert-as-service#starting-bertserver-from-python) for details.",issue fixed new feature available since please bash pip install update shutdown server please help check,issue,positive,positive,positive,positive,positive,positive
474649510,"1. I'm afraid your GPU is not utilized. So please read the server log, check the GPU usage of `nvidia-smi`.
2. Set `-max_seq_len=NONE` instead of 512",afraid please read server log check usage set instead,issue,negative,negative,negative,negative,negative,negative
474649023,"if you read closely to the log, you will see the upper bound of `max_seq_len` in Google's pretrained BERT is 512. Thus, it makes no sense to use >512

Besides, I would suggest to use `-max_seq_len=NONE`, which can nearly always speedup the inference.",read closely log see upper bound thus sense use besides would suggest use nearly always inference,issue,negative,positive,neutral,neutral,positive,positive
474377844,Found the cause. Mistake in my own code. Closing the issue.,found cause mistake code issue,issue,negative,neutral,neutral,neutral,neutral,neutral
474358712,"You can encode sentences using BERT embeddings and use custom model to solve your task. 
I did text classification you can check here: [Text classification using BERT embeddings](https://github.com/rameshjesswani/NLP-/blob/master/dl_models/bert_imdb_lstm.ipynb)",encode use custom model solve task text classification check text classification,issue,negative,neutral,neutral,neutral,neutral,neutral
474183718,"Running with the `-cpu` flag produces normal embedding results. 
I'm considering a possibility that tensorflow 1.13.1 does not work with CUDA 10.1, just 10.0. ",running flag normal considering possibility work,issue,negative,positive,positive,positive,positive,positive
473613724,it seems there may be some problems with the latest tensorflow version. i downgrade  tensorflow-gpu==1.12.0 to fix the problem.,may latest version downgrade fix problem,issue,negative,positive,positive,positive,positive,positive
473485667,"@danilo-augusto Wow, thanks a lot for the detailed diagnosis. Really appreciated!

#238 ",wow thanks lot detailed diagnosis really,issue,positive,positive,positive,positive,positive,positive
473482746,"I stumbled on this issue almost an hour ago, and sadly none of the solutions above worked for me.

So I started diagnosing the problem on my own and here is the fix I found, as well as the explanation to it (hopefully it will help other people in my situation). 😊

It really was the root cause in my case, since undoing the fix caused the server to fallback to CPU, and redoing it re-enabled GPU usage by the server each time.

### TL;DR

`sudo nvidia-smi -pm 1`

### Low-level stuff

The NVIDIA kernel driver has (among many other options) two modes: persistent and non-persistent.

Roughly speaking this comes down to a trade-off: **non-persistent mode** means the GPU will try to remain **idle as long as possible** (to save battery etc.) and it will fire up on connections (loading the driver application, triggering ECC scrubbing and so on), which can take many seconds.

On the other hand, the **persistent mode** will trigger it all once and keep the driver application loaded and ready for connections (at the expense of greater battery usage).
[cf. [driver-persistence](https://docs.nvidia.com/deploy/driver-persistence/index.html)]

This also explains the strangely high volatile GPU-Util in ""nvidia-smi"" (for me it was consistently 90-99%) in non-persistent mode, even with no other programs using the GPU (!): the program `nvidia-smi` itself is querying the GPU, hence ""activating"" it and causing the spikes aforementioned. By the way, this was the event that caught my attention and led to this solution.
It's funny how this reminded me of the [observer effect](https://en.wikipedia.org/wiki/Observer_effect_(physics)) in physics: the observation itself is changing the phenomenon, but I digress...
[reference: [this discussion](https://devtalk.nvidia.com/default/topic/539632/k20-with-high-utilization-but-no-compute-processes-/)]

So I thought: doesn't the BERT server check for GPU availability before allocation? If it does, and if the GPU is idle (i.e. if the driver is configured not to persist), it will trigger the driver application to wake up and the GPU to spike like crazy (for some seconds), and by doing this, **the GPU makes itself unavailable** to clients demanding a **minimum level of availability**.
And indeed, [this line of code](https://github.com/hanxiao/bert-as-service/blob/9e8824e830bb25fd16304c804244a64629a79b23/server/bert_serving/server/__init__.py#L196) requires the GPU to have a maximum load of 90% (as well as a maximum memory usage of 90%). So in the event of temporary unavailability, it will return an empty list (I tested), even when all else seems good (GPU is there, CUDA driver configured, tensorflow-gpu installed etc.).

So the command line in the TL;DR above, which sets persistence mode in the GPUs, was the only fix for me, since it will cause the GPU to be much more stable, and thus bypassing the ""spikes on connection"" that makes it so busy it cannot connect.

Hope it helps!",issue almost hour ago sadly none worked problem fix found well explanation hopefully help people situation really root cause case since undoing fix server fallback usage server time stuff kernel driver among many two persistent roughly speaking come mode try remain idle long possible save battery fire loading driver application take many hand persistent mode trigger keep driver application loaded ready expense greater battery usage also strangely high volatile consistently mode even program querying hence causing way event caught attention led solution funny observer effect physic observation phenomenon digress reference discussion thought server check availability allocation idle driver persist trigger driver application wake spike like crazy unavailable demanding minimum level availability indeed line code maximum load well maximum memory usage event temporary unavailability return empty list tested even else good driver command line persistence mode fix since cause much stable thus connection busy connect hope,issue,positive,positive,positive,positive,positive,positive
473210936,"@hanxiao Hi
hmm...
Thanks for your help, makebe a little mistake，

but when I use the following code(100 sentence in the same file, two ways of reading it mentioned above), there are still different between them:

#shape(1,100,1024) and shape(100,1,1024),all should be the first sentence
**numpy.testing.assert_almost_equal(vectors3[0][0], vectors_n[0][0])**

but the result is as follow ,you know they are very similar but not the same, and top 5 or 6 are the same:
(mismatch 77.34375%)
 x: array([-0.1779817,  0.5355914,  0.6215196, ...,  0.0466645, -0.0653478,
       -0.1269192])
 y: array([-0.1779817,  0.5355911,  0.62152  , ...,  0.0466648, -0.0653483,
       -0.1269193])

[CLS]', 'action', 'taken', 'on', 'parliament', ""'"", 's', 'resolutions', ':', 'see', 'minutes', '[SEP]']]
['[CLS]', 'action', 'taken', 'on', 'parliament', ""'"", 's', 'resolutions', ':', 'see', 'minutes', '[SEP]']


so when I have a big text, can I choose the following way to get the embedding of each sentence?

     f3 = open(""a1"")
     f4 = open(""output2.txt"", 'a+')
     line = f3.readline()
     line = line.strip('\n')
        while line:
          vectors = bc.encode([line])
          f4.write(str(vectors))
          line = f3.readline()
         f3.close()
         f4.close()
",hi thanks help little use following code sentence file two way reading still different shape shape first sentence result follow know similar top mismatch array array big text choose following way get sentence open open line line line line line,issue,positive,positive,neutral,neutral,positive,positive
473170582,"@hanxiao 
 When I use numpy.testing.assert_almost_equal, result is as follow:
the two way of reading the file make them different...bu I don't know why

(shapes (1, 2, 1024), (2, 1, 1024) mismatch)
 x: array([[[-0.2231579,  0.3980872,  0.5846128, ...,  0.2056947,
         -0.1711089, -0.2720026],
        [-0.1779817,  0.5355911,  0.62152  , ...,  0.0466648,
         -0.0653483, -0.1269193]]])
 y: array([[[-0.2231579,  0.3980872,  0.5846128, ...,  0.2056947,
         -0.1711089, -0.2720026]],
",use result follow two way reading file make different bu know mismatch array array,issue,negative,neutral,neutral,neutral,neutral,neutral
473159396,"@hanxiao 
   thanks for your help!
   but two way mentioned above read the same content from the same file,  and some part of the result of the following code is as follow..

   -0.2231578677892685, 0.39808693528175354, 0.5846118330955505, 0.37325534224510193, 0.02612251788377762, 0.013674980960786343, -0.15880845487117767, -0.04073430597782135, 0.1628953218460083, -0.41229990124702454, 0.2838746905326843, -0.03617042675614357, -0.02640213631093502, -0.03176272287964821, 0.5393915772438049, -0.3016863763332367, -0.33172035217285156, -0.14470969140529633, -0.0805496796965599, 0.35383638739585876, -0.505082905292511, 0.09550865739583969, 0.11147267371416092, 0.17666691541671753, 1.0787615776062012, 0.12268967181444168, -0.26354333758354187, -0.5836504697799683, 0.2230411320924759, -0.1913362741470337, 0.13490252196788788, -0.42437076568603516, -0.05619550868868828, -0.618446409702301, 0.03052470088005066, -0.06663989275693893, 0.356494665145874, -0.03170517459511757, -0.5458877086639404, 0.061470624059438705, 0.02722652442753315, -0.1629238873720169, -0.2165013700723648, 0.13430343568325043,  

[-0.22315794229507446, 0.39808717370033264, 0.5846127867698669, 0.3732558488845825, 0.02612287364900112, 0.013675312511622906, -0.15880845487117767, -0.040734659880399704, 0.16289560496807098, -0.4122997224330902, 0.28387463092803955, -0.03617057576775551, -0.026402465999126434, -0.03176291659474373, 0.5393921136856079, -0.30168652534484863, -0.33172106742858887, -0.14471007883548737, -0.08054981380701065, 0.35383662581443787, -0.5050831437110901, 0.09550920128822327, 0.11147237569093704, 0.17666731774806976, 1.0787614583969116, 0.12268977612257004, -0.2635434567928314, -0.583650529384613, 0.2230411320924759, -0.19133596122264862, 0.13490261137485504, -0.42437076568603516, -0.05619519576430321, -0.6184461712837219, 0.030524643138051033, -0.06663913279771805, 0.35649406909942627, -0.031704846769571304, -0.5458874106407166, 0.06147034093737602, 0.027226099744439125, -0.1629236787557602, -0.21650123596191406, 0.13430272042751312, 

     import sys
     import time

     from bert_serving.client import BertClient

     if __name__ == '__main__':
     bc = BertClient(port=int(sys.argv[1]), port_out=int(sys.argv[2]), show_server_config=True, output_fmt='list')
    

     with open('a1') as fp:
        data = [v.strip('\n') for v in fp]   

    vectors = bc.encode(data)
    f2 = open(""output1.txt"", 'a+')
    f2.write(str(vectors))
    f2.close()

    f3 = open(""a1"")
    f4 = open(""output2.txt"", 'a+')
    line = f3.readline()
    #print(line)
    line = line.strip('\n')
    print([line])
    while line:
        vectors = bc.encode([line])
        f4.write(str(vectors))
        line = f3.readline()
    f3.close()
    f4.close()",thanks help two way read content file part result following code follow import import time import open data data open open open line print line line print line line line line,issue,positive,positive,neutral,neutral,positive,positive
473159077,"a minimum example can be found here
https://github.com/hanxiao/bert-as-service/issues/267#issuecomment-473158634

```python
import numpy.testing
from bert_serving.client import BertClient

with BertClient() as bc:
    numpy.testing.assert_almost_equal(bc.encode(['如何查询社保']), bc.encode(['如何查询社保']))

# no exception is raised
```",minimum example found python import import exception raised,issue,negative,neutral,neutral,neutral,neutral,neutral
473158634,"sorry, can't reproduce.

```python
import numpy.testing
from bert_serving.client import BertClient

with BertClient() as bc:
    numpy.testing.assert_almost_equal(bc.encode(['如何查询社保']), bc.encode(['如何查询社保']))

# no exception is raised
```

",sorry ca reproduce python import import exception raised,issue,negative,negative,negative,negative,negative,negative
473157530,"impossible. the embedding is deterministic. must be some inconsistency on your input.

1. you should *not* use `=` to assert if two vectors are equal or not, see https://github.com/hanxiao/bert-as-service/issues/167#issuecomment-450928500

2. the best way to debug is to enable `-show_tokens_to_client` on the server and check the tokenization returned. see https://github.com/hanxiao/bert-as-service/#using-your-own-tokenizer",impossible deterministic must inconsistency input use assert two equal see best way enable server check returned see,issue,positive,positive,positive,positive,positive,positive
472759162,"Not Found
The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.",found found server manually please check spelling try,issue,negative,neutral,neutral,neutral,neutral,neutral
472742602,"Firstly, thank you for great job! I just have a question about the benchmark: in `Speed wrt. pooling_layer` part, I don't quiet understand why the depth of the pooling layer can affect the speed? Thanks again!",firstly thank great job question speed part quiet understand depth layer affect speed thanks,issue,positive,positive,positive,positive,positive,positive
472691447,I personally don't use conda and not familiar with its eco-system. All I'm sure is that this package can be installed smoothly via `pip`. Maybe related: https://stackoverflow.com/questions/41060382/using-pip-to-install-packages-to-anaconda-environment,personally use familiar sure package smoothly via pip maybe related,issue,negative,positive,positive,positive,positive,positive
472407071,"By tuning `-pooling_layer`, you may use [lower level representation as well to speedup the inference](https://github.com/hanxiao/bert-as-service/#speed-wrt-pooling_layer).",tuning may use lower level representation well inference,issue,negative,neutral,neutral,neutral,neutral,neutral
472406056,"First of all, please note that `bert-as-service` is not designed for edge computing. The `BertServer` is computational intensive and therefore should rest on a more powerful machine rather than an edge device (e.g. raspberry pi). On the other hand, `BertClient` is very lightweight and can be deployed on less powerful machine. I can totally understand the need and interest of putting the server side on the edge device, but again [`bert-as-service` isn't designed for such case](https://hanxiao.github.io/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ#Serving-with-low-latency).

`bert-as-service` is optimized for scalability and therefore has reduced the memory footprint significantly. You can further reduce the memory consumption and speed up the inference [by using `-fp16`](https://bert-as-service.readthedocs.io/en/latest/source/server.html#Serving%20Configs), provide that your machine supports half precision instruction (otherwise you will get extremely slow result).

Finally `bert-as-service` implements model compression on the engineering-level but not algorithmic-level.",first please note designed edge computational intensive therefore rest powerful machine rather edge device raspberry pi hand lightweight le powerful machine totally understand need interest server side edge device designed case therefore reduced memory footprint significantly reduce memory consumption speed inference provide machine half precision instruction otherwise get extremely slow result finally model compression,issue,positive,positive,neutral,neutral,positive,positive
471803040,"I have the same problem using Amazon's Deep Learning AMI. 

From the same environment, I run: 
```
import tensorflow as tf
with tf.device('/gpu:0'):
    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
    c = tf.matmul(a, b)

with tf.Session() as sess:
    print (sess.run(c))
```

The graph computes and I see a python3 process appearing in nvidia-smi

However, when I run 

`
bert-serving-start -model_dir multi_cased_L-12_H-768_A-12 -max_seq_len 64
`

The server runs, but on CPU only (same observations as alsuhr-c above wrt nvidia-smi and top)
The server mentions: 
I:VENTILATOR:[__i:_ge:221]:device map:
                worker  0 -> gpu  0

bertbert-serving-start -version shows: 
bert-serving-start 1.8.3
",problem deep learning ami environment run import sess print graph see python process however run server top server ventilator device map worker,issue,negative,positive,positive,positive,positive,positive
469674885,"I have the same question and I update my nvidia driver from version 390 to 396, then it runs on my GPU.",question update driver version,issue,negative,neutral,neutral,neutral,neutral,neutral
469156835,"> @hanxiao 
clone this repo and run in linux

lsb_release -a
Description:	Ubuntu 17.10
Release:	17.10
Codename:	artful
Python 3.6.3 (default, Oct 3 2017, 21:45:48)
[GCC 7.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
import tensorflow as tf
tf.version
'1.8.0'

and using this command to start the server:
bert-serving-start  -model_dir /home/jack/p/chinese_L-12_H-768_A-12 -num_worker=4
Then this issue shows up:
bert-serving-start: command not found

i find this cls in this path 
~/.local/bin$ ls
bert-serving-benchmark  bert-serving-start 
cd to here and updata tensorflow and run it ，it work！

Why can't I run here /bert-as-service$
thank you",clone run description release artful python default type help copyright license information import command start server issue command found find path run ca run thank,issue,positive,neutral,neutral,neutral,neutral,neutral
469155643,"out of scope of this repo, please make a PR in another repo",scope please make another,issue,negative,neutral,neutral,neutral,neutral,neutral
469150284,"without any information about version and OS you are using, there's little that I can help with.

please refer to https://github.com/hanxiao/bert-as-service/issues/194 ",without information version o little help please refer,issue,positive,negative,negative,negative,negative,negative
469127754,"1. CLI `bert-serving-start` works fine, provided that you install it correctly via `pip`. This is validated by users on Linux, Windows and Mac. If not, please search history issues for help.

2. Theoretically, the optimal value is 2 if you have two GPUs. In practice, you can use any many workers as you want until you get OOM (each worker takes 700MB (idle)-1.6G GPU memory). In that case, multiple workers will be stacked on the same GPU. As a consequence, one may observe marginal speedup that is slightly larger than 2. The following example shows a case where I allocate 16 workers on the same GPU and I can expect a marginal speedup around 1~1.5.
![image](https://user-images.githubusercontent.com/2041322/53713002-28a4b980-3e84-11e9-8d08-ae4835e96d3a.png)
See `-device_map` in `bert-serving-start --help` or README.md or issues for more details.

3. You should see `I:WORKER-0:[__i:gen:506]:ready and listening!` when the server is ready.
",work fine provided install correctly via pip mac please search history help theoretically optimal value two practice use many want get worker idle memory case multiple consequence one may observe marginal slightly following example case allocate expect marginal around image see help see gen ready listening server ready,issue,positive,positive,positive,positive,positive,positive
469125587,"@monajalal and does `/scratch2/NAACL2018/test/english_L-12_H-768_A-12/bert_config.json` exist? could you do `ls` and double check?

I just tried minutes ago, works fine:
```bash
hanxiao:~/data# wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip -O temp.zip; unzip temp.zip; rm temp.zip
--2019-03-04 13:37:47--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip
Connecting to 10.197.1.187:52107... connected.
Proxy request sent, awaiting response... 200 OK
Length: 1247797031 (1.2G) [application/zip]
Saving to: ‘temp.zip’

100%[=================================================================================================>] 1,247,797,031  109MB/s   in 11s

2019-03-04 13:37:59 (106 MB/s) - ‘temp.zip’ saved [1247797031/1247797031]

Archive:  temp.zip
   creating: uncased_L-24_H-1024_A-16/
  inflating: uncased_L-24_H-1024_A-16/bert_model.ckpt.meta
  inflating: uncased_L-24_H-1024_A-16/bert_model.ckpt.data-00000-of-00001
  inflating: uncased_L-24_H-1024_A-16/vocab.txt
  inflating: uncased_L-24_H-1024_A-16/bert_model.ckpt.index
  inflating: uncased_L-24_H-1024_A-16/bert_config.json
hanxiao:~/data# bert-serving-start -model_dir uncased_L-24_H-1024_A-16/
usage: /data1/cips/.pyenv/versions/3.6.4/bin/bert-serving-start -model_dir uncased_L-24_H-1024_A-16/
                 ARG   VALUE
__________________________________________________
           ckpt_name = bert_model.ckpt
         config_name = bert_config.json
                cors = *
                 cpu = False
          device_map = []
  fixed_embed_length = False
                fp16 = False
 gpu_memory_fraction = 0.5
       graph_tmp_dir = None
    http_max_connect = 10
           http_port = None
        mask_cls_sep = False
      max_batch_size = 256
         max_seq_len = 25
           model_dir = uncased_L-24_H-1024_A-16/
          num_worker = 1
       pooling_layer = [-2]
    pooling_strategy = REDUCE_MEAN
                port = 5555
            port_out = 5556
       prefetch_size = 10
 priority_batch_size = 16
show_tokens_to_client = False
     tuned_model_dir = None
             verbose = False
                 xla = False

I:VENTILATOR:[__i:__i: 66]:freeze, optimize and export graph, could take a while...
I:GRAPHOPT:[gra:opt: 52]:model config: uncased_L-24_H-1024_A-16/bert_config.json
I:GRAPHOPT:[gra:opt: 55]:checkpoint: uncased_L-24_H-1024_A-16/bert_model.ckpt
I:GRAPHOPT:[gra:opt: 59]:build graph...
I:GRAPHOPT:[gra:opt:128]:load parameters from checkpoint...
I:GRAPHOPT:[gra:opt:132]:optimize...
I:GRAPHOPT:[gra:opt:140]:freeze...
I:GRAPHOPT:[gra:opt:145]:write graph to a tmp file: /data1/cips/tmp/tmp24sqz2cd
I:VENTILATOR:[__i:__i: 74]:optimized graph is stored at: /data1/cips/tmp/tmp24sqz2cd
I:VENTILATOR:[__i:_ru:106]:bind all sockets
I:VENTILATOR:[__i:_ru:110]:open 8 ventilator-worker sockets
I:VENTILATOR:[__i:_ru:113]:start the sink
I:VENTILATOR:[__i:_ge:188]:get devices
I:SINK:[__i:_ru:270]:ready
I:VENTILATOR:[__i:_ge:221]:device map:
		worker  0 -> gpu  7
I:WORKER-0:[__i:_ru:478]:use device gpu: 7, load graph from /data1/cips/tmp/tmp24sqz2cd
I:WORKER-0:[__i:gen:506]:ready and listening!
```
",exist could double check tried ago work fine bash connected proxy request sent response length saving saved archive usage value false false false none none false port false none verbose false false ventilator freeze optimize export graph could take gra opt model gra opt gra opt build graph gra opt load gra opt optimize gra opt freeze gra opt write graph file ventilator graph ventilator bind ventilator open ventilator start sink ventilator get sink ready ventilator device map worker use device load graph gen ready listening,issue,positive,negative,negative,negative,negative,negative
469125570,"@hanxiao nvm that was a stupid path problem.

```
[jalal@goku test]$ python start-bert-as-service.py -model_dir /scratch2/NAACL2018/test/english_L-12_H-768_A-12/uncased_L-24_H-1024_A-16 -num_worker=4
/scratch/sjn-p3/anaconda/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
usage: start-bert-as-service.py -model_dir /scratch2/NAACL2018/test/english_L-12_H-768_A-12/uncased_L-24_H-1024_A-16 -num_worker=4
                 ARG   VALUE
__________________________________________________
           ckpt_name = bert_model.ckpt
         config_name = bert_config.json
                cors = *
                 cpu = False
          device_map = []
  fixed_embed_length = False
                fp16 = False
 gpu_memory_fraction = 0.5
       graph_tmp_dir = None
    http_max_connect = 10
           http_port = None
        mask_cls_sep = False
      max_batch_size = 256
         max_seq_len = 25
           model_dir = /scratch2/NAACL2018/test/english_L-12_H-768_A-12/uncased_L-24_H-1024_A-16
          num_worker = 4
       pooling_layer = [-2]
    pooling_strategy = REDUCE_MEAN
                port = 5555
            port_out = 5556
       prefetch_size = 10
 priority_batch_size = 16
show_tokens_to_client = False
     tuned_model_dir = None
             verbose = False
                 xla = False

I:VENTILATOR:[__i:__i: 66]:freeze, optimize and export graph, could take a while...
I:GRAPHOPT:[gra:opt: 52]:model config: /scratch2/NAACL2018/test/english_L-12_H-768_A-12/uncased_L-24_H-1024_A-16/bert_config.json
I:GRAPHOPT:[gra:opt: 55]:checkpoint: /scratch2/NAACL2018/test/english_L-12_H-768_A-12/uncased_L-24_H-1024_A-16/bert_model.ckpt
I:GRAPHOPT:[gra:opt: 59]:build graph...
```

A couple of questions for you:
1. so `bert-serving-start` would not work and I should just stick to `python start-bert-as-service.py`?
2. what is the max number of worker I could use if I have two 1080Ti GPUs?
3. When do I know it is ready for running the client? for example right now I am seeing the following:
```
I:WORKER-0:[__i:_ru:478]:use device gpu: 1, load graph from /tmp/tmp2ggnx73k
I:WORKER-1:[__i:_ru:478]:use device gpu: 0, load graph from /tmp/tmp2ggnx73k
I:WORKER-2:[__i:_ru:478]:use device gpu: 1, load graph from /tmp/tmp2ggnx73k
I:WORKER-3:[__i:_ru:478]:use device gpu: 0, load graph from /tmp/tmp2ggnx73k
```

",stupid path problem test python conversion second argument float future float import usage value false false false none none false port false none verbose false false ventilator freeze optimize export graph could take gra opt model gra opt gra opt build graph couple would work stick python number worker could use two ti know ready running client example right seeing following use device load graph use device load graph use device load graph use device load graph,issue,negative,negative,negative,negative,negative,negative
469124726,"@hanxiao downloaded the uncased large model from here, where else should I have downloaded it? Could you please guide? 
`https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip`",uncased large model else could please guide,issue,negative,positive,positive,positive,positive,positive
469124491,"@hanxiao thanks a lot for the prompt response:

```
[jalal@goku test]$ bert-serving-start -model_dir /scratch2/NAACL2018/test/english_L-12_H-768_A-12/ -num_worker=4 
bash: bert-serving-start: command not found...
[jalal@goku test]$ python start-bert-as-service.py -model_dir /scratch2/NAACL2018/test/english_L-12_H-768_A-12/ -num_worker=4
/scratch/sjn-p3/anaconda/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
usage: start-bert-as-service.py -model_dir /scratch2/NAACL2018/test/english_L-12_H-768_A-12/ -num_worker=4
                 ARG   VALUE
__________________________________________________
           ckpt_name = bert_model.ckpt
         config_name = bert_config.json
                cors = *
                 cpu = False
          device_map = []
  fixed_embed_length = False
                fp16 = False
 gpu_memory_fraction = 0.5
       graph_tmp_dir = None
    http_max_connect = 10
           http_port = None
        mask_cls_sep = False
      max_batch_size = 256
         max_seq_len = 25
           model_dir = /scratch2/NAACL2018/test/english_L-12_H-768_A-12/
          num_worker = 4
       pooling_layer = [-2]
    pooling_strategy = REDUCE_MEAN
                port = 5555
            port_out = 5556
       prefetch_size = 10
 priority_batch_size = 16
show_tokens_to_client = False
     tuned_model_dir = None
             verbose = False
                 xla = False

I:VENTILATOR:[__i:__i: 66]:freeze, optimize and export graph, could take a while...
I:GRAPHOPT:[gra:opt: 52]:model config: /scratch2/NAACL2018/test/english_L-12_H-768_A-12/bert_config.json
I:GRAPHOPT:[gra:opt: 55]:checkpoint: /scratch2/NAACL2018/test/english_L-12_H-768_A-12/bert_model.ckpt
E:GRAPHOPT:[gra:opt:150]:fail to optimize the graph!
Traceback (most recent call last):
  File ""/home/grad3/jalal/.local/lib/python3.6/site-packages/bert_serving/server/graph.py"", line 57, in optimize_graph
    bert_config = modeling.BertConfig.from_dict(json.load(f))
  File ""/scratch/sjn-p3/anaconda/anaconda3/lib/python3.6/json/__init__.py"", line 296, in load
    return loads(fp.read(),
  File ""/home/grad3/jalal/.local/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py"", line 125, in read
    self._preread_check()
  File ""/home/grad3/jalal/.local/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py"", line 85, in _preread_check
    compat.as_bytes(self.__name), 1024 * 512, status)
  File ""/home/grad3/jalal/.local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 526, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: /scratch2/NAACL2018/test/english_L-12_H-768_A-12/bert_config.json; No such file or directory
Traceback (most recent call last):
  File ""start-bert-as-service.py"", line 9, in <module>
    server = BertServer(args)
  File ""/home/grad3/jalal/.local/lib/python3.6/site-packages/bert_serving/server/__init__.py"", line 70, in __init__
    self.graph_path, self.bert_config = pool.apply(optimize_graph, (self.args,))
TypeError: 'NoneType' object is not iterable
```
",thanks lot prompt response test bash command found test python conversion second argument float future float import usage value false false false none none false port false none verbose false false ventilator freeze optimize export graph could take gra opt model gra opt gra opt fail optimize graph recent call last file line file line load return file line read file line status file line file directory recent call last file line module server file line object iterable,issue,positive,negative,negative,negative,negative,negative
469124169,"@monajalal in your log
```bash
tensorflow.python.framework.errors_impl.NotFoundError: english_L-12_H-768_A-12/bert_config.json; No such file or directory
```
apparently, your model path is wrong. Try the absolute path if the relative one doesn't work for you.",log bash file directory apparently model path wrong try absolute path relative one work,issue,negative,negative,neutral,neutral,negative,negative
469124112,"I get the same error on CentOS 7:

```
[jalal@goku test]$ bert-serving-start -model_dir english_L-12_H-768_A-12/ -num_worker=4 
bash: bert-serving-start: command not found...

$ lsb_release -a
LSB Version:	:core-4.1-amd64:core-4.1-noarch
Distributor ID:	CentOS
Description:	CentOS Linux release 7.6.1810 (Core) 
Release:	7.6.1810
Codename:	Core
$ uname -a
Linux goku.bu.edu 3.10.0-957.5.1.el7.x86_64 #1 SMP Fri Feb 1 14:54:57 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux

$ python
Python 3.6.4 |Anaconda custom (64-bit)| (default, Jan 16 2018, 18:10:19) 
[GCC 7.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
tf./scratch/sjn-p3/anaconda/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
>>> tf.__version__
'1.11.0'

```",get error test bash command found version distributor id description release core release core python python custom default type help copyright license information import conversion second argument float future float import,issue,negative,neutral,neutral,neutral,neutral,neutral
469123766,"@eggachecat using your method I get the following error:

```
[jalal@goku test]$ python start-bert-as-service.py -model_dir english_L-12_H-768_A-12/ -num_worker=4
/scratch/sjn-p3/anaconda/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
usage: start-bert-as-service.py -model_dir english_L-12_H-768_A-12/ -num_worker=4
                 ARG   VALUE
__________________________________________________
           ckpt_name = bert_model.ckpt
         config_name = bert_config.json
                cors = *
                 cpu = False
          device_map = []
  fixed_embed_length = False
                fp16 = False
 gpu_memory_fraction = 0.5
       graph_tmp_dir = None
    http_max_connect = 10
           http_port = None
        mask_cls_sep = False
      max_batch_size = 256
         max_seq_len = 25
           model_dir = english_L-12_H-768_A-12/
          num_worker = 4
       pooling_layer = [-2]
    pooling_strategy = REDUCE_MEAN
                port = 5555
            port_out = 5556
       prefetch_size = 10
 priority_batch_size = 16
show_tokens_to_client = False
     tuned_model_dir = None
             verbose = False
                 xla = False

I:VENTILATOR:[__i:__i: 66]:freeze, optimize and export graph, could take a while...
I:GRAPHOPT:[gra:opt: 52]:model config: english_L-12_H-768_A-12/bert_config.json
I:GRAPHOPT:[gra:opt: 55]:checkpoint: english_L-12_H-768_A-12/bert_model.ckpt
E:GRAPHOPT:[gra:opt:150]:fail to optimize the graph!
Traceback (most recent call last):
  File ""/home/grad3/jalal/.local/lib/python3.6/site-packages/bert_serving/server/graph.py"", line 57, in optimize_graph
    bert_config = modeling.BertConfig.from_dict(json.load(f))
  File ""/scratch/sjn-p3/anaconda/anaconda3/lib/python3.6/json/__init__.py"", line 296, in load
    return loads(fp.read(),
  File ""/home/grad3/jalal/.local/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py"", line 125, in read
    self._preread_check()
  File ""/home/grad3/jalal/.local/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py"", line 85, in _preread_check
    compat.as_bytes(self.__name), 1024 * 512, status)
  File ""/home/grad3/jalal/.local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 526, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: english_L-12_H-768_A-12/bert_config.json; No such file or directory
Traceback (most recent call last):
  File ""start-bert-as-service.py"", line 9, in <module>
    server = BertServer(args)
  File ""/home/grad3/jalal/.local/lib/python3.6/site-packages/bert_serving/server/__init__.py"", line 70, in __init__
    self.graph_path, self.bert_config = pool.apply(optimize_graph, (self.args,))
TypeError: 'NoneType' object is not iterable
```
",method get following error test python conversion second argument float future float import usage value false false false none none false port false none verbose false false ventilator freeze optimize export graph could take gra opt model gra opt gra opt fail optimize graph recent call last file line file line load return file line read file line status file line file directory recent call last file line module server file line object iterable,issue,negative,negative,negative,negative,negative,negative
468848285,"Can report similar issue locally with cpu only version of tensorflow and:
tensorflow 1.13.1
bert-as-a-service 1.8.3 

```
INFO:tensorflow:Calling model_fn.
I:VENTILATOR:[__i:_ru:148]:new config request	req id: 1	client: b'f01b84db-4dd7-423e-b1bc-c9ab46a470df'
I:SINK:[__i:_ru:320]:send config	client b'f01b84db-4dd7-423e-b1bc-c9ab46a470df'
I:VENTILATOR:[__i:_ru:164]:new encode request	req id: 2	size: 3	client: b'f01b84db-4dd7-423e-b1bc-c9ab46a470df'
I:SINK:[__i:_ru:317]:job register	size: 3	job id: b'f01b84db-4dd7-423e-b1bc-c9ab46a470df#2'
```
Nothing seems to happen after this point.

Also max_batch_size 16 did not help",report similar issue locally version calling ventilator new request id client sink send client ventilator new encode request id size client sink job register size job id nothing happen point also help,issue,negative,positive,neutral,neutral,positive,positive
468774043,"Sorry. Solved it . I was using a local path and running. But the program needs the full path of the files .
My apologies and thank you for the amazing tool.",sorry local path running program need full path thank amazing tool,issue,positive,positive,positive,positive,positive,positive
468524356,"Not a bug. It's written in the frontend code. For performance sake and simplicity, sever does not persist history data.

Feel free to change the behavior by yourself. Note that dashboard (and its UX/UI) serves merely as an toy example, maintaining it is not part of my priority work.",bug written code performance sake simplicity sever persist history data feel free change behavior note dashboard merely toy example part priority work,issue,positive,negative,neutral,neutral,negative,negative
468510380,"As an update, I couldn't get it to run on GPU still, but 20,000 sentences only took about 20min in the end on CPU.",update could get run still took min end,issue,negative,neutral,neutral,neutral,neutral,neutral
468507304,"One should not use `namedtuple` as args to start a server. I know I wrote something like that in an old version, but this is depreciated. The visualization script is not updated accordingly I guess.

So first, update the package via `pip install -U bert-serving-client bert-serving-server`

To use BertServer in your code, please do the following:

```python

from bert_serving.server.helper import get_args_parser
args = get_args_parser().parse_args(['-model_dir', 'YOUR_MODEL_PATH_HERE',
                                             '-port', '5555',
                                             '-port_out', '5556',
                                             '-max_seq_len', 'NONE',
                                             '-mask_cls_sep',
                                             '-cpu'])
server = BertServer(args)
server.start()
```

Note that it's basically mirroring the arg-parsing behavior in CLI, so everything in that `.parse_args([])` list should be string, e.g. `['-port', '5555']` not `['-port', 5555]`.",one use start server know wrote something like old version visualization script accordingly guess first update package via pip install use code please following python import server note basically behavior everything list string,issue,positive,positive,positive,positive,positive,positive
468505185,"Thanks for being an awesome user!

Two ways of citing it:
1. `\footnote{https://github.com/hanxiao/bert-as-service}`
2. 
```latex
@misc{xiao2018bertservice,
  title={bert-as-service},
  author={Xiao, Han},
  howpublished={\url{https://github.com/hanxiao/bert-as-service}},
  year={2018}
}
```",thanks awesome user two way latex han,issue,positive,positive,positive,positive,positive,positive
468441996,"It get this same error when I try to spin up the BERT server in python, copying example 7.
```
base = {
    'model_dir': '/data/models/bert/uncased_L-12_H-768_A-12/',
    'num_worker': 1,
    'max_seq_len': 283,
    'pooling_strategy': PoolingStrategy.REDUCE_MEAN,
    'num_repeat': 10,
    'port': 5555,
    'port_out': 5556,
    'client_batch_size': 4096,
    'max_batch_size': 256,
    'num_client': 1,
    'pooling_layer': [-2],
    'gpu_memory_fraction': 0.5,
    'xla': False,
    'cpu': False,
    'verbose': False,
    'device_map': []
}
args = namedtuple('args_namedtuple', ','.join(base.keys()))
for k, v in base.items():
    setattr(args, k, v)
server = BertServer(args)
server.start()
```
><class '__main__.args_namedtuple'>
I:VENTILATOR:[__i:__i: 66]:freeze, optimize and export graph, could take a while...
Traceback (most recent call last):
  File ""./src/encode_with_bert.py"", line 114, in <module>
    server = BertServer(args)
  File ""/usr/local/lib/python3.5/dist-packages/bert_serving/server/__init__.py"", line 70, in __init__
    self.graph_path, self.bert_config = pool.apply(optimize_graph, (self.args,))
  File ""/usr/lib/python3.5/multiprocessing/pool.py"", line 253, in apply
    return self.apply_async(func, args, kwds).get()
  File ""/usr/lib/python3.5/multiprocessing/pool.py"", line 608, in get
    raise self._value
  File ""/usr/lib/python3.5/multiprocessing/pool.py"", line 385, in _handle_tasks
    put(task)
  File ""/usr/lib/python3.5/multiprocessing/connection.py"", line 206, in send
    self._send_bytes(ForkingPickler.dumps(obj))
  File ""/usr/lib/python3.5/multiprocessing/reduction.py"", line 50, in dumps
    cls(buf, protocol).dump(obj)
_pickle.PicklingError: Can't pickle <class '__main__.args_namedtuple'>: attribute lookup args_namedtuple on __main__ failed",get error try spin server python example base false false false server class ventilator freeze optimize export graph could take recent call last file line module server file line file line apply return file line get raise file line put task file line send file line protocol ca pickle class attribute,issue,negative,negative,negative,negative,negative,negative
468174276,"@feiyuchen7 @yuchenlin PCA seems a bad method for reducing dimension. I heard a way to fix it. They say we can connect a dense neural network after the last layer.But I still can't realize it. Maybe I still not understand the BERT construction.If you fix it use this way, please tell me~",bad method reducing dimension way fix say connect dense neural network last still ca realize maybe still understand fix use way please tell,issue,negative,negative,negative,negative,negative,negative
468082272,Hi . I have the same I issue. Can you please share how you fixed it @lyshello123 ?,hi issue please share fixed,issue,positive,positive,neutral,neutral,positive,positive
467692858,"fyi, this issue is fixed in #252, the new feature is available in 1.8.3.

To start a dashboard, make sure you start bert-as-service with `bert-serving-start -http_port xxxx -model_dir ...` and then open [index.html](https://github.com/hanxiao/bert-as-service/blob/master/plugin/dashboard/index.html) in your browser (and of course you need CSS and JS files as well). Change the URL (with port!) in the text box on the top-right. You should see the server statistics immediately.

Play with the javascript to customize the layout or behavior of the dashboard by yourself.",issue fixed new feature available start dashboard make sure start open browser course need well change port text box see server statistic immediately play layout behavior dashboard,issue,positive,positive,positive,positive,positive,positive
466731614,"what if I want to specific [sep] by myself for special tasks.

For example, I would like to make a QA pair become a sentence like `[CLS] question [sep] answer [sep]`. Is this possible?",want specific special example would like make pair become sentence like question answer possible,issue,positive,positive,positive,positive,positive,positive
466297087,"> What do you mean ""transform""?
> 
> You can use `fp16` out-of-the-box in `bert-as-service`. [No extra preprocessing is needed.](https://github.com/hanxiao/bert-as-service#speed-wrt--fp16-and--xla)

Thank you, I found that float32 variables are freezed to float16 in `graph.py` when using `fp16` mode, can this `graph.py`  work when the network changed?",mean transform use extra thank found float float mode work network,issue,negative,negative,negative,negative,negative,negative
466294591,"Please, do not push. This is not how OSS works. I also have my own daily agenda and priority. Hope you understand and respect that.

Always remember in an open-source community, everyone (including you) has the freedom to modify the code and works independently.",please push work also daily agenda priority hope understand respect always remember community everyone freedom modify code work independently,issue,positive,neutral,neutral,neutral,neutral,neutral
466293751,"@hanxiao did you finish this work?, hope you can reply me.",finish work hope reply,issue,negative,neutral,neutral,neutral,neutral,neutral
466292331,"What do you mean ""transform""?

You can use `fp16` out-of-the-box in `bert-as-service`. [No extra preprocessing is needed.](https://github.com/hanxiao/bert-as-service#speed-wrt--fp16-and--xla)",mean transform use extra,issue,negative,negative,negative,negative,negative,negative
465824086,"Not on my list, feel free to fork this repo and implement this feature.",list feel free fork implement feature,issue,positive,positive,positive,positive,positive,positive
465823314,"I understand this PR is mainly for improving python2 compatibility of `BertClient`, right?

As in py3, the code is much simpler
```python
class BertClient:
    ...

class MyBertClient(BertClient):
    def __init__(self, **kwargs):
        super().__init__()
```",understand mainly improving python compatibility right code much simpler python class class self super,issue,positive,positive,positive,positive,positive,positive
465461213,"@hanxiao  if you finish this work, please tell me, thx!",finish work please tell,issue,negative,neutral,neutral,neutral,neutral,neutral
465457472,let me check later. The dashboard may not work normally due to the recent update.,let check later dashboard may work normally due recent update,issue,negative,negative,neutral,neutral,negative,negative
463881918,"Basically I had the **same problem**, you don't need to restart, maybe they already fixed something now. simply **kill -9 pid** would solve this problem. However, after two days work of installing & uninstalling, I finally succeeded in running the sentence encoding for once, so hype. 
  
[edit] Hi,  HanXiao, thanks again for the brilliant work. Another interesting fact is that when I run bert-serving-start ... in the ~ directory, it works fine. However, when I run the same command in another folder, say ~/myfolder, the zmq.error.ZMQError file not found error raised. If you had explained this issue somewhere I didn't notice, sorry for it. ",basically problem need restart maybe already fixed something simply kill would solve problem however two day work finally running sentence edit hi thanks brilliant work another interesting fact run directory work fine however run command another folder say file found error raised issue somewhere notice sorry,issue,negative,positive,positive,positive,positive,positive
463513389,"length restriction on the server side can now be waived in 1.8.2. This issue is fixed in #236 and the new feature is available since 1.8.2. Please do 
```bash
pip install bert-serving-client bert-serving-server -U
```
for the update.

You can now set `max_seq_len=NONE` when starting a server. In this case, the `max_seq_len` will be determined by the longest sequence in a batch (or mini-batch if parallelization is activated). That means you can send any sequence shorter than max_position_embeddings (usually 512) defined in `bert.json`.

You may also want to check the new argument `-fixed_embed_length` by `bert-serving-start --help` especially if you intend to use it as ELMo-like embedding.",length restriction server side issue fixed new feature available since please bash pip install update set starting server case determined sequence batch parallelization send sequence shorter usually defined may also want check new argument help especially intend use,issue,negative,positive,neutral,neutral,positive,positive
463511949,"You are partially right. setting a restriction `max_seq_len` on the server side isn't the best option. The reason for doing that was because in Google's BERT implementation `max_seq_len` has to be a python integer (not a tf.tensor and then dynamically determined).

In #236 I've removed such restriction by patching the BERT implementation. You can now set `max_seq_len=NONE` when starting a server. In this case, the `max_seq_len` will be determined by the longest sequence in a batch (or mini-batch if parallelization is activated). That means you can send any sequence shorter than max_position_embeddings (usually 512) defined in `bert.json`.

You may also want to check the new argument `-fixed_embed_length` by `bert-serving-start --help` especially if you intend to use it as ELMo-like embedding.

However, using BERT to encode a long long sentence (> 1000 tokens as you mentioned) is not a good idea, see https://github.com/hanxiao/bert-as-service/issues/232#issuecomment-462762704",partially right setting restriction server side best option reason implementation python integer dynamically determined removed restriction implementation set starting server case determined sequence batch parallelization send sequence shorter usually defined may also want check new argument help especially intend use however encode long long sentence good idea see,issue,positive,positive,positive,positive,positive,positive
463451501,"I wasn't aware of [this branch](https://github.com/thorjohnsen/bert/tree/gpu_optimizations) when implementing `fp16` support. In bert-as-service fp16 support is a basically preprocessing *in the inference stage* (or a postprocessing in the training stage if you like), meaning that all float32 variables are freezed to float16 in `graph.py` before serving. See:
https://github.com/hanxiao/bert-as-service/blob/179356874048338b2374447aa0ae42e3c3578b19/server/bert_serving/server/graph.py#L153",aware branch support support basically inference stage training stage like meaning float float serving see,issue,positive,positive,positive,positive,positive,positive
462762704,"I don't think this is the expected use of BERT. BERT is a network trained at the sentence embedding level, thus the representation of more than one sentence should be pretty inaccurate and the computation needed beyond 512 tokens would be huge (remember, the computation isn't linear to the number of tokens).

There's many strategies for you to try if you want a more accurate paragraph representation, for example:
- do element wise average / max over the sequence of sentence embeddings that compose your paragraphs (no additional training), thus your resulting paragraph embedding will be of the same number of dims than each sentence embedding.
- use a paragraph embedding from weighted sentences if you have any extra information about how each sentence matters to your overall paragraph representation (no additional training)
- use a RNN / CNN downstream layer to get a paragraph embedding, however this requires training on a target label (regression / classification task).",think use network trained sentence level thus representation one sentence pretty inaccurate computation beyond would huge remember computation linear number many try want accurate paragraph representation example element wise average sequence sentence compose additional training thus resulting paragraph number sentence use paragraph weighted extra information sentence overall paragraph representation additional training use downstream layer get paragraph however training target label regression classification task,issue,positive,positive,positive,positive,positive,positive
461802842,"Hi, Currently what I have decided to do is set `max_seq_length` to 1500 and pass multiple sentences as part of a single sequence separated by ` ||| ` but I am encountering 
`ValueError: The seq length (1500) cannot be greater than `max_position_embeddings` (512)`.

This is due to the fact that, in https://github.com/google-research/bert/blob/master/modeling.py#L43
the default value of max_position_embeddings is set to 512. Can you open an option to set this from your library as a cmd argument? I can work on this if you want. ",hi currently decided set pas multiple part single sequence length greater due fact default value set open option set library argument work want,issue,negative,positive,neutral,neutral,positive,positive
461410715,"Are you certain you updated to 1.8.1 the client and server on both sides? This issue is the same as the one I was facing in 1.8.0 which has then been solved.
",certain client server side issue one facing,issue,negative,positive,positive,positive,positive,positive
460040562,"Thanks for the blazing fast reply. I can't reproduce anymore the 10x slow down I experienced before since I moved to 1.8.1 so please disregard my comment.

The fact that we can now get the tokens back in the response opens up new interesting uses of your embedding server, such as NER (like in the paper using last 4 layer feature extraction) or Q&A to name a few applications.

Was wondering what was your point of view regarding suggestion 2) (ability to start with mix strategies)? Right now my work around is to start the server without pooling & reduce as needed on client side - while this is very flexible it puts lots of pressure on networking. Token level embeddings quickly saturate the bandwidth and become the bottleneck.

Also since moving away from 1.7.9 I'm facing multiprocess issues with pyTorch data loader when using several workers but I'll better open a new issue about it.

Thanks again!",thanks blazing fast reply ca reproduce slow experienced since please disregard comment fact get back response new interesting server like paper last layer feature extraction name wondering point view regarding suggestion ability start mix right work around start server without reduce client side flexible lot pressure token level quickly saturate become bottleneck also since moving away facing data loader several better open new issue thanks,issue,positive,positive,positive,positive,positive,positive
460035691,"> 5556

What does port=5555, port_out=5556 mean? Do you have any reference? Thanks in advance!",mean reference thanks advance,issue,negative,negative,neutral,neutral,negative,negative
460014243,"@kramer425 from the code in the traceback, you didn't update properly, it's still the old one.",code update properly still old one,issue,negative,positive,neutral,neutral,positive,positive
459979763,"Still having this issue with current (docker build):
```
Traceback (most recent call last):
  File ""/usr/local/bin/bert-serving-start"", line 7, in <module>
    from bert_serving.server.cli import main
  File ""/usr/local/lib/python3.5/dist-packages/bert_serving/server/__init__.py"", line 286
    logger.error(f'received a wrongly-formatted request (expected 4 frames, got {len(msg)})')
                                                                                           ^
SyntaxError: invalid syntax
```",still issue current docker build recent call last file line module import main file line request got invalid syntax,issue,negative,positive,neutral,neutral,positive,positive
459927073,fixed in #228 and please do `pip install -U` for the update,fixed please pip install update,issue,negative,positive,neutral,neutral,positive,positive
459927005,"Warning issue is fixed in #228 do `pip install -U bert-serving-server bert-serving-client` for the update.

Regarding the speed, no that's not normal. Here is a simple benchmark showing there is not significant overhead with `-show_tokens_to_client`. You may reproduce it by yourself with `example/example1.py`

![image](https://user-images.githubusercontent.com/2041322/52158589-ae421780-26d4-11e9-9a14-55f71dc3c94f.png)


",warning issue fixed pip install update regarding speed normal simple showing significant overhead may reproduce image,issue,negative,positive,positive,positive,positive,positive
459626951,"Tokenization is a very lightweight extra information right? If that is the only extra info it shouldn't impact the sink much, am I mistaken?

However when trying to embed sentences with show_tokens=True the embedding takes forever compared to before. Is it normal?

Also new issue (should I start a new one?) since I moved to 1.8 on server and client: when starting the server **without** the param `-show_tokens_to_client` I get a warning that `show_tokens=True` even though I ask to encode without it as shown here using python console:

Server side (2x P100 GPUs):
```
bert-serving-start -model_dir ~/um/uncased_L-12_H-768_A-12/ -pooling_layer -1 -2 -3 -4 -pooling_strategy NONE -num_worker=2 -max_seq_len=40 -mask_cls_sep -max_batch_size=256 -gpu_memory_fraction=1.0
```

Client side:
```
>>> from bert_serving.client import BertClient
>>> bc = BertClient(ip='10.164.0.5')
>>> encoded = bc.encode(['hello world!', 'thisis it'])
/home/ubuntu/p36-torch100/lib/python3.6/site-packages/bert_serving/client/__init__.py:285: UserWarning: ""show_tokens=True"", but the server does not support showing tokenization info to clients.
here is what you can do:
- start a new server with ""bert-serving-start -show_tokens_to_client ...""
- or, use ""encode(show_tokens=False)""
  warnings.warn('""show_tokens=True"", but the server does not support showing tokenization info to clients.\n'
```

Notes: updated the server & client to 1.8",lightweight extra information right extra impact sink much mistaken however trying embed forever normal also new issue start new one since server client starting server without param get warning even though ask encode without shown python console server side none client side import world server support showing start new server use encode server support showing server client,issue,positive,positive,positive,positive,positive,positive
459577609,"as a follow-up on #226 , it turns out the biggest challenge wasn't API, it's the communication and memory overhead when adding extra information (e.g. tokenization, multiple embeddings), hurting its overall scalability. This becomes very serious when `max_seq_len` and client batch size is high. Hence there are some effort in #226 on refactoring `BertSink` to make it more efficient.",turn biggest challenge communication memory overhead extra information multiple hurting overall becomes serious client batch size high hence effort make efficient,issue,negative,negative,neutral,neutral,negative,negative
459342942,"Hi @ghaddarAbs , I was wondering if you're going to fork and implement this? Maybe we can work on this together; seems there are many such hub ready to use ",hi wondering going fork implement maybe work together many hub ready use,issue,negative,positive,positive,positive,positive,positive
459279419,"fyi, this feature (retrieving tokenization from server) is implemented in #226 and is available since 1.8.0. Please do 
```bash
pip install -U bert-serving-server bert-serving-client
```
for the update. Usage can be found in the tutorial https://github.com/hanxiao/bert-as-service#using-your-own-tokenizer or documentation: https://bert-as-service.readthedocs.io/",feature server available since please bash pip install update usage found tutorial documentation,issue,negative,positive,positive,positive,positive,positive
459085849,"Thanks @ofirnk Your solution worked like a charm! Maybe this needs to be reopened since it gives an impression that gpu is being utilized
",thanks solution worked like charm maybe need since impression,issue,positive,positive,positive,positive,positive,positive
459057356,"Thanks for your answer.

Problem I am having (or I think I am having) is when I use my own tokenizer BERT might have marking some important words as [UNK] since it's not in the dictionary. So I was wondering if it's possible, without any modifications to BERT, tokenize my sentences and receive those tokens from BERT so I can calc tf-idf then also get the corresponding BERT vectors. I will checkout verbose but it's being on the server side is not helping. Would it be a another feature to serve tokenization or some weight algo. like tf idf etc.?

Thanks",thanks answer problem think use might marking important since dictionary wondering possible without receive also get corresponding verbose server side helping would another feature serve weight like thanks,issue,positive,positive,positive,positive,positive,positive
459045985,"In essence, what @ofirnk was the problem. When bert service was installed, it was done prior to doing the source activate tensorflow_p36. Even though the bert service was saying that it was using GPU, in fact it was not. 
Doing the install after doing source activate tensorflow_p36 on a brand new instance shows orders of magnitude speed up, as expected, when using GPU so I consider this closed. Thanks",essence problem service done prior source activate even though service saying fact install source activate brand new instance magnitude speed consider closed thanks,issue,negative,positive,neutral,neutral,positive,positive
458924869,"You probably need to switch to AWS provided conda environment, see the server hello when using `Deep Learning AMI`.
It's probably just running `source activate tensorflow_p36`  then installing pip and bert-as-service",probably need switch provided environment see server hello deep learning ami probably running source activate pip,issue,negative,neutral,neutral,neutral,neutral,neutral
458872698,"Thanks for the quick response!

About #1) I agree it changes quite a lot the API. Of course there's different ways to do it. Maybe the easiest way to not break legacy code and keep the same number of methods would be to add a server start parameter allowing us to enable returning an object with both embeddings & tokens?

About #2) no I wasn't suggesting to move the pooling strategy to the client side. Client side would be the same. I was only suggesting to be able to start the server with mixed pooling strategies defined during server start, the server would be returning always the same sequence of defined pooling strategies instead of returning only one type of embeddings (right now). The response object could contain a list of embeddings as they have been declared when starting the server. So if we combine #1 and #2 a possible returned message from server would be (with server started with 4 different pooling strategies):

```
{ 
  'tokens': [ 
      [ ""[CLS]"" ""The"" ""horse"" ""jump"" ""##ed"" ""over"" ""the"" ""fe"" ""##nce"" ""."" ""[SEP]"" ],
      [ ""[CLS]"" ""I"" ""destroyed"" ""there"" ""."" ""[SEP]"" ] ],
  'embs': [ 
      ndarray(...),         # shape (batch,sent_num,tok_emb)
      ndarray(...),         # shape (batch,sent_emb)
      ndarray(...),         # shape (batch,sent_emb)
      ndarray(...)          # shape (batch,sent_emb)
}
```
",thanks quick response agree quite lot course different way maybe easiest way break legacy code keep number would add server start parameter u enable object suggesting move strategy client side client side would suggesting able start server mixed defined server start server would always sequence defined instead one type right response object could contain list declared starting server combine possible returned message server would server different horse jump fe shape batch shape batch shape batch shape batch,issue,positive,positive,positive,positive,positive,positive
458804810,"Both are valid and reasonable requests.

Regarding the first one, the major challenge would be the API redesign. Let me see what I can do.

For the second, are you suggesting moving/adding `pooling_strategy` to the client side API? Note that the combination of the pooling strategy can be quite a lot, and they all need to be frozen into **one** multi-output graph (each output emits one pooling strategy). The client could select which output to actually use when calling `encode()`. I can expect that API redesign (both server and client) and frozen graph optimization would be the major work.",valid reasonable regarding first one major challenge would redesign let see second suggesting client side note combination strategy quite lot need frozen one graph output one strategy client could select output actually use calling encode expect redesign server client frozen graph optimization would major work,issue,negative,positive,neutral,neutral,positive,positive
458519012,"Looks like you made the right guess. Thus, i should probably update the drivers to enable FP 16 instructions?
Here are the results

## `fp32`

```
encoding        512 sentences   0.34s   1492 samples/s   13859 tokens/s
encoding       1024 sentences   0.62s   1661 samples/s   15429 tokens/s
encoding       2048 sentences   1.20s   1702 samples/s   15812 tokens/s
encoding       4096 sentences   2.41s   1701 samples/s   15806 tokens/s
encoding       8192 sentences   4.81s   1703 samples/s   15819 tokens/s
encoding      16384 sentences   9.50s   1723 samples/s   16008 tokens/s
encoding      32768 sentences   19.07s  1717 samples/s   15955 tokens/s
encoding      65536 sentences   37.87s  1730 samples/s   16073 tokens/s
encoding     131072 sentences   76.13s  1721 samples/s   15989 tokens/s
encoding     262144 sentences   150.24s 1744 samples/s   16205 tokens/s

```
## `fp16`
```
encoding        512 sentences   2.29s    223 samples/s    2080 tokens/s
encoding       1024 sentences   4.49s    228 samples/s    2118 tokens/s
encoding       2048 sentences   8.87s    230 samples/s    2143 tokens/s
encoding       4096 sentences   17.70s   231 samples/s    2149 tokens/s
encoding       8192 sentences   35.42s   231 samples/s    2147 tokens/s
encoding      16384 sentences   70.81s   231 samples/s    2148 tokens/s
encoding      32768 sentences   141.61s  231 samples/s    2148 tokens/s
encoding      65536 sentences   283.16s  231 samples/s    2149 tokens/s
encoding     131072 sentences   564.28s  232 samples/s    2157 tokens/s
encoding     262144 sentences   1134.71s         231 samples/s    2145 tokens/s
```
",like made right guess thus probably update enable,issue,negative,positive,positive,positive,positive,positive
458480516,+1 it would be very helpful to include other optional contextualized embedding model in such a powerful service tool ,would helpful include optional model powerful service tool,issue,positive,positive,positive,positive,positive,positive
458479135,"Hi @davidlenz thanks for your time and effort on benchmarking this. Your result suggests that your GPU/driver may not support FP16 instruction. Before running into another exhaustive benchmark, could you do a quick test by using `python example/example1.py PORT PORT_OUT`? Namely,

1. Start a server: `bert-serving-start ...`
2. Test with `python example/example1.py PORT PORT_OUT`
3. Start a FP16 server `bert-serving-start -fp16 ...`
4. Test again with `python example/example1.py PORT PORT_OUT`

then copy-paste the client output here? Thanks a lot!",hi thanks time effort result may support instruction running another exhaustive could quick test python port namely start server test python port start server test python port client output thanks lot,issue,positive,positive,positive,positive,positive,positive
458474625,"### Benchmark Results with Titan V and fp32

After reviewing the results i think i maybe need to rerun  the -fp16 benchmark skript, as it seems something was going on while it has been running that slowed down the process. Any thoughts?
Edit: The benchmark procedure took around 3 hours this time.

`bert-serving-benchmark -model_dir ""multi_cased_L-12_H-768_A-12"" -client_vocab_file ""multi_cased_L-12_H-768_A-12\vocab.txt""`

|`client_batch_size`	|samples/s|
|---|---|
|1	|75|
|16	|599|
|256	|956|
|4096	|1523|


|`max_batch_size`	|samples/s|
|---|---|
|8	|667|
|32	|1220|
|128	|1402|
|512	|1594|


|`max_seq_len`	|samples/s|
|---|---|
|32	|1223|
|64	|671|
|128	|342|
|256	|164|


|`num_client`	|samples/s|
|---|---|
|1	|1538|
|4	|469|
|16	|124|
|64	|30|

|`pooling_layer`	|samples/s|
|---|---|
|[-1]	|1422|
|[-2]	|1533|
|[-3]	|1620|
|[-4]	|1679|
|[-5]	|1694|
|[-6]	|1724|
|[-7]	|1741|
|[-8]	|1771|
|[-9]	|1797|
|[-10]	|1822|
|[-11]	|1830|
|[-12]	|1848|
",think maybe need rerun something going running process edit procedure took around time,issue,negative,neutral,neutral,neutral,neutral,neutral
458390496,"Related: https://github.com/hanxiao/bert-as-service/issues/169

- `bert-as-service` supports customized tokenizer, see tutorial: https://github.com/hanxiao/bert-as-service#using-your-own-tokenizer
- when you use `is_tokenized=True`, you probably also want to turn on `-mask_cls_sep` on the server side to omit the embedding on `[CLS]` and `[SEP]` tokens
- to get the tokenization information (for debugging), you can add `-verbose` when starting the server. This enables the logging for tokenization, mask etc. For now, such information is not available on the client side, and you can only see it from the server log. 


",related see tutorial use probably also want turn server side omit get information add starting server logging mask information available client side see server log,issue,negative,positive,positive,positive,positive,positive
458389776,"No. Depending on your batch size, `max_batch_size` and `prefetch_size ` on the server side. A request may be decomposed into multiple subtasks and computed on multiple devices. 

More details: https://bert-as-service.readthedocs.io/en/latest/source/server.html#server-side-api  
Or read the source of the ventilator: https://github.com/hanxiao/bert-as-service/blob/56e7099457c3eafc6f1f882898efab7e36669ed8/server/bert_serving/server/__init__.py#L173
",depending batch size server side request may decomposed multiple multiple read source ventilator,issue,negative,neutral,neutral,neutral,neutral,neutral
458212659,"usage: /usr/local/bin/bert-serving-start -model_dir=/home/ubuntu/bert/models/cased_L-12_H-768_A-12 -max_seq_len=25
                 ARG   VALUE
__________________________________________________
           ckpt_name = bert_model.ckpt
         config_name = bert_config.json
                cors = *
                 cpu = False
          device_map = []
                fp16 = False
 gpu_memory_fraction = 0.5
       graph_tmp_dir = None
    http_max_connect = 10
           http_port = None
        mask_cls_sep = False
      max_batch_size = 256
         max_seq_len = 25
           model_dir = /home/ubuntu/bert/models/cased_L-12_H-768_A-12
          num_worker = 1
       pooling_layer = [-2]
    pooling_strategy = REDUCE_MEAN
                port = 5555
            port_out = 5556
       prefetch_size = 10
 priority_batch_size = 16
     tuned_model_dir = None
             verbose = False
                 xla = False

I:VENTILATOR:[__i:__i: 63]:freeze, optimize and export graph, could take a while...
I:GRAPHOPT:[gra:opt: 52]:model config: /home/ubuntu/bert/models/cased_L-12_H-768_A-12/bert_config.json
I:GRAPHOPT:[gra:opt: 55]:checkpoint: /home/ubuntu/bert/models/cased_L-12_H-768_A-12/bert_model.ckpt
I:GRAPHOPT:[gra:opt: 59]:build graph...
I:GRAPHOPT:[gra:opt:128]:load parameters from checkpoint...
I:GRAPHOPT:[gra:opt:132]:optimize...
I:GRAPHOPT:[gra:opt:140]:freeze...
I:GRAPHOPT:[gra:opt:145]:write graph to a tmp file: /tmp/tmp3fig1p1_
I:VENTILATOR:[__i:__i: 71]:optimized graph is stored at: /tmp/tmp3fig1p1_
I:VENTILATOR:[__i:_ru:103]:bind all sockets
I:VENTILATOR:[__i:_ru:107]:open 8 ventilator-worker sockets
I:VENTILATOR:[__i:_ru:110]:start the sink
I:SINK:[__i:_ru:262]:ready
I:VENTILATOR:[__i:_ge:185]:get devices
I:VENTILATOR:[__i:_ge:217]:device map: 
		worker  0 -> gpu  0
I:WORKER-0:[__i:_ru:375]:use device gpu: 0, load graph from /tmp/tmp3fig1p1_
I:WORKER-0:[__i:gen:402]:ready and listening!

Turned off the http proxy and went back to 25 max length - it is marginally faster at 10.7 records/sec",usage value false false none none false port none verbose false false ventilator freeze optimize export graph could take gra opt model gra opt gra opt build graph gra opt load gra opt optimize gra opt freeze gra opt write graph file ventilator graph ventilator bind ventilator open ventilator start sink sink ready ventilator get ventilator device map worker use device load graph gen ready listening turned proxy went back length marginally faster,issue,positive,negative,negative,negative,negative,negative
457984543,"Besides TCP request, you may use HTTP request https://github.com/hanxiao/bert-as-service#using-bert-as-service-to-serve-http-requests-in-json

There is no such thing as ""a python request"", a request has to rely on some interface, e.g. TCP or HTTP.",besides request may use request thing python request request rely interface,issue,negative,neutral,neutral,neutral,neutral,neutral
457983345,"A second check on python2, there was indeed some bugs. Thanks @HYPJUDY and @xpact for pointing it out.

Fixed in #222 and available since 1.7.9, please do
```bash
pip install -U bert-serving-server bert-serving-client
```
for the update.",second check python indeed thanks pointing fixed available since please bash pip install update,issue,positive,positive,positive,positive,positive,positive
457976282,"hmm, 10/sec is too slow and seems like a speed on CPU only. could you copy paste the server log **before** the first actual request? Something like this:
![image](https://user-images.githubusercontent.com/2041322/51810336-2af28180-22e2-11e9-8f9a-012b45795dde.png)
This indicates that a worker has been successfully assigned with a GPU.",slow like speed could copy paste server log first actual request something like image worker successfully assigned,issue,positive,positive,positive,positive,positive,positive
457893062,"Thanks @xpact and @hanxiao . I modified L186 from `raise t_e from _e` to `raise t_e`. Then I run `from bert_serving.client import BertClient` without error.
https://github.com/hanxiao/bert-as-service/blob/b79a832b17e78060575fada6aa78dcd3083a9456/client/bert_serving/client/__init__.py#L183-L186
I print `_py2` and the output is `True` but I don't know why the code will go into L186 (which means `_py2` is `False`).
``` bash
>>> sys.version_info
sys.version_info(major=2, minor=7, micro=15, releaselevel='final', serial=0)
>>> sys.version_info >= (3, 0)
False
```
Then I call the client via the following codes without error:
```
from bert_serving.client import BertClient
bc = BertClient()
bc.encode(['First do it', 'then do it right', 'then do it better'])
```
but I encoutered error with the following codes (from [README](https://github.com/hanxiao/bert-as-service#using-your-own-tokenizer)):
```
>>> texts = ['hello world!', 'good day']
>>> texts2 = [s.split() for s in texts]
>>> vecs = bc.encode(texts2, is_tokenized=True)
Traceback (most recent call last):
    File ""<stdin>"", line 1, in <module>
    File ""/home/user/.local/lib/python2.7/site-packages/bert_serving/client/__init__.py"", line 176, in arg_wrapper
         return func(self, *args, **kwargs
    File ""/home/user/.local/lib/python2.7/site-packages/bert_serving/client/__init__.py"", line 254, in encode
         texts = _unicode(texts)
    File ""/home/user/.local/lib/python2.7/site-packages/bert_serving/client/__init__.py"", line 31, in <lambda>
        _unicode = lambda x: [BertClient._force_to_unicode(y) for y in x]
    File ""/home/user/.local/lib/python2.7/site-packages/bert_serving/client/__init__.py"", line 361, in _force_to_unicode
        return text if isinstance(text, unicode) else text.decode('utf-8')
AttributeError: 'list'object has no attribute 'decode'
```
",thanks raise raise run import without error print output true know code go false bash false call client via following without error import right better error following world day recent call last file line module file line return self file line encode file line lambda lambda file line return text text else attribute,issue,positive,positive,neutral,neutral,positive,positive
457815854,maybe you might want do a simple PCA to do the dimension reduction ,maybe might want simple dimension reduction,issue,negative,neutral,neutral,neutral,neutral,neutral
457793720,"No need to change anything. It supports py2 out of the box. See:

https://github.com/hanxiao/bert-as-service/blob/b79a832b17e78060575fada6aa78dcd3083a9456/client/bert_serving/client/__init__.py#L21-L31

Not sure what's going on there, it works on my test env.
```bash
Python 2.7.14 (default, Sep 25 2017, 09:54:19)
[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.37)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from bert_serving.client import BertClient
>>>
```",need change anything box see sure going work test bash python default compatible apple type help copyright license information import,issue,positive,positive,positive,positive,positive,positive
457716244,As a workaround copy the code from the BertClient and remove from _e in that line.,copy code remove line,issue,negative,neutral,neutral,neutral,neutral,neutral
457662939,"My hack to do this was save out the weights after fine-tuning BERT:
```
weights = estimator.get_variable_value('output_weights')
bias = estimator.get_variable_value('output_bias')
np.savetxt('weights.csv', weights, delimiter="","")
np.savetxt('bias.csv', bias, delimiter="","")
```

Then to edit `graph.py` and hijack the `CLS_TOKEN` pooling strategy.

My edit to line 108 in this file (https://github.com/hanxiao/bert-as-service/blob/master/server/bert_serving/server/graph.py)
```
elif args.pooling_strategy == PoolingStrategy.FIRST_TOKEN or \
    args.pooling_strategy == PoolingStrategy.CLS_TOKEN:

    #pooled = tf.squeeze(encoder_layer[:, 0:1, :], axis=1)
    output_layer = model.get_pooled_output()
    hidden_size = output_layer.shape[-1].value
    output_weights = tf.convert_to_tensor(np.loadtxt('weights.csv', delimiter=','), dtype=tf.float32)
    output_bias = tf.convert_to_tensor(np.loadtxt('bias.csv', delimiter=','), dtype=tf.float32)

    logits = tf.matmul(output_layer, output_weights, transpose_b=True)
    logits = tf.nn.bias_add(logits, output_bias)
    pooled = tf.nn.softmax(logits, axis=-1)
```
(Yes, I know this is a crappy hack)

Better solution might be to add `PREDICT` or something to the `PoolingStrategy` ENUM class.  Also this might be re-loading the files for every prediction which is unnecessary.
",hack save bias bias edit hijack strategy edit line file yes know hack better solution might add predict something class also might every prediction unnecessary,issue,positive,positive,neutral,neutral,positive,positive
457280612,"After updating to the latest release the CLI works very well, thank you!

###  Benchmark Results with Titan V and `-fp16 `
(see https://github.com/hanxiao/bert-as-service/issues/204#issuecomment-456052362 for driver and cuda details) 

`bert-serving-benchmark -model_dir ""path_to_bert\multi_cased_L-12_H-768_A-12"" -client_vocab_file ""path_to_bert\multi_cased_L-12_H-768_A-12\vocab.txt"" -fp16`

The benchmark procedure took around 17 hours. 

|`client_batch_size`	|samples/s|
|---|---|
|1	|60|
|16	|192|
|256	|209|
|4096	|231|

|`max_batch_size`	|samples/s|
|---|---|
|8	|194|
|32	|226|
|128	|232|
|512	|231|

|`max_seq_len`	|samples/s|
|---|---|
|32	|250|
|64	|216|
|128	|178|
|256	|124|

|`num_client`	|samples/s|
|---|---|
|1	|231|
|4	|61|
|16	|16|
|64	|3|

|`pooling_layer`	|samples/s|
|---|---|
|[-1]	|209|
|[-2]	|227|
|[-3]	|250|
|[-4]	|277|
|[-5]	|313|
|[-6]	|357|
|[-7]	|417|
|[-8]	|492|
|[-9]	|600|
|[-10]	|789|
|[-11]	|1116|
|[-12]	|1454|

I'll update with fp32 results once available.",latest release work well thank see driver procedure took around update available,issue,positive,positive,positive,positive,positive,positive
457186330,"requirements are already written in README

chenbaicheng <notifications@github.com>于2019年1月24日 周四下午8:48写道：

> @hanxiao <https://github.com/hanxiao> can give a advice ？ which
> tensorflow version can use for this project。。。thanks
>
> —
> You are receiving this because you were mentioned.
>
>
> Reply to this email directly, view it on GitHub
> <https://github.com/hanxiao/bert-as-service/issues/163#issuecomment-457184686>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AB8l6i-W7a0wJ8e6r1R5S3C6h05yoTLmks5vGaubgaJpZM4Zhdys>
> .
>
",already written give advice version use reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
457184686,@hanxiao   can give a advice  ？  which  tensorflow  version   can use for this project。。。thanks ,give advice version use,issue,negative,neutral,neutral,neutral,neutral,neutral
457145702,"@w6688j @brandonchow1997  look at the error traceback, you can see that the `MemoryError` is thrown from tensorflow. So it's highly likely a tensorflow problem.",look error see thrown highly likely problem,issue,negative,neutral,neutral,neutral,neutral,neutral
457142968,"related to #210 #216 

okay I understand serving an end-to-end model seems like a common requirement. However, this isn't the motivation behind `bert-as-service`, of which I provide [a scalable feature extraction service for downstream tasks](https://hanxiao.github.io/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/#engineering-building-a-scalable-service).

Supporting end2end model isn't *that* difficult, a quick hack will do. The major work is to rethink a friendly-yet-compatible-enough API. I understand one may not care about API, but I do.

That said, serving end2end model **isn't my priority** for now. You are welcome to make your own by forking this repo or contribute to this repo by making a PR.",related understand serving model like common requirement however motivation behind provide scalable feature extraction service downstream supporting model difficult quick hack major work rethink understand one may care said serving model priority welcome make contribute making,issue,positive,positive,neutral,neutral,positive,positive
457137484,"> How is this problem solved?
> ![image](https://user-images.githubusercontent.com/16603643/51464345-4d474500-1da0-11e9-9886-736703ff82b4.png)

I got the same problem...Wondering how to solve it??",problem image got problem wondering solve,issue,negative,neutral,neutral,neutral,neutral,neutral
457050192,"Thanks for the advice. 

I have read the code of [http file](https://github.com/hanxiao/bert-as-service/blob/master/server/bert_serving/server/http.py). But I think the problem is due to sock.send() as mentioned in the above [stackoverflow](https://stackoverflow.com/a/14374796) links, the http restful api seemed to only encapsulate the bert client. 

Does my understanding of http api right?",thanks advice read code file think problem due link restful encapsulate client understanding right,issue,negative,positive,positive,positive,positive,positive
457045214,"hmm, not sure the reason behind, will dig in.

fyi, if your goal is to use it as a HTTP service, e.g. call the service via RESTful API. You may want to refer to this tutorial: https://github.com/hanxiao/bert-as-service/#using-bert-as-service-to-serve-http-requests-in-json

Or you may want to refactor [this file](https://github.com/hanxiao/bert-as-service/blob/master/server/bert_serving/server/http.py).",sure reason behind dig goal use service call service via restful may want refer tutorial may want file,issue,positive,positive,neutral,neutral,positive,positive
457036966,"""So I try to search for answers to reduce the 768-dimensions or any other useful solution under the condition that cannot change computer reccently""
@zysNLP  Hi I got this problem as well(768 too large) and i am looking for a method reducing it to 256. I'm wondering that how did you fix it? Thanks a lot!",try search reduce useful solution condition change computer hi got problem well large looking method reducing wondering fix thanks lot,issue,positive,positive,positive,positive,positive,positive
456724497,"got some new result
![image](https://user-images.githubusercontent.com/2041322/51595343-73e8b580-1f31-11e9-9acd-341274c3b4f7.png)
",got new result image,issue,negative,positive,positive,positive,positive,positive
456710281,Got the same problem when using it on server. And I think the problem might be https://stackoverflow.com/a/14374796,got problem server think problem might,issue,negative,neutral,neutral,neutral,neutral,neutral
456684166,"I'm not sure this would help, but I use the commands below to start/stop the server.
It kills all running workers. Wrap it up in to a script like `bert-server-ctl.sh` and enjoy 😺

```bash
# Start
nohup bert-serving-start ${SERVER_ARGS} > tmp/bert-server.log 2>&1 &
echo $! > tmp/bert-server.pid
# Stop
BERT_PID=$(cat tmp/bert-server.pid)
kill -TERM -$(ps -p ${BERT_PID} -o pgid --no-headers)
```",sure would help use server running wrap script like enjoy bash start echo stop cat kill,issue,negative,positive,positive,positive,positive,positive
456638229,"I've got the same issue. I tried 'sudo kill' many times, still cannot restart the bert_serving_start.",got issue tried kill many time still restart,issue,negative,positive,positive,positive,positive,positive
456636711,"Besides that benchmark script, `python example/example1.py PORT PORT_OUT` can also give you a quick overview on the speed. On my Tesla V100 (fp16 supported), here are the results:

## FP32
```bash
encoding 717 strs in 0.41s, speed: 1729/s
encoding 7887 strs in 3.39s, speed: 2328/s
encoding 15057 strs in 6.39s, speed: 2357/s
encoding 22227 strs in 9.59s, speed: 2318/s
encoding 29397 strs in 12.92s, speed: 2275/s
encoding 36567 strs in 16.66s, speed: 2194/s
encoding 43737 strs in 20.97s, speed: 2086/s
encoding 50907 strs in 24.30s, speed: 2094/s
encoding 58077 strs in 27.79s, speed: 2089/s
encoding 65247 strs in 31.29s, speed: 2085/s
encoding 72417 strs in 34.79s, speed: 2081/s
encoding 79587 strs in 38.11s, speed: 2088/s
encoding 86757 strs in 41.65s, speed: 2082/s
encoding 93927 strs in 44.96s, speed: 2089/s
encoding 101097 strs in 48.21s, speed: 2096/s
encoding 108267 strs in 51.68s, speed: 2094/s
encoding 115437 strs in 55.16s, speed: 2092/s
encoding 122607 strs in 58.38s, speed: 2100/s
encoding 129777 strs in 62.14s, speed: 2088/s
encoding 136947 strs in 65.44s, speed: 2092/s
```


## FP16 (`bert-serving-start -fp16 -model_dir ...`)
```bash
~/alg/bert-as-service# python example/example1.py 5570 5571
encoding 717 strs in 0.28s, speed: 2547/s
encoding 7887 strs in 2.88s, speed: 2739/s
encoding 15057 strs in 5.28s, speed: 2851/s
encoding 22227 strs in 7.75s, speed: 2868/s
encoding 29397 strs in 10.32s, speed: 2848/s
encoding 36567 strs in 13.03s, speed: 2805/s
encoding 43737 strs in 15.54s, speed: 2814/s
encoding 50907 strs in 17.75s, speed: 2867/s
encoding 58077 strs in 20.22s, speed: 2871/s
encoding 65247 strs in 23.05s, speed: 2831/s
encoding 72417 strs in 25.92s, speed: 2794/s
encoding 79587 strs in 28.31s, speed: 2811/s
encoding 86757 strs in 30.86s, speed: 2811/s
encoding 93927 strs in 33.30s, speed: 2820/s
encoding 101097 strs in 35.54s, speed: 2844/s
encoding 108267 strs in 37.38s, speed: 2896/s
encoding 115437 strs in 40.92s, speed: 2820/s
encoding 122607 strs in 43.48s, speed: 2819/s
encoding 129777 strs in 46.00s, speed: 2821/s
encoding 136947 strs in 47.51s, speed: 2882/s
```

So, ~1.4x speedup using `-fp16`.",besides script python port also give quick overview speed bash speed speed speed speed speed speed speed speed speed speed speed speed speed speed speed speed speed speed speed speed bash python speed speed speed speed speed speed speed speed speed speed speed speed speed speed speed speed speed speed speed speed,issue,negative,positive,positive,positive,positive,positive
456633227,"Thanks a lot for the detailed investigation. I did improve benchmark.py in 1.7.7, now it's a part of the CLI. After `pip install -U`, you can now use `bert-serving-benchmark` to benchmark. It reuses the argparser defined for `bert-serving-start`, thus accepts what `bert-serving-start` accepts. Details can be found here:
https://github.com/hanxiao/bert-as-service/blob/master/server/bert_serving/server/cli/__init__.py
and here:
https://github.com/hanxiao/bert-as-service/blob/master/server/bert_serving/server/helper.py#L176

btw, the broken CLI link on Windows (#194) is also fixed.",thanks lot detailed investigation improve part pip install use defined thus found broken link also fixed,issue,negative,positive,neutral,neutral,positive,positive
456605414,I'm trying to use bert-as-service exactly for the same purpose and stumbled upon the same issue. Guide on how to get class probabilities in sentence pair classification tasks (like MRPC or MNLI) would be very helpful!,trying use exactly purpose upon issue guide get class sentence pair classification like would helpful,issue,positive,positive,positive,positive,positive,positive
456363260,"fyi, this is fixed in #212, the new feature is available since 1.7.7. Please do 
```bash
pip install -U bert-serving-server bert-serving-client
```
for the update.",fixed new feature available since please bash pip install update,issue,negative,positive,positive,positive,positive,positive
456336735,"I also got this problem. My version is 1.7.6, os is win7 .",also got problem version o win,issue,negative,positive,positive,positive,positive,positive
456259449,"@susht3 but also make sure you are using it correctly. The number of tmp files should be fixed while you are using the service. If not, then you probably use it in a wrong way.",also make sure correctly number fixed service probably use wrong way,issue,negative,positive,neutral,neutral,positive,positive
456258964,"Yes, it will generate some tmp files, either for sockets or for frozen models.

Typically, tmp files are managed by OS, e.g. a simple reboot will delete all tmp files on some OS.

I did have a attempt to delete those files on exit long time ago, see the branch `clean-clutter-onexit`. Later, I gave up on that. Reason is: I don't believe **actively deleting your files** is something this software should do and will put me into danger. What if I somehow delete the wrong files? I'd rather play safe here.",yes generate either frozen typically o simple delete o attempt delete exit long time ago see branch later gave reason believe actively something put danger somehow delete wrong rather play safe,issue,negative,negative,neutral,neutral,negative,negative
456232050,"These files are **needed**. 

Objects communicate through these files. You have to produce these files.

However, if these files bother you, you can choose a different place to store them (to store them in a thrash folder or wherever you want) : https://github.com/hanxiao/bert-as-service#q-after-running-the-server-i-have-several-garbage-tmpxxxx-folders-how-can-i-change-this-behavior-",communicate produce however bother choose different place store store thrash folder wherever want,issue,negative,neutral,neutral,neutral,neutral,neutral
456119536,"Thanks for the clarifications! The benchmark is currently running, however i had to do some additional steps, which i will shortly outline in the following, to determine whether i introduced inconsistencies.

### Why not just run benchmark.py as is?
Since ~bert-server-start~ `bert-serving-start` does not work for me as described in #99, i used the fix suggested by https://github.com/hanxiao/bert-as-service/issues/99#issuecomment-452769280 , i.e. starting the server via `python start-bert-as-service.py` with the necessary flags. 

### So why is this a problem for benchmark.py?
The server is started from within benchmark.py, which did not work for me. I guess this is related to the previous point.

https://github.com/hanxiao/bert-as-service/blob/9a5b015c1b9d925d88769d93eb17704d5ddb8691/benchmark.py#L70
https://github.com/hanxiao/bert-as-service/blob/9a5b015c1b9d925d88769d93eb17704d5ddb8691/benchmark.py#L71
https://github.com/hanxiao/bert-as-service/blob/9a5b015c1b9d925d88769d93eb17704d5ddb8691/benchmark.py#L94

### Cool. How did you work around that?
I commented out the server starting and closing procedures in the experimental loop in `benchmark.py`, see above, and made the server starting a one time procedure using 
`python start-bert-as-service.py -model_dir E:\bert\pretrained_model\multi_cased_L-12_H-768_A-12 -port 7779 -port_out 7780 -fp16 `

With the running server i then proceeded to run benchmark.py, which is going well thus far.

### Any trouble caused by this?
~Based on my (probably limited) understanding, the changes i made should not affect the benchmark results, right? If this is the case i'll update this comment with the results once they are available. In the case this introduced inconsistencies, please let me know what i missed and (if possible) how to circumvent it.~
Yes, since some of the benchmark arguments need to be passed to the server starting procedure to be effective!

Thanks! Also, for clarification, i used the version of benchmark.py from commit 9a5b015 but hard coded the `model_dir` flag to where my model is located.

Edit: fixed formatting.
Edit2: fixed spelling and explained in more detail which version of `benchmark.py` was used.
Edit3: updated the shown lines of code from `benchmark.py` to reflect the correct commit (9a5b015)


## Update
Based on the comparison of the obtained results with the results posted in the readme i am now pretty certain that the approach i took did skew the results (now that i think about it again it makes a lot of sense, since parameters like `pooling_layer` have to be passed during start of the server). I guess i'll have to fix `bert-serving-start` first before i can deliver reliable results. I'll post the obtained results anyway for completeness.

#### Speed wrt. `client_batch_size`

|`client_batch_size`|seqs/s|
|---|---|
|1|58|
|4|129|
|8|163|
|16|195|
|64|213|
|256|216|
|512|225|
|1024|229|
|2048|230|
|4096|232|
#### Speed wrt. `max_batch_size`

|`max_batch_size`|seqs/s|
|---|---|
|32|229|
|64|231|
|128|230|
|256|231|
|512|233|
#### Speed wrt. `max_seq_len`

|`max_seq_len`|seqs/s|
|---|---|
|20|231|
|40|231|
|80|225|
|160|232|
|320|230|
#### Speed wrt. `num_client`

|`num_client`|seqs/s|
|---|---|
|2|120|
|4|59|
|8|29|
|16|14|
|32|7|
#### Speed wrt. `pooling_layer`

|`pooling_layer`|seqs/s|
|---|---|
|[-1]|233|
|[-2]|233|
|[-3]|233|
|[-4]|233|
|[-5]|234|
|[-6]|233|
|[-7]|233|
|[-8]|233|
|[-9]|233|
|[-10]|233|
|[-11]|233|
|[-12]|233|
",thanks currently running however additional shortly outline following determine whether run since work used fix starting server via python necessary problem server within work guess related previous point cool work around server starting experimental loop see made server starting one time procedure python running server run going well thus far trouble probably limited understanding made affect right case update comment available case please let know possible circumvent yes since need server starting procedure effective thanks also clarification used version commit hard flag model edit fixed edit fixed spelling detail version used edit shown code reflect correct commit update based comparison posted pretty certain approach took skew think lot sense since like start server guess fix first deliver reliable post anyway completeness speed speed speed speed speed,issue,positive,positive,neutral,neutral,positive,positive
456092818,"Instead of bumping versions, I'd keep it as it is for now. In the next version, I will remove logger in `graph.py` for better compatibility on Windows. For all windows users, if you encounter such problem, please refer to the quick fix as https://github.com/hanxiao/bert-as-service/issues/207#issuecomment-455890549 suggested. Meanwhile stay tuned.",instead bumping keep next version remove logger better compatibility encounter problem please refer quick fix meanwhile stay tuned,issue,negative,positive,positive,positive,positive,positive
456082655,"@hanxiao, thanks for the update.  Unfortunately I get the same error with `1.7.6`, and it is again fixed by commenting out the logger lines 194 and 248 in `graph.py`.",thanks update unfortunately get error fixed logger,issue,negative,negative,neutral,neutral,negative,negative
456079419,"Somehow my logger mysteriously fails on Windows for this *particular* part. I did a quick fix, could you please give a try on 1.7.6 using `pip install -U`?",somehow logger mysteriously particular part quick fix could please give try pip install,issue,negative,positive,positive,positive,positive,positive
456076440,"I also have the same issue,

- Windows 10
- tf version 1.12.0 (conda tensorflow-gpu venv)
- Same issue with and without `-cpu` flag
- Python version 3.6.8
- `bert-serving-start -version` gives: 1.7.4, upgraded now to 1.7.5 but the result is the same.

UPDATE: @davidlenz solution seems to work (tested with GPU)",also issue version issue without flag python version result update solution work tested,issue,negative,neutral,neutral,neutral,neutral,neutral
456074310,@hanxiao I'm sorry to have wasted your valuable time with this.  ,sorry wasted valuable time,issue,negative,negative,negative,negative,negative,negative
456054195,"Thanks a lot for your initiative! Yes, correct. Just add `-fp16` to 
https://github.com/hanxiao/bert-as-service/blob/6d57dddf2528751cfd8e8ea470c22df4e8651362/benchmark.py#L16
and it should work.

No, you don't need Chinese BERT at all. You can use whatever BERT you like, as the speed is measured on the sample-level not on the token-level, changing BERT language should not affect the speed. ",thanks lot initiative yes correct add work need use whatever like speed measured language affect speed,issue,positive,positive,positive,positive,positive,positive
456052362,"Would love to help. I am running a Titan V on Win 10 with the following specs

![grafik](https://user-images.githubusercontent.com/25869733/51473423-9c7b7e80-1d7c-11e9-875a-fbdb79044857.png)


I guess i just run `benchmark.py` and add the `-fp16` flag to line 16, i.e. 

`common = vars(get_args_parser().parse_args(['-model_dir', MODEL_DIR, '-port', str(PORT), '-port_out', str(PORT_OUT)]))`, 

is that correct?

Also, should the benchmark be run with the chinese language model as in `benchmark.py` or can i stick with the multi language model i am currently running?",would love help running win following spec guess run add flag line common port correct also run language model stick language model currently running,issue,positive,positive,positive,positive,positive,positive
455990969,"you are using the wrong model. `example8.py` is reproducible with `uncased_L-12_H-768_A-12` whereas you are using `multi_cased_L-12_H-768_A-12`. The latter is trained on character-level for maximizing its compatibility on multi-lingual, thus doesn't understand English very well.",wrong model reproducible whereas latter trained compatibility thus understand well,issue,negative,negative,negative,negative,negative,negative
455959573,"1.when I use tf.estimator.Estimator to optimize model file
2. I use optimize file in my predict file,  when I run predict, It seem init model every time. my code is down here, can anyone help me?

function get_estimator
![image](https://user-images.githubusercontent.com/30991932/51455914-9ee2d600-1d86-11e9-9759-abd962839d88.png)

function input_fn_build
![image](https://user-images.githubusercontent.com/30991932/51455956-c639a300-1d86-11e9-953e-6b04bff3422a.png)

function predict
![image](https://user-images.githubusercontent.com/30991932/51455968-d2bdfb80-1d86-11e9-8b02-2e44a9f79df6.png)



 ",use optimize model file use optimize file predict file run predict seem model every time code anyone help function image function image function predict image,issue,positive,neutral,neutral,neutral,neutral,neutral
455957789,"but then on the second thought, you are right. I need to provide a more gentle CLI/API for shutting down server besides keyboard interrupt. I'm marking it as a feature for now.",second thought right need provide gentle shutting server besides keyboard interrupt marking feature,issue,negative,positive,positive,positive,positive,positive
455929538,"Could you do `bert-serving-start -version` and tell me your version? as I already solved the logger issue on Windows in #183 and shouldn't be any problem since 1.7.0.

Please do `pip install -U bert-serving-server bert-serving-client` and upgrade to the latest version and retry.
",could tell version already logger issue problem since please pip install upgrade latest version retry,issue,negative,positive,positive,positive,positive,positive
455929091,"Ctrl-C is the safe way to terminate the server. No need to reboot the machine.

`kill` is not the a safe way to terminate the server.",safe way terminate server need machine kill safe way terminate server,issue,negative,positive,positive,positive,positive,positive
455890549,"I had the same issue (Win 10).  What fixed it for me was to comment out two lines of logger code in the `/server/graph.py` file.

```
logger.info(""freezing %d variables..."", len(returned_variables))

logger.info(""Converted %d variables to const ops."", how_many_converted) 
```
",issue win fixed comment two logger code file freezing converted,issue,positive,positive,positive,positive,positive,positive
455875987,"I was able to solve this by changing the base image to one that doesn't need nvidia-docker:
```
FROM tensorflow/tensorflow:1.12.0-py3
```

Rebuilt, and using the same run command without the nvidia runtime works:
```
docker run -dit \
-p 5555:5555 \
-p 5556:5556 \
-v /root/Documents/uncased_L-12_H-768_A-12:/model \
-e CUDA_VISIBLE_DEVICES='0' \
-t bert-as-service
```

",able solve base image one need rebuilt run command without work docker run,issue,negative,negative,negative,negative,negative,negative
455840335,"I think there is something happening with identifying questions because I get 35 questions and maybe that is skewing my results somehow?

```
nobu@nobu-ThinkPad-T420 ~ $ python
Python 3.6.5 |Anaconda custom (64-bit)| (default, Mar 29 2018, 18:21:58) 
[GCC 7.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import numpy as np
>>> from bert_serving.client import BertClient
>>> from termcolor import colored
>>> 
>>> prefix_q = '##### **Q:** '
>>> topk = 5
>>> 
>>> with open('README.md') as fp:
...     questions = [v.replace(prefix_q, '').strip() for v in fp if v.strip() and v.startswith(prefix_q)]
...     print('%d questions loaded, avg. len of %d' % (len(questions), np.mean([len(d.split()) for d in questions])))
... 
35 questions loaded, avg. len of 9
>>> 

```",think something happening get maybe somehow python python custom default mar type help copyright license information import import import colored open print loaded loaded,issue,negative,neutral,neutral,neutral,neutral,neutral
455839845,"After changing the ports to reflect that I have both server and client locally:

```
bc = BertClient(port=5555, port_out=5556)
doc_vecs = bc.encode(questions)

```
My results are so bad, I basically get the same suggestions for every question:

```
your question: what is the best pooling?
top 5 questions similar to ""what is the best pooling?""
> 995.9	What is backend based on?
> 948.8	What are the available pooling strategies?
> 937.1	Could I use other pooling techniques?
> 932.5	What is the parallel processing model behind the scene?
> 908.7	Why does the server need two ports?
your question: is there Chinese support?
top 5 questions similar to ""is there Chinese support?""
> 923.0	What is backend based on?
> 885.2	Could I use other pooling techniques?
> 867.2	What is the parallel processing model behind the scene?
> 864.7	What are the available pooling strategies?
> 851.3	Why does the server need two ports?
your question: does it support chinese
top 5 questions similar to ""does it support chinese""
> 924.8	What is backend based on?
> 905.3	Could I use other pooling techniques?
> 865.1	Why does the server need two ports?
> 861.0	What are the available pooling strategies?
> 860.8	What is the parallel processing model behind the scene?
your question: what is the best pooling strategy?
top 5 questions similar to ""what is the best pooling strategy?""
> 967.8	What is backend based on?
> 939.5	What are the available pooling strategies?
> 921.3	Could I use other pooling techniques?
> 918.9	What is the parallel processing model behind the scene?
> 892.0	Why does the server need two ports?
your question: 

```",reflect server client locally bad basically get every question question best top similar best based available could use parallel model behind scene server need two question support top similar support based could use parallel model behind scene available server need two question support top similar support based could use server need two available parallel model behind scene question best strategy top similar best strategy based available could use parallel model behind scene server need two question,issue,positive,positive,positive,positive,positive,positive
455839358,I guess the ports are for when you are connecting remotely to the bert server from a client and finally I'm guessing because this is a 3 minute example that the accuracy is not that good.  ,guess remotely server client finally guessing minute example accuracy good,issue,negative,positive,positive,positive,positive,positive
455552112,"fyi, from 1.7.3 you can directly use `ConcurrentBertClient` instead of `pop` and `append` manually. see 
https://github.com/hanxiao/bert-as-service/blob/827d2742e4b107e6c25ae552aa24219095133712/example/example4.py#L29-L38

Again `pip install -U bert-serving-server bert-serving-client` for the update.",directly use instead pop append manually see pip install update,issue,negative,positive,neutral,neutral,positive,positive
455550736,"It works. but given that most GPUs have limited support of FP16, not sure how useful this will be.",work given limited support sure useful,issue,positive,positive,positive,positive,positive,positive
455471477,"Then it's not related to this repo, please google or check stackoverflow for gpu configuration.",related please check configuration,issue,negative,neutral,neutral,neutral,neutral,neutral
455469691,"You are right, the GPU is not working.
But I configured the cuda environment in .bashrc.

```bash
2019-01-18 16:09:33.469748: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-01-18 16:09:33.501422: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2019-01-18 16:09:33.501495: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] retrieving CUDA diagnostic information for host: gpu06
2019-01-18 16:09:33.501515: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:170] hostname: gpu06
2019-01-18 16:09:33.501580: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:194] libcuda reported version is: 384.81.0
2019-01-18 16:09:33.501619: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:198] kernel reported version is: 384.81.0
2019-01-18 16:09:33.501632: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:305] kernel version seems to match DSO: 384.81.0
......
2019-01-18 16:09:38.234526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-01-18 16:09:38.385533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0000:8b:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2019-01-18 16:09:38.385705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-01-18 16:09:38.818074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-18 16:09:38.818135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2019-01-18 16:09:38.818162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2019-01-18 16:09:38.818578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8138 MB memory) -> physical GPU (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:8b:00.0, compute capability: 6.0)
2019-01-18 16:09:38.845026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-18 16:09:38.845083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2019-01-18 16:09:38.845096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2019-01-18 16:09:38.845683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8138 MB memory) -> physical GPU (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:8a:00.0, compute capability: 6.0)
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
2019-01-18 16:09:41.937668: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 16:09:41.945445: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 16:09:41.956287: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 16:09:41.961941: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 16:09:41.972661: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 16:09:41.978281: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 16:09:41.988818: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 16:09:41.994510: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 16:09:42.005241: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 16:09:42.011053: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 16:09:42.021607: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 16:09:42.027575: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
```
",right working environment bash binary use call device diagnostic information host version kernel version kernel version match visible found device name major minor visible device interconnect strength edge matrix device memory physical device name bus id compute capability device interconnect strength edge matrix device memory physical device name bus id compute capability running done running running done running create handle create handle create handle create handle create handle create handle create handle create handle create handle create handle create handle create handle,issue,positive,positive,neutral,neutral,positive,positive
455446065,"seems that your GPU isn't working correctly, could you do `bert-serving-start -verbose` and check copy-paste the log here?",working correctly could check log,issue,negative,neutral,neutral,neutral,neutral,neutral
455445282,"from the traceback you can see that the error is thrown from `numpy`, not relevant to this repo. You can validate it by simply start a empty python instance and run `import numpy as np`",see error thrown relevant validate simply start empty python instance run import,issue,negative,positive,positive,positive,positive,positive
455036440,"@policeme there is **absolutely no reason** to hard-code the path in the source. Why can't you use CLI `-model_dir` etc.?

This is not a bug of this repo. It's basic Linux/Windows knowledge. Closed.",absolutely reason path source ca use bug basic knowledge closed,issue,negative,positive,neutral,neutral,positive,positive
455033904,"@HannahXu see now you have the absolute path, copy paste as `-model_dir`",see absolute path copy paste,issue,negative,positive,positive,positive,positive,positive
455031735,"but isn't your picture indicating `xx/bert/tmp/chinese_L-12_H-768_A-12/`? 

![image](https://user-images.githubusercontent.com/2041322/51294197-db43c880-1a4c-11e9-889a-9984377005ed.png)

could you `cd` to the folder in which json config located, and then type `pwd` there?",picture image could folder type,issue,negative,neutral,neutral,neutral,neutral,neutral
455030708,"Please look at the third last figure, there is a bert_config.json file in the folder.",please look third last figure file folder,issue,negative,neutral,neutral,neutral,neutral,neutral
455029656,"@hanxiao That script worked! Thank you!

@Colanim I tried and added `bert_serving` to environment variables as you suggested, it's still not recognized, not sure if I did something else wrong, but thank you for trying to help!",script worked thank tried added environment still sure something else wrong thank trying help,issue,positive,neutral,neutral,neutral,neutral,neutral
455026687,"@sailormoon2016 please refer to the step 1 of https://github.com/hanxiao/bert-as-service/issues/99#issuecomment-452769280

for the latest `bert-as-service` >=1.7.0, you don't need to do step 2 in that comment",please refer step latest need step comment,issue,negative,positive,positive,positive,positive,positive
455026363,looks like your model config `/tmp/chinese_L-12_H-768_a-12/bert_config.json` is not found? could you double check that?,like model found could double check,issue,negative,neutral,neutral,neutral,neutral,neutral
455025842,"OK, when my jupyter firstly  stack in 'Kernal starting, Please waiting', uninstalling ```bert-serving-client``` can't resolve it. then I upgrade my ```ipykernal```.But When I install the package, same problem happen agin, I resolve it by uninstalling it.",firstly stack starting please waiting ca resolve upgrade install package problem happen resolve,issue,positive,positive,positive,positive,positive,positive
454271662,"First, tensorflow doesn't play well in multi-thread env, see https://github.com/hanxiao/bert-as-service/issues/161

So the best way to work with tensorflow in parallel environment is making a strict separation (starting from `import tensorflow as tf`) across processes. It saves a lot of troubles.

Besides this, using process instead of thread has the following advantages:
- Clear separation, not abusing ""global/shared"" variables;
- Better GPU utilization, memory management;
- Support remote workers, who said that `BertWorker` must be on the same machine as `BertServer` and `BertSink`? They don't have to be. When one becomes the bottleneck, just scale it on a larger machine. This greatly improves the scalability and flexibility. All you need is changing the ZMQ socket from `ipc` to `tcp`.",first play well see best way work parallel environment making strict separation starting import across lot besides process instead thread following clear separation better utilization memory management support remote said must machine one becomes bottleneck scale machine greatly flexibility need socket,issue,positive,positive,positive,positive,positive,positive
454270141,"As I understood, this paragraph of your [blog](https://hanxiao.github.io/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/) describe the reason behind the choice of processes :

> Say multiple clients are sending requests to a server simultaneously. Parallelization on the computational work is one thing, but first, how should the server even handle receiving? Should it receive the first request, hold this connection until it sends back the result; then proceed to the second request? What happens if there are 100 clients? Should the server use the same logic to manage 100 connections?

However I'm not sure of me..

",understood paragraph describe reason behind choice say multiple sending server simultaneously parallelization computational work one thing first server even handle receive first request hold connection back result proceed second request server use logic manage however sure,issue,negative,positive,neutral,neutral,positive,positive
453981424,"Yeah~
My test result is that no matter whether the bert server receives the request or not, the server will throw this exception after a period of time.",test result matter whether server request server throw exception period time,issue,negative,neutral,neutral,neutral,neutral,neutral
453954106,"huh? @yanzp0214 no request to the server and then it's down?

I'm reopening this issue.",huh request server issue,issue,negative,neutral,neutral,neutral,neutral,neutral
453953130,my bert service version is 1.6.9 and  the latest throwing bug is after i starting bert server several hours .i never send a request to the server,service version latest throwing bug starting server several never send request server,issue,negative,positive,positive,positive,positive,positive
453928565,"could you please be more specific?

**Prerequisites**

> Please fill in by replacing `[ ]` with `[x]`.

* [ ] Are you running the latest `bert-as-service`?
* [ ] Did you follow [the installation](https://github.com/hanxiao/bert-as-service#install) and [the usage](https://github.com/hanxiao/bert-as-service#usage) instructions in `README.md`?
* [ ] Did you check the [FAQ list in `README.md`](https://github.com/hanxiao/bert-as-service#speech_balloon-faq)?
* [ ] Did you perform [a cursory search on existing issues](https://github.com/hanxiao/bert-as-service/issues)?

**System information**

> Some of this information can be collected via [this script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh).

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- `bert-as-service` version: 
- GPU model and memory:
- CPU model and memory:

---

### Description

> Please replace `YOUR_SERVER_ARGS` and `YOUR_CLIENT_ARGS` accordingly. You can also write your own description for reproducing the issue.

I'm using this command to start the server:

```bash
bert-serving-start YOUR_SERVER_ARGS
```

and calling the server via:
```python
bc = BertClient(YOUR_CLIENT_ARGS)
bc.encode()
```

Then this issue shows up:

...",could please specific please fill running latest follow installation usage check list perform cursory search system information information collected via script o platform distribution source binary version python version version model memory model memory description please replace accordingly also write description issue command start server bash calling server via python issue,issue,positive,positive,positive,positive,positive,positive
453927898,"![image](https://user-images.githubusercontent.com/46669263/51102013-32ad2300-1818-11e9-9b6c-7f1de9169362.png)
my bert running server throw this error every once in a while whether i have used the server ",image running server throw error every whether used server,issue,negative,neutral,neutral,neutral,neutral,neutral
453900574,"thanks shall try again

Sent from my iPhone

On 14 Jan 2019, at 11:49, Han Xiao <notifications@github.com<mailto:notifications@github.com>> wrote:


I can't reproduce the error. Maybe it's your JSON file problem. I tested with https://cail.oss-cn-qingdao.aliyuncs.com/cail2018_small_0518.tar.gz and it's no problem.

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub<https://github.com/hanxiao/bert-as-service/issues/178#issuecomment-453896634>, or mute the thread<https://github.com/notifications/unsubscribe-auth/Aab_R4RDVmCH71V-ZLqh4IaipQepLkA8ks5vC_5DgaJpZM4ZzTA5>.
",thanks shall try sent han wrote ca reproduce error maybe file problem tested problem thread reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
453898941,"fyi, this issue are fixed in #188 and the new feature is available since 1.7.0. Please do the following for the upgrade:
```bash
pip install -U bert-serving-server bert-serving-client
```
",issue fixed new feature available since please following upgrade bash pip install,issue,negative,positive,positive,positive,positive,positive
453897423,"thanks for the PR.

I changed the default `graph_tmp_dir` to `None`, as `/tmp` may not be a valid path on Windows. Setting `dir=None` just leaves the dir naming problem to python.",thanks default none may valid path setting leaf naming problem python,issue,negative,positive,positive,positive,positive,positive
453896634,I can't reproduce the error. Maybe it's your JSON file problem. I tested with https://cail.oss-cn-qingdao.aliyuncs.com/cail2018_small_0518.tar.gz and it's no problem.,ca reproduce error maybe file problem tested problem,issue,negative,neutral,neutral,neutral,neutral,neutral
453826700,thanks for diagnosing and providing details. Will check and fix soon. ,thanks providing check fix soon,issue,negative,positive,positive,positive,positive,positive
453565244,"Hi Alexander, 

Thanks for the reply! I'll give it a try with your guidelines.

Eugene",hi thanks reply give try,issue,negative,positive,positive,positive,positive,positive
453533725,"no problem. yeah, for cpu batch size doesnt affect speed significantly.",problem yeah batch size doesnt affect speed significantly,issue,negative,positive,positive,positive,positive,positive
453456242,"Terribly sorry for the issue, I will fill in every issue form from now.

Thanks very much for the patient and detailed response! 

The reason for the low-speed is that my GPU did not work(`nvidia-smi: No running processes found`) even after I set the `device_map`. I don't know why, and after I changed to another machine, it worked great. ",terribly sorry issue fill every issue form thanks much patient detailed response reason work running found even set know another machine worked great,issue,negative,positive,positive,positive,positive,positive
453406834,"Which `bert-as-service` are you running on? You can do `bert-serving-start -version` to see that.

There maybe some issue in `benchmark.py` due to the recent updates on server API. I will do a check.

",running see maybe issue due recent server check,issue,negative,negative,neutral,neutral,negative,negative
453406022,"Please always fill in the issue form, saving time for both of us. Currently, you are not showing enough information, e.g. are you running on GPU/CPU, what's the server config? can you do BertClient.server_status? what's you tf vesion, did you self-complied it? what's your bert-as-service version? what is the `num_worker` and `max_seq_length`? The easiest way to answer them all is to follow the issue form.

Million-level sentences is not a problem for ` bert-as-service`.

There is a **significant difference** between encoding one at a time and a batch a time, see https://github.com/hanxiao/bert-as-service#speed-wrt-client_batch_size 

If you are interested in knowing how `bert-as-service` handle large batch and doing scheduling, please refer to https://hanxiao.github.io/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/

That said, If you don't find any difference (between bc.encode(['sent']) and bc.encode(['sent']*1024)), then there is problem, e.g. the bottleneck is probably at generating the sentences.



",please always fill issue form saving time u currently showing enough information running server version easiest way answer follow issue form problem significant difference one time batch time see interested knowing handle large batch please refer said find difference problem bottleneck probably generating,issue,positive,positive,positive,positive,positive,positive
453009877,Great! The server can start in win10 by this approach. Thanks a lot!,great server start win approach thanks lot,issue,positive,positive,positive,positive,positive,positive
453006994,"@eggachecat I refactor the logger part in #183 as you suggested. The new feature is available since 1.6.9 and please do
```bash
pip install -U bert-serving-server bert-serving-client
```
for the update. That should work on Windows.

To all Windows users, please try 1.6.9. And if there is still a problem feel free to reopen this issue.",logger part new feature available since please bash pip install update work please try still problem feel free reopen issue,issue,positive,positive,positive,positive,positive,positive
452956437,"> For those who need run this on Windows 10 right now...
> 
> * to solve the `bert-serving-start not found` problem
> 
> Create a start-bert-as-service.py with the following code
> 
> ```python
> import sys
> 
> from bert_serving.server import BertServer
> from bert_serving.server.helper import get_run_args
> 
> 
> if __name__ == '__main__':
>     args = get_run_args()
>     server = BertServer(args)
>     server.start()
>     server.join()
> ```
> 
> so you can run with the following command
> `python start-bert-as-service.py -model_dir ./tmp/chinese_L-12_H-768_A-12/ -num_worker=1`
> 
> * to solve the `TypeError: can't pickle _thread.RLock objects` problem
> 
> Replace the _set_logger_ function in the _bert_serving/server/helper_ with yours
> 
> ```python
> # def set_logger(context, verbose=False):
> #     logger = logging.getLogger(context)
> #     logger.setLevel(logging.DEBUG if verbose else logging.INFO)
> #     formatter = logging.Formatter(
> #         '%(levelname)-.1s:' + context + ':[%(filename).3s:%(funcName).3s:%(lineno)3d]:%(message)s', datefmt=
> #         '%m-%d %H:%M:%S')
> #     console_handler = logging.StreamHandler()
> #     console_handler.setLevel(logging.DEBUG if verbose else logging.INFO)
> #     console_handler.setFormatter(formatter)
> #     logger.handlers = []
> #     logger.addHandler(console_handler)
> #     return logger
> class FakeLogger:
>     def __init__(self, *args, **kwargs):
>         pass
> 
>     def info(self, *args, **kwargs):
>         print(*args, **kwargs)
> 
>     def debug(self, *args, **kwargs):
>         print(*args, **kwargs)
> 
> 
> def set_logger(context, verbose=False):
>     return FakeLogger()
> ```

Yes，the server can start in Windows 10 using this method! Thx!",need run right solve found problem create following code python import import import server run following command python solve ca pickle problem replace function python context logger context verbose else context message verbose else return logger class self pas self print self print context return server start method,issue,negative,positive,neutral,neutral,positive,positive
452869269,"Thanks @eggachecat for pointing out a solution for the logger, I will do a PR to fix this.",thanks pointing solution logger fix,issue,positive,positive,positive,positive,positive,positive
452769280,"For those who need run this on Windows 10 right now...
- to solve the `bert-serving-start not found` problem

Create a start-bert-as-service.py with the following code 
```python
import sys

from bert_serving.server import BertServer
from bert_serving.server.helper import get_run_args


if __name__ == '__main__':
    args = get_run_args()
    server = BertServer(args)
    server.start()
    server.join()
```
so you can  run with the following command
`python start-bert-as-service.py  -model_dir ./tmp/chinese_L-12_H-768_A-12/ -num_worker=1`


- to solve the `TypeError: can't pickle _thread.RLock objects` problem

Replace the *set_logger* function in the *bert_serving/server/helper* with yours
```python
# def set_logger(context, verbose=False):
#     logger = logging.getLogger(context)
#     logger.setLevel(logging.DEBUG if verbose else logging.INFO)
#     formatter = logging.Formatter(
#         '%(levelname)-.1s:' + context + ':[%(filename).3s:%(funcName).3s:%(lineno)3d]:%(message)s', datefmt=
#         '%m-%d %H:%M:%S')
#     console_handler = logging.StreamHandler()
#     console_handler.setLevel(logging.DEBUG if verbose else logging.INFO)
#     console_handler.setFormatter(formatter)
#     logger.handlers = []
#     logger.addHandler(console_handler)
#     return logger
class FakeLogger:
    def __init__(self, *args, **kwargs):
        pass

    def info(self, *args, **kwargs):
        print(*args, **kwargs)

    def debug(self, *args, **kwargs):
        print(*args, **kwargs)


def set_logger(context, verbose=False):
    return FakeLogger()
```


",need run right solve found problem create following code python import import import server run following command python solve ca pickle problem replace function python context logger context verbose else context message verbose else return logger class self pas self print self print context return,issue,negative,positive,neutral,neutral,positive,positive
452687179,"Hi Eugene,

We are also working on just that. These are the steps we have been doing. 

1.  First step would be to change the graph.py to the new model, since the graph and outputs are different. 
2. Then in the server init file you need to change the input function and model function to reflect the new model, features and outputs. Most of these functions can be directly imported from run_squad in the bert repo such at tokenization. 
3. The tricky part might be to pass the result to the client side, since we are not passing just a tensor but spans of text. The easiest(hacky?) way might just be to pass the output logits, and then parse the output on the client side. This would require you to re-tokenize the text and map the logits to the correct span in the text.  

Alexander
@Gridlabs",hi also working first step would change new model since graph different server file need change input function model function reflect new model directly tricky part might pas result client side since passing tensor text easiest hacky way might pas output parse output client side would require text map correct span text,issue,negative,positive,positive,positive,positive,positive
452676590,"I am using win10 and just updated bert-serving-server and bert-serving-client to 1.6.8. 

when run the server, I got the following error:

...lib\multiprocessing\reduction.py"", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
TypeError: can't pickle _thread.RLock objects
...
lib\multiprocessing\reduction.py"", line 82, in steal_handle
    _winapi.PROCESS_DUP_HANDLE, False, source_pid)
OSError: [WinError 87] The parameter is incorrect

The server cannot be started.",win run server got following error line dump file protocol ca pickle line false parameter incorrect server,issue,negative,positive,positive,positive,positive,positive
452614225,"Connect the dense network after the output of the Transformer of BERT.

So basically, you will have to use bert-as-service with the parameter `pooling_strategy` to `None` in order to receive the output of transformer.

Then you use these vectors as input of a fully connected network, with an output size of 256. And here you go, you have your vectors of size 256. Of course you need to train this dense network on a task.",connect dense network output transformer basically use parameter none order receive output transformer use input fully connected network output size go size course need train dense network task,issue,negative,neutral,neutral,neutral,neutral,neutral
452590344,"> @chiyuzhang94 which version of tensorflow are you using?

I am using Version: 1.12.0+{cloud service name}, but I think it is a special version for the cloud service.",version version cloud service name think special version cloud service,issue,negative,positive,positive,positive,positive,positive
452572843,"Thanks, we will try version 1.6.7 and report back to you later if the issue persists.",thanks try version report back later issue,issue,negative,positive,neutral,neutral,positive,positive
452564335,"(rasa_chatbot_cn) [root@izwz9h57hlxvofmi9b5ufvz envs]# pip freeze
absl-py==0.6.1
astor==0.7.1
bert-serving-client==1.6.6
bert-serving-server==1.6.6
certifi==2018.11.29
gast==0.2.0
GPUtil==1.4.0
grpcio==1.16.1
h5py==2.9.0
Keras-Applications==1.0.6
Keras-Preprocessing==1.0.5
Markdown==3.0.1
mkl-fft==1.0.6
mkl-random==1.0.2
numpy==1.15.4
protobuf==3.6.1
pyzmq==17.1.2
scipy==1.1.0
six==1.12.0
tensorboard==1.12.1
tensorflow==1.12.0
termcolor==1.1.0
Werkzeug==0.14.1

I  use conda ,   then   I  go to see  you give me address  ,  it seem  not  suitable, the  console has not print  error  massage  , only  stop  here.  
-------------------------------------
I:GRAPHOPT:[gra:opt:121]:load parameters from checkpoint...
I:GRAPHOPT:[gra:opt:123]:freeze...
INFO:tensorflow:Froze 181 variables.
INFO:tensorflow:Converted 181 variables to const ops.
I:GRAPHOPT:[gra:opt:126]:optimize...
-------------------------------------
thanks  ",root pip freeze use go see give address seem suitable console print error massage stop gra opt load gra opt freeze froze converted gra opt optimize thanks,issue,negative,positive,positive,positive,positive,positive
452549683,"> I refactor the server part according to @aron3312 suggestion. To Windows users, could you guys do `pip install -U bert-serving-server bert-serving-client` to upgrade to 1.6.7, and then give it a try?

i'm try it and it working
remove about logger (ex self.logger.info...etc.)   or replace yourself log module 
that working for me",server part according suggestion could pip install upgrade give try try working remove logger ex replace log module working,issue,negative,neutral,neutral,neutral,neutral,neutral
452491696,"fyi, @felixhao28 `timeout` issue is solved in #181 and is available since 1.6.7. Please do 
```bash
pip install -U bert-serving-server bert-serving-client
```
for the update.",issue available since please bash pip install update,issue,negative,positive,positive,positive,positive,positive
452491149,"I refactor the server part according to @aron3312 suggestion. To Windows users, could you guys do `pip install -U bert-serving-server bert-serving-client` to upgrade to 1.6.7, and then give it a try?",server part according suggestion could pip install upgrade give try,issue,negative,neutral,neutral,neutral,neutral,neutral
452475031,"@wanghm92 could you open a new issue and fill in the issue form, knowing the version and sys info can help me to understand problem much much faster.",could open new issue fill issue form knowing version help understand problem much much faster,issue,negative,positive,positive,positive,positive,positive
452474106,"Like felixhao28 said, this issue is kind of random, which is probably why you could not reproduce it, at least not easily. 

However, it does happen every time for sure, at least for me. It's just a matter of how long it holds until it crashes. I use one server hosted on one GPU and let multiple programs query it at random frequency. You may want to reopen this issue and hope for more inputs from more users.

I've not changed the default timeout, so I guess that's a separate issue. 

Thanks!",like said issue kind random probably could reproduce least easily however happen every time sure least matter long use one server one let multiple query random frequency may want reopen issue hope default guess separate issue thanks,issue,positive,positive,neutral,neutral,positive,positive
452461390,"`timeout` is another issue, will be addressed separately. Please always create a new issue if you are reporting something new. thanks",another issue separately please always create new issue something new thanks,issue,positive,positive,positive,positive,positive,positive
452458544,"@felixhao28 please do `pip install -U bert-serving-server bert-serving-client` and use the latest version.

Better start a new issue and fill in the issue form, helping me to locate the problem faster.

Note that `BertClient.encode()` is not thread-safe, whereas `BertClient` itself is thread-safe. Therefore if you are using `BertClient` in a MT environment, please be careful and refer to https://github.com/hanxiao/bert-as-service/issues/29#issuecomment-440487707",please pip install use latest version better start new issue fill issue form helping locate problem faster note whereas therefore environment please careful refer,issue,positive,positive,positive,positive,positive,positive
452456198,nice! good to know. will make a PR to address these two points.,nice good know make address two,issue,positive,positive,positive,positive,positive,positive
452455668,"looks like a tensorflow problem, which version of tensorflow are you using? 

please fill in the issue form, also please refer to #163 and check if it's a tensorflow problem",like problem version please fill issue form also please refer check problem,issue,negative,neutral,neutral,neutral,neutral,neutral
452270890,"> > Thanks gotcha. Now I see the `invalid syntax` is raised from `single_image_random_dot_sterograms` not from `bert_as_service`. After googling, I found it's a tensorflow problem, and [this issue](https://github.com/tensorflow/tensorflow/issues/21164) as well as [this PR](https://github.com/tensorflow/tensorflow/pull/22044) discussed and addressed the exact problem.
> > You may follow the instruction in those threads or google the solution by yourself, or post issue to the tensorflow repo.
> > I'm closing this issue as it's unrelated to `bert-as-service`.
> 
> Sure. Thanks!

so may I know how did u manage to solve this issue??
",thanks see invalid syntax raised found problem issue well exact problem may follow instruction solution post issue issue unrelated sure thanks may know manage solve issue,issue,positive,positive,positive,positive,positive,positive
452252415,"Windows pickle error caused by multiprocessing can't pickle logger , it can be solved by removing logger.
Another common issue in windows is znq tcp connect .
origin address is tcp://0.0.0.0:port, should be replaced by tcp://127.0.0.1:port because origin address will not caught by recv() and result in waiting for long time",pickle error ca pickle logger removing logger another common issue connect origin address port port origin address caught result waiting long time,issue,negative,negative,negative,negative,negative,negative
452167985,"you can set num_worker according to the resources you have, doesn’t have to be one.

Feel free to change entrypoint.sh, it’s just an example.",set according one feel free change example,issue,positive,positive,positive,positive,positive,positive
452159843,Also the client hangs forever when server crashes even when timeout=5000 is set.,also client forever server even set,issue,negative,neutral,neutral,neutral,neutral,neutral
452159763,"I already use  pip install -U bert-serving-server bert-serving-client  ,then run ' bert-serving-start -model_dir chinese_L-12_H-768_A-12/ -num_worker=1 -pooling_strategy=REDUCE_MEAN_MAX -cpu -max_batch_size 16 -verbose'",already use pip install run,issue,negative,neutral,neutral,neutral,neutral,neutral
452150989,"1. use this command  :
bert-serving-start -model_dir chinese_L-12_H-768_A-12/ -num_worker=1 -pooling_strategy=REDUCE_MEAN_MAX -cpu -max_batch_size 16 -verbose

2.last screen   :
2019-01-08 10:02:45.629528: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-08 10:02:45.638923: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
I:GRAPHOPT:[gra:opt:121]:load parameters from checkpoint...
I:GRAPHOPT:[gra:opt:123]:freeze...
INFO:tensorflow:Froze 181 variables.
INFO:tensorflow:Converted 181 variables to const ops.
I:GRAPHOPT:[gra:opt:126]:optimize...:",use command screen binary use new thread pool default inter setting tune best performance gra opt load gra opt freeze froze converted gra opt optimize,issue,positive,positive,positive,positive,positive,positive
452149567,"![image](https://user-images.githubusercontent.com/2417391/50805241-77265380-132c-11e9-9066-34fe7584f48a.png)

We ran into a similar issue on version 1.6.4. The case seemingly appears at random and is not reliably reproducible.",image ran similar issue version case seemingly random reliably reproducible,issue,negative,negative,negative,negative,negative,negative
452141277,"> hmm, will check

I have read through some materials about Tensorflow. Is it because that all the data has been added for the number of rounds being setted, and we still trying to add them into the queue??
",check read data added number still trying add queue,issue,negative,neutral,neutral,neutral,neutral,neutral
451995357,"> `-num_worker=$1` means taking first argument of `entrypoint.sh` as number of workers, whereas `-num_worker=1` means single worker only.

So the $NUM_WORKERS must be valued before using it according to your README.md, I think?
```
NUM_WORKER=1
docker run --runtime nvidia -dit -p 5555:5555 -p 5556:5556 -v $PATH_MODEL:/model -t bert-as-service $NUM_WORKER
```

Otherwise, use 1 instead of $1?",taking first argument number whereas single worker must valued according think docker run otherwise use instead,issue,negative,positive,neutral,neutral,positive,positive
451990427,"`-num_worker=$1` means taking first argument of `entrypoint.sh` as number of workers, whereas `-num_worker=1` means single worker only.",taking first argument number whereas single worker,issue,negative,positive,neutral,neutral,positive,positive
451987997,maybe it's better to move set linger to `BertClient.close()`,maybe better move set linger,issue,negative,positive,positive,positive,positive,positive
451946506,"It worked 
There was issue with the RAM size",worked issue ram size,issue,negative,neutral,neutral,neutral,neutral,neutral
451941792,"got same error on windows, could be a common multiprocessing issue.",got error could common issue,issue,negative,negative,negative,negative,negative,negative
451929014,"- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): NAME=""Amazon Linux AMI""
VERSION=""2018.03""
ID=""amzn""
ID_LIKE=""rhel fedora""
VERSION_ID=""2018.03""
PRETTY_NAME=""Amazon Linux AMI 2018.03""
ANSI_COLOR=""0;33""
CPE_NAME=""cpe:/o:amazon:linux:2018.03:ga""

- TensorFlow installed from (source or binary): preinstalled on server
- TensorFlow version: 1.12.0
- Python version: 3.6.5
- `bert-as-service` version: 1.6.6
- CPU model and memory: Intel(R) Xeon(R) CPU E5-2676 v3 @ 2.40GHz  mem - 1009432 kB

---",o platform distribution ami ami ga source binary server version python version version model memory mem,issue,negative,neutral,neutral,neutral,neutral,neutral
451925724,could you provide the `tensorflow` version and `bert-as-service` version number? please fill in the issue form.,could provide version version number please fill issue form,issue,negative,neutral,neutral,neutral,neutral,neutral
451636210,"`example4.py` and `example5.py` use the same data, which can be downloaded from here: https://github.com/thunlp/CAIL/blob/master/README_en.md#23-dataset-download

You will see the same directory structure after decompress it",use data see directory structure decompress,issue,negative,neutral,neutral,neutral,neutral,neutral
451636019,"If `pip install -U bert-serving-server bert-serving-client` does not solve your problem, then

1. please fill in the issue form, saving time for both of us;
2. please run server with `-verbose` and copy paste the last screen here.

",pip install solve problem please fill issue form saving time u please run server copy paste last screen,issue,negative,neutral,neutral,neutral,neutral,neutral
451632017,"Thank you very much! I use your serving for my work to embed words,but 768 dimension seems too large for my GPU.Before train models, when I try to save the embedded words to my tfrecords.It made my computer die. So I try to search for answers to  reduce the 768-dimentions or any other useful solution under the condition that cannot change computer reccently.Thank you very much!",thank much use serving work embed dimension large train try save made computer die try search reduce useful solution condition change computer much,issue,positive,positive,positive,positive,positive,positive
451631719,"""if you want 256-dimensional vector, either add a dense(num_units=256) then train on some downstream task, or do subsampling directly in the feature space (make sense if your goal is to build a random forest)""
As you talked about reducing dimensions above. Can you explain the solution in detail? ""Add a dense"",what does that mean? Where can we connect the dense like this? Also the sentence you said: ""subsampling directly in the feature space "",I don't quite understand this, could you explain it in a little more detail?",want vector either add dense train downstream task directly feature space make sense goal build random forest reducing explain solution detail add dense mean connect dense like also sentence said directly feature space quite understand could explain little detail,issue,positive,negative,negative,negative,negative,negative
451279282,"fyi, these two issues are fixed in #171 and the new feature is available since 1.6.6. Please do the following for the upgrade:
```bash
pip install -U bert-serving-server bert-serving-client
```

1. To get the tokenization information, you can add `-verbose` when starting the server. This enables the logging for tokenization, mask etc. For now, such information is not available on the client side.
2. You can add `-mask_cls_sep` when starting the server. This will set the embedding of [CLS] and [SEP] to zero before pooling. If you `-pooling_strategy` is one of `{CLS_TOKEN, FIRST_TOKEN, SEP_TOKEN, LAST_TOKEN}`, then the embedding on [CLS] and [SEP] are preserved.",two fixed new feature available since please following upgrade bash pip install get information add starting server logging mask information available client side add starting server set zero one,issue,negative,positive,positive,positive,positive,positive
451001009,Both are very good points and I will implement them in the next version. Thanks!,good implement next version thanks,issue,positive,positive,positive,positive,positive,positive
450929723,"The embedding model is deterministic, meaning same inputs should always result the same output (despite numerical jitter, i.e. abs<1e-6). Otherwise, it is wrong.

So maybe there is some bug in your code when calling the original BERT, I'm not sure. But since the issue is not about `bert-as-service`, I suggest you open an issue in the original BERT repo.",model deterministic meaning always result output despite numerical jitter otherwise wrong maybe bug code calling original sure since issue suggest open issue original,issue,negative,positive,positive,positive,positive,positive
450926247,"yes, it supports Korean, but only if you are using [BERT-Base, Multilingual Cased (New)](https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip). Otherwise all Korean tokens will be considered as `<UNK>`, which explains your observation.",yes multilingual cased new otherwise considered observation,issue,negative,positive,positive,positive,positive,positive
450800420,"There's some misunderstanding.
For making sure, I'm sharing original BERT output vectors for 'hello world'

1) first time
[CLS] [0.476536, 0.372158, 1.465642]
hello [0.111987, 1.128579, 0.990834]
world [0.416854, 1.073154, 1.456178]
[SEP] [-0.338621, 0.568671, 0.049002]

2) second time
[CLS] [0.842703, 0.942707, 0.420058]
hello [0.866664, 0.40403, -0.005693]
world [1.921718, -0.325165, -1.704712]
[SEP] [-0.475752, 0.405486, -1.818929]

As you can see, I've got different values on same sentence.
But bert-as-service always give me same value on same sentence.
I'd like to know how it is possible.
Can you give me more advise on that?
Thank you :)",misunderstanding making sure original output first time hello world second time hello world see got different sentence always give value sentence like know possible give advise thank,issue,positive,positive,positive,positive,positive,positive
450789062,"The original `extract_features.py` is using the embedding of the first token as the sentence embedding, which only make sense after fine-tuning the model on the downstream task.

Please read the FAQ for more details.
- https://github.com/hanxiao/bert-as-service#q-what-are-the-available-pooling-strategies
- https://github.com/hanxiao/bert-as-service#q-why-not-use-the-hidden-state-of-the-first-token-as-default-strategy-ie-the-cls
- https://github.com/hanxiao/bert-as-service#q-bert-has-1224-layers-so-which-layer-are-you-talking-about
- https://github.com/hanxiao/bert-as-service#q-why-not-the-last-hidden-layer-why-second-to-last

And a blog post explaining why in more details: https://hanxiao.github.io/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/
",original first token sentence make sense model downstream task please read post explaining,issue,positive,positive,positive,positive,positive,positive
450714110,"The design philosophy of `bert-as-service` can be found in my blog post: https://hanxiao.github.io/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/

Close",design philosophy found post close,issue,negative,neutral,neutral,neutral,neutral,neutral
450481153,"close, unrelated to bert-as-service. note that this a place for reporting issues related to this repo only.",close unrelated note place related,issue,negative,neutral,neutral,neutral,neutral,neutral
450260252,"> Thanks gotcha. Now I see the `invalid syntax` is raised from `single_image_random_dot_sterograms` not from `bert_as_service`. After googling, I found it's a tensorflow problem, and [this issue](https://github.com/tensorflow/tensorflow/issues/21164) as well as [this PR](https://github.com/tensorflow/tensorflow/pull/22044) discussed and addressed the exact problem.
> 
> You may follow the instruction in those threads or google the solution by yourself, or post issue to the tensorflow repo.
> 
> I'm closing this issue as it's unrelated to `bert-as-service`.

Sure. Thanks!",thanks see invalid syntax raised found problem issue well exact problem may follow instruction solution post issue issue unrelated sure thanks,issue,positive,positive,positive,positive,positive,positive
450257621,"Thanks gotcha. Now I see the `invalid syntax` is raised from `single_image_random_dot_sterograms` not from `bert_as_service`. After googling, I found it's a tensorflow problem, and [this issue](https://github.com/tensorflow/tensorflow/issues/21164) as well as [this PR](https://github.com/tensorflow/tensorflow/pull/22044) discussed and addressed the exact problem.

You may follow the instruction in those threads or google the solution by yourself, or post issue to the tensorflow repo.

I'm closing this issue as it's unrelated to `bert-as-service`.",thanks see invalid syntax raised found problem issue well exact problem may follow instruction solution post issue issue unrelated,issue,negative,positive,positive,positive,positive,positive
450212748,"Thanks! I updated the modules and tried again. Here is the error report. 
![1227](https://user-images.githubusercontent.com/33407613/50491685-67bc1780-09c8-11e9-9efe-07a1d4a23275.png)
",thanks tried error report,issue,negative,positive,positive,positive,positive,positive
450124093,"thanks, I just included the traceback for the graph optimization part, could you update via
```bash
pip install -U bert-serving-server bert-serving-client
```
and give the new version a try? It wasn't for solving this bug, but for providing more details.

please copy paste the server log here, thanks
",thanks included graph optimization part could update via bash pip install give new version try bug providing please copy paste server log thanks,issue,positive,positive,positive,positive,positive,positive
450123666,"fyi, the issue is fixed in #164 and is available since 1.6.5, please do the following for the update:

```bash
pip install -U bert-serving-server bert-serving-client
```
",issue fixed available since please following update bash pip install,issue,negative,positive,positive,positive,positive,positive
450090940,"despite your imperfect way of using BertClient before, you pointed out a bug on the server side. It is a very corner case, but yes it is a bug. It is solved in #164 ",despite imperfect way pointed bug server side corner case yes bug,issue,positive,neutral,neutral,neutral,neutral,neutral
450070682,"> Could you please double check the python version on the cloud server? I found this line is particular suspicious.
> 
> ```
> invalid syntax (<string>, line 28)
> ```
> which points to this place:
> [bert-as-service/server/bert_serving/server/graph.py](https://github.com/hanxiao/bert-as-service/blob/5e5bad07911dc44fd959dec75d7fb56bc7ac5f53/server/bert_serving/server/graph.py#L28-L30)
> 
> Lines 28 to 30 in [5e5bad0](/hanxiao/bert-as-service/commit/5e5bad07911dc44fd959dec75d7fb56bc7ac5f53)
> 
>  @staticmethod 
>  def from_string(s): 
>      try: 
> 
> This often indicates a wrong python version (2 instead of 3, or some other versions that don't appreciate `@staticmethod` in `Enum`); or some random typos in the code. We can rule out the latter if you are installing `bert-serving-server` via pip. But that said, if you are using py2, the server shouldn't be able to start in the first place.

I checked my python version is 3.6.3.",could please double check python version cloud server found line particular suspicious invalid syntax string line place try often wrong python version instead appreciate random code rule latter via pip said server able start first place checked python version,issue,negative,negative,neutral,neutral,negative,negative
450069971,"@saravananpsg hmm, strange. I will add more logging there and print the full error traceback in the next version.",strange add logging print full error next version,issue,negative,positive,neutral,neutral,positive,positive
450069593,"Could you please double check the python version on the cloud server? I found this line is particular suspicious.
```
invalid syntax (<string>, line 28)
``` 

which points to this place:
https://github.com/hanxiao/bert-as-service/blob/5e5bad07911dc44fd959dec75d7fb56bc7ac5f53/server/bert_serving/server/graph.py#L28-L30
This often indicates a wrong python version (2 instead of 3, or some other versions that don't appreciate `@staticmethod` in `Enum`); or some random typos in the code. We can rule out the latter if you are installing `bert-serving-server` via pip. But that said, if you are using py2, the server shouldn't be able to start in the first place.",could please double check python version cloud server found line particular suspicious invalid syntax string line place often wrong python version instead appreciate random code rule latter via pip said server able start first place,issue,negative,negative,neutral,neutral,negative,negative
450069171,"@hanxiao Thank you for the repo. 

Even I had the same issue however when I re-downloaded the recommended pre-trained model, I solved this error. Not sure why it occurred in the first place. Also, I have increased the number of workers to 8

bert-serving-start -model_dir /tmp/multi_cased_L-12_H-768_A-12 -num_worker=8 
",thank even issue however model error sure first place also number,issue,negative,positive,positive,positive,positive,positive
450063352,"> @chiyuzhang94
> I met this problem when I try `BERT-Base, Chinese` and sovled it by redownload the pretrained model. Maybe your file is broken, have a MD5 check could help.
Thanks, I tried to use other models, it still didn't work. But any model can work on my local computer.
",met problem try model maybe file broken check could help thanks tried use still work model work local computer,issue,negative,negative,neutral,neutral,negative,negative
450063138,"> @chiyuzhang94 is this issue still there?

Hi Han, Thanks for your attention and sorry for the inconveniences. It still doesn't work. I am using a virtual environment with python 3.6 on a cloud server and version 1.6.4 for bert-serving-server. I unzipped BERT-Large, Uncased in a fold call ""uncased_L-24_H-1024_A-16"", and created a shell file bert-server.sh : `#!/bin/bash
bert-serving-start  -model_dir ~/py3.6/uncased_L-24_H-1024_A-16 -num_worker=8 
`
Then, I used salloc to run this job: `salloc --time=10:0:0 --ntasks=1 --cpus-per-task=8 --mem=16G --gres=gpu:1 --account=***** --job-name=bert-server srun $VIRTUAL_ENV/bert-server.sh`

Finally, the termial return such errors:
![screen shot 2018-12-26 at 7 07 28 pm](https://user-images.githubusercontent.com/33407613/50463588-83c8a600-0941-11e9-87ec-4c28c251ae2b.png)
",issue still hi han thanks attention sorry still work virtual environment python cloud server version uncased fold call shell file used run job finally return screen shot,issue,negative,negative,neutral,neutral,negative,negative
450052578,"To have a full control of tokenization, please refer to:
https://github.com/hanxiao/bert-as-service#using-your-own-tokenizer

make sure you are using the latest version via
```bash
pip install -U bert-serving-server bert-serving-clinet
```
",full control please refer make sure latest version via bash pip install,issue,positive,positive,positive,positive,positive,positive
449961301,"Good to know. 

Some technical details here: if you worry about ""long-term connection"" in `BertClient`, then please  don't. The `BertClient` does not persist a connection with server. It pushes the data and subscribes the result independently. This is designed on purpose to make the overall system more tangible and reliable, e.g. more robust to server restart.

One example you could try is starting the BertServer, then making a BertClient; then killing and restarting the server. Now using **the same** BertClient to communicate with server. Guess what, still works!

Why? If you look at the communication flow here https://github.com/hanxiao/bert-as-service#q-what-is-the-parallel-processing-model-behind-the-scene

![image](https://user-images.githubusercontent.com/2041322/50445934-f3905e00-094c-11e9-8e47-48b123d9e9e3.png)

Note, the client is using PUSH and SUBSCRIBE for communicating with the server. You can imagine them as ""one-time"" protocol, meaning no delay no wait for response, and are setup only on-demand.

",good know technical worry connection please persist connection server data result independently designed purpose make overall system tangible reliable robust server restart one example could try starting making killing server communicate server guess still work look communication flow image note client push subscribe communicating server imagine protocol meaning delay wait response setup,issue,negative,positive,positive,positive,positive,positive
449959476,"@chiyuzhang94 
I met this problem when I try `BERT-Base, Chinese` and sovled it by redownload the pretrained model. Maybe your file is broken, have a MD5 check could help.",met problem try model maybe file broken check could help,issue,negative,negative,negative,negative,negative,negative
449958667,"Thanks !  I have modified the code.
It's a Django  service ,  i was afraid  of causing  long-term connection  problems by persisting one BertClient in init part.",thanks code service afraid causing connection persisting one part,issue,negative,negative,negative,negative,negative,negative
449947229,"why can't you persist one BertClient and reuse it for every `get_recommand`?

```python
class YourClass:
  def __init__(self):
   self.bc = BertClient()

  def get_recommend(self, question):
    test_vec = self.bc.encode([question])
```

Why can't you do this?",ca persist one reuse every python class self self question question ca,issue,negative,neutral,neutral,neutral,neutral,neutral
449946651,"1、sorry...but i need to  encode one sentence a time , and create a new `BertClient()` after every request . Because i'm developing a service  to compare one new sentence with base sentences , which likes a  simple chatbot.
2、the full log:

> I:VENTILATOR[__i:run:133]:new config request        req id: 0       client: b'01f59169-0beb-4afb-a3dd-6932c43c6169'
> I:VENTILATOR[__i:run:133]:new config request        req id: 0       client: b'0d516f0e-a988-4257-bf6e-29fb33589ba2'
> I:SINK[__i:run:286]:send config     client b'01f59169-0beb-4afb-a3dd-6932c43c6169'
> I:VENTILATOR[__i:run:149]:new encode request        req id: 1       size: 1 client: b'01f59169-0beb-4afb-a3dd-6932c43c6169'
> I:WORKER-6[__i:gen:384]:new job     socket: 0       size: 1 client: b'01f59169-0beb-4afb-a3dd-6932c43c6169#1'
> I:WORKER-6[__i:run:364]:job done    size: (1, 768)  client: b'01f59169-0beb-4afb-a3dd-6932c43c6169#1'
> I:SINK[__i:run:286]:send config     client b'0d516f0e-a988-4257-bf6e-29fb33589ba2'
> I:VENTILATOR[__i:run:149]:new encode request        req id: 1       size: 1 client: b'0d516f0e-a988-4257-bf6e-29fb33589ba2'
> Process BertSink-2:
> Traceback (most recent call last):
>   File ""/usr/lib64/python3.6/multiprocessing/process.py"", line 258, in _bootstrap
>     self.run()
>   File ""/home/appuser/.local/lib/python3.6/site-packages/bert_serving/server/__init__.py"", line 225, in run
>     self._run()
>   File ""/usr/lib64/python3.6/site-packages/zmq/decorators.py"", line 75, in wrapper
>     return func(*args, **kwargs)
>   File ""/usr/lib64/python3.6/site-packages/zmq/decorators.py"", line 75, in wrapper
>     return func(*args, **kwargs)
>   File ""/usr/lib64/python3.6/site-packages/zmq/decorators.py"", line 75, in wrapper
>     return func(*args, **kwargs)
>   File ""/home/appuser/.local/lib/python3.6/site-packages/bert_serving/server/__init__.py"", line 264, in _run
>     job_checksum[job_id]))
> KeyError: b'01f59169-0beb-4afb-a3dd-6932c43c6169#1'

3、While every request to my service , i  would create  bc = BertClient() , use bc.encode() to get sentence encode , then bc.close() : 

>  def get_recommend(self,question):
>         bc = BertClient()
>         test_sentence = question
>         sentence_list = []
>         sentence_list.append(test_sentence)
>         test_vec =  bc.encode(sentence_list)
>         bc.close()
>         ……
",need encode one sentence time create new every request service compare one new sentence base simple full log ventilator run new request id client ventilator run new request id client sink run send client ventilator run new encode request id size client gen new job socket size client run job done size client sink run send client ventilator run new encode request id size client process recent call last file line file line run file line wrapper return file line wrapper return file line wrapper return file line every request service would create use get sentence encode self question question,issue,positive,positive,neutral,neutral,positive,positive
449938216,"I need more information about how you use `BertClient`.  Looks like some corner case that can not be reproduced accurately.

1. Is there any reason why you encode **one sentence a time**? Looks like you do! I said clearly in the README that encoding one sentence a time is a VERY VERY BAD idea: https://github.com/hanxiao/bert-as-service#speed-wrt-client_batch_size
I hope you are not creating a new `BertClient()` after every sentence, that's even worse. 

2. When you encounter the `KeyError`, e.g. `KeyError: b'01f59169-0beb-4afb-a3dd-6932c43c6169#1'`, could you search the server log for `01f59169-0beb-4afb-a3dd-6932c43c6169`? What did the `VENTILATOR`, `WORKER`, `SINK` say about this id? Could you copy paste the relevant lines here?

3. Did you use `BertClient.encode()` or `BertClient.encode_async()`?",need information use like corner case accurately reason encode one sentence time like said clearly one sentence time bad idea hope new every sentence even worse encounter could search server log ventilator worker sink say id could copy paste relevant use,issue,positive,negative,neutral,neutral,negative,negative
449931633,"yes, update bert-serving-start solved the ImportError
but about an hour later , KeyError again...

I:VENTILATOR:[__i:_ru:149]:new encode request   req id: 1       size: 1 client: b'0d516f0e-a988-4257-bf6e-29fb33589ba2'
Process BertSink-2:
Traceback (most recent call last):
  File ""/usr/lib64/python3.6/multiprocessing/process.py"", line 258, in _bootstrap
    self.run()
  File ""/home/appuser/.local/lib/python3.6/site-packages/bert_serving/server/__init__.py"", line 225, in run
    self._run()
  File ""/usr/lib64/python3.6/site-packages/zmq/decorators.py"", line 75, in wrapper
    return func(*args, **kwargs)
  File ""/usr/lib64/python3.6/site-packages/zmq/decorators.py"", line 75, in wrapper
    return func(*args, **kwargs)
  File ""/usr/lib64/python3.6/site-packages/zmq/decorators.py"", line 75, in wrapper
    return func(*args, **kwargs)
  File ""/home/appuser/.local/lib/python3.6/site-packages/bert_serving/server/__init__.py"", line 264, in _run
    job_checksum[job_id]))
KeyError: b'01f59169-0beb-4afb-a3dd-6932c43c6169#1'
",yes update hour later ventilator new encode request id size client process recent call last file line file line run file line wrapper return file line wrapper return file line wrapper return file line,issue,negative,positive,neutral,neutral,positive,positive
449928456,"Please fill out the issue form, those version and env-info help me diagnose the issue faster.

Also please copy paste the entire server log instead of just the last line.

I'm guessing you don't have permission to write on the cloud server. That's why optimize graph failed.",please fill issue form version help diagnose issue faster also please copy paste entire server log instead last line guessing permission write cloud server optimize graph,issue,positive,neutral,neutral,neutral,neutral,neutral
449918400,"no, i’m guessing you manually create ‘/bin/bert-serving-start’, which still
links to the old version. Please check.

D.D.Conan <notifications@github.com>于2018年12月26日 周三下午3:09写道：

> after i do
> pip install -U bert-serving-server bert-serving-client
> version now is
> bert-serving-client 1.6.4
> bert-serving-server 1.6.4
>
> then i start server :
> -bash-4.2$ bert-serving-start -model_dir ../multi_cased_L-12_H-768_A-12
> -num_worker=8 -cpu
> Traceback (most recent call last):
> File ""/bin/bert-serving-start"", line 9, in
> from bert_serving.server.bert.extract_features import PoolingStrategy
> ImportError: cannot import name 'PoolingStrategy'
>
> is there missing any python package to install ?
>
> —
> You are receiving this because you commented.
>
>
> Reply to this email directly, view it on GitHub
> <https://github.com/hanxiao/bert-as-service/issues/162#issuecomment-449918084>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AB8l6g8grHAck_piYIX36hcIVSBJq9wMks5u8yCtgaJpZM4ZhbpT>
> .
>
",guessing manually create still link old version please check pip install version start server recent call last file line import import name missing python package install reply directly view mute thread,issue,negative,neutral,neutral,neutral,neutral,neutral
449918084,"after i do 
pip install -U bert-serving-server bert-serving-client
version now is 
  bert-serving-client 1.6.4     
  bert-serving-server 1.6.4

then i start server :
-bash-4.2$  bert-serving-start -model_dir ../multi_cased_L-12_H-768_A-12 -num_worker=8  -cpu
Traceback (most recent call last):
  File ""/bin/bert-serving-start"", line 9, in <module>
    from bert_serving.server.bert.extract_features import PoolingStrategy
ImportError: cannot import name 'PoolingStrategy'

is there missing any python package to install ?",pip install version start server recent call last file line module import import name missing python package install,issue,negative,negative,neutral,neutral,negative,negative
449916641,"version too old, please first do

pip install -U bert-serving-server bert-serving-client

if the problem still arises, then please report it here again.

D.D.Conan <notifications@github.com>于2018年12月26日 周三下午2:49写道：

> Process BertSink-1:
> Traceback (most recent call last):
> File ""/usr/lib64/python3.6/multiprocessing/process.py"", line 258, in
> _bootstrap
> self.run()
> File ""/usr/lib/python3.6/site-packages/bert_serving/server/*init*.py"",
> line 226, in run
> job_checksum[job_id]))
> KeyError: b'd4853427-8e43-48ee-8b8f-a24eb357face#1'
>
> After this , i can't connect the server to get sentence encode , how to
> solve it?
> release:
> python3.6
> pip list：
> bert-serving-client 1.5.0
> bert-serving-server 1.5.0
> server is started by :
> nohup bert-serving-start -model_dir ../multi_cased_L-12_H-768_A-12
> -num_worker=8 -cpu &
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/hanxiao/bert-as-service/issues/162>, or mute the
> thread
> <https://github.com/notifications/unsubscribe-auth/AB8l6mTW-sqIsWkp169yQruqtBTXUEyMks5u8xvcgaJpZM4ZhbpT>
> .
>
",version old please first pip install problem still please report process recent call last file line file line run ca connect server get sentence encode solve release python pip server thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
449892625,"Finally, I give up building on windows.
Now i building on Centos7 and test OK",finally give building building test,issue,negative,neutral,neutral,neutral,neutral,neutral
449891547,"I also test this service(1.5.8) in Ubuntu(14.04), and got same error, but after update service to 1.6.4, it's OK, 
I don't recommend to hack system code.",also test service got error update service recommend hack system code,issue,negative,neutral,neutral,neutral,neutral,neutral
449890015,"@HuarongLi Good to know! Thanks a lot.

I'd like to encourage everyone who encountered this problem on Windows system (apparently this is Windows-only problem) give a try on @HuarongLi solution or provide other clues. As I don't have any Windows dev-env on hands, it would be very difficult for me to verify this issue.",good know thanks lot like encourage everyone problem system apparently problem give try solution provide would difficult verify issue,issue,positive,positive,positive,positive,positive,positive
449888911,"> > I run this command:
> > python C:\Users\Liheng\Desktop\BERT\bert-serving-start.py -model_dir C:\Users\Liheng\Desktop\BERT\model\cased_L-12_H-768_A-12\ -num_worker=1,
> > but thie issue occurs:
> > ![image](https://user-images.githubusercontent.com/24928240/49624309-e21dfb00-fa0b-11e8-88bd-a629cca92d93.png)
> 
> Same error, how to solve it? Thx!

The problem seems like this:
[https://github.com/chainer/chainerrl/issues/175](url)
and I got it to run by hacking multiprocessing.

reduction.py:
``` diff
- import pickle
+ import dill as pickle
```
Maybe it will broke something else，but the error is solved",run command python issue image error solve problem like got run hacking import pickle import dill pickle maybe broke something error,issue,negative,neutral,neutral,neutral,neutral,neutral
449853417,"It is a feature extraction service, no prediction is provided. To do prediction, you need to build your own downstream network using this service. Please refer to this example https://github.com/hanxiao/bert-as-service/blob/master/README.md#training-a-text-classifier-using-bert-features-and-tfestimator-api",feature extraction service prediction provided prediction need build downstream network service please refer example,issue,negative,neutral,neutral,neutral,neutral,neutral
449838900,"nice that you notice this. 

I've tried the simple way. However, there will be some weird side-effect when the `BertServer` calls tensorflow and then start the `BertWorker` (which calls tensorflow again). 

One example I remember is once `tf.ConfigProto(device_count={'GPU': 0})` is set in the process of `BertServer`, the worker won't be able to find any GPU how ever you config `tf.ConfigProto` in its own process, sometimes the worker is just hanging there forever. Seems that some environment variables are unexpectedly inherited from `BertServer` and affect `BertWorker` mysteriously.

I did encounter this problem on the self-compiled `tensorflow-gpu` 1.10. One may claim the other way on newer or customized versions.

That said, the best way to work with tensorflow in multi-process environment is make a strict separation (starting from `import tensorflow as tf`) across processes. It saves a lot of troubles.

Reference: 
- Checkout my old blog post mentioned this issue: https://hanxiao.github.io/2017/07/07/Get-10x-Speedup-in-Tensorflow-Multi-Task-Learning-using-Python-Multiprocessing/#using-tensorflow-for-preprocessing-in-subprocess
- this issue: https://github.com/tensorflow/tensorflow/issues/8220",nice notice tried simple way however weird start one example remember set process worker wo able find ever process sometimes worker hanging forever environment unexpectedly affect mysteriously encounter problem one may claim way said best way work environment make strict separation starting import across lot reference old post issue issue,issue,positive,positive,positive,positive,positive,positive
449815343,"hmm, weird. but glad to hear that! close",weird glad hear close,issue,negative,neutral,neutral,neutral,neutral,neutral
449815167,"I don't know why this error cause, but i re-install ubuntu and virtual env , then it works! 
Anyway, thanks for your awesome works.",know error cause virtual work anyway thanks awesome work,issue,positive,positive,positive,positive,positive,positive
449797783,"hmm, it actually doesn't work in the way as you expect. Note that unlike word2vec/Glove/fastText, word embedding from BERT is *contextualized*, meaning that the value of `woman` embedding depends on the other words in the sentence, which also means that two `woman` can have completely different embedding values when they appear in two different sentences. This makes ""most similar words"" function spurious for `bert-as-service`",actually work way expect note unlike word meaning value woman sentence also two woman completely different appear two different similar function spurious,issue,negative,neutral,neutral,neutral,neutral,neutral
449797323,the whole graph optimization should be done in at most 1 minute. Seems that something wrong at `graph.py:124`. Did you modify the `cased_L...` model or is it the original pretrained model released by Google?,whole graph optimization done minute something wrong modify model original model,issue,negative,positive,neutral,neutral,positive,positive
449280038,after upgrade the version.it works fine now .thank you very much,upgrade work fine much,issue,negative,positive,positive,positive,positive,positive
449043911,"Hi @basque21,
I followed this guide and it's a good start for approaching ELMo:
https://towardsdatascience.com/how-to-use-elmo-word-vectors-for-spam-classification-1891c0da8f1d

",hi basque guide good start approaching,issue,negative,positive,positive,positive,positive,positive
448988852,"check your BertClient port, apparently you are connecting to an old (and living!) 1.5.7 server.",check port apparently old living server,issue,negative,positive,neutral,neutral,positive,positive
448987555,"commond：
bert-serving-start -model_dir ./chinese_L-12_H-768_A-12/ -max_seq_len 128 -port 5557 -port_out 5558 -pooling_strategy REDUCE_MEAN -num_worker=1

but the package info：
(bert) pengwei@ubuntu:~/bert-as-service-master$ conda list

#packages in environment at /home/pengwei/.conda/envs/bert:
#Name                    Version                   Build  Channel
_tflow_select             2.3.0                       mkl  
absl-py                   0.6.1                    py36_0  
astor                     0.7.1                    py36_0  
bert-serving-client       1.6.2                     <pip>
bert-serving-server       1.6.2                     <pip>
blas                      1.0                         mkl  

the return is：
AttributeError: version mismatch! server version is 1.5.7 but client version is 1.6.2!
consider ""pip install -U bert-serving-server bert-serving-client""
or disable version-check by ""BertClient(check_version=False)""
",package list environment name version build channel astor pip pip blas return version mismatch server version client version consider pip install disable,issue,negative,neutral,neutral,neutral,neutral,neutral
448967781,"fyi, this issue is fixed in #155 and is available since 1.6.2, please do 
```bash
pip install -U bert-serving-server bert-serving-client
```
for the update.
",issue fixed available since please bash pip install update,issue,negative,positive,positive,positive,positive,positive
448952974,"@anpark 

> Same question, any advice?

Apparently you didn't read my answer or read faq or search for past issues. There is no point to ask same question again, saving time for both of us.

https://github.com/hanxiao/bert-as-service#q-the-cosine-similarity-of-two-sentence-vectors-is-unreasonably-high-eg-always--08-whats-wrong

#132 #30",question advice apparently read answer read search past point ask question saving time u,issue,negative,negative,neutral,neutral,negative,negative
448952517,"apparently you didn't read faq and search for past issues. there is no point to ask same question again, saving time for both of us.

https://github.com/hanxiao/bert-as-service#q-the-cosine-similarity-of-two-sentence-vectors-is-unreasonably-high-eg-always--08-whats-wrong

#132 #30",apparently read search past point ask question saving time u,issue,negative,negative,neutral,neutral,negative,negative
448909610,"@BingqiMiao  thanks.  Actually I want to get similarity between two labels.  BERT don't include out-of-box sim tools, hope @hanxiao can share ?",thanks actually want get similarity two include hope share,issue,positive,positive,neutral,neutral,positive,positive
448907091,"@anpark fine tune bert as a bi-classifier. I follow the demo of MRPC classification task of bert, result seems ok. But this is a supervised learning.",fine tune follow classification task result learning,issue,negative,positive,positive,positive,positive,positive
448844146,"> Before changing your model or doing fine-tuning, I **strongly suggest** you to use the latest `bert-as-service` and give a retry. Please do
> 
> ```shell
> pip install -U bert-serving-server bert-serving-client
> ```
> 
> Based on your server command and the form, I suspect you are using a pretty old version, which was even not installed via pip. Note that, there was some bugs in previous version, and was fixed [since 1.5.0](https://github.com/hanxiao/bert-as-service/releases?after=v1.5.2). These bugs may degrade the accuracy.

OK,thanks.I will try it.",model strongly suggest use latest give retry please shell pip install based server command form suspect pretty old version even via pip note previous version fixed since may degrade accuracy try,issue,negative,positive,positive,positive,positive,positive
448838276,ok got it. i think it's same as #90  which happens only on tf 1.12 CPU version. will do a fix.,got think version fix,issue,negative,neutral,neutral,neutral,neutral,neutral
448836943,"got it, may relate to https://github.com/hanxiao/bert-as-service/issues/90#issuecomment-446074747

could you do `bc.server_status` and `bc.status` and paste the result here? remember to remove sensitive info for the sake of your privacy.",got may relate could paste result remember remove sensitive sake privacy,issue,negative,positive,neutral,neutral,positive,positive
448835180,"1. please fill the issue form, save time for both of us.
2. please provide detailed server logs.",please fill issue form save time u please provide detailed server,issue,positive,positive,positive,positive,positive,positive
448834873,"Before changing your model or doing fine-tuning, I **strongly suggest** you to use the latest `bert-as-service` and give a retry. Please do
```bash
pip install -U bert-serving-server bert-serving-client
```
Based on your server command and the form, I suspect you are using a pretty old version, which was even not installed via pip. Note that, there was some bugs in previous version, and was fixed [since 1.5.0](https://github.com/hanxiao/bert-as-service/releases?after=v1.5.2). These bugs may degrade the accuracy.",model strongly suggest use latest give retry please bash pip install based server command form suspect pretty old version even via pip note previous version fixed since may degrade accuracy,issue,negative,positive,positive,positive,positive,positive
448832651,"> could you fill in the issue form so that I can know which version of `bert-as-service` are you using?

OK，I have update the issue form.
If I fine tune the model,will it take a long time?
Thank you!",could fill issue form know version update issue form fine tune model take long time thank,issue,positive,positive,positive,positive,positive,positive
448831338,"absolute no need.

in fact, you can get embedding on the [CLS] [SEP] token if you like, see: https://github.com/hanxiao/bert-as-service#q-what-are-the-available-pooling-strategies",absolute need fact get token like see,issue,negative,positive,positive,positive,positive,positive
448826705,could you fill in the issue form so that I can know which version of `bert-as-service` are you using?,could fill issue form know version,issue,negative,neutral,neutral,neutral,neutral,neutral
448823497,"> could you paste the command for starting the server?

python app.py -pooling_strategy NONE -model_dir ./chinese_L-12_H-768_A-12/ -num_worker=1
",could paste command starting server python none,issue,negative,neutral,neutral,neutral,neutral,neutral
448552519,"Same problem here, validation accuracy not improving. Any help? and, @simonefrancia could you give me some tutorial or code where it is trained with Elmo embeddings? Are your results much more better? 
thanks!",problem validation accuracy improving help could give tutorial code trained much better thanks,issue,positive,positive,positive,positive,positive,positive
448550645,"Is upgrading to the latest version solve the problem?

```bash
pip install -U bert-serving-server bert-serving-client
```
",latest version solve problem bash pip install,issue,negative,positive,positive,positive,positive,positive
448544136,"Starting from 1.5.5, `bert-as-service` freeze and optimize the graph before serving to reduce the memory footprint. The GPU memory consumption of `bert-as-service` should be constant regardless pretrained/fine-tuned model, and it is even smaller when you use deeper `pooling_layer`. Please do 
```bash
pip install -U bert-serving-server bert-serving-client
```
for the upgrade.
",starting freeze optimize graph serving reduce memory footprint memory consumption constant regardless model even smaller use please bash pip install upgrade,issue,positive,neutral,neutral,neutral,neutral,neutral
448542999,"Yes, other pooling blocks need additional training. Note, starting from 1.5.5 `bert-as-service` freezes and optimizes the graph during preprocessing and thus can be considered as a *feed-forward-only* library, which concentrates only on the inference stage.

Support training in `bert-as-service` is not my priority now. One exception is dimensionality reduction (mentioned in #62 ), one could train a 768-64-768 MLP on top with L1/L2 loss fairly easy.  This could be a next feature.",yes need additional training note starting graph thus considered library inference stage support training priority one exception dimensionality reduction one could train top loss fairly easy could next feature,issue,positive,positive,positive,positive,positive,positive
448538876,"`example5.py` is a very simple toy example for showing how to use it with `tf.Estimator`, it's a starting point to guide you building more sophisticated models. That said, there are indeed many ways to improve `example5.py`, but that's not my priority now. Contributions are welcome.

For fine-tuning script, please refer to Google's [`run_classifier.py`](https://github.com/google-research/bert/blob/master/run_classifier.py).
For serving a fine-tuned model using `bert-as-service`, please follow https://github.com/hanxiao/bert-as-service#serving-a-fine-tuned-bert-model
",simple toy example showing use starting point guide building sophisticated said indeed many way improve priority welcome script please refer serving model please follow,issue,positive,positive,positive,positive,positive,positive
448537203,could you paste the command for starting the server?,could paste command starting server,issue,negative,neutral,neutral,neutral,neutral,neutral
448537022,"Doesn't looks like the issue of this project. And without any information provided in the form, I can only suggest you to upgrade your `zmq` and `pyzmq` to the latest version.",like issue project without information provided form suggest upgrade latest version,issue,negative,positive,positive,positive,positive,positive
448533575,"@Colanim Since 1.5.5 `bert-as-service` supports customized Tokenization (see: https://github.com/hanxiao/bert-as-service#using-your-own-tokenizer). 

As for @MichaelZhouwang :
1. You should use `bc.encode([['I', 'anticipate', 'that']], is_tokenized=True)`, note it's `[[]]` not `[]` as in your OP. I believe it's just your typo, otherwise it won't pass the type check on the client side.
2. When you set `pooling_strategy=NONE` on the server side, the server won't perform any pooling strategy, but return a `[batch_size, max_seq_len, 768]` tensor as it is.
3. When you call the server via `bc.encode(is_tokenized=True)` from the client, you are telling the server to **respect** your input as tokenized and **do not** tokenize your words again. Thus there are two possibilities:

- (a) `anticipate` is in server's vocabulary. All good, you get the avg. of `I`, `anticipate`, `that` when using `pooling_strategy=REDUCE_MEAN` for instance.
- (b) `anticipate` is *not* in the server's vocabulary, this word is marked as `<UNK>` and you get the avg. of `I`, `<UNK>`, `that` when using `pooling_strategy=REDUCE_MEAN` for instance.",since see use note believe typo otherwise wo pas type check client side set server side server wo perform strategy return tensor call server via client telling server respect input thus two anticipate server vocabulary good get anticipate instance anticipate server vocabulary word marked get instance,issue,positive,positive,positive,positive,positive,positive
448528800,"fyi, serving a fine-tuned BERT model for feature extraction is fixed in #147 and available since 1.6.1. Please do 
```bash
pip install -U bert-serving-server bert-serving-client
```

More on this topics: https://github.com/hanxiao/bert-as-service#serving-a-fine-tuned-bert-model",serving model feature extraction fixed available since please bash pip install,issue,negative,positive,positive,positive,positive,positive
448527639,"> Do you have an implementation that works correctly ?

Yes, I used the same model(except the word embedding that I initialized it randomly) and had a not bad results whose accuracy is 70%,however, the bert in this github is only 24%.
The word embedding in my code about bert is as follow：
`    
        
    for epoch in range(config.num_epochs):  
        batch_train = batch_iter(x_train, y_train, config.batch_size)
        for x_batch, y_batch in batch_train:
            x_batch = bc.encode(x_batch)
            feed_dict = feed_data(x_batch, y_batch, config.dropout_keep_prob)
            session.run(model.optim, feed_dict=feed_dict)  # 运行优化
            total_batch += 1`
Here， the x_batch is（128，80，768）

The total code of my model is：
`
#!/usr/bin/python

import tensorflow as tf
from data.cnews_loader import attention
# from service.client import BertClient

class TRNNConfig(object):
    """"""RNN配置参数""""""

    # 模型参数
    embedding_dim = 768      # 词向量维度
    seq_length = 80        # 序列长度
    num_classes = 10        # 类别数
    vocab_size = 5000       # 词汇表达小

    num_layers= 2           # 隐藏层层数
    hidden_dim = 128        # 隐藏层神经元
    rnn = 'gru'             # lstm 或 gru

    attention_dim = 50
    l2_reg_lambda = 0.01

    dropout_keep_prob = 1.0 # dropout保留比例
    learning_rate = 1e-3    # 学习率

    batch_size = 128         # 每批训练大小
    num_epochs = 20          # 总迭代轮次

    print_per_batch = 100    # 每多少轮输出一次结果
    save_per_batch = 20      # 每多少轮存入tensorboard
class TextRNN(object):
    """"""文本分类，RNN模型""""""
    def __init__(self, config):
        self.config = config

        # 三个待输入的数据
        # self.input_x = tf.placeholder(tf.int32, [None, self.config.seq_length], name='input_x')
        self.input_x = tf.placeholder(tf.float32, [None, self.config.seq_length, self.config.embedding_dim], name='input_x')
        self.input_y = tf.placeholder(tf.float32, [None, self.config.num_classes], name='input_y')
        self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')

        self.rnn()

    def rnn(self):
        """"""rnn模型""""""

        def lstm_cell():   # lstm核
            return tf.contrib.rnn.BasicLSTMCell(self.config.hidden_dim, state_is_tuple=True)

        def gru_cell():  # gru核
            return tf.contrib.rnn.GRUCell(self.config.hidden_dim)

        def dropout(): # 为每一个rnn核后面加一个dropout层
            if (self.config.rnn == 'lstm'):
                cell = lstm_cell()
            else:
                cell = gru_cell()
            return tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob)

        # 词向量映射
        # with tf.device('/cpu:0'):
        #     embedding = tf.get_variable('embedding', [self.config.vocab_size, self.config.embedding_dim])
        #     embedding_inputs = tf.nn.embedding_lookup(embedding, self.input_x)

        with tf.name_scope(""rnn""):
            # 多层rnn网络
            cells = [dropout() for _ in range(self.config.num_layers)]
            # cell = dropout()
            rnn_cell = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)
            # (batch_size, num_step, embeddings)  (b, 80, 128)
            _outputs, _ = tf.nn.dynamic_rnn(cell=rnn_cell, inputs=self.input_x, dtype=tf.float32)

            # new
            # output = attention(_outputs, self.config.attention_dim, self.config.l2_reg_lambda)
            # (b, 128)
            last = _outputs[:, -1, :]  # 取最后一个时序输出作为结果(b, 128)

        with tf.name_scope(""score""):
            # bc = BertClient()
            # self.input (128)
            # output = bc.encode(self.input_x)  # (batch_size, 768)
            # 全连接层，后面接dropout以及relu激活
            fc = tf.layers.dense(last, self.config.hidden_dim, name='fc1')
            fc = tf.contrib.layers.dropout(fc, self.keep_prob)
            fc = tf.nn.relu(fc)
            # fc = tf.layers.dense(fc, 512, name='fc2')
            # fc = tf.contrib.layers.dropout(fc, self.keep_prob)
            # fc = tf.nn.relu(fc)
            #
            # fc = tf.layers.dense(fc, 256, name='fc3')
            # fc = tf.contrib.layers.dropout(fc, self.keep_prob)
            # fc = tf.nn.relu(fc)
            # fc = tf.layers.dense(fc, 128, name='fc4')
            # fc = tf.contrib.layers.dropout(fc, self.keep_prob)
            # fc = tf.nn.relu(fc)

            # 分类器(b, 128)->(b, 10)
            self.logits = tf.layers.dense(fc, self.config.num_classes, name='fc5')
            self.y_pred_cls = tf.argmax(tf.nn.softmax(self.logits), 1)  # 预测类别

        with tf.name_scope(""optimize""):
            # 损失函数，交叉熵
            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.input_y)
            self.loss = tf.reduce_mean(cross_entropy)
            # 优化器
            self.optim = tf.train.AdamOptimizer(learning_rate=self.config.learning_rate).minimize(self.loss)

        with tf.name_scope(""accuracy""):
            # 准确率
            correct_pred = tf.equal(tf.argmax(self.input_y, 1), self.y_pred_cls)
            self.acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))

`",implementation work correctly yes used model except word randomly bad whose accuracy however word code epoch range total code model import import attention import class object class object self none none none self return return dropout cell else cell return cell dropout range cell dropout new output attention last score output last optimize accuracy,issue,negative,negative,negative,negative,negative,negative
448465570,"It uses the Tokenizer of the official BERT.

So as you said, for `anticipate`, the word will be tokenized into `anti`, `##ci`, and `##pate`. With pooling strategy being `NONE`, you will get one embedding for each token, which means you will have one embedding for `anti`, one for `##ci` and one for `##pate`. It is then up to you to do whatever you want with these embeddings, you can indeed average them.

Using `REDUCE_MEAN` as pooling strategy will average **all** the tokens of the sentence.

---

_@hanxiao please correct me if I'm wrong_",official said anticipate word anti pate strategy none get one token one anti one one pate whatever want indeed average strategy average sentence please correct,issue,negative,negative,negative,negative,negative,negative
448455480,"I have used the sentence embedding to the model—— text classification, just connected a DNN and a classifier to predict the label(such as price, power, space, security and so on) base on comment on cars.
But the result is very bad.
Then I used the word embedding to the RNN and softmax,is bad too.I do not know why.",used sentence text classification connected classifier predict label price power space security base comment result bad used word bad know,issue,negative,negative,negative,negative,negative,negative
448448137,"> > For english sentences, if I feed a word that is not in the original vocabulary and take is as an token. Then how does the model return the embedding in the pooling_layer for me?
> 
> Depending on your setting on `-pooling_strategy` and `-pooling_layer`, see:
> https://github.com/hanxiao/bert-as-service#q-what-are-the-available-pooling-strategies
> https://github.com/hanxiao/bert-as-service#q-so-which-layer-and-which-pooling-strategy-is-the-best
> https://github.com/hanxiao/bert-as-service#server-side-configs
> 
> > Is the embedding for these word pieces get averaged?
> 
> By default yes, see
> https://github.com/hanxiao/bert-as-service#server-side-configs

Thanks a lot. However what I mean is not the pooling strategy of the input sequence, but for one token which is not in default vocabulary but treated as one token by setting is_tokenized=True. For example:
embedded = bc.encode(['I', 'anticipate', 'that'], is_tokenized=True) with pooling strategy = NONE. In this sentence, I try to get the embedding of 'anticipate', is this embedding calculated by averaging the embedding of anti, ##ci, and ##pate which are original wordpieces of anticipate? Thanks!!",feed word original vocabulary take token model return depending setting see word get default yes see thanks lot however mean strategy input sequence one token default vocabulary one token setting example strategy none sentence try get calculated anti pate original anticipate thanks,issue,positive,positive,positive,positive,positive,positive
448442504,"> For english sentences, if I feed a word that is not in the original vocabulary and take is as an token. Then how does the model return the embedding in the pooling_layer for me?

Depending on your setting on `-pooling_strategy` and `-pooling_layer`, see:
https://github.com/hanxiao/bert-as-service#q-what-are-the-available-pooling-strategies
https://github.com/hanxiao/bert-as-service#q-so-which-layer-and-which-pooling-strategy-is-the-best
https://github.com/hanxiao/bert-as-service#server-side-configs

> Is the embedding for these word pieces get averaged? 

By default yes, see
https://github.com/hanxiao/bert-as-service#server-side-configs
",feed word original vocabulary take token model return depending setting see word get default yes see,issue,positive,positive,positive,positive,positive,positive
448214788,Awesome. Keep up the good work.,awesome keep good work,issue,positive,positive,positive,positive,positive,positive
447787820,"ok, i'm reopening #90 and mark this thread as duplicate.

Frankly, hosting BertServer on Windows is not my priority at the moment; neither have I the Windows dev-environment. That's why I mark #90 as help-wanted.

As a workaround, you may run it in a Linux  container on Windows using docker, by following https://github.com/hanxiao/bert-as-service/#2-start-the-bert-service",mark thread duplicate frankly hosting priority moment neither mark may run container docker following,issue,negative,neutral,neutral,neutral,neutral,neutral
447752545,"fyi, this issue was fixed in #137 and the feature is available since 1.5.8, please do
```bash
pip install -U bert-serving-server bert-serving-client
```
More info see: https://github.com/hanxiao/bert-as-service#q-can-i-specify-which-gpu-to-use",issue fixed feature available since please bash pip install see,issue,negative,positive,positive,positive,positive,positive
447743928,please create a new issue and provide detailed traceback.,please create new issue provide detailed,issue,positive,positive,positive,positive,positive,positive
447741470,"> I run this command:
> python C:\Users\Liheng\Desktop\BERT\bert-serving-start.py -model_dir C:\Users\Liheng\Desktop\BERT\model\cased_L-12_H-768_A-12\ -num_worker=1,
> but thie issue occurs:
> ![image](https://user-images.githubusercontent.com/24928240/49624309-e21dfb00-fa0b-11e8-88bd-a629cca92d93.png)

Same error, how to solve it? Thx!",run command python issue image error solve,issue,negative,neutral,neutral,neutral,neutral,neutral
447739873,"But it can only controls the number of gpu to use, but not the specific gpu to use. I tried CUDA_VISIBLE_DEVICES too, it didn't work. Can you add an argument to specify the service in server-side? It is really important to me. Thanks :)",number use specific use tried work add argument specify service really important thanks,issue,negative,positive,positive,positive,positive,positive
447721345,"fyi, this feature was fixed in #136 and is available since 1.5.7, please do 
```bash
pip install -U bert-serving-server bert-serving-client
```",feature fixed available since please bash pip install,issue,negative,positive,positive,positive,positive,positive
447530220,"duplicate to #44 

Again, using absolute value to measure similarity is in every way a **bad idea**.  Reason:
- FAQ: https://github.com/hanxiao/bert-as-service#q-the-cosine-similarity-of-two-sentence-vectors-is-unreasonably-high-eg-always--08-whats-wrong
- https://github.com/hanxiao/bert-as-service/issues/44#issuecomment-441332196",duplicate absolute value measure similarity every way bad idea reason,issue,negative,negative,negative,negative,negative,negative
447329823,"@nmfisher Just bump to 1.5.5, with more ""multi-process friendly"" BertWorker. I do think this fix is related to your problem. Please kindly `pip install -U bert-serving-server bert-serving-client` to upgrade and try. Thanks!",bump friendly think fix related problem please kindly pip install upgrade try thanks,issue,positive,positive,positive,positive,positive,positive
447329232,"Starting from 1.5.5, `bert-as-server` implements the following pipeline:
1. load graph
2. freeze (constant-ized all variables) 
3. optimize (remove inference-irrelevant nodes)
4. serialize
5. serve graph

, for which you can say is similar to ""ExportModel()"" or ""SavedModelBuilder()"" in tf-serving. 

You can do `pip install -U bert-serving-server bert-serving-client` to upgrade.

Benchmarks shows this feature does *not* affect the inference speed or memory footprint significantly. So if you are hoping switching the communication stack to `tf-serving` can improve the efficiency, that's negative sign. 

Nonetheless, I do see it opens the possibility for more sophisticated graph optimization, e.g. using XLA.",starting following pipeline load graph freeze optimize remove serialize serve graph say similar pip install upgrade feature affect inference speed memory footprint significantly switching communication stack improve efficiency negative sign nonetheless see possibility sophisticated graph optimization,issue,positive,positive,neutral,neutral,positive,positive
447291081,"please, fill in the form. doesn't take so much time.

If you don't provide any information then you don't expect any useful reply.

plus, very likely your problem is not related to this repo, more like a general linux question.

close",please fill form take much time provide information expect useful reply plus likely problem related like general question close,issue,positive,positive,positive,positive,positive,positive
446972139,"no activity, close.

maybe related to #124 and #125

Please use the latest version 1.5.2 by
```bash
pip install -U bert-serving-client bert-serving-server
```
and follows this tutorial: https://github.com/hanxiao/bert-as-service#using-your-own-tokenizer",activity close maybe related please use latest version bash pip install tutorial,issue,negative,positive,positive,positive,positive,positive
446969915,"fyi, this issue has been solved and the feature is available since 1.5.2, please do 
```bash
pip install -U bert-serving-server bert-serving-client
```
Usage and more info can be found in the tutorial: https://github.com/hanxiao/bert-as-service#using-your-own-tokenizer",issue feature available since please bash pip install usage found tutorial,issue,negative,positive,positive,positive,positive,positive
446969735,"fyi, this issue is solved and available since 1.5.2, please do 
```bash
pip install -U bert-serving-server bert-serving-client
```
Usage and more info can be found in the tutorial: https://github.com/hanxiao/bert-as-service#using-your-own-tokenizer",issue available since please bash pip install usage found tutorial,issue,negative,positive,positive,positive,positive,positive
446857259,also want to know how to server a classification with fine-tuned,also want know server classification,issue,negative,neutral,neutral,neutral,neutral,neutral
446852081,"fyi, this issue is fixed and available in 1.5.1, please do 
```bash
pip install -U bert-serving-server bert-serving-client
```",issue fixed available please bash pip install,issue,negative,positive,positive,positive,positive,positive
446851864,"Just implement your code (add the classification layer) into Bert-as-service.
If you don't want to touch Bert-as-service, you have to implement your classification layer outside of Bert-as-service.",implement code add classification layer want touch implement classification layer outside,issue,negative,neutral,neutral,neutral,neutral,neutral
446840911,"But please keep in mind that the pretrained BERT Chinese is a character-based model, so if you use your own segmentation without changing the underlying BERT model, you will get a bunch of OOV.

It only makes sense when the served BERT model is *not* character-based and follows the same vocabulary as your segmentation.",please keep mind model use segmentation without underlying model get bunch sense model vocabulary segmentation,issue,negative,neutral,neutral,neutral,neutral,neutral
446822952,"require some refactoring, but I see the needs. 

will do, mark it as enhancement.",require see need mark enhancement,issue,negative,neutral,neutral,neutral,neutral,neutral
446815595,"The current version behaves as follows:
```python
for k in ([], [''], ['', ''], ['', []], ['\n', '\r\n'], ['', 's', 'ss'], ['s', 'ss'], ['', 2, 'ss'], [2, 'ss']):
    print('%s\t%s' % (BertClient._is_valid_input(k), k))
```

```
False	[]
False	['']
False	['', '']
False	['', []]
False	['\n', '\r\n']
False	['', 's', 'ss']
True	['s', 'ss']
False	['', 2, 'ss']
False	[2, 'ss']
```

But you are right, would be better to raise an error on the empty case. Mark it as enhancement and will fix it",current version python print false false false false false false true false false right would better raise error empty case mark enhancement fix,issue,negative,negative,negative,negative,negative,negative
446812011,"In the current version, if you choose `-pooling_strategy NONE`, then no post-processing (e.g. masking) is applied on the `[batch_size, max_seq_len, num_dim]` output. On the contrary, if you choose a `pooling_strategy`, then a mask is applied *before* pooling to ensure the correctness.

But you are right, the masking behavior should be consistent across all `pooling_strategy` including `NONE`, and though setting padding symbol to zero in `[batch_size, max_seq_len, num_dim]` may not be the perfect solution (I'm imagining some weird case where some column is already all-zero on the last axis of `[B,T,D]`), it should work in most of the cases. 

Will solve this soon.",current version choose none applied output contrary choose mask applied ensure correctness right behavior consistent across none though setting padding symbol zero may perfect solution weird case column already last axis work solve soon,issue,positive,positive,positive,positive,positive,positive
446809537,"I'm reopening this issue for now as it seems a common error for Windows users. I will try to address this issue later.

Meanwhile, since I don't work on Windows and thus don't have the dev-env on hand, it would be really great if someone can help me on solving this issue especially if you are a Windows user. I mark this issue as ""help wanted"".",issue common error try address issue later meanwhile since work thus hand would really great someone help issue especially user mark issue help,issue,positive,positive,positive,positive,positive,positive
446808171,"thanks a lot for the PR. I think #73 was talking about the tmp files generated by the Estimator due to the default `model_dir=None`. But you are right, `ipc` indeed generates tmp files as well.

Personally, I found using `*` always give a valid tmp file path that is writeable by the current user (if current user not root), whereas hard-coded path is often not the case.

I will not merge it for now and wait for the better solution.",thanks lot think talking estimator due default right indeed well personally found always give valid file path writeable current user current user root whereas path often case merge wait better solution,issue,positive,positive,positive,positive,positive,positive
446806167,"yes, for sure.  768-dim is just an example when using uncased BERT model.",yes sure example uncased model,issue,positive,positive,positive,positive,positive,positive
446425614,"> I run this command:
> python C:\Users\Liheng\Desktop\BERT\bert-serving-start.py -model_dir C:\Users\Liheng\Desktop\BERT\model\cased_L-12_H-768_A-12\ -num_worker=1,
> but thie issue occurs:
> ![image](https://user-images.githubusercontent.com/24928240/49624309-e21dfb00-fa0b-11e8-88bd-a629cca92d93.png)

i got same error ",run command python issue image got error,issue,negative,neutral,neutral,neutral,neutral,neutral
446414187,"do you mean token-level embedding? see: https://github.com/hanxiao/bert-as-service/blob/master/README.md#getting-elmo-like-contextual-word-embedding

or do you mean just tokens: “something” “like” “this”, “.”?",mean see mean something like,issue,negative,negative,negative,negative,negative,negative
446097421,"Resource temporarily unavailable (src/xsub.cpp:164)
Aborted (core dumped)",resource temporarily unavailable aborted core,issue,negative,neutral,neutral,neutral,neutral,neutral
446078086,"Thanks @nmfisher for digging the problem. I also had a vague feeling that the problem may due to the `prefetch()` but can't really tell for sure. 

I didn't test with docker container, but w/o container both CPU and GPU work fine with `prefetch()`. FYI, `prefetch()` can improve about 20% inference speed, as it overlaps the CPU time (for preparing the data) and GPU time (for model computing).

Anyway, I can at least add a argument to CLI for controlling whether to use `prefetch`.",thanks digging problem also vague feeling problem may due ca really tell sure test docker container container work fine improve inference speed time data time model anyway least add argument whether use,issue,negative,positive,neutral,neutral,positive,positive
446074747,"Removing Dataset.prefetch() seems to fix the issue. 

https://github.com/hanxiao/bert-as-service/blob/6ad68827d6c7ad1e545f0a5aaaaac9db0b24f98b/server/bert_serving/server/__init__.py#L336

Not super familiar with the tf.data API so I'm not exactly sure what's going on, or why this only seems to be an issue on the Docker/CPU-only instance.",removing fix issue super familiar exactly sure going issue instance,issue,positive,positive,positive,positive,positive,positive
446072141,"If I manually raise StopIteration at this line:

https://github.com/hanxiao/bert-as-service/blob/6ad68827d6c7ad1e545f0a5aaaaac9db0b24f98b/server/bert_serving/server/__init__.py#L324

I can at least guarantee a single response (though of course, nothing will work after that as the worker has exited).

Not sure if this is related to the tf.data.Dataset or the zmq worker.recv_multipart()",manually raise line least guarantee single response though course nothing work worker sure related,issue,negative,positive,neutral,neutral,positive,positive
446054184,"I've run into the same issue with a CPU-only Docker instance. I've upgraded to 1.5.0 but no dice. 
Both the server and client are instantiated and running:

```
                ARG   VALUE
__________________________________________________
                 cpu = True
 gpu_memory_fraction = 0.5
      max_batch_size = 8
         max_seq_len = 25
           model_dir = /app/model/cased_L-12_H-768_A-12/
          num_worker = 1
       pooling_layer = [-2]
    pooling_strategy = REDUCE_MEAN
                port = 5555
            port_out = 5556

I:VENTILATOR:[__i:run:122]:device_map:
                worker  0 -> gpu -1
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpmleudcse
WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7ff9e5d5f598>) includes params argument, but params are not passed to Estimator.
2018-12-11 02:52:10.841796: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2018-12-11 02:52:10.855842: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: UNKNOWN ERROR (-1)
2018-12-11 02:52:10.856818: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel driver does not appear to be running on this host (76b3a55b3fff): /proc/driver/nvidia/version does not exist
2018-12-11 02:52:11.669188: W tensorflow/core/framework/allocator.cc:122] Allocation of 89075712 exceeds 10% of system memory.
I:WORKER-0:[__i:gen:312]:ready and listening!
```

```
I:VENTILATOR:[__i:run:133]:new config request   req id: 0       client: b'e991674e-68b8-4887-88fc-e4fe739d4fcd'
I:SINK:[__i:run:250]:send config        client b'e991674e-68b8-4887-88fc-e4fe739d4fcd'
server config:
                max_batch_size  =       8
                   max_seq_len  =       25
                          port  =       5555
          ventilator -> worker  =       ipc://tmplIlyWd/socket
                   num_request  =       0
                worker -> sink  =       ipc://tmpKHJTL4/socket
           server_current_time  =       2018-12-11 02:53:40.172891
                num_subprocess  =       2
             server_start_time  =       2018-12-11 02:53:06.977076
            tensorflow_version  =       1.12.0
           ventilator <-> sink  =       ipc://tmp1RPUtK/socket
                 pooling_layer  =       [-2]
                        client  =       e991674e-68b8-4887-88fc-e4fe739d4fcd
                     model_dir  =       /app/model/cased_L-12_H-768_A-12/
              pooling_strategy  =       2
                python_version  =       3.5.2 (default, Nov 23 2017, 16:37:01)
[GCC 5.4.0 20160609]
                server_version  =       1.5.0
                    run_on_gpu  =       False
                      port_out  =       5556
                    num_worker  =       1
I:VENTILATOR:[__i:run:133]:new config request   req id: 0       client: b'4288bb38-18b9-477c-86b7-180568dfd4ab'
I:SINK:[__i:run:250]:send config        client b'4288bb38-18b9-477c-86b7-180568dfd4ab'
server config:
                max_batch_size  =       8
                   max_seq_len  =       25
                          port  =       5555
          ventilator -> worker  =       ipc://tmplIlyWd/socket
                   num_request  =       0
                worker -> sink  =       ipc://tmpKHJTL4/socket
           server_current_time  =       2018-12-11 02:53:40.307398
                num_subprocess  =       2
             server_start_time  =       2018-12-11 02:53:06.977076
            tensorflow_version  =       1.12.0
           ventilator <-> sink  =       ipc://tmp1RPUtK/socket
                 pooling_layer  =       [-2]
                        client  =       4288bb38-18b9-477c-86b7-180568dfd4ab
                     model_dir  =       /app/model/cased_L-12_H-768_A-12/
              pooling_strategy  =       2
                python_version  =       3.5.2 (default, Nov 23 2017, 16:37:01)
[GCC 5.4.0 20160609]
                server_version  =       1.5.0
                    run_on_gpu  =       False
                      port_out  =       5556
                    num_worker  =       1
```

But when I try to encode, it looks like the server hangs:
```
I:VENTILATOR:[__i:run:147]:new encode request   req id: 1       client: b'e991674e-68b8-4887-88fc-e4fe739d4fcd'
I:SINK:[__i:run:247]:job register       size: 2 job id: b'e991674e-68b8-4887-88fc-e4fe739d4fcd#1'
I:WORKER-0:[__i:gen:316]:new job        size: 2 client: b'e991674e-68b8-4887-88fc-e4fe739d4fcd#1'
```
It just sits there and never returns. *Occasionally*, the first call to encode() will work, but never any subsequent calls.",run issue docker instance dice server client running value true port ventilator run worker warning temporary folder model directory warning estimator function argument estimator binary use call unknown error kernel driver appear running host exist allocation system memory gen ready listening ventilator run new request id client sink run send client server port ventilator worker worker sink ventilator sink client default false ventilator run new request id client sink run send client server port ventilator worker worker sink ventilator sink client default false try encode like server ventilator run new encode request id client sink run job register size job id gen new job size client never occasionally first call encode work never subsequent,issue,negative,positive,neutral,neutral,positive,positive
446045427,"Sorry, can't reproduce the result. 

- Note, `example5.py` is a very simple case for using `bert-as-service` as the feature extraction pipeline. It's showing an idea and nothing more. Please don't over-rely on the model `DNNClassifier` I wrote there.
- Nonetheless, the bug shouldn't exist. Unfortunately, I can't reproduce the result based on the information you provided. Please [create a new issue and follow the issue template](https://github.com/hanxiao/bert-as-service/issues/new) to provide more information.",sorry ca reproduce result note simple case feature extraction pipeline showing idea nothing please model wrote nonetheless bug exist unfortunately ca reproduce result based information provided please create new issue follow issue template provide information,issue,negative,negative,negative,negative,negative,negative
445714290,"请问bert可以表示词的word embedding结果吗？发自我的iPhone------------------ 原始邮件 ------------------发件人: Han Xiao  <notifications@github.com>发送时间: 2018年12月10日 14:16收件人: hanxiao/bert-as-service <bert-as-service@noreply.github.com>抄送: Jackie127 <18810424351@163.com>, Comment <comment@noreply.github.com>主题: 回复：[hanxiao/bert-as-service] 按照教程来弄的 但是完全没有反应   (#97)if you encounter such problem on a CPU-only machine, please try the latest version 1.5.0 by
pip install -U bert-serving-server bert-serving-client
When start the server side, remember to add a -cpu flag, e.g.
bert-serving-start -model_dir /your/model/path -cpu

For better performance on CPU, please give a smaller max_batch_size, e.g. 16, 32 as suggested in https://github.com/hanxiao/bert-as-service#q-can-i-run-the-server-side-on-cpu-only-machine

—You are receiving this because you commented.Reply to this email directly, view it on GitHub, or mute the thread.
{""api_version"":""1.0"",""publisher"":{""api_key"":""05dde50f1d1a384dd78767c55493e4bb"",""name"":""GitHub""},""entity"":{""external_key"":""github/hanxiao/bert-as-service"",""title"":""hanxiao/bert-as-service"",""subtitle"":""GitHub repository"",""main_image_url"":""https://assets-cdn.github.com/images/email/message_cards/header.png"",""avatar_image_url"":""https://assets-cdn.github.com/images/email/message_cards/avatar.png"",""action"":{""name"":""Open in GitHub"",""url"":""https://github.com/hanxiao/bert-as-service""}},""updates"":{""snippets"":[{""icon"":""PERSON"",""message"":""@hanxiao in #97: if you encounter such problem on a CPU-only machine, please try the latest version 1.5.0 by \r\n```bash\r\npip install -U bert-serving-server bert-serving-client\r\n```\r\n\r\nWhen start the server side, remember to add a `-cpu` flag, e.g.\r\n```\r\nbert-serving-start -model_dir /your/model/path -cpu\r\n```\r\n\r\nFor better performance on CPU, please give a smaller `max_batch_size`, e.g. 16, 32 as suggested in https://github.com/hanxiao/bert-as-service#q-can-i-run-the-server-side-on-cpu-only-machine""}],""action"":{""name"":""View Issue"",""url"":""https://github.com/hanxiao/bert-as-service/issues/97#issuecomment-445699899""}}}
[
{
""@context"": ""http://schema.org"",
""@type"": ""EmailMessage"",
""potentialAction"": {
""@type"": ""ViewAction"",
""target"": ""https://github.com/hanxiao/bert-as-service/issues/97#issuecomment-445699899"",
""url"": ""https://github.com/hanxiao/bert-as-service/issues/97#issuecomment-445699899"",
""name"": ""View Issue""
},
""description"": ""View this Issue on GitHub"",
""publisher"": {
""@type"": ""Organization"",
""name"": ""GitHub"",
""url"": ""https://github.com""
}
},
{
""@type"": ""MessageCard"",
""@context"": ""http://schema.org/extensions"",
""hideOriginalBody"": ""false"",
""originator"": ""AF6C5A86-E920-430C-9C59-A73278B5EFEB"",
""title"": ""Re: [hanxiao/bert-as-service] 按照教程来弄的 但是完全没有反应   (#97)"",
""sections"": [
{
""text"": """",
""activityTitle"": ""**Han Xiao**"",
""activityImage"": ""https://assets-cdn.github.com/images/email/message_cards/avatar.png"",
""activitySubtitle"": ""@hanxiao"",
""facts"": [

]
}
],
""potentialAction"": [
{
""name"": ""Add a comment"",
""@type"": ""ActionCard"",
""inputs"": [
{
""isMultiLine"": true,
""@type"": ""TextInput"",
""id"": ""IssueComment"",
""isRequired"": false
}
],
""actions"": [
{
""name"": ""Comment"",
""@type"": ""HttpPOST"",
""target"": ""https://api.github.com"",
""body"": ""{\n\""commandName\"": \""IssueComment\"",\n\""repositoryFullName\"": \""hanxiao/bert-as-service\"",\n\""issueId\"": 97,\n\""IssueComment\"": \""{{IssueComment.value}}\""\n}""
}
]
},
{
""targets"": [
{
""os"": ""default"",
""uri"": ""https://github.com/hanxiao/bert-as-service/issues/97#issuecomment-445699899""
}
],
""@type"": ""OpenUri"",
""name"": ""View on GitHub""
},
{
""name"": ""Unsubscribe"",
""@type"": ""HttpPOST"",
""target"": ""https://api.github.com"",
""body"": ""{\n\""commandName\"": \""MuteNotification\"",\n\""threadId\"": 421008588\n}""
}
],
""themeColor"": ""26292E""
}
]",han comment comment encounter problem machine please try latest version pip install start server side remember add flag better performance please give smaller directly view mute thread publisher name entity title subtitle repository encounter problem machine please try latest version install start server side remember add flag better performance please give smaller issue context type type target name view issue description view issue publisher type organization name type context false originator title text han name add comment type true type id false name comment type target body o default type name view name type target body,issue,positive,positive,positive,positive,positive,positive
445700202,"if you encounter such problem on a CPU-only machine, please try the latest version 1.5.0 by 
```bash
pip install -U bert-serving-server bert-serving-client
```

When start the server side, remember to add a `-cpu` flag, e.g.
```
bert-serving-start -model_dir /your/model/path -cpu
```

For better performance on CPU, please give a smaller `max_batch_size`, e.g. 16, 32 as suggested in https://github.com/hanxiao/bert-as-service#q-can-i-run-the-server-side-on-cpu-only-machine

Finally, #111 was the PR for fixing this issue.",encounter problem machine please try latest version bash pip install start server side remember add flag better performance please give smaller finally fixing issue,issue,positive,positive,positive,positive,positive,positive
445699899,"if you encounter such problem on a CPU-only machine, please try the latest version 1.5.0 by 
```bash
pip install -U bert-serving-server bert-serving-client
```

When start the server side, remember to add a `-cpu` flag, e.g.
```
bert-serving-start -model_dir /your/model/path -cpu
```

For better performance on CPU, please give a smaller `max_batch_size`, e.g. 16, 32 as suggested in https://github.com/hanxiao/bert-as-service#q-can-i-run-the-server-side-on-cpu-only-machine",encounter problem machine please try latest version bash pip install start server side remember add flag better performance please give smaller,issue,positive,positive,positive,positive,positive,positive
445698093,"fyi, this issue is fixed by adding a `-cpu` flag on the `bert-serving-start` CLI and is available since `1.5.0`. please do `pip install -U bert-serving-server bert-serving-client`. Thanks again!",issue fixed flag available since please pip install thanks,issue,positive,positive,positive,positive,positive,positive
445694251,"@RedRedZhang How you resolved it I also have same Windows 10, even declared the path in ENV Variable but still getting the same error.",resolved also even declared path variable still getting error,issue,negative,neutral,neutral,neutral,neutral,neutral
445652266,"> 跑了很久都没有跑出结果，请问这是啥原因？
Follow https://github.com/hanxiao/bert-as-service#install and https://github.com/hanxiao/bert-as-service#1-download-a-pre-trained-bert-model, compare yours with screenshots in README

> 请问bert-as-service可以表示词的特征吗
https://github.com/hanxiao/bert-as-service#getting-elmo-like-contextual-word-embedding

Please consult README first
",follow compare please consult first,issue,negative,positive,positive,positive,positive,positive
445648384,"                 ARG   VALUE
__________________________________________________
      max_batch_size = 256
         max_seq_len = 25
           model_dir = /Users/fangchunhua/Desktop/BERTMODEL/uncased_L-12_H-768_A-12/
          num_worker = 0
       pooling_layer = [-2]
    pooling_strategy = REDUCE_MEAN
                port = 5555
            port_out = 5556

I:VENTILATOR:[ser:__i: 78]:frontend-sink ipc: ipc:///var/folders/v3/1g6dr1wn5k305c3w7q1_rmv80000gp/T/tmpJOsKtz
W:VENTILATOR:[ser:run:100]:nvidia-smi is missing, often means no gpu found on this machine. will run service on cpu instead

>>> bc = BertClient()
connect success!
server returns the following config:
bc.encode(['First do it', 'then do it right', 'then do it better'])

跑了很久都没有跑出结果，请问这是啥原因？请问bert-as-service可以表示词的特征吗",value port ventilator ser ventilator ser run missing often found machine run service instead connect success server following right better,issue,positive,positive,positive,positive,positive,positive
445447843,"sorry but i can't reproduce the error on `example5.py`.

some general suggestions:
- Update to latest version 1.4.9 `pip install -U bert-serving-server bert-serving-client`
- Remove `.prefetch(20)` if you are running the server side on CPU, not necessary also prone to the race condition
- Set `num_parallel_calls=1`
- Or please provide concrete environment and steps for reproducing the error",sorry ca reproduce error general update latest version pip install remove running server side necessary also prone race condition set please provide concrete environment error,issue,negative,positive,neutral,neutral,positive,positive
445437371,"latest pip version, same error again
![2018-12-08 14-53-22](https://user-images.githubusercontent.com/33157235/49683023-1fac8200-faf9-11e8-8740-2862fa9593e3.png)
",latest pip version error,issue,negative,positive,positive,positive,positive,positive
445282691,"Is TCP supposed to work on linux? When client connects, server config response never arrives. (It's worth noting that I'm running it in docker on windows)",supposed work client server response never worth running docker,issue,negative,negative,negative,negative,negative,negative
445258902,"i tried again, there is a new error.

![image](https://user-images.githubusercontent.com/1172727/49654911-14792980-fa07-11e8-9096-f3a7bb361f55.png)
",tried new error image,issue,negative,positive,positive,positive,positive,positive
445220012,"FYI, I made a visualization of different BERT layers, may help you understand the pool_layer and pool_strategy. See: https://github.com/hanxiao/bert-as-service#q-so-which-layer-and-which-pooling-strategy-is-the-best",made visualization different may help understand see,issue,negative,neutral,neutral,neutral,neutral,neutral
445130697,fixed in #101 and available since 1.4.7 please do `pip install -U bert-serving-server bert-serving-client`,fixed available since please pip install,issue,negative,positive,positive,positive,positive,positive
445126049,"Ok, get what you mean, see please follow the thread in #70 ",get mean see please follow thread,issue,negative,negative,negative,negative,negative,negative
445125880,"Could you be more specific? Which feature or procedure do you think is too complicated and why tf-serving is the solution to that?

Also see the discussion here: https://github.com/hanxiao/bert-as-service/issues/70",could specific feature procedure think complicated solution also see discussion,issue,negative,negative,negative,negative,negative,negative
445117662,add close() method to client,add close method client,issue,negative,neutral,neutral,neutral,neutral,neutral
445102682,"I run this command:
python C:\Users\Liheng\Desktop\BERT\bert-serving-start.py -model_dir C:\Users\Liheng\Desktop\BERT\model\cased_L-12_H-768_A-12\ -num_worker=1,
but thie issue occurs:
![image](https://user-images.githubusercontent.com/24928240/49624309-e21dfb00-fa0b-11e8-88bd-a629cca92d93.png)
",run command python issue image,issue,negative,neutral,neutral,neutral,neutral,neutral
445101104,😱 that's serious for sure. thanks a million,serious sure thanks million,issue,positive,positive,positive,positive,positive,positive
445091950,pip install the latest version and try again. https://github.com/hanxiao/bert-as-service#install,pip install latest version try,issue,negative,positive,positive,positive,positive,positive
445091449,"looks like `bert-serving-start` is not in your `PATH` env variable. update your `PATH` variable.
",like path variable update path variable,issue,negative,neutral,neutral,neutral,neutral,neutral
445091051,"i dont quite understand your question.

anyway, please install via `pip` https://github.com/hanxiao/bert-as-service#install
",dont quite understand question anyway please install via pip,issue,negative,neutral,neutral,neutral,neutral,neutral
444847554,"I have tried new version but my training doesn't seem to be better. 
```
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 0 into data/model/1206-081741/model.ckpt.
INFO:tensorflow:loss = 264.92365, step = 1
INFO:tensorflow:global_step/sec: 0.194579
INFO:tensorflow:loss = 142.13977, step = 101 (513.941 sec)
INFO:tensorflow:global_step/sec: 0.194874
INFO:tensorflow:loss = 152.33533, step = 201 (513.145 sec)
INFO:tensorflow:global_step/sec: 0.19485
INFO:tensorflow:loss = 159.28421, step = 301 (513.216 sec)
INFO:tensorflow:global_step/sec: 0.194599
INFO:tensorflow:loss = 152.00076, step = 401 (513.898 sec)
INFO:tensorflow:global_step/sec: 0.194962
INFO:tensorflow:loss = 144.81601, step = 501 (512.911 sec)
INFO:tensorflow:global_step/sec: 0.194442
INFO:tensorflow:loss = 134.33696, step = 601 (514.280 sec)
INFO:tensorflow:global_step/sec: 0.192623
INFO:tensorflow:loss = 144.52493, step = 701 (519.150 sec)
INFO:tensorflow:global_step/sec: 0.193265
INFO:tensorflow:loss = 144.21286, step = 801 (517.422 sec)
INFO:tensorflow:global_step/sec: 0.193592
INFO:tensorflow:loss = 159.57654, step = 901 (516.557 sec)
INFO:tensorflow:Saving checkpoints for 1000 into data/model/1206-081741/model.ckpt.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Starting evaluation at 2018-12-06-09:43:44
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Restoring parameters from data/model/1206-081741/model.ckpt-1000
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Evaluation [10/100]
INFO:tensorflow:Evaluation [20/100]
INFO:tensorflow:Evaluation [30/100]
INFO:tensorflow:Evaluation [40/100]
INFO:tensorflow:Evaluation [50/100]
INFO:tensorflow:Evaluation [60/100]
INFO:tensorflow:Evaluation [70/100]
INFO:tensorflow:Evaluation [80/100]
INFO:tensorflow:Evaluation [90/100]
INFO:tensorflow:Evaluation [100/100]
INFO:tensorflow:Finished evaluation at 2018-12-06-09:53:55
INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.433125, average_loss = 1.1821911, global_step = 1000, loss = 151.32047
INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: data/model/1206-081741/model.ckpt-1000
INFO:tensorflow:global_step/sec: 0.0884598
INFO:tensorflow:loss = 143.34467, step = 1001 (1130.451 sec)
INFO:tensorflow:global_step/sec: 0.242282
INFO:tensorflow:loss = 152.80426, step = 1101 (412.750 sec)
INFO:tensorflow:global_step/sec: 0.170403
INFO:tensorflow:loss = 156.10487, step = 1201 (586.844 sec)
INFO:tensorflow:global_step/sec: 0.191289
INFO:tensorflow:loss = 180.17056, step = 1301 (522.766 sec)
INFO:tensorflow:global_step/sec: 0.189388
INFO:tensorflow:loss = 131.78491, step = 1401 (528.014 sec)
INFO:tensorflow:global_step/sec: 0.191926
INFO:tensorflow:loss = 159.46521, step = 1501 (521.031 sec)
INFO:tensorflow:global_step/sec: 0.189784
INFO:tensorflow:loss = 149.55505, step = 1601 (526.921 sec)
INFO:tensorflow:global_step/sec: 0.190483
INFO:tensorflow:loss = 171.7102, step = 1701 (524.980 sec)
INFO:tensorflow:global_step/sec: 0.190815
INFO:tensorflow:loss = 144.19461, step = 1801 (524.061 sec)
INFO:tensorflow:global_step/sec: 0.191387
INFO:tensorflow:loss = 155.31046, step = 1901 (522.512 sec)
INFO:tensorflow:Saving checkpoints for 2000 into data/model/1206-081741/model.ckpt.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Starting evaluation at 2018-12-06-11:20:31
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Restoring parameters from data/model/1206-081741/model.ckpt-2000
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Evaluation [10/100]
INFO:tensorflow:Evaluation [20/100]
INFO:tensorflow:Evaluation [30/100]
INFO:tensorflow:Evaluation [40/100]
INFO:tensorflow:Evaluation [50/100]
INFO:tensorflow:Evaluation [60/100]
INFO:tensorflow:Evaluation [70/100]
INFO:tensorflow:Evaluation [80/100]
INFO:tensorflow:Evaluation [90/100]
INFO:tensorflow:Evaluation [100/100]
INFO:tensorflow:Finished evaluation at 2018-12-06-11:31:09
INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.43398437, average_loss = 1.1828988, global_step = 2000, loss = 151.41104
INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2000: data/model/1206-081741/model.ckpt-2000
INFO:tensorflow:global_step/sec: 0.0858941
INFO:tensorflow:loss = 160.1148, step = 2001 (1164.219 sec)
INFO:tensorflow:global_step/sec: 0.235182
INFO:tensorflow:loss = 158.74728, step = 2101 (425.207 sec)
```

If it can help,  I provide my code [finetuning_bert.zip] that is substantially the same to yours. 

[finetuning_bert.zip](https://github.com/hanxiao/bert-as-service/files/2652946/finetuning_bert.zip)

Thanks
",tried new version training seem better calling done calling create graph running done running saving loss step loss step sec loss step sec loss step sec loss step sec loss step sec loss step sec loss step sec loss step sec loss step sec saving calling done calling starting evaluation graph running done running evaluation evaluation evaluation evaluation evaluation evaluation evaluation evaluation evaluation evaluation finished evaluation saving global step accuracy loss saving summary global step loss step sec loss step sec loss step sec loss step sec loss step sec loss step sec loss step sec loss step sec loss step sec loss step sec saving calling done calling starting evaluation graph running done running evaluation evaluation evaluation evaluation evaluation evaluation evaluation evaluation evaluation evaluation finished evaluation saving global step accuracy loss saving summary global step loss step sec loss step sec help provide code substantially thanks,issue,negative,negative,neutral,neutral,negative,negative
444828534,"awesome, would be nice if you can make a pull request, so i will just merge it.",awesome would nice make pull request merge,issue,positive,positive,positive,positive,positive,positive
444800640,looks like the server never starts. please refer to #81 #90 #82 ,like server never please refer,issue,negative,neutral,neutral,neutral,neutral,neutral
444791943,"fyi, this is issue is fixed. `bert-serving` is now available on pypi. with separate server and client packages. check latest readme for details.",issue fixed available separate server client check latest,issue,negative,positive,positive,positive,positive,positive
444791624,"this issue is fixed in latest master, Windows and inter-process TCP are now fully supported.
Please do `pip install -U bert-serving-server  bert-serving-client` and retry.",issue fixed latest master fully please pip install retry,issue,negative,positive,positive,positive,positive,positive
444791045,"add windows support by switching to tcp and bind port randomly, fix #39 
the package is now available on pypi, fix #50 ",add support switching bind port randomly fix package available fix,issue,negative,negative,neutral,neutral,negative,negative
444712857,"nice, good to hear. close #85",nice good hear close,issue,positive,positive,positive,positive,positive,positive
444702282,"Your fix is amazing ! 🥇 

Updated results :
- Using layer `-2`, I now got a score of ~80%
- Using layer `-11`, I now got a score of ~82%

Awesome job !",fix amazing layer got score layer got score awesome job,issue,positive,positive,positive,positive,positive,positive
444673111,"I used to think grpc would be the best path forward to future path interoperability with trained models and microservices. But with python 2 incompatibility with grpclib - it's an upward battle to get some code working.  https://github.com/google/sling/issues/210

zmq - is out of the box state of the art. The youtube videos maybe old - https://www.youtube.com/results?search_query=zmq but the underlying technology was built for stock trading - with high availability / fault tolerance. There's a lot of client libraries supporting different configurations. ",used think would best path forward future path trained python incompatibility upward battle get code working box state art maybe old underlying technology built stock trading high availability fault tolerance lot client supporting different,issue,positive,positive,positive,positive,positive,positive
444601703,"I seem to run into a similar problem:

<img width=""436"" alt=""wx20181205-105558 2x"" src=""https://user-images.githubusercontent.com/17476726/49537068-cafcd300-f87c-11e8-9634-477314f4e728.png"">

Any updates on the bug fix? Thanks!",seem run similar problem bug fix thanks,issue,negative,positive,neutral,neutral,positive,positive
444507546,"there is a bug in my pooling strategies: REDUCE_MEAN, REDUCE_MAX and REDUCE_MEAN_MAX. I fixed it in the latest master, please do git pull to check out the latest master. If you redo your experiment, result could be different.

in principle, this bug affects the most when ‘max_seq_len’ is much longer than the actual sequence length.",bug fixed latest master please git pull check latest master redo experiment result could different principle bug much longer actual sequence length,issue,negative,positive,positive,positive,positive,positive
444504435,"I found the reason. Because I used ""tensorflow-gpu"" instead of ""tensorflow"".
I created a new environment with conda to solve this problem.",found reason used instead new environment solve problem,issue,negative,positive,positive,positive,positive,positive
444491066,"my bad. I forgot to mask the sequence when doing reduce_mean, reduce_max and reduce_mean_max. this is fixed in #94 and now merged into master. please give it a try and may produce different result.

in principle, this will affect the most when `max_seq_len` is much longer than actual sequence length from clients.",bad forgot mask sequence fixed master please give try may produce different result principle affect much longer actual sequence length,issue,negative,negative,neutral,neutral,negative,negative
444490343,"thanks again for pointing out this issue. I've fixed in #94 and now it's in master, specifically at here https://github.com/hanxiao/bert-as-service/blob/master/bert/extract_features.py#L66",thanks pointing issue fixed master specifically,issue,negative,positive,positive,positive,positive,positive
444483925,"geez, shame on me. thought i was doing that. thanks a lot",shame thought thanks lot,issue,negative,positive,positive,positive,positive,positive
444454035,"#90 is closed as the reason has been found, maybe you can refer to it?",closed reason found maybe refer,issue,negative,negative,neutral,neutral,negative,negative
444452310,"> seems related to #81
> maybe a bug on CPU-only machine, looks like the bert-worker on the server never starts?

Find the reason.
The server did not started correctly.
For my machine, it should add some environmental path for python.
Like this:
/opt/compiler/gcc-xxx/lib/ld-linux-x86-64.so.x --library-path /opt/compiler/gcc-xxx/lib  ../../bin/python3.6  app.py -model_dir ./models/chinese_L-12_H-768_A-12/ -num_worker=1 

",related maybe bug machine like server never find reason server correctly machine add environmental path python like,issue,positive,neutral,neutral,neutral,neutral,neutral
444428198,ok i will reopen this issue as somebody else also encounter this on cpu-only machine,reopen issue somebody else also encounter machine,issue,negative,neutral,neutral,neutral,neutral,neutral
444427451,"seems related to #81 
maybe a bug on CPU-only machine, looks like the bert-worker on the server never starts?",related maybe bug machine like server never,issue,negative,neutral,neutral,neutral,neutral,neutral
444381485,"![image](https://user-images.githubusercontent.com/20255598/49495780-28098080-f89f-11e8-9f3c-d2911144d684.png)
It stays here and can't go on.",image stay ca go,issue,negative,neutral,neutral,neutral,neutral,neutral
444371412,i think you are using pretty old master. checkout the latest master and retry.,think pretty old master latest master retry,issue,negative,positive,positive,positive,positive,positive
444042011,"btw below is result from the [original BERT paper](https://arxiv.org/pdf/1810.04805.pdf) (Sect. 5.4) Note that they use pretrained-fixed-BERT to output contextualized word embedding, and then feed to **a randomly initialized two-layer 768-dimensional BiLSTM** before the classification layer. And it is only 0.3 F1 behind fine-tuning the entire model

![image](https://user-images.githubusercontent.com/2041322/49433813-c76e3b00-f7ed-11e8-97e8-e7288272aca9.png)

may also related to #76",result original paper sect note use output word feed randomly classification layer behind entire model image may also related,issue,negative,negative,negative,negative,negative,negative
444032811,"I can add my two cents about this. Currently I'm using a Tornado application (so not WSGI) to serve models. Models instances are singleton through the application. In my case I'm using Tornado App because it's completely async, while having WSGI application I could not have any singleton model running on the main thread. So I could move to ZMQ instead of using Tornado, or combining them using @hanxiao multiprocessing approach via ZMQ queueing. That's pretty powerful!",add two currently tornado application serve singleton application case tornado completely application could singleton model running main thread could move instead tornado combining approach via pretty powerful,issue,positive,positive,positive,positive,positive,positive
444031158,"Well, interesting findings, but I won't over-interpret this observation based on one experiment.

`pooling_layer=-1` close to the training output, may be biased to the training targets. If you didn't fine tune the model, then this could be a bad representation.
`pooling_layer=-12` close to the word embedding, may preserve the very original word information (with no fancy self-interaction etc.). On the other hand, you may achieve the same performance by simply using a word-embedding only.
Anything in-between [-1, -12] is then a trade-off.

Plus, they also give different speed: https://github.com/hanxiao/bert-as-service#speed-wrt-pooling_layer

just saying.",well interesting wo observation based one experiment close training output may training fine tune model could bad representation close word may preserve original word information fancy hand may achieve performance simply anything plus also give different speed saying,issue,positive,positive,neutral,neutral,positive,positive
444027466,"read again
https://github.com/hanxiao/bert-as-service#q-how-can-i-get-word-embedding-instead-of-sentence-embedding

pretty sure you forgot to set `pooling_strategy=NONE`",read pretty sure forgot set,issue,positive,positive,positive,positive,positive,positive
443947637,"Easy, 700MB GPU memory is normal. Considering a 12-layer, 768-hidden, 12-heads BERT has 110 million parameters in `tf.float32`, which takes 110 million * 32 bits = 400mb for the feedforward path (i.e no gradient and auxiliary variables).

But you are right, there should be some test to ensure that we are not accidentally break the model. I therefore mark this as enhancement and will add some unit test later.",easy memory normal considering million million path gradient auxiliary right test ensure accidentally break model therefore mark enhancement add unit test later,issue,negative,positive,positive,positive,positive,positive
443945396,"@hanxiao Thanks a lot for this detailed explanation. 

As my familiar serving stack with tensorflow is `tf-serving`, so my comparison here with ZMQ is `tf-serving`, for sure;). I'm writing a `tf-serving` version `bert-as-service` for the moment, maybe we can discuss and compare later in details, especially for benchmark part. ",thanks lot detailed explanation familiar serving stack comparison sure writing version moment maybe discus compare later especially part,issue,positive,positive,positive,positive,positive,positive
443756473,"I restart the server through checking running python processes and kill it.
ps ax | grep python # this command will list the python process backend.
kill -9 pid # choose the running processes to kill 
",restart server running python kill ax python command list python process kill choose running kill,issue,negative,neutral,neutral,neutral,neutral,neutral
443675395,"Despite how much I like to read such *""why not `A` instead of `B`""* debate on the web, I personally found it difficult to convince others on this type of question. People who ask this often have preconceived idea about `A` and `B`, and it's hard to change people's mind.

Let's first agree on the C/S structure of this design. By decoupling the main BERT model and the downstream network, you gain scalability, reusability and efficiency. For example, your team can share one well-trained BERT model as a feature extractor and deploy it on an powerful and cost-per-use GPU machine. All they have to do is using `BertClient` to connect with this shared server and do some feedforward prediction, which is not necessarily on GPU or using deep learning. If feature extraction is the bottleneck, then scale up your GPU machines. If downstream network is the bottleneck, then add more CPU machines. If feature is too old or concept-drifted, then retrain your BERT and version-control it, all downstream networks immediately enjoy the updated features. Finally, as all feature extraction requests come to one `BertServer`, you have less idle cycle on the GPU machine and every penny is spent worthy.

Now let me take a wild guess, you guys mainly concern the communication stack: *why not `A` instead of ZMQ?* Despite all three concerns are listed under the same issue, @applenob and @usakey didn't point out the alternative, whereas @jayjaywg clearly asked for the comparison with `tf-serving`. Note that `A` can be also `RabbitMQ`, `Apache Kafka`, `DDS` etc. So I wouldn't say you all have *the* same question.

That being said, I'd like to answer this question in two directions.

#### Why not `A` instead of ZMQ, where `A` is something less popular/backed by smaller community/home-made communication stack?
**A:** ZMQ allows complex messaging exchange patterns with minimal effort. Personally, I really like how simple its APIs are and how rich patterns can be implemented with just `send()` and `recv()`. In this project, I used `PUSH-PULL`, `PUB-SUB`, inter-process communication and intra-process communication. In fact, any message patterns I can think of turns into real working code in a few hours. If not, I can always get support from its active community on Stackoverflow or Github.

Just fyi, I'm not the only one who marry ZMQ with tensorflow, here is a more popular one: https://github.com/tensorpack/tensorpack who uses ZMQ for internal communication. In fact, it was him [pointed to me first in my blog post](https://hanxiao.github.io/2017/07/07/Get-10x-Speedup-in-Tensorflow-Multi-Task-Learning-using-Python-Multiprocessing/).

#### Why not `A` instead of ZMQ, where `A` is something more popular/backed by bigger community, e.g. `tf-serving`?
**A:** I don't have much experience of `tf-serving`, but a few-minute read on its doc seems an overkill to this problem: more dependencies (docker container, etc.), longer round-trip per request, less flexible request batching on the server side. To me, `tf-serving` is a well-abstracted high-level package which already considers problems that I don't see at the moment. I also believe one can implement most (if not all) features of `bert-as-service` using `tf-serving` (even [`PUB-SUB`](https://github.com/hanxiao/bert-as-service#broadcasting-to-multiple-clients) and [""Async encoding""](https://github.com/hanxiao/bert-as-service#asynchronous-encoding)). Nonetheless, I still prefer *not to over-engineer* the problem and only solve the problems & requirements I see at the moment. After all,  the value of a software is determined by whether it solves your problem, not by the tech stack its using. If I want to extend a new feature, say more sophisticated server batching or scheduling, I can always use ZMQ to implement it from the middle level, giving me much clearer picture under the hood.

That said, I'm not stopping anyone to explore alternatives to the ZMQ communication stack. If you have experience in both `A` and ZMQ, perfect! Then teach me your lessons. Discussions and improvements are the very essence of OSS community. If you find anything better or anything bad of the current repo, you are always welcome to contribute.",despite much like read instead debate web personally found difficult convince type question people ask often preconceived idea hard change people mind let first agree structure design main model downstream network gain efficiency example team share one model feature extractor deploy powerful machine connect server prediction necessarily deep learning feature extraction bottleneck scale downstream network bottleneck add feature old retrain downstream immediately enjoy finally feature extraction come one le idle cycle machine every penny spent worthy let take wild guess mainly concern communication stack instead despite three listed issue point alternative whereas clearly comparison note also apache would say question said like answer question two instead something le smaller communication stack complex exchange minimal effort personally really like simple rich send project used communication communication fact message think turn real working code always get support active community one marry popular one internal communication fact pointed first post instead something bigger community much experience read doc problem docker container longer per request le flexible request server side package already see moment also believe one implement even nonetheless still prefer problem solve see moment value determined whether problem tech stack want extend new feature say sophisticated server always use implement middle level giving much clearer picture hood said stopping anyone explore communication stack experience perfect teach essence community find anything better anything bad current always welcome contribute,issue,positive,positive,positive,positive,positive,positive
443644662,"Thanks a lot for your advices. The prob is solved.
Thank you.",thanks lot prob thank,issue,positive,positive,positive,positive,positive,positive
443621811,"same question, is there any intuition behind the scene for choosing ZMQ here? Thanks.",question intuition behind scene choosing thanks,issue,negative,negative,neutral,neutral,negative,negative
443606816,"Client-side hanging is usually a sign of bad (or no) connection to the server.

- Make sure the server is running successfully and the BERT model is loaded. What's the log output from the server?
- Make sure you give the correct ip address and port numbers when running `BertClient()`.
- What's the server output when you do `BertClient()`?
- What's the server output when you do `bc.encode()`?",hanging usually sign bad connection server make sure server running successfully model loaded log output server make sure give correct address port running server output server output,issue,positive,positive,positive,positive,positive,positive
443586565,My GPU is in use. I can't make it use the CPU automatically.,use ca make use automatically,issue,negative,neutral,neutral,neutral,neutral,neutral
443492817,"I solved a similar problem today. 

After checking `docker logs CONTAINER`, I found the Error:

> tensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /model/bert_model.ckpt
> I:SINK:[ser:run:226]:new job b'a8b5f75c-97ff-4515-b71b-448328b48492#9927e59c-c6d0-4c7c-9166-258f098996ed' size: 2 is registered!

It is because I only put the latest Tensorflow checkpoint of my fine-tuned model in `model_dir`.

It could be a common mistake. Hope this helps.
",similar problem today docker container found error unsuccessful constructor find matching sink ser run new job fed size registered put latest model could common mistake hope,issue,negative,positive,neutral,neutral,positive,positive
442899697,"Sorry, now I am very busy and I am using this commit ```b05a985dd6f36016090371e7751fc96f328a64c7``` in order to make possible a BERT fine-tuning. I cannot try that at the moment. 
When I am done, I will surely try new code, thanks.",sorry busy commit order make possible try moment done surely try new code thanks,issue,positive,positive,neutral,neutral,positive,positive
442771487,"Could you copy-paste the server and client output when using docker?
Client side, please use:
```python
bc = BertClient(ip='192.xxxx.xx.xx', show_server_config=True) 
```",could server client output docker client side please use python,issue,negative,neutral,neutral,neutral,neutral,neutral
442757775,Still doesn't work after your advice. Thanks,still work advice thanks,issue,negative,positive,positive,positive,positive,positive
442753844,"hmm, I think it's because `bert-as-server` now use two ports since v1.1 if I remember correctly. And I forgot to update the docker command in `README.md` accordingly.

Make sure your docker container open two ports 5555 and 5556, e.g.
```bash
docker run --runtime nvidia -it -p 5555:5555 -p 5556:5556 -v $PATH_MODEL:/model -t bert-as-service
```
Please check if this works.

More on two ports: https://github.com/hanxiao/bert-as-service#q-why-does-the-server-need-two-ports",think use two since remember correctly forgot update docker command accordingly make sure docker container open two bash docker run please check work two,issue,positive,positive,positive,positive,positive,positive
442685893,not sure what you are talking about. there is no such thing like `autograph` in the code.,sure talking thing like autograph code,issue,positive,positive,positive,positive,positive,positive
442677542,"good point, will add this feature",good point add feature,issue,negative,positive,positive,positive,positive,positive
442405241,"tf version is too low, should >=1.10, please refer to README.md

no need to change any path. just update the package.

adrianhust <notifications@github.com>于2018年11月28日 周三下午6:43写道：

> clone repo and run, throw errors in finding model files, like below:
> '''
> usage: app.py -model_dir /tmp/chinese_L-12_H-768_A-12/ -num_worker=4
> ARG VALUE
> ------------------------------
>
>   max_batch_size = 256
>      max_seq_len = 25
>        model_dir = /tmp/chinese_L-12_H-768_A-12/
>       num_worker = 4
>    pooling_layer = [-2]
> pooling_strategy = REDUCE_MEAN
>             port = 5555
>         port_out = 5556
>
> I:VENTILATOR:[ser:__i: 78]:frontend-sink ipc: ipc://tmpFYdJp7/socket
> WARNING:tensorflow:Using temporary folder as model directory:
> /tmp/tmp6bk_6gdw
> WARNING:tensorflow:Estimator's model_fn (<function
> model_fn_builder..model_fn at 0x7fb7af77dd90>) includes params argument,
> but params are not passed to Estimator.
> I:WORKER-0:[ser:run:273]:ready and listening
> self._model_dir: /tmp/tmp6bk_6gdw, checkpoint_path: None
> Process BertWorker-2:
> Traceback (most recent call last):
> File ""/home/anaconda/lib/python3.6/multiprocessing/process.py"", line 258,
> in _bootstrap
> self.run()
> File ""/ssd1/NLP/bert-as-service/service/server.py"", line 275, in run
> for r in self.estimator.predict(input_fn, yield_single_examples=False):
> File
> ""/home/anaconda/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"",
> line 491, in predict
> self._model_dir))
> ValueError: Could not find trained model in model_dir: /tmp/tmp6bk_6gdw.
> WARNING:tensorflow:Using temporary folder as model directory:
> /tmp/tmpkrst4jsl
> WARNING:tensorflow:Estimator's model_fn (<function
> model_fn_builder..model_fn at 0x7fb7ac6f12f0>) includes params argument,
> but params are not passed to Estimator.
> I:WORKER-1:[ser:run:273]:ready and listening
> self._model_dir: /tmp/tmpkrst4jsl, checkpoint_path: None
> Process BertWorker-3:
> Traceback (most recent call last):
> File ""/home/anaconda/lib/python3.6/multiprocessing/process.py"", line 258,
> in _bootstrap
> self.run()
> File ""/ssd1/NLP/bert-as-service/service/server.py"", line 275, in run
> for r in self.estimator.predict(input_fn, yield_single_examples=False):
> File
> ""/home/anaconda/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"",
> line 491, in predict
> self._model_dir))
> ValueError: Could not find trained model in model_dir: /tmp/tmpkrst4jsl.
> WARNING:tensorflow:Using temporary folder as model directory:
> /tmp/tmpid1u669g
> WARNING:tensorflow:Estimator's model_fn (<function
> model_fn_builder..model_fn at 0x7fb7ac6f1730>) includes params argument,
> but params are not passed to Estimator.
> I:WORKER-2:[ser:run:273]:ready and listening
> self._model_dir: /tmp/tmpid1u669g, checkpoint_path: None
> Process BertWorker-4:
> Traceback (most recent call last):
> File ""/home/anaconda/lib/python3.6/multiprocessing/process.py"", line 258,
> in _bootstrap
> self.run()
> File ""/ssd1/NLP/bert-as-service/service/server.py"", line 275, in run
> for r in self.estimator.predict(input_fn, yield_single_examples=False):
> File
> ""/home/anaconda/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"",
> line 491, in predict
> self._model_dir))
> ValueError: Could not find trained model in model_dir: /tmp/tmpid1u669g.
> WARNING:tensorflow:Using temporary folder as model directory:
> /tmp/tmp_s9pmmxl
> WARNING:tensorflow:Estimator's model_fn (<function
> model_fn_builder..model_fn at 0x7fb7ac6f1b70>) includes params argument,
> but params are not passed to Estimator.
> I:WORKER-3:[ser:run:273]:ready and listening
> self._model_dir: /tmp/tmp_s9pmmxl, checkpoint_path: None
> '''
>
> then I set model_dir and checkpoint_path manually in predict, then throw
> exception ""Key global_step not found in checkpoint""
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/hanxiao/bert-as-service/issues/66>, or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AB8l6vC9qcaFedp7opVdiat_xtPsiNNgks5uzmjJgaJpZM4Y3U3a>
> .
>
",version low please refer need change path update package clone run throw finding model like usage value port ventilator ser warning temporary folder model directory warning estimator function argument estimator ser run ready listening none process recent call last file line file line run file line predict could find trained model warning temporary folder model directory warning estimator function argument estimator ser run ready listening none process recent call last file line file line run file line predict could find trained model warning temporary folder model directory warning estimator function argument estimator ser run ready listening none process recent call last file line file line run file line predict could find trained model warning temporary folder model directory warning estimator function argument estimator ser run ready listening none set manually predict throw exception key found thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
442399739,"3rd:
https://github.com/hanxiao/bert-as-service/blob/master/README.md#q-bert-has-1224-layers-so-which-layer-are-you-talking-about


Tong Guo <notifications@github.com>于2018年11月28日 周三下午6:13写道：

> Two of the three link are the same.
>
> —
> You are receiving this because you commented.
>
>
> Reply to this email directly, view it on GitHub
> <https://github.com/hanxiao/bert-as-service/issues/64#issuecomment-442393322>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AB8l6i7JMdDcuauy1huLgs6dv4dEy9Fdks5uzmHfgaJpZM4Y3K-j>
> .
>
",tong two three link reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
442393322,"Two of the three link are the same.
Thank you!",two three link thank,issue,negative,neutral,neutral,neutral,neutral,neutral
442392342,"not exactly,

please refer to
https://github.com/hanxiao/bert-as-service/blob/master/README.md#q-what-are-the-available-pooling-strategies

https://github.com/hanxiao/bert-as-service/blob/master/README.md#q-why-not-use-the-hidden-state-of-the-first-token-as-default-strategy-ie-the-cls

and

https://github.com/hanxiao/bert-as-service/blob/master/README.md#q-why-not-use-the-hidden-state-of-the-first-token-as-default-strategy-ie-the-cls


more related to this topic can be found in FAQ


Tong Guo <notifications@github.com>于2018年11月28日 周三下午5:18写道：

> You take the final representation of [CLS] as the sentence vector?
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/hanxiao/bert-as-service/issues/64>, or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AB8l6pS824apYWdh-J_KDmdDg3qzkKODks5uzlTvgaJpZM4Y3K-j>
> .
>
",exactly please refer related topic found tong take final representation sentence vector thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
442374333,"@hanxiao amazing 💯 🥇 This is exactly what I was looking for! The reference [CAIL](https://github.com/thunlp/CAIL/blob/master/README_en.md#221-the-fields-of-data) dataset is also a very good example for both the text embedding and labels encoding.
Closing! Thank you!",amazing exactly looking reference also good example text thank,issue,positive,positive,positive,positive,positive,positive
442362300,"no, because the number of hidden units is predefined. if you are using BERT model released by Google, then the number of hidden units is either 768 or 1024. 

if you want 256-dimensional vector, either add a `dense(num_units=256)` then train on some downstream task, or do subsampling directly in the feature space (make sense if your goal is to build a random forest).",number hidden model number hidden either want vector either add dense train downstream task directly feature space make sense goal build random forest,issue,negative,negative,negative,negative,negative,negative
442362241,"I did as you said, and it works like a charm !

---

For those who are interested, here is my code in Keras for multi-threaded.

The `Sequence` ([documentation](https://keras.io/utils/#sequence)) : (class acting like a generator, but easier for multi-threading)
```
class PairSentenceSequence(Sequence):
    """""" Please refer to : https://keras.io/utils/#sequence """"""

    def __init__(self, dataset, batch_size, bert_clients):
        self.dataset = dataset
        self.batch_size = batch_size
        self.bc = bert_clients

    def __len__(self):
        sample_per_epoch = len(self.dataset)
        batch_nb = int(sample_per_epoch / self.batch_size) + 1
        return batch_nb

    def __getitem__(self, idx):
        # Extract from dataset the data for this batch
        batch_data = self.dataset[idx * self.batch_size:(idx + 1) * self.batch_size]

        # Extract the sentences and construct the sentence pair
        pair_sen = [""{} ||| {}"".format(data.doc_1, data.doc_2) for data in batch_data]

        # Sentence embeddings using BERT service 
        bert_client = self.bc.get()     # Wait a BertClient to be available
        sen_emb = bert_client.encode(pair_sen)
        self.bc.put(bert_client)        # Put it back after use

        return np.array(sen_emb), np.array([data.label for data in batch_data])
```
As you can see, before calling the `encode` method, I extract one `BertClient` from the queue (if it is in the queue it means it's available). After finishing using it, I put it back in the queue, so it is available to other threads.

In my training script, I simply create a Queue of BertClient (among other things like retrieving the dataset):
```
workers = 4
bert_clients = queue.Queue()
    for _ in range(workers):
        bert_clients.put(bert_service_lib.BertClient(show_server_config=False))
```

And the actual training :
```
train_seq = PairSentenceSequence(dataset=train_data,
            batch_size=args.batch_size, bert_clients=bert_clients)
dev_seq = PairSentenceSequence(dataset=dev_data,  
            batch_size=args.batch_size, bert_clients=bert_clients)

history = model.fit_generator(generator=train_seq, epochs=args.epochs, 
        callbacks=callbacks, validation_data=dev_seq, workers=workers)
```",said work like charm interested code sequence documentation class acting like generator easier class sequence please refer self self return self extract data batch extract construct sentence pair data sentence service wait available put back use return data see calling encode method extract one queue queue available finishing put back queue available training script simply create queue among like range actual training history,issue,positive,positive,positive,positive,positive,positive
442353254,"@loretoparisi I write one based on `tf.estimator.DNNClassifier`, checkout [example5](https://github.com/hanxiao/bert-as-service/blob/master/example5.py) is this something you have in mind?

",write one based example something mind,issue,negative,neutral,neutral,neutral,neutral,neutral
442327896,"sorry, It's my environmental problem, not about system. thanks for your code again.",sorry environmental problem system thanks code,issue,negative,negative,negative,negative,negative,negative
442316021,"**You cannot change this.**

This is from the architecture of the BERT model itself. For example, for `BERT-Base, uncased` :
> 12-layer, **768-hidden**, 12-heads, 110M parameters

---

If you **really** need a 256-dimensional vector, you have to train from scratch your own BERT architecture.
",change architecture model example uncased really need vector train scratch architecture,issue,negative,positive,positive,positive,positive,positive
442298680,"for linux/mac there shouldn't be any problem, they support ipc protocol.

as I said, I will run benchmark and then decide whether to replace ipc with tcp for better compatibility on windows. Yet personally, i don't think hosting bert-as-service on a windows server is common.",problem support protocol said run decide whether replace better compatibility yet personally think hosting server common,issue,negative,positive,neutral,neutral,positive,positive
442282713,"yes, `BertClient.encode()` is not thread-safe, thus you need to make a pool of `BertClient` and reuse them one by one.",yes thus need make pool reuse one one,issue,negative,neutral,neutral,neutral,neutral,neutral
442264444,"So, if I understand well, I need to work with a pool of `BertClient` ? ",understand well need work pool,issue,negative,neutral,neutral,neutral,neutral,neutral
442043041,"@Colanim I believe this is close to what you need https://github.com/hanxiao/bert-as-service/blob/afe66168b45053e7052a102c396e8e2e70e1a744/example4.py#L33-L37

In this example, bert-as-service is used within tf.data, which I'm currently working on, therefore in a separate branch.

The trick is making multiple `BertClient` in a row; each time take one, do encoding, then put it back; in this way your `tf.data.map()` can leverage `num_parrallel_calls` and encode strings very efficiently.

Note that there is a pending bug when making multiple `BertClient` in a row, as I wrote in #60 . for now you can simply turn off `show_server_info` as a work-round.",believe close need example used within currently working therefore separate branch trick making multiple row time take one put back way leverage encode efficiently note pending bug making multiple row wrote simply turn,issue,negative,neutral,neutral,neutral,neutral,neutral
442012286,"> > > 1. check if you can directly connect to `172.18.7.254`, via e.g. `ping`
> > > 2. check if the ports (default are 5555 and 5556) are opened and you have access to these ports, is your firewall block those ports?
> > 
> > 
> > I've tried both , I can connect to `172.18.7.254`, and I also tried use these port, neither of them works. then what can I do,please?
> 
> can you connect on localhost?
yes, and I've updated the code, there is no response when doing bc.encode(['women'])
",check directly connect via ping check default access block tried connect also tried use port neither work please connect yes code response,issue,negative,positive,neutral,neutral,positive,positive
441994878,"yeah, please checkout the latest master",yeah please latest master,issue,positive,positive,positive,positive,positive,positive
441991762,"I checked the code and figured out why it couldn't connect in my case. The codes are updated recently and my server was checked out last week, but the client are the newest. The new client is trying to connect port_out (default 5556), and cannot connect to the old server. I think you might also have the inconsistent server and client version, try to update to the newest and it may work well. ",checked code figured could connect case recently server checked last week client new client trying connect default connect old server think might also inconsistent server client version try update may work well,issue,negative,positive,neutral,neutral,positive,positive
441982846,"> > 1. check if you can directly connect to `172.18.7.254`, via e.g. `ping`
> > 2. check if the ports (default are 5555 and 5556) are opened and you have access to these ports, is your firewall block those ports?
> 
> I've tried both , I can connect to `172.18.7.254`, and I also tried use these port, neither of them works. then what can I do,please?

can you connect on localhost? ",check directly connect via ping check default access block tried connect also tried use port neither work please connect,issue,negative,positive,neutral,neutral,positive,positive
441964033,"tf 1.2 is too old and not supported, please make sure your tensorflow >= 1.10",old please make sure,issue,positive,positive,positive,positive,positive,positive
441962492,"> 1. check if you can directly connect to `172.18.7.254`, via e.g. `ping`
> 2. check if the ports (default are 5555 and 5556) are opened and you have access to these ports, is your firewall block those ports?

I've tried both ,  I can connect to `172.18.7.254`, and I also tried use these port,  neither of them works. then what can I do,please?",check directly connect via ping check default access block tried connect also tried use port neither work please,issue,negative,positive,neutral,neutral,positive,positive
441959841,"I got the similar problem. The problem happened when I use other system accounts on local or tried to use the service remotely. The BertClient could be initialized, but it was stuck when I tried to encode sentences. However, it works well when I use the system account, who has started the bert service. I checked the log, it seems that the service cannot receive any requests of other local users or remotely. 


```
bc = BertClient()

bc.encode([""as""])

```",got similar problem problem use system local tried use service remotely could stuck tried encode however work well use system account service checked log service receive local remotely,issue,negative,negative,neutral,neutral,negative,negative
441955897,"1. check if you can directly connect to `172.18.7.254`, via e.g. `ping`
2. check if the ports (default are 5555 and 5556) are opened and you have access to these ports, is your firewall blocking those ports?
",check directly connect via ping check default access blocking,issue,negative,positive,neutral,neutral,positive,positive
441864524,"this may be some bug of multi-threading in previous `BertServer`.
please checkout the latest master or try the release v1.2 and see if the problem is still there.",may bug previous please latest master try release see problem still,issue,negative,positive,positive,positive,positive,positive
441493179,looks like a driver problem. reboot may help.,like driver problem may help,issue,negative,neutral,neutral,neutral,neutral,neutral
441427594,"hi, could I put this sentence vector with pooling_strategy being CLS_TOKEN to a two-layers NN to do classification tasks?",hi could put sentence vector classification,issue,negative,neutral,neutral,neutral,neutral,neutral
441364814,"tf 1.10 is the minimum requirement as written in README.md, thanks @wqxu for making it clear again.

please upgrade your tensorflow.",minimum requirement written thanks making clear please upgrade,issue,positive,positive,positive,positive,positive,positive
441332196," I quote from https://github.com/google-research/bert/issues/164#issuecomment-441324222 

> And even if they are decent representations when fed into a DNN trained for a downstream task, it doesn't mean that they will be meaningful in terms of cosine distance. (Since cosine distance is a linear space where all dimensions are weighted equally).


Suggestion from my side, if you want to use cosine distance anyway, then please focus on the rank not the absolute value. Namely, do not use:
```
cosine(A, B) > 0.9 then A and B are similar
```
please consider the following instead:
```
if cosine(A, B) > cosine(A, C) then A is more similar to B than C.
```
",quote even decent fed trained downstream task mean meaningful cosine distance since cosine distance linear space weighted equally suggestion side want use cosine distance anyway please focus rank absolute value namely use cosine similar please consider following instead cosine cosine similar,issue,positive,negative,neutral,neutral,negative,negative
441252311,"@lucyboll I fix readme by providing more explanation for tokenization, see
https://github.com/hanxiao/bert-as-service#q-how-can-i-get-word-embedding-instead-of-sentence-embedding
https://github.com/hanxiao/bert-as-service#q-do-i-need-to-do-segmentation-for-chinese
",fix providing explanation see,issue,negative,neutral,neutral,neutral,neutral,neutral
441209179,"they are same, to certain extent (i.e. precision)
note, you should always tolerate some jitter when doing numerical comparison like this.
e.g.
```python
(abs(a[0] - a[1])<1e-6).all()
```",certain extent precision note always tolerate jitter numerical comparison like python,issue,positive,positive,positive,positive,positive,positive
441173760,"It should be :

```
from service.client import BertClient
bc = BertClient(ip='192.168.0.102', port=5555)
vectors = bc.encode(['First do it', 'then do it right', 'then do it better'])

print(vectors)
```",import right better print,issue,negative,positive,positive,positive,positive,positive
441173433,"This is my client code:
from service.client import BertClient
bc = BertClient(ip='192.168.0.102', port=5555)
bc.encode(['First do it', 'then do it right', 'then do it better'])

print(bc)",client code import right better print,issue,negative,positive,positive,positive,positive,positive
441172891,"Can you show a bit more of your code, in client side ? How do you call encode for  example and how do you print the result might help",show bit code client side call encode example print result might help,issue,negative,neutral,neutral,neutral,neutral,neutral
441166275,"I've tried and it seems to work, I got correct results. ",tried work got correct,issue,negative,neutral,neutral,neutral,neutral,neutral
441147828,"
`Traceback (most recent call last):
  File ""app.py"", line 9, in <module>
    from service.server import BertServer
  File ""/mnt/haohhxx/e/code/python3workspace/bert-as-service/service/server.py"", line 101
    jsonapi.dumps({**{'client': client.decode('ascii'),
                    ^
SyntaxError: invalid syntax
`

just like this ？",recent call last file line module import file line invalid syntax like,issue,negative,neutral,neutral,neutral,neutral,neutral
441142513,"My problem is not solved.

> usage: app.py -model_dir=./tmp/tmppclqiu4 -num_worker=4
>                  ARG   VALUE
> __________________________________________________
>       max_batch_size = 256
>          max_seq_len = 25
>            model_dir = ./tmp/tmppclqiu4
>           num_worker = 4
>        pooling_layer = -2
>     pooling_strategy = REDUCE_MEAN
>                 port = 5555
> 
> Exception in thread Thread-1:
> Traceback (most recent call last):
>   File ""/home/chenmeng/anaconda3/envs/python36/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
>     self.run()
>   File ""/home/chenmeng/project/bert-as-service/service/server.py"", line 92, in run
>     process = BertWorker(i, self.args, backend_addr, sink_thread.address)
>   File ""/home/chenmeng/project/bert-as-service/service/server.py"", line 193, in __init__
>     self.tokenizer = tokenization.FullTokenizer(vocab_file=self.vocab_fp)
>   File ""/home/chenmeng/project/bert-as-service/bert/tokenization.py"", line 115, in __init__
>     self.vocab = load_vocab(vocab_file)
>   File ""/home/chenmeng/project/bert-as-service/bert/tokenization.py"", line 77, in load_vocab
>     token = convert_to_unicode(reader.readline())
>   File ""/home/chenmeng/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py"", line 178, in readline
>     self._preread_check()
>   File ""/home/chenmeng/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py"", line 80, in _preread_check
>     compat.as_bytes(self.__name), 1024 * 512, status)
>   File ""/home/chenmeng/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 519, in __exit__
>     c_api.TF_GetCode(self.status.status))
> tensorflow.python.framework.errors_impl.NotFoundError: ./tmp/tmppclqiu4/vocab.txt; No such file or directory",problem usage value port exception thread recent call last file line file line run process file line file line file line token file line file line status file line file directory,issue,negative,neutral,neutral,neutral,neutral,neutral
441140428,"> You may see a warning message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c, running initialization to predict.` This message is expected, it just means that we are using the init_from_checkpoint() API rather than the saved model API. It's a tensorflow behavior.
> 
> Besides this warning, you should be able to run service after specifying a valid `python app.py -model_dir`, if you still face the problem, please let me know.

Thanks for the reply. I just ignored the warnings, but it still didn't work. I started the service on CentOS 7, and tried to execute the client apis, but it was stuck and had no responses. Did I miss something?  The codes are shown as follows:

Server side
```
usage: app.py -model_dir=/tmp/bert/chinese_L-12_H-768_A-12 -num_worker=1
                 ARG   VALUE
__________________________________________________
      max_batch_size = 256
         max_seq_len = 25
           model_dir = /tmp/bert/chinese_L-12_H-768_A-12
          num_worker = 1
       pooling_layer = -2
    pooling_strategy = REDUCE_MEAN
                port = 5555

WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpk49dbrch
WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7fb20e35a598>) includes params argument, but params are not passed to Estimator.
I:WORKER-2:[ser:run:227]:ready and listening
Process BertWorker-1:
Traceback (most recent call last):
  File ""/usr/lib64/python3.6/multiprocessing/process.py"", line 258, in _bootstrap
    self.run()
  File ""/home/fangzhou/workspace/github/bert-as-service/service/server.py"", line 229, in run
    for r in self.estimator.predict(input_fn, yield_single_examples=False):
  File ""/home/fangzhou/pyenv/ternary/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 488, in predict
    self._model_dir))
ValueError: Could not find trained model in model_dir: /tmp/tmpk49dbrch.
```

Client Side, no response
```
>>> bc = BertClient()
the server at localhost:5555 returns the following config:
                      ipc_sink  =       ipc://tmptzmgqG/socket
                python_version  =       3.6.3 (default, Jan  4 2018, 16:40:53)
[GCC 4.8.5 20150623 (Red Hat 4.8.5-16)]
            tensorflow_version  =       1.8.0
                   ipc_backend  =       ipc://tmpWXaVhP/socket
                max_batch_size  =       256
                          port  =       5555
                   max_seq_len  =       25
                        client  =       36109636-d435-4307-aaa0-3f16f3b16f9c
                     model_dir  =       /tmp/bert/chinese_L-12_H-768_A-12
                   server_time  =       2018-11-23 09:11:37.716187
                   num_process  =       2
                    num_worker  =       1
you should NOT see this message multiple times! if you see it appears repeatedly, please consider moving ""BertClient()"" out of the loop.
>>> bc.encode(['First do it', 'then do it right', 'then do it better'])
```

",may see warning message like could find trained model running message rather saved model behavior besides warning able run service valid python still face problem please let know thanks reply still work service tried execute client stuck miss something shown server side usage value port warning temporary folder model directory warning estimator function argument estimator ser run ready listening process recent call last file line file line run file line predict could find trained model client side response server following default red hat port client see message multiple time see repeatedly please consider moving loop right better,issue,negative,positive,positive,positive,positive,positive
441134095,"Isn't it already implemented ?

From `bert/extract_features.py`:

```
def read_examples(lst_strs):
       {...}
        m = re.match(r""^(.*) \|\|\| (.*)$"", line)
        if m is None:
            text_a = line
        else:
            text_a = m.group(1)
            text_b = m.group(2)
       {...}
```",already line none line else,issue,negative,neutral,neutral,neutral,neutral,neutral
441068855,"@hanxiao thank you that would be awesome! I was looking at the bert repo classification task here https://github.com/google-research/bert/blob/60454702590a6c69bd45c5d4258c7e17b8a3e1da/run_classifier.py
By the way this is far more complex  how it could actually be.",thank would awesome looking classification task way far complex could actually,issue,positive,positive,positive,positive,positive,positive
441060722,"@lucyboll thanks for pointing it out, i believe it's a bug. will fix it",thanks pointing believe bug fix,issue,negative,positive,positive,positive,positive,positive
441058614,"Second-to-last layer is less biased towards the training targets (i.e. masked LM and next sentence prediction), it's also what the BERT author suggested:
https://github.com/google-research/bert/issues/71#issuecomment-436507081

Of course, these are mostly intuition so far. I will conduct more comprehensive experiments to validate the choice, meanwhile feel free to run and test on your downstream task.",layer le towards training masked next sentence prediction also author course mostly intuition far conduct comprehensive validate choice meanwhile feel free run test downstream task,issue,positive,positive,positive,positive,positive,positive
441057780,"currently no, but may be a good future feature.",currently may good future feature,issue,negative,positive,positive,positive,positive,positive
441056625,"You may see a warning message like `Could not find trained model in model_dir: /tmp/tmpuB5g5c, running initialization to predict.` This message is expected, it just means that we are using the init_from_checkpoint() API rather than the saved model API. It's a tensorflow behavior.

Besides this warning, you should be able to run service after specifying a valid `python app.py -model_dir`, if you still face the problem, please let me know.",may see warning message like could find trained model running message rather saved model behavior besides warning able run service valid python still face problem please let know,issue,negative,positive,positive,positive,positive,positive
441055173,"Currently, it doesn't support nor be tested on Windows, as Windows doesn't have `ipc` protocol, which is used to communicate between ventilator, worker and sink. 
Quick solution for now would be change all `icp` to valid `tcp` address as @KevinChen1994 suggested.
 
I marked it as an enhancement-feature and will fix it in the next version.",currently support tested protocol used communicate ventilator worker sink quick solution would change valid address marked fix next version,issue,positive,positive,positive,positive,positive,positive
441052439,"yes. client-side for sure support GPU and CPU-only.
Server side supports CPU since #4 . You can run it on CPU without any problem.
Note the server doesn't supports GPU+CPU, or no available GPU thus back-off to CPU, though shouldn't be hard to support it.",yes sure support server side since run without problem note server available thus though hard support,issue,positive,positive,positive,positive,positive,positive
440984647,How can I obtain word embedding instead of character embedding please?,obtain word instead character please,issue,negative,neutral,neutral,neutral,neutral,neutral
440962033,"@hanxiao  I also encountered this problem,I tried the methods you provided，but not work.  Some irrelevant sentences will also calculate very high similarities（cosin）.",also problem tried work irrelevant also calculate high,issue,negative,negative,negative,negative,negative,negative
440961435,I want to know why the 0_PAD matrix is meaningless,want know matrix meaningless,issue,negative,negative,negative,negative,negative,negative
440953669,"I am curious, how did you know the embedding correspond to which word / character ?
",curious know correspond word character,issue,negative,negative,neutral,neutral,negative,negative
440949040,"> I have encountered the same problem, change the path to this：./tmp/tmp_st5oe05 may solve your problem.

But I don't have the path named ./tmp. Every time I run app.py, it will create two tempfile, in those file only have a socket file",problem change path may solve problem path every time run create two file socket file,issue,negative,neutral,neutral,neutral,neutral,neutral
440947616,"I have encountered the same problem, change the path to this：./tmp/tmp_st5oe05 may solve your problem.",problem change path may solve problem,issue,negative,neutral,neutral,neutral,neutral,neutral
440945666,"> > I sovled this question. You can check version of your pyzmq, it must >= 17.1.0. you can use conda to install or update this library. Try it!
> 
> @KevinChen1994 I just tried it, but still not working.
> 
> ```
> pip freeze list
> ...
> pyzmq==17.1.2
> requests==2.19.1
> scipy==1.1.0
> seaborn==0.9.0
> six==1.11.0
> tensorboard==1.12.0
> tensorflow==1.12.0
> termcolor==1.1.0
> tflearn==0.3.2
> urllib3==1.23
> Werkzeug==0.14.1
> wincertstore==0.2
> You are using pip version 9.0.1, however version 18.1 is available.
> You should consider upgrading via the 'python -m pip install --upgrade pip' command.
> 
> F:\Work\Github\bert-as-service (master)
> (pgportfolio) λ python app.py -model_dir F:\data\chinese_L-12_H-768_A-12\chinese_L-12_H-768_A-12\
> usage: app.py -model_dir F:\data\chinese_L-12_H-768_A-12\chinese_L-12_H-768_A-12\
>                  ARG   VALUE
> __________________________________________________
>       max_batch_size = 256
>          max_seq_len = 25
>            model_dir = F:\data\chinese_L-12_H-768_A-12\chinese_L-12_H-768_A-12\
>           num_worker = 1
>        pooling_layer = -2
>     pooling_strategy = REDUCE_MEAN
>                 port = 5555
> 
> Exception in thread Thread-1:
> Traceback (most recent call last):
>   File ""D:\Anaconda3\envs\pgportfolio\lib\threading.py"", line 916, in _bootstrap_inner
>     self.run()
>   File ""F:\Work\Github\bert-as-service\service\server.py"", line 72, in run
>     self.backend.bind('ipc://*')
>   File ""zmq\backend\cython\socket.pyx"", line 547, in zmq.backend.cython.socket.Socket.bind
>   File ""zmq\backend\cython\checkrc.pxd"", line 25, in zmq.backend.cython.checkrc._check_rc
> zmq.error.ZMQError: Protocol not supported
> ```

Maybe pyzmq depend on other library. Try to install libsodium and zeromq. If you success just tell me.",question check version must use install update library try tried still working pip freeze list pip version however version available consider via pip install upgrade pip command master python usage value port exception thread recent call last file line file line run file line file line protocol maybe depend library try install success tell,issue,positive,positive,positive,positive,positive,positive
440942218,"> I sovled this question. You can check version of your pyzmq, it must >= 17.1.0. you can use conda to install or update this library. Try it!

@KevinChen1994 I just tried it, but still not working.

```
pip freeze list
...
pyzmq==17.1.2
requests==2.19.1
scipy==1.1.0
seaborn==0.9.0
six==1.11.0
tensorboard==1.12.0
tensorflow==1.12.0
termcolor==1.1.0
tflearn==0.3.2
urllib3==1.23
Werkzeug==0.14.1
wincertstore==0.2
You are using pip version 9.0.1, however version 18.1 is available.
You should consider upgrading via the 'python -m pip install --upgrade pip' command.

F:\Work\Github\bert-as-service (master)
(pgportfolio) λ python app.py -model_dir F:\data\chinese_L-12_H-768_A-12\chinese_L-12_H-768_A-12\
usage: app.py -model_dir F:\data\chinese_L-12_H-768_A-12\chinese_L-12_H-768_A-12\
                 ARG   VALUE
__________________________________________________
      max_batch_size = 256
         max_seq_len = 25
           model_dir = F:\data\chinese_L-12_H-768_A-12\chinese_L-12_H-768_A-12\
          num_worker = 1
       pooling_layer = -2
    pooling_strategy = REDUCE_MEAN
                port = 5555

Exception in thread Thread-1:
Traceback (most recent call last):
  File ""D:\Anaconda3\envs\pgportfolio\lib\threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""F:\Work\Github\bert-as-service\service\server.py"", line 72, in run
    self.backend.bind('ipc://*')
  File ""zmq\backend\cython\socket.pyx"", line 547, in zmq.backend.cython.socket.Socket.bind
  File ""zmq\backend\cython\checkrc.pxd"", line 25, in zmq.backend.cython.checkrc._check_rc
zmq.error.ZMQError: Protocol not supported
```",question check version must use install update library try tried still working pip freeze list pip version however version available consider via pip install upgrade pip command master python usage value port exception thread recent call last file line file line run file line file line protocol,issue,negative,positive,positive,positive,positive,positive
440938301,"I sovled this question. You can check version of your pyzmq, it must >= 17.1.0. you can use conda to install or update this library. Try it!",question check version must use install update library try,issue,negative,neutral,neutral,neutral,neutral,neutral
440936853,"I had the same question. When I run app.py on my Mac, It can run successfully. But on CentOS 7 it run failed. My parameters is same.",question run mac run successfully run,issue,negative,positive,positive,positive,positive,positive
440847462,"Is there any advantage of using something else than the last layer ? Concatenating several last layers increase accuracy (from the BERT paper), but taking a single layer other than the last layer will decrease accuracy, right ?",advantage something else last layer several last increase accuracy paper taking single layer last layer decrease accuracy right,issue,positive,positive,neutral,neutral,positive,positive
440615756,"in the current version, `-pooling_layer=-4` means fourth-to-last, namely the 9th layer of a 12-layer BERT or the 21st layer of a 24-layer BERT.

i will allow `-pooling_layer` to be a list in the future, so you can use it as `-pooling_layer=[-4,-3,-2,-1]`.",current version namely th layer st layer allow list future use,issue,negative,neutral,neutral,neutral,neutral,neutral
440490202,"Thanks for the clarification.

One last question :
If I declare the `BertClient` inside my generator, it will allow multi-threading. But by doing so, it means that a single thread will redeclare a new instance of `BertClient` every batch.

**Which one is better ?**
 - Declare new `BertClient` every batch, but allowing multi-threading ?
 - Declare just one shared `BertClient` among all bacthes, not allowing multi-threading ?",thanks clarification one last question declare inside generator allow single thread redeclare new instance every batch one better declare new every batch declare one among,issue,positive,positive,positive,positive,positive,positive
440489516,"i think this is a quite common issue, will add this to faq",think quite common issue add,issue,negative,negative,negative,negative,negative,negative
440487707,"hi, just fyi, the client-side supports multi-thread/process. that’s part of the design principle. It’s just that you can’t reuse a BertClient in multiple threads/processes. For example:

# BAD
```python
bc = BertClient()

# in Proc1/Thread1 scope:
bc.encode(lst_str)

# in Proc2/Thread2 scope:
bc.encode(lst_str)
```

Instead, please do:

# GOOD
```python
# in Proc1/Thread1 scope:
bc1 = BertClient()
bc1.encode(lst_str)

# in Proc2/Thread2 scope:
bc2 = BertClient()
bc2.encode(lst_str)
```
",hi part design principle reuse multiple example bad python scope scope instead please good python scope scope,issue,negative,positive,neutral,neutral,positive,positive
440478021,"This is due to multi-threading. Without multi-threading on client-side, this error does not appear anymore.

---

Note :
I put `bc.encode(...)` on a Keras generator. By default, Keras `fit_generator` have the argument `multi-threading` to `False`. However this is not enough, you need to also set the argument `workers` to `0` (default to `1`), or several request will be done at the same time, resulting in above error.",due without error appear note put generator default argument false however enough need also set argument default several request done time resulting error,issue,negative,negative,negative,negative,negative,negative
440291217,"Roughly:
- model starts only once
- input pipeline starts only once, note that tf.data pipeline is just a queue, so we need to set it to ""on-hold"" until new input data arrives.
- do estimator.predict() this will on-hold because input pipeline is empty for now.
- we are done with initialization, now feed the data to the input pipeline, and get results from predict()

please refer to [service.py](https://github.com/hanxiao/bert-as-service/blob/master/service/server.py) for more details.
",roughly model input pipeline note pipeline queue need set new input data input pipeline empty done feed data input pipeline get predict please refer,issue,negative,negative,neutral,neutral,negative,negative
440288923,"my experience is that the Chinese BERT released by Google gives reasonable results in simple clustering task, i.e. sentences have similar semantics are gathered. I also observe this applies to text from other domains (the released BERT is trained on Chinese Wiki).

Some remarks may help you solve the problem:
1. Chinese BERT is a character-based model. No need to do segmentation beforehand.
2. Try different pooling strategy, please read https://github.com/hanxiao/bert-as-service#q-what-are-the-available-pooling-strategies
3. How long is your sentence? It shouldn't be too short, say less than 5 Chinese characters. It shouldn't be too long, if the sequence  is longer than `max_seq_len` defined on the server side, the sequence will be truncated on the right side. Say you have a ""sentence"" that has 500 characters, whereas server-side `max_seq_len=20`, then the last 480 characters are not fed to BERT at all.",experience reasonable simple clustering task similar semantics also observe text trained may help solve problem model need segmentation beforehand try different strategy please read long sentence short say le long sequence longer defined server side sequence truncated right side say sentence whereas last fed,issue,positive,positive,neutral,neutral,positive,positive
440283930,"@susht3 fyi, this is done in [v1.1](https://github.com/hanxiao/bert-as-service/tree/v1.1), please check out latest master and [this thread](https://github.com/hanxiao/bert-as-service#q-how-can-i-get-word-embedding-instead-of-sentence-embedding) for more details.",done please check latest master thread,issue,negative,positive,positive,positive,positive,positive
440283313,"@lucyboll fyi, this is done in [v1.1](https://github.com/hanxiao/bert-as-service/tree/v1.1), please check out latest master and [this thread](https://github.com/hanxiao/bert-as-service#q-how-can-i-get-word-embedding-instead-of-sentence-embedding) for more details.

close #25 ",done please check latest master thread close,issue,negative,positive,positive,positive,positive,positive
440148627,"this issue is duplicated with #23 , which I will implement soon.
@susht3 also notice that the service does not return the last hidden states of a sequence, please refer to https://github.com/hanxiao/bert-as-service#q-what-are-the-available-pooling-strategies",issue implement soon also notice service return last hidden sequence please refer,issue,negative,negative,neutral,neutral,negative,negative
440103924,"correct. that’s why i said only API of my side needs to be changed a little
bit.

Colanim <notifications@github.com>于2018年11月20日 周二上午9:27写道：

> Word vectors is already supported by BERT itself. See Readme of Bert
> <https://github.com/google-research/bert#using-bert-to-extract-fixed-feature-vectors-like-elmo>
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/hanxiao/bert-as-service/issues/25#issuecomment-440102893>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AB8l6qXPutqAvA2QlCfl-Uu6u4i60DRkks5uw1p0gaJpZM4YpQE2>
> .
>
",correct said side need little bit word already see thread reply directly view mute thread,issue,negative,negative,neutral,neutral,negative,negative
439815880,"good point. in principle, there is nothing stops us from getting the word embedding. I also understand the importance of this feature as people may want to use this service in an ELMO-like fashion. It's just the API and interface might be changed a little bit when considering word embedding.

So point taken, I will consider it as a feature and implement it soon.",good point principle nothing u getting word also understand importance feature people may want use service fashion interface might little bit considering word point taken consider feature implement soon,issue,positive,positive,positive,positive,positive,positive
439662923,"Thanks! Supporting classification isn't hard. I will implement it as a feature in the future.

For now, I'd like to implement more pooling strategies.",thanks supporting classification hard implement feature future like implement,issue,positive,positive,neutral,neutral,positive,positive
438954846,"OOPS, my bad, all GPU's were allocated by my other microservice. After, i killed that process, it worked perfectly fine.

```
app.py -num_worker=4 -model_dir ../model/chinese_L-12_H-768_A-12/
parameters: 
      max_batch_size = 256
         max_seq_len = 25
           model_dir = ../model/chinese_L-12_H-768_A-12/
          num_worker = 4
                port = 5555
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpizz2yhn6
WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f50e8164488>) includes params argument, but params are not passed to Estimator.
I:[server.py:191]:worker 0 is ready and listening
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpxldufu5g
WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f50e7623048>) includes params argument, but params are not passed to Estimator.
I:[server.py:191]:worker 1 is ready and listening
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpzez93h0n
WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f50e7623730>) includes params argument, but params are not passed to Estimator.
I:[server.py:191]:worker 2 is ready and listening
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmph47wvze9
WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f50e7623ae8>) includes params argument, but params are not passed to Estimator.
I:[server.py:191]:worker 3 is ready and listening
I:[server.py:153]:available workers:  3	job queue:   0	pending clients:   1
I:[server.py:195]:job b'8b0f03e7-22d1-4852-b6cb-51060d95263d' is done in 0.02s
```
Server is serving and client was able to access the the service.
Thanks @hanxiao .
will close the isssue.",bad process worked perfectly fine port warning temporary folder model directory warning estimator function argument estimator worker ready listening warning temporary folder model directory warning estimator function argument estimator worker ready listening warning temporary folder model directory warning estimator function argument estimator worker ready listening warning temporary folder model directory warning estimator function argument estimator worker ready listening available job queue pending job done server serving client able access service thanks close,issue,negative,positive,positive,positive,positive,positive
438944381,"hmm `W:[server.py:85]:only 0 GPU(s) is available, but ask for 4`, this basically says it can't find any GPU available on this machine, maybe they are occupied?

regarding the hanging issue:
1. check whether server is running already, usually you have to wait for about 5s even after seeing `worker-x is ready and listening`
2. check the port, ip address",available ask basically ca find available machine maybe regarding hanging issue check whether server running already usually wait even seeing ready listening check port address,issue,negative,positive,positive,positive,positive,positive
438903599,"wow, really great! thanks a lot",wow really great thanks lot,issue,positive,positive,positive,positive,positive,positive
438712121,"check whether you fulfill the dependency requirements, i.e.
requirements.txt in this repo

it won’t work on python2 or tf <1.10

YUWEI <notifications@github.com>于2018年11月14日 周三下午11:43写道：

> Traceback (most recent call last):
>   File ""app.py"", line 8, in <module>
>     from service.server import BertServer
>   File ""/home/123/bert-as-service/service/server.py"", line 12, in <module>
>     import tensorflow as tf
>   File ""/home/123/.local/lib/python3.5/site-packages/tensorflow/__init__.py"", line 24, in <module>
>     from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
>   File ""/home/123/.local/lib/python3.5/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
>     from tensorflow.python import pywrap_tensorflow
>   File ""/home/s1902744/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 25, in <module>
>     from tensorflow.python.platform import self_check
> ImportError: No module named 'tensorflow.python.platform'
>
> Thanks
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/hanxiao/bert-as-service/issues/9>, or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AB8l6t7DaDc2iUEsHflyuiszZiwK43qMks5uvDoogaJpZM4YeAmX>
> .
>
",check whether fulfill dependency work python recent call last file line module import file line module import file line module import file line module import file line module import module thanks thread reply directly view mute thread,issue,positive,positive,neutral,neutral,positive,positive
438513496,"hi, could you checkout this branch https://github.com/hanxiao/bert-as-service/tree/server-v2 and test it? should work on cpu now.",hi could branch test work,issue,negative,neutral,neutral,neutral,neutral,neutral
438501214,"i will do some refactoring on the service to support cpu.

i will check how [SEP] is used in their paper and original code.
",service support check used paper original code,issue,positive,positive,positive,positive,positive,positive
438375547,"Can I also ask if you did include also [SEP] token embedding. 
I saw that you excluded [CLS] token, and you explained well why. 
Thanks ",also ask include also token saw token well thanks,issue,positive,positive,positive,positive,positive,positive
438320622,"If I understood well, I am already using BERT using google official repo and feature extractor works also on cpu. 
",understood well already official feature extractor work also,issue,negative,neutral,neutral,neutral,neutral,neutral
438314348,"hmm...never thought that, as BERT is actually a pretty large and computational intensive model.
but maybe supporting CPU is something meaningful for production. I will consider it as a future feature.",never thought actually pretty large computational intensive model maybe supporting something meaningful production consider future feature,issue,negative,positive,positive,positive,positive,positive
438301241,"Thanks!

1. It supports multi-lingual BERT off-the-shelf, I gave an English example in `README.md` but I'm using Chinese BERT pretrained by Google without any problem. No modification is needed.

2. Looks like you are using python 2, where `super()` (i.e. threading.Thread) is defined differently vs. in py3. Please make sure you are using python > 3.5 and tensorflow >1.10 and fulfill `requirements.txt`
",thanks gave example without problem modification like python super defined differently please make sure python fulfill,issue,positive,positive,positive,positive,positive,positive
437884403,"oh, sorry, I forgot to remove unnecessary code in the helper.py. 
ruamel_yaml is not needed.",oh sorry forgot remove unnecessary code,issue,negative,negative,negative,negative,negative,negative
