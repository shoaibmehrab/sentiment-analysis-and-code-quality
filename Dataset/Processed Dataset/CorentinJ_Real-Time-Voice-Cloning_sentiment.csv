id,original_comment,processed_comment,source,sentiment_VADER,sentiment_textblob,sentiment_pattern,sentiment_bert,sentiment_spacy,max_voted_sentiment
1991199765,"Bro, `np.float` is a deprecated alias for the built in `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here. Deprecated in NumPy 1.20; for more details and guidance: [numpy.org/devdocs/release/1.20.0-notes.html#deprecations](url)(https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations)

This was the standard python `float` object, but as mentioned, `numpy.float` has been deprecated... and removed in 1.24. You can either use `float` or pick one of the `np.float32`, `np.float64`, `np.float128`.",alias built float silence warning use float modify behavior safe specifically scalar type use guidance standard python float object removed either use float pick one,issue,negative,positive,positive,positive,positive,positive
1990983897,"I have completed all steps, just before running this command - python demo_toolbox.py i am not getting Ui window also my CMD do not showing any error just this is show: kindly anyone resolve this issue:
@DrStoop @projoy @freecui @CorentinJ @andresgomesz 
![WhatsApp Image 2024-03-12 at 12 55 08_ad8b91c4](https://github.com/CorentinJ/Real-Time-Voice-Cloning/assets/107794270/1c6e5d3a-1ea4-47f2-86f6-a9ebfebaa08a)
![WhatsApp Image 2024-03-12 at 12 55 08_ad8b91c4](https://github.com/CorentinJ/Real-Time-Voice-Cloning/assets/107794270/8feca8ec-88db-4fa7-8e46-7a5965f1f07f)
",running command python getting window also showing error show kindly anyone resolve issue image image,issue,negative,positive,positive,positive,positive,positive
1974898225,Any luck? I'm facing the exact same issue.,luck facing exact issue,issue,negative,positive,positive,positive,positive,positive
1932702848,"I downgraded the python version on my brother's computer and it worked as well, thanks for your help again ;)",python version brother computer worked well thanks help,issue,positive,positive,positive,positive,positive,positive
1930076172,"Thank you for trying to help me, well I switched over to linux mint because I had a few issues with fedora and it now works perfectly, it appears that is the python version because I tried to install the same program on my brother's computer (he uses windows) and there was the same issue with the latest python version, I looked the link and it appears that solves the issues for linux, but maybe do you know by any chance how to install python-dev on windows, if not it's okay, thanks again for your help ;) @brothersw ",thank trying help well switched mint work perfectly python version tried install program brother computer issue latest python version link maybe know chance install thanks help,issue,positive,positive,positive,positive,positive,positive
1926755859,"I will not maintain it anymore except to fix critical bugs, as for other recommendations I give a few in the readme:
https://github.com/CorentinJ/Real-Time-Voice-Cloning?tab=readme-ov-file#heads-up",maintain except fix critical give,issue,negative,neutral,neutral,neutral,neutral,neutral
1912429904,"How to solve load LibriSpeech Train-clean-100. Even after calling that correctly the modules are not getting loaded:

Warning: you do not have any of the recognized datasets in  The recognized datasets are: LibriSpeech/dev-clean LibriSpeech/dev-other LibriSpeech/test-clean LibriSpeech/test-other LibriSpeech/train-clean-100 LibriSpeech/train-clean-360 LibriSpeech/train-other-500 LibriTTS/dev-clean LibriTTS/dev-other LibriTTS/test-clean ",solve load even calling correctly getting loaded warning,issue,negative,neutral,neutral,neutral,neutral,neutral
1873022210,"I just started tinkering with this so I'm sure there will be more errors to find but it is a version problem I think.

In encoder/audio.py line 58 
The line frames = librosa.feature.melspectrogram(wav, sampling_rate, ...) uses positional arguments..., 
which is incompatible with the newer librosa version you're using.
Change wav and sampling_rate to: y=wav, and sr=sampling_rate

That got it to run for me.
",sure find version problem think line line positional incompatible version change got run,issue,negative,positive,positive,positive,positive,positive
1872616475,"Yes, it is open source as it says in the licence
""Permission is hereby granted, free of charge, to any person obtaining a copy of this software""",yes open source permission hereby free charge person copy,issue,positive,positive,positive,positive,positive,positive
1872208635,"RuntimeError in Realtime Voice Cloning can occur due to various reasons, such as incorrectly configured parameters or incorrect use of the program. Here are some ways to solve this problem:

1. Verify that all necessary libraries and dependencies are correctly installed and configured for Realtime Voice Cloning to work. Make sure all components are installed and configured correctly.

2. Make sure that you are using a version of the program and all dependencies that is compatible with each other. Some errors may occur due to version incompatibility.

3. Check that the data and program parameters are entered correctly. Make sure all files and paths are correct.

4. Try the community or forums for Realtime Voice Cloning. Perhaps someone has already encountered a similar problem and can help solve it.

5. Update the program and all dependencies to the latest versions. The bug may have been fixed in the new version.",voice occur due various incorrectly incorrect use program way solve problem verify necessary correctly voice work make sure correctly make sure version program compatible may occur due version incompatibility check data program correctly make sure correct try community voice perhaps someone already similar problem help solve update program latest bug may fixed new version,issue,positive,positive,positive,positive,positive,positive
1868583382,"> pip install librosa==0.9.2

Thank you so much, this solved the error",pip install thank much error,issue,negative,positive,positive,positive,positive,positive
1868234179,"In 2023, I think the goto method for this purpose is Retrieval Based Voice Conversion (RVC): For anyone here looking to do the same thing as the author of this issue. :)",think method purpose retrieval based voice conversion anyone looking thing author issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1866584350,"I don't know if this is ""The right way"" or even the working way, but this is what I did. It seems to have helped but it's a little intermittent.

I downloaded NVidia's CUDA-MEMCHECK utility from their developer website.  After installing the file which was 3GB, everything was like speed central. I'm rendering 8k images in seconds now.  I suspect this issue is mainly a Windows one. I didn't actually do anything with the files it installed. 

I also added this to my python at the start
```
torch.cuda.init()  # Initialize CUDA
torch.cuda.empty_cache()  # Clear cache if needed
torch.cuda.reset_peak_memory_stats()  # Reset peak memory stats - Potentially not wise to add this.
```
Then this at the end. 
```
torch.cuda.empty_cache()
```



",know right way even working way little intermittent utility developer file everything like speed central rendering suspect issue mainly one actually anything also added python start initialize clear cache reset peak memory potentially wise add end,issue,positive,positive,positive,positive,positive,positive
1848005571,"and the fuction preprocess.waw in audio.py: 

def preprocess_wav(fpath_or_wav: Union[str, Path, np.ndarray],
                   source_sr: Optional[int] = None,
                   normalize: Optional[bool] = True,
                   trim_silence: Optional[bool] = True):
    """"""
    Applies the preprocessing operations used in training the Speaker Encoder to a waveform 
    either on disk or in memory. The waveform will be resampled to match the data hyperparameters.

    :param fpath_or_wav: either a filepath to an audio file (many extensions are supported, not 
    just .wav), either the waveform as a numpy array of floats.
    :param source_sr: if passing an audio waveform, the sampling rate of the waveform before 
    preprocessing. After preprocessing, the waveform's sampling rate will match the data 
    hyperparameters. If passing a filepath, the sampling rate will be automatically detected and 
    this argument will be ignored.
    """"""
    # Load the wav from disk if needed
    if isinstance(fpath_or_wav, str) or isinstance(fpath_or_wav, Path):
        wav, source_sr = librosa.load(str(fpath_or_wav), sr=None)
    else:
        wav = fpath_or_wav
    
    # Resample the wav if needed
    if source_sr is not None and source_sr != sampling_rate:
        wav = librosa.resample(wav, source_sr, sampling_rate)
        
    # Apply the preprocessing: normalize volume and shorten long silences 
    if normalize:
        wav = normalize_volume(wav, audio_norm_target_dBFS, increase_only=True)
    if webrtcvad and trim_silence:
        wav = trim_long_silences(wav)
    
    return wav
",union path optional none normalize optional bool true optional bool true used training speaker either disk memory match data param either audio file many either array param passing audio sampling rate sampling rate match data passing sampling rate automatically argument load disk path else resample none apply normalize volume shorten long normalize return,issue,negative,positive,positive,positive,positive,positive
1847968516,"hi the CODE is this;
im have the same error 


#@title Record or Upload
#@markdown * Either record audio from microphone or upload audio from file (.mp3 or .wav) 
!pip install --upgrade librosa
SAMPLE_RATE = 22050
record_or_upload = ""Upload (.mp3 or .wav)"" #@param [""Record"", ""Upload (.mp3 or .wav)""]
record_seconds =   10#@param {type:""number"", min:1, max:10, step:1}

embedding = None
def _compute_embedding(audio):
  display(Audio(audio, rate=SAMPLE_RATE, autoplay=True))
  global embedding
  embedding = None
  embedding = encoder.embed_utterance(encoder.preprocess_wav(audio, SAMPLE_RATE)) #in this line it is the error 
def _record_audio(b):
  clear_output()
  audio = record_audio(record_seconds, sample_rate=SAMPLE_RATE)
  _compute_embedding(audio)
def _upload_audio(b):
  clear_output()
  audio = upload_audio(sample_rate=SAMPLE_RATE)
  _compute_embedding(audio)

if record_or_upload == ""Record"":
  button = widgets.Button(description=""Record Your Voice"")
  button.on_click(_record_audio)
  display(button)
else:
  #button = widgets.Button(description=""Upload Voice File"")
  #button.on_click(_upload_audio)
  _upload_audio("""")",hi code error title record markdown either record audio microphone audio file pip install upgrade param record param type number min step none audio display audio audio global none audio line error audio audio audio audio record button record voice display button else button voice file,issue,negative,neutral,neutral,neutral,neutral,neutral
1833115853,"i cant solve this even after installing librosa 0.9.2, i downgraded to 0.8.0 too
can anyone tell what version of python you were using pls",cant solve even anyone tell version python,issue,negative,neutral,neutral,neutral,neutral,neutral
1827536391,Aborted (core dumped) I am getting this error,aborted core getting error,issue,negative,neutral,neutral,neutral,neutral,neutral
1823989853,"Hello ,I run this on windows 10. 
![image](https://github.com/CorentinJ/Real-Time-Voice-Cloning/assets/110754016/3e8de359-ea17-4224-be18-1cf51ead1f2a)
what does this mean exactly?",hello run image mean exactly,issue,negative,negative,neutral,neutral,negative,negative
1801752186,"Hi Guys!

@OmairAhmad1998  if you are using most resent numpy version, you need to edit the code ""numpy.int32"" to int only, same for ""numpy.float32"", to dtype=float.

Also, I need help, I have an error when I trying to load an dataset using the toolbox interface, when I click on ""load"" I am getting this error, img attached, ""Exception: load() takes 1 positional argument but 2 were given""

I have not idea why, I do not modify the code. Do you have any ideas?

![error](https://github.com/CorentinJ/Real-Time-Voice-Cloning/assets/1333581/96be4994-1569-48e5-81e3-347549c94a3b)
",hi resent version need edit code also need help error trying load toolbox interface click load getting error attached exception load positional argument given idea modify code error,issue,negative,neutral,neutral,neutral,neutral,neutral
1791940472,"[I've made public a repo with a workflow for creating a dataset to perform synthesizer fine tuning.](https://github.com/samoliverschumacher/voice-cloning-workflow)

Not sure if this is the best place to let people know, but hopefully it helps someone.",made public perform synthesizer fine tuning sure best place let people know hopefully someone,issue,positive,positive,positive,positive,positive,positive
1771653615,"Nice, so I think you can happily close this issue :)",nice think happily close issue,issue,positive,positive,positive,positive,positive,positive
1769297110,I tried the numpy version mentioned in the requirement.txt and realized that version of numpy could not be installed on python 3.10.12 and so I used older version of python which is 3.8 and that version of numpy was successfully installed.,tried version version could python used older version python version successfully,issue,negative,positive,positive,positive,positive,positive
1769213405,"I am not sure about the quality either. If I use the samples provided, I can generate reasonably good speech. If I use my own (e.g., by recording it through the UI), I was not able to produce any valuable output. ",sure quality either use provided generate reasonably good speech use recording able produce valuable output,issue,positive,positive,positive,positive,positive,positive
1769211940,Please read and follow the [setup](https://github.com/CorentinJ/Real-Time-Voice-Cloning#setup),please read follow setup,issue,negative,neutral,neutral,neutral,neutral,neutral
1769210738,"What exactly are you looking for? It is ""released""; everything's working for me (Mac).",exactly looking everything working mac,issue,negative,positive,positive,positive,positive,positive
1769210154,Seems like you just got to install the missing module(s) via _pip_,like got install missing module via,issue,negative,negative,negative,negative,negative,negative
1769208964,Meaning what? Why are you planning to use it on python3.10 if it does on 3.8? What about using a different numpy version running on python3.10 and then trying again?,meaning use python different version running python trying,issue,negative,neutral,neutral,neutral,neutral,neutral
1763458776,"Hello guyz. I am facing the issue when i try to import audio file (TypeError: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool. ) please someone help me , it's really important
",hello facing issue try import audio file ca convert type float float float complex complex bool please someone help really important,issue,positive,negative,neutral,neutral,negative,negative
1751921795,"> I was messing around a little bit and managed to get it working with Windows 11. I installed these:
> 
> pip install inflect==5.3.0 pip install librosa==0.9.2 pip install matplotlib==3.5.1 pip install numpy==1.20.3 pip install Pillow==8.4.0 pip install PyQt5==5.15.6 pip install scikit-learn==1.0.2 pip install scipy==1.7.3 pip install sounddevice==0.4.3 pip install SoundFile==0.10.3.post1 pip install tqdm==4.62.3 pip install umap-learn==0.5.2 pip install Unidecode==1.3.2 pip install urllib3==1.26.7 pip install visdom==0.1.8.9 pip install webrtcvad==2.0.10
> 
> I'm not sure if that was everything I did, but after continuously getting that error I went ahead and pasted this list in the Anaconda Prompt.
> 
> I was also getting ""...anaconda3\lib\site-packages\paramiko\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated ""class"": algorithms.Blowfish,"". This seems to not matter anymore since it's up and running?

Nice work it worked for me... I guess it can only work for windows 11 users

",messing around little bit get working pip install pip install pip install pip install pip install pip install pip install pip install pip install pip install post pip install pip install pip install pip install pip install pip install sure everything continuously getting error went ahead pasted list anaconda prompt also getting blowfish class matter since running nice work worked guess work,issue,negative,positive,positive,positive,positive,positive
1739324602,"Tks @stsykin!

I was able to install it by following these steps.

1.  Using iTerm [Duplication terminal in macOS Ventura](https://stackoverflow.com/questions/74198234/duplication-of-terminal-in-macos-ventura)
2. Your suggestion",able install following duplication terminal suggestion,issue,negative,positive,positive,positive,positive,positive
1732296011,"Thanks for the tip!
I had the following error with the melspectrogram function:

> TypeError: melspectrogram() takes 0 positional arguments but 1 positional argument (and 1 keyword-only argument) were given

Installing librosa 0.9.2 fixed it",thanks tip following error function positional positional argument argument given fixed,issue,negative,positive,positive,positive,positive,positive
1728346984,"> Thank you I solved, I had a typo in the script!

@ireneb612 what typo, bcos i am facing the same issue
![Uploading Screenshot (114).png…]()
",thank typo script typo facing issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1728339599,"hey! i kept on getting this message each time i tries loading a sample audio file. please i need help

![Screenshot (114)](https://github.com/CorentinJ/Real-Time-Voice-Cloning/assets/145538350/37aa2916-30e8-4aa5-8c61-e395fa034d99)

File ""C:\Users\PC\Desktop\voice\toolbox\__init__.py"", line 94, in <lambda>
    func = lambda: self.load_from_browser(self.ui.browse_file())
  File ""C:\Users\PC\Desktop\voice\toolbox\__init__.py"", line 160, in load_from_browser
    self.add_real_utterance(wav, name, speaker_name)
  File ""C:\Users\PC\Desktop\voice\toolbox\__init__.py"", line 180, in add_real_utterance
    encoder_wav = encoder.preprocess_wav(wav)
  File ""C:\Users\PC\Desktop\voice\encoder\audio.py"", line 48, in preprocess_wav
    wav = trim_long_silences(wav)
  File ""C:\Users\PC\Desktop\voice\encoder\audio.py"", line 83, in trim_long_silences
    pcm_wave = struct.pack(""%dh"" % len(wav), *(bool(wav * int16_max)).astype(np.int16))
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",hey kept getting message time loading sample audio file please need help file line lambda lambda file line name file line file line file line bool truth value array one element ambiguous use,issue,positive,neutral,neutral,neutral,neutral,neutral
1727177655,"Hi @Soul0702, when you read the error messages closely it's telling you that somewhere in your scripts there is a ""np.float"" explicitly used which is depreciated and it also advises you what to replace it with. I would recommend to try to replace ""np.float"" with ""np.float64"" first (not ""float"") as this repo is based on numpy (a.f.a.I.c.r.). Probably you'll find the ""np.float"" in ""C:\User\PC\Desktop\voice\toolbox\ui.py"", if not check the other files listed.
General advice, [openAI's ChatGPT](https://openai.com/blog/chatgpt) is often a good adviser for coding errors and how-to-questions as long as they don't go too much into depth.
Good luck!",hi soul read error closely telling somewhere explicitly used also replace would recommend try replace first float based probably find check listed general advice often good adviser long go much depth good luck,issue,positive,positive,positive,positive,positive,positive
1727111678,"Hey @DrStoop I was trying to launch the app but it kept giving me this

![Screenshot (107)~2](https://github.com/CorentinJ/Real-Time-Voice-Cloning/assets/145538350/5f3b7429-f87b-4c2d-add8-d3585ddb6dd3)

AttributeError: module 'numpy' has no attribute 'float'.
`np.float` was a deprecated alias for the builtin `float`. To avoid this error in 
existing code, use `float` by itself. Doing this will not modify any behavior and is 
safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
The aliases was originally deprecated in NumPy 1.20; for more details and 
guidance see the original release note at:
https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",hey trying launch kept giving module attribute alias float avoid error code use float modify behavior safe specifically scalar type use originally guidance see original release note,issue,negative,positive,positive,positive,positive,positive
1723547961,"It was a problem with `PyQt5`, you can solve it by Rossetta (enable Rossetta for Terminal -> install requirements), here are links that can help(more details, screenshots..):
1. [Link](https://dev.to/courier/tips-and-tricks-to-setup-your-apple-m1-for-development-547g) how to enable Rosetta on your M1: 
1. [Answer](https://stackoverflow.com/questions/65901162/how-can-i-run-pyqt5-on-my-mac-with-m1chip-ppc64el-architecture) on StackOverflow about `PyQt5` ",problem solve enable terminal install link help link enable answer,issue,negative,neutral,neutral,neutral,neutral,neutral
1698754546,"Preparing the encoder, the synthesizer and the vocoder...
Loaded encoder ""encoder.pt"" trained to step 1564501
Synthesizer using device: cuda
Building Wave-RNN
Trainable Parameters: 4.481M
Loading model weights at saved_models\default\vocoder.pt
Testing your configuration with small inputs.
        Testing the encoder...
Traceback (most recent call last):
  File ""C:\voice\demo_cli.py"", line 83, in <module>
    embedding = encoder.embed_utterance(audio_waveform)
  File ""C:\voice\encoder\inference.py"", line 144, in embed_utterance
    frames = audio.wav_to_mel_spectrogram(wav)
  File ""C:\voice\encoder\audio.py"", line 58, in wav_to_mel_spectrogram
    frames = librosa.feature.melspectrogram(
TypeError: melspectrogram() takes 0 positional arguments but 2 positional arguments (and 2 keyword-only arguments) were given",synthesizer loaded trained step synthesizer device building trainable loading model testing configuration small testing recent call last file line module file line file line positional positional given,issue,negative,negative,neutral,neutral,negative,negative
1695599178,"> I have tried running the notebook, though it worked well but cloned voice has no resemblance to speaker and has a noice. Is there any update coming to this model?
> 
> I have tried[ voice cloning](https://wavel.ai/studio/ai-voice-cloner/), they have a working model. It is cloning my voice with just 10sec clip. Not sure which model they are using. Check out yourself.

The guy who wrote this code (Corentin) for his master's thesis works for this company now - https://www.resemble.ai/team/",tried running notebook though worked well voice resemblance speaker update coming model tried voice working model voice sec clip sure model check guy wrote code master thesis work company,issue,positive,positive,positive,positive,positive,positive
1684924361,"try to run it in a anaconda virtual env with python 3.6 or 3.7 this problem is refered to your python version
",try run anaconda virtual python problem python version,issue,negative,neutral,neutral,neutral,neutral,neutral
1675798292,"pip install torchvision
pip install torchaudio",pip install pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
1672845124,"


> same issue for me
> 
> > i am done with demo_cli perfectly fine
> > and ran the command python demo_toolbox.py but its just running and it shows the next line , the tool box tab is not opening, no error message or anything its just jumpinr to new line , i am running this in anaconda cmd prompt

change your windows from 11 to 10 ",issue done perfectly fine ran command python running next line tool box tab opening error message anything new line running anaconda prompt change,issue,positive,positive,positive,positive,positive,positive
1672721021,"same issue for me 

> i am done with demo_cli perfectly fine
> 
> and ran the command python demo_toolbox.py but its just running and it shows the next line , the tool box tab is not opening, no error message or anything its just jumpinr to new line , i am running this in anaconda cmd prompt

",issue done perfectly fine ran command python running next line tool box tab opening error message anything new line running anaconda prompt,issue,positive,positive,positive,positive,positive,positive
1659135022,"> pip install webrtcvad==2.0.10



> I was messing around a little bit and managed to get it working with Windows 11. I installed these:
> 
> pip install inflect==5.3.0 pip install librosa==0.9.2 pip install matplotlib==3.5.1 pip install numpy==1.20.3 pip install Pillow==8.4.0 pip install PyQt5==5.15.6 pip install scikit-learn==1.0.2 pip install scipy==1.7.3 pip install sounddevice==0.4.3 pip install SoundFile==0.10.3.post1 pip install tqdm==4.62.3 pip install umap-learn==0.5.2 pip install Unidecode==1.3.2 pip install urllib3==1.26.7 pip install visdom==0.1.8.9 pip install webrtcvad==2.0.10
> 
> I'm not sure if that was everything I did, but after continuously getting that error I went ahead and pasted this list in the Anaconda Prompt.
> 
> I was also getting ""...anaconda3\lib\site-packages\paramiko\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated ""class"": algorithms.Blowfish,"". This seems to not matter anymore since it's up and running?

Thanks it worked",pip install messing around little bit get working pip install pip install pip install pip install pip install pip install pip install pip install pip install pip install post pip install pip install pip install pip install pip install pip install sure everything continuously getting error went ahead pasted list anaconda prompt also getting blowfish class matter since running thanks worked,issue,negative,positive,positive,positive,positive,positive
1658777155,"mine shows the same after following all the procedures please help
",mine following please help,issue,positive,neutral,neutral,neutral,neutral,neutral
1646861403,"> Keep using python 3.11.3. Then do the following: `pip unsinstall numpy` `pip install numpy==1.23.5` This version worked for me.

It worked for me too!. Thanks!",keep python following pip pip install version worked worked thanks,issue,negative,positive,neutral,neutral,positive,positive
1642356429,"I had the same problem on MacOS w/ m1 processor.
The fix was: Setting  ""Open using Rosetta"" on iTerm and the reinstalling the venv and requirements.txt again. 
",problem processor fix setting open,issue,negative,neutral,neutral,neutral,neutral,neutral
1635878114,"> Python 3.7 is recommended. Python 3.5 or greater should work, but you'll probably have to tweak the dependencies' versions. I recommend setting up a virtual environment using venv, but this is optional.

Please downgrade to 3.7 versions for it to work. ",python python greater work probably tweak recommend setting virtual environment optional please downgrade work,issue,positive,positive,positive,positive,positive,positive
1635608412,"when I try to record the audio I got an exception as 

Traceback (most recent call last):
  File ""F:\GenAi\Real-Time-Voice-Cloning\toolbox\__init__.py"", line 163, in record
    wav = self.ui.record_one(encoder.sampling_rate, 5)
  File ""F:\GenAi\Real-Time-Voice-Cloning\toolbox\ui.py"", line 224, in record_one
    self.set_loading(i, duration)
  File ""F:\GenAi\Real-Time-Voice-Cloning\toolbox\ui.py"", line 384, in set_loading
    self.loading_bar.setValue(value * 100)
TypeError: setValue(self, value: int): argument 1 has unexpected type 'numpy.float64'",try record audio got exception recent call last file line record file line duration file line value self value argument unexpected type,issue,negative,positive,neutral,neutral,positive,positive
1635596999,"Thank you Clawsmos, let me check it again and LOL proud to claim the issue #1234 :D",thank let check claim issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1634762366,"Hi DavidH-Tech. According to my evaluation and experience,

The error message shows that there is an issue with the usage of the np.float attribute in the code. It seems that np.float is being used as an alias for the built-in float type, but it has been deprecated in recent versions of NumPy (Meaning its been stopped in recent versions)

To resolve this error, you need to replace *np.float* with *float* in the code. The recommended fix is to use *float* directly instead of the deprecated alias.

Here's the modified line of code that should resolve the issue:

```], dtype=float) / 255```

By making this change, you will avoid the warning and ensure the compatibility with the version of NumPy you are using.

Cheers,

theClawsmos

P.S. Please note that if you specifically intended to use the NumPy scalar type, you can use np.float64 instead of np.float. However, based on the error message, it seems that using the built-in float type should be sufficient for your needs.",hi according evaluation experience error message issue usage attribute code used alias float type recent meaning stopped recent resolve error need replace float code fix use float directly instead alias line code resolve issue making change avoid warning ensure compatibility version please note specifically intended use scalar type use instead however based error message float type sufficient need,issue,negative,positive,neutral,neutral,positive,positive
1634757860,"Hi bobwatcherx. According to my evaluation of the error,

The error message suggests that there is an issue with the usage of the resample() function from the library, probably librosa. It seems that you are passing more arguments than the function wants/needs (similar to onibaken's issue #1234).

According to my experience, to resolve this error, you need to ensure that you are passing the correct arguments (and # of arguments) to the resample() function. Based on the traceback, the problematic line is in the preprocess_wav() function in the audio.py file.

In order to help you further, I would need to see the code snippet from the file where the preprocess_wav() function is defined, most likely audio.py. If you can provide that code, it will be very beneficial.

Cheers,

theClawsmos",hi according evaluation error error message issue usage resample function library probably passing function similar issue according experience resolve error need ensure passing correct resample function based problematic line function file order help would need see code snippet file function defined likely provide code beneficial,issue,negative,neutral,neutral,neutral,neutral,neutral
1634744959,"Hi onibanken,

The error message you shared suggests that there is an issue with the usage of the melspectrogram() function from the library (probably librosa). It appears that you are passing more arguments than the function wants.

To resolve this error, you need to ensure that you are passing the correct arguments to the melspectrogram() function. Based on the traceback error, the problematic line is in the audio.py file, specifically in the wav_to_mel_spectrogram() function.

If you want me to help you further, I need to see the code snippet. More specifically, where the wav_to_mel_spectrogram() function is defined (probably in audio.py). That code (if provided) could be very helpful.

Cheers,

theClawsmos

P.S. Nice :) You got issue 1234



<img width=""170"" alt=""image"" src=""https://github.com/CorentinJ/Real-Time-Voice-Cloning/assets/115119611/22a73dda-be32-4651-b11a-be44d5f9fd9b"">",hi error message issue usage function library probably passing function resolve error need ensure passing correct function based error problematic line file specifically function want help need see code snippet specifically function defined probably code provided could helpful nice got issue image,issue,negative,positive,positive,positive,positive,positive
1619129662,"How did you get it installed or setup after downloading, what folder did you place things and all 😩",get setup folder place,issue,negative,neutral,neutral,neutral,neutral,neutral
1619128312,Can anyone help me with detail instructions for installing and getting all the tools ready?,anyone help detail getting ready,issue,positive,positive,positive,positive,positive,positive
1616835415,"same issue. np.float is depreciated and the simple fix would just to use python float
np.float = float   
would fix this",issue simple fix would use python float float would fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1614708367,"Keep using python 3.11.3. Then do the following:
`pip unsinstall numpy`
`pip install numpy==1.23.5`
This version worked for me. ",keep python following pip pip install version worked,issue,negative,neutral,neutral,neutral,neutral,neutral
1610907557,"how many voice samples of a particular voice are required to train the model ?
",many voice particular voice train model,issue,negative,positive,positive,positive,positive,positive
1606661070,Convert this back to UTF-8 please: `iconv -f utf16 < requirements.txt > new && mv new requirements.txt`. Preferably remove the CRLF line endings too.,convert back please new new preferably remove line,issue,negative,positive,neutral,neutral,positive,positive
1605494952,"> This is not an easy undertaking so before you start, make sure you satisfy the prerequisites. You must be able to answer ""yes"" to all questions below:
> 
>     * Does your computer have a NVIDIA GPU?
> 
>     * Do you have coding experience?
> 
>     * Are you willing to devote at least 20 hours to the task?
> 
> 
> I have not gone through the process myself, but I'll try to outline it since we don't have a good explanation. What you need to do is to fine-tune the pretrained synthesizer and vocoder models on a suitable dataset.
> 
>     1. Find a suitable dataset. Freely available resources include [AccentDB](https://accentdb.org/) (Indian accent) and [VCTK](https://datashare.is.ed.ac.uk/handle/10283/3443) (other English accents). For best results on your own voice, record your own dataset though this will take many hours.
> 
>     2. Follow the steps in [README.md](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/README.md) to enable GPU support.
> 
>     3. Go to the [training wiki page](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training) and follow the steps for the synthesizer and vocoder training on the LibriSpeech dataset.
>        
>        * Review the preprocessing code and understand what it is doing.
>        * Understand the format of the files in the <datasets_root>/SV2TTS folder
> 
>     4. Preprocess your dataset from step 1 to generate training data for the synthesizer.
>        
>        * At a minimum, this requires editing the preprocessing scripts.
>        * You will likely need to write your own code to process the data into a suitable format for the toolbox.
>        * **We do not have a tutorial for this. You are on your own here!**
> 
>     5. Continue training the pretrained synthesizer model on your dataset until it has converged.
> 
>     6. Using your new synthesizer model, preprocess your dataset to generate training data for the vocoder.
> 
>     7. Continue training the pretrained vocoder model on your dataset until the output is satisfactory.
> 
> 
> With luck, your trained models will now generalize to your voice and impart the desired accent. **There are no guarantees this will work.**
> 
> If you succeed, please share your models and I will add them to the list in #400.

Hi, I have looked up on your comments and I need to clone my own voice with ascent so I produce it with text. Can you share step by step direction. I also open an issue #1228 ",easy undertaking start make sure satisfy must able answer yes computer experience willing devote least task gone process try outline since good explanation need synthesizer suitable find suitable freely available include accent best voice record though take many follow enable support go training page follow synthesizer training review code understand understand format folder step generate training data synthesizer minimum likely need write code process data suitable format toolbox tutorial continue training synthesizer model new synthesizer model generate training data continue training model output satisfactory luck trained generalize voice impart desired accent work succeed please share add list hi need clone voice ascent produce text share step step direction also open issue,issue,positive,positive,positive,positive,positive,positive
1602659777,"Install the version of librosa they used, or go to each loaction and give parameters y=, n_mels=
example librosa.stft(y=y, n_fft=hparams.n_fft, hop_length=get_hop_size(hparams), win_length=hparams.win_size)",install version used go give example,issue,negative,neutral,neutral,neutral,neutral,neutral
1599652531,"Okay. I have found a solution.
https://github.com/gitmylo/audio-webui

Training is straight forward, the webui is Gradio (like A1111), and I'm getting pretty decent generations on models trained off of 7 seconds of audio at 300 epochs. Though it's not amazing. Could be that I haven't figured out how to ""prompt"" for it yet and have to tweak some settings.

It only has like 100 stars, but it was updated last week.
Seems really promising and also seems to do everything I want it to (at the moment).

It has some UI quirks that I'm not the biggest fan of (in regards to sending models trained between tabs, and needing to generate base audio before running it through the voice cloning), but the core functionality is there.

Easy to setup.
`git clone https://github.com/gitmylo/audio-webui`
Literally just use the `run.bat` in it.

-=-=-=-=-=-=-=-

It also has 40-something API endpoints, so it would be fairly easy to handle all of the API calls with another ""overlord"" interface.

I have a working ""real-time"" whisper app working on my computer that I've made (and it doesn't seem to crash my audio interface anymore haha), so that integrated with KoboldAI using a Wizard LLM (which could be side-fed into stable diffusion for image generation), then that output to this ""audio-webui"", and play the audio. That's the loop. haha.

Plus you could have ""watchers"" for keyboard commands to tie it all into the Windows API to control your computer that way too. Actually, it could be really solid for accessibility.....

Sure, you can do all of these things with Google API sorts of things, but this **can be entirely hosted locally**.
What a fascinating time to be alive.

**Anyways, if someone steals this idea and beats me to the integration, at least tag me in it! haha.**

I'll release the whisper ""real-time"" code on my github at some point. It's pretty handy. It's just a hotkey (F9, right now) that you hold, talk, then release. It keeps the audio buffer, runs it through whisper, then outputs the text. I also have it so it ""types"" it out for you. Pretty handy when trying to tell ChatGPT some specific long prompt.",found solution training straight forward like getting pretty decent trained audio though amazing could figured prompt yet tweak like last week really promising also everything want moment biggest fan sending trained needing generate base audio running voice core functionality easy setup git clone literally use also would fairly easy handle another overlord interface working whisper working computer made seem crash audio interface wizard could stable diffusion image generation output play audio loop plus could keyboard tie control computer way actually could really solid accessibility sure entirely locally fascinating time alive anyways someone idea integration least tag release whisper code point pretty handy right hold talk release audio buffer whisper text also pretty handy trying tell specific long prompt,issue,positive,positive,positive,positive,positive,positive
1598466922,"(mocking2) F:\LSX\MockingBird>python control\cli\encoder_preprocess.py F:\LSX\MockingBird\testaudio 
Arguments:
    datasets_root:   F:\LSX\MockingBird\testaudio
    out_dir:         F:\LSX\MockingBird\testaudio\SV2TTS\encoder
    datasets:        ['librispeech_other', 'voxceleb1', 'aidatatang_200zh']
    skip_existing:   False

Preprocessing librispeech_other
Couldn't find F:\LSX\MockingBird\testaudio\LibriSpeech\train-other-500, skipping this dataset.
Preprocessing voxceleb1
Couldn't find F:\LSX\MockingBird\testaudio\VoxCeleb1, skipping this dataset.
Preprocessing aidatatang_200zh
Couldn't find F:\LSX\MockingBird\testaudio\aidatatang_200zh, skipping this dataset.",python false could find skipping could find skipping could find skipping,issue,negative,negative,negative,negative,negative,negative
1595946010,"Install Python 3.7.9 then create a new VENV using 3.7
'python3.7 -m venv myenv'
Install the requirements.txt and the correct version of numpy, 1.20.3 will be installed",install python create new install correct version,issue,negative,positive,positive,positive,positive,positive
1580183664,"Hey with the new update of librosa, you have to specify the argument name, it will work.",hey new update specify argument name work,issue,negative,positive,positive,positive,positive,positive
1578391556,"I fix this error by importing on Real-Time-Voice-Cloning/toolbox/ui.py

from PyQt5.QtCore import Qt, QStringListModel
from PyQt5.QtWidgets import *
from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas

before 

import matplotlib.pyplot as plt",fix error import import import import,issue,negative,neutral,neutral,neutral,neutral,neutral
1577898971,"Thanks to ajay-sampath
It works with librosa 0.9.2
Command: pip install librosa==0.9.2",thanks work command pip install,issue,negative,positive,positive,positive,positive,positive
1577035699,"For me this didn't work for compatibility issue with librosa. I resolved this by specifying the version of numba :
`numba==0.56.4`",work compatibility issue resolved version,issue,negative,neutral,neutral,neutral,neutral,neutral
1565205364,"For PYTORCH_CUDA_ALLOC_CONF, Is it more values we set, the more available memory we got? or less. Sorry, I am not a coder.",set available memory got le sorry coder,issue,negative,negative,neutral,neutral,negative,negative
1550889784,"you can try installing unidecode 
pip install unidecode 
I would suggest if you already haven't then create a virtual environment with python3.7 and then activate it. Following that install ffmpeg, pytorch and then the requirements. After this the code should run without problem. Most of the issues come from dependencies with specific versions of numpy and librosa.",try pip install would suggest already create virtual environment python activate following install code run without problem come specific,issue,positive,neutral,neutral,neutral,neutral,neutral
1550885915,"install librosa==0.8.1 as mentioned in the requirements document. Ideally create a virtual environment with Python 3.7 then install ffmpeg, pytorch and then the requirements in the environment and then run the code.",install document ideally create virtual environment python install environment run code,issue,positive,positive,positive,positive,positive,positive
1550883737,"Make sure you install the correct version of dependencies as specified in the requirements file. Create a virtual environment with python3.7, then install ffmpeg and py torch and then the requirements file in that virtual environment. Then try to run the code and it should work.",make sure install correct version file create virtual environment install torch file virtual environment try run code work,issue,positive,positive,positive,positive,positive,positive
1550880061,"I think its a librosa version issue. Make sure you create a virtual environment with python3.7 and then install ffmpeg, torch, and then the requirements in that order. Specifically make sure you have librosa==0.8.1",think version issue make sure create virtual environment python install torch order specifically make sure,issue,positive,positive,positive,positive,positive,positive
1550877062,"I think its a librosa version issue. Install the following in colab before running the code and it should work
!pip install unidecode
!pip install webrtcvad
!pip install librosa==0.8.1",think version issue install following running code work pip install pip install pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
1546922263,"If your results are good with hifigan, can you send your codes to me please? I really really need your help about this. Please answer me.
my email: rahaamiri304@gmail.com",good send please really really need help please answer,issue,positive,positive,positive,positive,positive,positive
1546283922,"> ### Get FFmpeg
> 9. Download FFmpeg from here: https://github.com/BtbN/FFmpeg-Builds/releases/download/autobuild-2021-10-31-12-23/ffmpeg-N-104454-gd92fdc7144-win64-lgpl.zip
> 10. Extract the zip file. Move `ffmpeg.exe` to `C:\Real-Time-Voice-Cloning-master` (the same folder as demo_toolbox.py)

Alternatively `winget install FFmpeg` would be more elegant. 
",get extract zip file move folder alternatively install would elegant,issue,negative,positive,positive,positive,positive,positive
1544753710,"Hi, try using librosa version 0.9.2
`pip install librosa==0.9.2`",hi try version pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
1529153660,nothing works! on windows python3 please help!,nothing work python please help,issue,negative,neutral,neutral,neutral,neutral,neutral
1529012908,"This issue solved . you need cmake. check for webrtcvad repo for a solution

https://docs.microsoft.com/en-us/answers/questions/136595/error-microsoft-visual-c-140-or-greater-is-require.html",issue need check solution,issue,negative,neutral,neutral,neutral,neutral,neutral
1528994710,"Building wheels for collected packages: webrtcvad
  Building wheel for webrtcvad (pyproject.toml) ... error
  error: subprocess-exited-with-error

  × Building wheel for webrtcvad (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─> [9 lines of output]
      running bdist_wheel
      running build
      running build_py
      creating build
      creating build\lib.win-amd64-cpython-39
      copying webrtcvad.py -> build\lib.win-amd64-cpython-39
      running build_ext
      building '_webrtcvad' extension
      error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for webrtcvad
Failed to build webrtcvad
ERROR: Could not build wheels for webrtcvad, which is required to install pyproject.toml-based projects",building collected building wheel error error building wheel run successfully exit code output running running build running build running building extension error visual greater get build end output note error likely problem pip error building wheel build error could build install,issue,negative,positive,positive,positive,positive,positive
1528994068,Even I have same error. 14.2 is running. it is needed fr noise reduct,even error running noise reduct,issue,negative,neutral,neutral,neutral,neutral,neutral
1528715748,"> `webrtcvad-wheels` added it to requirements.txt and it worked in my venv on macos

I added the line `webrtcvad-wheels` to my requirements.txt file via Visual Studio Code, (normal text editor works fine too) and the vocode test passed. Thanks for the help.",added worked added line file via visual studio code normal text editor work fine test thanks help,issue,positive,positive,positive,positive,positive,positive
1522807762,"I have them same question for 11lab voice conversion, example: [https://www.youtube.com/watch?v=17_xLsqny9E]",question lab voice conversion example,issue,negative,neutral,neutral,neutral,neutral,neutral
1522557195,Hello. An example path would be saved_models/<run_id>/<model_type>.pt = saved_models/encoder/encoder.pt?. Thanks for any help.,hello example path would thanks help,issue,positive,positive,positive,positive,positive,positive
1518709708,"I have installed the latest version, and older versions still get this error.

(base) C:\Users\ryanh\Desktop\Real-Time-Voice-Cloning-master>pip install webrtcvad
Defaulting to user installation because normal site-packages is not writeable
Collecting webrtcvad
  Using cached webrtcvad-2.0.10.tar.gz (66 kB)
  Preparing metadata (setup.py) ... done
Building wheels for collected packages: webrtcvad
  Building wheel for webrtcvad (setup.py) ... error
  error: subprocess-exited-with-error

  × python setup.py bdist_wheel did not run successfully.
  │ exit code: 1
  ╰─> [9 lines of output]
      running bdist_wheel
      running build
      running build_py
      creating build
      creating build\lib.win-amd64-cpython-310
      copying webrtcvad.py -> build\lib.win-amd64-cpython-310
      running build_ext
      building '_webrtcvad' extension
      error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for webrtcvad
  Running setup.py clean for webrtcvad
Failed to build webrtcvad
Installing collected packages: webrtcvad
  Running setup.py install for webrtcvad ... error
  error: subprocess-exited-with-error

  × Running setup.py install for webrtcvad did not run successfully.
  │ exit code: 1
  ╰─> [11 lines of output]
      running install
      X:\anaconda2\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.
        warnings.warn(
      running build
      running build_py
      creating build
      creating build\lib.win-amd64-cpython-310
      copying webrtcvad.py -> build\lib.win-amd64-cpython-310
      running build_ext
      building '_webrtcvad' extension
      error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: legacy-install-failure

× Encountered error while trying to install package.
╰─> webrtcvad

note: This is an issue with the package mentioned above, not pip.
hint: See above for output from the failure.",latest version older still get error base pip install user installation normal writeable done building collected building wheel error error python run successfully exit code output running running build running build running building extension error visual greater get build end output note error likely problem pip error building wheel running clean build collected running install error error running install run successfully exit code output running install install use build pip running build running build running building extension error visual greater get build end output note error likely problem pip error error trying install package note issue package pip hint see output failure,issue,negative,positive,positive,positive,positive,positive
1515040811,"I get this error too, let me know if you find uot hot to fix it
",get error let know find hot fix,issue,negative,positive,positive,positive,positive,positive
1510772348,"fill this form
https://cn01.mmai.io/keyreq/voxceleb?
you will get the links to the dataset through mail
",fill form get link mail,issue,negative,neutral,neutral,neutral,neutral,neutral
1505138283,"@baraalmasri Hello, I'm trying to create one, check the link of the issue I opened here:

https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/1189#issue-1664407508

We can work together on it via social media platforms or even by email (My email is: saleh200220032004@gmail.com).

Also feel free to follow the steps I provided in the new issue to see if you can work out something, I'm planning on releasing it to the public once I finish working on it. It can have both our names if we can work together on this. 👍",hello trying create one check link issue work together via social medium even also feel free follow provided new issue see work something public finish working work together,issue,positive,positive,positive,positive,positive,positive
1504833099,"> I was messing around a little bit and managed to get it working with Windows 11. I installed these:
> 
> pip install inflect==5.3.0 pip install librosa==0.9.2 pip install matplotlib==3.5.1 pip install numpy==1.20.3 pip install Pillow==8.4.0 pip install PyQt5==5.15.6 pip install scikit-learn==1.0.2 pip install scipy==1.7.3 pip install sounddevice==0.4.3 pip install SoundFile==0.10.3.post1 pip install tqdm==4.62.3 pip install umap-learn==0.5.2 pip install Unidecode==1.3.2 pip install urllib3==1.26.7 pip install visdom==0.1.8.9 pip install webrtcvad==2.0.10
> 
> I'm not sure if that was everything I did, but after continuously getting that error I went ahead and pasted this list in the Anaconda Prompt.
> 
> I was also getting ""...anaconda3\lib\site-packages\paramiko\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated ""class"": algorithms.Blowfish,"". This seems to not matter anymore since it's up and running?

it worked ",messing around little bit get working pip install pip install pip install pip install pip install pip install pip install pip install pip install pip install post pip install pip install pip install pip install pip install pip install sure everything continuously getting error went ahead pasted list anaconda prompt also getting blowfish class matter since running worked,issue,negative,positive,positive,positive,positive,positive
1503347735,"@padmalcom i want to clone a german voice for my instagram reels 
can you make an video how to install it on windows?",want clone german voice make video install,issue,negative,neutral,neutral,neutral,neutral,neutral
1501857248,"Hey guys, has anyone found a solution for hindi voice cloning? Thanks",hey anyone found solution voice thanks,issue,positive,positive,positive,positive,positive,positive
1501653772,"Muhammad,
No, I can't. That is a much bigger project than what I have done. Sorry.

________________________________
From: Muhammad Waqar ***@***.***>
Sent: Monday, April 10, 2023 3:56:08 AM
To: CorentinJ/Real-Time-Voice-Cloning ***@***.***>
Cc: Tomcattwo ***@***.***>; Comment ***@***.***>
Subject: Re: [CorentinJ/Real-Time-Voice-Cloning] Tutorial: Windows installation (#647)


i want to train for other language, can you able to help me?

—
Reply to this email directly, view it on GitHub<https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/647#issuecomment-1501518697>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/ALF47JAQOPUWGMT6C5XGJ5DXAO4JRANCNFSM4W6EC57Q>.
You are receiving this because you commented.Message ID: ***@***.***>
",ca much bigger project done sorry sent comment subject tutorial installation want train language able help reply directly view id,issue,negative,negative,neutral,neutral,negative,negative
1501518697,"i want to train for other language, can you able to help me?",want train language able help,issue,negative,positive,positive,positive,positive,positive
1501128466,"> IF SOMEONE HAD THIS ERROR, THE PROBLEM IS IN WINDOWS, THE ONLY THING THAT SOLVED FOR ME WAS BACK FROM WINDOWS 11 TO WINDOWS 10

![image_2023-04-09_192057723](https://user-images.githubusercontent.com/126923861/230775115-a0d37b51-3345-405a-bc61-27c1748b1b68.png)
hey can u please help me with this error?",someone error problem thing back hey please help error,issue,negative,neutral,neutral,neutral,neutral,neutral
1501054916,"https://github.com/VOICEVOX/voicevox_engine this project is the only one i found, that is better than elevenlabs, but it works in japanesse only",project one found better work,issue,negative,positive,positive,positive,positive,positive
1500851851,Hi @AadharshAadhithya No man. Things require good hardware. But I do have recording in backups. Hopefully I'll do it one day.,hi man require good hardware recording hopefully one day,issue,positive,positive,positive,positive,positive,positive
1500850841,"Hi, @mcnaveen did you go about training a multispeaker model?",hi go training model,issue,negative,neutral,neutral,neutral,neutral,neutral
1499656262,"@tony2023 I've got almost the same issue at Windows, conda version change of librosa fixed this, but demo_toolbox.py still not running...",tony got almost issue version change fixed still running,issue,negative,positive,neutral,neutral,positive,positive
1498005626,"I have a project I'm working on where I'd like to train a model similar to this on a voice sample, and then have it speak dynamically generated text. Higher quality than this would be great, but the uncanny-value aspect of mid quality voice cloning is also fine actually. I've got a 99% completed CSCI degree and DS certificate, and I'd be down to develop a basic solution since we're all looking for an open source version of the same thing basically. My discord is Lego#3891, and my reddit is... CrabbyAlmond. Had to dig deep for that lol",project working like train model similar voice sample speak dynamically text higher quality would great aspect mid quality voice also fine actually got degree certificate develop basic solution since looking open source version thing basically discord dig deep,issue,positive,positive,positive,positive,positive,positive
1497043969,"Yes, it works fine for me. I think this issue is for the new release of Librosa. Librosa wants parameters with names.",yes work fine think issue new release,issue,positive,positive,positive,positive,positive,positive
1493656782,"I'm facing the same issue. what is the solution for this one?
  print(""Max input length (text chars): %d"" % max(len(m[5]) for m in metadata))",facing issue solution one print input length text,issue,negative,neutral,neutral,neutral,neutral,neutral
1493490719,LA carpeta  librespeech no esta en el codigo alguien q me ayude xfavor discullpe la molestia,la en el la,issue,negative,neutral,neutral,neutral,neutral,neutral
1493192532,"You can shoot me a message on Reddit if you'd like. I'm not too active on Discord anymore.
[Here's my Reddit account](https://www.reddit.com/user/remghoost7).

Just finishing up one of the pieces for that project. Got whisper to work in ""real-time"" by holding a key, recording the mic input to a temp file, transcribing that with whisper, then ""typing"" it out using pyautogui.typewriter. I'll probably put it on github in the next day or so.",shoot message like active discord account finishing one project got whisper work holding key recording input temp file whisper probably put next day,issue,negative,negative,neutral,neutral,negative,negative
1492106220,"I have. One obstacle is that the default hifigan models don't match the mel spectrogram definition used in this repo. You'll either need to train a new RTVC model using the hifigan melspec definition, or go the other direction and train a hifigan model to match the RTVC melspecs. I've tried both and a better result is obtained with the latter approach. 

When I get a chance, I will clean up the code and commit it to my fork of RTVC. Will keep you posted here.",one obstacle default match mel spectrogram definition used either need train new model definition go direction train model match tried better result latter approach get chance clean code commit fork keep posted,issue,positive,positive,positive,positive,positive,positive
1492045884,"Batch size affects training time when gpu is the bottleneck. The default batch of 12 is too low to saturate the gpu, which is why you're noticing that training speed remains the same when the batch size is increased.",batch size training time bottleneck default batch low saturate training speed remains batch size,issue,negative,neutral,neutral,neutral,neutral,neutral
1491857442,"So, I too am on the hunt for something like this.

[tortoise-tts](https://github.com/neonbjb/tortoise-tts) seems decent, but even the [""fast""](https://github.com/152334H/tortoise-tts-fast) variant takes around 10 seconds for a 3 second audio clip. Which is probably usable, I just haven't gotten it to actually _work_. I tried a month or so ago and gave up. Tried again tonight and no dice. Both of the repos are abandon-ware as well. If you get it to work, please let me know. Might try again at some point.

I've recently stumbled upon [Mycroft](https://github.com/MycroftAI), which seems to be an _entire platform_ for this sort of thing. Updated in the past few weeks as well. I haven't tried it yet though.

If I had any experience with torch, I'd be into making this sort of thing, but I severely doubt my 1060 6gb would be up for training voice models. It can't even train SD models. lol.

Anyways, just wanted to pass along some information I've found on this topic.

I have a dream of piping my voice through Whisper (for voice to text), into [LLaMA via the Oobabooga repo](https://github.com/oobabooga/text-generation-webui) (the new poster boy on the block for ChatGPT clones), then back out through a cloned voice via tts. 

Essentially to have a locally run virtual assistant that I could talk to. Pair that with an image recognition suite for LLaMA (which could more than likely be done) and you'd have a voice controlled computer. Sure, auto hotkey and such exist, but you can do _so much more_ with LLMs than you can with simple pre-programmed hotkeys.

Best of luck on your search! Let me know if you find anything that works for you.",hunt something like decent even fast variant around second audio clip probably usable gotten actually tried month ago gave tried tonight dice well get work please let know might try point recently upon sort thing past well tried yet though experience torch making sort thing severely doubt would training voice ca even train anyways pas along information found topic dream piping voice whisper voice text llama via new poster boy block back voice via essentially locally run virtual assistant could talk pair image recognition suite llama could likely done voice computer sure auto exist much simple best luck search let know find anything work,issue,positive,positive,positive,positive,positive,positive
1489816870,I am also facing this issue has anyone have update on this issue,also facing issue anyone update issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1488212043,"



> @CorentinJ @yaguangtang @tail95 @zbloss @HumanG33k I am finetuning the encoder model by Chhinese data of 3100 persons. I want to know how to judge whether the train of finetune is OK. In Figure0, The blue line is based on 2100 persons , the yellow line is based on 3100 persons which is trained now. Figure0: ![image](https://user-images.githubusercontent.com/40649244/63139015-3f446480-c00f-11e9-9ec2-3eadebd6023f.png)
> 
> Figure1:(finetune 920k , from 1565k to 1610k steps, based on 2100 persons) ![image](https://user-images.githubusercontent.com/40649244/63139038-5aaf6f80-c00f-11e9-85ca-f54fdd81431e.png)
> 
> Figure2:(finetune 45k from 1565k to 1610k steps, based on 3100 persons) ![image](https://user-images.githubusercontent.com/40649244/63139112-a8c47300-c00f-11e9-9097-fb65452df9fd.png)
> 
> I also what to know how mang steps is OK , in general. Because, I only know to train the synthesizer model and vocoder mode oneby one to judge the effect. But it will cost very long time. How about my EER or Loss ? Look forward your reply!

Could you plz share the Chinese encoder model with me? @UESTCgan ",tail model data want know judge whether train figure blue line based yellow line based trained figure image figure based image figure based image also know mang general know train synthesizer model mode one judge effect cost long time eer loss look forward reply could share model,issue,negative,positive,neutral,neutral,positive,positive
1486149058,"> I upload the latest pretained model on steps 183W ![newplot](https://user-images.githubusercontent.com/108937/81632553-26050300-943d-11ea-9453-1529e314df4c.png) ![newplot1](https://user-images.githubusercontent.com/108937/81632568-2f8e6b00-943d-11ea-9eae-0b7a51369ba6.png) ![0505_umap_1830000](https://user-images.githubusercontent.com/108937/81632486-fd7d0900-943c-11ea-8c59-b9095003f0a0.png)

Where did you put your pretrained model on?  Seems not see any links on your forked [repo](https://github.com/iwater/Real-Time-Voice-Cloning-Chinese)? @iwater ",latest model put model see link forked,issue,negative,positive,positive,positive,positive,positive
1481789015,"Indeed! Yes, very strange. I suspect that ai singing will take over soon.
Sigh...it will be great to hear such perfect singing voices..but at what
price? The further destruction of culture etc.. I will say, I was very
impressed with the app
https://play.google.com/store/apps/details?id=com.voicecopy.app


PLEASE let me know if you discover any quality real time solutions.
Voicemod seems maybe okay, looks like it can be ran on a windows server and
then controlled via Android app.. still, nothing too great.

On Thu, Mar 23, 2023, 1:10 PM Lolagatorade ***@***.***> wrote:

> ElevenLabs in terms of quality has a really effective voice cloning in my
> opinion. Does anyone have a guess / know what their training protocol may
> have been? So the base model, and then what else they added to it to bring
> it to where it is today. Breaking it down is the first step to making an
> open source alternative which I’m very interested in doing!
>
> Honestly, there's not much things that are open in terms of Voice cloning.
> You can go on GitHub and type in voice, cloning and search for whatever
> comes up I believe some of those results have research papers. I remember
> there was some Chinese repository that has it running locally but of course
> I don't know Chinese. I just find it very strange how open image generation
> face swap, and all the other things are, but there's so many companies that
> are private when it comes to Voice cloning
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/1165#issuecomment-1481752247>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/A56URW75TIBARCUGIBRTRCLW5SNYPANCNFSM6AAAAAAVCHA7AU>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>
",indeed yes strange suspect ai singing take soon sigh great hear perfect singing price destruction culture say please let know discover quality real time maybe like ran server via android still nothing great mar wrote quality really effective voice opinion anyone guess know training protocol may base model else added bring today breaking first step making open source alternative interested honestly much open voice go type voice search whatever come believe research remember repository running locally course know find strange open image generation face swap many private come voice reply directly view id,issue,positive,positive,positive,positive,positive,positive
1481752247,"> ElevenLabs in terms of quality has a really effective voice cloning in my opinion. Does anyone have a guess / know what their training protocol may have been? So the base model, and then what else they added to it to bring it to where it is today. Breaking it down is the first step to making an open source alternative which I’m very interested in doing!

Honestly, there's not much things that are open in terms of Voice cloning. You can go on GitHub and type in voice, cloning and search for whatever comes up I believe some of those results have research papers. I remember there was some Chinese repository that has it running locally but of course I don't know Chinese. I just find it very strange how open image generation face swap, and all the other things are, but there's so many companies that are private when it comes to Voice cloning",quality really effective voice opinion anyone guess know training protocol may base model else added bring today breaking first step making open source alternative interested honestly much open voice go type voice search whatever come believe research remember repository running locally course know find strange open image generation face swap many private come voice,issue,positive,positive,neutral,neutral,positive,positive
1481694014,"Not real time but surprisingly great,

https://play.google.com/store/apps/details?id=com.voicecopy.app

On Thu, Mar 23, 2023, 6:57 AM Jack Stones ***@***.***> wrote:

> ElevenLabs in terms of quality has a really effective voice cloning in my
> opinion. Does anyone have a guess / know what their training protocol may
> have been? So the base model, and then what else they added to it to bring
> it to where it is today. Breaking it down is the first step to making an
> open source alternative which I’m very interested in doing!
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/1165#issuecomment-1481148209>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/A56URW2OAGGVUALZ4Q77YR3W5RCCFANCNFSM6AAAAAAVCHA7AU>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>
",real time surprisingly great mar jack wrote quality really effective voice opinion anyone guess know training protocol may base model else added bring today breaking first step making open source alternative interested reply directly view id,issue,positive,positive,positive,positive,positive,positive
1481160103,@raccoonML I’m coming upon this now as I’m trying to learn this for the purposes of getting something working to a ElevenLabs level. Where would you start to learn these example projects ? Voice seems to be a lesser covered / lesser resourced topic in AI so appreciate anything you found helpful that you’d recommend. ,coming upon trying learn getting something working level would start learn example voice lesser covered lesser topic ai appreciate anything found helpful recommend,issue,positive,neutral,neutral,neutral,neutral,neutral
1481148209,"ElevenLabs in terms of quality has a really effective voice cloning in my opinion. Does anyone have a guess / know what their training protocol may have been? So the base model, and then what else they added to it to bring it to where it is today. Breaking it down is the first step to making an open source alternative which I’m very interested in doing!",quality really effective voice opinion anyone guess know training protocol may base model else added bring today breaking first step making open source alternative interested,issue,positive,positive,neutral,neutral,positive,positive
1479628881,"Same issue here on a T480
Ubuntu 20.04
Not sure if aplay -l will help (as its audio stuff can't hurt)

ThinkPad-T480:~/TEMP/Real-Time-Voice-Cloning$ aplay -l
**** List of PLAYBACK Hardware Devices ****
card 0: PCH [HDA Intel PCH], device 0: ALC257 Analog [ALC257 Analog]
  Subdevices: 1/1
  Subdevice #0: subdevice #0
card 0: PCH [HDA Intel PCH], device 3: HDMI 0 [HDMI 0]
  Subdevices: 1/1
  Subdevice #0: subdevice #0
card 0: PCH [HDA Intel PCH], device 7: HDMI 1 [HDMI 1]
  Subdevices: 1/1
  Subdevice #0: subdevice #0
card 0: PCH [HDA Intel PCH], device 8: HDMI 2 [HDMI 2]
  Subdevices: 1/1
  Subdevice #0: subdevice #0
card 0: PCH [HDA Intel PCH], device 9: HDMI 3 [HDMI 3]
  Subdevices: 1/1
  Subdevice #0: subdevice #0
card 0: PCH [HDA Intel PCH], device 10: HDMI 4 [HDMI 4]
  Subdevices: 1/1
  Subdevice #0: subdevice #0
",issue sure help audio stuff ca hurt list playback hardware card device card device card device card device card device card device,issue,negative,positive,positive,positive,positive,positive
1479293959,"Change the relevant function like this:

def wav_to_mel_spectrogram(wav):
    """"""
    Derives a mel spectrogram ready to be used by the encoder from a preprocessed audio waveform.
    Note: this not a log-mel spectrogram.
    """"""
    frames = librosa.feature.melspectrogram(
        y=wav,  # changed row
        sr=sampling_rate,  # changed row
        n_fft=int(sampling_rate * mel_window_length / 1000),
        hop_length=int(sampling_rate * mel_window_step / 1000),
        n_mels=mel_n_channels
    )
    return frames.astype(np.float32).T",change relevant function like mel spectrogram ready used audio note row row return,issue,positive,positive,positive,positive,positive,positive
1476462384,"Really sucks there is AI for everything you can run in local hardware. AI art stable diffusion, text GPT you got alpaca and llama. But nothing good for voice cloning. ",really ai everything run local hardware ai art stable diffusion text got alpaca llama nothing good voice,issue,negative,positive,positive,positive,positive,positive
1476175572,"I get as well:
`RuntimeError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 10.76 GiB total capacity; 4.07 GiB already allocated; 646.44 MiB free; 8.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF`

I am not sure what values to try to set in `PYTORCH_CUDA_ALLOC_CONF`",get well memory tried allocate mib gib total capacity gib already mib free gib reserved total reserved memory memory try setting avoid fragmentation see documentation memory management sure try set,issue,positive,positive,positive,positive,positive,positive
1475427838,`CUDA out of memory. Tried to allocate 9.90 GiB (GPU 0; 16.00 GiB total capacity; 2.44 GiB already allocated; 10.04 GiB free; 4.05 GiB reserved in total by PyTorch) `,memory tried allocate gib gib total capacity gib already gib free gib reserved total,issue,positive,positive,positive,positive,positive,positive
1475416256,"> I was messing around a little bit and managed to get it working with Windows 11. I installed these:
> 
> pip install inflect==5.3.0 pip install librosa==0.9.2 pip install matplotlib==3.5.1 pip install numpy==1.20.3 pip install Pillow==8.4.0 pip install PyQt5==5.15.6 pip install scikit-learn==1.0.2 pip install scipy==1.7.3 pip install sounddevice==0.4.3 pip install SoundFile==0.10.3.post1 pip install tqdm==4.62.3 pip install umap-learn==0.5.2 pip install Unidecode==1.3.2 pip install urllib3==1.26.7 pip install visdom==0.1.8.9 pip install webrtcvad==2.0.10
> 
> I'm not sure if that was everything I did, but after continuously getting that error I went ahead and pasted this list in the Anaconda Prompt.
> 
> I was also getting ""...anaconda3\lib\site-packages\paramiko\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated ""class"": algorithms.Blowfish,"". This seems to not matter anymore since it's up and running?

You rock!!! Thank you!!!!",messing around little bit get working pip install pip install pip install pip install pip install pip install pip install pip install pip install pip install post pip install pip install pip install pip install pip install pip install sure everything continuously getting error went ahead pasted list anaconda prompt also getting blowfish class matter since running rock thank,issue,negative,positive,positive,positive,positive,positive
1475123968,"pip install torchvision
pip install torchaudio

Solve",pip install pip install solve,issue,negative,neutral,neutral,neutral,neutral,neutral
1473985342,"Hi everyone , i would like to know how much training time does every module requires using GPU (approx.).",hi everyone would like know much training time every module,issue,negative,positive,positive,positive,positive,positive
1471588787,"I was messing around a little bit and managed to get it working with Windows 11. I installed these:

pip install inflect==5.3.0
pip install librosa==0.9.2
pip install matplotlib==3.5.1
pip install numpy==1.20.3
pip install Pillow==8.4.0
pip install PyQt5==5.15.6
pip install scikit-learn==1.0.2
pip install scipy==1.7.3
pip install sounddevice==0.4.3
pip install SoundFile==0.10.3.post1
pip install tqdm==4.62.3
pip install umap-learn==0.5.2
pip install Unidecode==1.3.2
pip install urllib3==1.26.7
pip install visdom==0.1.8.9
pip install webrtcvad==2.0.10

I'm not sure if that was everything I did, but after continuously getting that error I went ahead and pasted this list in the Anaconda Prompt. 

I was also getting ""...anaconda3\lib\site-packages\paramiko\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated ""class"": algorithms.Blowfish,"". This seems to not matter anymore since it's up and running?",messing around little bit get working pip install pip install pip install pip install pip install pip install pip install pip install pip install pip install post pip install pip install pip install pip install pip install pip install sure everything continuously getting error went ahead pasted list anaconda prompt also getting blowfish class matter since running,issue,negative,positive,positive,positive,positive,positive
1469470069,"try using model architecture (eg. location vs. contact-based) and loss functions as hparams and see if those help fine tune it. i'm trying out SGD optimization to see if that would improve the results. oh yeah, maybe pitch shifting would be interesting as well...",try model architecture location loss see help fine tune trying optimization see would improve oh yeah maybe pitch shifting would interesting well,issue,positive,positive,positive,positive,positive,positive
1468498842,"> 
Still I'm facing this issue... even after install ffmpeg
![image](https://user-images.githubusercontent.com/67197973/225083764-1473a680-7c52-491e-8cce-94c96e5854b2.png)
",still facing issue even install image,issue,negative,neutral,neutral,neutral,neutral,neutral
1464571369,also wondering this. is there a simple setting somewhere?,also wondering simple setting somewhere,issue,negative,neutral,neutral,neutral,neutral,neutral
1464245185,"IF SOMEONE HAD THIS ERROR, THE PROBLEM IS IN WINDOWS, THE ONLY THING THAT SOLVED FOR ME WAS BACK FROM WINDOWS 11 TO WINDOWS 10",someone error problem thing back,issue,negative,neutral,neutral,neutral,neutral,neutral
1456273940,"@luan78zaoha, did you experience better result after employing 'guided attention loss'  in this synthesizer training?
Do you strongly recommend to add? ",experience better result attention loss synthesizer training strongly recommend add,issue,positive,positive,positive,positive,positive,positive
1442302653,"I seemed to have this problem too and it had something to do with the updated version of the librosa package. Installing an older version of librosa solved my problem:

`pip install librosa==0.9.2`",problem something version package older version problem pip install,issue,negative,positive,positive,positive,positive,positive
1438341689,"This commercial solution  is quite good, I've tried it a lot and it's great.   It's still not fully open, but you can request a test drive, https://www.resemble.ai/speech-to-speech/",commercial solution quite good tried lot great still fully open request test drive,issue,positive,positive,positive,positive,positive,positive
1434057118,"It took a lot of digging but I found that if I went into the ui.py file and moved the matplotlib imports after the PyQt5 imports then the UI would actually open. I don't know if I ruined anything else but at least the UI opens.
Example: 
```
from PyQt5.QtCore import Qt, QStringListModel
from PyQt5.QtWidgets import *
import matplotlib
import matplotlib.pyplot as plt
from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
```",took lot digging found went file would actually open know ruined anything else least example import import import import import,issue,negative,negative,negative,negative,negative,negative
1433753742,"Can you please describe how you trained the model? What custom dataset did you use, what equipment do you have, how long did you train, etc? Also, do you mind sharing your models?",please describe trained model custom use equipment long train also mind,issue,negative,negative,neutral,neutral,negative,negative
1433749570,"I'm not sure if this is still relevant. If it is, please take a look at line 34 of demo_toolbox.py. If you open utils.default_models.py you'll see the method checks the size of the models and downloads the default ones from goodle drive if the size is different from the expected. So you can try to remove line demo_toolbox:34 or comment it out",sure still relevant please take look line open see method size default drive size different try remove line comment,issue,positive,positive,positive,positive,positive,positive
1433742994,"What I did to fix it was in the `params_model.py` The file is in the `Real-Time-Voice-Cloning-master\encoder\params_model.py`
I change the values from:

`## Model parameters`
`model_hidden_size = 256`
`model_embedding_size = 256`
`model_num_layers = 3`

to

`## Model parameters`
`model_hidden_size = 128`
`model_embedding_size = 128`
`model_num_layers = 3`

and that fixed the problem for me. 
It is not too little memory problem, because you will get the same error with a 3090.

Are you able to run `demo_toolbox.py` if not what is the error?
",fix file change model model fixed problem little memory problem get error able run error,issue,negative,positive,positive,positive,positive,positive
1433680601,"> @craftpagЭто не параметр, который можно найти в коде здесь, команда PyTorch, которую необходимо установить как переменную среду. Попробуйте установить `PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:<cache in mb here>`.
> 
> Цитата из документа: « `max_split_size_mb`примерное разбиение распределителем блоков большего размера (в МБ). Это может помочь взломать фрагментацию и может быть реализовано без нехватки памяти».
> 
> Перейдите по этой ссылке, чтобы просмотреть полную документацию по управлению памятью PyTorch: [https://pytorch.org/docs/stable/notes/cuda.html.](https://pytorch.org/docs/stable/notes/cuda.html)

Sorry the same mistake, the memory is over)), I read your posts and do not understand where to insert what to download.
Is this a file of some kind ? PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:<cache in mb here>
Or is it necessary to prescribe it somewhere?
Write: I have entered into a variable environment (what is it?)) I see you programmers understand each other right off the bat))
Sorry, but I'm zero.
I didn't download anything of my own, I double-checked 10 times what I download and what the blogger advises to download
I think I'm such a fool that it's easier for me to buy another 16 gigs of memory))",cache sorry mistake memory read understand insert file kind cache necessary prescribe somewhere write variable environment see understand right bat sorry zero anything time think fool easier buy another memory,issue,negative,negative,neutral,neutral,negative,negative
1416756710,"This might be working, if it doesn't try removing all the versions then try it again.
`pip install -r requirements.txt -U`
",might working try removing try pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
1415871553,"> bu arada türkçe için birkaç model yetiştirdim ama bunları paylaşırsam ülkedeki devasa ""kazıyıcı"" kültür arasında bilinen amaçlarla ""suistimal edilebilir"" diyebilirim.

Did you create your own dataset or did you go with common voice?
",bu model ama create go common voice,issue,negative,negative,negative,negative,negative,negative
1412284530,This is likely an issue with `matplotlib` parameters; [see here for a solution that works on WSL 2 (Windows 10)](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/1096#issuecomment-1411416239). You might need a different value than `GTK3Cairo` for MacOS though.,likely issue see solution work might need different value though,issue,positive,neutral,neutral,neutral,neutral,neutral
1411416239,"I was able to get mine working on WSL 2 (Windows 10) by editing the imports of `toolbox/ui.py` like so:

```python
import matplotlib
matplotlib.rcParams['backend']='GTK3Cairo'
import matplotlib.pyplot as plt
```",able get mine working like python import import,issue,negative,positive,positive,positive,positive,positive
1409621267,"@neonsecret Hey! the problem with the fact that only a few words out of a dozen are voiced is still relevant. tell me, is there a solution for this?",hey problem fact dozen voiced still relevant tell solution,issue,negative,positive,positive,positive,positive,positive
1399220966,"> It works with ""Desktop Development with C++"". Not sure if everything is needed, but I finally completed installation.
> 
> ![image](https://user-images.githubusercontent.com/1828532/148664790-f6cee607-f4f5-4a39-bfec-48436d233aea.png)
> 
> Possibly, you could mention this in installation guide.

This worked for me. Thanks a lot @andkirby ",work development sure everything finally installation image possibly could mention installation guide worked thanks lot,issue,positive,positive,positive,positive,positive,positive
1387267630,"```
import os

for root, dirs, files in os.walk(r'C:\LibriSpeech\train-clean-100'):
    if len(files) == 0:
        continue
    try: 
        head, book = os.path.split(root)
        head, speaker = os.path.split(head)
        transFilePath = os.path.join(root, f""{speaker}-{book}.trans.txt"")
        transFile = open(transFilePath)
        transText = transFile.readlines()
        for line in transText:
            utterance = line.split("" "")[0]
            utteranceFilePath = os.path.join(root, f""{utterance}.txt"")
            if(os.path.exists(utteranceFilePath)):
                os.remove(utteranceFilePath)
            utteranceFile = open(utteranceFilePath, 'w')
            utteranceFile.write("" "".join(line.split("" "")[1:]))
            utteranceFile.close()
        transFile.close()
        # os.remove(transFilePath)
    except Exception as e:
        print(e)
        continue
```",import o root continue try head book root head speaker head root speaker book open line utterance root utterance open except exception print continue,issue,negative,neutral,neutral,neutral,neutral,neutral
1384876723,"btw i have trained a few models for turkish language but i can say, if i share them they might be ""abused"" for known purposes amongst the huge ""scraper"" culture in the country.",trained language say share might known amongst huge scraper culture country,issue,positive,positive,positive,positive,positive,positive
1384870359,"> I also get an error like this in the train part of the synthesizer ->EOFError:Ran out of input .I would also like to mention that I got this for LibriSpeech in the synthesizer both in German and the original git clone. I would appreciate it if you could help me on how to solve this problem. Thank you in advance :) ![image](https://user-images.githubusercontent.com/81026327/120107131-76debb00-c168-11eb-879a-31af9fed3f10.png)

low memory, try to increase the swap size at windows settings or better add more ram to your pc. ",also get error like train part synthesizer ran input would also like mention got synthesizer german original git clone would appreciate could help solve problem thank advance image low memory try increase swap size better add ram,issue,positive,positive,positive,positive,positive,positive
1384576061,"I have downloaded LibriSpeech train-clean-100 dataset and I'll practice finetuning the model with one of the speakers in the dataset before I create my own dataset and finetune it.
The train-clean-100 dataset contains .flac audio files with all transcript in < speaker >-< book >.trans.txt as opposed to what this program requires. i.e. utterance-xx.flac and utterance-xx.txt

Does anyone have any code written to get the transcripts in the required format? 

I found a script for preprocessing these transcripts for the Mozilla datasets but nothing for LibriSpeech.
I will wait till tomorrow, otherwise I will write my own, share it here for anyone who needs it, and close this issue. ",practice model one create audio transcript speaker book opposed program anyone code written get format found script nothing wait till tomorrow otherwise write share anyone need close issue,issue,positive,neutral,neutral,neutral,neutral,neutral
1384568498,Went through some github issues and concluded that I don't need that SV2TTS/ directory. It will be created fresh when I run synthesizer_preprocess_audio.py,went need directory fresh run,issue,negative,positive,positive,positive,positive,positive
1384319343,Update: Fixed it by updating all the dependencies to the latest version,update fixed latest version,issue,negative,positive,positive,positive,positive,positive
1384303976,"> Looks like I can ""fix"" it by forcing librosa - audioread to use ffmpeg instead of gstreamer by commenting out this: [beetbox/audioread@`6a8e728`/audioread/__init__.py#L76-L78](https://github.com/beetbox/audioread/blob/6a8e7283f380498d28f94a60631b65d3fa623be9/audioread/__init__.py#L76-L78)
> 
> Considering librosa.load still worked outside of your GUI though I think this is still an issue here and not with my installation of gstreamer etc.
> 
> Can you say which OS you are on and if you have gstreamer installed? Maybe for you it always uses ffmpeg..

Did that. Not fixing it",like fix forcing use instead ae considering still worked outside though think still issue installation say o maybe always fixing,issue,negative,neutral,neutral,neutral,neutral,neutral
1373020733,"I had a similar issue even with batch size 1:
![image](https://user-images.githubusercontent.com/12040950/210911539-ca30c154-91d2-4a06-985f-a93307c3279e.png)


I change the number of workers in the dataloader from 4 to 2 and it was solved. 
",similar issue even batch size image change number,issue,negative,neutral,neutral,neutral,neutral,neutral
1368223378,"I was able to recreate and resolve this issue with the solution presented here:
https://stackoverflow.com/questions/49333582/portaudio-library-not-found-by-sounddevice",able recreate resolve issue solution,issue,positive,positive,positive,positive,positive,positive
1357470964,"Dedicated GPU or an APU? If it's a dedicated GPU then it's coil whine on the hardware. It is generated from small vibrations from the inductors on your GPU's PCB as voltage is passed through.

There's nothing wrong with it.

EVGA Nvidia RTX cards are known for having higher than normal GPU coil whine.

If you hate it, you can use MSI afterburner [to under-volt your GPU](https://www.youtube.com/watch?v=zUkPAVcb9Xc&t=0s).",coil whine hardware small voltage nothing wrong known higher normal coil whine hate use afterburner,issue,negative,negative,negative,negative,negative,negative
1353183493,Follow [ImanuillKant1](https://github.com/ImanuillKant1) last recommendations and also read https://github.com/coqui-ai/TTS/discussions/1975#discussioncomment-3667050 where you could have some more help,follow last also read could help,issue,negative,neutral,neutral,neutral,neutral,neutral
1352560212,"Hi Team, 
Any update on this issue, when trying to install pip install fix-yahoo-finance, I am getting the following error below
""error: metadata-generation-failed"" &  error: subprocess-exited-with-error",hi team update issue trying install pip install getting following error error error,issue,negative,neutral,neutral,neutral,neutral,neutral
1350733834,how to tarin heroin voice model any help me,tarin heroin voice model help,issue,negative,neutral,neutral,neutral,neutral,neutral
1337073797,"There is already the description of the training process here:
[https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training)",already description training process,issue,negative,neutral,neutral,neutral,neutral,neutral
1328327324,"Hi
You need to install sounddevice library. just do pip install sounddevice.
Thanks",hi need install library pip install thanks,issue,negative,positive,positive,positive,positive,positive
1328281813,"Well, the same problem
the spyder requires pyqt ver 5.12
`spyder 5.2.2 requires pyqt5<5.13, but you have pyqt5 5.15.4 which is incompatible.`

while the pyqt-tools and pyqt-plugin only compatible with pyqt ver 5.15
`pyqt5-tools 5.15.4.3.2 requires pyqt5==5.15.4, but you have pyqt5 5.12.3 which is incompatible.`
`pyqt5-plugins 5.15.4.2.2 requires pyqt5==5.15.4, but you have pyqt5 5.12.3 which is incompatible.`

it's somehow I need to sacrifice one of those tools. Any enlightenment?

regards,
Budi",well problem compatible somehow need sacrifice one enlightenment,issue,negative,neutral,neutral,neutral,neutral,neutral
1328110070,"The solution is probably just installing the latest supported pip version.
```py
python.exe -m pip install pip==21.3.1
```",solution probably latest pip version pip install,issue,negative,positive,positive,positive,positive,positive
1327988054,"the pc is cursed, throw away immediately.
",cursed throw away immediately,issue,negative,neutral,neutral,neutral,neutral,neutral
1325590358,"@JartanFTW My suggestion: if you would like to receive help, please avoid starting your comment with a tone of mocking derision, and post an actual question. 

The original poster did not ask a question. Furthermore, if they had followed up with actual questions asking for explanations, I or someone else in the forum would probably have obliged. Since they didn't follow up, either the answer helped them or they no longer care. 

My suggestion on googling those terms would have sufficed to allow anyone to learn more about the relevant subject matter. An answer about Linux and package managers, etc., information which is available elsewhere and not specifically pertinent to *this* project, would have been redundant and a waste of my time. 

I have been a useful contributor here and elsewhere and my yardstick for what is helpful and appropriate is my opinion. If you disagree, feel free to conduct yourself however you wish, and be an exemplar of what you aspire to.

> It is on a coding/source code platform where I think teaching should be encouraged the most, not discouraged. If the 
> world's largest public source of programming knowledge were to discourage learning about computers, that would
> suggest far larger problems.

No. In my opinion, the issues board, for a project hosted on Github, is not the best nor the most appropriate place to teach or learn about operating Linux; there are far better forums, Stackoverflow or unix.stackexchange.com come to mind.",suggestion would like receive help please avoid starting comment tone derision post actual question original poster ask question furthermore actual someone else forum would probably obliged since follow either answer longer care suggestion would allow anyone learn relevant subject matter answer package information available elsewhere specifically pertinent project would redundant waste time useful contributor elsewhere yardstick helpful appropriate opinion disagree feel free conduct however wish exemplar aspire code platform think teaching world public source knowledge discourage learning would suggest far opinion board project best appropriate place teach learn operating far better come mind,issue,positive,positive,positive,positive,positive,positive
1321220460,"> Hmm... apologies then? Condescension was not intended, but did you check out the original issue report? As in, ""No description provided.""  

Whether there was a description provided or not is irrelevant. While it is not ideal, there was enough information provided that I could identify not only their issue, but that I was having the same issue also.

> Otherwise, installing packages in a Linux environment is actually pretty standard stuff;  

Knowledge about Linux environments may be standard knowledge for you, but for most people, it's something they don't learn without taking university courses or other formal training.  
Going by your premise, it would be considered standard knowledge that requirements.txt should include the dependencies necessary to use the software. The PortAudio dependency is not listed.

> not sure if people who are taking time to answer questions and provide solutions should be expected to teach the basics of operating system usage  

You didn't answer their question. You provided a solution, but you did not make any effort to answer them. You did not explain why the solution works, what it's actually doing, nor did you provide any resources for them to learn it themselves. How can you expect someone to know or learn these things, while simultaneously making no effort at all to help them achieve this? You are doing a disservice by providing the solution without helping them to understand why the solution works. You are teaching people to find the answer and use it, without understanding why.
Unfortunately, the ability to self-teach is certainly an acquired skill, one that is not taught in our education systems. If we want people to develop these skills, we must show them ourselves.

> especially in a coding/source code platform?  

It is on a coding/source code platform where I think teaching should be encouraged the most, not discouraged. If the world's largest public source of programming knowledge were to discourage learning about computers, that would suggest far larger problems.",condescension intended check original issue report description provided whether description provided irrelevant ideal enough information provided could identify issue issue also otherwise environment actually pretty standard stuff knowledge may standard knowledge people something learn without taking university formal training going premise would considered standard knowledge include necessary use dependency listed sure people taking time answer provide teach operating system usage answer question provided solution make effort answer explain solution work actually provide learn expect someone know learn simultaneously making effort help achieve disservice providing solution without helping understand solution work teaching people find answer use without understanding unfortunately ability certainly acquired skill one taught education want people develop must show especially code platform code platform think teaching world public source knowledge discourage learning would suggest far,issue,positive,positive,neutral,neutral,positive,positive
1320920046,"❤️

Очень хочу, выразить свои эмоции максимально культурно . Эти бланки  я
заполнял очень очень давно уже! В чем дело?
?

Сб, 19 нояб. 2022 г. в 18:27, Kasra Samareh Golestani <
***@***.***>:

> Hi. Try this two commands below :
>
> pip install torchvision
> pip install torchaudio
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/1113#issuecomment-1320918951>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/A3A64CUYLTNFJEZK2O3QM3LWJD5YJANCNFSM6AAAAAAQNLLCYQ>
> .
> You are receiving this because you are subscribed to this thread.Message
> ID: ***@***.***>
>
",hi try two pip install pip install reply directly view id,issue,negative,positive,neutral,neutral,positive,positive
1320918951,"Hi. Try this two commands below : 
```
pip install torchvision
pip install torchaudio
```",hi try two pip install pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
1320354205,"> > Googling 'ubuntu install portaudio' would have resulted in you learning about this:
> > ```
> > sudo apt install -y portaudio19-dev
> > ```
> 
> Ah, a solution without explanation plus added condescension, my favourite!

Hmm... apologies then? Condescension was not intended, but did you check out the original issue report? As in, ""No description provided."" Otherwise, installing packages in a Linux environment is actually pretty standard stuff; not sure if people who are taking time to answer questions and provide solutions should be expected to teach the basics of operating system usage, especially in a coding/source code platform?",install would learning apt install ah solution without explanation plus added condescension condescension intended check original issue report description provided otherwise environment actually pretty standard stuff sure people taking time answer provide teach operating system usage especially code platform,issue,positive,positive,positive,positive,positive,positive
1320238599,"> Googling 'ubuntu install portaudio' would have resulted in you learning about this:
> 
> ```
> sudo apt install -y portaudio19-dev
> ```

Ah, a solution without explanation plus added condescension, my favourite!",install would learning apt install ah solution without explanation plus added condescension,issue,negative,positive,positive,positive,positive,positive
1311514424,"pip install shap

 note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.


(please help)",pip install shap note error likely problem pip error error generating package see output note issue package pip hint see please help,issue,negative,neutral,neutral,neutral,neutral,neutral
1311163446,"Thanks, I have put voice cloning at the back of my to do list, and started learning to code python instead. I wanted voice cloning for youtube, now I have changed career paths to coding.

Sent from Mail<https://go.microsoft.com/fwlink/?LinkId=550986> for Windows

From: ***@***.***>
Sent: Thursday, 27 October 2022 11:46 PM
To: ***@***.***>
Cc: ***@***.***>; ***@***.***>
Subject: Re: [CorentinJ/Real-Time-Voice-Cloning] pip install -r requirements.txt, ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt' (Issue #1117)


you need to run
pip install -r >full path to requirements.txt file in the download<
for me its
pip install -r F:\Downloads\Real-Time-Voice-Cloning-master\Real-Time-Voice-Cloning-master\requirements.txt
it will look a bit different but you have to go exactly to where your requirements.txt is copy the path above the files and add \requirements.txt to it

—
Reply to this email directly, view it on GitHub<https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/1117#issuecomment-1293550702>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/A3IHARDO4BW6OZDIAAVCTSLWFKBRXANCNFSM6AAAAAAQU464PQ>.
You are receiving this because you authored the thread.Message ID: ***@***.***>

",thanks put voice back list learning code python instead voice career sent mail sent subject pip install error could open file file directory issue need run pip install full path file pip install look bit different go exactly copy path add reply directly view id,issue,negative,positive,neutral,neutral,positive,positive
1306029085,Are there simpler instructions to train and synthesize Brazilian Portuguese? I followed instructions to train with the GUI of demo_toolbox.py and the end result sounds like someone from Mississipi.  ,simpler train synthesize train end result like someone,issue,negative,neutral,neutral,neutral,neutral,neutral
1306020918,"Yes, please clarify as to how to use languages other than English.",yes please clarify use,issue,positive,neutral,neutral,neutral,neutral,neutral
1303801211,"You can find mirrors on this page: https://www.openslr.org/12

Scroll down to `train-clean-100.tar.gz` and then choose one of the mirrors e.g. [US].",find page scroll choose one u,issue,negative,neutral,neutral,neutral,neutral,neutral
1293550702,"you need to run 
pip install -r >full path to requirements.txt file in the download<
for me its 
pip install -r F:\Downloads\Real-Time-Voice-Cloning-master\Real-Time-Voice-Cloning-master\requirements.txt
it will look a bit different but you have to go exactly to where your requirements.txt is copy the path above the files and add \requirements.txt to it",need run pip install full path file pip install look bit different go exactly copy path add,issue,negative,positive,positive,positive,positive,positive
1287256476,#437 has a dropbox link which dont exist so kinda hard to reproduce ,link dont exist hard reproduce,issue,negative,negative,negative,negative,negative,negative
1282613463,"> > Thank you very much! I think we can close the issue.
> 
> @e0xextazy Is it possible for you to share synthesizer code with Tacotron2 model?

@inji0129 Can you provide me this code please, if you can get this?",thank much think close issue possible share synthesizer code model provide code please get,issue,positive,positive,neutral,neutral,positive,positive
1282609573,"> Thank you very much! I think we can close the issue.

@e0xextazy Hi dear Mark,
Can you provide me your implemetation of this repo using tacotron2?
If you help me, I am very thankfull for this favor.",thank much think close issue hi dear mark provide help favor,issue,positive,positive,positive,positive,positive,positive
1280721552,"> Hello! I've the same problem for the french language ! Any idea?


I dont know, today I have done more 10k steps, so total is 40k+ steps and this is the result
![attention_step_42000_sample_1](https://user-images.githubusercontent.com/95704232/196167465-f5ad363e-d48f-48eb-a4e1-54ed1b68e6a5.png)
",hello problem language idea dont know today done total result,issue,negative,neutral,neutral,neutral,neutral,neutral
1280580016,"Hello! 
I've the same problem for the french language ! 
Any idea? ",hello problem language idea,issue,negative,neutral,neutral,neutral,neutral,neutral
1273640285,Updating numpy version to `1.21` fixed issues I was having running the app on Win10 with Python 3.10.5.,version fixed running win python,issue,positive,positive,positive,positive,positive,positive
1268175104,"> @LinkleZe probably the synthetiser has not learnt the attention, check the attention plots.

what does the attention plot represent please? ",probably learnt attention check attention attention plot represent please,issue,negative,neutral,neutral,neutral,neutral,neutral
1267793553,"> I faced the same problem and resolved it by degrading the PyTorch version from 1.10.1 to 1.8.1 with code 11.3. In my case, I am using GPU RTX 3060, which works only with Cuda version 11.3 or above, and when I installed Cuda 11.3, it came with PyTorch 1.10.1. So I degraded the PyTorch version, and now it is working fine.
> 
> $ **pip3 install torch==1.8.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html**
> 
> 2- You can check by reducing train batch size also.

Yes, reducing the batch size worked with me many times :-)",faced problem resolved degrading version code case work version came degraded version working fine pip install check reducing train batch size also yes reducing batch size worked many time,issue,negative,positive,positive,positive,positive,positive
1266278165,"I also installed the train-clean-500 files under Dataset and still only see ""default"" for ""Encoder"" and ""Synthesizer"" and only ""default"" and ""Griffin-Lim"" for Vocoder.  In the video there was a veritable cornucopia of Vocoder choices, how do I get those?",also still see default synthesizer default video veritable cornucopia get,issue,negative,neutral,neutral,neutral,neutral,neutral
1265685602,"I read this and the linked issue but I couldn't find the answer to ""how do I get the gen_s_mel_raw vocoder"".  I only get two options: ""default"" and ""Griffin-Lim"".  The latter sounds significantly better than ""default"" and about the same quality as in the video, i.e. not really sounding much like the input source.  I feel like with ""gen_s_mel_raw"" it would reach the quality shown in the demo video.

So if I may ask, what am I missing that I do not see the ""gen_s_mel_raw"" vocoder?  I have installed and am using both  train-clean-100 and train-clean-360 as recommended, and am passing the path to the Dataset directory on the command line (and train-clean-100 shows up under Dataset in the upper left (Don't see train-clean-360 anywhere though)",read linked issue could find answer get get two default latter significantly better default quality video really sounding much like input source feel like would reach quality shown video may ask missing see passing path directory command line upper left see anywhere though,issue,positive,positive,neutral,neutral,positive,positive
1265466044,@babysor any chance we can get Chinese model in google drive link? Baidu is not accessible outside china,chance get model drive link accessible outside china,issue,negative,positive,positive,positive,positive,positive
1264862020,"Fixed with:

    pip uninstall pyqt5
    pip uninstall pyqt5-sip
    pip uninstall pyqt5-qt5
    pip install pyqt5 --user
",fixed pip pip pip pip install user,issue,negative,positive,neutral,neutral,positive,positive
1264809282,"I was able to get past the audioread issue by manually installing it in a separate step:

    pip install audioread --user

After this I reran: `pip install -r R:\requirements.txt --user` and everything succeeded in installing.

Then there was one more slight hiccup.  After finishing Step #1 of the install instructions, I skipped the optional 2nd step and tried to run the optional 3rd step... only for `demo_cli.py` to be not found.  I try to follow install instructions exactly, and it appears the step on how to actually install this repo is missing.  It didn't take me too long to figure out (after searching my entire disk for `demo*py` to download the zipfile and put it somewhere, but as it stands I believe the instructions are technically incomplete.
",able get past issue manually separate step pip install user pip install user everything one slight hiccup finishing step install optional step tried run optional step found try follow install exactly step actually install missing take long figure searching entire disk put somewhere believe technically incomplete,issue,negative,positive,neutral,neutral,positive,positive
1263085302,"Hmm,i don't speak English so i may make some grammar mistakes.Please forgive me.There is a file called requirements.txt in the root directory of each project, which contains all dependencies of the project.You may not be in the root directory of the project or you may have lost the requirements.txt file.I suggest you download the whole project again and run it again in the root directory of the project.",speak may make grammar forgive file root directory project may root directory project may lost suggest whole project run root directory project,issue,negative,positive,positive,positive,positive,positive
1255255370,"Did you run pip install - requirements?
if it ran into any error open requirements file and install each module individually. 
in your case pip install scipy . run this commmand from git bash . 
",run pip install ran error open file install module individually case pip install run git bash,issue,negative,neutral,neutral,neutral,neutral,neutral
1251908241,"> I faced the same problem. There were two issues I had to resolve:
> 
> 1. Looks like librosa does not like a Path object being sent to the load function. You will need to convert it to a string with full path, like this: str(path). You can change this in synthesizer/inference.py's load_preprocess_wav function.
> 2. Secondly, make sure your WAV file is 16000 Hz, 16 bit PCM. You can use audacity on windows to convert audio format.
> 
> Hope this helps.
> 
> Cheers Raghu

def load_preprocess_wav(fpath):
        """"""
        Loads and preprocesses an audio file under the same conditions the audio files were used to
        train the synthesizer.
        """"""
        wav = librosa.load(str(fpath), hparams.sample_rate)[0]
        if hparams.rescale:
            wav = wav / np.abs(wav).max() * hparams.rescaling_max
        return wav
it is...but still not fixed?
",faced problem two resolve like like path object sent load function need convert string full path like path change function secondly make sure file bit use audacity convert audio format hope audio file audio used train return still fixed,issue,positive,positive,positive,positive,positive,positive
1251754099,"Hey wjy9902.

Not completely, but take a look to [this other thread](https://github.com/coqui-ai/TTS/discussions/1975#discussioncomment-3667050) since you can find useful things there.",hey completely take look thread since find useful,issue,negative,positive,positive,positive,positive,positive
1250561223,"Hello, I meet same issue now. Do you already solve this problem?",hello meet issue already solve problem,issue,negative,neutral,neutral,neutral,neutral,neutral
1247932500,"@neonsecret the pretrained synth model is gone from google drive, any chance you could reupload it?",model gone drive chance could,issue,negative,neutral,neutral,neutral,neutral,neutral
1247427920,...which I just fixed by running `pip install -U pyqt5`,fixed running pip install,issue,negative,positive,neutral,neutral,positive,positive
1247425267,"I have this issue too, NOT in Colab. Python 3.10, Windows 10.

I see it might have something to do with OpenCV, and [this answer](https://stackoverflow.com/a/60285850) says to install `pip install opencv-python==4.1.2.30`, which is a super old version and is no longer supported, so not an ideal solution. I haven't even tested it to see if that actually does work anyway, but that's all I've found on the matter too. Would be cool to see this one get fixed.

EDIT

My error is this:

```
qt.qpa.plugin: Could not find the Qt platform plugin ""windows"" in """"
This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.
```",issue python see might something answer install pip install super old version longer ideal solution even tested see actually work anyway found matter would cool see one get fixed edit error could find platform application start platform could application may fix problem,issue,positive,positive,positive,positive,positive,positive
1247125418,"hello there, please I would like to be part of the project(voice-to-voice). please add me up.",hello please would like part project please add,issue,positive,neutral,neutral,neutral,neutral,neutral
1241690696,""" You're free to download any dataset "" should be the correct way rather than ""You're free not to download""",free correct way rather free,issue,positive,positive,positive,positive,positive,positive
1240445323,"It can be improved by training a new vocoder model from scratch on higher quality data. You can preprocess the dataset at a higher sampling rate, and the vocoder will output at that sample rate.",training new model scratch higher quality data higher sampling rate output sample rate,issue,negative,positive,positive,positive,positive,positive
1240438261,"Finetune the synthesizer model on your own dataset. You don't have to train the encoder or vocoder. Instead of training from scratch, resume the training on pretrained models so they will work better with your voice.

Before finetuning, you should get experience training a synthesizer from scratch, using a dataset that is known to work. It helps a lot.",synthesizer model train instead training scratch resume training work better voice get experience training synthesizer scratch known work lot,issue,negative,positive,positive,positive,positive,positive
1240433930,"Start here https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/431#issuecomment-673555684

Here are the code changes for spanish, you can use these to get an idea what needs to be changed. https://github.com/raccoonML/Real-Time-Voice-Cloning/commit/e7aa707c976db0cebbdf2cc68f140edcdd31e7f4 If your text transcripts contain abbreviations you'll also want to update your text cleaners so the abbreviations will be expanded for training.",start code use get idea need text contain also want update text expanded training,issue,negative,neutral,neutral,neutral,neutral,neutral
1240425120,"I don't think the toolbox will run on google colab. At best you might get demo_cli.py to run there.

You can try running the repo locally on your own computer. A gpu is not required unless training a model.",think toolbox run best might get run try running locally computer unless training model,issue,positive,positive,positive,positive,positive,positive
1240419929,"Install ffmpeg if you want this repo to work with mp3 files. Most windows users can take the ffmpeg.exe from this archive, and copy it to the same level as demo_toolbox.py. https://github.com/BtbN/FFmpeg-Builds/releases/download/latest/ffmpeg-master-latest-win64-lgpl.zip",install want work take archive copy level,issue,negative,neutral,neutral,neutral,neutral,neutral
1240414889,"You need to change directory in the command prompt to the location of the files. Get the actual path from windows explorer. Then you can run pip install -r requirements.txt.
```
cd C:\path\to\RTVC\files
```",need change directory command prompt location get actual path explorer run pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
1234507231,"I think we better discuss this via email, since it's not part of the issue, but yeah, in my opinion, the results, even of the most recent models, don't sound like the targets. If you want to achieve this, by now, you need to finetune the model with a dataset of the target voice, that works when you have many audios from the target to clone.",think better discus via since part issue yeah opinion even recent sound like want achieve need model target voice work many target clone,issue,positive,positive,positive,positive,positive,positive
1234484223,"> 

Excellent ! one more question: in this issue you comment that the results you obtained do not resemble those of the target voice. Could you solve this problem? any suggestion? Thanks for your help AlexSteveChungAlvarez!",excellent one question issue comment resemble target voice could solve problem suggestion thanks help,issue,positive,positive,positive,positive,positive,positive
1234477351,Of course! You just need to put your dataset in the correct structure.,course need put correct structure,issue,negative,neutral,neutral,neutral,neutral,neutral
1234383926,"> 
Hello, I want to use a dataset in Spanish from Argentina, can this implementation be adapted for that? Any information is welcome. Thanks a lot !",hello want use implementation information welcome thanks lot,issue,positive,positive,positive,positive,positive,positive
1232806780,"I tried to type that command and typed this comand :  python demo_toolbox.py
but there was too much error so I just gived up.. thanks for your help",tried type command python much error thanks help,issue,negative,positive,positive,positive,positive,positive
1232375027,"ECAPA-TDNN works well and show superior results on SV tasks, but its performance is not doomed to be better than dvector or other speaker embeddings when applied to other speaker-related tasks as a speaker representation. At least from my experiments...

> First, Thanks for the excellent work by CorentinJ! I noticed that the speaker encoder used in this work is ge2e, performance of which is far fall behind the SOTA. So I replaced the ge2e encoder with ECAPA-TDNN model. One difference between ge2e and **ECAPA-TDNN** is that the dimension of embedding is **192** in ECAPA-TDNN while **256** in ge2e. I changed the `speaker embedding` and `batch size`parameter in `hparams.py` and followed the `synthesizer_train.py` to train Tacotron synthesizer. The parameter I used are as follows:
> 
> `tts_schedule = [(2, 1e-3, 10_000, 32),` ` (2, 5e-4, 15_000, 32),` ` (2, 2e-4, 20_000, 32),` ` (2, 1e-4, 30_000, 32),` ` (2, 5e-5, 40_000, 32),` ` (2, 1e-5, 60_000, 32),` ` (2, 5e-6, 160_000, 32),` ` (2, 3e-6, 320_000, 32),` ` (2, 3e-6, 640_000, 32)]` `speaker_embedding_size = 192`
> 
> However, when i have trained the Tacotron 200k steps, i found my loss is **0.53** but the attention plot is blank. The mel output of each 500 steps is similar with the ground truth. The synthesized result with the 200k pretrained pt file is poor, but similar with the speaker used to synthesized. It is so weird.
> 
> Does anyone meet the same problem? Or do i need to change other parameter when i change the dimension of speaker embedding?

ECAPA-TDNN works well and show superior results on SV tasks, but its performance is not doomed to be better than dvector or other speaker embeddings (sometime even i-vector) when applied to other speaker-related tasks as a speaker representation. At least from my experiments...",work well show superior performance better speaker applied speaker representation least first thanks excellent work speaker used work gee performance far fall behind gee model one difference gee dimension gee speaker batch size parameter train synthesizer parameter used however trained found loss attention plot blank mel output similar ground truth result file poor similar speaker used weird anyone meet problem need change parameter change dimension speaker work well show superior performance better speaker sometime even applied speaker representation least,issue,positive,positive,positive,positive,positive,positive
1228723556,"Try running this command : 
`python3 demo_cli.py`

This will automatically download all the models and put them in the correct folder. ",try running command python automatically put correct folder,issue,negative,neutral,neutral,neutral,neutral,neutral
1225179306,"@CorentinJ 

How do you think this repo will hold up in attempting to do the stuff below nowadays? Do you think this repo is a good fit or are there better options out there? I don't need to do text to speech and I don't need a GUI. All I need to do is be able to run the voice to voice code on demand. 

-I will have a number of different audio file samples of different voices reading a variety of sentences. These will be the training models.
-In real time, an input audio file of a person reading a sentence will be sent over the internet and this repo will receive it, convert it into an output file of one of the training models, then send it back over the internet.
-The words in the output audio file should be able to be understood as clearly as they are in the input audio file.

 ",think hold stuff nowadays think good fit better need text speech need need able run voice voice code demand number different audio file different reading variety training real time input audio file person reading sentence sent receive convert output file one training send back output audio file able understood clearly input audio file,issue,positive,positive,positive,positive,positive,positive
1220974403,"PS C:\Users\dacia\OneDrive\Skrivbord\RTVC-Swedish> python demo_toolbox.py
2022-08-19 21:21:27.615361: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found
2022-08-19 21:21:27.615492: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Traceback (most recent call last):
  File ""demo_toolbox.py"", line 2, in <module>
    from toolbox import Toolbox
  File ""C:\Users\dacia\OneDrive\Skrivbord\RTVC-Swedish\toolbox\__init__.py"", line 1, in <module>
    from toolbox.ui import UI
  File ""C:\Users\dacia\OneDrive\Skrivbord\RTVC-Swedish\toolbox\ui.py"", line 15, in <module>
    import umap
  File ""C:\Users\dacia\AppData\Local\Programs\Python\Python37\lib\site-packages\umap\__init__.py"", line 7, in <module>
    from .parametric_umap import ParametricUMAP
  File ""C:\Users\dacia\AppData\Local\Programs\Python\Python37\lib\site-packages\umap\parametric_umap.py"", line 14, in <module>
    import tensorflow as tf
  File ""C:\Users\dacia\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 99, in <module>
    from tensorflow_core import *
  File ""C:\Users\dacia\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""<frozen importlib._bootstrap>"", line 1019, in _handle_fromlist
  File ""C:\Users\dacia\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\dacia\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\dacia\AppData\Local\Programs\Python\Python37\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\dacia\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\__init__.py"", line 52, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""C:\Users\dacia\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\core\framework\graph_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import node_def_pb2 as tensorflow_dot_core_dot_framework_dot_node__def__pb2
  File ""C:\Users\dacia\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\core\framework\node_def_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2
  File ""C:\Users\dacia\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\core\framework\attr_value_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2
  File ""C:\Users\dacia\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\core\framework\tensor_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2
  File ""C:\Users\dacia\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\core\framework\resource_handle_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2
  File ""C:\Users\dacia\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\core\framework\tensor_shape_pb2.py"", line 42, in <module>
    serialized_options=None, file=DESCRIPTOR),
  File ""C:\Users\dacia\AppData\Local\Programs\Python\Python37\lib\site-packages\google\protobuf\descriptor.py"", line 560, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

how do i fix this? ",python could load dynamic library found ignore set machine recent call last file line module toolbox import toolbox file line module import file line module import file line module import file line module import file line module import file line module import file frozen line file line module file line module file line return name level package level file line module import file line module import file line module import file line module import file line module import file line module import file line module file line directly call came file code date must immediately regenerate possible downgrade package lower set use much information fix,issue,negative,positive,neutral,neutral,positive,positive
1220581283,"You can also check [this repo](https://github.com/yui-mhcp/text_to_speech) on which I have shared my french models (single and multi-speaker)

Good luck !",also check single good luck,issue,positive,positive,positive,positive,positive,positive
1220563853,@bryant0918 I have moved to CoquiTTS and haven't kept the things I had made with this repository since it was discontinued late last year. ,kept made repository since late last year,issue,negative,negative,negative,negative,negative,negative
1220031293,@Ca-ressemble-a-du-fake Would you mind sharing your trained French synthesizer?,would mind trained synthesizer,issue,negative,neutral,neutral,neutral,neutral,neutral
1214432819,"Same here.

I attempt to perform a test by running demo_cli.py
I type in the filepath to the sample file I wish to use.

I get this in the terminal.

```
warnings.warn(""PySoundFile failed. Trying audioread instead."")
Caught exception: NoBackendError()
Restarting
```",attempt perform test running type sample file wish use get terminal trying instead caught exception,issue,negative,neutral,neutral,neutral,neutral,neutral
1208465066,"Hello sir, i have installed every dependencies but while i run th audio i get an error which says Numpy not available .... what can i do to resolve it ??? ",hello sir every run th audio get error available resolve,issue,negative,positive,positive,positive,positive,positive
1207474020,"you have to really bother installing all the versions of ""torch==XXX+CuYYY"" to be sure that it works maybe with luck if it's a sunny day... it's not serious.  there should be a solution for 99% of the cases.",really bother sure work maybe luck sunny day serious solution,issue,positive,positive,positive,positive,positive,positive
1207248248,"Numpy 1.20.3 does not work with Python 3.10, I had the same problem. The recommended Python version for this project is 3.7, I'm using 3.7.10 and it's working perfectly fine. I'm using [pyenv](https://github.com/pyenv/pyenv) to manage my Python versions, but you could also use Anaconda or PyCharm if you're on Windows for example.",work python problem python version project working perfectly fine manage python could also use anaconda example,issue,positive,positive,positive,positive,positive,positive
1207240583,This error means you're missing `Python3.dll` according to StackOverflow. You can try the accepted solution for [this](https://stackoverflow.com/questions/42863505/dll-load-failed-when-importing-pyqt5) question.,error missing according try accepted solution question,issue,negative,negative,negative,negative,negative,negative
1204236604,"I haven't tried with an audio of that length since my objective of using this code was to clone voices from few length speech audios and few target audios from the people I wanted to clone. When you have access to more audios from the person you want to clone, you will get better results by fine-tuning the model (there is a guide somewhere in the repo on how to do this). I haven't done this, but yeah, you need to train the model with your own voices, by applying transfer learning in the pre-trained model. I think it would be a better solution in your case (if you have many audios of the target voice). If not, try sharing the process you are doing in order to clone with your 23 min audio, maybe you are writing with periods (""."") at the end of each sentence and it does make the synthesis go wrong, try using commas instead of periods!",tried audio length since objective code clone length speech target people clone access person want clone get better model guide somewhere done yeah need train model transfer learning model think would better solution case many target voice try process order clone min audio maybe writing end sentence make synthesis go wrong try instead,issue,positive,positive,positive,positive,positive,positive
1203617660,"Thanks, @AlexSteveChungAlvarez, I've solved by changing the type of Synthesizer. I'm confused right now. I've used the pretrained models and used an audio file of 23 minutes, however, I cannot synthesize correctly my voice using simple sentences in Spanish. What I'm doing wrong? Should I train a specific model with my own voices? ",thanks type synthesizer confused right used used audio file however synthesize correctly voice simple wrong train specific model,issue,negative,negative,neutral,neutral,negative,negative
1203133444,"I also get the same error when I use first one model and then change it to the other, I just downloaded everything again and made sure to run the correct one from the beginning to overcome it, so, whenever I want to use any model, I do it from its respective folder (I have like 2-3 different folders to run different models). I don't know what should be the correct way to fix that error.",also get error use first one model change everything made sure run correct one beginning overcome whenever want use model respective folder like different run different know correct way fix error,issue,negative,positive,positive,positive,positive,positive
1201348008,"Now, I've used the code from the spanish version and, I can see the pretrained options. However, I get this error:
![image](https://user-images.githubusercontent.com/8438920/182183685-9cc66b76-fc8f-4500-aeb6-0fb79aca74a2.png)


Any clue?",used code version see however get error image clue,issue,negative,neutral,neutral,neutral,neutral,neutral
1201147743,"I've followed this [video ](https://www.youtube.com/watch?v=vHSulnOxfGo)and could run the program. However:
1- It only selects the default, Encoder, Synthesizer and vocoder. I've copy&paste the latest version of these folders into the Git content (it had save_models folder).
2- I've run the program already 4 times and the voice still cannot talk properly. How many times do you think should I run the program? The wav files has a length of 23 minutes. 

I add a screenshort of the program:
![image](https://user-images.githubusercontent.com/8438920/182149150-b293b333-475c-4db8-94e1-3b702d92b767.png)

I just added the wav file and click on Synthesize and vocode. Is this the procedure so far? ",video could run program however default synthesizer copy paste latest version git content folder run program already time voice still talk properly many time think run program length add program image added file click synthesize procedure far,issue,negative,positive,positive,positive,positive,positive
1200238325,"Another question, I don't have an NVIDIA GPU, can I use the CPU instead?",another question use instead,issue,negative,neutral,neutral,neutral,neutral,neutral
1200237622,"Pls share in here results for try later...

El sáb, 30 de jul. de 2022, 10:42 a. m., AlexSteveChungAlvarez <
***@***.***> escribió:

> No need to train with audio files of the wanted length to produce, just
> try to use a reference (target) audio to clone of that length and it will
> work.
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/789#issuecomment-1200231419>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AADIWFFYXA2BCOFTN6YY37DVWVEP5ANCNFSM473UA5FA>
> .
> You are receiving this because you were mentioned.Message ID:
> ***@***.***>
>
",share try later el de de need train audio length produce try use reference target audio clone length work reply directly view id,issue,negative,positive,neutral,neutral,positive,positive
1200231419,"No need to train with audio files of the wanted length to produce, just try to use a reference (target) audio to clone of that length and it will work.",need train audio length produce try use reference target audio clone length work,issue,negative,neutral,neutral,neutral,neutral,neutral
1200229978,"Sure! I'll share my model with you. As recommendation, I would like to generate synthesize audios of approximately 10 min, hence, do you recommend me to train with audio files of length 10 min? Or which is, in your experience, the most efficient length for an excellent quality? ",sure share model recommendation would like generate synthesize approximately min hence recommend train audio length min experience efficient length excellent quality,issue,positive,positive,positive,positive,positive,positive
1200212366,"If you want to train your own model, you should follow the instructions given in this repo, the toolbox does the synthesis when you click on the button it has to generate voice (there's a video in this repo explaining that), there isn't a maximum length of text to synthesize, but the recommended length is to synthesize an audio of a similar length to the original (if not, it happens sometimes that it has some kind of silences or noise). If you train your own model, please share it to me, since my college team is working now on a web interface to calculate the MOS of the Spanish models shared by the community!",want train model follow given toolbox synthesis click button generate voice video explaining maximum length text synthesize length synthesize audio similar length original sometimes kind noise train model please share since college team working web interface calculate community,issue,positive,positive,positive,positive,positive,positive
1200168588,Thank @AlexSteveChungAlvarez s. I have some questions: 1) where should I locate my datasets? 2) the synthesis phase is included when running demo_toolbox.py? 3) is there a maximum length of text to synthesize? If you have a tutorial/documentation beside the paper is welcome. Thank you again,thank locate synthesis phase included running maximum length text synthesize beside paper welcome thank,issue,positive,positive,positive,positive,positive,positive
1198756577,"> I've read that WaveGlow is more robust in handling several languages, but WaveRNN is language dependent and quickly degrades when you train on an additional language. If I were to create a multilingual system would it still be better to use WaveRNN and train several different models? Or use a single WaveGlow model that could essentially handle any language? What would my cost be in quality and Speed?

I did not play around with it yet, but some time ago I came across this repo on multilingual TTS in a single synthesizer: https://github.com/Tomiinek/Multilingual_Text_to_Speech

They're using WaveRNN, so I assume the quality really just depends on whether the Vocoder has been trained with good multilingual samples. In the end the vocoder is used to render a generated AI voice more natural; so it 'should' not matter which language the voice is speaking in.",read robust handling several language dependent quickly train additional language create multilingual system would still better use train several different use single model could essentially handle language would cost quality speed play around yet time ago came across multilingual single synthesizer assume quality really whether trained good multilingual end used render ai voice natural matter language voice speaking,issue,positive,positive,positive,positive,positive,positive
1185535213,"You need to be more specific on the various versions of the libraries you're using.
The error is often caused by a mismatch between python's version, pytorch and numpy",need specific various error often mismatch python version,issue,negative,neutral,neutral,neutral,neutral,neutral
1184814771,"@arianaglande Hi, how did you manage to preprocess the italian dataset into the format the scripts accept?",hi manage format accept,issue,negative,neutral,neutral,neutral,neutral,neutral
1180435016,"I faced the same problem and resolved it by degrading the PyTorch version from 1.10.1 to 1.8.1 with code 11.3.
In my case, I am using GPU RTX 3060, which works only with Cuda version 11.3 or above, and when I installed Cuda 11.3, it came with PyTorch 1.10.1. So I degraded the PyTorch version, and now it is working fine.

$ **pip3 install torch==1.8.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html**

2- You can check by reducing train batch size also.",faced problem resolved degrading version code case work version came degraded version working fine pip install check reducing train batch size also,issue,negative,positive,positive,positive,positive,positive
1176265169,"I will look into it, it's an attention problem of some kind",look attention problem kind,issue,negative,positive,positive,positive,positive,positive
1176243228,"Tried recent version and now it produces proper speech, thanks! 

The problem is that our ot 10 words in sentence it speaks only 1-2. Any chance this can be fixed and longer sequence be possible to generate?",tried recent version proper speech thanks problem sentence chance fixed longer sequence possible generate,issue,negative,positive,neutral,neutral,positive,positive
1174451198,"I've tried various lengths and bitrates and normalisation strategies on the input wav file and I have not been able to replicate any decent results yet, performance degrades rapidly over time such that after the 10th second it's just noise and although the first second or so sounds ok, the rest is consistently poor quality, nothing like the demo video.  Same results when using the cli demo and my own rehacking of the code.",tried various input file able replicate decent yet performance rapidly time th second noise although first second rest consistently poor quality nothing like video code,issue,negative,positive,neutral,neutral,positive,positive
1172975080,"> Synth Trained on LibriTTS 200k steps with old /original encoder.
> 
> https://drive.google.com/drive/folders/1ah6QNyB8jIcFuKusPOVdx0pPIZxeZeul?usp=sharing
> 
> Let me know if the link works. or not and if any files are missing.

Hi @mbdash, did you train the synthesizer using your trained 1M steps encoder afterwards? Cause I find your encoder is really good but this synthesizer is only based on original encoder and in Tensorflow form, I can't use it in Pytorch code now.",trained old let know link work missing hi train synthesizer trained afterwards cause find really good synthesizer based original form ca use code,issue,positive,positive,positive,positive,positive,positive
1164492117,"OK I think I made some dumb mistakes. 

After line 84 in ```train.py```
```
y_hat = model(x, m)
```
The shape of ```y_hat``` is [batch, seq len, 2**bits], F.softmax should be done to dim=2. 
After changing this dim index, I could have ```sample``` in shape [batch, seq len]

I think this is already a signal sampled from the distribution? 
Will keep this issue open for a while more. ",think made dumb line model shape batch done dim index could sample shape batch think already signal distribution keep issue open,issue,negative,negative,neutral,neutral,negative,negative
1162142672,"After downloading one of the supported datasets you must pass it in as an argument while running the demo_toolbox.py:

python demo_toolbox.py -d <datasets_root>",one must pas argument running python,issue,negative,neutral,neutral,neutral,neutral,neutral
1154481356,"> I have polish voice database that is maybe not the best but I still want to try to make polish model. I have loaded my database into encoder training and there were some errors connected with ""visdom_server"" Please help me find some resources to train it. Some kind of tutorial or guide?

Can you share the Polish database you're using as well as the model if you ever completed it?",polish voice maybe best still want try make polish model loaded training connected please help find train kind tutorial guide share polish well model ever,issue,positive,positive,positive,positive,positive,positive
1154224585,"I have similar issue... i already posted my issue but i will attach here again for fast response. Thank you

![not browsing ](https://user-images.githubusercontent.com/71806009/173416942-487755c9-3c13-4329-9f09-97923ae0035e.jpg)
.",similar issue already posted issue attach fast response thank browsing,issue,negative,positive,neutral,neutral,positive,positive
1152853099,Your version of pytorch was not compiled with numpy support. Numpy must be installed prior to building/install pytorch for support to be compiled in.,version support must prior support,issue,positive,neutral,neutral,neutral,neutral,neutral
1152852963,"Googling 'ubuntu install portaudio' would have resulted in you learning about this:
```
sudo apt install -y portaudio19-dev
```",install would learning apt install,issue,negative,positive,positive,positive,positive,positive
1152665229,What did you train your Swedish models on? I would love a pointer to some good datasets. I'm trying to replicate your Swedish Tensorflow models in PyTorch. @ViktorAlm,train would love pointer good trying replicate,issue,positive,positive,positive,positive,positive,positive
1149237937,"I've read that WaveGlow is more robust in handling several languages, but WaveRNN is language dependent and quickly degrades when you train on an additional language. If I were to create a multilingual system would it still be better to use WaveRNN and train several different models? Or use a single WaveGlow model that could essentially handle any language? What would my cost be in quality and Speed?",read robust handling several language dependent quickly train additional language create multilingual system would still better use train several different use single model could essentially handle language would cost quality speed,issue,positive,positive,positive,positive,positive,positive
1147347711,"it's working as expected, the audio quality is just too low yet, also the sample audio snippet you provided might be too short to catch the voice.
just wait for updates, the quality will improve with some time",working audio quality low yet also sample audio snippet provided might short catch voice wait quality improve time,issue,negative,neutral,neutral,neutral,neutral,neutral
1147338768,"Sorry, for some reason synth was from original release, despite i've downloaded russian one, now there are no errors. 

But it's not reading russian letters, for some reason, that's what i have:
https://www.dropbox.com/s/72xgcr17oisrzln/demo_output_00.wav?dl=0

Reference voice: enter an audio filepath of a voice to be cloned (mp3, wav, m4a, flac, ...):
C:\Games\Thief Voice\WEBCALL\Garrett\english\gar0112.wav
Loaded file succesfully
Created the embedding
Write a sentence (+-20 words) to be synthesized:
Это хороший день для ограбления, попробуем
['e1', 't', 'o0', '<eos>', 'h', 'o0', 'r', 'o1', 'sh', 'i0', 'j', '<eos>', 'dj', 'e1', 'nj', '<eos>', 'd', 'lj', 'a1', '<eos>', 'o0', 'g', 'r', 'a0', 'b', 'lj', 'e1', 'nj', 'i0', 'j', 'a0', '<eos>', ',', 'p', 'o0', 'p', 'r', 'o0', 'b', 'u1', 'j', 'e0', 'm', '<eos>']

| Generating 1/1


Done.

Created the mel spectrogram
Synthesizing the waveform:
{| ████████████████ 437000/441600 | Batch Size: 46 | Gen Rate: 30.0kHz | }float64

Saved output as demo_output_00.wav",sorry reason original release despite one reading reason reference voice enter audio voice loaded file write sentence generating done mel spectrogram batch size gen rate float saved output,issue,positive,negative,neutral,neutral,negative,negative
1147280471,"and put it into ""default"" folder of the ""saved_models"" folder ",put default folder folder,issue,negative,neutral,neutral,neutral,neutral,neutral
1147276377,"you downloaded the wrong model, be careful, download the synthesizer from the Google drive link from readme and overwrite the file.
and did you clone the repository from scratch after the recent fixes?",wrong model careful synthesizer drive link overwrite file clone repository scratch recent,issue,negative,negative,negative,negative,negative,negative
1147170976,"Sadly it still gives me this error: 

`PS C:\Users\babud\Downloads\Real-Time-Voice-Cloning-Multilang-master> python .\demo_cli.py
Arguments:
    enc_model_fpath:   saved_models\default\encoder.pt
    syn_model_fpath:   saved_models\default\synthesizer.pt
    voc_model_fpath:   saved_models\default\vocoder.pt
    cpu:               False
    no_sound:          False
    seed:              None

Running a test of your configuration...

Found 1 GPUs available. Using GPU 0 (NVIDIA GeForce RTX 2080 with Max-Q Design) of compute capability 7.5 with 8.6Gb total memory.

Preparing the encoder, the synthesizer and the vocoder...
Loaded encoder ""encoder.pt"" trained to step 1564501
Synthesizer using device: cuda
Building Wave-RNN
Trainable Parameters: 4.481M
Loading model weights at saved_models\default\vocoder.pt
Testing your configuration with small inputs.
        Testing the encoder...
        Testing the synthesizer...
Trainable Parameters: 30.936M
Traceback (most recent call last):
  File "".\demo_cli.py"", line 95, in <module>
    mels = synthesizer.synthesize_spectrograms(texts, embeds)
  File ""C:\Users\babud\Downloads\Real-Time-Voice-Cloning-Multilang-master\synthesizer\models\tacotron_tweaked\inference.py"", line 87, in synthesize_spectrograms
    self.load()
  File ""C:\Users\babud\Downloads\Real-Time-Voice-Cloning-Multilang-master\synthesizer\models\tacotron_tweaked\inference.py"", line 65, in load
    self._model.load(self.model_fpath)
  File ""C:\Users\babud\Downloads\Real-Time-Voice-Cloning-Multilang-master\synthesizer\models\tacotron_tweaked\tacotron.py"", line 499, in load
    self.load_state_dict(checkpoint[""model_state""])
  File ""C:\Users\babud\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\module.py"", line 1498, in load_state_dict
    self.__class__.__name__, ""\n\t"".join(error_msgs)))
RuntimeError: Error(s) in loading state_dict for Tacotron:
        size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([66, 512]) from checkpoint, the shape in current model is torch.Size([194, 512]).`",sadly still error python false false seed none running test configuration found available design compute capability total memory synthesizer loaded trained step synthesizer device building trainable loading model testing configuration small testing testing synthesizer trainable recent call last file line module file line file line load file line load file line error loading size mismatch param shape shape current model,issue,negative,negative,negative,negative,negative,negative
1146776957,"> sound quality is very poor

yes, the model isn't trained that good yet, I'm working on it",sound quality poor yes model trained good yet working,issue,negative,positive,positive,positive,positive,positive
1146674546,"okay, readme updated, files updated, model trained, everything should work fine except for the audio quality, model still needs finetuning, so feel free to delete everything from the directory and clone from scratch ",model trained everything work fine except audio quality model still need feel free delete everything directory clone scratch,issue,positive,positive,positive,positive,positive,positive
1145350382,"The Encoder and Vocoder I believe are working but I am having issues with the synthesizer. I keep getting the error: ""Exception: Could not find any synthesizer weights under synthesizer\saved_models\default\taco_pretrained."" Any Ideas?",believe working synthesizer keep getting error exception could find synthesizer,issue,negative,neutral,neutral,neutral,neutral,neutral
1142709257,"So thankful for your replys. I got a problem installing version 1.21.0 but it seems it doesnt find it, i tried ""pip install numpy"" after the uninstall but i get the same problem :/

D:\Google Drive\EXOMENTUM\BAÚLES\INVENTARIO Y HERRAMIENTAS\Real-Time-Voice-Cloning-master>pip uninstall numpy
Found existing installation: numpy 1.19.3
Uninstalling numpy-1.19.3:
  Would remove:
    c:\users\rodri\appdata\local\programs\python\python36\lib\site-packages\numpy-1.19.3.dist-info\*
    c:\users\rodri\appdata\local\programs\python\python36\lib\site-packages\numpy\*
    c:\users\rodri\appdata\local\programs\python\python36\scripts\f2py.exe
Proceed (Y/n)? y
  Successfully uninstalled numpy-1.19.3

D:\Google Drive\EXOMENTUM\BAÚLES\INVENTARIO Y HERRAMIENTAS\Real-Time-Voice-Cloning-master>pip install numpy==1.21.0
ERROR: Could not find a version that satisfies the requirement numpy==1.21.0 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0rc1, 1.13.0rc2, 1.13.0, 1.13.1, 1.13.3, 1.14.0rc1, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0rc1, 1.15.0rc2, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0rc1, 1.16.0rc2, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0rc1, 1.17.0rc2, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0rc1, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0rc1, 1.19.0rc2, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5)
ERROR: No matching distribution found for numpy==1.21.0

D:\Google Drive\EXOMENTUM\BAÚLES\INVENTARIO Y HERRAMIENTAS\Real-Time-Voice-Cloning-master>pip install numpy
Collecting numpy
  Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB)
Installing collected packages: numpy
Successfully installed numpy-1.19.5

D:\Google Drive\EXOMENTUM\BAÚLES\INVENTARIO Y HERRAMIENTAS\Real-Time-Voice-Cloning-master>python demo_cli.py
Traceback (most recent call last):
  File ""demo_cli.py"", line 2, in <module>
    from utils.argutils import print_args
  File ""D:\Google Drive\EXOMENTUM\BAÚLES\INVENTARIO Y HERRAMIENTAS\Real-Time-Voice-Cloning-master\utils\argutils.py"", line 2, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
",thankful got problem version doesnt find tried pip install get problem pip found installation would remove proceed successfully uninstalled pip install error could find version requirement post error matching distribution found pip install collected successfully python recent call last file line module import file line module import module,issue,negative,positive,positive,positive,positive,positive
1142701872,"> pip install numpy

I tried but its not working 😢. I also did ""pip install matplotlib"" but it gave me the same reply as in ""Requirement already satisfied"". Any Ideas?

D:\Google Drive\EXOMENTUM\BAÚLES\INVENTARIO Y HERRAMIENTAS\Real-Time-Voice-Cloning-master>pip install numpy
Requirement already satisfied: numpy in c:\users\rodri\appdata\local\programs\python\python36\lib\site-packages (1.19.3)

D:\Google Drive\EXOMENTUM\BAÚLES\INVENTARIO Y HERRAMIENTAS\Real-Time-Voice-Cloning-master>python demo_cli.py
Traceback (most recent call last):
  File ""demo_cli.py"", line 2, in <module>
    from utils.argutils import print_args
  File ""D:\Google Drive\EXOMENTUM\BAÚLES\INVENTARIO Y HERRAMIENTAS\Real-Time-Voice-Cloning-master\utils\argutils.py"", line 2, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'

D:\Google Drive\EXOMENTUM\BAÚLES\INVENTARIO Y HERRAMIENTAS\Real-Time-Voice-Cloning-master>
",pip install tried working also pip install gave reply requirement already satisfied pip install requirement already satisfied python recent call last file line module import file line module import module,issue,positive,positive,positive,positive,positive,positive
1141042344,"> In line 109: convert it to a string.
> 
> ```
>    self.conv_project1 = BatchNormConv(len(self.bank_kernels) * channels, proj_channels[0], 3)
> ```
> 
> try converting it to (len(str
> 
> len() function cannot be called with an integer, try dir or range if it does not work.

no its not that, I'm working on the project now, I will reply in this issue when I'm done and did all the tests",line convert string try converting function integer try range work working project reply issue done,issue,negative,neutral,neutral,neutral,neutral,neutral
1140800364,"As implied in the first post, I did pip install -r requirements.txt, which installs scipy==1.7.3. That version causes this error. scipy 1.8.0+ is required. I eventually got past the second error but I don't remember what other dependencies I had to change. I eventually gave up on using AI for my project after several days.",first post pip install version error eventually got past second error remember change eventually gave ai project several day,issue,negative,neutral,neutral,neutral,neutral,neutral
1140782228,"Please post your installed packages of your ""VoiceCloning"" environment. Did you install the requirements.txt?

`pip install -r requirements.txt`",please post environment install pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
1139976577,"sadly **demo_cli.py** gives me this error:

```
PS C:\Users\babud\Downloads\RRUS> python .\demo_cli.py
Arguments:
    enc_model_fpath:   saved_models\default\encoder.pt
    syn_model_fpath:   saved_models\rusmodeltweaked\synthesizer.pt
    voc_model_fpath:   saved_models\default\vocoder.pt
    cpu:               False
    no_sound:          False
    seed:              None

Running a test of your configuration...

Found 1 GPUs available. Using GPU 0 (NVIDIA GeForce RTX 2080 with Max-Q Design) of compute capability 7.5 with 8.6Gb total memory.

Preparing the encoder, the synthesizer and the vocoder...
Loaded encoder ""encoder.pt"" trained to step 1564501
Synthesizer using device: cuda
Building Wave-RNN
Trainable Parameters: 4.481M
Loading model weights at saved_models\default\vocoder.pt
Testing your configuration with small inputs.
        Testing the encoder...
        Testing the synthesizer...
Trainable Parameters: 30.936M
Loaded synthesizer ""synthesizer.pt"" trained to step 68800

| Generating 1/1
Traceback (most recent call last):
  File "".\demo_cli.py"", line 95, in <module>
    mels = synthesizer.synthesize_spectrograms(texts, embeds)
  File ""C:\Users\babud\Downloads\RRUS\synthesizer\models\tacotron\inference.py"", line 109, in synthesize_spectrograms
    text_lens = [len(text) for text in batch]
  File ""C:\Users\babud\Downloads\RRUS\synthesizer\models\tacotron\inference.py"", line 109, in <listcomp>
    text_lens = [len(text) for text in batch]
TypeError: object of type 'int' has no len()
```",sadly error python false false seed none running test configuration found available design compute capability total memory synthesizer loaded trained step synthesizer device building trainable loading model testing configuration small testing testing synthesizer trainable loaded synthesizer trained step generating recent call last file line module file line text text batch file line text text batch object type,issue,negative,negative,negative,negative,negative,negative
1139962103,"![image](https://user-images.githubusercontent.com/36692361/170776712-8f0e2cc3-f47c-4d71-973d-e261cbfe0d8b.png)
Looks like here on synth is not a hyper link.  And demo_toolbox.py is still missing. ",image like hyper link still missing,issue,negative,negative,negative,negative,negative,negative
1139956667,"1.delete your repository and clone it again
2. only synthesizer matters for language 
3. the synthetizer I've provided might not have the best audio quality, it's a lot of trial and error to train a good model",repository clone synthesizer language synthetizer provided might best audio quality lot trial error train good model,issue,positive,positive,positive,positive,positive,positive
1139954780,"I've redownload zip file. Now GUI launch file is missing, no demo_toolbox.py file. 

I've tried demo_cli, but it shows same error about size i previously had in gui:

_PS C:\Users\babud\Downloads\Real-Time-Voice-Cloning-Multilang-master> python demo_cli.py
Arguments:
    enc_model_fpath:   saved_models\default\encoder.pt
    syn_model_fpath:   saved_models\rusmodeltweaked\synthesizer.pt
    voc_model_fpath:   saved_models\default\vocoder.pt
    cpu:               False
    no_sound:          False
    seed:              None

Running a test of your configuration...

Found 1 GPUs available. Using GPU 0 (NVIDIA GeForce RTX 2080 with Max-Q Design) of compute capability 7.5 with 8.6Gb total memory.

Preparing the encoder, the synthesizer and the vocoder...
Loaded encoder ""encoder.pt"" trained to step 1564501
Synthesizer using device: cuda
Building Wave-RNN
Trainable Parameters: 4.481M
Loading model weights at saved_models\default\vocoder.pt
Testing your configuration with small inputs.
        Testing the encoder...
        Testing the synthesizer...
Trainable Parameters: 30.936M
Traceback (most recent call last):
  File ""demo_cli.py"", line 95, in <module>
    mels = synthesizer.synthesize_spectrograms(texts, embeds)
  File ""C:\Users\babud\Downloads\Real-Time-Voice-Cloning-Multilang-master\synthesizer\models\tacotron\inference.py"", line 88, in synthesize_spectrograms
    self.load()
  File ""C:\Users\babud\Downloads\Real-Time-Voice-Cloning-Multilang-master\synthesizer\models\tacotron\inference.py"", line 65, in load
    self._model.load(self.model_fpath)
  File ""C:\Users\babud\Downloads\Real-Time-Voice-Cloning-Multilang-master\synthesizer\models\tacotron\tacotron.py"", line 506, in load
    self.load_state_dict(checkpoint[""model_state""])
  File ""C:\Users\babud\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\module.py"", line 1498, in load_state_dict
    self.__class__.__name__, ""\n\t"".join(error_msgs)))
RuntimeError: Error(s) in loading state_dict for Tacotron:
        **size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([66, 512]) from checkpoint, the shape in current model is torch.Size([194, 512]).**_

p.s. i was downloading pretrained models from readme (https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models) but aren't they from english-only release? Maybe you can upload russian one? Thanks!",zip file launch file missing file tried error size previously python false false seed none running test configuration found available design compute capability total memory synthesizer loaded trained step synthesizer device building trainable loading model testing configuration small testing testing synthesizer trainable recent call last file line module file line file line load file line load file line error loading size mismatch param shape shape current model release maybe one thanks,issue,negative,negative,neutral,neutral,negative,negative
1139481662,"by the way, the error you had about no such file or directory means you had to download a pretrained model, I will update Readme to add a link",way error file directory model update add link,issue,negative,neutral,neutral,neutral,neutral,neutral
1139478078,"okay l will look into it, but don't copy files from original repo, it doesn't work that way",look copy original work way,issue,negative,positive,positive,positive,positive,positive
1139469291,"Okay i managed to turn on toolbox by copying some files from original build, now when i add wav and try synth +vocode i get this error:

size mismatch for encoder.embeddingweight: copying a param with shape torch.5ize([66, 512]) from chequoint, the shape in current model is tord1.Size([194, 512]).",turn toolbox original build add try get error size mismatch param shape shape current model,issue,negative,positive,positive,positive,positive,positive
1138888349,"> see my fork https://github.com/neonsecret/Real-Time-Voice-Cloning-Multilang it is adjusted to train the bilingual ru+en model and is easily adjustable for adding new languages

Sir, that's exactly what i'm looking for. I wanna correct some wrong voiceover in old game, but since i can't get in touch with actor i want to simulate his voice. 

The subj tool works, but can't do russian voice https://youtu.be/lDbpoaaBJSo
Your fork gives me errors:

```
PS C:\Users\babud\Downloads\Real-Time-Voice-Cloning-Multilang-master> python demo_toolbox.py
Traceback (most recent call last):
  File ""demo_toolbox.py"", line 7, in <module>
    from utils.default_models import ensure_default_models
ModuleNotFoundError: No module named 'utils.default_models'

```

My knowledge on all these python stuff is low so i just copy paste commands, sometimes try to understand its errors, but this looks unsolvable with my level of knowledge. 

I want simple thing, launch GUI, point program to WAV files with actor voice, enter text and get voiceover files :)

I also tried python demo_cli.py, got lot's of stuff but in the end it was this:

`FileNotFoundError: [Errno 2] No such file or directory: 'saved_models\\rusmodeltweaked\\synthesizer.pt'`",see fork train bilingual model easily adjustable new sir exactly looking wan na correct wrong old game since ca get touch actor want simulate voice tool work ca voice fork python recent call last file line module import module knowledge python stuff low copy paste sometimes try understand unsolvable level knowledge want simple thing launch point program actor voice enter text get also tried python got lot stuff end file directory,issue,negative,negative,neutral,neutral,negative,negative
1138575797,"> > python demo_toolbox.py -d datasets
> 
> hoo...finally it's working...thanks a bunch

i did exaclty that and still get the error
",python finally working thanks bunch still get error,issue,negative,positive,neutral,neutral,positive,positive
1138247321,"A more advanced solution is to save the attention layer alignments from inference, stretch them by the desired slowdown amount, then run the decoder loop again replacing the attention network output with the stretched alignments. This has the effect of forcing the decoder to use more frames when synthesizing each letter of every word, slowing down the output. With similar techniques, it's also possible to slow down selected words, or speed up.",advanced solution save attention layer inference stretch desired slowdown amount run loop attention network output effect forcing use letter every word output similar also possible slow selected speed,issue,positive,positive,neutral,neutral,positive,positive
1130116555,"For anyone comes to this issue in the future: 
Somehow I guess the issue is that the length of some audio samples are too short. 
Please check the value of ```utterance_min_duration``` in ```synthesizer/hparams.py```. 
I adjusted the value, preprocessed the training data again, 
and so far have trained the vocoder for more than 1 epoch without getting this error, 
so I consider my issue is solved. ",anyone come issue future somehow guess issue length audio short please check value value training data far trained epoch without getting error consider issue,issue,positive,positive,neutral,neutral,positive,positive
1127403456,"Hi, you can save the wav by adding `save_wav(wav, filename)` before `return wav` inside the `infer_waveform` function in vocoder/inference.py
PS: make sure you define a variable with the filename",hi save return inside function make sure define variable,issue,positive,positive,positive,positive,positive,positive
1126877656,Fixed. Kernel configuration issue,fixed kernel configuration issue,issue,negative,positive,neutral,neutral,positive,positive
1120411340,@padmalcom please can you guide me how is adding Arabic language to [Real-Time-Voice-Cloning] program step by step,please guide language program step step,issue,negative,neutral,neutral,neutral,neutral,neutral
1120407870,@ghost please can you guide me how is adding Arabic language to [Real-Time-Voice-Cloning] program step by step,ghost please guide language program step step,issue,negative,neutral,neutral,neutral,neutral,neutral
1119446667,While providing seed option how do we know which value gives most correct output?,providing seed option know value correct output,issue,negative,neutral,neutral,neutral,neutral,neutral
1119414011,"Notice how the synthesized output is now identical in each case, with a length of 172800 timesteps. Some variation in inference speed is normal, and does not affect output.",notice output identical case length variation inference speed normal affect output,issue,negative,positive,positive,positive,positive,positive
1119382828,"After providing the seed option still Gen rate is varrying for same audio and text -
Synthesizing the waveform:
{| ████████████████ 171000/172800 | Batch Size: 18 | Gen Rate: 3.4kHz | }float64
Synthesizing the waveform:
{| ████████████████ 171000/172800 | Batch Size: 18 | Gen Rate: 4.6kHz | }float64
Synthesizing the waveform:
{| ████████████████ 171000/172800 | Batch Size: 18 | Gen Rate: 4.2kHz | }float64",providing seed option still gen rate audio text batch size gen rate float batch size gen rate float batch size gen rate float,issue,negative,neutral,neutral,neutral,neutral,neutral
1119300835,"It's possible this will be resolved with more training steps. You can also try restarting the training with a higher reduction factor to make it easier to learn attention. Once learned, the reduction factor can be decreased.",possible resolved training also try training higher reduction factor make easier learn attention learned reduction factor,issue,positive,positive,positive,positive,positive,positive
1119297370,"Provide an example? It may be the quality of your recordings or your microphone setup, like distance from speaker or recording environment.",provide example may quality microphone setup like distance speaker recording environment,issue,negative,neutral,neutral,neutral,neutral,neutral
1119296762,"Are you using the latest code? That error message pertains to checkpoints developed for an older version of this repo, which used tensorflow.",latest code error message older version used,issue,negative,positive,positive,positive,positive,positive
1119289017,"It's a command line argument for demo_cli.py and demo_toolbox.py. You also need to specify the value of the seed. For example:
```
python demo_cli.py --seed 0
python demo_toolbox.py --seed 0
```",command line argument also need specify value seed example python seed python seed,issue,negative,neutral,neutral,neutral,neutral,neutral
1119284891,"Can you tell me how we can use this --seed option ??

",tell use seed option,issue,negative,neutral,neutral,neutral,neutral,neutral
1119274796,"The output varies because dropout is used in inference, in the encoder and decoder prenets. Dropout causes some tensor elements to be zeroed out at random. Its purpose is to help the model generalize in training, but as the Tacotron authors explain, it is preserved for inference to introduce some variation in the output. For completely deterministic output, use the `--seed` option (it causes the random number generator to be initialized to the same state when generating each time).",output dropout used inference dropout tensor random purpose help model generalize training explain inference introduce variation output completely deterministic output use seed option random number generator state generating time,issue,negative,negative,negative,negative,negative,negative
1113282747,"@Abdelrahman-Shahda I think you should just train as normal, if your emotional audio has exclamation signs in transcript (like ""hello!"" or ""hello!!"") you should be fine. ",think train normal emotional audio exclamation transcript like hello hello fine,issue,positive,positive,positive,positive,positive,positive
1113275088,@neonsecret Okay great. For the emotion part should I keep extracting the embedding each time rather than once for a single user(I don't know if this will cause the encoder embeddings to vary based on the emotions),great emotion part keep time rather single user know cause vary based,issue,positive,positive,positive,positive,positive,positive
1113262475,"@Abdelrahman-Shahda 
no you should train only the synthetizer and edit the symbols.py file, see this https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/941",train synthetizer edit file see,issue,negative,neutral,neutral,neutral,neutral,neutral
1113251443,@CorentinJ I am planning to use your pre-trained modules to generate English audio but in my case I want my source audio to be Spanish so I should only worry about training the encoder right? And If I wanted to add emotions to the generated voice does the vocoder supports this?,use generate audio case want source audio worry training right add voice,issue,negative,positive,positive,positive,positive,positive
1111645643,"> Please try [my tutorial](https://github.com/raccoonML/Real-Time-Voice-Cloning/releases/tag/rtvc_upstream_2-3-22). Open an issue in my fork if anything is not working.

Wow it worked amazingly! Thank you so much!",please try tutorial open issue fork anything working wow worked amazingly thank much,issue,positive,positive,positive,positive,positive,positive
1109351009,"Hi, could you please share the literatures which mentions the definition of real-time generation? Many thanks!",hi could please share definition generation many thanks,issue,positive,positive,positive,positive,positive,positive
1105993186,Please try [my tutorial](https://github.com/raccoonML/Real-Time-Voice-Cloning/releases/tag/rtvc_upstream_2-3-22). Open an issue in my fork if anything is not working.,please try tutorial open issue fork anything working,issue,negative,neutral,neutral,neutral,neutral,neutral
1102539957,Cannot check it now. I guess previous comment can be considered as resolution.,check guess previous comment considered resolution,issue,negative,negative,negative,negative,negative,negative
1100036701,"see my fork
https://github.com/neonsecret/Real-Time-Voice-Cloning-Multilang
it is adjusted to train the bilingual ru+en model and is easily adjustable for adding new languages ",see fork train bilingual model easily adjustable new,issue,negative,positive,positive,positive,positive,positive
1100036016," I've made a custom fork https://github.com/neonsecret/Real-Time-Voice-Cloning-Multilang
It now supports training a bilingual en+ru model, and it's easy to add new languages based on my fork",made custom fork training bilingual model easy add new based fork,issue,negative,positive,positive,positive,positive,positive
1096401908,"> Which speech dataset are you using? You should be using LibriSpeech or LibriTTS if you want to compare results to the pretrained models of this repo.

I don't use LibriSpeech or LibriTTS yet. The dataset used is AISHELL-3, based on mandarin.  I think it might not be the reason, because attention line should occur after 200k steps training normally.",speech want compare use yet used based mandarin think might reason attention line occur training normally,issue,negative,positive,positive,positive,positive,positive
1094335745,"don't know myself since I'm still learning the basics of python,  but I'll
have to learn both wxpython and py-qt so I could somehow simulate what
wxpython has into qt

On Sun, 10 Apr 2022 at 15:38, raccoonML ***@***.***> wrote:

> Is there a way to do this with PyQT so we don't need to rewrite the
> interface?
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/1046#issuecomment-1094288379>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AT2FKJSLWOCI6I4CDMSRCZ3VELRXFANCNFSM5SE5QBAA>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>
",know since still learning python learn could somehow simulate sun wrote way need rewrite interface reply directly view id,issue,negative,positive,neutral,neutral,positive,positive
1094288379,Is there a way to do this with PyQT so we don't need to rewrite the interface?,way need rewrite interface,issue,negative,neutral,neutral,neutral,neutral,neutral
1094285392,"it will be accessible if the text of the buttons, lists, combo boxes, and
all the other items is labeled, if it isn't labeled internally, it'll just
say button, combo box, etc without any kind of info as to what it contains
etc.

On Sun, 10 Apr 2022 at 09:21, raccoonML ***@***.***> wrote:

> Does the developer need to do anything special with wxPython to provide
> that accessibility info to the screen reader? Another way of stating the
> question is, if a wxPython interface is constructed by someone with
> absolutely no knowledge of this issue, will you get enough info from the
> screen reader?
>
> We can assume that the dev is thoughtful enough to use text labels instead
> of images. Are there any other considerations for making an accessible UI?
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/1046#issuecomment-1094216544>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AT2FKJSCKOVQOBZJYI7OAFDVEKFRJANCNFSM5SE5QBAA>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>
",accessible text button internally say button box without kind sun wrote developer need anything special provide accessibility screen reader another way question interface someone absolutely knowledge issue get enough screen reader assume dev thoughtful enough use text instead making accessible reply directly view id,issue,negative,positive,positive,positive,positive,positive
1094216544,"Does the developer need to do anything special with wxPython to provide that accessibility info to the screen reader? Another way of stating the question is, if a wxPython interface is constructed by someone with absolutely no knowledge of this issue, will you get enough info from the screen reader?

We can assume that the dev is thoughtful enough to use text labels instead of images. Are there any other considerations for making an accessible UI?",developer need anything special provide accessibility screen reader another way question interface someone absolutely knowledge issue get enough screen reader assume dev thoughtful enough use text instead making accessible,issue,positive,positive,positive,positive,positive,positive
1094200103,"ok here goes nothing!
So a screen reader, doesn't rely on ai to read the screen, it relies on the
accessibility information provided by apps and then reads it. So for
example, qt doesn't get along with screen readers very well it seams, not
by default, and here's my reason why I suggested wxpython.
wxpython, is a gui library which uses the default gui framework of the
system(windows form on windows) which provides all the accessibility
information the screen reader needs, it's the most accessible gui library
in python. Hope that was useful

On Sat, 9 Apr 2022 at 20:37, raccoonML ***@***.***> wrote:

> I don't intend to work on this issue, but I suggest that you come up with
> detailed requirements to help a developer who is interested in solving this
> problem. What features do you need? Can you explain how a screen reader
> works, and what features in the UI work well with it? You need to help us
> understand why it is beneficial to use wxPython.
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/1046#issuecomment-1094112972>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AT2FKJS4JMDMJOUDPG2KVM3VEHL7JANCNFSM5SE5QBAA>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>
",go nothing screen reader rely ai read screen accessibility information provided example get along screen well default reason library default framework system form accessibility information screen reader need accessible library python hope useful sat wrote intend work issue suggest come detailed help developer interested problem need explain screen reader work work well need help u understand beneficial use reply directly view id,issue,positive,positive,positive,positive,positive,positive
1094112972,"I don't intend to work on this issue, but I suggest that you come up with detailed requirements to help a developer who is interested in solving this problem. What features do you need? Can you explain how a screen reader works, and what features in the UI work well with it? You need to help us understand why it is beneficial to use wxPython.",intend work issue suggest come detailed help developer interested problem need explain screen reader work work well need help u understand beneficial use,issue,positive,positive,positive,positive,positive,positive
1094096024,"thanks but I can't use this thing properly, it doesn't seam to do what I
wanted it to do, what's more, I want to costomize the parameters, as well
as exporting the models which aren't doable in this one.

On Sat, 9 Apr 2022 at 16:59, raccoonML ***@***.***> wrote:

> The major benefit of the toolbox is the audio visualizations, in the form
> of speaker embeds and spectrograms. If you don't need images, a very basic
> interface could suffice. Maybe this one works for you.
> https://huggingface.co/spaces/akhaliq/Real-Time-Voice-Cloning
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/1046#issuecomment-1094074214>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AT2FKJRLY5X6HRROGZZVFRTVEGSOZANCNFSM5SE5QBAA>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>
",thanks ca use thing properly seam want well doable one sat wrote major benefit toolbox audio form speaker need basic interface could suffice maybe one work reply directly view id,issue,positive,positive,neutral,neutral,positive,positive
1094074214,"The major benefit of the toolbox is the audio visualizations, in the form of speaker embeds and spectrograms. If you don't need images, a very basic interface could suffice. Maybe this one works for you. https://huggingface.co/spaces/akhaliq/Real-Time-Voice-Cloning",major benefit toolbox audio form speaker need basic interface could suffice maybe one work,issue,negative,positive,neutral,neutral,positive,positive
1081370384,"I solved using this:
1)
sudo apt install pyqt5-dev-tools pyqt5-dev

https://stackoverflow.com/questions/57512730/cannot-install-pyqt5-tools-could-not-find-a-version-that-satisfies-the-requir

2) 
pip install pyqt5-sip==12.9.0

let me know, I got the same issue but its just module problems",apt install pip install let know got issue module,issue,negative,positive,positive,positive,positive,positive
1080970241,"Thank you very much @ireneb612 . :)

I cloned the repo and my current directory structure is like this:

```
encoder
samples
synthesizer
toolbox
vocoder
synthesizer_preprocess_audio.py
synthesizer_preprocess_embeds.py
synthesizer_train.py
etc.
```

Issue 437 mentions the following directions for training:

```
Here is a [preprocessed p240 dataset](https://www.dropbox.com/s/qskoopjcdjdwuvw/dataset_p240.zip?dl=0) if you would like to repeat this experiment. The embeds for utterances 002-380 are overwritten with the one for 001, as the hardcoding makes for a more consistent result. Use the audio file p240_001.flac to generate embeddings for inference. The audios are not included to keep the file size down, so if you care to do vocoder training you will need to get and preprocess VCTK.

Directions:

    Copy the folder synthesizer/saved_models/logs-pretrained to logs-vctkp240 in the same location. This will make a copy of your pretrained model to be finetuned.
    Unzip the dataset files to datasets_p240 in your Real-Time-Voice-Cloning folder (or somewhere else if you desire)
    Train the model: python synthesizer_train.py vctkp240 dataset_p240/SV2TTS/synthesizer --checkpoint_interval 100
    Let it run for 200 to 400 iterations, then stop the program.
        This should complete in a reasonable amount of time even on CPU.
        You can safely stop and resume training at any time though you will lose all progress since the last checkpoint
    Test the finetuned model in the toolbox using dataset_p240/p240_001.flac to generate the embedding
```

but the link no longer works so I couldn't figure out the proper format for the files. Could you please help me with that?

Thanks again. :)",thank much current directory structure like synthesizer toolbox issue following training would like repeat experiment one consistent result use audio file generate inference included keep file size care training need get copy folder location make copy model folder somewhere else desire train model python let run stop program complete reasonable amount time even safely stop resume training time though lose progress since last test model toolbox generate link longer work could figure proper format could please help thanks,issue,positive,positive,positive,positive,positive,positive
1080809664,"The thing that I would do is to use the pretrained models that work good for english and then finetune on 12 minutes of your voice! You just have to put the data in the right ormat and run the synthesizer_preprocess_audio the syhtnesizer_preprocess_embeds and the synthesizer_train.

I personally used the repository with the older direcotry set up for the saved models, but it's not a big difference, just the path to the saved models now are all in the same directory.

",thing would use work good voice put data right run personally used repository older set saved big difference path saved directory,issue,positive,positive,positive,positive,positive,positive
1079906300,"Hi @raccoonML and @ireneb612 you guys also seem to know how to train a model yourself. Could you help me here?

Thanks. :)",hi also seem know train model could help thanks,issue,positive,positive,positive,positive,positive,positive
1079903720,"Hi @sveneschlbeck Would it be possible for you to guide me here?

Thanks. :)",hi would possible guide thanks,issue,negative,positive,neutral,neutral,positive,positive
1076138869,"the first 0.5 second go's good, than it rapidly falters into hisisng and static... using demo-cli.. I dont beleive its properly training .pth model when it ask to choose audio file ",first second go good rapidly static dont properly training model ask choose audio file,issue,negative,positive,positive,positive,positive,positive
1075872124,"I have an [open issue](https://github.com/sveneschlbeck/Multi-Language-RTVC/issues/20) to integrate hifigan with the MLRTVC fork. It's on hold for reasons: 1) lackluster results with the hifigan pretrained models ([here](https://github.com/sveneschlbeck/Multi-Language-RTVC/issues/20#issuecomment-1059116700)); 2) more cleanup required before I'd feel comfortable releasing the code, and 3) my supporters have me working on voice conversion instead of TTS.

If this is something you want to try, I would suggest integrating Nvidia tacotron2 with hifigan since those repos use the same mel scaling. Another possibility is to train a hifigan model on RTVC mels like [this example](https://github.com/raccoonML/hifigan-demo/releases/tag/MLRTVC-v1).

Edit: Add to list of reasons: 4) perceived lack of interest in continued development of RTVC or MLRTVC.",open issue integrate fork hold lackluster cleanup feel comfortable code working voice conversion instead something want try would suggest since use mel scaling another possibility train model like example edit add list lack interest continued development,issue,positive,positive,positive,positive,positive,positive
1075831785,"@raccoonML sorry for not being specific, i was referring to the use of hifi-gan with this repository.

were you able to implement it? or did you manage to implement some other vocoder?",sorry specific use repository able implement manage implement,issue,negative,neutral,neutral,neutral,neutral,neutral
1075818696,"@manuel3265 Be more specific? If you're referring to [this](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/1035#issuecomment-1069733604), it's not something I can help with since problems are often particular to the dataset used. For those types of problems, switch to LibriSpeech or LibriTTS train-clean-100/360 and see if it goes away. Since those datasets are known to work, it will help you determine whether it's a problem of your code or the training data.",specific something help since often particular used switch see go away since known work help determine whether problem code training data,issue,negative,positive,neutral,neutral,positive,positive
1075760783,"Hello @raccoonML , sorry to bother you, I wanted to know if you found a solution for this. I hope you can answer me, thank you.",hello sorry bother know found solution hope answer thank,issue,positive,negative,negative,negative,negative,negative
1074664933,You can install them [here](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models). This link is in the (Optional) installation procedures in the project's README.md.,install link optional installation project,issue,negative,neutral,neutral,neutral,neutral,neutral
1074664230,"You will need to download the encoder.pt, synthesizer.pt, and vocoder.pt files [here](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models).

Make sure that the **paths** are correct. If you put your encoder model at ""saved_models/default/encoder.pt"", you need to change the path to that instead of encoder/encoder.pt.",need make sure correct put model need change path instead,issue,negative,positive,positive,positive,positive,positive
1069733604,"@raccoonML 
I retrained synthesizer after replacing new audio.py and after ~100k steps (~35 epochs) result is better but it is still noisy ([this file](https://drive.google.com/file/d/1E2M1BMFTdl93O31ZMvT9HDkOeL4R0i63/view?usp=sharing) is a sample result) and loss is about 0.64 and does not seem to change very much after this.
What do you think is the problem?",synthesizer new result better still noisy file sample result loss seem change much think problem,issue,negative,positive,positive,positive,positive,positive
1067433267,"> see here, this may help. It should be configured to use GPU #1037

Ty",see may help use,issue,negative,neutral,neutral,neutral,neutral,neutral
1065083248,"No updates to hparams.py are required. With the new audio.py, some settings no longer have an effect, like `min_level_db` and `ref_level_db`.",new longer effect like,issue,negative,positive,positive,positive,positive,positive
1063940362,"Yes! it's webrtcvad... I'm also having the same error
And I think I found a solution for this [here](https://www.reddit.com/r/learnpython/comments/ltc3kz/cant_install_webrtcvad/)

Simply put . . . I used `pip install webrtcvad-wheels` to install it",yes also error think found solution simply put used pip install install,issue,negative,neutral,neutral,neutral,neutral,neutral
1063926070,"> This is not an easy undertaking so before you start, make sure you satisfy the prerequisites. You must be able to answer ""yes"" to all questions below:
> 
> * Does your computer have a NVIDIA GPU?
> * Do you have coding experience?
> * Are you willing to devote at least 20 hours to the task?
> 
> I have not gone through the process myself, but I'll try to outline it since we don't have a good explanation. What you need to do is to fine-tune the pretrained synthesizer and vocoder models on a suitable dataset.
> 
> 1. Find a suitable dataset. Freely available resources include [AccentDB](https://accentdb.org/) (Indian accent) and [VCTK](https://datashare.is.ed.ac.uk/handle/10283/3443) (other English accents). For best results on your own voice, record your own dataset though this will take many hours.
> 2. Follow the steps in [README.md](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/README.md) to enable GPU support.
> 3. Go to the [training wiki page](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training) and follow the steps for the synthesizer and vocoder training on the LibriSpeech dataset.
>    
>    * Review the preprocessing code and understand what it is doing.
>    * Understand the format of the files in the <datasets_root>/SV2TTS folder
> 4. Preprocess your dataset from step 1 to generate training data for the synthesizer.
>    
>    * At a minimum, this requires editing the preprocessing scripts.
>    * You will likely need to write your own code to process the data into a suitable format for the toolbox.
>    * **We do not have a tutorial for this. You are on your own here!**
> 5. Continue training the pretrained synthesizer model on your dataset until it has converged.
> 6. Using your new synthesizer model, preprocess your dataset to generate training data for the vocoder.
> 7. Continue training the pretrained vocoder model on your dataset until the output is satisfactory.
> 
> With luck, your trained models will now generalize to your voice and impart the desired accent. **There are no guarantees this will work.**
> 
> If you succeed, please share your models and I will add them to the list in #400.

Is anyone got result for training indian assent? please let me know",easy undertaking start make sure satisfy must able answer yes computer experience willing devote least task gone process try outline since good explanation need synthesizer suitable find suitable freely available include accent best voice record though take many follow enable support go training page follow synthesizer training review code understand understand format folder step generate training data synthesizer minimum likely need write code process data suitable format toolbox tutorial continue training synthesizer model new synthesizer model generate training data continue training model output satisfactory luck trained generalize voice impart desired accent work succeed please share add list anyone got result training assent please let know,issue,positive,positive,positive,positive,positive,positive
1063905835,"> Your synthesizer is predicting spectrograms with a different scaling than the hifigan model expects. To fix this, you will need to retrain your model with properly scaled data. Replace your synthesizer/audio.py with [this file](https://github.com/raccoonML/Multi-Language-RTVC/blob/hifigan/mlrtvc/src/core/synthesizer/audio.py) and preprocess data again.

To retrain synthesizer I should just replace new audio.py file? It's not needed any changes in [hparams.py](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/synthesizer/hparams.py) or any other configuration in this project? ",synthesizer different scaling model fix need retrain model properly scaled data replace file data retrain synthesizer replace new file configuration project,issue,negative,positive,neutral,neutral,positive,positive
1063719201,"> Is it possible to train hifi-gan with this parameters instead of retraining synthesizer?

Yes. Here's a pretrained model for testing: https://github.com/raccoonML/hifigan-demo/releases/tag/MLRTVC-v1
No training code provided, unfortunately.

> I mean that if I retrain synthesizer with new parameters, then which vocoders can I use (except hifi-gan that you say)?

Waveglow should work.",possible train instead synthesizer yes model testing training code provided unfortunately mean retrain synthesizer new use except say work,issue,negative,negative,neutral,neutral,negative,negative
1062934102,"I haven't decided whether I should make a web UI yet, I probably will make one with Angular but I'm open to suggestions.",decided whether make web yet probably make one angular open,issue,negative,neutral,neutral,neutral,neutral,neutral
1062814542,"Thanks for your quick answer.
Which parameter should change? Is it possible to train hifi-gan with this parameters instead of retraining synthesizer?

Another question is that is there any standard parameters for this task? I mean that if I retrain synthesizer with new parameters, then which vocoders can I use (except hifi-gan that you say)? I should train synthesizer for each vocoder specifically ?",thanks quick answer parameter change possible train instead synthesizer another question standard task mean retrain synthesizer new use except say train synthesizer specifically,issue,negative,positive,neutral,neutral,positive,positive
1062669427,"Your synthesizer is predicting spectrograms with a different scaling than the hifigan model expects. To fix this, you will need to retrain your model with properly scaled data. Replace your synthesizer/audio.py with [this file](https://github.com/raccoonML/Multi-Language-RTVC/blob/hifigan/mlrtvc/src/core/synthesizer/audio.py) and preprocess data again.",synthesizer different scaling model fix need retrain model properly scaled data replace file data,issue,negative,neutral,neutral,neutral,neutral,neutral
1062636198,"@Umisyus Interesting, any progress? are you making a  web ui or something to interact with it?",interesting progress making web something interact,issue,positive,positive,positive,positive,positive,positive
1062629521,"I figured out what I should do to fix this error:
`x = x.unsqueeze(0)`
But the synthesized speech is very noisy! you can listen to result [here](https://drive.google.com/file/d/124ANFwzpOthto-5wCsYHXyraG8Ao8tZS/view?usp=sharing).
Thank you if anyone has experience using HiFi-GAN and say how to run it with the output obtained from the current project.",figured fix error speech noisy listen result thank anyone experience say run output current project,issue,negative,neutral,neutral,neutral,neutral,neutral
1057897334,"@raccoonML  ok so I think I had some issues with the encoding of my txt file, Nayway I selected the basic_cleaners that do not have that function! Thank you a lot :)",think file selected function thank lot,issue,negative,neutral,neutral,neutral,neutral,neutral
1057895245,"The training data was the training set from LibriSpeech, VoxCeleb1 Dev A - D and VoxCeleb2, resulting into 3201 hours of data with 8371 different speakers.",training data training set dev resulting data different,issue,negative,neutral,neutral,neutral,neutral,neutral
1057713141,Facing this same issue. Very frustrating.  Any help is hugely appreciated!,facing issue help hugely,issue,negative,positive,positive,positive,positive,positive
1057042873,"In preprocess, the text is copied verbatim from the transcripts to the train.txt file. Check your transcripts and make sure they contain accented letters. The text processing functions are applied during training. In hparams, there is a line for selecting a text cleaner ([tts_cleaner_names](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/0713f860a3dd41afb56e83cff84dbdf589d5e11a/synthesizer/hparams.py#L45)). The selected [text cleaner](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/synthesizer/utils/cleaners.py) should not include `convert_to_ascii()` as that will remove accents from the letters.",text copied verbatim file check make sure contain text applied training line text cleaner selected text cleaner include remove,issue,positive,positive,positive,positive,positive,positive
1055525500,Yes 20 minutes is nt enough! you can use this much data if you fine tune on pre trained weights!,yes enough use much data fine tune trained,issue,positive,positive,positive,positive,positive,positive
1050171147,"Hi @ireneb612! Yes, actually the code itself has a script for preprocessing. I found out with help of the community that the mozilla's commonvoice dataset was the best because of the variety of accents for the language. Yes, I specified the characters in symbols and cleaners as was mentioned in the different issues (and I think in the guide too). Here is the repo with the resulting code of my work: https://github.com/AlexSteveChungAlvarez/Real-Time-Voice-Cloning-Spanish , it includes the script to prepare the dataset too!",hi yes actually code script found help community best variety language yes different think guide resulting code work script prepare,issue,positive,positive,positive,positive,positive,positive
1049687309,"@AlexSteveChungAlvarez To train the synthetizer, did you pre process the dataset? Did you get the right accents fro the spanish language?? Did you specify different characters in symbols and cleaners? Thank you!",train synthetizer process get right fro language specify different thank,issue,negative,positive,positive,positive,positive,positive
1048791887,"@ireneb612 How do I do that? I only downloaded the synthesizer pt file from online since it was missing, I don't know how to train it.",synthesizer file since missing know train,issue,negative,negative,negative,negative,negative,negative
1048609414,"@raccoonML Sorry for bothering, When retraining the synthetizer I also need to re do the preprocessing of the dataset with the new symbols right? Because I retrained on and older pre processed dataset and it is not pronouncing the new letters :( ",sorry synthetizer also need new right older new,issue,negative,positive,neutral,neutral,positive,positive
1048601639,"@LinkleZe probably the synthetiser has not learnt the attention, check the attention plots.",probably learnt attention check attention,issue,negative,neutral,neutral,neutral,neutral,neutral
1047243798,"@AntonFirc Did you make any progress? I am specifically interested in Czech language, is there a way to train a usable model?",make progress specifically interested language way train usable model,issue,positive,positive,positive,positive,positive,positive
1046611122,@raccoonML Thank you! yes one idea was to then translate! :) ,thank yes one idea translate,issue,positive,neutral,neutral,neutral,neutral,neutral
1046308142,Here is an idea for how to deal with numbers in other languages. Code not available at this time. https://github.com/sveneschlbeck/Multi-Language-RTVC/issues/13#issuecomment-1001240560,idea deal code available time,issue,negative,positive,positive,positive,positive,positive
1046306112,"@raccoonML Hi! Yes I started retraining the synthétiser, I was wondering how you felt with NUMBERS because I know there are libraries that convert digits to English numbers but not other languages! How did you deal? Thank you",hi yes wondering felt know convert deal thank,issue,positive,neutral,neutral,neutral,neutral,neutral
1046066409,"Place the model files in this location:
```
File location                         Filesize
-----------------------------------------------
saved_models/default/encoder.pt         17 MB
saved_models/default/synthesizer.pt    370 MB
saved_models/default/vocoder.pt         53 MB
```

If you continue having trouble, you can try my RTVC_Windows.zip file that contains the repo's code, model files, and a standalone FFmpeg.exe in a ready-to-go package.
https://github.com/raccoonML/Real-Time-Voice-Cloning/releases/tag/rtvc_upstream_2-3-22",place model location file location continue trouble try file code model package,issue,negative,negative,negative,negative,negative,negative
1046028147,"Is Spanish or Latino? because i've tried and not sounds like Spanish of Spain, I think that is Español from Latinoamerica
Anyone knows?",tried like think anyone,issue,negative,neutral,neutral,neutral,neutral,neutral
1045961217,"The vocoder is by far the slowest of the models. The one used in this repo (wavernn) has been surpassed by recent vocoders that are 2-3 orders of magnitude faster with better quality. If you need faster inference, swap out the vocoder model. However, it's much easier said than done and I lack the time to write detailed instructions on how to do it.

You can study the code of similar projects like [Mockingbird](https://github.com/babysor/Mockingbird), which has the option to use a hifi-gan vocoder. I also have an open task to integrate the same hifi-gan vocoder into a similar voice cloning TTS project. For updates, watch this issue. https://github.com/sveneschlbeck/Multi-Language-RTVC/issues/20",far one used recent magnitude faster better quality need faster inference swap model however much easier said done lack time write detailed study code similar like mockingbird option use also open task integrate similar voice project watch issue,issue,positive,positive,positive,positive,positive,positive
1045460971,"The Swedish pretrained synthesizer model is not my work, but it is trained from scratch and uses the English speaker encoder as ViktorAlm explains in #257 . I made these updates to symbols.py: https://github.com/raccoonML/Real-Time-Voice-Cloning/commit/b358eb5d8f64a00cfee32a0e46ce4122355f7d41

Updating the symbol list will require training a new synthesizer model from scratch.",synthesizer model work trained scratch speaker made symbol list require training new synthesizer model scratch,issue,negative,positive,positive,positive,positive,positive
1044123759,"@raccoonML I was wondering if you changed the symbols.py for swedish! Because I have to change che _characters  for italian but it gives me error:

size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([66, 512]) from checkpoint, the shape in current model is torch.Size([74, 512])

Wich models did you start off? did you train synthesizer from scratch?",wondering change che error size mismatch param shape shape current model start train synthesizer scratch,issue,negative,neutral,neutral,neutral,neutral,neutral
1040201317,"yaa, m to facing same argument problem
![image](https://user-images.githubusercontent.com/58507710/154059995-dedeea31-1fec-4b8b-96c6-6d1fa0ce1710.png)
",facing argument problem image,issue,negative,neutral,neutral,neutral,neutral,neutral
1038764188,"> Hi @blue-fish , no I coudn't find progress on this one. I tried fine-tuning https://github.com/Kyubyong/dc_tts instead, which gave clearer pronunciation of hindi words. Edit - I tried fine-tuning https://github.com/Kyubyong/dc_tts on Source 1 i.e. https://cvit.iiit.ac.in/research/projects/cvit-projects/text-to-speech-dataset-for-indian-languages

Can you share your work?
",hi find progress one tried instead gave clearer pronunciation edit tried source share work,issue,positive,neutral,neutral,neutral,neutral,neutral
1036978937,"Hello, I am currently working on doing that as a personal project. I look forward to getting into AI myself and I think that this would be a nice start.",hello currently working personal project look forward getting ai think would nice start,issue,negative,positive,positive,positive,positive,positive
1032274933,Is it possible to get a phoneme printed output live? (even inaccurate),possible get phoneme printed output live even inaccurate,issue,negative,positive,neutral,neutral,positive,positive
1028818806,"I have uploaded model files and a complete working setup to Github. See this page for download links.
https://github.com/raccoonML/Real-Time-Voice-Cloning/releases/tag/rtvc_upstream_2-3-22",model complete working setup see page link,issue,negative,positive,neutral,neutral,positive,positive
1028611196,So I would like to ask you guys to upload this package on GitHub,would like ask package,issue,negative,neutral,neutral,neutral,neutral,neutral
1026520529,"This is a speech synthesis (voice cloning) repo, for voice conversion find open source projects here:
https://paperswithcode.com/task/voice-conversion",speech synthesis voice voice conversion find open source,issue,negative,neutral,neutral,neutral,neutral,neutral
1024991332,"> I solved by installing `sudo apt-get install libportaudio2` on Ubuntu `18.04`

Thank you, also work's in Ubuntu 20.04",install thank also work,issue,negative,neutral,neutral,neutral,neutral,neutral
1024898228,"Oh sorry, I realized I didn't read the last part lol. Closing the issue for now, I guess Microsoft Visual C++ 14.0 will solve it for me.",oh sorry read last part issue guess visual solve,issue,negative,negative,negative,negative,negative,negative
1023666843,"@Bebaam Thanks I still wonder why files scrapped from YT are not accepted by the model and an attention is not achieved. Btw, as I struggled from the start with createin a proper path of files structure, I made a complete .py code that creates the whole structure from scratch + deals with some problems I encoutered while training model (e.g. checking if number of wav and txt files are equal, converting mp3 to wav, cutting files, scrapping videos from YT and converting to wav, using Google Speach basic API to process speech to text (as this step wasn't working for me in the original code), etc.). The only this there is to add the autor's code in the main folder. I will send a link here in a day or two, maybe someone finds that useful.

EDIT:

Here it is :)

https://github.com/gabrielrdw20/Real-Time-Voice-Cloning-Polish/tree/main/start_here",thanks still wonder scrapped accepted model attention start proper path structure made complete code whole structure scratch training model number equal converting cutting scrapping converting basic process speech text step working original code add code main folder send link day two maybe someone useful edit,issue,positive,positive,neutral,neutral,positive,positive
1022182173,"I don't understand a word, but it sounds good :smile: ",understand word good smile,issue,positive,positive,positive,positive,positive,positive
1022180185,"Yeah sometimes you never know where the error comes from, so I am glad to hear that it works now, attention looks good now in my opinion :)",yeah sometimes never know error come glad hear work attention good opinion,issue,positive,positive,positive,positive,positive,positive
1022176345,check if the embedding size of the encoder does fit to the size of your model in encoder/params_model.py,check size fit size model,issue,negative,positive,positive,positive,positive,positive
1019558878,"You're thinking of voice conversion, which transforms the voice in a given audio clip. Voice cloning means that you can reproduce the voice in general, i.e. on any text input. 

Faster than real time means that the audio generated is longer than the time it took to generate, which is generally true for this repo under some mild assumptions. What you're thinking of is online vs. offline, where an online version of TTS is able to playback speech as the remainder of the audio is still generating. Inference is fully offline for this repo, but could be made online with some development work.",thinking voice conversion voice given audio clip voice reproduce voice general text input faster real time audio longer time took generate generally true mild thinking version able playback speech remainder audio still generating inference fully could made development work,issue,negative,positive,positive,positive,positive,positive
1019254887,"For anyone who also encounters this problem:
I solved the Problem with installing the C++ Compiler. Her is how to do it on Ubuntu:
Install G++ with this command:
`sudo apt install g++`
Then install the Build essentails:
`sudo apt install build-essential`

I didn' t try it on Windows but it should work:
https://techdecodetutorials.com/c/how-to-install-c-compiler-in-windows-10/",anyone also problem problem compiler install command apt install install build apt install try work,issue,negative,positive,positive,positive,positive,positive
1019122549,"**Update**

@Bebaam you were right, attention showed up before synthesizer's 10k iterations. What I don't understand is the audio file selection issue. I was confident that my samples were of good quality and long enough. Nevertheless, I didn't listen to them one by one and there are probably too long pauses somewhere. The thread can be closed :)

![attention_step_29500_sample_1](https://user-images.githubusercontent.com/61107405/150632569-59513d74-e907-4f1d-acf9-15d6a41c9695.png)

A new audio sample is in ""new outcome"" folder:
https://drive.google.com/drive/folders/1-SKYHRP8zy7vETqtMMJpKv1n7XKidBZL?usp=sharing
",update right attention synthesizer understand audio file selection issue confident good quality long enough nevertheless listen one one probably long somewhere thread closed new audio sample new outcome folder,issue,positive,positive,positive,positive,positive,positive
1017069255,"> @Rainer2465 Mudei as saídas/passo para quatro (r=4 ~91 ) um pouco tem resultados decentes por enquanto. Mas com r=2 os resultados permanecem os mesmos (42 ~ 43 passos /s).

@MGSousa @Rainer2465 vocês possuem algum modelo treinado hoje? podemos conversar?",rainer para um ma o o rainer algum,issue,negative,neutral,neutral,neutral,neutral,neutral
1017065135,"> @fdaherbarros teve algum progresso? se quiser posso ajudar com o processo

@badjano você tem um modelo treinando em português? ",algum se um em,issue,negative,neutral,neutral,neutral,neutral,neutral
1016763764,"Hi @Bebaam, thanks for your support. I've check the recommended solutions. From the start, the  synthesizer file has implemented softmax instead of sigmoid function. I wrote a script checking if a wav file is not too much noisy to be included into adataset and I only selected the ones that are ok. That's the reason I'm a bit surprised that the syntesizer cannot learn attention from a properly trained encoder. I will try to use 2 different data and do it from scratch once again.",hi thanks support check start synthesizer file instead sigmoid function wrote script file much noisy included selected reason bit learn attention properly trained try use different data scratch,issue,positive,positive,neutral,neutral,positive,positive
1016511085,"Ok. Sometimes when attention is not achieved, a complete retraining of synthesizer could be worth it.

Furthermore, did you use the dataformat as in #437. Not sure whether it is neccessary, but was recommended. Maybe encoder needs to be retrained then, too.

Otherwise, maybe this could help: https://github.com/fatchord/WaveRNN/issues/154#issuecomment-567851857
The first one is implemented here afaik, the second one - starting with a higher reduction factor - could be helpful and could make training faster.

I would also monitor used VRAM and increase the batch_size if possible.",sometimes attention complete synthesizer could worth furthermore use sure whether maybe need otherwise maybe could help first one second one starting higher reduction factor could helpful could make training faster would also monitor used increase possible,issue,positive,positive,positive,positive,positive,positive
1016478042,"> Which batchsize did you use for training the synthesizer?

I use the one implemented originally by the code author. Nothing was changed here. 

![batch](https://user-images.githubusercontent.com/61107405/150142279-ecb9affc-ecfc-432e-8cdd-6f4b04e19c33.PNG)

",use training synthesizer use one originally code author nothing batch,issue,negative,positive,positive,positive,positive,positive
1016474571,"@Bebaam I apologize, I confused speakers number with embedding. In the file encoder > params_model.py it is stated:

**Model parameters**
model_hidden_size = 256
model_embedding_size = 256
model_num_layers = 3

**Training parameters**
learning_rate_init = 1e-4
speakers_per_batch = 64
utterances_per_speaker = 10

so nothing was changed here. Attention is still not properly generated and I have no idea if I should change some parameters or just wait to make more that a 100k for both the synthesizer and  vocoder.
",apologize confused number file stated model training nothing attention still properly idea change wait make synthesizer,issue,negative,negative,negative,negative,negative,negative
1016469763,"With your parameters (if you did not change anything else significantly) in my experience you should see the attention plot after a few thousand steps (5-25k). Which batchsize did you use for training the synthesizer? I would use all the VRAM you have, so the higher the batch_size, the faster the training should be (again, at least in my experience).",change anything else significantly experience see attention plot thousand use training synthesizer would use higher faster training least experience,issue,negative,positive,positive,positive,positive,positive
1016466781,"Why did you change from 256 to 243? Pretrained vocoder has more than one million epochs, so to get at least the same quality, you'll need way more than 100k steps for your vocoder.
The encoder looks fine, in general if error is less than 0.01, you can stop training there.
For the synthesizer: If you did not learn attention yet, you will have no success training a vocoder. So at first you need to fix this.",change one million get least quality need way fine general error le stop training synthesizer learn attention yet success training first need fix,issue,negative,positive,positive,positive,positive,positive
1016452576,"Hi @Bebaam Thanks for your response. Unfortunately, my embedding has size of 243. My main goal was to train all 3 networks from scratch anyway. 

> Only having 14k iterations will be the problem here. Sure the GPU is used for training?

 Yes, it is shown in my cmd that GPU was involved. I will try the vocoder to reach that 100k.  I am also wondering if the main problems are not caused by no visible attention line after training synthesizer. 

Encoder (166800):
![encoder_umap_166800](https://user-images.githubusercontent.com/61107405/150139673-389542f0-8442-4654-bf8d-2cb237c4a94f.png)

Synthesizer (80k):
![attention_step_80000_sample_1](https://user-images.githubusercontent.com/61107405/150139658-ec5320c5-7a1d-4837-91f0-a95824227af9.png)


",hi thanks response unfortunately size main goal train scratch anyway problem sure used training yes shown involved try reach also wondering main visible attention line training synthesizer synthesizer,issue,negative,positive,positive,positive,positive,positive
1016428795,"It should be possible to use the pretrained vocoder, if you did not change the sampling rate (16k) or the embedding size of 256. Did you try to use it? 

Only having 14k iterations will be the problem here. Sure the GPU is used for training? 14k after 2 days sounds like the CPU is used here. Usually you will have proper results after a few 100k steps.",possible use change sampling rate size try use problem sure used training day like used usually proper,issue,negative,positive,positive,positive,positive,positive
1016423502,"For training:
https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training
For the structure of your training data:
https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-666099538",training structure training data,issue,negative,neutral,neutral,neutral,neutral,neutral
1016367935,"@Archviz360 I spent a few hours getting the Swedish model to work and collected all required files at this link. The setup is a little tricky because it uses tensorflow 1.15. Enjoy!
https://github.com/raccoonML/Real-Time-Voice-Cloning/releases/tag/Swedish-1",spent getting model work collected link setup little tricky enjoy,issue,negative,positive,neutral,neutral,positive,positive
1013645118,"hm? its gone i cant down load the required saved models for swedish version. :( cna some one please send it to my email peacenet@hotmail.se 

when i go to this link https://github.com/blue-fish/Real-Time-Voice-Cloning/tree/400_pretrained_swe_301 it says not found. i remeber there it was important files that i needed to get it to work. if some one could send them to me so it would be greate. does some one have all the files so we can restore the page?? ",gone cant load saved version one please send go link found important get work one could send would one restore page,issue,negative,positive,positive,positive,positive,positive
1013055976,"My only purpose, fix small problems that annoy no-one",purpose fix small annoy,issue,negative,negative,negative,negative,negative,negative
1012216845,"Hi, I think that 2000 iterations correspond to one epoch! So I think it would be nice to train more! ",hi think correspond one epoch think would nice train,issue,negative,positive,positive,positive,positive,positive
1008814909,"Had the same problem with the same card, solved it by installing a newer version of PyTorch, this version excactly:
pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html
",problem card version version pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
1008637642,"Sorry, I don't know what is `webrtcvad`.
Just googled... This one https://github.com/wiseman/py-webrtcvad ?
What should I check?",sorry know one check,issue,negative,negative,negative,negative,negative,negative
1008589117,"Guh, this is still a problem?
Do you know which package it was? Probably `webrtcvad`?",still problem know package probably,issue,negative,neutral,neutral,neutral,neutral,neutral
1008584816,"Thanks for reporting.
Yeah dropbox won't let me keep this public link this size with this much traffic.
Google drive doesn't allow for direct download for files larger than 100mb. I tried to split the file in parts at some point but eventually gave up on that idea.",thanks yeah wo let keep public link size much traffic drive allow direct tried split file point eventually gave idea,issue,positive,positive,positive,positive,positive,positive
1008502188,"I figured it out by digging through the code. The dropbox link is now dead, and it's throwing an exception because of it. This needs to be addressed. The code runs if you remove the line of code in demo_toolbox.py calling the function, but there are other issues that arise (see #976)",figured digging code link dead throwing exception need code remove line code calling function arise see,issue,negative,negative,negative,negative,negative,negative
1008200759,"It works with ""Desktop Development with C++"". Not sure if everything is needed, but I finally completed installation.

![image](https://user-images.githubusercontent.com/1828532/148664790-f6cee607-f4f5-4a39-bfec-48436d233aea.png)

Possibly, you could mention this in installation guide.
",work development sure everything finally installation image possibly could mention installation guide,issue,negative,positive,positive,positive,positive,positive
1008193919,"Corentin, 
Thank you so much for developing and sharing this valuable tool! Congratulations on the birth of your child, and wishing you a lifetime of enjoyment with your now-expanded family! 

Do not underestimate the effect that your Masters thesis and the toolbox have had upon the world...there are likely many more people than you could have ever imagined who are using it for numerous new and creative things, all over the world. Blessings and thanks.
Regards,
Tomcattwo",thank much valuable tool birth child wishing lifetime enjoyment family underestimate effect thesis toolbox upon world likely many people could ever numerous new creative world thanks,issue,positive,positive,positive,positive,positive,positive
1005468285,Does anyone have the dropbox links? they're invalid right now.,anyone link invalid right,issue,negative,positive,positive,positive,positive,positive
1005438547,"> `qt.qpa.xcb: could not connect to display`

Make sure your X forwarding is working properly, then try again.",could connect display make sure forwarding working properly try,issue,negative,positive,positive,positive,positive,positive
1003747175,"thank you, will be nice to have this link also in error when say please manually download from Gdrive, save lot of time",thank nice link also error say please manually save lot time,issue,positive,positive,positive,positive,positive,positive
1003730522,"If an alternative download location is needed, download the pretrained models zip file from this link and rename `vocoder/saved_models/pretrained/pretrained.pt` to vocoder.pt.

https://github.com/raccoonML/Real-Time-Voice-Cloning/releases/tag/RTVC-7 ",alternative location zip file link rename,issue,negative,neutral,neutral,neutral,neutral,neutral
1002828452,"> Thank you very much! I think we can close the issue.

@e0xextazy  Is it possible for you to share synthesizer code with Tacotron2 model? ",thank much think close issue possible share synthesizer code model,issue,positive,positive,neutral,neutral,positive,positive
1002258502,Your python version is too recent. You can should get away with it by unpinning the dependencies. I'll add a note in the readme,python version recent get away add note,issue,negative,neutral,neutral,neutral,neutral,neutral
1002240268,"> If you were to build from this repo to start a serious project, code-wise it's okay-ish.

I really like how you perform synthesizer audio preprocessing in this repo. The code gets reused in a lot of my personal projects. I find the train.txt and individual .npy files are much easier to work with than pickle datasets.",build start serious project really like perform synthesizer audio code lot personal find individual much easier work pickle,issue,positive,positive,neutral,neutral,positive,positive
1002157762,"I see, the synthesizer download is failing. You can download it manually [here](https://drive.google.com/u/0/uc?export=download&id=1EqFMIbvxffxtjiVrtykroF6_mUh-5Z3s) while I find a fix.",see synthesizer failing manually find fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1001251259,"Hello! I want to ask a few things about attention...My synthesizer model is already above 225k steps but the graphics about attention seem worse than previous graphics. For example:
step 210500
![attention_step_210500_sample_1](https://user-images.githubusercontent.com/42816278/147421661-2c070e65-8bd3-4a92-8173-86d2fa909368.png)
step 229000
![attention_step_229000_sample_1](https://user-images.githubusercontent.com/42816278/147421663-c7b4ceb4-31e0-4ff2-92a5-afb982b282aa.png)
Like these two examples, there are a lot of graphics that some times seem more likely to the 210500 and other times to the 229000, I am worried that it may be overfitting maybe? I also want to know if this metric and the mel-spectrogram are the only ones that I can compare to Corentin's model, or if I can make another comparison between the two models of the sinthesizer. I don't know when I should stop training the synthesizer, too.
",hello want ask attention synthesizer model already graphic attention seem worse previous graphic example step step like two lot graphic time seem likely time worried may maybe also want know metric compare model make another comparison two know stop training synthesizer,issue,negative,negative,neutral,neutral,negative,negative
1000944755,"> I got past this error by installing https://anaconda.org/conda-forge/python-sounddevice

Can confirm this helped me too. Thanks a lot, you just saved my Bachelor-Thesis!",got past error confirm thanks lot saved,issue,negative,negative,neutral,neutral,negative,negative
1000057829,I refactored to be able to run on TPU but wasn't successful with it. Otherwise as far as I remember training could run on GPU. Have a look at https://github.com/Ca-ressemble-a-du-fake/Real-Time-Voice-Cloning/tree/lightning .,able run successful otherwise far remember training could run look,issue,positive,positive,positive,positive,positive,positive
999583410,"Nevermind, the code was made for windows filepaths with double \\. 
I needed to replace it with single /.",code made double replace single,issue,negative,negative,neutral,neutral,negative,negative
997292457,"@bamel-yashvander Sorry that I'm so late! I didn't try it. I just accepted to use shorter texts. Not the best solution but acceptable in my case.

Best regards
Marc",sorry late try accepted use shorter best solution acceptable case best marc,issue,positive,positive,positive,positive,positive,positive
995961027,"I dragged it into the local branch,ran the demobox once again and got this 

No module named 'torch'

i installed pytorch too.do i have to deinstall it?",dragged local branch ran got module,issue,negative,neutral,neutral,neutral,neutral,neutral
995266736,"Man, this makes me sad.

I wish you the best wherever you go.",man sad wish best wherever go,issue,positive,positive,positive,positive,positive,positive
995164296,"Well, the project actually has all the files I showed you in the image. Try cloning again the project or pulling it to your local branch.",well project actually image try project local branch,issue,negative,neutral,neutral,neutral,neutral,neutral
995070799,"**Installation overly simplified**  by Aster

**Step 1**
> all you need to do is download the whole folder in the google drive it's the whole repo and the models are already included so you don't need to download it separately.

**Link:** https://drive.google.com/drive/folders/1couiJINgIAaT3R73_i1rkIFnSxl7reOh?usp=sharing

**Step 2** 
> You **MUST** run it using python 3.6 or 3.7
> run cd command to the folder 

**Example:** `cd  C:\Users\Asus\Downloads\Real-Time-Voice-Cloning-master`

**Step 3**
> check your numpy ver.
> You **MUST** have numpy ver below 1.20

> if you're using conda and doesn't have pip yet run `conda install pip`

> if you have numpy ver above 1.20 run command `pip uninstall numpy` 
> then run the command `pip install numpy==1.20` after uninstalling the numpy

> after that install pytorch (https://pytorch.org/get-started/locally/)

> pick in these options in the pytorch website
> **PyTorch Build** = Stable (1.10)
> **Your OS** = [pick the OS that you're using]
> **Package** = [pick the package you're using ex. Conda, pip, LibTorch, or Source]
> **Language** = Python
> **Compute Platform** = CUDA 10.2
> **Run this Command** = [a command should appear here. The command that appears here depends on your choices]

>  copy the command that appeared in the 'Run this command' then run it

**Example:** `conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch`

**Step 4**
> install ffmpeg 
> run `pip install ffmpeg`

**Step 5**
> run `pip install -r requirements.txt` to install the remaining necessary packages. 

**Step 6 (completely optional)**
> run `python demo_cli.py` to check if there's any error. If there's none you're good to go.

**Step 7 **
> run the program
> run the command `python demo_toolbox.py`

Then you're done it should've run by now.",installation overly simplified aster step need whole folder drive whole already included need separately link step must run python run command folder example step check must pip yet run install pip run command pip run command pip install install pick build stable o pick o package pick package ex pip source language python compute platform run command command appear command copy command command run example install step install run pip install step run pip install install necessary step completely optional run python check error none good go step run program run command python done run,issue,negative,positive,positive,positive,positive,positive
994559155,"> have you tried downloading it directly?

Yes, i have tried.
The pretrained model page is displaying message ""Not Found""",tried directly yes tried model page message found,issue,negative,positive,neutral,neutral,positive,positive
992985801,"> How large is your dataset? :)

""I'm using"" approximately 3900 speakers ""890k audios"", separated by folders.

I have another dataset with thousands of speakers but they are all mixed up in the same folder.
I'm not using them, because I don't know if they can be used while they are all in the same folder. ""

Edit: I tried to pass the data to the ssd disk and the difference is brutal. It takes a few hours to transfer the data from one disk to the other. But it's flying now. I didn't think I was going to see that much of a difference.
![aaa](https://user-images.githubusercontent.com/78159081/146083274-f237d542-6a7b-43eb-b903-9e8059265e28.jpg)
",large approximately another mixed folder know used edit tried pas data disk difference brutal transfer data one disk flying think going see much difference,issue,negative,negative,negative,negative,negative,negative
992918244,"I use the google translator, sorry for the spelling.
I am training a model from scratch, my programming knowledge is basic ""I learn from google"".
I was reading and I leave you the image of how my training is going, and you will see that my loss is less than yours with 12,280 steps. I do not know what the problem will be.
I use Windows, anaconda, Rtx3090. I made the mistake of using the bad hard disk, does it improve a lot if I use the sdd?
I take this opportunity to ask, do you see my graph correct, how long do I have to train him? Your time mean: and std: they are better than mine, do I have to change something?
![buen](https://user-images.githubusercontent.com/78159081/145889936-ad0d4f47-088e-4bc3-98dc-a6f47ead2271.jpg)

",use translator sorry spelling training model scratch knowledge basic learn reading leave image training going see loss le know problem use anaconda made mistake bad hard disk improve lot use take opportunity ask see graph correct long train time mean better mine change something,issue,negative,negative,negative,negative,negative,negative
992373079,"Loss is way too high, after this amount of steps it should be at 0.0x., at least with this batch_size and with commonVoice dataset. Do you have proper folder structure as we thought about in #934 ?
Could you show a picture of the embeddings? It should be saved in encoder/saved_models/your_model.
Furthermore, you could try another trick, which may increase training speed a bit. Does training speed improve, if you change ""pin_memory"" to True in encoder/data_objects/speaker_verification_dataset?",loss way high amount least proper folder structure thought could show picture saved furthermore could try another trick may increase training speed bit training speed improve change true,issue,positive,positive,neutral,neutral,positive,positive
992333210,"Do these values look like they should? This is encoder training with batch size of 150. I am continuing the german training of padmalcom with only the mailabs dataset (6 Speakers and 900 hours). 
I thought maybe the loss is not improving a lot anymore and that it could be due to the poor speaker amount.

Maybe after I manage to add commonvoice with 15k speakers it should continue improving? 

![Bildschirmfoto 2021-12-13 um 11 39 16](https://user-images.githubusercontent.com/76405661/145798105-e470382c-f37c-4cee-9178-74292410a8f2.png)
 ",look like training batch size german training thought maybe loss improving lot could due poor speaker amount maybe manage add continue improving um,issue,negative,negative,negative,negative,negative,negative
991954582,"for need of proper structure I refer to this: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/431#issuecomment-673555684
",need proper structure refer,issue,negative,neutral,neutral,neutral,neutral,neutral
991943602,"> I used ffmpeg and a shell script as I am not fluent with python scripting. I haven't read into how python imports filesystem stuff.
> 
> Only some got corrupted, so I cannot reproduce why. I will surely use @AlexSteveChungAlvarez script for sorting since he proved it's stability. Can you tell me if it sorts the files into speaker directories? It doesn't look like it. You also mentioned before that you are not doing that, but @Babaam said it is required for encoder training.
> 
> ```shell
> #! /bin/bash
> 
> srcExt=mp3
> destExt=wav
> 
> srcDir=$1
> destDir=$2
> 
> for filename in ""$srcDir""/*.$srcExt; do
> 
>         basePath=${filename%.*}
>         baseName=${basePath##*/}
> 
>         ffmpeg -i ""$filename""  ""$destDir""/""$baseName"".""$destExt""
> 
> done
> ```

It doesn't, it just puts everything into one directory, as you may have noticed in the code itself. That code was made from a script that bluefish shared in one past issue with me, I just modified it to be used with commonvoice.",used shell script fluent python read python stuff got corrupted reproduce surely use script since proved stability tell speaker look like also said training shell done everything one directory may code code made script bluefish one past issue used,issue,positive,positive,positive,positive,positive,positive
991934212,"As far as I know the directory structure is mandatory, but I am not sure for which model. As @AlexSteveChungAlvarez stated, structure wasn't neccessary (at least for synthesizer), so if the model works with that, you could use the script.

Furthermore, maybe just training a synthesizer may be enough, I don't know that. But if I would have a 3090, I would want to try to train from scratch :D",far know directory structure mandatory sure model stated structure least synthesizer model work could use script furthermore maybe training synthesizer may enough know would would want try train scratch,issue,negative,positive,neutral,neutral,positive,positive
991933125,"ok. Yeah, u can ignore it, I also get this warning.",yeah ignore also get warning,issue,negative,neutral,neutral,neutral,neutral,neutral
991929014,"> U could try using utf-8, which is standard for me in linux instead of charmap, which is apparently standard in windows. Change line 77 in synthesizer/preprocess.py to: with text_fpath.open(""r"",encoding=""utf-8"") as text_file:
> 
> Maybe you'll have this problem later again, I think you will always get the information in the error, so just repeat this step there, too. Let me know if that helps.

I think it solved that issue thanks. However, it still says ""UserWarning: PySoundFile failed. Trying audioread instead.""
Do you know how to solve this or I do not need it since it is just a warning?",could try standard instead apparently standard change line maybe problem later think always get information error repeat step let know think issue thanks however still trying instead know solve need since warning,issue,negative,positive,neutral,neutral,positive,positive
991927125,"I used ffmpeg and a shell script as I am not fluent with python scripting. 
I haven't read into how python imports filesystem stuff. 

Only some got corrupted, so I cannot reproduce why. I will surely use @AlexSteveChungAlvarez  script for sorting since he proved it's stability. Can you tell me if it sorts the files into speaker directories? It doesn't look like it. You also mentioned before that you are not doing that, but @Babaam said it is required for encoder training. 

```sh
#! /bin/bash

srcExt=mp3
destExt=wav

srcDir=$1
destDir=$2

for filename in ""$srcDir""/*.$srcExt; do

        basePath=${filename%.*}
        baseName=${basePath##*/}

        ffmpeg -i ""$filename""  ""$destDir""/""$baseName"".""$destExt""

done
```",used shell script fluent python read python stuff got corrupted reproduce surely use script since proved stability tell speaker look like also said training sh done,issue,positive,positive,positive,positive,positive,positive
991914204,"I think chances are good that data got errors when converting, as copying shouldn't do any harm. How did you do converting? I just used pydub, something like:

```
from pydub import AudioSegment
src = elem
dst = elem.split(""."")[0]+"".wav""
# convert mp3 to wav                                                            
sound = AudioSegment.from_mp3(src)
sound.export(dst, format=""wav"")
os.remove(elem)
```
",think good data got converting harm converting used something like import convert sound,issue,negative,positive,positive,positive,positive,positive
991912410,"@AlexSteveChungAlvarez thank you for the script. I am a  python noob but I should be able to adapt it for my german dataset. Okay I believe the preprocessing will be much faster using wavs and since I have the 500 GB space I will continue using them. maybe I will get less corrupt files. maybe I already got corrupted when transcoding into wav. Don't know.

@Bebaam I am copying files with an awk script. Maybe you are faster since you are not copying but moving. But I wanted to preserve the original wavs so I can resort them for synthesizer later. 

@everyone: what are the differences of the .tsvs of the cv datasets? what is train.tsv? I always use validated.tsv and sort for up and downvotes to my desires. 

```awk
#!/usr/bin/awk -f

BEGIN {
	FS = ""\t""
	src = ""de/wavs/""
	dist = ""de/processed/""
print dist
	while(""cat de/validated-wav.tsv"" | getline)
	{
		if($4 < 2 || $5 > 0) continue
		client_id = $1
		mp3path = $2
		sub(/wav/, ""txt"", $2)
		sentence = $3
		up_votes = $4
		down_votes = $5
		age = $6
		gender = $7
		accent = $8
		locale = $9
		segment = $10
		if(system(""test -e ""src mp3path) == 0)
		{
			system(""mkdir -p ""dist client_id""/book0/wavs/"")
			system(""cp ""src mp3path"" ""dist client_id""/book0/wavs/"")
			system(""echo "" sentence "">"" dist client_id""/book0/wavs/""$2)
			printf(""Created entries for %s\n"", client_id)
		}
	}
}
```",thank script python able adapt german believe much faster since space continue maybe get le corrupt maybe already got corrupted know script maybe faster since moving preserve original resort synthesizer later everyone always use sort begin print cat continue sub sentence age gender accent locale segment system test system system system echo sentence,issue,positive,positive,neutral,neutral,positive,positive
991911245,"Furthermore, I kept only I think 100 files per speaker, this should balance the dataset a bit.",furthermore kept think per speaker balance bit,issue,negative,neutral,neutral,neutral,neutral,neutral
991909567,How do you do mimicing file hierarchy? It should usually be just moving files into directories and I think took around 15 minutes on my SSD.,file hierarchy usually moving think took around,issue,negative,negative,negative,negative,negative,negative
991908482,"For which language are you using it? Right now @Andredenise and I are using it to train synthesizer in Spanish (#941), and the preprocessing part took like 2 days. Now, we are in the training part. I prepared the dataset, and I didn't convert the mp3 to wav, since it indeed would take a lot of time (there are like 196 006 files). The part of mimicing file hierarchy also takes a lot of time, but didn't introduce me corrupt files (all the text files are 1 kb). If you get corrupt files in this step the most probable is that the preprocessing won't work well (it already happened to me, as I explained here https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/789#issuecomment-977916048 ). Maybe you can see my script to prepare the dataset and figure out how to solve your issue, here: https://github.com/AlexSteveChungAlvarez/Real-Time-Voice-Cloning/blob/master/split_transcript.py the cvcorpus function is the one that prepares the commonvoice and directly put the audios and texts in the file hierarchy. Before, I tried with the audios from validated.tsv, now I am trying with the audios from train.tsv, everything seems good until now, we have started training today.",language right train synthesizer part took like day training part prepared convert since indeed would take lot time like part file hierarchy also lot time introduce corrupt text get corrupt step probable wo work well already maybe see script prepare figure solve issue function one directly put file hierarchy tried trying everything good training today,issue,positive,positive,neutral,neutral,positive,positive
991869419,"I am having severe Issues with preparing the commonvoice Dataset. 
- First I am converting the mp3s to wav. That takes an eternity (a little over 24 hours). At least preprocessing seems to be a lot faster that way.  
- next I am mimicing file hierarchy (root/speaker/book/wav/files.*)and placing .txts next to wavs. Takes also forever and seemingly introduces a lot of corrupt files. 
- cleaning corrupt files (those that are 0 kb ) consumes time. 
- then after preprocess the data is unusable. Seemingly there were still some corrupt things inside. 

Result: I am left with nothing but wasted time. It seems that file corruption is a big issue. Maybe my script for sorting the files to folder is shit. 

Any Ideas? ",severe first converting eternity little least lot faster way next file hierarchy next also forever seemingly lot corrupt cleaning corrupt time data unusable seemingly still corrupt inside result left nothing wasted time file corruption big issue maybe script folder,issue,negative,negative,negative,negative,negative,negative
991788104,"I found a fix:
$ python -m pip install matplotlib==3.3

The version I had was 3.4.3 and it has a problem.",found fix python pip install version problem,issue,negative,neutral,neutral,neutral,neutral,neutral
991742146,"@Dannypeja I already contacted Corentin about this. This project definitely needs a maintainer, otherwise it will be undocumented, crowded with issues and out-fashioned in a short period of time...",already project definitely need maintainer otherwise undocumented crowded short period time,issue,negative,neutral,neutral,neutral,neutral,neutral
990835662,"For some reason, running it off my CPU seems to work. Is there any downside to this?",reason running work downside,issue,negative,neutral,neutral,neutral,neutral,neutral
990758397,"Im not sure which directory youre pointing at,heres my Real Time Voice Cloning Directory.
https://i.gyazo.com/d607558bea7a75793d2004c8a6c7ff36.png",sure directory pointing real time voice directory,issue,negative,positive,positive,positive,positive,positive
990710404,"Hey @Marcophono2 did you try to increase the sample size in order to generate longer texts? Did it work?

@CorentinJ When I increased the sample size, the voice quality got degraded and whole words started to end up in noise. Is this normal behaviour? Also can you suggest a way where longer texts can be generated from this?

BTW very nice work. I really appreciate it :)",hey try increase sample size order generate longer work sample size voice quality got degraded whole end noise normal behaviour also suggest way longer nice work really appreciate,issue,positive,positive,positive,positive,positive,positive
990678374,"Please see : https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/929#issuecomment-984159090 

I checked. All .pt files are in the drive. 
#929 ",please see checked drive,issue,negative,neutral,neutral,neutral,neutral,neutral
990256570,"The code is not yet running so I cannot tell.

There is a set of pre-trained models here: https://drive.google.com/file/d/1n1sPXvT34yXFLT47QZA6FIRGrwMeSsZc/view which is in the github.",code yet running tell set,issue,negative,neutral,neutral,neutral,neutral,neutral
990238652,"@AlexSteveChungAlvarez  could you check on your local machine when the files were created? We are trying to figure out over here #942 if those from 06.02.21 are latest, thus RTVC 7.  If your Date is newer, then we are behind. 
If I download unzipped files from your drive they get date of today.",could check local machine trying figure latest thus date behind drive get date today,issue,negative,positive,neutral,neutral,positive,positive
990011054,"https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models link is dead.
The pretrained models uploaded by francisco-reteria are of the previous release. Not the latest.",link dead previous release latest,issue,negative,positive,neutral,neutral,positive,positive
989922381,"Thank you Bebaam, I asked for the 3060 because @Andredenise is helping me with his gpu, we will work on it later, right now I will prepare the data. For tomorrow I hope we have good news!",thank helping work later right prepare data tomorrow hope good news,issue,positive,positive,positive,positive,positive,positive
989904487,"Okay, now I understand you idea. If it works with your target voice in english, then it may be fine.
I see your older post, I would try with train.tsv and if the error still occurs, then I would try to search for nan-values in the data. Maybe a few files are corrupt.
The batch_size depends on your gpu VRAM. The more the better in my opinion, so just try with your 2060 6GB I assume, how high you can set the batch_size without getting cuda memory errors. For the 3060 12GB, you can easily double the amount.",understand idea work target voice may fine see older post would try error still would try search data maybe corrupt better opinion try assume high set without getting memory easily double amount,issue,negative,positive,positive,positive,positive,positive
989887587,"By the way, which batch size do you recommend for a RTX 2060 and for a RTX 3060?",way batch size recommend,issue,negative,neutral,neutral,neutral,neutral,neutral
989876981,"I already tried the pretrained.pt files with the same audio of my voice and it worked, that's why I don't think it may be the encoder, if it was, then with the pretrained.pt wouldn't have worked. As I said before, the target audio is me speaking in English like for 10-11 seconds. Then, the problem should be in the model I get from the synthesizer. I am pretty sure the issue is caused because I don't have enough speakers to train on. Now that you said I should use train.tsv, maybe that was the issue with commonvoice. Did you see my older post here https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/789#issuecomment-977916048? All of that was with the validated.tsv, I will try right now with the train.tsv and see how it works. I will try and error until I get this Spanish model! Thank you for being alert!",already tried audio voice worked think may would worked said target audio speaking like problem model get synthesizer pretty sure issue enough train said use maybe issue see older post try right see work try error get model thank alert,issue,positive,positive,positive,positive,positive,positive
989865931,"But I am afraid that the problem lies in the encoder, as the cloning quality depends mainly on the encoder. I remember it was stated in some issue, but did not find it. But this by blue-fish should indicate in this direction:
https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/162#issuecomment-704524966

1. quality may differ from person to person
2. if the encoder isn't familiar with voices like yours, it can't encode it accordingly.
So if you are really interested in having very good quality, I would think about training an encoder. But keep in mind that this will need much more time than training a synthesizer.",afraid problem quality mainly remember stated issue find indicate direction quality may differ person person familiar like ca encode accordingly really interested good quality would think training keep mind need much time training synthesizer,issue,positive,positive,positive,positive,positive,positive
989855239,"CommonVoice should be the best dataset by far, the sheer amount of speakers I did not find anywhere else. 
For me, the quality of validated.tsv was not good enough, I assume there are all speaker, for which the corresponding texts are more or less verified. In contrast, train.tsv fits better, maybe this is the subset of validated having comparatively good quality.",best far sheer amount find anywhere else quality good enough assume speaker corresponding le contrast better maybe subset comparatively good quality,issue,positive,positive,positive,positive,positive,positive
989822328,"Did you try to use commonVoice with the code in this repo? What suggestions can you give me about it? I haven't found another dataset with such many speakers as it has, yet.",try use code give found another many yet,issue,negative,positive,positive,positive,positive,positive
989814375,"Oh, now I get what you were asking, I used validated.tsv to copy all the audios from the original into a new directory with only the validated ones and from this file also got the .txts",oh get used copy original new directory file also got,issue,negative,positive,positive,positive,positive,positive
989761912,"Yes, speakers_per_batch is the batch size. utterances_per_speaker can remain as it is. I think I would start with batch size of 150, but I would track it with ""watch nvidia-smi"" or ""nvidia-smi -l 5"" for example. I would maximise GPU VRAM usage, so increase batch_size to the limit if possible. Training the encoder will take the most time in my experience.",yes batch size remain think would start batch size would track watch example would usage increase limit possible training take time experience,issue,positive,neutral,neutral,neutral,neutral,neutral
989755420,"Ok I only know commonVoice for other languages, there we have multiple .tsv files. Maybe it is different to spanish commonvoice dataset. ",know multiple maybe different,issue,negative,neutral,neutral,neutral,neutral,neutral
989362972,"For commonvoice I used the one that comes with it, and generated each .txt for each audio. To solve the problem of training a new encoder for the language, I have tried to clone an audio in English, so it detects it well, and then put the text in Spanish, since the synthesizer is being trained in spanish.",used one come audio solve problem training new language tried clone audio well put text since synthesizer trained,issue,negative,positive,positive,positive,positive,positive
989334468,"Training loss varies with datasets, but it doesn't look wrong. 
As mentioned in #30, maybe you need to train an own encoder for a new language.
For commonvoice, which tsv file did you use? 
Maybe it is really important to have each speaker as an own folder to distinguish voices, as we discussed in #934 
",training loss look wrong maybe need train new language file use maybe really important speaker folder distinguish,issue,negative,positive,neutral,neutral,positive,positive
989062744,"What I want to achieve is to be able to clone any unseen voice during training, as the English pretrained model does, but in Spanish. That's why I didn't finetune it. Here is the loss at 50k steps:
![image](https://user-images.githubusercontent.com/42816278/145261310-27036809-633d-4279-b878-b9ce7c5fc3e3.png)
Unfortunately, there isn't any information about the number of speakers in this dataset.
Before, I tried with samples of the cv-corpus dataset (https://commonvoice.mozilla.org/es/datasets), which has plenty of voices, but I don't know why the outputs from target audios were a lot of noise, like whispering, or silences, even when the target audios was one from the training it didn't output the text passed, but it did output the same audio with much less quality. I tried with samples of that dataset, because with the entire dataset there was an error which I attached in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/789#issuecomment-977916048 . Should I continue training with Crowdsourced high-quality Peruvian Spanish speech data set until 100k+ steps? 
Or maybe you know how to solve the issue with the cv-corpus so that I can train on it?",want achieve able clone unseen voice training model loss image unfortunately information number tried plenty know target lot noise like whispering even target one training output text output audio much le quality tried entire error attached continue training speech data set maybe know solve issue train,issue,negative,positive,neutral,neutral,positive,positive
988714981,"yes the link is broken. can anyone please upload the ""synthesizer\saved_models\pretrained\pretrained.pt"" file
",yes link broken anyone please file,issue,negative,negative,negative,negative,negative,negative
988711980,"How many different speaker are there? At least 300 is suggested. If you train with a batch size of 12 as you mentioned in #940, maybe the model did not yet converge after 50k steps. Maybe it will need 100k steps+. Moreover, I assume you did learn attention, as you have kinda good quality, but wrong voice? Then the question is: Did you finetune your model? To get good results it is crucial to finetune it for a single speaker, this will vastly improve quality. Have a look on #437 ",many different speaker least train batch size maybe model yet converge maybe need moreover assume learn attention good quality wrong voice question model get good crucial single speaker vastly improve quality look,issue,positive,positive,positive,positive,positive,positive
988707788,"Dear Mr. Blue-Fish, 

if you ever read this I want to tell you how much your experiments meaned to me as they helps so much in understanding how different parameters work and therefore opened the door to deeper understanding of this project. 

Whatever the reasons are you put everything offline, I wanted to really kindly ask if there was any possibility to reupload your experiments. With those files less work needs to be done to contribute to the project and to get a new maintainer. 
I would be really happy hearing from you some way in the future. 
If you want you can leave me a message at doctolibforme@gmail.com.

Wishing you all the very best, 
Dape",dear ever read want tell much meaned much understanding different work therefore door understanding project whatever put everything really kindly ask possibility le work need done contribute project get new maintainer would really happy hearing way future want leave message wishing best,issue,positive,positive,positive,positive,positive,positive
988699791,"Nevermind Blue-fish is gone :(((((((((( #928
If you ever read this, I'd be so happy if you could at least leave your experience you learned on /experiments. 
This is a huge loss for all of us who are getting started. 

We wish you all the very best but are sad that all of your knowledge is now gone!",gone ever read happy could least leave experience learned huge loss u getting wish best sad knowledge gone,issue,positive,positive,positive,positive,positive,positive
988685535,"Sorry to ask again:

Real-Time-Voice-Cloning/encoder/params_model.py

 learning_rate_init = 1e-4 
 speakers_per_batch = 64 
 utterances_per_speaker = 10 
 
 is speakers_per_batch the batch size? Or is it utterances_per_speaker or is it both kinda? 
 what values would you imagine for a rtx 3090? ",sorry ask batch size would imagine,issue,negative,negative,negative,negative,negative,negative
988280871,Hello i have same issue like that but i couldnt solve it. Can you help me pls?,hello issue like solve help,issue,positive,neutral,neutral,neutral,neutral,neutral
988233973,"I trained the synthesizer with this dataset: http://openslr.org/73/ .
The models obtained until 50k steps are here: https://drive.google.com/drive/folders/1pYc0YK6YfdikMONkR-29054_uMxTgy_g?usp=sharing . Though, the results are not even near to the target voice to clone. Any suggestions? 
It does sound like a human, but not like the target.",trained synthesizer though even near target voice clone sound like human like target,issue,positive,positive,positive,positive,positive,positive
988088836,"U could try using utf-8, which is standard for me in linux instead of charmap, which is apparently standard in windows. Change line 77 in synthesizer/preprocess.py to:
with text_fpath.open(""r"",encoding=""utf-8"") as text_file:

Maybe you'll have this problem later again, I think you will always get the information in the error, so just repeat this step there, too. Let me know if that helps.",could try standard instead apparently standard change line maybe problem later think always get information error repeat step let know,issue,negative,neutral,neutral,neutral,neutral,neutral
987939338,"This paper seems very interesting, I already had a quick look at Coqui TTS, which is the tool they used for their experiments (Coqui TTS was developed by the ex team mozilla). Hopefully I cant take a look at it later. Maybe you can give me a hand in understanding Coqui?
I think you should close this issue since the issue was solved already, and we should talk about the helping hand in #789 :) .",paper interesting already quick look tool used ex team hopefully cant take look later maybe give hand understanding think close issue since issue already talk helping hand,issue,positive,positive,positive,positive,positive,positive
987935420,"I just trained the synthesizer. But I think the encoder uses a similarity function to distinguish between different speakers, I'm not sure if the folder structure has something to do with it.",trained synthesizer think similarity function distinguish different sure folder structure something,issue,negative,positive,positive,positive,positive,positive
987773531,"1 % would indicate GPU is not used here, but the training speed sounds like gpu is used. Maybe the speed is ok for a 2060, but there are also some bugs with training speed, which have unfortunately not really solutions yet, like  #700  ",would indicate used training speed like used maybe speed also training speed unfortunately really yet like,issue,negative,positive,positive,positive,positive,positive
987764896,"Did you train an encoder or did you always just train a syn? As I understood, the encoder will need to distinguish between different speakers with help of the folder structure, but I didn't go into it. ",train always train understood need distinguish different help folder structure go,issue,negative,neutral,neutral,neutral,neutral,neutral
987696756,"@AlexSteveChungAlvarez Sorry I couldn't help you with the Spanish model last week, I don't have the resources or skill to do so, but maybe [this new paper](https://edresson.github.io/YourTTS/) might help you reach that goal and deadline (TTS language synthesis, zero shot, low resource languages ...) ",sorry could help model last week skill maybe new paper might help reach goal deadline language synthesis zero shot low resource,issue,negative,negative,neutral,neutral,negative,negative
987182085,"Yes, it is! I tried without them and it didn't recognize the audios! You can put them any name you want, just remember to include the names of the subfolder and folder1 in the command, for example: --datasets_name subfolder --subfolders folder1",yes tried without recognize put name want remember include folder command example folder,issue,negative,neutral,neutral,neutral,neutral,neutral
987180885,"Thanks a lot! 
Just to further clarify: Is it really essential that you have folder 1, 2 and 3 or are those just an example? ",thanks lot clarify really essential folder example,issue,negative,positive,neutral,neutral,positive,positive
987176730,"I already trained 3 times with different datasets, I am doing a fourth one right now, just by using this folder hierarchy:
```
*datasets_root
    * subfolder
        * folder1
            * folder2
                * folder3
                    * audio
                    * text
                    * audio
                    * text
                    * .
                    * .
                    * .
```
You can leave the whole 15000 recordings in one directory with the txts, I did that.",already trained time different fourth one right folder hierarchy folder folder folder audio text audio text leave whole one directory,issue,negative,positive,positive,positive,positive,positive
987153191,"I don’t understand. Bebaam confirmed earlier that the speaker folders are used to recognise different Speakers. So do you say the opposite? 
what do you mean by folder hierarchy? Could you give an example? Would it be possible to simply leave all recordings of all 15.000 speakers in one directory and add the .txts alongside them? ",understand confirmed speaker used different say opposite mean folder hierarchy could give example would possible simply leave one directory add alongside,issue,negative,positive,neutral,neutral,positive,positive
987116319,"You don't need to split them into one subdirectory for each speaker, but the folder levels (hierarchy) need to be the same.",need split one speaker folder hierarchy need,issue,negative,neutral,neutral,neutral,neutral,neutral
987114342,"If this issue is already fixed, don't forget to close it. Thanks!",issue already fixed forget close thanks,issue,negative,positive,positive,positive,positive,positive
987112193,"Check if you have this directory inside the Real-Time-Voice-Cloning directory:
![image](https://user-images.githubusercontent.com/42816278/144909170-7719a479-fab3-4bd5-aad9-bb994666c227.png)
",check directory inside directory image,issue,negative,neutral,neutral,neutral,neutral,neutral
986986484,"you don't need to train with your friends voice. If English just train with the datasets given, then give your friends voice to clone them.",need train voice train given give voice clone,issue,negative,neutral,neutral,neutral,neutral,neutral
986138015,"Again all the links are broken. Can anyone upload the ""synthesizer\\saved_models\\pretrained\\pretrained.pt"" file in google drive or onedrive please...",link broken anyone file drive please,issue,negative,negative,negative,negative,negative,negative
986042356,I tried to clone my friends voice to prank him but I uploaded 2 to 3 minutes of his voice and just did Synthesize and Vocode to every audio file I had 3 of them,tried clone voice prank voice synthesize every audio file,issue,negative,neutral,neutral,neutral,neutral,neutral
986041778,"Here is you answer and just SAY ME HOW TO TRAIN!?!?!?!

Answer :
try install C++ Visual Studio Build Tools? V14 or higher?

It works for me here is all the modules I installed just for this program
You dont need to install all of them
```
  Package                   Version
---------------          ------------
absl-py                      1.0.0
appdirs                      1.4.4
argumentparser               1.2.1
asttokens                    2.0.5
astunparse                   1.6.3
atomicwrites                 1.4.0
attrs                        21.2.0
audioread                    2.1.9
auxlib                       0.0.43
base58                       2.1.1
Brownie                      0.5.1
cached-property              1.5.2
cachetools                   4.2.4
certifi                      2021.10.8
cffi                         1.15.0
cftime                       1.5.1.1
chardet                      4.0.0
charset-normalizer           2.0.8
click                        8.0.3
colorama                     0.4.4
conda                        4.2.7
config                       0.5.1
config-parser                0.0.1
configparser                 5.1.0
cpp                          1.0.0.6
cycler                       0.11.0
Cython                       0.29.24
dataclassy                   0.11.1
decorator                    5.1.0
dill                         0.3.4
enum34                       1.1.10
ffmpeg                       1.4
ffmpeg-python                0.2.0
ffmpeg-sdk                   4.0.0
ffmpegwrapper                0.1.dev0
ffmpy                        0.3.0
flatbuffers                  2.0
fonttools                    4.28.2
future                       0.18.2
gast                         0.4.0
google-auth                  2.3.3
google-auth-oauthlib         0.4.6
google-pasta                 0.2.0
grpcio                       1.42.0
h5py                         3.6.0
hexbytes                     0.2.2
hypothesis                   6.30.0
idna                         3.3
importlib-metadata           4.8.2
importlib-resources          5.4.0
inflect                      5.3.0
inflection                   0.5.1
iniconfig                    1.1.1
ipfshttpclient               0.6.1
joblib                       1.1.0
jsonpatch                    1.32
jsonpointer                  2.2
jsonschema                   4.2.1
keras                        2.7.0
Keras-Preprocessing          1.1.2
kiwisolver                   1.3.2
libclang                     12.0.0
librosa                      0.8.1
llvmlite                     0.37.0
Markdown                     3.3.6
matplotlib                   3.5.0
mpegCoder                    3.1.0b0
multiaddr                    0.0.9
multidict                    5.2.0
multiprocess                 0.70.12.2
netaddr                      0.8.0
netCDF4                      1.5.8
numba                        0.54.1
numpy                        1.19.3
oauthlib                     3.1.1
opt-einsum                   3.3.0
packaging                    21.3
pandas                       1.1.5
Pillow                       8.4.0
pip                          21.3.1
pluggy                       1.0.0
pooch                        1.5.2
protobuf                     3.19.1
py                           1.11.0
py2                          0.1.0
pyasn1                       0.4.8
pyasn1-modules               0.2.8
pycparser                    2.21
pycryptodome                 3.11.0
pyee                         8.2.2
pyffmpeg                     2.0.5.1
pyffmpeg-bin                 1.8.0.1
pynndescent                  0.5.5
pyparsing                    3.0.6
PyQt5                        5.15.6
PyQt5-Qt5                    5.15.2
PyQt5-sip                    12.9.0
pyrsistent                   0.18.0
pysound                      0.1
pytest                       6.2.5
pytho                        0.0.1
python-dateutil              2.8.2
python-ffmpeg                1.0.11
python2                      1.2
pytz                         2021.3
PyYAML                       6.0
pyzmq                        22.3.0
regex                        2021.11.10
requests                     2.26.0
requests-oauthlib            1.3.0
resampy                      0.2.2
rsa                          4.8
scikit-learn                 1.0.1
scipy                        1.7.3
seaborn                      0.11.2
semantic-version             2.8.5
setuptools                   59.4.0
setuptools-scm               6.3.2
sip                          6.4.0
six                          1.16.0
solc                         0.0.0a0
sortedcontainers             2.4.0
sounddevice                  0.4.3
SoundFile                    0.10.3.post1
synthesizer                  0.2.0
tensorboard                  2.7.0
tensorboard-data-server      0.6.1
tensorboard-plugin-wit       1.8.0
tensorflow                   2.7.0
tensorflow-estimator         2.7.0
tensorflow-io-gcs-filesystem 0.22.0
termcolor                    1.1.0
threadpoolctl                3.0.0
toml                         0.10.2
tomli                        1.2.2
toolz                        0.11.2
torch                        1.10.0+cu102
torchfile                    0.1.0
tornado                      6.1
tqdm                         4.62.3
typing_extensions            4.0.0
umap-learn                   0.5.2
Unidecode                    1.3.2
urllib3                      1.26.7
varint                       1.0.2
visdom                       0.1.8.9
vyper                        0.3.1
warp                         0.0.1
webrtcvad                    2.0.10
websocket-client             1.2.1
Werkzeug                     2.0.2
wheel                        0.37.0
wrapt                        1.13.3
xarray                       0.20.1
yarl                         1.7.2
zipp                         3.6.0
```",answer say train answer try install visual studio build higher work program dont need install package version base brownie click cycler decorator dill dev future gast hypothesis inflect inflection markdown pillow pip pluggy pooch python sip six post synthesizer torch tornado warp wheel yarl,issue,negative,negative,negative,negative,negative,negative
986006063,"I'm not sure, but I don't think that it would be a problem. But you could easily convert CommonVoice to wav, it will take a few hours.",sure think would problem could easily convert take,issue,negative,positive,positive,positive,positive,positive
985994067,Will it be a problem that commonvoice is mp3 and mailabs is wav if I want to use them in the same training? ,problem want use training,issue,negative,neutral,neutral,neutral,neutral,neutral
985925546,"Yes, only that encoder does not need text files. That's why it is easier to obtain train data. U can use 10000+ speakers for encoder, they don't need best quality and also benefit from some lower quality files such that they can make use of noisy audio, too. But for the synthesizer you should only use good quality audio with I think at least 300 speakers.",yes need text easier obtain train data use need best quality also benefit lower quality make use noisy audio synthesizer use good quality audio think least,issue,positive,positive,positive,positive,positive,positive
985729475,So the trainers for all components use folder structure to distinguish speakers? ,use folder structure distinguish,issue,negative,neutral,neutral,neutral,neutral,neutral
985651461,"https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-666099538

I would suggest to use that structure. You can just sort the files according to the respective speaker. These informations should be contained in the commonVoice dataset.",would suggest use structure sort according respective speaker,issue,negative,neutral,neutral,neutral,neutral,neutral
985086287,"I want to collaborate I have 2 GPU 3060, pls send me all steps and I bring
u power of compute
Stay alert.

El mié., 1 de dic. de 2021, 7:50 p. m., AlexSteveChungAlvarez <
***@***.***> escribió:

> Please I need some help here, with cv-corpus dataset I tried using less
> data and I could made it preprocess and train, but the results were very
> bad.
> By the way, all the process from preprocessing until training took me
> about a week each time I tried on a different dataset or dataset subsample.
> I am using a laptop with a NVidia GeForce RTX-2060 gpu, so I think it
> shouldn't be that slow. Any help? I am considering using another different
> dataset since both cv-corpus and tux100h didn't give me results compared to
> the original release of the project, but I don't know which one to choose
> now and just have 1 month to finish this.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/789#issuecomment-984191327>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AADIWFAH76SA6MGOEPJKFNDUO27FDANCNFSM473UA5FA>
> .
> Triage notifications on the go with GitHub Mobile for iOS
> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>
> or Android
> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.
>
>
",want collaborate send bring power compute stay alert el de de please need help tried le data could made train bad way process training took week time tried different subsample think slow help considering another different since give original release project know one choose month finish thread reply directly view triage go mobile android,issue,positive,negative,neutral,neutral,negative,negative
984608419,"Tested the pretrained.pt models and they work! Marvelous and a big thanks! 
I'll store them locally this time and have a version in a cloud service so my students can !wget the files (using OneDrive)
If Sven or CoretinJ would deem it necessary to store critical files on a more secure cloud storage, I'm willing to chip in. ",tested work marvelous big thanks store locally time version cloud service would deem necessary store critical secure cloud storage willing chip,issue,positive,positive,positive,positive,positive,positive
984368013,does anybody plan to put pretrained models  in 1 wget accessible archive as before?:) ,anybody plan put accessible archive,issue,negative,positive,positive,positive,positive,positive
984191327,"Please I need some help here, with cv-corpus dataset I tried using less data and I could made it preprocess and train, but the results were very bad. 
By the way, all the process from preprocessing until training took me about a week each time I tried on a different dataset or dataset subsample. I am using a laptop with a NVidia GeForce RTX-2060 gpu, so I think it shouldn't be that slow. Any help? I am considering using another different dataset since both cv-corpus and tux100h didn't give me results compared to the original release of the project, but I don't know which one to choose now and just have 1 month to finish this.",please need help tried le data could made train bad way process training took week time tried different subsample think slow help considering another different since give original release project know one choose month finish,issue,positive,negative,negative,negative,negative,negative
984178310,"@AlexSteveChungAlvarez Apparently the archive mentioned earlier contains older models which are not compatible with current master branch ([source](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models/2cd3887f379d4921b193214973b463043efa5c23)).

I have tested your pretrained models and it works without any problems 👍  

",apparently archive older compatible current master branch source tested work without,issue,negative,positive,neutral,neutral,positive,positive
984167020,"first of all thanks a lot for the link. You can always contact me via this mail: r0673226@student.luca-arts.be, it is currently nighttime here so I will contact you tomorrow if you already send me a short message by mail.",first thanks lot link always contact via mail currently nighttime contact tomorrow already send short message mail,issue,negative,positive,positive,positive,positive,positive
984159090,"@Tymoteusz pointed out this does not contain the synthetizer pretrained model, but I don't know why the comment is not here anymore. I read it and uploaded the 3 pretrained models to this drive: https://drive.google.com/drive/folders/1lb-LlS8Sx9RqcGzuV6GxvKHk-PC9TqQx?usp=sharing . It is not a zip, so anyone who needs the models may download each of them. They are ordered in the original structure.

> @RobbeW In the docs, there's still an initial commit `.zip` file: https://drive.google.com/file/d/1n1sPXvT34yXFLT47QZA6FIRGrwMeSsZc/view Should contain all three pretrains according to the file size

",pointed contain synthetizer model know comment read drive zip anyone need may ordered original structure still initial commit file contain three according file size,issue,positive,positive,positive,positive,positive,positive
984027098,"https://github.com/AlexChungA/Jarvis/blob/master/voice_interact.py this is the code I used last year (I took it from another code, which I don't remember right now where it is), your audio_to_text function is pretty similar to the record_audio function, in the code I provide here, I remember the speech to text worked fine. Maybe it helps. By the way, I will be back in 3 hours, if you still have problems then, I can work with you on it if needed.",code used last year took another code remember right function pretty similar function code provide remember speech text worked fine maybe way back still work,issue,positive,positive,positive,positive,positive,positive
984017820,"It's great to hear someone else had the same idea. Before closing the program I had thought of a similar thing.

I still experience trouble converting my audio to text. Currently my code for the audio imput and converting to text looks like this.


```# Choose a sample rate that is compatible with your hardware
sampling_rate = 44100
duration = 5  # seconds

# Start recording the user
original_wav = sd.rec(duration*sampling_rate, sampling_rate, 1)

# Display a progress bar while recording
# Get current time to keep track of recording length
start_time = time()
print(""Press enter to stop recording"")
input()  # This blocks the program from continuing until user presses enter

# Trim wav to actual length of recording
recording_length = time() - start_time
if recording_length < duration:
    original_wav = original_wav[:int(recording_length*sampling_rate)]


def there_exists(terms):
    for term in terms:
        if term in voice_data:
            return True

r = sr.Recognizer() # initialise a recogniser
# listen for audio and convert it to text:
def audio_to_text(ask=False):
    with sd.play(original_wav, sampling_rate) as source: # wav as source
    #with sr.AudioFile('recording.wav') as source: # File as source
        if ask:
            print(ask)
        audio = r.listen(source)  # listen for the audio via source
        voice_data = ''
        try:
            voice_data = r.recognize_google(audio)  # convert audio to text
        except sr.UnknownValueError: # error: recognizer does not understand
            print('I did not get that')
        except sr.RequestError:
            print('Sorry, the service is down') # error: recognizer is not connected
        print(f"">> {voice_data.lower()}"") # print what user said
        return voice_data.lower()


voice_data = audio_to_text() # get the voice input
print(voice_data)
```

My program only has problems listening to the audio and converting it. Is it possible to help me with this? 
Thanks in advance!

",great hear someone else idea program thought similar thing still experience trouble converting audio text currently code audio converting text like choose sample rate compatible hardware duration start recording user duration display progress bar recording get current time keep track recording length time print press enter stop recording input program user enter trim actual length recording time duration term term return true listen audio convert text source source source file source ask print ask audio source listen audio via source try audio convert audio text except error recognizer understand print get except print service error recognizer connected print print user said return get voice input print program listening audio converting possible help thanks advance,issue,positive,positive,positive,positive,positive,positive
984017365,"Can any of you help me with the spanish model for the synthesizer? Issue #789 has my work until last week, but since then I just tried with subsamples of the last dataset I used for training. It hasn't given me good results, though. I think I will need to find another spanish dataset. The main problem is that I just have 1 month left to achieve it and each try has taken me about 1 week.",help model synthesizer issue work last week since tried last used training given good though think need find another main problem month left achieve try taken week,issue,negative,positive,positive,positive,positive,positive
983998760,"I'm really interested into this project, actually I am trying to make the RTVC work in Spanish first, in order to do exactly this for next semester (I'm a bachelor student of computer science at Universidad Nacional de Ingenieria, Peru). I made a conversational web last year, so maybe you could use something like this to quit the program:
```
while 1:
        user_input=""""
        user_input=vi.record_audio("""")
        vi.respond(user_input)
        if (""bye"" in user_input) or (""goodbye"" in user_input) or (""see you later"" in user_input) or ...(here you may put all the options for saying goodbye):
            break
```
In this code, first the user tells the assistant anything and the assistant responds. When the user says ""goodbye"", the assistant answers and it stops hearing, here you may quit the program. The ""record_audio"" function is basically a speech to text, so ""user_input"" is the text input you enter to the GPT. I hope you publish your work, so next semester I can use it as a reference!",really interested project actually trying make work first order exactly next semester bachelor student computer science de made conversational web last year maybe could use something like quit program bye see later may put saying break code first user assistant anything assistant user assistant hearing may quit program function basically speech text text input enter hope publish work next semester use reference,issue,positive,positive,neutral,neutral,positive,positive
983998006,"@sveneschlbeck @AlexSteveChungAlvarez thanks a lot for the help. Kinda also my own fault, since I used the Colab version of the code containing paths to the three pretrained.pt files, which relied a !wget and the model blue-fish contributed. I should have, at least once, bothered to make a local copy of the file tree. 

I will give it a go with the G-Drive link above. Will try and figure out how to adjust the code to use the synthesizers multiple files, instead of the pretrained.pt single file path reference. 

If I can contribute in any way, let me know. If some solution comes out of this for my classes and students, I owe you all a coffee / beer. ",thanks lot help also fault since used version code three model least make local copy file tree give go link try figure adjust code use multiple instead single file path reference contribute way let know solution come class owe coffee beer,issue,positive,negative,neutral,neutral,negative,negative
983985230,"@RobbeW @AlexSteveChungAlvarez This is a pretty good example of why we should definitely improve the docs on this repo. There were several people wishing tutorials on trainings (also in other languages) and deleting pretrained models/accounts without notification is something that should NEVER happen. Don't know what went wrong there...

Should wait for @CorentinJ `s opinion",pretty good example definitely improve several people wishing also without notification something never happen know went wrong wait opinion,issue,positive,positive,neutral,neutral,positive,positive
983983967,"That should work for him.

> @RobbeW In the docs, there's still an initial commit `.zip` file: https://drive.google.com/file/d/1n1sPXvT34yXFLT47QZA6FIRGrwMeSsZc/view Should contain all three pretrains according to the file size

",work still initial commit file contain three according file size,issue,negative,neutral,neutral,neutral,neutral,neutral
983982501,"@RobbeW In the docs, there's still an initial commit ``.zip`` file: https://drive.google.com/file/d/1n1sPXvT34yXFLT47QZA6FIRGrwMeSsZc/view Should contain all three pretrains according to the file size",still initial commit file contain three according file size,issue,negative,neutral,neutral,neutral,neutral,neutral
983981825,"Hello, @RobbeW!  I also have the files, I have been trying to train a spanish model for a month, with no luck. I'm also doing a class project. I can pass you the files via gmail or drive as a fast solution for you if you need them fast. By the way, I need some help with the work of training in spanish, since I'm running out of time and the datasets I have worked with haven't given me results (#789).",hello also trying train model month luck also class project pas via drive fast solution need fast way need help work training since running time worked given,issue,positive,positive,positive,positive,positive,positive
983980002,"@RobbeW @CorentinJ We got a problem, the files are way too large to just upload them and Git LFS is not working on forked repos, so we really depend on @CorentinJ to add or restore them...",got problem way large git working forked really depend add restore,issue,negative,positive,positive,positive,positive,positive
983964866,"@RobbeW I do, this is a hell of a lucky day we're having. I coincidentally downloaded the repo two days back or so but were not able to do something yet. This needs to be fixed, otherwise the link in the README is leading to an error page, too.

The files are too big for an upload, I'll try to add them in a new commit, will keep you posted

@CorentinJ What are you proposing on doing here?",hell lucky day coincidentally two day back able something yet need fixed otherwise link leading error page big try add new commit keep posted,issue,negative,positive,positive,positive,positive,positive
983929574,"Unfortunately there is no other way than here, and I am also just a beginner with this stuff. I will try to check the repo sometimes, feel free to ask more questions. Maybe it will help other people too :)",unfortunately way also beginner stuff try check sometimes feel free ask maybe help people,issue,positive,negative,neutral,neutral,negative,negative
983927140,"You are welcome. I needed a lot of time to check which things I need to change, so I am happy if it helps now :) 
Sorry if it was wrongly communicated, but attention is important for synthesizer only. For the encoder the loss itself should be enough, you also get a plot of some speakers every x steps. 

About the tts schedule: A higher r-value would faster training but could have some disadvantages (can't rembember which :D). Easiest would be to stay at a value of 2. If the model does not learn attention over all, you could try a higher value.
Yeah the values of the schedule are just from experiments. The steps where you change the learning rate depend on the batch size, as the model will train faster with a higher batch size. 

If you use e.g. 48k sample rate for the synthesizer, the sound generated by the 16k pretrained vocoder here would be three times as fast as usual.
So the vocoder should be fine and language independent. You can try it after you have your synthesizer and fine tune it. Or you can train one from scratch, with your GPU it should be fast and can be done with the same datasets which you use for training the synthesizer. Just follow the steps for training in the wiki I mentioned earlier :) 
",welcome lot time check need change happy sorry wrongly attention important synthesizer loss enough also get plot every schedule higher would faster training could ca easiest would stay value model learn attention could try higher value yeah schedule change learning rate depend batch size model train faster higher batch size use sample rate synthesizer sound would three time fast usual fine language independent try synthesizer fine tune train one scratch fast done use training synthesizer follow training,issue,positive,positive,positive,positive,positive,positive
983714476,"Aha, I guess issue #928 answers my concern, but kinda does not solve it. (ghost made me miss that this comment was from BlueFish). 

Maybe on the off chance anybody (@sveneschlbeck ) still has some pretrained.pt's stored locally? ",aha guess issue concern solve ghost made miss comment bluefish maybe chance anybody still locally,issue,negative,neutral,neutral,neutral,neutral,neutral
983680938,"Hey thank you so much, that was really detailled and also helps me to get a better feeling for all those values! 
I will leave embedding size at 256 for now. 

You are talking about attention for encoder. Do I need to change any parameters for that? I assume not, since you wrote only 
to increase bs. How should I do so? Just increase batch_size until out-of-memory errors occur? 

Is attention also a thing for synthesizer?
About the tts_schedule: You did some adjustments besides bs there. Could you elaborate how you guessed r, lr, and step? 
Just from experience? I'm asking just to understand your gut feeling :) 
What is the r-factor in this context?

About the vocoder: Do you mean that vocoding a mel spectrogram is language Independent? So the only reason to retrain it is in order to get higher bitrate as output? 

Common voice seems large, but there is a lot of junk inside there and I hope i will be clean enough. I have no feeling how clean it has to be for the synthesizer. 
After I get the Data sorted... jeez... I will try out everything and report!
Is there a way to contact you (discord) where we can continue the conversation? 
I still don't know if here is the right place since it should be for technical issues as I thought. :) 

Thanks a lot!",hey thank much really also get better feeling leave size talking attention need change assume since wrote increase increase occur attention also thing synthesizer besides could elaborate step experience understand gut feeling context mean mel spectrogram language independent reason retrain order get higher output common voice large lot junk inside hope clean enough feeling clean synthesizer get data sorted try everything report way contact discord continue conversation still know right place since technical thought thanks lot,issue,positive,positive,positive,positive,positive,positive
983633675,"When training the synthesizer after the encoder, it is important that your models learns attention. For this purpose, you can look into your synthesizer/saved_models/plots. The attention plots should form some kind of a line from top left to top right (maybe they are cut at some point). Until attention is learned, the plots are empty or have some random noise. I think you will see it after 5-10k steps. I would train the synthesizer until it converges, and change the synthesizer/hparams.py values before training. As you won't use a batch size of 12 as it is by default, I would decrease the steps. Maybe something like:

tts_schedule = [(2,  1e-3,  5_000,  50),   # Progressive training schedule
                        (2,  5e-4,  10_000,  50),   # (r, lr, step, batch_size)
                        (2,  2e-4,  20_000,  50),   #
                        (2,  1e-4, 30_000,  50),   # r = reduction factor (# of mel frames
                        (2,  3e-5, 50_000,  50),   #     synthesized for each decoder iteration)
                        (2,  1e-5, 70_000,  50)],  # lr = learning rate

But I would test it and adjust it properly :)",training synthesizer important attention purpose look attention form kind line top left top right maybe cut point attention learned empty random noise think see would train synthesizer change training wo use batch size default would decrease maybe something like progressive training schedule step reduction factor mel iteration learning rate would test adjust properly,issue,positive,positive,positive,positive,positive,positive
983619733,"Okay. For the encoder you could change the embedding size or hidden embedding size from 256 to e.g. 768 in encoder/params_model.py, like it was discussed in #126 . IMHO and after the question in #840 I would suggest to keep it at 256 for embedding size and hidden size, and only if you do not get good results, I would start increasing hidden embedding size first, as it does not influence synthesizer model size and allows a faster training. I think you don't need to adjust anything else if you increase the batch size.
The Common Voice dataset is the biggest by far for german models AFAIK, and should be sufficient for a first training. If you want to extend it, you could also have a look on the LibriVox dataset.
When I need to make a bet for training time, I would say:
For Encoder, with the GTX 3090 about 3 days, I think it also depends a bit on hard disk and CPU. It is not sure if it ever converges, but I would train it until loss is less than 0.01 or better 0.005. With the batch_size possible on gtx 3090, I would say 50k-100k steps could be enough. After that, synthesizer training will take only one day. If you use 16k as audio sample rate, you don't need to train a vocoder, the pretrained one in this repo will be sufficient. 
But I can't guarantee all of this, I only make guesses :D",could change size hidden size like question would suggest keep size hidden size get good would start increasing hidden size first influence synthesizer model size faster training think need adjust anything else increase batch size common voice biggest far german sufficient first training want extend could also look need make bet training time would say day think also bit hard disk sure ever would train loss le better possible would say could enough synthesizer training take one day use audio sample rate need train one sufficient ca guarantee make,issue,positive,positive,neutral,neutral,positive,positive
983100298,@craftpag Your batch and embedding sizes look reasonable to me...glad it worked :),batch size look reasonable glad worked,issue,negative,positive,positive,positive,positive,positive
983092816,"yes @sveneschlbeck 

Tried the Nvidia-smi, but that didn't fix it.
i have imported the `torch.Cuda.empty_cache()`
to `train.py` but that didn't solve it ether.

the problem was in `params_model.py`
The values are now.
`## Model parameters`
`model_hidden_size = 128`
`model_embedding_size = 128`
`model_num_layers = 3`

`## Training parameters`
`learning_rate_init = 1e-4`
`speakers_per_batch = 128`
`utterances_per_speaker = 5`

They were much higher 
If you have some improvements say what I should change, but this ""Works""

Thank you!



",yes tried fix solve ether problem model training much higher say change work thank,issue,positive,positive,positive,positive,positive,positive
983069442,"@craftpag

There's a couple of things remaining until I am out of answers, too:

**1. Are you running any other scripts/games/programs that might be taking up GPU memory? If so, do the following:**

Type ``nvidia-smi`` into the terminal and find the ``PID`` of the process using most GPU memory (apart from PyTorch of course), then kill it by typing ``taskkill /F /PID <your PID here>``

**2. Try to reduce memory-intensive (hyper)parameters, e.g. train/test size, batch size, etc.**

**3. Run the following**

```python
import torch
torch.cuda.empty_cache()
```

My guess is that it's the ``batch_size`` since that is where you specify how much data is loaded into the memory at once. See #914 to get an idea on where you can decrease the batch size. I'd do it file after file to see where the error is caused. Alternatively, you can change it in all files at once. But keep in mind that a lower batch size results in a longer training/testing duration...
",couple running might taking memory following type terminal find process memory apart course kill try reduce hyper size batch size run following python import torch guess since specify much data loaded memory see get idea decrease batch size file file see error alternatively change keep mind lower batch size longer duration,issue,negative,positive,neutral,neutral,positive,positive
983056772,"Hello
Thank you for replying @sveneschlbeck

I have tried to add those environment variables, with no luck.
I have tried to add it in different ways, but I still get the same error.
Do you think it can be other solutions out there?


",hello thank tried add environment luck tried add different way still get error think,issue,negative,neutral,neutral,neutral,neutral,neutral
982468507,"> You might have to downgrade to `torch==1.4.0` to get DataParallel to work.
> 
> * [How to use multi-GPU? fatchord/WaveRNN#189](https://github.com/fatchord/WaveRNN/issues/189)
> * [Pytorch 1.5 DataParallel huggingface/transformers#3936](https://github.com/huggingface/transformers/issues/3936)

As Synergyst mentioned, using torch version 1.4 dosen't work. The error i got is:
""AttributeError: 'PosixPath' object has no attribute 'tell'""
I googled it and find that to solve it i have to use torch version above 1.6.
Awkward face...",might downgrade get work use torch version dose work error got object attribute find solve use torch version awkward face,issue,negative,negative,negative,negative,negative,negative
981699108,"@CorentinJ I just did, didn't notice that I added the new one without removing the old, my bad :)",notice added new one without removing old bad,issue,negative,negative,negative,negative,negative,negative
981323454,"Thanks for the quick answer! 
I mistakenly disguised my intentions: I want to train new models for the German language to better understand how this all works. 
I want to the mailabs dataset + Mozilla common voice. that makes up about 15.000 voices of very various quality and I am not sure if quality will be enough for vocoder and Synthesizer. 
What did you mean by “use a greater model for the encoder”? Increase model dimensions? Where would I do so? 
also: do I need to adjust anything else if I increase batch size? 
Is there any approximation how long training could possibly take? 
- encoder: 1.5kk steps 
- Synthesizer: 295k steps
- Vocoder 1.1 kk steps 
(I am looking towards rtvc 7 as a comparison)",thanks quick answer mistakenly disguised want train new german language better understand work want common voice various quality sure quality enough synthesizer mean use greater model increase model would also need adjust anything else increase batch size approximation long training could possibly take synthesizer looking towards comparison,issue,positive,positive,positive,positive,positive,positive
981178497,"@craftpag This is not a parameter to be found in the code here but a PyTorch command that (if I'm not wrong) needs to be set as an environment variable.
Try setting ``PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:<cache in mb here>``.

Doc Quote: ""``max_split_size_mb`` prevents the allocator from splitting blocks larger than this size (in MB). This can help prevent fragmentation and may allow some borderline workloads to complete without running out of memory.""

Checkout this link to see the full documentation for PyTorch's memory management:
https://pytorch.org/docs/stable/notes/cuda.html",parameter found code command wrong need set environment variable try setting cache doc quote allocator splitting size help prevent fragmentation may allow borderline complete without running memory link see full documentation memory management,issue,negative,negative,neutral,neutral,negative,negative
981173706,"Hey,
the locations for batch_size changes look fine. The huge amount of VRAM should allow a relatively fast training, so I would just try using different batch sizes and monitor the VRAM usage if possible.
Depending on your overall intent, you could also use a greater model for training - at least for the encoder (I wouldn't suggest that, if you don't have much more than 10000 different speaker). 
More important is: What do you want to achieve? Do you want to train a model from scratch (#126 - quite old, but imo you get good insights of training), maybe in a new language? Then which datasets do you want to use? Or do you want to further finetune existing models? Maybe even go for a single speaker (#437)?
And training procedure:
https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training
",hey look fine huge amount allow relatively fast training would try different batch size monitor usage possible depending overall intent could also use greater model training least would suggest much different speaker important want achieve want train model scratch quite old get good training maybe new language want use want maybe even go single speaker training procedure,issue,positive,positive,positive,positive,positive,positive
977916048,"> 

I found out tux100h has the same voice or a very similar voice in all audios. That may be the problem. Then, I started the preprocessing on cvcorpus dataset which has multiple speakers, but I got the following error:
```
(voiceclonenv) D:\tesis2\Real-Time-Voice-Cloning>python synthesizer_preprocess_audio.py datasets_root -n 6 -s --no_trim --no_alignments --datasets_name 
tux100h-cvcorpus --subfolders valid
D:\tesis2\Real-Time-Voice-Cloning\encoder\audio.py:13: UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended.  warn(""Unable to import 'webrtcvad'. This package enables noise removal and is recommended."")
Arguments:
    datasets_root:   datasets_root
    out_dir:         datasets_root\SV2TTS\synthesizer
    n_processes:     6
    skip_existing:   True
    hparams:
    no_alignments:   True
    datasets_name:   tux100h-cvcorpus
    subfolders:      valid

Using data from:
    datasets_root\tux100h-cvcorpus\valid
tux100h-cvcorpus:   0%|                                                                                                    | 0/1 [00:00<?, ?speakers/s]D:\tesis2\Real-Time-Voice-Cloning\encoder\audio.py:13: UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended. 
  warn(""Unable to import 'webrtcvad'. This package enables noise removal and is recommended."")
D:\tesis2\Real-Time-Voice-Cloning\encoder\audio.py:13: UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended.  warn(""Unable to import 'webrtcvad'. This package enables noise removal and is recommended."")
D:\tesis2\Real-Time-Voice-Cloning\encoder\audio.py:13: UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended.  warn(""Unable to import 'webrtcvad'. This package enables noise removal and is recommended."")
D:\tesis2\Real-Time-Voice-Cloning\encoder\audio.py:13: UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended.  warn(""Unable to import 'webrtcvad'. This package enables noise removal and is recommended."")
D:\tesis2\Real-Time-Voice-Cloning\encoder\audio.py:13: UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended.  warn(""Unable to import 'webrtcvad'. This package enables noise removal and is recommended."")
D:\tesis2\Real-Time-Voice-Cloning\encoder\audio.py:13: UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended.  warn(""Unable to import 'webrtcvad'. This package enables noise removal and is recommended."")
D:\tesis2\voiceclonenv\lib\site-packages\librosa\core\audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.
  warnings.warn(""PySoundFile failed. Trying audioread instead."")
D:\tesis2\voiceclonenv\lib\site-packages\librosa\core\audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.
  warnings.warn(""PySoundFile failed. Trying audioread instead."")
D:\tesis2\Real-Time-Voice-Cloning\synthesizer\preprocess.py:72: RuntimeWarning: invalid value encountered in true_divide
  wav = wav / np.abs(wav).max() * hparams.rescaling_max
tux100h-cvcorpus:   0%|                                                                                                 | 0/1 [44:54:59<?, ?speakers/s]
multiprocessing.pool.RemoteTraceback: 
""""""
Traceback (most recent call last):
  File ""D:\Programas\Python3.7\lib\multiprocessing\pool.py"", line 121, in worker
    result = (True, func(*args, **kwds))
  File ""D:\tesis2\Real-Time-Voice-Cloning\synthesizer\preprocess.py"", line 88, in preprocess_speaker
    skip_existing, hparams))
  File ""D:\tesis2\Real-Time-Voice-Cloning\synthesizer\preprocess.py"", line 219, in process_utterance
    mel_spectrogram = audio.melspectrogram(wav, hparams).astype(np.float32)
  File ""D:\tesis2\Real-Time-Voice-Cloning\synthesizer\audio.py"", line 60, in melspectrogram
    D = _stft(preemphasis(wav, hparams.preemphasis, hparams.preemphasize), hparams)
  File ""D:\tesis2\Real-Time-Voice-Cloning\synthesizer\audio.py"", line 121, in _stft
    return librosa.stft(y=y, n_fft=hparams.n_fft, hop_length=get_hop_size(hparams), win_length=hparams.win_size)
  File ""D:\tesis2\voiceclonenv\lib\site-packages\librosa\core\spectrum.py"", line 217, in stft
    util.valid_audio(y)
  File ""D:\tesis2\voiceclonenv\lib\site-packages\librosa\util\utils.py"", line 310, in valid_audio
    raise ParameterError(""Audio buffer is not finite everywhere"")
librosa.util.exceptions.ParameterError: Audio buffer is not finite everywhere
""""""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""synthesizer_preprocess_audio.py"", line 59, in <module>
    preprocess_dataset(**vars(args))
  File ""D:\tesis2\Real-Time-Voice-Cloning\synthesizer\preprocess.py"", line 35, in preprocess_dataset
    for speaker_metadata in tqdm(job, datasets_name, len(speaker_dirs), unit=""speakers""):
  File ""D:\tesis2\voiceclonenv\lib\site-packages\tqdm\std.py"", line 1180, in __iter__
    for obj in iterable:
  File ""D:\Programas\Python3.7\lib\multiprocessing\pool.py"", line 748, in next
    raise value
librosa.util.exceptions.ParameterError: Audio buffer is not finite everywhere
```",found voice similar voice may problem multiple got following error python valid unable import package noise removal warn unable import package noise removal true true valid data unable import package noise removal warn unable import package noise removal unable import package noise removal warn unable import package noise removal unable import package noise removal warn unable import package noise removal unable import package noise removal warn unable import package noise removal unable import package noise removal warn unable import package noise removal unable import package noise removal warn unable import package noise removal trying instead trying instead trying instead trying instead invalid value recent call last file line worker result true file line file line file line file line return file line file line raise audio buffer finite everywhere audio buffer finite everywhere exception direct cause following exception recent call last file line module file line job file line iterable file line next raise value audio buffer finite everywhere,issue,negative,negative,negative,negative,negative,negative
977521557,"Thank you so much for your help.

On Wed, 24 Nov 2021, 1:57 am blue-fish, ***@***.***> wrote:

> Closed #903
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/903>.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/903#event-5662936952>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AWUGWODUW3XHDXCBQW3FVJLUNP53JANCNFSM5IUBZ5QA>
> .
> Triage notifications on the go with GitHub Mobile for iOS
> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>
> or Android
> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.
>
>
",thank much help wed wrote closed thread reply directly view triage go mobile android,issue,positive,positive,neutral,neutral,positive,positive
977162994,"Synthesizer model fine-tuning is not a supported function of this repo, so I am going to close this issue and any similar ones that are opened in the future. However, I will make one quick comment to help point you in the right direction. Pay attention to this part of the screen output:

```
Loading weights at synthesizer\saved_models\my_run\my_run.pt
Tacotron weights loaded from step 0
```

Make sure that the name you are assigning to the run (here `my_run`) has a pretrained model located at `synthesizer\saved_models\<name_of_run>`. If it can't find a model, it trains a new one from scratch. That is what is happening here, with a new model initialized at step 0.

Try copying the `synthesizer\saved_models\pretrained\pretrained.pt` over to `synthesizer\saved_models\my_run\my_run.pt` and start the training again. You should see weights initialized at step 295000.",synthesizer model function going close issue similar future however make one quick comment help point right direction pay attention part screen output loading loaded step make sure name run model ca find model new one scratch happening new model step try start training see step,issue,positive,positive,positive,positive,positive,positive
976742461,"Okay super! I have currently chosen to put all 4 parts in the while loop, so I will continue to work on that as it is also interesting for several participants. Most of it is done, but I'm still thinking about a quit option to quit the program, since pressing a button doesn't seem that interesting in a conversation with the program.
Huge thanks for the help!",super currently chosen put loop continue work also interesting several done still thinking quit option quit program since pressing button seem interesting conversation program huge thanks help,issue,positive,positive,positive,positive,positive,positive
976731872,"All 4 parts in the [program outline](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/890#issuecomment-968402829) can be inside the while loop. This will cause the speaker embedding to be generated on each iteration using the latest voice recording. It might be a nice feature so the program does not need to be restarted when switching between different human participants.

If this is **not** what you want, you can prevent the embed from being updated, by initializing it to `None` and running the speaker encoder if it doesn't already exist. It would look something like this:

```
# This goes in the setup portion of the script
embed = None

while program_is_running:
    ## 1. Record the user with the computer's microphone
    original_wav = sd.rec(duration*sampling_rate, sampling_rate, 1)

    ## 2. Use automatic speech recognition to determine what the user said
    ## 3. Input to GPT-2 and get a response (text)

    ## 4. Provide text and voice recording to Real-Time-Voice-Cloning to generate audio in the user's voice and play it on speakers or headphones
    if not embed:    # This makes it run only on the first iteration of the while loop
        preprocessed_wav = encoder.preprocess_wav(original_wav, sampling_rate)
        embed = encoder.embed_utterance(preprocessed_wav)
```

If you have been following the development steps in order, you should have a working speech recognition and voice cloning, which both start by loading a wav file from disk into memory. The code I provided **records the user's voice directly into memory** so you do not need to check for a wav_path or create any files.",program outline inside loop cause speaker iteration latest voice recording might nice feature program need switching different human want prevent embed none running speaker already exist would look something like go setup portion script embed none record user computer microphone duration use automatic speech recognition determine user said input get response text provide text voice recording generate audio user voice play embed run first iteration loop embed following development order working speech recognition voice start loading file disk memory code provided user voice directly memory need check create,issue,positive,positive,positive,positive,positive,positive
976689395,"I would also like to see if ""pointwise operations"" can benefit from the [PyTorch JIT](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#fuse-pointwise-operations). Are there such operations ?",would also like see pointwise benefit,issue,positive,neutral,neutral,neutral,neutral,neutral
976672570,Ok it works (well actually it does not look like it does not work). It does not bring many more steps / s but it should follow their [recipe](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html?highlight=device#pre-allocate-memory-in-case-of-variable-input-length). I am not sure how to test it indeed.,work well actually look like work bring many follow recipe sure test indeed,issue,positive,positive,positive,positive,positive,positive
976665786,"First of all, thank you for replying so quickly!

If I understand correctly I should put this part in the beginning of my program: 

`original_wav = sd.rec(duration*sampling_rate, sampling_rate, 1)`

, along with the encoding for duration and sample_rate?
I was wondering if I didn't need an if statement that checks if the wav_path already exists and if so creates a new file so that it can run in the large while loop of the program? Or does the voice_cloning only need 1 sample wav to reconstruct the sound?

Thank you in advance!",first thank quickly understand correctly put part beginning program duration along duration wondering need statement already new file run large loop program need sample reconstruct sound thank advance,issue,positive,positive,positive,positive,positive,positive
976627433,"Here is the embedding code from [demo_rtvc.py](https://gist.github.com/blue-fish/ecabbca4f1a69701d32852f9f446c077) with comments added.
```
# This part loads a wav file from disk
wav_path = ""samples/p240_00000.mp3""
original_wav, sampling_rate = librosa.load(wav_path)

# This part creates the embedding
preprocessed_wav = encoder.preprocess_wav(original_wav, sampling_rate)
embed = encoder.embed_utterance(preprocessed_wav)
```

Now that you're ready to record the user through the microphone, the first part isn't needed. However, you still need to provide `original_wav` and `sampling_rate`. The sample rate is user-specified. The wav comes from sounddevice's rec() function.

For recording, you only need a [single line of code](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/toolbox/ui.py#L219). Here is a minimal example that uses it.

```python
# Add these imports as needed at the top of your program
import sounddevice as sd
from tqdm import trange
from time import sleep, time

# Choose a sample rate that is compatible with your hardware
sampling_rate = 44100
duration = 5  # seconds

# Start recording the user
original_wav = sd.rec(duration*sampling_rate, sampling_rate, 1)

# Display a progress bar while recording
# This also blocks the next part of the code from running before recording is complete
for i in trange(duration):
    sleep(1)

# This part creates the embedding
preprocessed_wav = encoder.preprocess_wav(original_wav, sampling_rate)
embed = encoder.embed_utterance(preprocessed_wav)
```

If you don't want to record for a fixed duration, you can replace the ""display a progress bar"" section with this code.

```python
# Get current time to keep track of recording length
start_time = time()
print(""Press enter to stop recording"")
input()  # This blocks the program from continuing until user presses enter

# Trim wav to actual length of recording
recording_length = time() - start_time
if recording_length < duration:
    original_wav = original_wav[:int(recording_length*sampling_rate)]
```",code added part file disk part embed ready record user microphone first part however still need provide sample rate come function recording need single line code minimal example python add top program import import time import sleep time choose sample rate compatible hardware duration start recording user duration display progress bar recording also next part code running recording complete duration sleep part embed want record fixed duration replace display progress bar section code python get current time keep track recording length time print press enter stop recording input program user enter trim actual length recording time duration,issue,positive,positive,neutral,neutral,positive,positive
976509804,Try it without the `with torch.no_grad()`. Just make sure the code doesn't include `optimizer.step()`.,try without make sure code include,issue,negative,positive,positive,positive,positive,positive
976461731,"hey

I still have 1 structural problem with my project that I cannot solve. In order for the Voice_cloner to work I have to enter a prerecorded wav sample in this code.

`wav_path = ""samples/p240_00000.mp3""
original_wav, sampling_rate = librosa.load(wav_path)
preprocessed_wav = encoder.preprocess_wav(original_wav, sampling_rate)
embed = encoder.embed_utterance(preprocessed_wav)
print(""Created the embedding"")`


Currently my code looks like this, but it doesn't work. 

`
import speech_recognition as sr
import sounddevice as sd
import numpy as np
import os
from scipy.io.wavfile import write
from time import sleep
import soundfile as sf
import matplotlib.pyplot as plt
from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
from matplotlib.figure import Figure
from PyQt5.QtCore import Qt, QStringListModel
from PyQt5.QtWidgets import *
from pathlib import Path
from typing import List, Set
import umap
import sys
import pyaudio
import wave
from pathlib import Path
import numpy as np
import soundfile as sf
import librosa
import argparse
import torch
from audioread.exceptions import NoBackendError

colormap = np.array([
    [0, 127, 70],
    [255, 0, 0],
    [255, 217, 38],
    [0, 135, 255],
    [165, 0, 165],
    [255, 167, 255],
    [97, 142, 151],
    [0, 255, 255],
    [255, 96, 38],
    [142, 76, 0],
    [33, 0, 127],
    [0, 0, 0],
    [183, 183, 183],
    [76, 255, 0],
], dtype=np.float) / 255 


def record_one(self, sample_rate, duration):
    
    self.log(""Recording %d seconds of audio"" % duration)
    sd.stop()
    try:
        wav = sd.rec(duration * sample_rate, sample_rate, 1)
    except Exception as e:
        print(e)
        self.log(""Could not record anything. Is your recording device enabled?"")
        return None
        
    for i in np.arange(0, duration, 0.1):
        self.set_loading(i, duration)
        sleep(0.1)
    self.set_loading(duration, duration)
    sd.wait()
        
    self.log(""Done recording."")

    wav_file_name = r""C:\Users\thoma\OneDrive\Documenten\Master\Scriptie\AI Writer\Final\Final_mapje\Speakerrecordings\recorded_voice.wav""
    if os.path.isfile(wav_file_name):
        expand = 1
        while True:
            expand += 1
            new_wav_file_name = wav_file_name.split("".wav"")[0] + str(expand) + "".wav""
            if os.path.isfile(new_wav_file_name):
                continue
            else:
                wav_file_name = new_wav_file_name
                break

    voice_file = write(wav_file_name, sample_rate, wav.astype(np.float32))  # Save as WAV file

    #return wav.squeeze()
    return voice_file

while(1):
    voice_file = record_one() # get the voice file
`

I was wondering if it was possible to set the sample_rate and duration yourself or if they depend on the other encodings? Is it possible to help me fix this issue?",hey still structural problem project solve order work enter sample code embed print currently code like work import import import import o import write time import sleep import import import import figure import import import path import list set import import import import wave import path import import import import import torch import self duration recording audio duration try duration except exception print could record anything recording device return none duration duration sleep duration duration done recording expand true expand expand continue else break write save file return return get voice file wondering possible set duration depend possible help fix issue,issue,positive,positive,neutral,neutral,positive,positive
976174580,"Thanks @blue-fish . Yet `Only Tensors of floating point and complex dtype can require gradients` appears when passing `requires_grad=true`. If I also pass `dtype=torch.floating32` to get floating point as required in the error, it yields `Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)`. What can be done ?",thanks yet floating point complex require passing also pas get floating point error tensor argument one following scalar long got instead done,issue,negative,negative,neutral,neutral,negative,negative
975955729,"> 1. How to generate the `stop` in `stop[j, :int(dataset.metadata[k][4])-1] = 0` ?

It only needs to be the correct size, so you can do this.
```
stop = torch.ones(batch_size, hparams.max_mel_frames)
```

> > RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
> > What should be done ?

Have you tried passing `requires_grad=True` when calling torch.randint and torch.rand?",generate stop stop need correct size stop element require grad done tried passing calling,issue,negative,neutral,neutral,neutral,neutral,neutral
975602573,"I am a bit stuck :
1. How to generate the `stop` in `stop[j, :int(dataset.metadata[k][4])-1] = 0` ?
2. I get the following runtime error while calling `loss.backward()` in the `with torch.no_grad()` block
> RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
What should be done ?
",bit stuck generate stop stop get following error calling block element require grad done,issue,negative,neutral,neutral,neutral,neutral,neutral
975393395,After 4 hours the process is not running anymore. [TPU use](https://huggingface.co/blog/pytorch-xla) looks more complicated than my first approach.,process running use complicated first approach,issue,negative,negative,negative,negative,negative,negative
975212356,"The symbol `’` has many names. The unicode name is ""right single quotation mark"", but it is sometimes called a curved quote, curly quote, or smart quote.",symbol many name right single quotation mark sometimes curved quote curly quote smart quote,issue,negative,positive,positive,positive,positive,positive
975165878,"Oh no, it is actually working. But after 17 minutes not a single step has been computed!",oh actually working single step,issue,negative,negative,neutral,neutral,negative,negative
975162724,"So I tried to use the TPU but it is not trivial now. It looks like the training process stops on the first [forward pass](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/7432046efc23cabf176f9fdc8d2fd67020059478/synthesizer/train.py#L178 ) 

Maybe the problem stems from XLA using PyTorch 1.9 whereas I was using PyTorch 1.10 so far.",tried use trivial like training process first forward pas maybe problem whereas far,issue,negative,positive,positive,positive,positive,positive
975139496,"Thank you @blue-fish you spotted it! Indeed in Siwis dataset, they use both apostroph signs (actually  `'` around 100 times, and  `’` 6k+ times). `convert_to_ascii` does not keep accented letters so I'll add it to `symbols.py` and use the `basic_cleaner` as recommended [by tacotron team](https://github.com/keithito/tacotron/blob/master/TRAINING_DATA.md).

By the way what's the name of `’` and how do you type it (I had to paste it from you message inside `symbols.py`!) ?",thank spotted indeed use actually around time time keep add use team way name type paste message inside,issue,negative,neutral,neutral,neutral,neutral,neutral
975092869,Great! Thanks a lot @blue-fish ! With your guide I should be able to do it!,great thanks lot guide able,issue,positive,positive,positive,positive,positive,positive
974894739,"@Ca-ressemble-a-du-fake Thank you for sharing the observations, and submitting pull request #895 which resolves this issue. I expect there is some performance optimization possible, and I would appreciate any assistance you can provide in locating the bottlenecks.",thank pull request issue expect performance optimization possible would appreciate assistance provide,issue,positive,neutral,neutral,neutral,neutral,neutral
974863999,"> Thanks @blue-fish . And is it expected to have more ""_"" (called `_pad` in `symbols.py`) than characters in the input ? Eg :
> 
> > Input at step 14000: nul ne doute quil faille muscler la réforme bancaire.~___________________________________________________________________________________________________________________________________________________
> 
> The input is 53 chars long whereas there are 147 ""_"" after ~

Text padding with `_` is used so all inputs in the batch have the same length. This allows the texts to be batched together and processed at the same time.


> One more thing. As you can see the ' (apostrophe) is filtered out (expected should be ""qu**'**il"" instead of ""quil""). I am not sure it is an issue because apostrophe has no particular sound but it should be present since it is in `symbols.py` :
> 
> `_characters = ""ABCDEFGHIJKLMNOPQRSTUVWXYZŸÜÛÙŒÔÏÎËÊÈÉÇÆÂÀabcdefghijklmnopqrstuvwxyzàâæçéèêëîïôœùûüÿ!\'\""(),-.:;? ""`
> 
> If I just run a test against this pipeline the apostrophe is kept, so it looks like the apostrophe is removed somewhere else than in the cleaners. Can you tell me if my cleaner works correctly regarding the apostrophe ?

What is most likely going on here, is your apostrophe in `train.txt` is using the `’` character. Notice how it is different from the apostrophe in symbols.py. If you use basic_cleaners, it will be missing the `convert_to_ascii` step which was converting `’` to `'`. When [symbols_to_sequence](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/7432046efc23cabf176f9fdc8d2fd67020059478/synthesizer/utils/text.py#L32) encounters a character that is not in symbols, it is dropped.

If there is a problem that prevents you from using convert_to_ascii, then you should either find-and-replace `’` to `'` in train.txt, or update your symbols.py to include `’`.
",thanks input input step nul ne faille la input long whereas text padding used batch length together time one thing see apostrophe instead sure issue apostrophe particular sound present since run test pipeline apostrophe kept like apostrophe removed somewhere else tell cleaner work correctly regarding apostrophe likely going apostrophe character notice different apostrophe use missing step converting character problem either update include,issue,positive,positive,positive,positive,positive,positive
974861582,There is no simple fix that will make this script perform singing voice synthesis. You need to find a different repo.,simple fix make script perform singing voice synthesis need find different,issue,negative,neutral,neutral,neutral,neutral,neutral
974861097,"> 1. generate a batch of inputs ?

```
texts = torch.randint(len(symbols), (batch_size, 200))  # replace 200 with desired max length
mels = torch.rand((batch_size, hparams.num_mels, hparams.max_mel_frames))
embeds = torch.rand((batch_size, hparams.speaker_embedding_size))
```

> 2. should I run train.py ? And how can I prevent the ""execution of an optimizer or a learning rate scheduler"" ?

You should perform [these operations](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/7432046efc23cabf176f9fdc8d2fd67020059478/synthesizer/train.py#L162-L188) once with the random inputs. It should be in a block `with torch.no_grad():`

> 4. I will run this function just before the actual training, right ?

Insert the new code [here](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/7432046efc23cabf176f9fdc8d2fd67020059478/synthesizer/train.py#L117).

",generate batch replace desired length run prevent execution learning rate perform random block run function actual training right insert new code,issue,negative,negative,neutral,neutral,negative,negative
974811826,Thanks @Ontopic for sharing your experience with XLA. That's too bad if it is hard to set up. Looking forward to reading more about your tests ! ,thanks experience bad hard set looking forward reading,issue,negative,negative,negative,negative,negative,negative
974792704,"One more thing regarding the use of `np.array` instead of `list` that I've just noticed while testing it on a Tesla P100 (16GB) GPU. With `r=7` and a batch_size of 64, the ""list version"" stops training after 84 steps while the ""array version"" keeps training. Memory usage amounts to around 14,5 GB.

With the ""list version"" a batch size of 20 was near the maximum I could reach with a P100 and [reportedly](https://github.com/coqui-ai/TTS/blob/main/docs/source/faq.md#how-can-i-check-model-performance) model convergence is hard to reach with a batch size less than 32. So this is really cool.

**Bottom line : using array is really worth it**!",one thing regarding use instead list testing list version training array version training memory usage around list version batch size near maximum could reach reportedly model convergence hard reach batch size le really cool bottom line array really worth,issue,positive,positive,positive,positive,positive,positive
974790093,I haven't noticed your reply so far. I'll do it. I have discovered interesting thing also in between.,reply far discovered interesting thing also,issue,negative,positive,positive,positive,positive,positive
974759885,"I launched some basic benchmarks because I did not understand why moving to `np.array` did not lead to better performance overall (higher steps / s). So I measured the time to create the speaker embedding `embeds` as a list and as an array :

`embeds = [x[2] for x in batch]` VS `embeds = np.array([x[2] for x in batch])`

On average array creation is 5.5 times slower than list creation.

Then I also measured the time to convert `embeds` as a tensor.

`embeds = torch.tensor(embeds)`

On average conversion from array to tensor was 12 times quicker than conversion from list to tensor.

**Bottom line : it is worth it using array.**

But also, as the ""steps / s"" rate does not increase, it means the bottleneck lies somewhere else.
@blue-fish  Should I further continue finding the bottleneck and improving the performance ?


",basic understand moving lead better performance overall higher measured time create speaker list array batch batch average array creation time list creation also measured time convert tensor average conversion array tensor time conversion list tensor bottom line worth array also rate increase bottleneck somewhere else continue finding bottleneck improving performance,issue,positive,positive,positive,positive,positive,positive
974706301,"I was able to run it on a TPU, but I'm not sure if I actualy used the TPU. I had a small dataset for some preliminary testing, so didn't explore any potential speed upgrades. I would unless it's not running not mess with the devices in a ""strange codebase"". XLA is probably the module I had the most issues ever with in my entire life of development (the versioning and different environments and everything being more cuda oriented, can become tough to find matching versions that play nice.

Still, for large scale training I'd like to know as well. Not expecting an answer here tbh (little experience with TPU) But let's post our findings here. Hope to get some tests in later today, will post what I find.",able run sure used small preliminary testing explore potential speed would unless running mess strange probably module ever entire life development different everything become tough find matching play nice still large scale training like know well answer little experience let post hope get later today post find,issue,positive,positive,neutral,neutral,positive,positive
974681189,`symbols.py` is still showing up as a changed file. You need to restore it to its original state. We can't merge that change because it would break compatibility with the pretrained model.,still showing file need restore original state ca merge change would break compatibility model,issue,positive,positive,positive,positive,positive,positive
974638238,"Thanks @blue-fish . And is it expected to have more ""_"" (called `_pad` in `symbols.py`) than characters in the input ? Eg :

> Input at step 14000: nul ne doute quil faille muscler la réforme bancaire.~___________________________________________________________________________________________________________________________________________________

The input is 53 chars long whereas there are 147 ""_"" after ~

One more thing. As you can see the ' (apostrophe) is filtered out (expected should be ""qu**'**il"" instead of ""quil""). I am not sure it is an issue because apostrophe has no particular sound but it should be present since it is in `symbols.py` :

`_characters = ""ABCDEFGHIJKLMNOPQRSTUVWXYZŸÜÛÙŒÔÏÎËÊÈÉÇÆÂÀabcdefghijklmnopqrstuvwxyzàâæçéèêëîïôœùûüÿ!\'\""(),-.:;? ""`

If I just run a test against this pipeline the apostrophe is kept, so it looks like the apostrophe is removed somewhere else than in the cleaners. Can you tell me if my cleaner works correctly regarding the apostrophe ?",thanks input input step nul ne faille la input long whereas one thing see apostrophe instead sure issue apostrophe particular sound present since run test pipeline apostrophe kept like apostrophe removed somewhere else tell cleaner work correctly regarding apostrophe,issue,positive,positive,positive,positive,positive,positive
974605208,"My bad, I forgot all lower case consonants in symbols.py. ",bad forgot lower case,issue,negative,negative,negative,negative,negative,negative
974597669,"According to [PyTorchXLA documentation](https://pytorch.org/xla/release/1.9/index.html#) there is only a ""couple lines"" to convert vanilla PyTorch code to TPU capable code :

> This snippet highlights how easy it is to switch your model to run on XLA. The model definition, dataloader, optimizer and training loop can work on any device. The only XLA-specific code is a couple lines that acquire the XLA device and mark the step. Calling xm.mark_step() at the end of each training iteration causes XLA to execute its current graph and update the model’s parameters.",according documentation couple convert vanilla code capable code snippet easy switch model run model definition training loop work device code couple acquire device mark step calling end training iteration execute current graph update model,issue,positive,positive,positive,positive,positive,positive
974486742,"> THere is no `~` in train.txt so where do all those ~ come from ?

It is the end of sequence marker. It helps train the stop prediction.
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/7432046efc23cabf176f9fdc8d2fd67020059478/synthesizer/utils/text.py#L38-L39",come end sequence marker train stop prediction,issue,negative,neutral,neutral,neutral,neutral,neutral
974435790,"I tried `basic_cleaners` but I read weird inputs in the console :

> 
+----------------+------------+---------------+------------------+
| Steps with r=7 | Batch Size | Learning Rate | Outputs/Step (r) |
+----------------+------------+---------------+------------------+
|   20k Steps    |     20     |     0.001     |        7         |
+----------------+------------+---------------+------------------+
 
{| Epoch: 2/44 (39/461) | Loss: 1.054 | 0.99 steps/s | Step: 0k | }**Input at step 500:     ,     é,   é  -ê     û   .~___________**
{| Epoch: 3/44 (78/461) | Loss: 0.9771 | 1.0 steps/s | Step: 1k | }**Input at step 1000: -   éé,  éà .~___________**
{| Epoch: 4/44 (117/461) | Loss: 0.9348 | 1.0 steps/s | Step: 1k | }**Input at step 1500:    é     éê.~__________________________**
{| Epoch: 5/44 (156/461) | Loss: 0.8881 | 1.0 steps/s | Step: 2k | }**Input at step 2000:          ,       . ~_____________________**
{| Epoch: 6/44 (195/461) | Loss: 0.8642 | 1.0 steps/s | Step: 2k | }**Input at step 2500:  -à ,  é       , î    ,      é.~**
{| Epoch: 7/44 (234/461) | Loss: 0.8479 | 1.0 steps/s | Step: 3k | }**Input at step 3000:   ,      è     .~__________________________**
...
{| Epoch: 10/44 (351/461) | Loss: 0.8224 | 1.0 steps/s | Step: 4k | }**Input at step 4500:  é é      é    ê    â**    

Previously, with the english_cleaners the inputs were legible (only accented letters were removed). When I test the cleaners with an input, they return the expected output eg :
 
`print(basic_cleaners(""J’attendais, suivant la coutume, que la phrase quotidienne fût prononcée.""))`
> j’attendais, suivant la coutume, que la phrase quotidienne fût prononcée.

`print(english_cleaners(""J’attendais, suivant la coutume, que la phrase quotidienne fût prononcée.""))`
> j'attendais, suivant la coutume, que la phrase quotidienne fut prononcee.

THere is no `~` in train.txt so where do all those ~ come from ?

What is actually going on, will it harm the training ?
",tried read weird console batch size learning rate epoch loss step input step epoch loss step input step epoch loss step input step epoch loss step input step epoch loss step input step epoch loss step input step epoch loss step input step previously legible removed test input return output print la la phrase la la phrase print la la phrase la la phrase fut come actually going harm training,issue,negative,negative,neutral,neutral,negative,negative
974081087,By the way is there a way to test and insure the cleaner works as expected without training ? ,way way test insure cleaner work without training,issue,negative,neutral,neutral,neutral,neutral,neutral
974014178,"> DirectML doesn't support Pytorch at this time. [microsoft/DirectML#52](https://github.com/microsoft/DirectML/issues/52)
> 
> You could spend hours converting the Pytorch model to ONNX, and then run the ONNX model with DirectML. This kind of request is outside the scope of what the current set of developers can provide support for. If you or anyone else is able to get it to work, please share. For now, you can still run this repo on CPU.

https://pypi.org/project/pytorch-directml/",support time could spend converting model run model kind request outside scope current set provide support anyone else able get work please share still run,issue,positive,positive,positive,positive,positive,positive
973787352,"For the records, I benchmarked both versions this morning with a batch size of 16 and a reduction factor of 7 with english_cleanners, on a P100 PCIE. I tested it only for the first epoch.

- *Original* version : 0.82 steps / s
- *Np Array ""pre conversion*"" version : 0.80 steps / s

So the fix only removes the warning, and does not bring performance boost. Anyway I'll submit a pull request.

Please note: the above mentionned issue with Cuda disappeared when batch size was decreased down to 20.",morning batch size reduction factor tested first epoch original version array conversion version fix warning bring performance boost anyway submit pull request please note issue batch size,issue,positive,positive,positive,positive,positive,positive
973299998,"How much do you think a model needs to be trained?  for example on a LibriTTS dataset consisting of 460 hours of audio data.  perhaps it is a certain number of steps or some value of the error function, at which the output will be good.  What else do you think about LibriTTS data processing?  it might be worth changing the current pipeline.  If my results are good, I can share the model weights for a sampling rate of 22050.

I am trying to train a synthesizer for further work with the HiFiGAN vocoder.",much think model need trained example audio data perhaps certain number value error function output good else think data might worth current pipeline good share model sampling rate trying train synthesizer work,issue,positive,positive,positive,positive,positive,positive
973286205,"I have been training since Monday on tux100h dataset for approximately 32-50h the model for the synthetizer. It has been saved with 50k steps, but I have stopped the training in 57k steps, there's a loss of 0.21-0.24 approximately. However, I have tried to clone my voice using that model in the demo_cli.py and the output does sound like a human, but it sounds like one from the dataset, not like my voice, which is the input. Any recommendations? I am using encoder and vocoder pre-trained models given in the repo.",training since approximately model synthetizer saved stopped training loss approximately however tried clone voice model output sound like human like one like voice input given,issue,positive,negative,neutral,neutral,negative,negative
973222428,"Thank you very much! I will give the basic cleaners a try. Your answer is very valuable, I understand better how the text processing works.",thank much give basic try answer valuable understand better text work,issue,positive,positive,positive,positive,positive,positive
973200864,"Here is how a different TTS does it for French. They train on phonemes instead of graphemes like this repo does.

https://github.com/coqui-ai/TTS/blob/33aa27e2d6a80d38a7ea8928ad14d032d2c53a40/TTS/tts/utils/text/cleaners.py#L103-L110

https://github.com/coqui-ai/TTS/blob/33aa27e2d6a80d38a7ea8928ad14d032d2c53a40/TTS/tts/utils/text/abbreviations.py#L29",different train instead like,issue,negative,neutral,neutral,neutral,neutral,neutral
973184182,"You should start with `basic_cleaners` and add functionality to handle numerals and abbreviations.

There is no need to rerun preprocess after updating cleaners. The cleaners intercept and replace text in real time as it is input to the model.",start add functionality handle need rerun intercept replace text real time input model,issue,negative,positive,positive,positive,positive,positive
973180256,"> My Mel-spectrograms look very bad, what can this be due to?

I have been comparing my LibriSpeech training plots to Taco2 with the old Tensorflow repo and quality is the same. When compared to Taco1, the training mels look comparatively worse. But Taco2 inference quality is still good.",look bad due training old quality training look comparatively worse inference quality still good,issue,negative,negative,neutral,neutral,negative,negative
973172555,"We should still fix it, to avoid confusing the user. Please submit a pull request.",still fix avoid user please submit pull request,issue,negative,neutral,neutral,neutral,neutral,neutral
973159766,On my GPU it worked like a charm but there were no noticeable performance improvement. Only the warning went away.,worked like charm noticeable performance improvement warning went away,issue,positive,neutral,neutral,neutral,neutral,neutral
972846955,"While doing this I came across this :
`| Epoch: 3/18 (207/288) | Loss: 0.5240 | 1.4 steps/s | Step: 15k | }Traceback (most recent call last):
  File ""synthesizer_train.py"", line 35, in <module>
    train(**vars(args))
  File ""/content/Real-Time-Voice-Cloning/synthesizer/train.py"", line 192, in train
    if np.isnan(grad_norm.cpu()):
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.`

And now this error comes every time I launch the training. I tried to revert the change but the error still appears. So it may not be linked to the `np.array` conversion but to the GPU (since it degraded to T100 instead of V100 initially). So I have to solve this first before doing the correction mentioned above.  ",came across epoch loss step recent call last file line module train file line train error illegal memory access kernel might call might incorrect consider passing error come every time launch training tried revert change error still may linked conversion since degraded instead initially solve first correction,issue,negative,negative,neutral,neutral,negative,negative
972642687,"This is what I have at the moment. I am also attaching the hyperparameters of training. Is there any way you could comment on the learning process?
[hparams.txt](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/7561228/hparams.txt)
I can't attach the archive as a file (some internal error on github), so I'm sharing a link to google drive where you can download it
[59500_steps.tar.gz](https://drive.google.com/file/d/1znYM_WHRbQZb9AjADZXbP9UJ_is2t-Gx/view?usp=sharing)
My Mel-spectrograms look very bad, what can this be due to?",moment also training way could comment learning process ca attach archive file internal error link drive look bad due,issue,negative,negative,negative,negative,negative,negative
972594422,I'll do it on my fork and report here before committing on master (it's my first contribution on github). ,fork report master first contribution,issue,negative,positive,positive,positive,positive,positive
972576271,"It's not intentional. I had to rewrite the dataloader for synthesizer training in #472, but didn't optimize the code since there were so many other things to be done.

Would you please fix it and benchmark the training speed before and after making the change?",intentional rewrite synthesizer training optimize code since many done would please fix training speed making change,issue,positive,positive,positive,positive,positive,positive
971968523,"Just a follow-up on that topic. I tried to train the model with SIWIS dataset in French on Google Colab. 
Environment is as follows :

- Drivers : `NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2`
- CPU is `Intel(R) Xeon(R) CPU @ 2.30GHz` with 2 processors and around 13GB of RAM.

While training with **batch size of 32 and r = 7,** I get on 
- K80 (around 11GB VRAM - Kepler arch from 2014) : **0,40 - 0,45 steps / s** (I tried both Pytorch installs with CUDA 10.2 and CUDA 11.3 the latter being a little bit slower). 
- V100-SXM2 (around 16GB VRAM  - Volta arch from 2017) : **1.4 steps / s** (default install with Cuda 10.2)

I am quite surprised it is not way faster on such high end GPU (around 3 times faster than on my old setup [except for the GPU]).

Is there a way to know if code is dealing with floating point 16 or 32 operations, because I read somewhere that one mode brought a performance boost ? For now it is way too far from my current understanding of the topic!

",topic tried train model environment driver version version around ram training batch size get around arch tried latter little bit around arch default install quite way faster high end around time faster old setup except way know code dealing floating point read somewhere one mode brought performance boost way far current understanding topic,issue,negative,positive,neutral,neutral,positive,positive
971801798,Thanks for sharing the thought. The nuance is definitely interesting.,thanks thought nuance definitely interesting,issue,positive,positive,positive,positive,positive,positive
971767276,"You can also try all of your audio files and pick the one the gives the best results. It's not recommended to pass an extremely long input to the speaker encoder, because it will just average out the features.

> Curious, why does it say on #437 that it's no longer voice cloning?

When using pretrained synthesizer model, the voice you are cloning is unseen. By finetuning the synthesizer, you are allowing it to see the voice you want it to clone, and adjust model weights for better performance. This distinction might not be important to you as an end user.",also try audio pick one best pas extremely long input speaker average curious say longer voice synthesizer model voice unseen synthesizer see voice want clone adjust model better performance distinction might important end user,issue,positive,positive,positive,positive,positive,positive
971760142,"The training mels are good because the decoder in the Tacotron model is allowed to see the ground truth mel that it is trying to construct. We give it a frame of the real spectrogram, and ask it to predict the next frame. Then we give it the next ground truth frame, and ask it to predict another. This training technique is called teacher forcing.

For inference, we don't provide a ground truth mel, so the decoder is on its own. It starts from zero (called the ""GO frame"" in the papers) and synthesizes 1 frame. Without the ground truth, it always needs to use its last predicted mel to synthesize the next. It is not always numerically stable, and the predictions will eventually break down producing unintelligible output. This is why we recommend synthesizing one sentence at a time.

During inference, Tacotron relies on the attention mechanism to tell it which part of the text input to synthesize at each step. Attention is very important for inference, so you'll want to make sure it was trained well. The alignment plots during training should contain a monotonic line.",training good model see ground truth mel trying construct give frame real spectrogram ask predict next frame give next ground truth frame ask predict another training technique teacher forcing inference provide ground truth mel zero go frame frame without ground truth always need use last mel synthesize next always numerically stable eventually break unintelligible output recommend one sentence time inference attention mechanism tell part text input synthesize step attention important inference want make sure trained well alignment training contain monotonic line,issue,positive,positive,positive,positive,positive,positive
971748456,"Ok. So, my options are then to concatenate a set of mp3 files together to create more audio for the speaker (and set a seed) or to use the finetuning method you linked to.

Curious, why does it say on #437 that it's no longer voice cloning?",concatenate set together create audio speaker set seed use method linked curious say longer voice,issue,positive,negative,neutral,neutral,negative,negative
971743229,"> I can see in the toolbox UI that you can record over and over to better the clone you've created.

Loading an audio in the toolbox does not train or alter the model in any way. It just changes the speaker embedding (an array of size (256,)) input to the synthesizer. Only the embedding from the last loaded audio is used during generation. Due to dropout, the synthesizer output is never the same for a given set of inputs (speaker embedding + text) **unless** the ""random seed"" parameter is set to force deterministic generation.

> For example, does `embed = encoder.embed_utterance(...)` maintain some ongoing state which I can save after I've embedded the 50 files I have?

The encoder is deterministic and doesn't learn when multiple samples are loaded. The toolbox output only depends on the last utterance loaded. Try setting ""random seed"" to a fixed number and you'll find the results will be consistent.

> I have a directory of 50-60 audio samples from the speaker I'm trying to clone.

For better results, you may wish to try the finetuning technique I describe in #437.

",see toolbox record better clone loading audio toolbox train alter model way speaker array size input synthesizer last loaded audio used generation due dropout synthesizer output never given set speaker text unless random seed parameter set force deterministic generation example embed maintain ongoing state save deterministic learn multiple loaded toolbox output last utterance loaded try setting random seed fixed number find consistent directory audio speaker trying clone better may wish try technique describe,issue,positive,positive,neutral,neutral,positive,positive
971733396,"Here's some of the code I've already written for loading the audio files.

```
## Get input mp3s
audiofiles = [os.path.join(args.sample_path, f) for f in listdir(args.sample_path) if isfile(join(args.sample_path, f))]
print(audiofiles)
    
embeds = []
for audiofile in audiofiles:
    in_fpath = Path(audiofile)

    # Load from filepath
    preprocessed_wav = encoder.preprocess_wav(in_fpath)

   # Then we derive the embedding. There are many functions and parameters that the 
   # speaker encoder interfaces. These are mostly for in-depth research. You will typically
   # only use this function (with its default parameters):
   embed = encoder.embed_utterance(preprocessed_wav)
   print(""Created the embedding"")
        
   embeds.push(embed)
```",code already written loading audio get input join print path load derive many speaker mostly research typically use function default embed print embed,issue,negative,positive,positive,positive,positive,positive
971540461,Just wanted to +100 that this discussion has been a very useful read for me and my project as well. Thanks @blue-fish !,discussion useful read project well thanks,issue,positive,positive,positive,positive,positive,positive
970515111,"@e0xextazy Please contact me here (on Github). Keep in mind that I only provide assistance for open-source projects, and only as my time and interest allow.",please contact keep mind provide assistance time interest allow,issue,positive,neutral,neutral,neutral,neutral,neutral
969436224,"No, those issues are unrelated. However, Pytorch recently added support for ROCm, so it may work with AMD cards now.",unrelated however recently added support may work,issue,negative,neutral,neutral,neutral,neutral,neutral
969367361,@blue-fish does that mean that we dont need cuda anymore and can use opencl and amd cards instead?,mean dont need use instead,issue,negative,negative,negative,negative,negative,negative
969303955,"This guide was written before #472 was merged and removed the requirement for Tensorflow 1.15. Installation should be a lot easier now.

I am closing this issue because these steps are obsolete. The repo can be set up without using a PPA to install an old version of Python. A new set of instructions is requested.",guide written removed requirement installation lot easier issue obsolete set without install old version python new set,issue,negative,positive,positive,positive,positive,positive
969108709,"Then I'll try to start a LibriTTS training tomorrow and I can keep you informed if you're interested.
P.S. But I want to teach a 22050 sample rate",try start training tomorrow keep informed interested want teach sample rate,issue,negative,positive,positive,positive,positive,positive
969106586,"> What do you want me to provide you with? I do not understand the request.

At the time, I wanted your code changes to add the speaker embedding. But after I finished checking the model with a single speaker dataset, I went ahead and made the code change myself in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/878#issuecomment-968587719 .

> One question that interests me is why are you going to use LibriSpeech and not LibriTTS? Are they different in some way?

LibriTTS is the better dataset, because the utterances are shorter and the transcripts contain punctuation. However, the punctuation does make it a little harder for the model to learn. I have already done a lot of experiments with LibriSpeech so I start with that one to benchmark the model against others I have developed.

> Dear blue-fish, could you give more advice on preparing data for training? Maybe you can combine some datasets together for better results and so on.
Can you tell me briefly what has been done and what else needs to be done in your opinion?

I am still trying to get a good TTS based on a single dataset: matching the results in the Tacotron 1/2 and SV2TTS papers. There is still a quality difference between our synthesizer and the one that Google demonstrated 3 years ago.",want provide understand request time code add speaker finished model single speaker went ahead made code change one question going use different way better shorter contain punctuation however punctuation make little harder model learn already done lot start one model dear could give advice data training maybe combine together better tell briefly done else need done opinion still trying get good based single matching still quality difference synthesizer one ago,issue,positive,positive,positive,positive,positive,positive
969062355,"> Both inference and training have been tested and work, but require CUDA for now. I am currently training a single-speaker model to make sure it can learn attention. After that, I will work on a pretrained model using LibriSpeech.

One question that interests me is why are you going to use LibriSpeech and not LibriTTS? Are they different in some way? ",inference training tested work require currently training model make sure learn attention work model one question going use different way,issue,negative,positive,positive,positive,positive,positive
968690622,Thank you very much! I think we can close the issue.,thank much think close issue,issue,negative,positive,positive,positive,positive,positive
968648002,"Dear blue-fish, could you give more advice on preparing data for training? Maybe you can combine some datasets together for better results and so on. 
Can you tell me briefly what has been done and what else needs to be done in your opinion?",dear could give advice data training maybe combine together better tell briefly done else need done opinion,issue,positive,positive,positive,positive,positive,positive
968636763,"> I have some exciting news to share, Nvidia's Tacotron2 has been integrated into my repo: https://github.com/blue-fish/Real-Time-Voice-Cloning/tree/878_tacotron2
> 
> It is almost done, it just needs some modifications in the decoder to account for the speaker embedding. @e0xextazy If you are able, would you please share the changes you made to save me some time?
> 
> Both inference and training have been tested and work, but require CUDA for now. I am currently training a single-speaker model to make sure it can learn attention. After that, I will work on a pretrained model using LibriSpeech.

What do you want me to provide you with? I do not understand the request.",exciting news share almost done need account speaker able would please share made save time inference training tested work require currently training model make sure learn attention work model want provide understand request,issue,positive,positive,positive,positive,positive,positive
968592570,Dropping this due to poor performance and lack of interest.,dropping due poor performance lack interest,issue,negative,negative,negative,negative,negative,negative
968407780,I cannot explain how much (thinking) work this saves me. This is very useful and clear! Thank you so much for helping me so hard!! I am always willing to share my final result if it would be interesting and useful.,explain much thinking work useful clear thank much helping hard always willing share final result would interesting useful,issue,positive,positive,positive,positive,positive,positive
968402829,"

> Is it possible to give me an overview of how the demo_cli.py (new version you sent) and the demo_rtvc.py should now work with the gpt-2 model.

The code in demo_rtvc.py is structured like this:

```
## Section 1: Setup
# Imports
# Load models
# Use encoder to make speaker embedding


## Section 2: Generation
# Use synthesizer to make mel spectrograms from text
# Use vocoder to make waveform audio from mels
# Play waveform audio
```

You put Section 1 into the beginning of [your script](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/890#issuecomment-968354706).

You have a while loop which uses GPT to make a variable called `text`. This needs to be split into a list of sentences called `texts`. After that, you can insert the code from Section 2 to generate and play back the audio.

> What should the structure look like as a whole?

To build the program I outlined, you have to figure out what each of the sections is doing. This will help you put the parts together.


```
## Setup
    # Put all the imports here.

## 1. Record the user with the computer's microphone
    # Inputs: None
    # Outputs: An audio waveform (as a numpy array)
    #
    # The code needs to initialize the input audio device and capture the audio.
    # This is how it is done in the toolbox:
    # https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/toolbox/ui.py#L219


## 2. Use automatic speech recognition to determine what the user said
    # Inputs: Audio waveform from step 1
    # Outputs: Text
    #
    # First, the code needs to set up your ASR package.
    # Next, pass the audio to ASR to get the user's transcribed speech.


## 3. Input to GPT-2 and get a response (text)
    # Inputs: Text (transcribed speech from user in step 2)
    # Outputs: Text
    #
    # Your script already does this.


## 4. Provide text and voice recording to Real-Time-Voice-Cloning to generate audio in the user's voice and play it on speakers or headphones
    # Inputs:  Audio waveform from step 1
    #.         Text (GPT output from step 3)
    #
    # Outputs: None
    #
    # Most of these functions are demonstrated in demo_rtvc.py
    # a. This section first creates a speaker embedding from the recording in step 1.
    # b. You may also want to break up the GPT text into individual sentences.
    # c. Input the embedding and text to the synthesizer, to get a mel spectrogram.
    # d. Give the mel spectrogram to the vocoder to get an audio waveform (as a numpy array)
    # e. Play back the audio through the computer's output sound device.
```

> My apologies for the many questions.

No need to apologize, but please understand that it takes away from my primary focus of improving the repo's codebase and model quality. I normally don't answer questions or provide help with personal projects, because I like to work on things that help all users. In this case, I made an exception because I remember what it is like to be new to programming and know that having a little guidance can make an impact.

Take some time to consider what I have provided here, and try to start building the program. Since I provided `demo_rtvc.py`, you can start at the end and work backwards. The development steps could look like this.

1. Run `demo_rtvc.py` in Python to make sure it is set up properly.
2. Add your GPT code, so the user will type in a prompt, and get an audio response played back through the computer speakers or headphones.
3. Find an ASR package. Do not use the microphone yet. Instead, load a prerecorded wav file and use that to generate the text prompt for GPT.
4. Finally, record the user's voice using the microphone. Use the recording in place of the prerecorded file for ASR. At this point you will be done!

Please try to make use of other support channels (Stack Overflow, etc.) whenever possible. And consider releasing your project as open source when it is done.",possible give overview new version sent work model code structured like section setup load use make speaker section generation use synthesizer make mel text use make audio play audio put section beginning script loop make variable text need split list insert code section generate play back audio structure look like whole build program outlined figure help put together setup put record user computer microphone none audio array code need initialize input audio device capture audio done toolbox use automatic speech recognition determine user said audio step text first code need set package next pas audio get user speech input get response text text speech user step text script already provide text voice recording generate audio user voice play audio step text output step none section first speaker recording step may also want break text individual input text synthesizer get mel spectrogram give mel spectrogram get audio array play back audio computer output sound device many need apologize please understand away primary focus improving model quality normally answer provide help personal like work help case made exception remember like new know little guidance make impact take time consider provided try start building program since provided start end work backwards development could look like run python make sure set properly add code user type prompt get audio response back computer find package use microphone yet instead load file use generate text prompt finally record user voice microphone use recording place file point done please try make use support stack overflow whenever possible consider project open source done,issue,positive,positive,positive,positive,positive,positive
968364848,"I've only been working with python for a few months, so I'm not very good at programming yet. You had previously given a good summary of what the program should look like as a whole. Namely like this: 

1. Record the user with the computer's microphone
2. Use automatic speech recognition to determine what the user said
3. Input to GPT-2 and get a response (text)
4. Provide text and voice recording to Real-Time-Voice-Cloning to generate audio in the user's voice and play it on speakers or headphones

Is it possible to give me an overview of how the demo_cli.py (new version you sent) and the demo_rtvc.py should now work with the gpt-2 model. What should the structure look like as a whole? My apologies for the many questions.",working python good yet previously given good summary program look like whole namely like record user computer microphone use automatic speech recognition determine user said input get response text provide text voice recording generate audio user voice play possible give overview new version sent work model structure look like whole many,issue,positive,positive,positive,positive,positive,positive
968361130,"Integrate this code with your script instead. It assumes you have the Real-Time-Voice-Cloning repository at the same level as your script.

https://gist.github.com/blue-fish/ecabbca4f1a69701d32852f9f446c077",integrate code script instead repository level script,issue,negative,neutral,neutral,neutral,neutral,neutral
968354706,"It's still a bit unclear to me. Below you will find the script for the interactive form of the gpt2 model that I use. How do I integrate your forwarded version into it?

`import fire
import json
import os
import numpy as np
import tensorflow as tf

import model, sample, encoder

def interact_model(
    model_name='124M',
    seed=None,
    nsamples=1,
    batch_size=1,
    length=None,
    temperature=1,
    top_k=0,
    top_p=1,
    models_dir='models',
):
    """"""
    Interactively run the model
    :model_name=124M : String, which model to use
    :seed=None : Integer seed for random number generators, fix seed to reproduce
     results
    :nsamples=1 : Number of samples to return total
    :batch_size=1 : Number of batches (only affects speed/memory).  Must divide nsamples.
    :length=None : Number of tokens in generated text, if None (default), is
     determined by model hyperparameters
    :temperature=1 : Float value controlling randomness in boltzmann
     distribution. Lower temperature results in less random completions. As the
     temperature approaches zero, the model will become deterministic and
     repetitive. Higher temperature results in more random completions.
    :top_k=0 : Integer value controlling diversity. 1 means only 1 word is
     considered for each step (token), resulting in deterministic completions,
     while 40 means 40 words are considered at each step. 0 (default) is a
     special setting meaning no restrictions. 40 generally is a good value.
     :models_dir : path to parent folder containing model subfolders
     (i.e. contains the <model_name> folder)
    """"""
    models_dir = os.path.expanduser(os.path.expandvars(models_dir))
    if batch_size is None:
        batch_size = 1
    assert nsamples % batch_size == 0

    enc = encoder.get_encoder(model_name, models_dir)
    hparams = model.default_hparams()
    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:
        hparams.override_from_dict(json.load(f))

    if length is None:
        length = hparams.n_ctx // 2
    elif length > hparams.n_ctx:
        raise ValueError(""Can't get samples longer than window size: %s"" % hparams.n_ctx)

    with tf.Session(graph=tf.Graph()) as sess:
        context = tf.placeholder(tf.int32, [batch_size, None])
        np.random.seed(seed)
        tf.set_random_seed(seed)
        output = sample.sample_sequence(
            hparams=hparams, length=length,
            context=context,
            batch_size=batch_size,
            temperature=temperature, top_k=top_k, top_p=top_p
        )

        saver = tf.train.Saver()
        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))
        saver.restore(sess, ckpt)

        while True:
            raw_text = input(""Model prompt >>> "")
            while not raw_text:
                print('Prompt should not be empty!')
                raw_text = input(""Model prompt >>> "")
            context_tokens = enc.encode(raw_text)
            generated = 0
            for _ in range(nsamples // batch_size):
                out = sess.run(output, feed_dict={
                    context: [context_tokens for _ in range(batch_size)]
                })[:, len(context_tokens):]
                for i in range(batch_size):
                    generated += 1
                    text = enc.decode(out[i])
                    print(""="" * 40 + "" SAMPLE "" + str(generated) + "" "" + ""="" * 40)
                    print(text)
            print(""="" * 80)

if __name__ == '__main__':
    fire.Fire(interact_model)`",still bit unclear find script interactive form model use integrate version import fire import import o import import import model sample run model string model use integer seed random number fix seed reproduce number return total number must divide number text none default determined model float value randomness distribution lower temperature le random temperature zero model become deterministic repetitive higher temperature random integer value diversity word considered step token resulting deterministic considered step default special setting meaning generally good value path parent folder model folder none assert open length none length length raise ca get longer window size sess context none seed seed output saver sess true input model prompt print empty input model prompt range output context range range text print sample print text print,issue,positive,negative,neutral,neutral,negative,negative
968341815,"Please retry the training with our code.
https://github.com/blue-fish/Real-Time-Voice-Cloning/tree/878_tacotron2

It is very difficult to learn attention with no reduction factor, as explained [here](https://erogol.com/gradual-training-with-tacotron-for-faster-convergence/). I recommend `hparams.n_frames_per_step=2` or higher.",please retry training code difficult learn attention reduction factor recommend higher,issue,negative,negative,negative,negative,negative,negative
968338661,"If you look carefully at the predicted spectrogram for `hparams.n_frames_per_step=7` above, you'll see some artifacts in the spectrogram. I don't have an explanation for this. But it does not happen when `hparams.n_frames_per_step=1`, after a sufficient number of steps have been trained.",look carefully spectrogram see spectrogram explanation happen sufficient number trained,issue,negative,negative,neutral,neutral,negative,negative
968336144,"Here is a version of `demo_cli.py` that doesn’t require any user interaction.
https://github.com/blue-fish/Real-Time-Voice-Cloning/blob/890_non_interactive_generation/demo_cli.py

You would run it like this. `gpt_output.txt` can have multiple lines. It is recommended to add a line break after each sentence, because the synthesizer struggles when the inputs become too long.

```
python demo_cli.py --skip_tests --no_save original_voice.wav gpt_output.txt
```

If the rest of your project uses Python, it is possible to use parts of demo_cli.py to do the generation without having to use Real-Time-Voice-Cloning separately like this. However, that is something you will have to figure out on your own. Good luck with your project!",version require user interaction would run like multiple add line break sentence synthesizer become long python rest project python possible use generation without use separately like however something figure good luck project,issue,positive,positive,positive,positive,positive,positive
968108906,"First of all, my apologies for the lack of clarity in my project. I am a master student graphic design at luca school of arts in Ghent, Belgium. This project is actually my thesis and therefore not a paying project, but very important to me. The 4 steps are completely what I want to achieve. At the moment this is a private project but if it works, it can certainly become open source for me. Concerning the gui or web interface, this is definitely not a priority for me, if this project is able to work, it is fine For me to run it in the prompt, as the focus is on speaking and conversation. Thank you very much!",first lack clarity project master student graphic design school project actually thesis therefore paying project important completely want achieve moment private project work certainly become open source concerning web interface definitely priority project able work fine run prompt focus speaking conversation thank much,issue,positive,positive,positive,positive,positive,positive
968101537,"I think what you want is:
1. Record the user with the computer's microphone
2. Use automatic speech recognition to determine what the user said
3. Input to GPT-2 and get a response (text)
4. Provide text and voice recording to Real-Time-Voice-Cloning to generate audio in the user's voice and play it on speakers or headphones

My questions are:
* Is your project open-source?
* Are you just looking for assistance to connect up the RTVC, or do you need help to build the end-to-end pipeline including some kind of GUI or web interface?
* Is this a paid assignment?",think want record user computer microphone use automatic speech recognition determine user said input get response text provide text voice recording generate audio user voice play project looking assistance connect need help build pipeline kind web interface assignment,issue,positive,positive,positive,positive,positive,positive
968093650,"My project is actually a kind of speech assistant that is based on the open Ai GPT 2 model with who you can have a conversation about a specific topic. The problem for me is that the Open Ai script of the model is not a text to speech software so it responds with text. My idea was to implement this script in the model so that you actually can have a conversation ""with yourself"" because of the cloning tool. So the real-time-voice-cloning tool should actually be my way of outputting the results of the open Ai model. I hope this is kind of clear?",project actually kind speech assistant based open ai model conversation specific topic problem open ai script model text speech text idea implement script model actually conversation tool tool actually way open ai model hope kind clear,issue,positive,positive,positive,positive,positive,positive
968092136,"Can you describe how you would want it to work? We can try to improve the interface.

For example:

```
import sounddevice as sd
from real_time_voice_cloning import RTVC

voice = RTVC() #performs initialization using config file
voice.clone(""target_voice.wav"")
audio = voice.text_to_speech([""Hello, this is your voice assistant."", ""What can I do for you today?""])
sd.play(audio)
```",describe would want work try improve interface example import import voice file audio hello voice assistant today audio,issue,negative,neutral,neutral,neutral,neutral,neutral
968081782,"Training outputs at 7500 steps. It has learned attention, but the mel prediction is not as good as Tacotron1 at a similar stage. Perhaps the implementation of reduction factor can be improved.

### Tacotron2, r=7, steps=7500
![image](https://user-images.githubusercontent.com/67130644/141648154-994d67cf-4c53-4a6b-bc21-bc0884c99c04.png)
![attention_step_7500_sample_1](https://user-images.githubusercontent.com/67130644/141648168-324ae1c4-7410-4530-b7e3-a3aba809c36c.png)

### Tacotron1, r=2, steps=7500
![image](https://user-images.githubusercontent.com/67130644/141648510-833d19aa-68ff-40d1-8569-efaddd7f0b4d.png)


",training learned attention mel prediction good similar stage perhaps implementation reduction factor image image,issue,negative,positive,positive,positive,positive,positive
968079062,"I have some exciting news to share, Nvidia's Tacotron2 has been integrated into my repo: https://github.com/blue-fish/Real-Time-Voice-Cloning/tree/878_tacotron2

It is almost done, it just needs some modifications in the decoder to account for the speaker embedding. @e0xextazy If you are able, would you please share the changes you made to save me some time?

Both inference and training have been tested and work, but require CUDA for now. I am currently training a single-speaker model to make sure it can learn attention. After that, I will work on a pretrained model using LibriSpeech.
",exciting news share almost done need account speaker able would please share made save time inference training tested work require currently training model make sure learn attention work model,issue,positive,positive,positive,positive,positive,positive
968068918,"Is there someone who is able to do this modifications, because I think this is out of my league ",someone able think league,issue,negative,positive,positive,positive,positive,positive
968068502,"No, it will require modifications. Read the comments and adapt it to suit your needs.",require read adapt suit need,issue,negative,neutral,neutral,neutral,neutral,neutral
968064643,Running this in a other script will automatically start the real time cloning without any inputs to give?,running script automatically start real time without give,issue,negative,positive,positive,positive,positive,positive
967708837,Closing inactive issue. Feel free to continue the discussion.,inactive issue feel free continue discussion,issue,positive,positive,positive,positive,positive,positive
966847677,"Thank you @pilnyjakub  for the fast response, I turned on again my laptop as soon as I saw it. I figured out that the files' names for the tux-100h dataset are just numbers, I just changed the first txt and wav files from ""0"" to ""audio-0"" and now it is running. Hope it goes well until the end.
For the 2nd one I was thinking on that, too, I will have to search in the code wherever it opens files. I will come up with updates later. Good night.",thank fast response turned soon saw figured first running hope go well end one thinking search code wherever come later good night,issue,positive,positive,positive,positive,positive,positive
966830440,"For the 1st issue: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/841#issuecomment-915287653

For the 2nd issue, in `synthesizer/preprocess.py` and `synthesizer/train.py`, wherever opening files use the same encoding as your dataset. e.g. encoding=""utf-8""",st issue issue wherever opening use,issue,negative,neutral,neutral,neutral,neutral,neutral
966810733,"I did it both with their own names and with the names you give as example. 
![image](https://user-images.githubusercontent.com/42816278/141408227-c03f6ed5-3464-4e0f-8544-fabc7c5ef81e.png)

However, I got some errors:

1. With tux-100h (https://discourse.mozilla.org/t/sharing-my-100h-of-single-speaker-spanish/45288):
```
python synthesizer_preprocess_audio.py datasets_root -n 6 --datasets_name LibriTTS --subfolders train-clean-100 --no_alignments --no_trim

D:\tesis2\Real-Time-Voice-Cloning\encoder\audio.py:13: UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended.
  warn(""Unable to import 'webrtcvad'. This package enables noise removal and is recommended."")
Arguments:
    datasets_root:   datasets_root
    out_dir:         datasets_root\SV2TTS\synthesizer2
    n_processes:     6
    skip_existing:   False
    hparams:
    no_alignments:   True
    datasets_name:   LibriTTS
    subfolders:      train-clean-100

Using data from:
    datasets_root\LibriTTS\train-clean-100
LibriTTS:   0%|                                                                                                             | 0/1 [00:00<?, ?speakers/s]D:\tesis2\Real-Time-Voice-Cloning\encoder\audio.py:13: UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended.  
  warn(""Unable to import 'webrtcvad'. This package enables noise removal and is recommended."")
D:\tesis2\Real-Time-Voice-Cloning\encoder\audio.py:13: UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended.
  warn(""Unable to import 'webrtcvad'. This package enables noise removal and is recommended."")
D:\tesis2\Real-Time-Voice-Cloning\encoder\audio.py:13: UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended.
  warn(""Unable to import 'webrtcvad'. This package enables noise removal and is recommended."")
D:\tesis2\Real-Time-Voice-Cloning\encoder\audio.py:13: UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended. 
  warn(""Unable to import 'webrtcvad'. This package enables noise removal and is recommended."")
LibriTTS:   0%|                                                                                                             | 0/1 [00:02<?, ?speakers/s]
multiprocessing.pool.RemoteTraceback:
""""""
Traceback (most recent call last):
  File ""D:\Programas\Python3.7\lib\multiprocessing\pool.py"", line 121, in worker
    result = (True, func(*args, **kwds))
  File ""D:\tesis2\Real-Time-Voice-Cloning\synthesizer\preprocess.py"", line 76, in preprocess_speaker
    assert text_fpath.exists()
AssertionError
""""""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""synthesizer_preprocess_audio.py"", line 59, in <module>
    preprocess_dataset(**vars(args))
  File ""D:\tesis2\Real-Time-Voice-Cloning\synthesizer\preprocess.py"", line 35, in preprocess_dataset
    for speaker_metadata in tqdm(job, datasets_name, len(speaker_dirs), unit=""speakers""):
  File ""D:\tesis2\voiceclonenv\lib\site-packages\tqdm\std.py"", line 1180, in __iter__
    for obj in iterable:
  File ""D:\Programas\Python3.7\lib\multiprocessing\pool.py"", line 748, in next
    raise value
AssertionError
```

2. With cv-corpus 7.0 (https://commonvoice.mozilla.org/es/datasets):
```
python synthesizer_preprocess_audio.py datasets_root -n 6 --datasets_name tux100h-cvcorpus --subfolders valid --no_trim --no_alignments
D:\tesis2\Real-Time-Voice-Cloning\encoder\audio.py:13: UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended.
  warn(""Unable to import 'webrtcvad'. This package enables noise removal and is recommended."")
Arguments:
    datasets_root:   datasets_root
    out_dir:         datasets_root\SV2TTS\synthesizer
    n_processes:     6
    skip_existing:   False
    hparams:
    no_alignments:   True
    datasets_name:   tux100h-cvcorpus
    subfolders:      valid

Using data from:
    datasets_root\tux100h-cvcorpus\valid
tux100h-cvcorpus:   0%|                                                                                                     | 0/1 [00:00<?, ?speakers/s]D:\tesis2\Real-Time-Voice-Cloning\encoder\audio.py:13: UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended.  
  warn(""Unable to import 'webrtcvad'. This package enables noise removal and is recommended."")
D:\tesis2\Real-Time-Voice-Cloning\encoder\audio.py:13: UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended. 
  warn(""Unable to import 'webrtcvad'. This package enables noise removal and is recommended."")
D:\tesis2\Real-Time-Voice-Cloning\encoder\audio.py:13: UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended. 
  warn(""Unable to import 'webrtcvad'. This package enables noise removal and is recommended."")
D:\tesis2\Real-Time-Voice-Cloning\encoder\audio.py:13: UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended. 
  warn(""Unable to import 'webrtcvad'. This package enables noise removal and is recommended."")
D:\tesis2\Real-Time-Voice-Cloning\encoder\audio.py:13: UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended. 
  warn(""Unable to import 'webrtcvad'. This package enables noise removal and is recommended."")
D:\tesis2\Real-Time-Voice-Cloning\encoder\audio.py:13: UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended. 
  warn(""Unable to import 'webrtcvad'. This package enables noise removal and is recommended."")
D:\tesis2\voiceclonenv\lib\site-packages\librosa\core\audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.
  warnings.warn(""PySoundFile failed. Trying audioread instead."")
D:\tesis2\voiceclonenv\lib\site-packages\librosa\core\audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.
  warnings.warn(""PySoundFile failed. Trying audioread instead."")
tux100h-cvcorpus:   0%|                                                                                                     | 0/1 [50:24<?, ?speakers/s]
multiprocessing.pool.RemoteTraceback:
""""""
Traceback (most recent call last):
  File ""D:\Programas\Python3.7\lib\multiprocessing\pool.py"", line 121, in worker
    result = (True, func(*args, **kwds))
  File ""D:\tesis2\Real-Time-Voice-Cloning\synthesizer\preprocess.py"", line 78, in preprocess_speaker
    text = """".join([line for line in text_file])
  File ""D:\tesis2\Real-Time-Voice-Cloning\synthesizer\preprocess.py"", line 78, in <listcomp>
    text = """".join([line for line in text_file])
  File ""D:\Programas\Python3.7\lib\encodings\cp1252.py"", line 23, in decode
    return codecs.charmap_decode(input,self.errors,decoding_table)[0]
UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 99: character maps to <undefined>
""""""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""synthesizer_preprocess_audio.py"", line 59, in <module>
    preprocess_dataset(**vars(args))
  File ""D:\tesis2\Real-Time-Voice-Cloning\synthesizer\preprocess.py"", line 35, in preprocess_dataset
    for speaker_metadata in tqdm(job, datasets_name, len(speaker_dirs), unit=""speakers""):
  File ""D:\tesis2\voiceclonenv\lib\site-packages\tqdm\std.py"", line 1180, in __iter__
    for obj in iterable:
  File ""D:\Programas\Python3.7\lib\multiprocessing\pool.py"", line 748, in next
    raise value
UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 99: character maps to <undefined>
```

With cv-corpus it processed some of the files (1 625/271 010), but then stopped (after at least 30 minutes) and displayed that UnicodeDecodeError.
",give example image however got python unable import package noise removal warn unable import package noise removal false true data unable import package noise removal warn unable import package noise removal unable import package noise removal warn unable import package noise removal unable import package noise removal warn unable import package noise removal unable import package noise removal warn unable import package noise removal recent call last file line worker result true file line assert exception direct cause following exception recent call last file line module file line job file line iterable file line next raise value python valid unable import package noise removal warn unable import package noise removal false true valid data unable import package noise removal warn unable import package noise removal unable import package noise removal warn unable import package noise removal unable import package noise removal warn unable import package noise removal unable import package noise removal warn unable import package noise removal unable import package noise removal warn unable import package noise removal unable import package noise removal warn unable import package noise removal trying instead trying instead trying instead trying instead recent call last file line worker result true file line text line line file line text line line file line decode return input ca decode position character undefined exception direct cause following exception recent call last file line module file line job file line iterable file line next raise value ca decode position character undefined stopped least displayed,issue,positive,negative,negative,negative,negative,negative
965976138,"@blue-fish thanks for your support. You are right according to `nvidia X server settings` PCIe generation is only `Gen2` (CPU i7 2600 does not support PCIe 3.0). Yet `maximum PCIe link width` is well reported as `x16` (5.0 GT/s).

Consequently you may be right old hardware could be the bottleneck for the GPU although CPU usage keeps around 12% while training, RAM usage is around 65% and `PCIe Bandwidth Utilization` is reported as low as 1% by `nvidia X server settings`. So no hardware seems to work at its full potential.",thanks support right according server generation gen support yet maximum link width well consequently may right old hardware could bottleneck although usage around training ram usage around utilization low server hardware work full potential,issue,positive,positive,positive,positive,positive,positive
965796867,"@MGSousa @StElysse @Ca-ressemble-a-du-fake 

For now, I am going to assume that ""slow training"" is caused by hardware being old or having limitations that prevent the GPU from operating at its full potential.

If this is not the case, please provide hardware details including:
* Motherboard manufacturer and model (confirm PCIe 3.0/4.0 support, the PCIe x16 slot supports x16 bandwidth)
* CPU model
* RAM speed and amount",going assume slow training hardware old prevent operating full potential case please provide hardware manufacturer model confirm support slot model ram speed amount,issue,positive,positive,neutral,neutral,positive,positive
965791315,"It is not necessary to use the same names, except `datasets_root`, `LibriTTS` and `train-clean-100` if you are using the preprocessing command that I give.

However, please try matching the names before reporting a problem, or when asking for help to troubleshoot an issue like this.",necessary use except command give however please try matching problem help issue like,issue,positive,neutral,neutral,neutral,neutral,neutral
965683668,"Does your motherboard support PCI Express 3.0? If it only supports PCIe 2.0 then that is likely the bottleneck.

Some motherboards are manufactured with slots that fit a x16 GPU, but don't include all 16 PCIe lanes for communication.",support express likely bottleneck fit include communication,issue,positive,positive,positive,positive,positive,positive
965655432,"For audios to be detected, the directory structure must match this exactly including the ""speaker"" and ""book_dir"" levels. https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-666099538",directory structure must match exactly speaker,issue,negative,positive,positive,positive,positive,positive
964952929,"So I could profile the training for a single step only. The main differences I see with your results are :

* different CUDA version (11.3 (although 11.5 in actually installed) VS 10.2)
* CUDA total time > CPU total time (in your case it is the opposite)

But the profiling has only one step.

I don't know how you can achieve to format comments so well, I was not successful in doing so. Detailed results are barely legible consequently they cannot be posted.

--------------------------------------------------------------------------------
  Environment Summary
--------------------------------------------------------------------------------
PyTorch 1.10.0+cu113 DEBUG compiled w/ CUDA 11.3
Running with Python 3.8 and 

`pip3 list` truncated output:
numpy==1.19.4
torch==1.10.0+cu113
torchaudio==0.10.0+cu113
torchfile==0.1.0
torchvision==0.11.1+cu113

--------------------------------------------------------------------------------
  cProfile output
--------------------------------------------------------------------------------
         1846962 function calls (1806034 primitive calls) in 9.077 seconds

 What should I look for / watch out in the results ?",could profile training single step main see different version although actually total time total time case opposite one step know achieve format well successful detailed barely legible consequently posted environment summary running python pip list truncated output output function primitive look watch,issue,positive,positive,positive,positive,positive,positive
964826610,"Trying to train the synthesizer with tux-100h (valid) dataset and cv-corpus-7.0-2021 (validated) dataset is giving me the following:
![image](https://user-images.githubusercontent.com/42816278/141061539-b88003b3-9450-48fe-9c02-fae707d1b08e.png)
In the image, it is the message from tux-100h, but the same appears for cv-corpus.
Both datasets are already structured as mentioned in previous issues:
![image](https://user-images.githubusercontent.com/42816278/141061719-c6cf7e5e-6a43-4f46-a869-05f186da0bd4.png)
I don't know why the audios are not being recognised, I think that is what is happening there...",trying train synthesizer valid giving following image image message already structured previous image know think happening,issue,negative,negative,neutral,neutral,negative,negative
964792718,"Thank you @blue-fish  for your guide! Unfortunately the computer is running out of ram after completing the second stage of the profiling (the one that involves Autograd). Neither GPU not CPU was more loaded than usual, but after the second stage completed, the RAM (and then swap space) skyrocketed and computer became unusable. 

I tried with 20, 10, and even 5 steps failed because 12 GB of RAM were depleted. When computer become usable again I will try to profile for 2 steps only.",thank guide unfortunately computer running ram second stage one neither loaded usual second stage ram swap space computer unusable tried even ram computer become usable try profile,issue,negative,negative,negative,negative,negative,negative
964281583,"> I tried to run PyTorch bottleneck as advised on [PT forum](https://discuss.pytorch.org/t/gpu-not-fully-used-how-to-optimize-the-code/84519) but could not correctly modify `train.py` so that PT profiler terminates in a given amount of time without messing up with the code.

### Training schedule update
First, update the training schedule in [synthesizer/hparams.py](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/7432046efc23cabf176f9fdc8d2fd67020059478/synthesizer/hparams.py#L51-L57) so it only runs 20 steps from scratch.
```
        ### Tacotron Training
        tts_schedule = [(2,  1e-3,  20,  12)],  # Train only 20 steps for benchmarking
```

### Dataloader update
Next, you will need to change this line so `num_workers=0`. The profiler doesn't work with multi-worker dataloaders.
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/7432046efc23cabf176f9fdc8d2fd67020059478/synthesizer/train.py#L150

### Command
Train a new model from scratch. You can call it anything, I called mine ""test"". The `--force_restart` option prevents the saved checkpoints from stopping training prematurely.

```
python -m torch.utils.bottleneck synthesizer_train.py --force_restart test datasets_root/SV2TTS/synthesizer/
```

### Output

<details>
<summary>Click here to display profiler output</summary>

```
--------------------------------------------------------------------------------
  Environment Summary
--------------------------------------------------------------------------------
PyTorch 1.7.1 DEBUG compiled w/ CUDA 10.2
Running with Python 3.8 and 

`pip3 list` truncated output:
numpy==1.19.4
torch==1.7.1
torchfile==0.1.0
--------------------------------------------------------------------------------
  cProfile output
--------------------------------------------------------------------------------
         6085394 function calls (5892177 primitive calls) in 43.148 seconds

   Ordered by: internal time
   List reduced from 7179 to 15 due to restriction <15>

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
       20   15.886    0.794   15.886    0.794 {method 'run_backward' of 'torch._C._EngineBase' objects}
     2992    8.177    0.003    8.177    0.003 {method 'read' of '_io.BufferedReader' objects}
      552    3.517    0.006    3.518    0.006 {built-in method io.open}
     5250    1.717    0.000    1.717    0.000 {method 'to' of 'torch._C._TensorBase' objects}
    31000    1.018    0.000    1.018    0.000 {built-in method addmm}
    12400    0.934    0.000    0.934    0.000 {built-in method lstm_cell}
     6200    0.803    0.000    9.254    0.001 synthesizer/models/tacotron.py:270(forward)
      480    0.709    0.001    0.710    0.001 {built-in method numpy.fromfile}
       40    0.626    0.016    0.626    0.016 {built-in method gru}
    19020    0.600    0.000    0.600    0.000 {method 'matmul' of 'torch._C._TensorBase' objects}
    12400    0.494    0.000    1.292    0.000 synthesizer/models/tacotron.py:265(zoneout)
     6200    0.491    0.000    2.612    0.000 synthesizer/models/tacotron.py:221(forward)
 94620/20    0.463    0.000   10.396    0.520 venv/lib/python3.8/site-packages/torch/nn/modules/module.py:715(_call_impl)
     6480    0.448    0.000    0.448    0.000 {built-in method conv1d}
    18720    0.419    0.000    0.419    0.000 {built-in method cat}


--------------------------------------------------------------------------------
  autograd profiler output (CPU mode)
--------------------------------------------------------------------------------
        top 15 events sorted by cpu_time_total

-----------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-----------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
              TBackward        18.46%     556.886ms        18.46%     556.886ms     556.886ms       0.000us           NaN       0.000us       0.000us             1  
              TBackward         9.39%     283.453ms         9.39%     283.453ms     283.453ms       0.000us           NaN       0.000us       0.000us             1  
                aten::t         9.39%     283.451ms         9.39%     283.451ms     283.451ms       0.000us           NaN       0.000us       0.000us             1  
           aten::conv1d         9.36%     282.482ms         9.36%     282.482ms     282.482ms       0.000us           NaN       0.000us       0.000us             1  
      aten::convolution         9.36%     282.480ms         9.36%     282.480ms     282.480ms       0.000us           NaN       0.000us       0.000us             1  
     aten::_convolution         9.36%     282.478ms         9.36%     282.478ms     282.478ms       0.000us           NaN       0.000us       0.000us             1  
             aten::add_         9.36%     282.389ms         9.36%     282.389ms     282.389ms       0.000us           NaN       0.000us       0.000us             1  
              TBackward         4.73%     142.740ms         4.73%     142.740ms     142.740ms       0.000us           NaN       0.000us       0.000us             1  
                aten::t         4.73%     142.737ms         4.73%     142.737ms     142.737ms       0.000us           NaN       0.000us       0.000us             1  
        aten::transpose         4.65%     140.276ms         4.65%     140.276ms     140.276ms       0.000us           NaN       0.000us       0.000us             1  
           BmmBackward0         2.35%      70.930ms         2.35%      70.930ms      70.930ms       0.000us           NaN       0.000us       0.000us             1  
              aten::bmm         2.35%      70.904ms         2.35%      70.904ms      70.904ms       0.000us           NaN       0.000us       0.000us             1  
               aten::to         2.33%      70.235ms         2.33%      70.235ms      70.235ms       0.000us           NaN       0.000us       0.000us             1  
    aten::empty_strided         2.33%      70.190ms         2.33%      70.190ms      70.190ms       0.000us           NaN       0.000us       0.000us             1  
              aten::gru         1.84%      55.546ms         1.84%      55.546ms      55.546ms       0.000us           NaN       0.000us       0.000us             1  
-----------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 3.017s
CUDA time total: 0.000us

--------------------------------------------------------------------------------
  autograd profiler output (CUDA mode)
--------------------------------------------------------------------------------
        top 15 events sorted by cpu_time_total

	Because the autograd profiler uses the CUDA event API,
	the CUDA time column reports approximately max(cuda_time, cpu_time).
	Please ignore this output if your code does not use CUDA.

----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
               AddmmBackward        21.06%     553.945ms        21.06%     553.945ms     553.945ms     553.600ms        27.44%     553.600ms     553.600ms             1  
                    aten::mm        21.06%     553.915ms        21.06%     553.915ms     553.915ms     553.572ms        27.44%     553.572ms     553.572ms             1  
          UnsqueezeBackward0        10.92%     287.145ms        10.92%     287.145ms     287.145ms     286.460ms        14.20%     286.460ms     286.460ms             1  
               aten::squeeze        10.92%     287.135ms        10.92%     287.135ms     287.135ms       0.000us         0.00%       0.000us       0.000us             1  
                 aten::addmm         6.06%     159.393ms         6.06%     159.393ms     159.393ms     159.392ms         7.90%     159.392ms     159.392ms             1  
                MulBackward0         5.76%     151.387ms         5.76%     151.387ms     151.387ms     150.768ms         7.47%     150.768ms     150.768ms             1  
               aten::dropout         3.20%      84.220ms         3.20%      84.220ms      84.220ms      84.218ms         4.17%      84.218ms      84.218ms             1  
        aten::_fused_dropout         3.20%      84.213ms         3.20%      84.213ms      84.213ms      84.214ms         4.17%      84.214ms      84.214ms             1  
                aten::stride         3.20%      84.148ms         3.20%      84.148ms      84.148ms       0.000us         0.00%       0.000us       0.000us             1  
    CudnnConvolutionBackward         2.90%      76.257ms         2.90%      76.257ms      76.257ms      70.970ms         3.52%      70.970ms      70.970ms             1  
                AddBackward0         2.59%      68.060ms         2.59%      68.060ms      68.060ms      68.057ms         3.37%      68.057ms      68.057ms             1  
                 CatBackward         2.34%      61.523ms         2.34%      61.523ms      61.523ms       1.808ms         0.09%       1.808ms       1.808ms             1  
                 CatBackward         2.31%      60.804ms         2.31%      60.804ms      60.804ms       1.012ms         0.05%       1.012ms       1.012ms             1  
                 CatBackward         2.31%      60.761ms         2.31%      60.761ms      60.761ms       1.804ms         0.09%       1.804ms       1.804ms             1  
                 CatBackward         2.19%      57.595ms         2.19%      57.595ms      57.595ms       1.712ms         0.08%       1.712ms       1.712ms             1  
----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.631s
CUDA time total: 2.018s
```

</details>",tried run bottleneck advised forum could correctly modify profiler given amount time without messing code training schedule update first update training schedule scratch training train update next need change line profiler work command train new model scratch call anything mine test option saved stopping training prematurely python test output summary click display profiler output environment summary running python pip list truncated output output function primitive ordered internal time list reduced due restriction function method method method method method method forward method method method forward method method cat profiler output mode top sorted name self self total total time self self total time nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan self time total time total profiler output mode top sorted profiler event time column approximately please ignore output code use name self self total total time self self total time self time total time total,issue,negative,positive,neutral,neutral,positive,positive
964199088,Increasing batch_size to 16 did not cause the CUDA out of memory but did not noticeably increase GPU utilization. VRAM usage as increased up to 6.9GB.,increasing cause memory noticeably increase utilization usage,issue,negative,neutral,neutral,neutral,neutral,neutral
964195611,"I also tried to increase batch_size to 20 and quickly got a `CUDA out of memory` error that advises something :

`RuntimeError: CUDA out of memory. Tried to allocate 210.00 MiB (GPU 0; 7.79 GiB total capacity; 5.01 GiB already allocated; 166.25 MiB free; 5.29 GiB reserved in total by PyTorch) **If reserved memory is >> allocated memory** try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF`
It looks like `reserved memory` > `allocated memory` (but not >>) so should I apply their advice ?",also tried increase quickly got memory error something memory tried allocate mib gib total capacity gib already mib free gib reserved total reserved memory memory try setting avoid fragmentation see documentation memory management like reserved memory memory apply advice,issue,negative,positive,positive,positive,positive,positive
964189553,"I tried to run PyTorch bottleneck as advised on [PT forum](https://discuss.pytorch.org/t/gpu-not-fully-used-how-to-optimize-the-code/84519) but could not correctly modify `train.py` so that PT profiler terminates in a given amount of time without messing up with the code. I tried to manually set `max_step` to 254010 because training was at step 254k and I wanted it to train for 10 steps before exiting, but it made the Loss increase, so I reverted my changes.",tried run bottleneck advised forum could correctly modify profiler given amount time without messing code tried manually set training step train made loss increase,issue,negative,neutral,neutral,neutral,neutral,neutral
964054517,"Nvidia driver is 470 (I tried several ones, this one is the proprietary one) Cuda is now 11.5 (I also tried 11.3).
CPU does not seem to be the bottleneck since its utilization lives around 50% its an old i7 2600.",driver tried several one proprietary one also tried seem bottleneck since utilization around old,issue,negative,positive,neutral,neutral,positive,positive
963629851,"In #437, I describe a method to finetune the pretrained models to a single target voice. It requires making a dataset of short audios and transcripts.",describe method single target voice making short,issue,negative,negative,neutral,neutral,negative,negative
963547320,"Thanks for the update @rushic24 . If it stops working, please reopen the issue and provide the warning message.",thanks update working please reopen issue provide warning message,issue,negative,positive,positive,positive,positive,positive
963545246,"Thanks  @blue-fish , I have no idea why but it is working today ",thanks idea working today,issue,negative,positive,positive,positive,positive,positive
963485823,"If GPU is running at P2, doesn't seem like it is the bottleneck. I am running out of ideas.
* Which NVIDIA driver and CUDA version is installed?
* Is there any possibility of a CPU bottleneck? For example a low-performance or obsolete CPU.",running seem like bottleneck running driver version possibility bottleneck example obsolete,issue,negative,neutral,neutral,neutral,neutral,neutral
963481811,"@rushic24 Can you remove the `if not device[""name""] in input_devices: ` on line 171 of [toolbox/ui.py](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/7432046efc23cabf176f9fdc8d2fd67020059478/toolbox/ui.py#L171) and let us know what warnings you see when starting up the toolbox?",remove device name line let u know see starting toolbox,issue,negative,neutral,neutral,neutral,neutral,neutral
962826067,Pleased to know that you are satisfied with the single voice models. Please reopen this issue if you start training a model from scratch.,know satisfied single voice please reopen issue start training model scratch,issue,positive,positive,positive,positive,positive,positive
962786217,"`nvidia-smi` command reports performance state P2 being used. GPU utilization dropped a little bit falling even as low as 0% sometimes, but most of the time between 20-50%.",command performance state used utilization little bit falling even low sometimes time,issue,negative,negative,neutral,neutral,negative,negative
962771300,"@blue-fish , I have not started on this project yet. I have a few other projects (semi-related) working now. I read the TTS Corpus paper and it sounds interesting. Frankly I have gotten very good results from the single voice trained models I am using for my current prohect, but there's always room for improvement. I would love to be able to ""help"" the synthesizer using punctuation to tell it where to place the emphasis on a syllable or syllables in multi-syllabic a word...
I would like to give a TTS-built-from-scratch synthesizer base a try once I get some of these other projects behind me. I will let you know when I start and I will keep you apprised of progress. No doubt I will hit some snags and will solicit your always-helpful advice.
Regards,
Tomcattwo",project yet working read corpus paper interesting frankly gotten good single voice trained current always room improvement would love able help synthesizer punctuation tell place emphasis syllable word would like give synthesizer base try get behind let know start keep progress doubt hit solicit advice,issue,positive,positive,positive,positive,positive,positive
962656875,"@Tomcattwo Did you end up pursuing this? If yes, how is the training coming along?",end yes training coming along,issue,negative,neutral,neutral,neutral,neutral,neutral
962656699,"This section of code is used to detect valid audio output devices. It is possible that the line 172 warning is not being displayed because your headphones have an integrated microphone and are considered an ""input device"".
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/7432046efc23cabf176f9fdc8d2fd67020059478/toolbox/ui.py#L154-L172

",section code used detect valid audio output possible line warning displayed microphone considered input device,issue,negative,neutral,neutral,neutral,neutral,neutral
962651361,"I made a branch that supports mixed precision training. It is not recommended for use at this time.
https://github.com/blue-fish/Real-Time-Voice-Cloning/tree/487_mixed_precision_training

For me, mixed precision training is much slower than without it, and loss is occasionally `nan`. I also had to set up my Python environment with Anaconda due to a problem with matrix multiplication. https://github.com/pytorch/pytorch/issues/56747#issuecomment-825559343

Pytorch AMP enabled (Python 3.9.7 with Anaconda, pytorch==1.10.0):
```
{| Epoch: 1/8 (20/2564) | Loss: nan | 0.24 steps/s | Step: 0k | }
Average execution time over 10 steps:
  Blocking, waiting for batch (threaded) (10/10):  mean:    0ms   std:    0ms
  Data to cuda (10/10):                            mean:    1ms   std:    0ms
  Forward pass (10/10):                            mean:  956ms   std:  151ms
  Loss (10/10):                                    mean:   14ms   std:    3ms
  Backward pass (10/10):                           mean: 3013ms   std:  456ms
  Parameter update (10/10):                        mean:   71ms   std:    5ms
  Extras (visualizations, saving) (10/10):         mean:    0ms   std:    0ms
```

Same setup without AMP:
```
{| Epoch: 1/8 (20/2564) | Loss: 5.778 | 0.76 steps/s | Step: 0k | }
Average execution time over 10 steps:
  Blocking, waiting for batch (threaded) (10/10):  mean:    0ms   std:    0ms
  Data to cuda (10/10):                            mean:    1ms   std:    0ms
  Forward pass (10/10):                            mean:  451ms   std:   52ms
  Loss (10/10):                                    mean:   10ms   std:    2ms
  Backward pass (10/10):                           mean:  753ms   std:   73ms
  Parameter update (10/10):                        mean:   31ms   std:    3ms
  Extras (visualizations, saving) (10/10):         mean:    5ms   std:    0ms
```",made branch mixed precision training use time mixed precision training much without loss occasionally nan also set python environment anaconda due problem matrix multiplication python anaconda epoch loss nan step average execution time blocking waiting batch threaded mean data mean forward pas mean loss mean backward pas mean parameter update mean saving mean setup without epoch loss step average execution time blocking waiting batch threaded mean data mean forward pas mean loss mean backward pas mean parameter update mean saving mean,issue,negative,negative,negative,negative,negative,negative
962597935,"Now that r has decreased to 2 with a batch size of 12, training rate is back to 0.52-0.54 step / s 

Backward pass takes around 1 s.

`{| Epoch: 2/208 (272/748) | Loss: 0.3196 | 0.56 steps/s | Step: 166k | }

Average execution time over 10 steps:

  Blocking, waiting for batch (threaded) (10/10):  mean:    0ms   std:    0ms
  Data to cuda (10/10):                            mean:    1ms   std:    0ms
  Forward pass (10/10):                            mean:  682ms   std:  169ms
  Loss (10/10):                                    mean:   13ms   std:    4ms
  Backward pass (10/10):                           mean: 1060ms   std:  259ms
  Parameter update (10/10):                        mean:   28ms   std:    1ms
  Extras (visualizations, saving) (10/10):         mean:    0ms   std:    0ms`

GPU utilization is still around 40% (range 20-60%) and memory has increased to 6.7GB (only for training)",batch size training rate back step backward pas around epoch loss step average execution time blocking waiting batch threaded mean data mean forward pas mean loss mean backward pas mean parameter update mean saving mean utilization still around range memory training,issue,negative,negative,negative,negative,negative,negative
962562529,"You can find all preprocessing not in dataset loading but in the model itself [here](https://github.com/yui-mhcp/text_to_speech/blob/master/models/tts/tacotron2.py) in the `get_mel_input` which call `load_mel` from `utils.audio.audio_io` file
",find loading model call file,issue,negative,neutral,neutral,neutral,neutral,neutral
962560323,"@Ananas120 you used Mozilla CommonVoices among other to train the synthesizer. I don't know for all corpuses but at least in corpus 1 that I downloaded, I noticed that samples  often contain silences at the beginning and at the end and I read somewhere (I can't find it now) that there must not have silences in these locations. Yet it does not seem that you try to trim those silence parts when [you preprocess this dataset](https://github.com/yui-mhcp/text_to_speech/blob/master/datasets/custom_datasets/audio_datasets.py). So actually should one bother removing silences from CV ?",ananas used among train synthesizer know least corpus often contain beginning end read somewhere ca find must yet seem try trim silence actually one bother removing,issue,negative,negative,negative,negative,negative,negative
962552736,"It was indeed (for r = 2) but then I applied gradual training like r = 16 till 20k, then 8 till 40k, then 7 till 80k then 5 till 160k, and finally 2 till the end. Batch size is set constant to 12. Does it still look reasonable to you although r has increased ? In 10k, r will switch back to 2 so I'll post the profiler results to compare to.

GPU utilization shows roughly 40% (range 20-60%) and memory usage 5.7/7.8 GB (mainly used by python3 with 5.3GB) so there may be room for improvements. Maybe by increasing batch size ?",indeed applied gradual training like till till till till finally till end batch size set constant still look reasonable although switch back post profiler compare utilization roughly range memory usage mainly used python may room maybe increasing batch size,issue,negative,positive,neutral,neutral,positive,positive
962529877,"I thought your training rate was 0.52-0.54 steps/s, did something change? Your profiler results look reasonable to me.",thought training rate something change profiler look reasonable,issue,negative,positive,positive,positive,positive,positive
962509117,"Thanks a lot for these precise instructions! Forward and backward passes are higher than what you showed in your comment 2 days ago. Otherwise if your comment just above deals with a GTX 1660s then my profiler results look good since RTX 3070 should be faster.

{| Epoch: 1/61 (30/748) | Loss: 0.3490 | 1.2 steps/s | Step: 115k | }

Average execution time over 10 steps:
 Blocking, waiting for batch (threaded) (10/10):   mean:    0ms   std:    0ms
  Data to cuda (10/10):                                         mean:    1ms   std:    0ms
  Forward pass (10/10):                                       mean:  292ms   std:   64ms
  Loss (10/10):                                                     mean:    5ms   std:    1ms
  Backward pass (10/10):                                    mean:  454ms   std:  101ms
  Parameter update (10/10):                                mean:   28ms   std:    1ms
  Extras (visualizations, saving) (10/10):             mean:    0ms   std:    0ms

(Sorry I did not succeed in formatting the table as you did!",thanks lot precise forward backward higher comment day ago otherwise comment profiler look good since faster epoch loss step average execution time blocking waiting batch threaded mean data mean forward pas mean loss mean backward pas mean parameter update mean saving mean sorry succeed table,issue,negative,negative,neutral,neutral,negative,negative
962508876,"Yeah indeed sometimes the cloning fail on colab, quite strange but anyway, thank you :D ",yeah indeed sometimes fail quite strange anyway thank,issue,negative,negative,negative,negative,negative,negative
962505840,I did exactly what I did earlier and now it is working ! Quality is pretty good from the online demo.,exactly working quality pretty good,issue,positive,positive,positive,positive,positive,positive
962487764,"> @Ananas120 I currently have 2 s / **step** on an RTX 3070 and you had 2 s / **epoch** on a older GPU. That's a huge gap or you meant 2 s / **step** ?
> 
> Will the model be better with a higher batch_size or it does not matter ? With 8 GB GPU I cannot reach a batch_size of 32 so maybe it is worth it to use google colab as they offer GPU with more VRAM, isn't it ?
> 
> I tried your online demo but was stuck in cell 4 with `ModuleNotFoundError: No module named 'models'` I could download the model replacing gdrive_sh with gdown --id XYZ -O same path you used. I am not sure how to import this models module.

You can check the yui's repo to see training metrics (it's 7s/step without reduction factor, RTX3080 and batch_size 32) and 10s/step on my GTX (batch_size of 16)

Did you well cloned the github (1st cell with git clone) ? Because the module `models` is in the github actually 

I left generated audio so you can listen to them without executing the code ;)",ananas currently step epoch older huge gap meant step model better higher matter reach maybe worth use offer tried stuck cell module could model id path used sure import module check see training metric without reduction factor well st cell git clone module actually left audio listen without code,issue,positive,positive,positive,positive,positive,positive
962486163,"@Ananas120 I currently have 2 s / **step** on an RTX 3070 and you had 2 s / **epoch** on a older GPU. That's a huge gap or you meant 2 s / **step** ?

Will the model be better with a higher batch_size or it does not matter ? With 8 GB GPU I cannot reach a batch_size of 32 so maybe it is worth it to use google colab as they offer GPU with more VRAM, isn't it ?

I tried your online demo but was stuck in cell 4 with `
ModuleNotFoundError: No module named 'models'
` I could download the model replacing gdrive_sh with gdown --id XYZ -O same path you used. I am not sure how to import this models module. ",ananas currently step epoch older huge gap meant step model better higher matter reach maybe worth use offer tried stuck cell module could model id path used sure import module,issue,positive,positive,positive,positive,positive,positive
962477911,"> I changed the `tts_cleaner_names` to `[""transliteration_cleaners""]` in `hparams.py` file but the accented letters keep being replaced by their unaccented counterparts during training although `train.txt` file has all accented letters. Is it just a minor display issue or this filtered text is actually feeding the model ?

`transliteration_cleaners` converts a text string to ASCII which will remove accents from characters. If you want to keep the accents, use `basic_cleaners` and add cleaning features as needed.

> By the way is it ok to continue posting questions linked to my experiment on this thread or should a new question be opened each time the topic differs ?

Open a new issue if you have a question or topic that will be useful to many users. Otherwise keep posting in this thread and we'll leave it open so long as it's active.

I generally only answer questions that I find interesting and/or benefit the whole community. Priority is given to individuals contributing to the repo's codebase or training a model that will be shared publicly.",file keep unaccented training although file minor display issue text actually feeding model text string ascii remove want keep use add cleaning way continue posting linked experiment thread new question time topic open new issue question topic useful many otherwise keep posting thread leave open long active generally answer find interesting benefit whole community priority given training model publicly,issue,positive,positive,positive,positive,positive,positive
962475711,"> Which command did you use for GPU utilization ?

```
watch -n 0.5 nvidia-smi
```

> How do you use the profiler you talked about earlier ?

I made a branch for this: https://github.com/blue-fish/Real-Time-Voice-Cloning/tree/700_slow_training
If you don't want to get the branch, make these modifications: https://github.com/blue-fish/Real-Time-Voice-Cloning/commit/989a3e43a834100ae76ac6ff7edd3e5d532ced80

Example training output with profiler:
```
{| Epoch: 1/8 (20/2564) | Loss: 6.107 | 0.75 steps/s | Step: 0k | }
Average execution time over 10 steps:
  Blocking, waiting for batch (threaded) (10/10):  mean:    0ms   std:    0ms
  Data to cuda (10/10):                            mean:    1ms   std:    0ms
  Forward pass (10/10):                            mean:  466ms   std:   98ms
  Loss (10/10):                                    mean:   12ms   std:    2ms
  Backward pass (10/10):                           mean:  780ms   std:  153ms
  Parameter update (10/10):                        mean:   16ms   std:    0ms
  Extras (visualizations, saving) (10/10):         mean:    4ms   std:    0ms
```
",command use utilization watch use profiler made branch want get branch make example training output profiler epoch loss step average execution time blocking waiting batch threaded mean data mean forward pas mean loss mean backward pas mean parameter update mean saving mean,issue,negative,negative,negative,negative,negative,negative
962409895,"@Ca-ressemble-a-du-fake I used a GTX1070 with 6Go RAM and batch-size of 16 (no reduction factor) if I well remembered and also tried with RTX3080 with batch size of 32 and both give similar result I think (not tested rigorously)

You can see parameters I used as well as pretrained models for French and some experiments to voice cloning in French in the yui's github [here](https://github.com/yui-mhcp/text_to_speech) where I contributed by adding my French pretrained model (trained on SIWIS) and an online demo on Google Colab

Note that the architecture used in this repo is the same (Tacotron2) but implemented in `tensorflow 2.x` with a different architecture (different number of layers) based on the NVIDIA pytorch open-sourced implementation",used go ram reduction factor well also tried batch size give similar result think tested rigorously see used well voice model trained note architecture used different architecture different number based implementation,issue,negative,neutral,neutral,neutral,neutral,neutral
962402994,"I changed the `tts_cleaner_names` to  `[""transliteration_cleaners""]` in `hparams.py` file but the accented letters keep being replaced by their unaccented counterparts during training although `train.txt` file has all accented letters. Is it just a minor display issue or this filtered text is actually feeding the model ?

By the way is it ok to continue posting questions linked to my experiment on this thread or should a new question be opened each time the topic differs ?",file keep unaccented training although file minor display issue text actually feeding model way continue posting linked experiment thread new question time topic,issue,negative,positive,neutral,neutral,positive,positive
962401163,"This is a nice benchmark. Which command did you use for GPU utilization ? And which dataset did you use ? RTX 3070 should be faster than GTX 1660s on the same dataset or it should be independent from it (my rate was given for a French dataset) ?

So can we say that around 1 step / s should be OK for r = 2  and batch_size = 12 ? Should the results be the same using [vanilla Tacotron 2](https://github.com/NVIDIA/tacotron2) training on same dataset ?

I just know basic Python (eg : basic string manipulation, basic external program call, ...). How do you use the profiler you talked about earlier ? I did not find any reference of it in `encoder_train.py`,should I add a call in `synthesizer_train.py`  ?",nice command use utilization use faster independent rate given say around step vanilla training know basic python basic string manipulation basic external program call use profiler find reference add call,issue,negative,positive,neutral,neutral,positive,positive
962135553,"> Same problem here too, on Ubuntu 20.04 without Anaconda and on RTX 3070 under Python 3.8.8. Around 0.52-0.54 step / s.

_Originally posted by @Ca-ressemble-a-du-fake in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/711#issuecomment-960483401_

With the whole repo in an unmodified state (including [synthesizer hparams](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/b5ba6d0371882dbab595c48deb2ff17896547de7/synthesizer/hparams.py)), I get 0.72-0.74 steps/s.

Switching to Tacotron2 on Tensorflow 1.x ([`5425557`](https://github.com/CorentinJ/Real-Time-Voice-Cloning/tree/5425557efe30863267f805851f918124191e0be0), installed with [these instructions](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Installation---Ubuntu-20.04)), I get 1.10-1.14 steps/s. Tensorflow training speed is variable, with the training getting faster as the model gets better. This experiment was done with the #538 model as a starting point, finetuning on a large single-speaker dataset.

Because I wanted to understand whether this difference was caused by using Taco1 vs Taco2, or PT vs TF, I also ran a training experiment with a PyTorch Taco2 implementation. Results below.

| Model      | PT/TF            | Model Parameters | Training Speed    |
|------------|------------------|------------------|-------------------|
| Tacotron 1 | PyTorch 1.3.1    | 30.87M           | 0.72-0.74 steps/s |
| Tacotron 2 | PyTorch 1.3.1    | 28.44M           | 0.65-0.66 steps/s |
| Tacotron 2 | Tensorflow 1.15  | 28.44M           | 1.10-1.14 steps/s |

We can conclude that PyTorch is slower training than Tensorflow 1.x. And there are some other unknown issues causing your training to be even slower.

```
* OS: Ubuntu 20.04
    * NVIDIA Driver Version: 460.91.03
    * CUDA Version: 11.2 

* GPU: GTX 1660S (desktop)
    * Performance state: P2
    * Power draw: 90W (range: 70-110W)
    * Utilization: 80% (range: 65-95%)
    * VRAM: 5.5/6.0 GB

* Python 3.7.9 (with Anaconda)
* Same training speed obtained for Python 3.8.10 (without Anaconda) with torch==1.7.1

* Dataset storage: HDD
```",problem without anaconda python around step posted whole unmodified state synthesizer get switching get training speed variable training getting faster model better experiment done model starting point large understand whether difference also ran training experiment implementation model model training speed conclude training unknown causing training even o driver version version performance state power draw range utilization range python anaconda training speed python without anaconda storage,issue,negative,positive,positive,positive,positive,positive
961734367,The repo includes a [profiler](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/utils/profiler.py) which is used for [encoder training](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/encoder/train.py). You can try something similar to find the bottleneck for synthesizer training.,profiler used training try something similar find bottleneck synthesizer training,issue,negative,neutral,neutral,neutral,neutral,neutral
961725662,"@OpeyemiOsakuade If the synthesizer model is bad, or if the voice is very different from those seen in training data, it will cause problems. You can try training again, after improving the quality and possibly the quantity of your dataset.  You can also work around it by synthesizing shorter utterances and stitching those together.",synthesizer model bad voice different seen training data cause try training improving quality possibly quantity also work around shorter stitching together,issue,negative,negative,negative,negative,negative,negative
961719529,"The command `nvidia-smi -q -g 0 -d UTILIZATION -l 1` shows GPU usage varying between 20 and 50% during training. So GPU is far from being fully utilized whereas it should. So it looks like there is a huge gain margin to harvest somewhere.

I would like to use `nvtop` to have a graph but it is not compatible with the latest nvidia drivers. If I force the install then there is a driver / library mismatch and the training runs even slower (less than 0.1 steps / s).",command utilization usage training far fully whereas like huge gain margin harvest somewhere would like use graph compatible latest force install driver library mismatch training even le,issue,positive,positive,positive,positive,positive,positive
961719115,"I can offer the following observations for MLS Spanish:

1. Using the default `max_mel_frames = 900` causes utterances longer than 11.25 sec to be discarded. This can be a problem because the audios are evenly distributed in duration between 10-20 sec (see MLS paper). The raw dataset has 917 hours of Spanish audio, but using defaults will cut that to 202 hours.
2. Here are the unique symbols in the transcripts (for [symbols.py](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/7432046efc23cabf176f9fdc8d2fd67020059478/synthesizer/utils/symbols.py#L11)):
    * `_characters = ""'-aábcdeéfghiíjklmnñoópqrstuúüvwxyz ""`
3. Frequently used abbreviations in the transcripts need to be added to [text cleaners](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/7432046efc23cabf176f9fdc8d2fd67020059478/synthesizer/utils/cleaners.py#L20-L40).
    * [numbers.py](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/synthesizer/utils/numbers.py) also needs to be updated for inference
4. The transcripts have serious quality control issues, but the resulting TTS was still usable.
    * `rand om space s appearing within a word`
    * `sometimes entire words have spaces i n t e r s p e r s e d  throughout`
    * failure to normalize numbers when they appear as Roman numerals, e.g. `xiii`
    * accented letters substituted with non-accented versions in some areas, e.g. using `a` in place of `á`
5. If there are problems with the synthesizer generating extra sounds, the [stop threshold](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/7432046efc23cabf176f9fdc8d2fd67020059478/synthesizer/models/tacotron.py#L458) can be lowered to help prevent this. A threshold of 0.00001 seems to work well.",offer following default longer sec problem evenly distributed duration sec see paper raw audio cut unique frequently used need added text also need inference serious quality control resulting still usable rand om space within word sometimes entire throughout failure normalize appear substituted place synthesizer generating extra stop threshold help prevent threshold work well,issue,negative,negative,neutral,neutral,negative,negative
961696396,"Thank you @blue-fish 

My own issue is that after about 5secs, the audio generated is no longer audible.
I had trained the model using 33 mins data with this code
`python3 synthesizer_preprocess_audio.py datasets_root --datasets_name LibriTTS --subfolders my_custom_data --no_alignments`
see sample below
",thank issue audio longer audible trained model data code python see sample,issue,negative,neutral,neutral,neutral,neutral,neutral
961694550,"**Experiment**
So far (without running the benchmark) I have noticed that the GPU power consumption is very low during training. Indeed on my RTX 3070 power draw (`nvidia-smi -q -d POWER | grep Draw`) shows around 60W (compared to 130W while mining or 100W while playing game). So I doubt GPU is used at its full capabilities.

**Expected behaviour**
 If the power limit is set to 130W I would expect the power draw to reach this limit while training since many users reports 100% utilization of their GPU during training. Behaviour should be similar to mining.",experiment far without running power consumption low training indeed power draw power draw around mining game doubt used full behaviour power limit set would expect power draw reach limit training since many utilization training behaviour similar mining,issue,negative,positive,neutral,neutral,positive,positive
961674905,"I use the gradual training technique and recommend it. It's a feature of the original Tacotron1 implementation that we selected for this repo ([see here](https://github.com/fatchord/WaveRNN/blob/3595219b2f2f5353f0867a7bb59abcb15aba8831/hparams.py#L84-L87)) but was removed during development. 

For the final merge of #472, I switched to constant r=2 and batch=12 to mimic the training conditions of my Tacotron2 model in #538. I did this to understand how Tacotron1 performed compared to the Tacotron2 model it replaced. I would have liked to change the default hparams back to gradual training, but did not have the time to train and benchmark a new pretrained model.",use gradual training technique recommend feature original implementation selected see removed development final merge switched constant mimic training model understand model would change default back gradual training time train new model,issue,positive,positive,positive,positive,positive,positive
961657136,"Nividia is providing a [benchmark](https://catalog.ngc.nvidia.com/orgs/nvidia/resources/tacotron_2_and_waveglow_for_pytorch/performance) for Tacotron 2. Does it make sense to run it and compare the results with what we have with Real Time Voice Cloning during training, or does it compare apples with oranges ? The goal of that is to check whether the poor performance comes from Drivers / CUDA or from code or from Anaconda.",providing make sense run compare real time voice training compare goal check whether poor performance come code anaconda,issue,negative,negative,neutral,neutral,negative,negative
961651892,"That's interesting, thanks! I found that 20 was the maximum for r on my machine. After reading the paper I confidently set it to 16 with a batch-size of 1 and resumed the training with the new parameters (I just stopped and relaunched it from the last checkpoint). It ran overnight but when I hear the output I found the 25k model much better (more intelligible), than the 150k model. Does it demonstrate that my r setting is too high ? Do you recommend ""[gradual training](https://erogol.com/gradual-training-with-tacotron-for-faster-convergence/)"" ?",interesting thanks found maximum machine reading paper confidently set training new stopped last ran overnight hear output found model much better intelligible model demonstrate setting high recommend gradual training,issue,positive,positive,positive,positive,positive,positive
961283477,"> Ah perfect timing. Was just deciding between ogg and m4a before turnin it into wav, but if ogg will work without change that’s an easy choice.
> 
> Thanks from over here as well. Everything running really smoothly. Gonna start training now though for a different language, so hope things stay that way 🤞
> 
> Are there any other resources besides the research result page and the data in the repo? This repo seems weirdly underused...

I passed the entire Friday searching for more updated repos which have their own papers, I took a look at mozilla's, tacotron's, tacotron2's, among others based on those repos...but for all of them you need a dataset to train the vocoder (or at least, that's what I understood from their documentation and discussions). With the code in this repo you only need one sample to hear a very similar voice to the target voice you want to clone. Which language are you going to train? It would be very helpful if you share your experience after doing it since I will start training with Spanish this weekend and I have seen that many more people wanted to do so, but they haven't shared their experiences after doing it (if they finally did it).",ah perfect timing work without change easy choice thanks well everything running really smoothly gon na start training though different language hope stay way besides research result page data weirdly entire searching took look among based need train least understood documentation code need one sample hear similar voice target voice want clone language going train would helpful share experience since start training weekend seen many people finally,issue,positive,positive,positive,positive,positive,positive
961249033,"A synthesizer model is only compatible with the speaker encoder that generated the embeds used for training. So our pretrained synthesizer cannot be used with UniSpeech-SAT.

You'll need to train a new synthesizer from scratch (making sure to generate the embed data using your UniSpeech-SAT encoder model).",synthesizer model compatible speaker used training synthesizer used need train new synthesizer scratch making sure generate embed data model,issue,negative,positive,positive,positive,positive,positive
961231244,"Ah perfect timing. Was just deciding between ogg and m4a before turnin it into wav, but if ogg will work without change that’s an easy choice.

Thanks from over here as well. Everything running really smoothly. Gonna start training now though for a different language, so hope things stay that way 🤞

Are there any other resources besides the research result page and the data in the repo? This repo seems weirdly  underused...",ah perfect timing work without change easy choice thanks well everything running really smoothly gon na start training though different language hope stay way besides research result page data weirdly,issue,positive,positive,positive,positive,positive,positive
961212132,"Thank you, I replaced the speaker encoder with UniSpeech-SAT, but the results are very bad (it just outputs noise). I'll try to figure out what the problem is, could UniSpeech-SAT really change the output that much?",thank speaker bad noise try figure problem could really change output much,issue,negative,negative,neutral,neutral,negative,negative
961134520,"The reduction factor r is explained in the Tacotron1 paper (1703.10135). You'll need to do your own experimentation to identify the best value for this parameter.

> An important trick we discovered was predicting multiple, non-overlapping output frames at each decoder step. Predicting r frames at once divides the total number of decoder steps by r, which reduces model size, training time, and inference time.",reduction factor paper need experimentation identify best value parameter important trick discovered multiple output step total number model size training time inference time,issue,positive,positive,positive,positive,positive,positive
961123622,"Partial embeds are described in section 3.3.2 of Corentin's thesis (link in README.md).

The toolbox does not use partial_embeds, so you can substitute None when making the Utterance namedtuple.",partial section thesis link toolbox use substitute none making utterance,issue,negative,negative,neutral,neutral,negative,negative
961058480,"Aaaah! Thank you very much! I was sure that 25 (or 25,000) is a product of two parameters but I couldn't find them. Thank you, blue-fish!

Best regards
Marc",thank much sure product two could find thank best marc,issue,positive,positive,positive,positive,positive,positive
961053448,Do you have the same problem when loading one of the mp3 files included in the samples folder?,problem loading one included folder,issue,negative,neutral,neutral,neutral,neutral,neutral
961027410,"It's this line of code. You can change `steps=2000` to a higher value to increase maximum generation time. This is a safeguard to prevent the synthesizer from getting stuck in an infinite loop. Each step is equal to 0.0125 sec.

https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/7432046efc23cabf176f9fdc8d2fd67020059478/synthesizer/models/tacotron.py#L417

",line code change higher value increase maximum generation time safeguard prevent synthesizer getting stuck infinite loop step equal sec,issue,positive,positive,neutral,neutral,positive,positive
960816473,"I found the inference inside toolbox in __init__.py:
embed, partial_embeds, _ = encoder.embed_utterance(encoder_wav, return_partials=True)

This function is called two times. What are ""partial_embeds""? Do I need them? Or can I just pass them as none?",found inference inside toolbox embed function two time need pas none,issue,negative,neutral,neutral,neutral,neutral,neutral
960619145,"I was not sure how to change the parameters for batch-size and r parameter. So I edited the `hparams.py` file and changed the second element from the array. Increasing r from 2 to 8 boosted the rate to 1.7 but after a while the program crashed. Now it is set to 4 and batch-size to 16 and the rate settled at around 1 step / s. Yet I don't know the consequences of these settings on the generated model. 

This [section of Nvidia Tacotron 2 repo](https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/SpeechSynthesis/Tacotron2/README.md#parameters) indicates a batch size of 48 for Ampere GPU with FP32 (enabled by default) but I cannot reach this value (they use 40 GB GPU  mine is only 8 GB).",sure change parameter file second element array increasing rate program set rate settled around step yet know model section batch size ampere default reach value use mine,issue,positive,positive,positive,positive,positive,positive
960485952,"@Ananas120 what did you use for hardware ? I will try to increase batch-size from the default 12 to 32 and the r parameter to 4 as reported by @MGSousa in [#700 (comment)](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/700#issuecomment-884408801).

@blue-fish : did you encounter too this slow training ? What should be the rate value ?",ananas use hardware try increase default parameter comment encounter slow training rate value,issue,positive,negative,negative,negative,negative,negative
960483401,"Same problem here too, on Ubuntu 20.04 without Anaconda and on RTX 3070 under Python 3.8.8. Around 0.52-0.54 step / s.",problem without anaconda python around step,issue,negative,neutral,neutral,neutral,neutral,neutral
960294265,"I just wanted to clean up a bit. As I can only train with 0.5 steps/s and as I train over colab only, it will take some time. The first test was not that promising, sometimes loss climbs up let's say from 4.1 to 4.8. But I am not sure if it matters with only a few thousand steps. Generally, should the loss never climb in this way (at least after certain amount of steps e.g. 100k,500k)?

I'll attend a picture where the loss in case of usual mels-gta training sometimes randomly went up and audio results are just noise afterwards. Maybe it gets better after e.g. 1 million steps?
![vocoderErrorRising](https://user-images.githubusercontent.com/44262699/140233017-a1af3c39-317e-47b6-8bf1-7a77de82de96.png)",clean bit train train take time first test promising sometimes loss let say sure thousand generally loss never climb way least certain amount attend picture loss case usual training sometimes randomly went audio noise afterwards maybe better million,issue,negative,positive,positive,positive,positive,positive
960199908,"> > If you have any handy code that could help with making the .txt from the transcript.txt
> 
> @AlexSteveChungAlvarez https://gist.github.com/blue-fish/11552e89e95f32c14a370935c58f426c

Thank you very much! I will be using it this weekend.


> I have also shared modifications to support audio preprocessing of the compressed .opus files: [blue-fish@b4e6c11](https://github.com/blue-fish/Real-Time-Voice-Cloning/commit/b4e6c11c429bc6f8cdd86c048cba32413ab4109e)

I've found out that the program itself also accepts .ogg files (most of the audios sent via Whatsapp).",handy code could help making thank much weekend also support audio compressed found program also sent via,issue,positive,positive,positive,positive,positive,positive
960077096,@Ca-ressemble-a-du-fake Slow training is a known bug that we don't have a solution for at this time. Please contribute ideas and observations to #700,slow training known bug solution time please contribute,issue,positive,negative,negative,negative,negative,negative
960058696,I have also shared modifications to support audio preprocessing of the compressed .opus files: https://github.com/blue-fish/Real-Time-Voice-Cloning/commit/b4e6c11c429bc6f8cdd86c048cba32413ab4109e,also support audio compressed,issue,negative,neutral,neutral,neutral,neutral,neutral
960038402,"> If you have any handy code that could help with making the .txt from the transcript.txt

@AlexSteveChungAlvarez https://gist.github.com/blue-fish/11552e89e95f32c14a370935c58f426c",handy code could help making,issue,negative,positive,positive,positive,positive,positive
959815168,"> Thanks again. I started training the synthesizer with @Ananas120 dataset (see [#492 comment](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/492#issuecomment-673670822)) since they reported to have good results. I encountered the following warning : `Real-Time-Voice-Cloning-master/synthesizer/synthesizer_dataset.py:84: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:201.) embeds = torch.tensor(embeds)`
> 
> So I followed this piece of advice and replaced the pointed line with `embeds = torch.tensor(np.array(embeds))`. However the training is still done at a rate of 0.52 step / s (GPU is RTX 3070). So 25k steps will be reached in approx 13 hours. Did I make the correct decision to change the code with regards to the warning hint ? Why wasn't it changed on this repo ?
> 
> Will the results come faster using Google Colab Notebook (I've just discovered this technology while browsing the topic, sorry if the question is silly) ?

It depends on multiple parameters such as the batch_size, whether you take into account the processing or not, ... For me performances were around 10sec / batch (batch_size 32) with the processing so yeah 2sec / epoch seems to be really good performances (for the Tacotron2 training)",thanks training synthesizer ananas see comment since good following warning tensor list extremely slow please consider converting list single converting tensor triggered internally piece advice pointed line however training still done rate step make correct decision change code warning hint come faster notebook discovered technology browsing topic sorry question silly multiple whether take account around sec batch yeah sec epoch really good training,issue,positive,positive,neutral,neutral,positive,positive
959806628,"Thanks again. I started training the synthesizer with @Ananas120 dataset (see [#492 comment](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/492#issuecomment-673670822)) since they reported to have good results. I encountered the following warning :
`Real-Time-Voice-Cloning-master/synthesizer/synthesizer_dataset.py:84: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)
  embeds = torch.tensor(embeds)`

So I followed this piece of advice and replaced the pointed line with `embeds = torch.tensor(np.array(embeds))`. However the training is still done at a rate of 0.52 step / s (GPU is RTX 3070). So 25k steps will be reached in approx 13 hours. Did I make the correct decision to change the code with regards to the warning hint ? Why wasn't it changed on this repo ?

Will the results come faster using Google Colab Notebook (I've just discovered this technology while browsing the topic, sorry if the question is silly) ?",thanks training synthesizer ananas see comment since good following warning tensor list extremely slow please consider converting list single converting tensor triggered internally piece advice pointed line however training still done rate step make correct decision change code warning hint come faster notebook discovered technology browsing topic sorry question silly,issue,positive,negative,neutral,neutral,negative,negative
957998299,"I propose that we use NVIDIA's Tacotron2 and train a voice cloning model from scratch on public datasets. Under these conditions, would your company need to be involved at all?",propose use train voice model scratch public would company need involved,issue,negative,neutral,neutral,neutral,neutral,neutral
957991291,"> @dumblob I did manage to get some usable output; however, the attention plots remained distorted. You can look [here](https://drive.google.com/drive/folders/1vlR-TA7gjKzjYylxzRnA_HzZEyWiLeOk?usp=sharing) at the synthesized samples in the Czech language.

Thanks.

>I plan to look into this problem once again as I have a few ideas on improving the final quality.

Feel free to share your findings, I'd be interested in them.

>Are you working on something related?

Nothing serious - I'm just looking for a way to have offline home automation with voice assistant which shall have ""flavours"" depending on the liking of the users.",manage get usable output however attention distorted look language thanks plan look problem improving final quality feel free share interested working something related nothing serious looking way home voice assistant shall depending liking,issue,positive,positive,neutral,neutral,positive,positive
957963468,"Maybe we should create some kind of space where we can communicate? I think the maximum I can share is a model, weights and a fairly detailed guide(without extending the code of this repository), but this still needs to be discussed with the company's management.",maybe create kind space communicate think maximum share model fairly detailed guide without extending code repository still need company management,issue,positive,positive,positive,positive,positive,positive
957941551,"@e0xextazy I am going to mark the previous set of posts as off-topic. If you are able to contribute to this repo, it would be appreciated. If not, I understand.",going mark previous set able contribute would understand,issue,negative,positive,positive,positive,positive,positive
957940660,"> In the second tacotron, the input to the attention layer has dimension 512, I concatenate to it the speaker embedding vector of dimension 256 and after that I apply the linear layer 768 -> 512

Try it without the ""linear layer 768 -> 512"". That is introducing an information bottleneck which is making it difficult to learn attention.",second input attention layer dimension concatenate speaker vector dimension apply linear layer try without linear layer information bottleneck making difficult learn attention,issue,negative,negative,negative,negative,negative,negative
957923784,"I didn’t quite use this repository and didn’t integrate into it, but I was inspired by it. I've been researching your repository for a very long time. Now I will tell you how I implemented everything. In the second tacotron, the input to the attention layer has dimension 512, I concatenate to it the speaker embedding vector of dimension 256 and after that I apply the linear layer 768 -> 512
I cannot share the complete code with my company policy",quite use repository integrate inspired repository long time tell everything second input attention layer dimension concatenate speaker vector dimension apply linear layer share complete code company policy,issue,positive,positive,neutral,neutral,positive,positive
957911926,"> It's just that I already tried to implement taco 2 myself

I was not aware of this. If you have made progress on integrating Tacotron2 into this repo, share your code and we can start from there.

The approach I suggest is to first try the pretrained models in the toolbox UI to make sure everything is integrated properly. Once that is working, it is a good foundation for remaining work.",already tried implement aware made progress share code start approach suggest first try toolbox make sure everything properly working good foundation work,issue,positive,positive,positive,positive,positive,positive
957891186,"It's just that I already tried to implement taco 2 myself, and now it is learning, but it seems to me that I did something wrong, because my alignments in the 40k range look something like this
![Снимок экрана 2021-11-02 в 14 07 41](https://user-images.githubusercontent.com/49094658/139898421-78b8424d-f9e6-41d4-9dcb-dcc5283a3d41.png)
![Снимок экрана 2021-11-02 в 14 07 52](https://user-images.githubusercontent.com/49094658/139898450-bf54ad27-74c9-4b09-b4a8-2260964b2091.png)
![Снимок экрана 2021-11-02 в 14 08 13](https://user-images.githubusercontent.com/49094658/139898459-8750b999-598b-4f28-b81e-b12363e467bd.png)
I took nvidia's implementation of Tacotron 2 and added the addition of a speaker vector in front of the attention layer. For training I use LibriTTS 100 + 360, HiFi TTS, VCTK",already tried implement learning something wrong range look something like took implementation added addition speaker vector front attention layer training use,issue,negative,negative,negative,negative,negative,negative
957805874,How should the architecture of the Tacotron 2 be modified to be able to clone a voice?,architecture able clone voice,issue,negative,positive,positive,positive,positive,positive
957665407,"Thank you, I will search the code for the inference part to check how the input goes into the model and where the output is passed to. When I find this part, I will replace it and make sure my output is a vector with the same size as the output of the original model.",thank search code inference part check input go model output find part replace make sure output vector size output original model,issue,positive,positive,positive,positive,positive,positive
957589098,"The speaker encoder is the easiest part of the model to swap out. All you need to ensure is that the inference produces an output of the same size.

I'm still experimenting with the encoder and so far have only made small tweaks to architecture. One thing that surprises me is how similar the voice cloning output sounds compared to our pretrained models, even when the new encoder model produces a completely different embedding for the same utterance. It must all even out in the synthesizer training.",speaker easiest part model swap need ensure inference output size still far made small architecture one thing similar voice output even new model completely different utterance must even synthesizer training,issue,positive,negative,neutral,neutral,negative,negative
957571978,"I will provide guidance to those who can work independently and will contribute the results as open source.

First task is to get our toolbox to use NVIDIA's pretrained Tacotron2 in place of our synthesizer. The result of this step will be a single voice TTS since the NVIDIA model does not support voice cloning.
* Get model files from https://github.com/NVIDIA/tacotron2
* Use Griffin-Lim as the vocoder, not Waveglow. The info you need to use NVIDIA's pretrained model can be found in [tacotron2:hparams.py](https://github.com/NVIDIA/tacotron2/blob/master/hparams.py) and [tacotron2:layers.py](https://github.com/NVIDIA/tacotron2/blob/master/layers.py)
* Rule 1: You're not allowed to modify any toolbox files except for [demo_toolbox.py](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/demo_toolbox.py).
* Rule 2: You must use [synthesizer/inference.py](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/synthesizer/inference.py). Try to change it as little as possible.
    * Hint: because the NVIDIA model does not support voice cloning, do not pass the speaker embedding to it.

By the way, this is very similar to how I started working on #472.",provide guidance work independently contribute open source first task get toolbox use place synthesizer result step single voice since model support voice get model use need use model found rule modify toolbox except rule must use try change little possible hint model support voice pas speaker way similar working,issue,positive,negative,neutral,neutral,negative,negative
957441944,"Okay thank you, and also thank you for all your support here!

I will then follow these steps now. If it does not work, I will change to 16k.",thank also thank support follow work change,issue,positive,neutral,neutral,neutral,neutral,neutral
957362439,"Actually, it would make sense to first try training on the ground truth mels (in `<datasets_root>/SV2TTS/synthesizer/mels`) to confirm the problem repeats there too.

Then use `synthesizer_preprocess_audio.py` to process the dataset again at 16k, and train a vocoder on the 16k ground truth mels. For an equal comparison set `trim_silence = False` in synthesizer/hparams.py , because webrtcvad works at 16k but not 48k.

If 48k ground truth fails, while 16k GT works, then we can probably conclude it is an issue of the vocoder model. Though I'm not sure anything could be done in that case.",actually would make sense first try training ground truth confirm problem use process train ground truth equal comparison set false work ground truth work probably conclude issue model though sure anything could done case,issue,positive,positive,neutral,neutral,positive,positive
957350212,"> There is no reason the models can't be substituted with more recent ones. Just need someone to work on it.

If you can tell me how to do it in detail, I can try it.",reason ca substituted recent need someone work tell detail try,issue,negative,neutral,neutral,neutral,neutral,neutral
957339546,"I didn't try it, but will give it a try with 16k to see whether it is training successfully (even if speed is then 3x).",try give try see whether training successfully even speed,issue,negative,positive,positive,positive,positive,positive
957339495,Closing this issue as the original problem has been resolved. Open a new issue to report other bugs or problems.,issue original problem resolved open new issue report,issue,negative,positive,positive,positive,positive,positive
957337603,"No idea why this is happening. Can you successfully train a vocoder on the same dataset, but at a lower sample rate?",idea happening successfully train lower sample rate,issue,negative,positive,positive,positive,positive,positive
957330483,"> The voices sound NOTHING like the originals.
> Hoping you have some directions for me to improve the output quality.

The quality is really hit and miss. Keep in mind the training data came from audiobooks. The model will struggle with background noise, music, or sound effects. The quality is something I am trying to improve, but it is difficult within the limitations of the SV2TTS framework, consumer hardware, and publicly-available datasets.

To better match a particular voice, you can try the single-speaker finetuning method I have described in #437. It involves making a labeled dataset and continuing the training of the pretrained models.",sound nothing like improve output quality quality really hit miss keep mind training data came model struggle background noise music sound effect quality something trying improve difficult within framework consumer hardware better match particular voice try method making training,issue,negative,positive,positive,positive,positive,positive
957255411,"@dumblob I did manage to get some usable output; however, the attention plots remained distorted. You can look [here](https://drive.google.com/drive/folders/1vlR-TA7gjKzjYylxzRnA_HzZEyWiLeOk?usp=sharing) at the synthesized samples in the Czech language. I plan to look into this problem once again as I have a few ideas on improving the final quality. Are you working on something related?",manage get usable output however attention distorted look language plan look problem improving final quality working something related,issue,negative,neutral,neutral,neutral,neutral,neutral
957127757,"Which data structure should I adopt in my datasets. The one you showed in [#437](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-666099538) or the one that is used in [logs-singlespeaker](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issue-663639627) ?

",data structure adopt one one used,issue,negative,neutral,neutral,neutral,neutral,neutral
957055142,"(Sorry I updated the last comment,  I've been updating as maneuvering around through the software.  Still need your help!)",sorry last comment around still need help,issue,negative,negative,negative,negative,negative,negative
956349942,"I downloaded your latest ffmpeg. exe (and ffplay.exe and ffprobe.exe) as directed from the newl link on 682 and replaced it as before.

Wanting to make sure I got it down, I watched and followed the videos at 
https://www.youtube.com/watch?v=nvBF0Rf2m2Y
and
https://www.youtube.com/watch?v=12rdn9jazwE
disrgarding their installation instructions, focusing only on the providing the 5-10 seconds mp3 clips for it to learn the target voices.  I made 4 for each.

The voices sound NOTHING like the originals.
Hoping you have some directions for me to improve the output quality.
The samples I stripped from blu-rays, saved to mp4, then converted to highest quality mp3s (256 kbps).

Thank you for your help, and additional thanks for your ongoing help.  It is appreciated!",latest directed link wanting make sure got watched installation providing clip learn target made sound nothing like improve output quality stripped saved converted highest quality thank help additional thanks ongoing help,issue,positive,positive,positive,positive,positive,positive
956051319,"I also tested with other voc upsample_factor (5,8,15), but without success.",also tested without success,issue,negative,positive,positive,positive,positive,positive
955951026,"> BUT... the link for ffmpeg in those instructions leads to an otherwise blank page with ""Not Found"" printed on it.

Thanks for reporting this. I have updated the ffmpeg link.

The instructions need to be followed to the letter to work. The extraction path in step 1 was messed up creating a nested folder. The simplest fix at this point is to move the ffmpeg.exe to `C:\Real-Time-Voice-Cloning-master\Real-Time-Voice-Cloning-master` so Python can see it.

The `python demo_toolbox.py` command failed because it couldn't find ffmpeg. You do not need to install webrtcvad. If you really want it, you can avoid installing the Build Tools for Visual Studio by switching the command to `pip install webrtcvad-wheels`.",link otherwise blank page found printed thanks link need letter work extraction path step folder fix point move python see python command could find need install really want avoid build visual studio switching command pip install,issue,negative,positive,neutral,neutral,positive,positive
955944754,"> Now the `run_ID` parameter for `synthesizer_train.py` has to be `pretrained`, right ?

You can use anything for run_ID. You will need to delete the English pretrained model if you choose `pretrained` (because the model will not be compatible with your new symbols list).

> use 0.2 hours of extracts from the target voice to fine tune the model for that voice.

When training the initial model, you can try including your target voice with the rest of the training data. Then it may not be necessary to finetune.

> By the way during synthesizer training are there parameters to monitor that tell the model is improving in quality ? When can I stop the training process ?

For your first model from scratch, I suggest training an English synthesizer model using the instructions on the [wiki page](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training). The model backs up every 25k steps, try each of those to get an idea for how many training steps are necessary. The experience is useful to know if your French model is training well.",parameter right use anything need delete model choose model compatible new list use target voice fine tune model voice training initial model try target voice rest training data may necessary way synthesizer training monitor tell model improving quality stop training process first model scratch suggest training synthesizer model page model every try get idea many training necessary experience useful know model training well,issue,positive,positive,positive,positive,positive,positive
955921880,"(venv) C:\Real-Time-Voice-Cloning-master\Real-Time-Voice-Cloning-master>pip install torch
Collecting torch
  Downloading torch-1.10.0-cp37-cp37m-win_amd64.whl (226.6 MB)
     |████████████████████████████████| 226.6 MB 70 kB/s
Collecting typing-extensions
  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)
Installing collected packages: typing-extensions, torch
Successfully installed torch-1.10.0 typing-extensions-3.10.0.2

(venv) C:\Real-Time-Voice-Cloning-master\Real-Time-Voice-Cloning-master>python demo_toolbox.py
C:\Real-Time-Voice-Cloning-master\Real-Time-Voice-Cloning-master\encoder\audio.py:13: UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended.
  warn(""Unable to import 'webrtcvad'. This package enables noise removal and is recommended."")
Arguments:
    datasets_root:    None
    enc_models_dir:   encoder\saved_models
    syn_models_dir:   synthesizer\saved_models
    voc_models_dir:   vocoder\saved_models
    cpu:              False
    seed:             None
    no_mp3_support:   False

Librosa will be unable to open mp3 files if additional software is not installed.
Please install ffmpeg or add the '--no_mp3_support' option to proceed without support for mp3 files.

(venv) C:\Real-Time-Voice-Cloning-master\Real-Time-Voice-Cloning-master>pip install webrtcvad
Collecting webrtcvad
  Downloading webrtcvad-2.0.10.tar.gz (66 kB)
     |████████████████████████████████| 66 kB 1.5 MB/s
  Preparing metadata (setup.py) ... done
Using legacy 'setup.py install' for webrtcvad, since package 'wheel' is not installed.
Installing collected packages: webrtcvad
    Running setup.py install for webrtcvad ... error
    ERROR: Command errored out with exit status 1:
     command: 'c:\real-time-voice-cloning-master\venv\scripts\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\Dad\\AppData\\Local\\Temp\\pip-install-c1zgdi5d\\webrtcvad_10269592aace4f04b275a145791bf471\\setup.py'""'""'; __file__='""'""'C:\\Users\\Dad\\AppData\\Local\\Temp\\pip-install-c1zgdi5d\\webrtcvad_10269592aace4f04b275a145791bf471\\setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'C:\Users\Dad\AppData\Local\Temp\pip-record-cdezpg5m\install-record.txt' --single-version-externally-managed --compile --install-headers 'c:\real-time-voice-cloning-master\venv\include\site\python3.7\webrtcvad'
         cwd: C:\Users\Dad\AppData\Local\Temp\pip-install-c1zgdi5d\webrtcvad_10269592aace4f04b275a145791bf471\
    Complete output (9 lines):
    running install
    running build
    running build_py
    creating build
    creating build\lib.win-amd64-3.7
    copying webrtcvad.py -> build\lib.win-amd64-3.7
    running build_ext
    building '_webrtcvad' extension
    error: Microsoft Visual C++ 14.0 is required. Get it with ""Build Tools for Visual Studio"": https://visualstudio.microsoft.com/downloads/
    ----------------------------------------
ERROR: Command errored out with exit status 1: 'c:\real-time-voice-cloning-master\venv\scripts\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\Dad\\AppData\\Local\\Temp\\pip-install-c1zgdi5d\\webrtcvad_10269592aace4f04b275a145791bf471\\setup.py'""'""'; __file__='""'""'C:\\Users\\Dad\\AppData\\Local\\Temp\\pip-install-c1zgdi5d\\webrtcvad_10269592aace4f04b275a145791bf471\\setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'C:\Users\Dad\AppData\Local\Temp\pip-record-cdezpg5m\install-record.txt' --single-version-externally-managed --compile --install-headers 'c:\real-time-voice-cloning-master\venv\include\site\python3.7\webrtcvad' Check the logs for full command output.

(venv) C:\Real-Time-Voice-Cloning-master\Real-Time-Voice-Cloning-master>


And with that I'm going to bed.",pip install torch torch collected torch successfully python unable import package noise removal warn unable import package noise removal none false seed none false unable open additional please install add option proceed without support pip install done legacy install since package collected running install error error command exit status command io o open else import setup setup code compile code install record compile complete output running install running build running build running building extension error visual get build visual studio error command exit status io o open else import setup setup code compile code install record compile check full command output going bed,issue,negative,negative,neutral,neutral,negative,negative
955920316,"and yes, I was running the command prompt as an administrator.  ",yes running command prompt administrator,issue,negative,neutral,neutral,neutral,neutral,neutral
955919963,"I did so.  It worked.  BUT... the link for ffmpeg in those instructions leads to an otherwise blank page with ""Not Found"" printed on it.
I did the previous version of ffmpeg and made it all the way to #13 of #642 and then received this error:

(venv) C:\Real-Time-Voice-Cloning-master\Real-Time-Voice-Cloning-master>python demo_toolbox.py
Traceback (most recent call last):
  File ""demo_toolbox.py"", line 2, in <module>
    from toolbox import Toolbox
  File ""C:\Real-Time-Voice-Cloning-master\Real-Time-Voice-Cloning-master\toolbox\__init__.py"", line 1, in <module>
    from toolbox.ui import UI
  File ""C:\Real-Time-Voice-Cloning-master\Real-Time-Voice-Cloning-master\toolbox\ui.py"", line 6, in <module>
    from encoder.inference import plot_embedding_as_heatmap
  File ""C:\Real-Time-Voice-Cloning-master\Real-Time-Voice-Cloning-master\encoder\inference.py"", line 2, in <module>
    from encoder.model import SpeakerEncoder
  File ""C:\Real-Time-Voice-Cloning-master\Real-Time-Voice-Cloning-master\encoder\model.py"", line 5, in <module>
    from torch.nn.utils import clip_grad_norm_
ModuleNotFoundError: No module named 'torch'

(venv) C:\Real-Time-Voice-Cloning-master\Real-Time-Voice-Cloning-master>",worked link otherwise blank page found printed previous version made way received error python recent call last file line module toolbox import toolbox file line module import file line module import file line module import file line module import module,issue,negative,negative,neutral,neutral,negative,negative
955906510,"Thanks for this piece of information. So in order to clone a single voice in French am I on the right tracks with the following procedure : 
- use ""general"" French dataset (publicly available) to train the synthesizer to learn French pronunciation. Now the `run_ID` parameter for `synthesizer_train.py` has to be `pretrained`, right ?
- use 0.2 hours of extracts from the target voice to fine tune the model for that voice. `run_ID` can be anything or does it still have to be `pretrained` ?

By the way during synthesizer training are there parameters to monitor that tell the model is improving in quality ? When can I stop the training process ?",thanks piece information order clone single voice right following procedure use general publicly available train synthesizer learn pronunciation parameter right use target voice fine tune model voice anything still way synthesizer training monitor tell model improving quality stop training process,issue,positive,positive,positive,positive,positive,positive
955874561,"What I meant is, train the synthesizer from scratch on a French dataset. The English encoder can be reused.

The English synthesizer was trained on 26,000 minutes of English data. You continued the synthesizer training on a custom dataset with only 1 minute of French data. That doesn't work because the French dataset is insufficient by a few orders of magnitude. Much more data is needed to learn French pronunciation. And if you have that much data, it is enough to train from scratch.",meant train synthesizer scratch synthesizer trained data continued synthesizer training custom minute data work insufficient magnitude much data learn pronunciation much data enough train scratch,issue,negative,positive,positive,positive,positive,positive
955774579,@Mremenar Try running the Windows command prompt as administrator.,try running command prompt administrator,issue,negative,neutral,neutral,neutral,neutral,neutral
955764117,"I made it to number 8 on #642:

C:\Users\Dad>cd C:\Real-Time-Voice-Cloning-master

C:\Real-Time-Voice-Cloning-master>python -m venv venv

C:\Real-Time-Voice-Cloning-master>venv\Scripts\activate.bat

(venv) C:\Real-Time-Voice-Cloning-master>pip install --upgrade pip
Collecting pip
  Using cached pip-21.3.1-py3-none-any.whl (1.7 MB)
Installing collected packages: pip
  Attempting uninstall: pip
    Found existing installation: pip 20.1.1
    Uninstalling pip-20.1.1:
      Successfully uninstalled pip-20.1.1
ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'C:\\Users\\Dad\\AppData\\Local\\Temp\\pip-uninstall-gezy930h\\pip.exe'
Consider using the `--user` option or check the permissions.


(venv) C:\Real-Time-Voice-Cloning-master>",made number python pip install upgrade pip pip collected pip pip found installation pip successfully uninstalled error could install due access consider user option check,issue,negative,positive,positive,positive,positive,positive
955761568,"Sorry, I responded before reading #642.  I'll start from scratch.",sorry reading start scratch,issue,negative,negative,negative,negative,negative,negative
955760137,Thanks for your answer. Is it because French is too far from English that [comment #492](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/492#issuecomment-673690870) is not valid or is it because you've noticed that using the English encoder for other languages eventually gives subpar results ? ,thanks answer far comment valid eventually,issue,negative,positive,positive,positive,positive,positive
955752752,Thank you blue-fish.  Can I use the #642 instructions like a band-aid for the previous install?  Or should I slate clean my PC from the previous python/cuda installation before attempt?,thank use like previous install slate clean previous installation attempt,issue,positive,positive,neutral,neutral,positive,positive
955657597,"> How can I use both? Combine them both together in my datasets_root and train the combination once to 300k steps?

Exactly.",use combine together train combination exactly,issue,negative,positive,positive,positive,positive,positive
955651341,"> 1. For each audio file, you'll need to make a corresponding .txt file using the data in `transcripts.txt`.
> 2. Write the text files to the same location as the .flac files.
> 3. Update the directory structure so it looks like LibriSpeech.
>    
>    * [Single speaker fine-tuning process and results #437 (comment)](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-666099538)
> 4. Run `synthesiser_preprocess_audio.py` with the `--no_alignments` option.
> 
> In my limited experience with MLS, the audio files were not cut well and often stopped in the middle of a word or contained extra sounds. This is probably an issue of the automatic segmentation method used by the dataset authors. The extra sounds caused problems when training the stop token prediction.

Hello! I have been all day reading the issues for getting to know what to do. I want to train on Spanish datasets tux-100h and Common Voice, which were proportioned in some of the past issues that refer to different languages. These days I will try to figure out how to order the datasets to have the same structure as LibriSpeech. If you have any handy code that could help with making the .txt from the transcript.txt it would be very useful, since first I will play around with train-clean-100 to get familiar with the process as you suggested in another issue. Thanks for all the effort, I found this experiment yesterday and I have seen how much you contributted to mantain it alive.",audio file need make corresponding file data write text location update directory structure like single speaker process comment run option limited experience audio cut well often stopped middle word extra probably issue automatic segmentation method used extra training stop token prediction hello day reading getting know want train common voice proportioned past refer different day try figure order structure handy code could help making would useful since first play around get familiar process another issue thanks effort found experiment yesterday seen much alive,issue,positive,positive,neutral,neutral,positive,positive
955648152,"Pretrained models only support English. You cannot finetune them to a different language.

If you want a French model, it needs to be trained from scratch with an appropriate dataset.",support different language want model need trained scratch appropriate,issue,negative,positive,positive,positive,positive,positive
955647564,"These setup instructions have worked very well for the community, if they are followed to the letter: https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542",setup worked well community letter,issue,negative,neutral,neutral,neutral,neutral,neutral
955647354,"The poorlydocumented.com instructions were written at a time when this repo required obsolete dependencies (Tensorflow 1.15) that made installation very difficult.

Since #472 we have removed the Tensorflow 1.x requirement which makes installation a lot easier. I suggest following this set of installation instructions for Windows: https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542",written time obsolete made installation difficult since removed requirement installation lot easier suggest following set installation,issue,negative,negative,negative,negative,negative,negative
955637168,"Hi,

Don't know what your system is, but did you try `python3` instead of `python` in `python demo_toolbox.py`?",hi know system try python instead python python,issue,negative,neutral,neutral,neutral,neutral,neutral
955597478,"Yes, I did it in the order indicated on the webpage.


“Fall seven times; stand up eight.”
—Japanese proverb

> On Oct 30, 2021, at 2:32 PM, SimonLckm ***@***.***> wrote:
> 
> ﻿
> Did the command ""pip install -r requirements.txt"" mentioned on the instruction site not work properly? It should have installed the neccessary packages.
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or unsubscribe.
> Triage notifications on the go with GitHub Mobile for iOS or Android. 
",yes order fall seven time stand proverb wrote command pip install instruction site work properly thread reply directly view triage go mobile android,issue,negative,positive,neutral,neutral,positive,positive
955596233,"Did the command ""pip install -r requirements.txt"" mentioned on the instruction site not work properly? It should have installed the neccessary packages.",command pip install instruction site work properly,issue,negative,neutral,neutral,neutral,neutral,neutral
955117848,"@blue-fish said: #I suggest using both train-clean-100 and 360 to more closely match the training of the pretrained models.#

How can I use both? Do I run training for 100k steps on train-cleanl100 then train another 200k steps using train-clean-360 on top of that? Or can I simply combine them both together in my datasets_root and train the combination once to 300k steps?

#If you decide to pursue this, good luck and please consider sharing your models.#
Absolutely, assuming that the models come out sounding good. Happy to share plots, mid-training .wavs etc. upon request.
Regards, 
TC2",said suggest closely match training use run training train another top simply combine together train combination decide pursue good luck please consider absolutely assuming come sounding good happy share upon request,issue,positive,positive,positive,positive,positive,positive
955039303,"I suggest using both train-clean-100 and 360 to more closely match the training of the pretrained models. If you decide to pursue this, good luck and please consider sharing your models.",suggest closely match training decide pursue good luck please consider,issue,positive,positive,positive,positive,positive,positive
955006390,"@blue-fish , thanks for the reply. If I decide to go forward on this effort, I would plan to use train-clean-360. Easier to download, smaller size. After reading #449 ,  I agree that limiting max_mel_frames to 500 is a good idea. Thanks also for the accelerated training hparams info.
R/
TC2",thanks reply decide go forward effort would plan use easier smaller size reading agree limiting good idea thanks also accelerated training,issue,positive,positive,positive,positive,positive,positive
954975022,"It can still be used, but if this is your first model from scratch, find a different dataset.

Also don't forget to update symbols.py with characters not found in the English alphabet.
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/7432046efc23cabf176f9fdc8d2fd67020059478/synthesizer/utils/symbols.py#L11",still used first model scratch find different also forget update found alphabet,issue,negative,positive,positive,positive,positive,positive
954957199,You can also decrease `max_mel_frames` to a lower number (like 500) to discard longer utterances. This will also increase training speed.,also decrease lower number like discard longer also increase training speed,issue,negative,neutral,neutral,neutral,neutral,neutral
954954355,"You can reuse the existing encoder and vocoder models. When training the synthesizer, make sure not to change the audio settings in the synthesizer hparams.

Our observations on LibriTTS are in #449.

Since this is your first time training a model from scratch, I suggest decreasing the model dimensions, and use a larger reduction factor. This will help the model train faster, at the expense of quality. When you are confident things are working, revert to the defaults.

```
tts_embed_dims = 256,
tts_postnet_dims = 256,
tts_lstm_dims = 512,

tts_schedule = [(5,  1e-3,  20_000,  26),   # Progressive training schedule
                (5,  5e-4,  40_000,  26),   # (r, lr, step, batch_size)
                (5,  2e-4,  80_000,  26),   #
                (5,  1e-4, 160_000,  26),   # r = reduction factor (# of mel frames
                (5,  3e-5, 320_000,  26),   #     synthesized for each decoder iteration)
                (5,  1e-5, 640_000,  26)],  # lr = learning rate
```",reuse training synthesizer make sure change audio synthesizer since first time training model scratch suggest decreasing model use reduction factor help model train faster expense quality confident working revert progressive training schedule step reduction factor mel iteration learning rate,issue,positive,positive,positive,positive,positive,positive
954945828,There is no reason the models can't be substituted with more recent ones. Just need someone to work on it.,reason ca substituted recent need someone work,issue,negative,neutral,neutral,neutral,neutral,neutral
954944838,"1. For each audio file, you'll need to make a corresponding .txt file using the data in `transcripts.txt`.
2. Write the text files to the same location as the .flac files.
3. Update the directory structure so it looks like LibriSpeech.
    * https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-666099538
4. Run `synthesiser_preprocess_audio.py` with the `--no_alignments` option.

In my limited experience with MLS, the audio files were not cut well and often stopped in the middle of a word or contained extra sounds. This is probably an issue of the automatic segmentation method used by the dataset authors. The extra sounds caused problems when training the stop token prediction.",audio file need make corresponding file data write text location update directory structure like run option limited experience audio cut well often stopped middle word extra probably issue automatic segmentation method used extra training stop token prediction,issue,negative,negative,neutral,neutral,negative,negative
954812952,"> Aha, thanks. The last link now works. The other two do not yet 😢.

Sorry I forgot other links, first I created a new account but finally I prefered to coontribute to an existing one, easier for me but I forgot to change links :D 

As a recap you can find all codes on [this Text-To-Speech project](https://github.com/yui-mhcp/text_to_speech) (where you can also find pretrained weights for both TTS and Siamese models)

For a more detailed tutorials / explainations on `Siamese Networks`, you can check [this repo](https://github.com/yui-mhcp/siamese_networks)

For the data processing with examples you can check [this one](https://github.com/yui-mhcp/data_processing)

They are all repo from [yui-mhcp](https://github.com/yui-mhcp) but in different projects that are more « specialized » in a specific topic :)",aha thanks last link work two yet sorry forgot link first new account finally one easier forgot change link recap find project also find detailed check data check one different specialized specific topic,issue,positive,positive,neutral,neutral,positive,positive
954801686,"qt.qpa.xcb: could not connect to display 
qt.qpa.plugin: Could not load the Qt platform plugin ""xcb"" in """" even though it was found.
This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.

Available platform plugins are: eglfs, linuxfb, minimal, minimalegl, offscreen, vnc, wayland-egl, wayland, wayland-xcomposite-egl, wayland-xcomposite-glx, webgl, xcb.",could connect display could load platform even though found application start platform could application may fix problem available platform minimal,issue,negative,positive,positive,positive,positive,positive
954792817,"Aha, thanks. The last link now works. The other two do not yet :cry:.",aha thanks last link work two yet cry,issue,negative,positive,neutral,neutral,positive,positive
954774565,"@dumblob sorry I updated the link, the git’s owner changed the name :D ",sorry link git owner name,issue,negative,negative,negative,negative,negative,negative
954768052,@Ananas120 I'd be interested in your work but unfortunately the links you provided no longer seem to work. Do you have your work in some repos somewhere?,ananas interested work unfortunately link provided longer seem work work somewhere,issue,negative,negative,negative,negative,negative,negative
954703915,"@blue-fish , thank you for the reply. If I were to try to train all three models (voice encoder, synthesizer and vocoder) from scratch, using LibriTTS, would you recommend using train-clean-100 or train-clean-500? My understanding from reading the doctoral papers and Corentin's  remarks, is that for voice encoder you need lots of voices and quality is less important than quantity and for synthesizer and vocoder quality>quantity. If I were to do this, training the synthesizer alone would take a week, but I may give it a go.

 Any hints, tips or settings for hparams you could share for such a project would be greatly appreciated. If I decide to try this, I would shoot for 300k steps to get to a 1e05 learning rate. Also, I have not tried any voice encoder training yet using this repo. Any helpful information or hparams for that evolution you could share?

I need to do a bit of research first on LibriTTS to see what it can and cannot do wrt punctuation. If it will be no better than the current LibriSpeech trained model, it may not be worth the time or effort. Your thoughts would be appreciated.
Regards,
TC2",thank reply try train three voice synthesizer scratch would recommend understanding reading doctoral voice need lot quality le important quantity synthesizer quality quantity training synthesizer alone would take week may give go could share project would greatly decide try would shoot get learning rate also tried voice training yet helpful information evolution could share need bit research first see punctuation better current trained model may worth time effort would,issue,positive,positive,positive,positive,positive,positive
954657933,@AntonFirc did you manage to train it to an acceptable degree in the end? Many slavic languages seem to have this problem...,manage train acceptable degree end many seem problem,issue,negative,positive,positive,positive,positive,positive
954563998,"Thanks for reporting this, but it is most likely a problem with your wav file.

See https://stackoverflow.com/a/57653026 for more information about: `Caught exception: ParameterError('Audio buffer is not finite everywhere')`
",thanks likely problem file see information caught exception buffer finite everywhere,issue,negative,positive,neutral,neutral,positive,positive
954558719,"To date, no one has shared an alternative pretrained model that is compatible with the current (pytorch) synthesizer. If you're willing to switch back to tensorflow 1.x, there are a few in #400 including one model on LibriTTS. However, you can consider training from scratch on LibriTTS with the current repo, since you have experience with the single voice finetuning. ",date one alternative model compatible current synthesizer willing switch back one model however consider training scratch current since experience single voice,issue,negative,positive,neutral,neutral,positive,positive
951085064,"> > All links to KuangDD's projects now are no longer accessible. I'm currently working on latest fork of this repo to support mandarin and if anyone want to use as reference, please be free to folk and train: https://github.com/babysor/Realtime-Voice-Clone-Chinese
> 
> i can not even synthesize ''ni hao' by your repo

try again",link longer accessible currently working latest fork support mandarin anyone want use reference please free folk train even synthesize ni hao try,issue,positive,positive,positive,positive,positive,positive
950150932,"PS C:\Users\dacia\Desktop\Eudora\Real-Time-Voice-Cloning-400_pretrained_swe_301> python3 demo_toolbox.py
Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases.
PS C:\Users\dacia\Desktop\Eudora\Real-Time-Voice-Cloning-400_pretrained_swe_301> python demo_toolbox.py
C:\Users\dacia\Desktop\Eudora\Real-Time-Voice-Cloning-400_pretrained_swe_301\encoder\audio.py:13: UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended.
  warn(""Unable to import 'webrtcvad'. This package enables noise removal and is recommended."")
Traceback (most recent call last):
  File ""demo_toolbox.py"", line 2, in <module>
    from toolbox import Toolbox
  File ""C:\Users\dacia\Desktop\Eudora\Real-Time-Voice-Cloning-400_pretrained_swe_301\toolbox\__init__.py"", line 1, in <module>
    from toolbox.ui import UI
  File ""C:\Users\dacia\Desktop\Eudora\Real-Time-Voice-Cloning-400_pretrained_swe_301\toolbox\ui.py"", line 15, in <module>
    import umap
  File ""C:\Users\dacia\AppData\Local\Programs\Python\Python37\lib\site-packages\umap\__init__.py"", line 2, in <module>
    from .umap_ import UMAP
  File ""C:\Users\dacia\AppData\Local\Programs\Python\Python37\lib\site-packages\umap\umap_.py"", line 47, in <module>
    from pynndescent import NNDescent
  File ""C:\Users\dacia\AppData\Local\Programs\Python\Python37\lib\site-packages\pynndescent\__init__.py"", line 3, in <module>
    from .pynndescent_ import NNDescent, PyNNDescentTransformer
  File ""C:\Users\dacia\AppData\Local\Programs\Python\Python37\lib\site-packages\pynndescent\pynndescent_.py"", line 16, in <module>
    import pynndescent.sparse as sparse
  File ""C:\Users\dacia\AppData\Local\Programs\Python\Python37\lib\site-packages\pynndescent\sparse.py"", line 10, in <module>
    from pynndescent.utils import norm, tau_rand
  File ""C:\Users\dacia\AppData\Local\Programs\Python\Python37\lib\site-packages\pynndescent\utils.py"", line 8, in <module>
    from numba.core import types
ModuleNotFoundError: No module named 'numba.core'
PS C:\Users\dacia\Desktop\Eudora\Real-Time-Voice-Cloning-400_pretrained_swe_301>",python python found run without install store disable manage execution python unable import package noise removal warn unable import package noise removal recent call last file line module toolbox import toolbox file line module import file line module import file line module import file line module import file line module import file line module import sparse file line module import norm file line module import module,issue,negative,negative,negative,negative,negative,negative
950140190,"i got this error 

Command ""python setup.py egg_info"" failed with error code 1 in C:\Users\dacia\AppData\Local\Temp\pip-install-1lqbf8bh\PyQt5\

obs, i run win11 could it be why I get that error??",got error command python error code run win could get error,issue,negative,positive,positive,positive,positive,positive
947330019,"> Error: Model files not found. even though they are there D;

Most of the time, can be solved with https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/758#issuecomment-850954893 or https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-864340065",error model found even though time,issue,negative,neutral,neutral,neutral,neutral,neutral
947212161,"Error: Model files not found.   even though they are there D;

Also UserWarning: PySoundFile failed.  also No such file or directory: 'samples/1320_00000.mp3' when run the test config

EDIT: my bad, skimmed over the 3.6/.7 only",error model found even though also also file directory run test edit bad skimmed,issue,negative,negative,negative,negative,negative,negative
947120583,You are using an older version of synthesize.py. Please pull the latest code from the repo and try again.,older version please pull latest code try,issue,negative,positive,positive,positive,positive,positive
945321963,"> I've installed ffmpeg and added its path to my environment variables but I still see this error. Any suggestions?

https://video.stackexchange.com/questions/20495/how-do-i-set-up-and-use-ffmpeg-in-windows/20496#20496

It works for me. The answer:

->
Installation
Go to the ffmpeg download site and download the zip file that best fits your computer's specs. Choose the ""static"" linking and the ""nightly git"" version for the most current usability.
Create a folder on your computer to unpack the zip file. This folder will be your ""installation"" folder. I chose C:\Program Files\ffmpeg\. This is a good idea because you will treat this like a regular program. Unpack the zip file into this folder.",added path environment still see error work answer installation go site zip file best computer spec choose static linking nightly git version current usability create folder computer unpack zip file folder installation folder chose good idea treat like regular program unpack zip file folder,issue,positive,positive,positive,positive,positive,positive
945098521,"for me, I just upgrade anaconda 3.8 python and spyder version to 4.2.5 
uninstall all your pyqt5-tools and pyqt5 
```
pip uninstall pyqt5
pip uninstall pyqt5-sip
pip uninstall pyqt5-tools
pip uninstall 
```
and try  to install pyqt5 like normal

```
pip install pyqt5
pip install pyqt5-tools
```

for example this result of `conda list pyqt5`

```
# Name                    Version                   Build  Channel
pyqt5-qt5                 5.15.2                   pypi_0    pypi
pyqt5-sip                 12.9.0                   pypi_0    pypi
```


if the command isn't recognized just 
add the path of pyqt5 to **PATH** environment variable 
in case if you are using anaconda this path 
C:\Users\<name>\Anaconda3\Library\bin

in this path, you will find your designer.exe pyqt5-tools
",upgrade anaconda python version pip pip pip pip try install like normal pip install pip install example result list name version build channel command add path path environment variable case anaconda path name path find,issue,negative,positive,positive,positive,positive,positive
944950027,"> From [here](https://matheo.uliege.be/bitstream/2268.2/6801/5/s123578Jemine2019.pdf#page=12):
> 
> > A particularity of the SV2TTS framework is that all models can be trained
> > separately and on distinct datasets. For the encoder, one seeks to have a model
> > that is robust to noise and able to capture the many characteristics of the human
> > voice. Therefore, a large corpus of many different speakers would be preferable to
> > train the encoder, without any strong requirement on the noise level of the audios.
> > Additionally, the encoder is trained with the GE2E loss which requires no labels other
> > than the speaker identity. (...) For the datasets of the synthesizer and the vocoder,
> > transcripts are required and the quality of the generated audio can only be as good
> > as that of the data. Higher quality and annotated datasets are thus required, which
> > often means they are smaller in size.
> 
> You'll need two datasets: ![image](https://user-images.githubusercontent.com/12038136/60648823-0c6c5580-9e41-11e9-9b3a-737379481824.png)
> 
> The first one should be a large dataset of untranscribed audio that can be noisy. Think thousands of speakers and thousands of hours. You can get away with a smaller one if you finetune the pretrained speaker encoder. Put maybe `1e-5` as learning rate. I'd recommend 500 speakers at the very least for finetuning. A good source for datasets of other languages is [M-AILABS](https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/).
> 
> The second one needs audio transcripts and high quality audio. Here, finetuning won't be as effective as for the encoder, but you can get away with less data (300-500 hours). You will likely not have the alignments for that dataset, so you'll have to adapt the preprocessing procedure of the synthesizer to not split audio on silences. See the code and you'll understand what I mean.
> 
> Don't start training the encoder if you don't have a dataset for the synthesizer/vocoder, you won't be able to do anything then.

this can be done with some audiobooks?",particularity framework trained separately distinct one model robust noise able capture many human voice therefore large corpus many different would preferable train without strong requirement noise level additionally trained gee loss speaker identity synthesizer quality audio good data higher quality thus often smaller size need two image first one large untranscribed audio noisy think get away smaller one speaker put maybe learning rate recommend least good source second one need audio high quality audio wo effective get away le data likely adapt procedure synthesizer split audio see code understand mean start training wo able anything done,issue,positive,positive,positive,positive,positive,positive
944881933,"Suggested resolution for similar error message here: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/97#issuecomment-525221189

We also provide a complete guide for installing on Windows: https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542",resolution similar error message also provide complete guide,issue,negative,positive,neutral,neutral,positive,positive
944881333,"I'll take note.
I have win7 and I can't switch to Linux. Many programs that only under Windows 7",take note win ca switch many,issue,positive,positive,positive,positive,positive,positive
944829884,"@pin240 Pytorch now supports ROCm 4.2 on Linux.
```
pip3 install torch torchvision==0.10.1 -f https://download.pytorch.org/whl/rocm4.2/torch_stable.html
```",pin pip install torch,issue,negative,neutral,neutral,neutral,neutral,neutral
940780424,"> Sorry, can't share. I don't have the time to clean up the code for release, and it is a low priority for me. Use the tensorflow version if you want Taco2: [#867 (comment)](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/867#issuecomment-937992354)

i have implemented taco2 this way:
encoder_outputs(768) = concat(encoder_outputs(512), speaker_embedding(256))
decoder_inputs(512) = Linear(768, 512)
in brackets are the dimensions.
Was it the same for you? If not. how?",sorry ca share time clean code release low priority use version want comment way linear,issue,negative,negative,neutral,neutral,negative,negative
940465036,"Thanks for the demo, but we're not going to allow our README to be used for promotional purposes.",thanks going allow used promotional,issue,positive,positive,positive,positive,positive,positive
940423892,"Sorry, can't share. I don't have the time to clean up the code for release, and it is a low priority for me. Use the tensorflow version if you want Taco2: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/867#issuecomment-937992354 ",sorry ca share time clean code release low priority use version want,issue,negative,negative,neutral,neutral,negative,negative
940421591,Closing this inactive issue. I have added it to a list of models in #30. Thank you for the contribution.,inactive issue added list thank contribution,issue,negative,neutral,neutral,neutral,neutral,neutral
940420963,"I'm going to close this issue as we will no longer provide support for any datasets not already incorporated into the repo. The end user is expected to reformat any dataset to resemble LibriSpeech/LibriTTS in structure.

@pilnyjakub If you would like to share dataset conversion scripts, please open a new issue.",going close issue longer provide support already incorporated end user resemble structure would like share conversion please open new issue,issue,positive,positive,neutral,neutral,positive,positive
940418549,@ireneb612 Please open a new issue to ask for help.,please open new issue ask help,issue,positive,positive,neutral,neutral,positive,positive
940176839,"> > For what reason, when switching from tensorflow to pytorch, the architecture was changed to takotron1?
> 
> I provided the reasons here: [#447 (comment)](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/447#issuecomment-664710815)
> 
> The synthesizer code is a lot cleaner now, which makes it a lot easier to update the model architecture if desired. I have tried both Taco1 and Taco2, and as I said in #472: ""Tacotron 1 learns faster than Tacotron 2. Training time per step is about the same, but the model is usable in fewer steps. Voice quality is similar. Attention is more robust in Taco2.""

Can you share your implementation Taco2? (if possible, then with a pre-trained model)",reason switching architecture provided comment synthesizer code lot cleaner lot easier update model architecture desired tried said faster training time per step model usable voice quality similar attention robust share implementation possible model,issue,positive,neutral,neutral,neutral,neutral,neutral
940174202,"> For what reason, when switching from tensorflow to pytorch, the architecture was changed to takotron1?

I provided the reasons here: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/447#issuecomment-664710815

The synthesizer code is a lot cleaner now, which makes it a lot easier to update the model architecture if desired. I have tried both Taco1 and Taco2, and as I said in #472: ""Tacotron 1 learns faster than Tacotron 2. Training time per step is about the same, but the model is usable in fewer steps. Voice quality is similar. Attention is more robust in Taco2.""",reason switching architecture provided synthesizer code lot cleaner lot easier update model architecture desired tried said faster training time per step model usable voice quality similar attention robust,issue,positive,neutral,neutral,neutral,neutral,neutral
939918993,"When running the voceder_preprocess.py I'm running into this error:


Arguments:
    datasets_root:   D:\data\cloning\dataset\cv-corpus-7.0-2021-07-21\it\
    model_dir:       synthesizer\saved_models\italian_1\
    hparams:
    no_trim:         False
    cpu:             False

{'allow_clipping_in_normalization': True,
 'clip_mels_length': True,
 'fmax': 7600,
 'fmin': 55,
 'griffin_lim_iters': 60,
 'hop_size': 200,
 'max_abs_value': 4.0,
 'max_mel_frames': 900,
 'min_level_db': -100,
 'n_fft': 800,
 'num_mels': 80,
 'power': 1.5,
 'preemphasis': 0.97,
 'preemphasize': True,
 'ref_level_db': 20,
 'rescale': True,
 'rescaling_max': 0.9,
 'sample_rate': 16000,
 'signal_normalization': True,
 'silence_min_duration_split': 0.4,
 'speaker_embedding_size': 256,
 'symmetric_mels': True,
 'synthesis_batch_size': 16,
 'trim_silence': True,
 'tts_cleaner_names': ['transliteration_cleaners'],
 'tts_clip_grad_norm': 1.0,
 'tts_decoder_dims': 128,
 'tts_dropout': 0.5,
 'tts_embed_dims': 512,
 'tts_encoder_K': 5,
 'tts_encoder_dims': 256,
 'tts_eval_interval': 500,
 'tts_eval_num_samples': 1,
 'tts_lstm_dims': 1024,
 'tts_num_highways': 4,
 'tts_postnet_K': 5,
 'tts_postnet_dims': 512,
 'tts_schedule': [(2, 0.001, 20000, 12),
                  (2, 0.0005, 40000, 12),
                  (2, 0.0002, 80000, 12),
                  (2, 0.0001, 160000, 12),
                  (2, 3e-05, 320000, 12),
                  (2, 1e-05, 640000, 12)],
 'tts_stop_threshold': -3.4,
 'use_lws': False,
 'utterance_min_duration': 1.6,
 'win_size': 800}
Synthesizer using device: cpu
Trainable Parameters: 30.870M

Loading weights at synthesizer\saved_models\italian_1\italian_1.pt
Tacotron weights loaded from step 25000
Using inputs from:
        D:\data\cloning\dataset\cv-corpus-7.0-2021-07-21\it\SV2TTS\synthesizer\train.txt
        D:\data\cloning\dataset\cv-corpus-7.0-2021-07-21\it\SV2TTS\synthesizer\mels
        D:\data\cloning\dataset\cv-corpus-7.0-2021-07-21\it\SV2TTS\synthesizer\embeds
Found 161835 samples
Traceback (most recent call last):
  File ""D:\PycharmProjects\Realtime\vocoder_preprocess.py"", line 59, in <module>
    run_synthesis(args.in_dir, args.out_dir, args.model_dir, modified_hp)
  File ""D:\PycharmProjects\Realtime\synthesizer\synthesize.py"", line 75, in run_synthesis
    for i, (texts, mels, embeds, idx) in tqdm(enumerate(data_loader), total=len(data_loader)):
  File ""D:\PycharmProjects\Realtime\venv\lib\site-packages\torch\utils\data\dataloader.py"", line 359, in __iter__
    return self._get_iterator()
  File ""D:\PycharmProjects\Realtime\venv\lib\site-packages\torch\utils\data\dataloader.py"", line 305, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File ""D:\PycharmProjects\Realtime\venv\lib\site-packages\torch\utils\data\dataloader.py"", line 918, in __init__
    w.start()
  File ""C:\Users\UserPC.LAPTOP-F0JUUKDE\AppData\Local\Programs\Python\Python39\lib\multiprocessing\process.py"", line 121, in start
    self._popen = self._Popen(self)
  File ""C:\Users\UserPC.LAPTOP-F0JUUKDE\AppData\Local\Programs\Python\Python39\lib\multiprocessing\context.py"", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File ""C:\Users\UserPC.LAPTOP-F0JUUKDE\AppData\Local\Programs\Python\Python39\lib\multiprocessing\context.py"", line 327, in _Popen
    return Popen(process_obj)
  File ""C:\Users\UserPC.LAPTOP-F0JUUKDE\AppData\Local\Programs\Python\Python39\lib\multiprocessing\popen_spawn_win32.py"", line 93, in __init__
    reduction.dump(process_obj, to_child)
  File ""C:\Users\UserPC.LAPTOP-F0JUUKDE\AppData\Local\Programs\Python\Python39\lib\multiprocessing\reduction.py"", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
AttributeError: Can't pickle local object 'run_synthesis.<locals>.<lambda>'
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\UserPC.LAPTOP-F0JUUKDE\AppData\Local\Programs\Python\Python39\lib\multiprocessing\spawn.py"", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File ""C:\Users\UserPC.LAPTOP-F0JUUKDE\AppData\Local\Programs\Python\Python39\lib\multiprocessing\spawn.py"", line 126, in _main
    self = reduction.pickle.load(from_parent)
EOFError: Ran out of input



I HAVE TRIED TO FIND A WAY TO AVOID USING THE LAMBDA FUNCTION BUT I COULD NOT FIND IT, PLEASE HELP ME DEAL WITH THIS ISSUE",running running error false false true true true true true true true false synthesizer device trainable loading loaded step found recent call last file line module file line enumerate file line return file line return self file line file line start self file line return file line return file line file line dump file protocol ca pickle local object lambda recent call last file string line module file line file line self ran input tried find way avoid lambda function could find please help deal issue,issue,positive,positive,neutral,neutral,positive,positive
939897244,"> The old version used Tacotron 2 and a different pretrained model. We lost that during the PyTorch conversion in #472. The new model will perform better on some voices, worse on others. You can compare audio samples on [my website](https://blue-fish.github.io/experiments/): RTVC-2 for the old model, and RTVC-7 for the new.
> 
> #### If you want to try the old version again:
> Here is a link to the last Tensorflow version: https://github.com/CorentinJ/Real-Time-Voice-Cloning/tree/5425557efe30863267f805851f918124191e0be0
> 
> And the instructions to install it. (Click the ""edited"" dropdown and select the original Jan 28 version. Otherwise the new instructions will not work!) [#642 (comment)](https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542)

For what reason, when switching from tensorflow to pytorch, the architecture was changed to takotron1?",old version used different model lost conversion new model perform better worse compare audio old model new want try old version link last version install click select original version otherwise new work comment reason switching architecture,issue,negative,positive,positive,positive,positive,positive
939179018,Use this issue to coordinate the development of an Arabic model.,use issue development model,issue,negative,neutral,neutral,neutral,neutral,neutral
939173314,The original issue has been edited to provide visibility of community-developed voice cloning models in other languages. I'll also use it to keep track of requests.,original issue provide visibility voice also use keep track,issue,negative,positive,positive,positive,positive,positive
939112190,"Minimum 10 utterances per speaker, 3-10 seconds in duration each.

Please do not open any more issues to ask questions. You will need to seek support elsewhere. We cannot walk you through the entire process of training a model. The issues board is for bug reports, feature requests, and development activities that benefit the entire open-source community. Your development of a Russian language voice cloning model would fall under that last category, and you can open an issue to document your progress if you wish.",minimum per speaker duration please open ask need seek support elsewhere walk entire process training model board bug feature development benefit entire community development language voice model would fall last category open issue document progress wish,issue,positive,neutral,neutral,neutral,neutral,neutral
939074077,"Your demo captures the essence of the repo, but there is still much toolbox functionality and visualizations that are missing. Ultimately it is Corentin who will decides whether to promote this in README.md, but it will need to be more toolbox-like before I will recommend a merge.

#### Requested functionality
1. Add utterance embedding plots
2. Add mel spectrogram plots
3. Match the toolbox UI (synthesize, vocode, synthesize and vocode buttons)
4. Add option to record an input from the user's microphone
5. More preset sample voices to choose from",essence still much toolbox functionality missing ultimately whether promote need recommend merge functionality add utterance add mel spectrogram match toolbox synthesize synthesize button add option record input user microphone preset sample choose,issue,negative,neutral,neutral,neutral,neutral,neutral
938720870,@blue-fish great let me know if you need anything else to merge,great let know need anything else merge,issue,positive,positive,positive,positive,positive,positive
938137471,"> > I'm still stymied by the inability to create custom datasets from scratch. Are you still working on this ""custom dataset"" tool that you mention here?
> 
> For those recording their own utterances, this is a useful tool: https://github.com/MycroftAI/mimic-recording-studio

Another dataset recording tool: https://github.com/babua/TTSDatasetRecorder",still inability create custom scratch still working custom tool mention recording useful tool another recording tool,issue,negative,positive,positive,positive,positive,positive
937992354,"The old version used Tacotron 2 and a different pretrained model. We lost that during the PyTorch conversion in #472. The new model will perform better on some voices, worse on others. You can compare audio samples on [my website](https://blue-fish.github.io/experiments/): RTVC-2 for the old model, and RTVC-7 for the new.

#### If you want to try the old version again:

Here is a link to the last Tensorflow version: https://github.com/CorentinJ/Real-Time-Voice-Cloning/tree/5425557efe30863267f805851f918124191e0be0

And the instructions to install it. (Click the ""edited"" dropdown and select the original Jan 28 version. Otherwise the new instructions will not work!) https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542",old version used different model lost conversion new model perform better worse compare audio old model new want try old version link last version install click select original version otherwise new work,issue,negative,positive,positive,positive,positive,positive
937980822,"We don't count epochs. Instead, we count the number of training steps and batch size.
This information for the pretrained models can be found here: https://blue-fish.github.io/experiments/RTVC-7.html

If you are asking about how long the training loop lasts by default:
* Encoder is practically infinite. The user decides when to stop training by pressing Ctrl+C.
* Synthesizer stops once the number of steps has reached the number in the final stage. This can be changed.
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/7432046efc23cabf176f9fdc8d2fd67020059478/synthesizer/hparams.py#L51-L57
* Vocoder stops after 350 epochs. This can also be changed in code.
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/7432046efc23cabf176f9fdc8d2fd67020059478/vocoder/train.py#L78",count instead count number training batch size information found long training loop default practically infinite user stop training pressing synthesizer number number final stage also code,issue,negative,negative,neutral,neutral,negative,negative
937918672,Send past synthesized speech samples and current,send past speech current,issue,negative,negative,negative,negative,negative,negative
937796602,"> And while we're on this, why is it saying cpu: false, but it's not using my GPU? And how do I use an AMD GPU for this? I have the right version of torch installed, and it is using that version.

Tensorflow 15 only supports CUDA so unless you have one of tjose rare amd cuda compatible gpus, it wont work.",saying false use right version torch version unless one rare compatible wont work,issue,negative,positive,neutral,neutral,positive,positive
937547802,Thanks for sharing the web demo. It can be a good option for those who want to test the repo without installation.,thanks web good option want test without installation,issue,positive,positive,positive,positive,positive,positive
937462631,"@lastmjs Now that the repo no longer requires Tensorflow, user AK391 has published it as a web app in #865.
https://huggingface.co/spaces/akhaliq/Real-Time-Voice-Cloning",longer user ak web,issue,negative,neutral,neutral,neutral,neutral,neutral
937462206,"In #865, user AK391 has shared a web app that does not require installation.
https://huggingface.co/spaces/akhaliq/Real-Time-Voice-Cloning",user ak web require installation,issue,negative,neutral,neutral,neutral,neutral,neutral
937461074,"You would need to change the code. Sorry, I'm unable to provide detailed instructions.",would need change code sorry unable provide detailed,issue,negative,negative,negative,negative,negative,negative
937456541,"The file is part of the VoxCeleb1 dataset: http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html
If you've already downloaded the dataset, make sure it can be found at `<datasets_root>/VoxCeleb1/vox1_meta.csv`",file part already make sure found,issue,negative,positive,positive,positive,positive,positive
937440974,Would I just need to increase the values in hparams or change the code?,would need increase change code,issue,negative,neutral,neutral,neutral,neutral,neutral
936984492,"Try expanding the dataset first, without modifying the embeddings. If you don't like the results, append a voice style element to the embedding, which will expand it from 256 to 257 elements.",try expanding first without like append voice style element expand,issue,negative,positive,positive,positive,positive,positive
936979152,"It was attempted in #126. Although the encoder model performed very well for speaker verification, the results for voice cloning were not good in that one instance.",although model well speaker verification voice good one instance,issue,positive,positive,positive,positive,positive,positive
936935793,"> 
> 
> If the symbols in your languages are mutually exclusive, it should be possible. However, I wouldn't recommend it until you have a lot of experience with training models.

By model I mean encoder",mutually exclusive possible however would recommend lot experience training model mean,issue,negative,negative,negative,negative,negative,negative
936898060,"> I want to train a model for Russian, but I don't have enough data for the synthesizer. Can you share the dataset again?

 PC went to hibernation when unloading :( New link https://dropmefiles.com/uhoQX",want train model enough data synthesizer share went hibernation new link,issue,negative,positive,neutral,neutral,positive,positive
936830966,"> Hi is there anyway to install this on apple osx catalina?

Install a supported version of Python (3.6-3.8), then open a terminal and follow the instructions in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/615#issuecomment-740270185 starting at step 4.",hi anyway install apple catalina install version python open terminal follow starting step,issue,negative,neutral,neutral,neutral,neutral,neutral
936824568,"Change this setting and train a new model if you need >8KHz output.
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/7432046efc23cabf176f9fdc8d2fd67020059478/synthesizer/hparams.py#L77",change setting train new model need output,issue,negative,positive,positive,positive,positive,positive
936819309,Please leave a message in #30 with your request.,please leave message request,issue,negative,neutral,neutral,neutral,neutral,neutral
936813052,Are you saying that I need to add a record of similar voices to the regular recordings?,saying need add record similar regular,issue,negative,neutral,neutral,neutral,neutral,neutral
936811218,"If the symbols in your languages are mutually exclusive, it should be possible. However, I wouldn't recommend it until you have a lot of experience with training models.",mutually exclusive possible however would recommend lot experience training,issue,negative,neutral,neutral,neutral,neutral,neutral
936806652,"1. No, the number of steps is arbitrary though I believe the training had converged earlier than that.
2. You need to experiment with batch_size and learning rate to find the best values. The reduction factor `r` is adjustable (integer >= 1). Training and inference are faster when it is set to a higher number. Quality is better when `r` is lower.
3. Train from scratch.",number arbitrary though believe training need experiment learning rate find best reduction factor adjustable integer training inference faster set higher number quality better lower train scratch,issue,positive,positive,positive,positive,positive,positive
936792424,"It should, if your speaker encoder can differentiate between the normal and target voice styles. If not, you can try appending a one-hot voice style element to the usual speaker embedding when training the synthesizer.",speaker differentiate normal target voice try voice style element usual speaker training synthesizer,issue,negative,negative,neutral,neutral,negative,negative
936752355,"It is possible to use a different sample rate for the vocoder. The vocoder hparams must be chosen carefully to be compatible with the synthesizer. There is some discussion here: https://github.com/mozilla/TTS/issues/520#issuecomment-700490677 . If you use our defaults for the synthesizer hparams, the mel spectrograms will not contain any frequency content above fmax=7600 Hz. You may wish to change this.",possible use different sample rate must chosen carefully compatible synthesizer discussion use synthesizer mel contain frequency content may wish change,issue,negative,negative,neutral,neutral,negative,negative
936745258,Would it work if I added a few hours of similar voices to the normal ones?,would work added similar normal,issue,negative,positive,neutral,neutral,positive,positive
936733507,"I have trained usable synthesizer and vocoder models with very small batch sizes (=2). Due to limited VRAM, I haven't conducted experiments with large batch sizes, so I can't tell you if quality would improve.",trained usable synthesizer small batch size due limited large batch size ca tell quality would improve,issue,negative,negative,neutral,neutral,negative,negative
936609448,"> Yes, you can use a different vocoder, I know for a fact that the encoder can be taught to audio of a different sample rate, but I am not sure about the other models

I also understand that the speaker's encoder should not be affected by this. I'm interested in using a synthesizer and vocoder with different sampel rates.",yes use different know fact taught audio different sample rate sure also understand speaker affected interested synthesizer different,issue,positive,positive,positive,positive,positive,positive
936603929,"Yes, you can use a different vocoder, I know for a fact that the encoder can be taught to audio of a different sample rate, but I am not sure about the other models",yes use different know fact taught audio different sample rate sure,issue,positive,positive,positive,positive,positive,positive
936310270,Should I teach the model in more than two languages? Will it affect the quality?,teach model two affect quality,issue,negative,neutral,neutral,neutral,neutral,neutral
935648542,"> Hi is there anyway to install this on apple osx catalina?

you can use something called a chroot to run a small debian or ubuntu native in osx.",hi anyway install apple catalina use something run small native,issue,negative,negative,negative,negative,negative,negative
934660147,"Unfortunately, no... I will try to teach the encoder to LibriSpeech, LibriTTS, voxforge(1,2) and mozilla common voice(Russian, Belarusian, English)",unfortunately try teach common voice,issue,negative,negative,negative,negative,negative,negative
934655077,"Familiarize yourself with the LibriSpeech dataset format. Then reformat your dataset to resemble LibriSpeech. This requires you to separate your audio files by speaker.

Hopefully you are provided enough information to do this. If not, you will need a different dataset.",familiarize format resemble separate audio speaker hopefully provided enough information need different,issue,negative,neutral,neutral,neutral,neutral,neutral
934586840,"Okay, as soon as the model is trained I will post the results and the model.",soon model trained post model,issue,negative,neutral,neutral,neutral,neutral,neutral
934574413,I'm just afraid to forget about it) In a couple of hours to unload,afraid forget couple unload,issue,negative,negative,negative,negative,negative,negative
934572205,"As I said earlier, we don't understand what it takes to get paper quality. You'll have to answer your question by training a model.",said understand get paper quality answer question training model,issue,negative,neutral,neutral,neutral,neutral,neutral
934567895,"I'll post the model in the public domain, I recently found a HUGE dataset of Russian speech(https://github.com/snakers4/open_stt), but thanks anyway.",post model public domain recently found huge speech thanks anyway,issue,positive,positive,positive,positive,positive,positive
934558506,"> I want to train a model for Russian, but I don't have enough data for the synthesizer. Can you share the dataset again?

https://dropmefiles.com/DZY79

Give your mail. It's more reliable.
Will you send me a model in the mail? pin240@mail.ru",want train model enough data synthesizer share give mail reliable send model mail pin,issue,negative,neutral,neutral,neutral,neutral,neutral
934300800,"For my language there is a huge dataset of speech of different quality (from noisy recordings to ~ studio quality) in it 20,000 hours of speech and tens of thousands of speakers, can I get paper quality with it?",language huge speech different quality noisy studio quality speech get paper quality,issue,negative,positive,positive,positive,positive,positive
933848760,"Based on my exploration, I believe their results are real. We need a better dataset for the encoder. Differences in model structure might also exist since theirs is closed source. To match the paper, we should start by reproducing the results in [1703.10135](https://google.github.io/tacotron/publications/tacotron/index.html) and [1712.05884](https://google.github.io/tacotron/publications/tacotron2/index.html) (the Tacotron 1 and 2 papers). After validating our synthesizer and vocoder implementations, we can return to [1806.04558](https://google.github.io/tacotron/publications/speaker_adaptation/index.html) to match the encoder.",based exploration believe real need better model structure might also exist since closed source match paper start synthesizer return match,issue,positive,positive,positive,positive,positive,positive
933841812,Is it possible that the google results are fake or this repo does not fully match the paper or do we need a different dataset?,possible fake fully match paper need different,issue,negative,negative,negative,negative,negative,negative
933829839,"Can't answer this question because we still can't get results as good as the paper.

Here are my results, which should still represent the state of the art for open source SV2TTS.
https://blue-fish.github.io/experiments",ca answer question still ca get good paper still represent state art open source,issue,negative,positive,positive,positive,positive,positive
933820540,"The speaker embeddings used by this repo function much like the global style tokens in GST-tacotron. For cloning unseen voices, this repo should perform better.",speaker used function much like global style unseen perform better,issue,positive,positive,positive,positive,positive,positive
933816481,Not much work has been done in this area. You can try training a speaker encoder on multiple languages. Here is a pretrained example of a [multi-language encoder](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/126#issuecomment-535936275) (use these [hparams](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/126#issuecomment-573523609) with that model).,much work done area try training speaker multiple example use model,issue,negative,positive,neutral,neutral,positive,positive
933806980,"This is from the paper:

> The speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with
median duration of 3.9 seconds from 18K English speakers in the United States.

Assuming the average duration is same as the median, that implies 3,900 hours in their dataset. The authors also tried using a combination of LibriSpeech-other-500 and VoxCeleb 1/2 (used by this repo's encoder), but found that to have worse speaker similarity. That is 3,200 hours total, but only 8,371 speakers. Their conclusion is that increasing the number of speakers improves the naturalness and similarity.",paper speaker trained proprietary voice search corpus median duration united assuming average duration median also tried combination used found worse speaker similarity total conclusion increasing number naturalness similarity,issue,negative,negative,negative,negative,negative,negative
933806192,@JohnJnotFKennedy Could you please comment on how the issue got resolved. I am facing the same issue.,could please comment issue got resolved facing issue,issue,negative,neutral,neutral,neutral,neutral,neutral
933797098,"Yes, but check if your output has higher frequency than 8kHz...",yes check output higher frequency,issue,negative,positive,positive,positive,positive,positive
933789586,"Does your original voice speak Polish? If so, the quality will be terrible with the pre-trained model. And common voice is only good for training the encoder, and the synthesizer and vocoder need pure",original voice speak polish quality terrible model common voice good training synthesizer need pure,issue,negative,negative,neutral,neutral,negative,negative
933778718,"Does the original voice usually have a big impact on quality, or is this the case with any voice?",original voice usually big impact quality case voice,issue,negative,positive,positive,positive,positive,positive
933776742,As told before: I've recorder something now for test purpose in toolbox. How does it impact output spectrum?,told recorder something test purpose toolbox impact output spectrum,issue,negative,neutral,neutral,neutral,neutral,neutral
933775570,The original voice is the reference voice you want to clone,original voice reference voice want clone,issue,negative,positive,positive,positive,positive,positive
933773039,It's Synthesized output. What do you mean by original voice? I've recorder something now for test purpose. ,output mean original voice recorder something test purpose,issue,negative,positive,neutral,neutral,positive,positive
933766381,"As i can't upload flac file here its ziped: [toolbox_output.flac.zip](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/7280612/toolbox_output.flac.zip)

To reproduce simply start python3 demo_toolbox.py, select all pretrained models and generate (Synthesize and vocode). Save to file. Open Audacity, select all and check Analize -> Draw Spectrum. 
",ca file reproduce simply start python select generate synthesize save file open audacity select check draw spectrum,issue,negative,neutral,neutral,neutral,neutral,neutral
933754668,Please send a sample speech,please send sample speech,issue,negative,neutral,neutral,neutral,neutral,neutral
933733197,I use Mozilla Common Voice but testing on pretrained models also make such output. Files are saved as 16 kHz but real output is max 8kHZ..,use common voice testing also make output saved real output,issue,negative,negative,neutral,neutral,negative,negative
933725561,I want to train a model with a dataset for my language and clone the American voice.,want train model language clone voice,issue,negative,neutral,neutral,neutral,neutral,neutral
932054451,"@blue-fish 
Can I ask some question about integrating new vocoder? 
I know that in vocoder_preprocess, it use trained synthesizer model and generated GTA mel-specs, here it did feed it to waveRNN and clip it to [-1,-1]:
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/7432046efc23cabf176f9fdc8d2fd67020059478/vocoder/vocoder_dataset.py#L28

**And for feeding new vocoder, is the same to use this way?** 
I mean, use generated GTA mel-specs directly and load it in the same way as follows:
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/7432046efc23cabf176f9fdc8d2fd67020059478/vocoder/vocoder_dataset.py#L27-L34

Is there any other preprocess for gta **mel-specs.npy** and **wav.npy** before feeding to new vocoder ? 
The following code is also necessary before feeding?
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/7432046efc23cabf176f9fdc8d2fd67020059478/vocoder/vocoder_dataset.py#L36-L41
Thanks for your reply!",ask question new know use trained synthesizer model feed clip feeding new use way mean use directly load way feeding new following code also necessary feeding thanks reply,issue,negative,positive,neutral,neutral,positive,positive
932013450,"> 
> 
> Advice
> 
>     * I do not recommend adding English, but it is something you can try if you need a model that works for both languages.
> 
>     * Train a new synthesizer model. Don't forget to edit `synthesizer/utils/symbols.py` to include all the letters of the Russian alphabet. Here is a good start for Russian: [`symbols.py`](https://github.com/vlomme/Multi-Tacotron-Voice-Cloning/blob/master/synthesizer/utils/symbols.py)
> 
>     * Realistically, CPU is too slow. The model needs to learn attention before inference will work. This usually requires 10,000 to 20,000 steps. The training speed on CPU is anywhere from 1 to 4 steps per **minute**. So you will be waiting 1 to 2 weeks until you know whether your settings are correct. Even after attention is learned, you will be waiting another month or longer to train the 100,000 to 200,000 steps that it takes for the model to become usable.
> 
> 
> If you do not have access to a GPU, try to set up this repo, which has a Russian pretrained model. Note: It uses tensorflow and you will need to apply the synthesizer changes in #366 to make it work on CPU. https://github.com/vlomme/Multi-Tacotron-Voice-Cloning

This model is too awful.",advice recommend something try need model work train new synthesizer model forget edit include alphabet good start realistically slow model need learn attention inference work usually training speed anywhere per minute waiting know whether correct even attention learned waiting another month longer train model become usable access try set model note need apply synthesizer make work model awful,issue,negative,negative,negative,negative,negative,negative
932009512,"> 
> 
> Valid 14 days (from 13.09) [https://dropmefiles.com/aA2xF](url) If you need to leave again.

I want to train a model for Russian, but I don't have enough data for the synthesizer. Can you share the dataset again?",valid day need leave want train model enough data synthesizer share,issue,negative,neutral,neutral,neutral,neutral,neutral
931036567,"Hey, 
AFAIK there is no pretrained model in french language here, so you would need to do it on your own.
It is a lot of work, as you have to train a french model from scratch and you need a lot of aligned data with french speakers. Please have a look at #30 to get an idea of the effort needed. If you are still interested after reading it and if you have a good PC, you could get data from commonVoice (https://commonvoice.mozilla.org/fr/datasets) and train your model with help of Corentin's instructions (https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training).",hey model language would need lot work train model scratch need lot data please look get idea effort still interested reading good could get data train model help,issue,positive,positive,positive,positive,positive,positive
929303284,Hi is there anyway to install this on apple osx catalina?,hi anyway install apple catalina,issue,negative,neutral,neutral,neutral,neutral,neutral
928160827,"> @blue-fish So you want the data structure to be `LibriSpeech/train-clean-100/spekers...` and then contain data from other datasets?

Yes, that's what I have in mind.
",want data structure contain data yes mind,issue,negative,neutral,neutral,neutral,neutral,neutral
927385310,I have specified the directory for the data roots and still no luck,directory data still luck,issue,negative,neutral,neutral,neutral,neutral,neutral
927355043,I've been able to install every thing else so far but can't figure out what I'm supposed to do with (pip install -r requirements.txt),able install every thing else far ca figure supposed pip install,issue,negative,positive,positive,positive,positive,positive
927218753,"@CarvellScott You can install ffmpeg via sourceforge in Anaconda using this command:
`conda install -c conda-forge ffmpeg-python`
See here for details: https://anaconda.org/conda-forge/ffmpeg-python
Regards, Tomcattwo",install via anaconda command install see,issue,negative,neutral,neutral,neutral,neutral,neutral
927185964,"So it's seeing them, and I've re-pre-processed them with the aligners, but it's still giving me this:

![integer error](https://user-images.githubusercontent.com/84992987/134786512-fc404625-0ef1-40b0-a308-7a6b60c764b9.png)
```

Arguments:
    datasets_root:    /media/maker/D330-3157/Audio_Dev_Bulk/Output
    enc_models_dir:   encoder/saved_models
    syn_models_dir:   synthesizer/saved_models
    voc_models_dir:   vocoder/saved_models
    cpu:              False
    seed:             None
    no_mp3_support:   False

Traceback (most recent call last):
  File ""/home/maker/RTVC/Real-Time-Voice-Cloning/Real-Time-Voice-Cloning/toolbox/__init__.py"", line 82, in <lambda>
    self.ui.browser_load_button.clicked.connect(lambda: self.load_from_browser())
  File ""/home/maker/RTVC/Real-Time-Voice-Cloning/Real-Time-Voice-Cloning/toolbox/__init__.py"", line 154, in load_from_browser
    self.ui.browser_select_next()
  File ""/home/maker/RTVC/Real-Time-Voice-Cloning/Real-Time-Voice-Cloning/toolbox/ui.py"", line 322, in browser_select_next
    index = (self.utterance_box.currentIndex() + 1) % len(self.utterance_box)
ZeroDivisionError: integer division or modulo by zero

```
And while we're on this, why is it saying cpu: false, but it's not using my GPU? 
And how do I use an AMD GPU for this? I have the right version of torch installed, and it is using that version.",seeing still giving integer error false seed none false recent call last file line lambda lambda file line file line index integer division modulo zero saying false use right version torch version,issue,negative,negative,negative,negative,negative,negative
927082412,"I got the big one, LibriSpeech 500. And I ran the encoder preprocess, it worked, the files exist!
Now I have to figure out where to put it and how to tell the program to use them.

Is this correct?
```
#!/bin/bash
source /home/maker/RTVC/Real-Time-Voice-Cloning/rtvc/bin/activate
cd /home/maker/RTVC/Real-Time-Voice-Cloning/Real-Time-Voice-Cloning
python3 demo_toolbox.py -d /home/maker/RTVC/Real-Time-Voice-Cloning/Real-Time-Voice-Cloning/LibriSpeech/train-other-500
```
The files are in the places indicated, and the alignment files, but the program still doesn't see them.

Ah, I see it, I had to go back one step, cause it was looking for the folder that contained the folder that contained the datasets.
",got big one ran worked exist figure put tell program use correct source python alignment program still see ah see go back one step cause looking folder folder,issue,negative,neutral,neutral,neutral,neutral,neutral
926433564,"@pilnyjakub  I trasformed all the files to flac format, I also got some problems with this Error:

UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 6 audio5: character maps to <undefined>

This is due to the fact that the encoding for the italian language is ""latin-1"". After changing that the synthetizer_prepocessing_audio.py seems to work! ",format also got error ca decode position audio character undefined due fact language work,issue,negative,negative,negative,negative,negative,negative
926127430,"Now I'm trying to train it, can't even DL the big one https://www.openslr.org/resources/12/train-other-500.tar.gz
But I have the Celebvox files, and I've managed to get the encoder preprocess to see it, but it's running into a problem at line 57.
```
assert args.datasets_root.exists()
    args.out_dir.mkdir(exist_ok=True, parents=True)
```

```
python3 encoder_preprocess.py -d /media/maker/Storage 2/Audio_Dev_Bulk/VoxCeleb2/dev
Traceback (most recent call last):
  File ""encoder_preprocess.py"", line 57, in <module>
    assert args.datasets_root.exists()
AssertionError

```

Tried moving some files for a cleaner file path, getting further, but now I'm getting halted at line 70.

`preprocess_func[dataset](**args)`

```
python3 encoder_preprocess.py /home/maker/RTVC/Real-Time-Voice-Cloning/Real-Time-Voice-Cloning/ -d /media/maker/D330-3157/Audio_Dev_Bulk/VoxCeleb2/test/aac/
Arguments:
    datasets_root:   /home/maker/RTVC/Real-Time-Voice-Cloning/Real-Time-Voice-Cloning
    out_dir:         /home/maker/RTVC/Real-Time-Voice-Cloning/Real-Time-Voice-Cloning/SV2TTS/encoder
    datasets:        ['/media/maker/D330-3157/Audio_Dev_Bulk/VoxCeleb2/test/aac/']
    skip_existing:   False

Preprocessing /media/maker/D330-3157/Audio_Dev_Bulk/VoxCeleb2/test/aac/
Traceback (most recent call last):
  File ""encoder_preprocess.py"", line 70, in <module>
    preprocess_func[dataset](**args)
KeyError: '/media/maker/D330-3157/Audio_Dev_Bulk/VoxCeleb2/test/aac/'

```

It's the last line, I'm not sure what it's actually trying to do, or why it's failing.

Ok, I changed the order a little and now it's running, but it's not actually doing the thing.
```

python3 encoder_preprocess.py /media/maker/D330-3157/Audio_Dev_Bulk/ -d voxceleb2 -o /home/maker/RTVC/Real-Time-Voice-Cloning/Real-Time-Voice-Cloning/
Arguments:
    datasets_root:   /media/maker/D330-3157/Audio_Dev_Bulk
    out_dir:         /home/maker/RTVC/Real-Time-Voice-Cloning/Real-Time-Voice-Cloning
    datasets:        ['voxceleb2']
    skip_existing:   False

Preprocessing voxceleb2
VoxCeleb2: Preprocessing data for 0 speakers.
VoxCeleb2: 0speakers [00:00, ?speakers/s]
Done preprocessing VoxCeleb2.

```",trying train ca even big one get see running problem line assert python recent call last file line module assert tried moving cleaner file path getting getting line python false recent call last file line module last line sure actually trying failing order little running actually thing python false data done,issue,negative,negative,neutral,neutral,negative,negative
925994596,"@ireneb612 This is probably due to an older version of librosa, try upgrading it or try one of these https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/395, https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/214, https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/198",probably due older version try try one,issue,negative,positive,neutral,neutral,positive,positive
925900178,"@pilnyjakub I got this error:
 Error opening 'D:\\data\\cloning\\dataset\\cv-corpus-7.0-2021-07-21\\it\\audio\\train1\\000507c663409bb1796c90a583f52209f5e08870240424f8eb0ac861e194a97c07bfe6ece88a7129d4c51
bc8a8895d61516a2dad5a89bdc45c8a216c0207bb49\\0\\common_voice_it_20041619.mp3': File contains data in an unknown format.

do I have to convert everything to flac? It seems quite expensive
",got error error opening file data unknown format convert everything quite expensive,issue,negative,negative,negative,negative,negative,negative
925809282,"Two steps forward, one step back. 
I had it working, was playing around, I closed the terminal window I was starting it from so I could test the shell script, and now I'm back to having it run the same numpy error.
I have the shell script structured as instructed above, and it doesn't even start the virtualenv.
If I run the steps myself in terminal they work, until it gets to the numpy error.

#!/bin/bash
source /home/maker/RTVC/Real-Time-Voice-Cloning/rtvc/bin/activate
cd /home/maker/RTVC/Real-Time-Voice-Cloning/Real-Time-Voice-Cloning
python3.6 demo_toolbox.py

I figured it out. Had to reinstall numpy, the right version, and rather than specifically running python3.6, I just give in python3 and it runs it's best self.",two forward one step back working around closed terminal window starting could test shell script back run error shell script structured instructed even start run terminal work error source python figured reinstall right version rather specifically running give python best self,issue,negative,positive,positive,positive,positive,positive
925737170,"Oooohhh, kay. I'll see about making it use 3.6 and get back to you.

Got it working, thank you. Now I need to make myself a shell script to launch it easier, and figure out how to actually use the program.
One thing, right now it's using my CPU, can I get it to use my AMD GPU?",kay see making use get back got working thank need make shell script launch easier figure actually use program one thing right get use,issue,positive,positive,neutral,neutral,positive,positive
925489496,"So I've been trying to get this to run, and this tutorial got me pretty close, but now I'm getting this:

**It does this for any version of python I try.**

python3.9 demo_toolbox.py
Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/numpy/core/__init__.py"", line 17, in <module>
    from . import multiarray
  File ""/usr/lib/python3/dist-packages/numpy/core/multiarray.py"", line 14, in <module>
    from . import overrides
  File ""/usr/lib/python3/dist-packages/numpy/core/overrides.py"", line 7, in <module>
    from numpy.core._multiarray_umath import (
ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/maker/RTVC/Real-Time-Voice-Cloning/Real-Time-Voice-Cloning/demo_toolbox.py"", line 2, in <module>
    from toolbox import Toolbox
  File ""/home/maker/RTVC/Real-Time-Voice-Cloning/Real-Time-Voice-Cloning/toolbox/__init__.py"", line 1, in <module>
    from toolbox.ui import UI
  File ""/home/maker/RTVC/Real-Time-Voice-Cloning/Real-Time-Voice-Cloning/toolbox/ui.py"", line 1, in <module>
    import matplotlib.pyplot as plt
  File ""/usr/lib/python3/dist-packages/matplotlib/__init__.py"", line 138, in <module>
    from . import cbook, rcsetup
  File ""/usr/lib/python3/dist-packages/matplotlib/cbook/__init__.py"", line 31, in <module>
    import numpy as np
  File ""/usr/lib/python3/dist-packages/numpy/__init__.py"", line 142, in <module>
    from . import core
  File ""/usr/lib/python3/dist-packages/numpy/core/__init__.py"", line 47, in <module>
    raise ImportError(msg)
ImportError: 

IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!

Importing the numpy c-extensions failed.
- Try uninstalling and reinstalling numpy.
- If you have already done that, then:
  1. Check that you expected to use Python3.9 from ""/usr/bin/python3.9"",
     and that you have no directories in your PATH or PYTHONPATH that can
     interfere with the Python and numpy version ""1.17.4"" you're trying to use.
  2. If (1) looks fine, you can open a new issue at
     https://github.com/numpy/numpy/issues.  Please include details on:
     - how you installed Python
     - how you installed numpy
     - your operating system
     - whether or not you have multiple versions of Python installed
     - if you built from source, your compiler versions and ideally a build log

- If you're working with a numpy git repository, try `git clean -xdf`
  (removes all files not under version control) and rebuild numpy.

Note: this error has many possible causes, so please don't comment on
an existing issue about this - open a new one instead.

Original error was: No module named 'numpy.core._multiarray_umath'



Now, I _DO_ have the right numpy installed... So how do I get this program in particular to use that version?
",trying get run tutorial got pretty close getting version python try python recent call last file line module import file line module import file line module import module handling exception another exception recent call last file line module toolbox import toolbox file line module import file line module import file line module import file line module import file line module import core file line module raise important please read advice solve issue try already done check use python path interfere python version trying use fine open new issue please include python operating system whether multiple python built source compiler ideally build log working git repository try git clean version control rebuild note error many possible please comment issue open new one instead original error module right get program particular use version,issue,positive,positive,positive,positive,positive,positive
925236226,@blue-fish So you want the data structure to be `LibriSpeech/train-clean-100/spekers...` and then contain data from other datasets? Otherwise you'd have to edit those files (technically only `encoder/config.py` and `synthesizer/symbols.py`).,want data structure contain data otherwise edit technically,issue,negative,neutral,neutral,neutral,neutral,neutral
924901643,"I agree with blue-fish! I wanted to use the The M-AILABS Speech Dataset in italian. Is there an implementation for it? 
",agree use speech implementation,issue,negative,neutral,neutral,neutral,neutral,neutral
923246196,"Valid 14 days (from 13.09) [https://dropmefiles.com/aA2xF](url)
If you need to leave again.",valid day need leave,issue,negative,neutral,neutral,neutral,neutral,neutral
922869230,"Please have a look at #47 , this will probably fix your problem.",please look probably fix problem,issue,negative,neutral,neutral,neutral,neutral,neutral
922866205,"Just for the record: I also had this problem. 
It occured for vocoder training only, as there were NaN values in some files in mels_gta. 
Deleting corresponding files in folders audio, embeds, mels and mels_gta as well as in synthesized.txt solved that error for me.
Probably deleting file in synthesized.txt will be sufficient.",record also problem training nan corresponding audio well error probably file sufficient,issue,negative,neutral,neutral,neutral,neutral,neutral
922345975,"Select an utterance, and generate the embed using Corentin's model and yours. Compare the results. I recall the embeds tend to be sparse due to use of ReLU activation in the model. You can try removing it.",select utterance generate embed model compare recall tend sparse due use activation model try removing,issue,negative,negative,negative,negative,negative,negative
922345138,Thanks for identifying the unused imports @Koooooo-7 . Will merge when larger changes are made in the future.,thanks unused merge made future,issue,negative,positive,neutral,neutral,positive,positive
922344781,"Instead of updating the preprocessing scripts to support other datasets, we should write programs to reformat the datasets so they resemble LibriSpeech in folder structure.",instead support write resemble folder structure,issue,negative,neutral,neutral,neutral,neutral,neutral
922343008,"Go ahead and share the dataset if you wish, but someone else will have to train the models.",go ahead share wish someone else train,issue,positive,neutral,neutral,neutral,neutral,neutral
922342879,"It's not finding the model file in the expected location. Follow steps 11 and 12 from this link very carefully.
https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542",finding model file location follow link carefully,issue,negative,negative,neutral,neutral,negative,negative
921763000,"Hey, I tried the 2 pretrained model of different versions from  GitHub repository link one by one but it's showing the issue in synthesizer part code. ",hey tried model different repository link one one showing issue synthesizer part code,issue,negative,neutral,neutral,neutral,neutral,neutral
921340399,It looks like the pre-trained file for the synthesizer folder was loaded incorrectly. Make sure it's exactly the same as the tutorial.,like file synthesizer folder loaded incorrectly make sure exactly tutorial,issue,positive,positive,positive,positive,positive,positive
920115759,"With only 527 speakers the default embedding size of 256 is much too large. Changing it to 64 should produce a better model for voice cloning. The SV2TTS authors used an embedding size of 64 for their smaller datasets with 1200 speakers.

![image](https://user-images.githubusercontent.com/67130644/132529669-85359535-bcba-4450-aeb7-50588550fc93.png) 

The encoder training loop continues indefinitely until stopped by the user. You'll have to decide when you are satisfied with the quality of the model. See section 3.3.3 of [Corentin's thesis](https://matheo.uliege.be/handle/2268.2/6801) for more details on encoder training. 

Training time depends on several variables, including: number of speakers, data quality, GPU speed, dataset stored on SSD vs. HDD, success criteria. The pretrained encoder was trained in 20 days on a GTX 1080 Ti with a batch size of 64. However, with fewer speakers you might find it to converge much faster, perhaps 2-3 days.",default size much large produce better model voice used size smaller image training loop indefinitely stopped user decide satisfied quality model see section thesis training training time several number data quality speed success criterion trained day ti batch size however might find converge much faster perhaps day,issue,positive,positive,positive,positive,positive,positive
919501792,"Unfortunately, we're unable to provide assistance for these sorts of issues. I hope you are able to resolve the error message.",unfortunately unable provide assistance hope able resolve error message,issue,negative,neutral,neutral,neutral,neutral,neutral
919500007,I understood about what article. But there is no such possibility.  I wanted to use it for audio books. I can give you a link to the Date Set for Training? ,understood article possibility use audio give link date set training,issue,negative,neutral,neutral,neutral,neutral,neutral
919496717,"This is a Pytorch repo, questions about Tensorflow are out of scope unfortunately. You can try asking your question in the fatchord/WaveRNN repo.",scope unfortunately try question,issue,negative,negative,negative,negative,negative,negative
919492097,Can you use a cloud GPU? This person figured out how to use this repo with Amazon Sagemaker. https://github.com/rustygentile/Real-Time-Voice-Cloning,use cloud person figured use,issue,negative,neutral,neutral,neutral,neutral,neutral
919489196,Thanks for the quick reply. Buying a video card from us is problematic. And I'm not good at programming. And the main machine on Win10,thanks quick reply video card u problematic good main machine win,issue,positive,positive,positive,positive,positive,positive
919484954,"I have not tried the deepmind version, which is not compatible with the training code (see https://github.com/fatchord/WaveRNN/issues/152).

This topic is out of scope, you are encouraged to open the issue in the fatchord/WaveRNN repo.",tried version compatible training code see topic scope open issue,issue,negative,neutral,neutral,neutral,neutral,neutral
919483020,"What you describe is possible, but difficult as the repo needs to be rewritten in Tensorflow. Instead, it would be less work to run Pytorch for ROCm, or modify the repo to work with Google Colab. Still, my recommendation is to get a Nvidia GPU if at all possible if you have a serious interest in this.

There are no instructions for training on a language other than English, but this might be helpful: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/431#issuecomment-673555684",describe possible difficult need instead would le work run modify work still recommendation get possible serious interest training language might helpful,issue,negative,negative,negative,negative,negative,negative
919388378,"In 1806.04558 SV-EER is calculated for the encoder, not the synthesizer. Please refer to section 3.3.3 of Corentin's thesis for more details on how EER is determined in this repo. ",calculated synthesizer please refer section thesis eer determined,issue,positive,neutral,neutral,neutral,neutral,neutral
919361057,"Replace `<datasets_root>` with the actual path to the directory containing datasets. If you did not download a dataset, simply run `python demo_toolbox.py`",replace actual path directory simply run python,issue,negative,neutral,neutral,neutral,neutral,neutral
918232322,"here is my solution:
```
pip uninstall numba
pip install llvmlite --ignore-installed
pip install numba==0.52
```
",solution pip pip install pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
916339783,"ERROR: Could not find a version that satisfies the requirement 5.15.4 (from versions: none)
ERROR: No matching distribution found for 5.15.4",error could find version requirement none error matching distribution found,issue,negative,neutral,neutral,neutral,neutral,neutral
915670719,Additional comments removed. Please review.,additional removed please review,issue,negative,neutral,neutral,neutral,neutral,neutral
915485031,"Well, I got a new error(below).  I couldn't figure this one out either.  I appreciate any help you can provide.


(deepvoice) C:\Users\Family\Desktop\Real-Time-Voice-Cloning-master>python synthesizer_preprocess_audio.py synthesizer/saved_models/logs-singlespeaker/datasets_root --datasets_name LibriTTS --no_alignments
Traceback (most recent call last):
  File ""synthesizer_preprocess_audio.py"", line 1, in <module>
    from synthesizer.preprocess import preprocess_dataset
ImportError: cannot import name 'preprocess_dataset' from 'synthesizer.preprocess' (C:\Users\Family\Desktop\Real-Time-Voice-Cloning-master\synthesizer\preprocess.py)",well got new error could figure one either appreciate help provide python recent call last file line module import import name,issue,positive,positive,neutral,neutral,positive,positive
915351281,"Okay, thank you. My encoder training set will have around 10000 speakers max. Thus, I will definitely use 256 for the embedding size. According to the quality I totally agree, but as I already have 48k Audio, I thought audio quality might even be a bit better when I also use this rate in training (I will train a vocoder with 48k from scratch as well).",thank training set around thus definitely use size according quality totally agree already audio thought audio quality might even bit better also use rate training train scratch well,issue,positive,positive,positive,positive,positive,positive
915302052,"I didn't notice any improvement in synthesizer quality when training with sberryman's encoder. It seems 768 is much too big for the number of speakers in the dataset.

You can use the table in 1806.04558 to inform further experimentation in this area.

![image](https://user-images.githubusercontent.com/67130644/132529669-85359535-bcba-4450-aeb7-50588550fc93.png)

My experience suggests that recording quality is much more important than sample rate.",notice improvement synthesizer quality training much big number use table inform experimentation area image experience recording quality much important sample rate,issue,positive,positive,positive,positive,positive,positive
915287653,"It is unable to find the corresponding .txt file for one or more wavs. Try checking file permissions.

You can apply [these changes](https://github.com/blue-fish/Real-Time-Voice-Cloning/commit/008cf9602e4162e72a38d24ac81db37a1e03b6f3) to make the synthesizer preprocess single-threaded. This will make it easier to troubleshoot issues.",unable find corresponding file one try file apply make synthesizer make easier,issue,negative,negative,negative,negative,negative,negative
915266276,Please try following the instructions in my [Windows setup tutorial](https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542).,please try following setup tutorial,issue,negative,neutral,neutral,neutral,neutral,neutral
915262239,"Please follow my [Windows setup tutorial](https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542) to the letter, and open a new issue if you continue to have problems.",please follow setup tutorial letter open new issue continue,issue,negative,positive,neutral,neutral,positive,positive
914335052,"Still, don know what's problem behind the method to creat environment with Anaconda, but I guess maybe it's because some connetion between files can only make by the system?",still know problem behind method creat environment anaconda guess maybe make system,issue,negative,negative,negative,negative,negative,negative
914331167,"Did you use Anaconda for environment creating?  ( I faced the same question without using Anaconda, and the f-ing caffe2_detectron_ops.dll is right where it should be.) Then I tried to use Anaconda.navigator to creat the environment and conda install torch. Bingo, I finally got everything fine.",use anaconda environment faced question without anaconda right tried use creat environment install torch bingo finally got everything fine,issue,negative,positive,positive,positive,positive,positive
912668024,"i am having the exact same problem, has anyone solved it somehow?",exact problem anyone somehow,issue,negative,positive,positive,positive,positive,positive
912491549,"Firstlly this is  my e-mail ->aysenurcakmakci53@hotmail.com
First of all, I increased the dataset from 205 to 666. Because I can't find the attention layer.That's already the problem. Learning rate increased from 0.001 to 0.005 for the second stage.But still can't find.Do I need to clean the dataset or how long should I wait I dont know.
![image](https://user-images.githubusercontent.com/81026327/132002156-0fb39f69-515c-44f4-9bbd-aa9bdc3ab71b.png)

For 26500k step:
![image](https://user-images.githubusercontent.com/81026327/132002737-873819c8-d862-4fd2-9022-11730d76145c.png)

However, the loss value is low and good at predicting sound
![image](https://user-images.githubusercontent.com/81026327/132002814-79059c1c-ccb4-4a47-b78d-ff5ad630a09d.png)




> > Merhaba Ekin.Öncelikle tanıştığıma memnun oldum.Yardımcı olabilirseniz çok sevinirim. Bazı noktalarda hala sorun yaşıyorum.
> 
> > > Merhaba ben Ekin. Ben Türkiyeliyim. Türkçe Ses Klonlama yapmakla ilgileniyorum. Hala çalışıyorsanız yardımcı olabilir miyim?
> 
> İsterdim. Projenin mevcut durumunu nereden öğrenebilirim?
> Ayrıca her türlü iletişim bilgilerinizi alabilir miyim? e-posta gibi mi?

",first ca find attention already problem learning rate second still ca need clean long wait dont know image step image however loss value low good sound image hala ben ben hala mi,issue,negative,positive,positive,positive,positive,positive
912471859,"> Hi Ekin.Firstlly nice to meet you.I would really appreciate if you could help me. I'm still having trouble with some points.

> > Hi, I'm Ekin. I'm from Turkey. I'm interested in doing Voice-Cloning in Turkish. If you are still working on it, can I help ?

I would love to. Where can I get the current state of the project?
Also can I get any kind of a contact info of yours? like email ?

",hi nice meet would really appreciate could help still trouble hi turkey interested still working help would love get current state project also get kind contact like,issue,positive,positive,positive,positive,positive,positive
912107506,"Hi Ekin.Firstlly nice to meet you.I would really appreciate if you could help me. I'm still having trouble with some points.

> > Hello first of all.I conducted a synthesis training from the Turkish dataset I created.
> > As the Loss value decreases for 205 data, it goes very well. However, when I run demo_toolbox.py over the cpu, I cannot understand the sounds.Is this because I use Encoder and Vocoder with trained data. Should I train them myself to be able to do TTS. I would be very happy if you could give an idea about this. I still have some deficiencies in this regard.
> > ![step-4500-mel-spectrogram_sample_1](https://user-images.githubusercontent.com/81026327/121594764-2b6cbc80-ca46-11eb-900c-e7f62a755f51.png)
> > ![attention_step_4500_sample_1](https://user-images.githubusercontent.com/81026327/121594783-33c4f780-ca46-11eb-8358-e00fb1964601.png)
> > Also, there should be a data line in the second image but it doesn't show up in this image. I still don't understand where I went wrong.
> > Apart from all these, when I try to do vocoder training, I get an error message that it cannot find the file in it. While it should work normally, the vocoder should be created in sv2tts, but this file does not create an error. What should I do for this?
> > ![image](https://user-images.githubusercontent.com/81026327/121596364-01b49500-ca48-11eb-9e95-740aa7a4cc2d.png)
> 
> Hi, I'm Ekin. I'm from Turkey. I'm interested in doing Voice-Cloning in Turkish. If you are still working on it, can I help ?

",hi nice meet would really appreciate could help still trouble hello first synthesis training loss value data go well however run understand use trained data train able would happy could give idea still regard step also data line second image show image still understand went wrong apart try training get error message find file work normally file create error image hi turkey interested still working help,issue,positive,positive,positive,positive,positive,positive
912075569,"> Hello first of all.I conducted a synthesis training from the Turkish dataset I created.
> As the Loss value decreases for 205 data, it goes very well. However, when I run demo_toolbox.py over the cpu, I cannot understand the sounds.Is this because I use Encoder and Vocoder with trained data. Should I train them myself to be able to do TTS. I would be very happy if you could give an idea about this. I still have some deficiencies in this regard.
> 
> ![step-4500-mel-spectrogram_sample_1](https://user-images.githubusercontent.com/81026327/121594764-2b6cbc80-ca46-11eb-900c-e7f62a755f51.png)
> 
> ![attention_step_4500_sample_1](https://user-images.githubusercontent.com/81026327/121594783-33c4f780-ca46-11eb-8358-e00fb1964601.png)
> 
> Also, there should be a data line in the second image but it doesn't show up in this image. I still don't understand where I went wrong.
> 
> Apart from all these, when I try to do vocoder training, I get an error message that it cannot find the file in it. While it should work normally, the vocoder should be created in sv2tts, but this file does not create an error. What should I do for this?
> ![image](https://user-images.githubusercontent.com/81026327/121596364-01b49500-ca48-11eb-9e95-740aa7a4cc2d.png)


Hi, I'm Ekin. I'm from Turkey. I'm interested in doing Voice-Cloning in Turkish. If you are still working on it, can I help ?",hello first synthesis training loss value data go well however run understand use trained data train able would happy could give idea still regard step also data line second image show image still understand went wrong apart try training get error message find file work normally file create error image hi turkey interested still working help,issue,positive,positive,positive,positive,positive,positive
910468891,Pull request #838 submitted for all of the above fixes. This issue is ready to be closed.,pull request issue ready closed,issue,negative,positive,neutral,neutral,positive,positive
910464923,"Pulls requested by @blue-fish
1) In synthesizer/train.py @Tomcattwo edited per issue CorentinJ#669 and blue-fish/Real-Time-Voice-Cloning@89a9964 to fix Win10 pickle issue. This will allow Windows users using a GPU to properly run synthesizer training.
2) In vocoder/train.py @Tomcattwo edited per issue CorentinJ#669 and blue-fish/Real-Time-Voice-Cloning@89a9964 to fix Win10 pickle issue. This will allow Windows users using a GPU to properly run synthesizer training.
3) In synthesizer/synthesize.py, following edits made by @tomcattwo (see issue CorentinJ#833):
    a) added Win10 pickle fix per issue CorentinJ#669 and blue-fish/Real-Time-Voice-Cloning@89a9964 (lines 12 and 70)
    b) edited line 19 to delete ""hparams"" as argument; hparams_debug_string() needs 0 arguments. Allows vocoder_preprocess.py to run properly on WIn10/GPU (CUDA) systems)
    c) edited line 87 to fix improper definition for mels_out per @blue-fish recommendation in issue CorentinJ#729 and issue CorentinJ#833; Allows vocoder_preprocess.py to run properly on WIn10/GPU (CUDA) systems)
4) In vocoder_preorocess.py @Tomcattwo changed line 47 per recommendation by @blue-fish in issue CorentinJ#833; allows --cpu argument to be properly recognized",per issue fix win pickle issue allow properly run synthesizer per issue fix win pickle issue allow properly run synthesizer following made see issue added win pickle fix per issue line delete argument need run properly line fix improper definition per recommendation issue issue run properly line per recommendation issue argument properly,issue,positive,positive,positive,positive,positive,positive
909824206,"I ran a single-voice synthesizer training on top of the taco_pretrained synthesizer from the @blue-fish zip file from #437 for 4000 steps on my V13M data. When I used it in the toolbox, I got nothing but gibberish. I verified that it did NOT build from scratch - the training added ~4050 steps starting with 20200 steps for a total of 24250 steps to the taco_pretrained LibriTTS synthesizer from the zip file.

I also finally got vocoder preprocessing and vocoder training done on the pretrained.pt vocoder from the original repo files (issue #833 ).  I then used this vocoder along with my single voice pretrained (on the original LibriSpeech synthesizer from the repo) V13M synthesizer in the toolbox. After training ~20 random V13M .wavs for embeddings, I tried some phrases and was very pleasantly surprised with the result. V13M lost the ""sore throat"" he had using Griffin-Lim, and the voice sounded good enough for use in my project. The output sounded better WITHOUT using ""Enhance vocoder output"" than with it on. The single-voice trained vocoder used in conjunction with the single-voice trained synthesizer produced a pretty decent output when compared to the ground truth wavs. It was much better than using just the single-voice pretrained synthesizer with Griffin-Lim or the original pre-trained vocoder. That said, I still have to do quite a bit of ""phoneme manipulation"" to get pronunciations closer to real speech. But this will work for my project.
Thanks @blue-fish for your help on this work. This issue is ready to close.
Regards,
Tomcattwo",ran synthesizer training top synthesizer zip file data used toolbox got nothing gibberish build scratch training added starting total synthesizer zip file also finally got training done original issue used along single voice original synthesizer synthesizer toolbox training random tried pleasantly result lost sore throat voice good enough use project output better without enhance output trained used conjunction trained synthesizer produced pretty decent output ground truth much better synthesizer original said still quite bit phoneme manipulation get closer real speech work project thanks help work issue ready close,issue,positive,positive,positive,positive,positive,positive
909525066,"I was able to train the vocoder on top of the pretrained WaveRNN vocoder. Took about 25 min,
starting at step 1159000 on the pretrained WaveRNV file Loss started at 2.8245 running about 1.2 steps/sec using the CUDA, batch size 100 LR 0.0001 Sequence Len 1000, 4 steps per Epoch. It rapidly converged to loss of ~2.53-2.54 Not seeing much improvement. It stopped on its own at Epoch349 loss = 2.5131.",able train top took min starting step file loss running batch size sequence per epoch rapidly loss seeing much improvement stopped epoch loss,issue,negative,positive,positive,positive,positive,positive
909148781,"@blue-fish Thanks I put in the fix you suggested and the vocoder_preprocess.py worked properly in the cpu. I will put in the pull requests.

Next I will try vocoder_train.py

@netman789 Thanks. Reducing sample size (to 1/3 of the total samples) was my ""Plan B"", then run the preprocessor 3 times (once for each batch of samples) and combine the output results manually.
R/,
TC2",thanks put fix worked properly put pull next try netman thanks reducing sample size total plan run time batch combine output manually,issue,positive,positive,neutral,neutral,positive,positive
908903007,"For a fixed model size, the Only way I know of to get around OOM is to cut the sample size. ",fixed model size way know get around cut sample size,issue,negative,positive,neutral,neutral,positive,positive
908890969,"> Then I tried again with the --cpu argument.
> Code said cpu = true, but the code after hparams stated: ""Synthesizer using device: cuda"", and it failed again on a CUDA out of memory error at 38%.

It seems the command line option is not successfully forcing CPU use. Try changing [this line](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/95adc699c1deb637f485e85a5107d40da0ad94fc/vocoder_preprocess.py#L46) to:
```
os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""
```",tried argument code said true code stated synthesizer device memory error command line option successfully forcing use try line,issue,negative,positive,positive,positive,positive,positive
908870883,"@netman789 , 
My first thought for your issue was hparams. But your hparams look to be the same as mine 
Then I noticed that your very first line after the command line was:
""initializing synthesizer/synthesize""

My run (see below) does not say that...mine goes straight to arguments.

Don't know why it would initialize synthesizer/synthesize
Check your folder structure maybe? As you postulate, seems like you are doing vocoder with synthesizer with maybe an incompatible model hence the mat error??

I just ran vocoder_preprocess.py after inserting the #729 solution in synthesize.py. It ran...up to 38% complete, then I got a CUDA out of memory halt. Here is the code:

````
(VoiceClone) C:\Utilities\SV2TTS>python vocoder_preprocess.py datasets_root --model_dir synthesizer/saved_models/V13M_LS_pretrained
Arguments:
    datasets_root:   datasets_root
    model_dir:       synthesizer/saved_models/V13M_LS_pretrained
    hparams:
    no_trim:         False
    cpu:             False

{'allow_clipping_in_normalization': True,
 'clip_mels_length': True,
 'fmax': 7600,
 'fmin': 55,
 'griffin_lim_iters': 60,
 'hop_size': 200,
 'max_abs_value': 4.0,
 'max_mel_frames': 900,
 'min_level_db': -100,
 'n_fft': 800,
 'num_mels': 80,
 'power': 1.5,
 'preemphasis': 0.97,
 'preemphasize': True,
 'ref_level_db': 20,
 'rescale': True,
 'rescaling_max': 0.9,
 'sample_rate': 16000,
 'signal_normalization': True,
 'silence_min_duration_split': 0.4,
 'speaker_embedding_size': 256,
 'symmetric_mels': True,
 'synthesis_batch_size': 16,
 'trim_silence': True,
 'tts_cleaner_names': ['english_cleaners'],
 'tts_clip_grad_norm': 1.0,
 'tts_decoder_dims': 128,
 'tts_dropout': 0.5,
 'tts_embed_dims': 512,
 'tts_encoder_K': 5,
 'tts_encoder_dims': 256,
 'tts_eval_interval': 500,
 'tts_eval_num_samples': 1,
 'tts_lstm_dims': 1024,
 'tts_num_highways': 4,
 'tts_postnet_K': 5,
 'tts_postnet_dims': 512,
 'tts_schedule': [(2, 0.001, 20000, 12),
                  (2, 0.0005, 40000, 12),
                  (2, 0.0002, 80000, 12),
                  (2, 0.0001, 160000, 12),
                  (2, 3e-05, 320000, 12),
                  (2, 1e-05, 640000, 12)],
 'tts_stop_threshold': -3.4,
 'use_lws': False,
 'utterance_min_duration': 1.6,
 'win_size': 800}
Synthesizer using device: cuda
Trainable Parameters: 30.870M

Loading weights at synthesizer\saved_models\V13M_LS_pretrained\V13M_LS_pretrained.pt
Tacotron weights loaded from step 297000
Using inputs from:
        datasets_root\SV2TTS\synthesizer\train.txt
        datasets_root\SV2TTS\synthesizer\mels
        datasets_root\SV2TTS\synthesizer\embeds
Found 325 samples
  0%|                                                                                                                                                                | 0/21 [00:00<?, ?it/s]C:\Users\Colt_\.conda\envs\VoiceClone\lib\site-packages\torch\nn\functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\c10/core/TensorImpl.h:1156.)
  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)
 38%|█████████████████████████████████████████████████████████▉                                                                                              | 8/21 [00:06<00:10,  1.20it/s]
Traceback (most recent call last):
  File ""vocoder_preprocess.py"", line 58, in <module>
    run_synthesis(args.in_dir, args.out_dir, args.model_dir, modified_hp)
  File ""C:\Utilities\SV2TTS\synthesizer\synthesize.py"", line 87, in run_synthesis
    _, mels_out, _, _ = model(texts, mels, embeds)
  File ""C:\Users\Colt_\.conda\envs\VoiceClone\lib\site-packages\torch\nn\modules\module.py"", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File ""C:\Utilities\SV2TTS\synthesizer\models\tacotron.py"", line 406, in forward
    postnet_out = self.postnet(mel_outputs)
  File ""C:\Users\Colt_\.conda\envs\VoiceClone\lib\site-packages\torch\nn\modules\module.py"", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File ""C:\Utilities\SV2TTS\synthesizer\models\tacotron.py"", line 161, in forward
    x, _ = self.rnn(x)
  File ""C:\Users\Colt_\.conda\envs\VoiceClone\lib\site-packages\torch\nn\modules\module.py"", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File ""C:\Users\Colt_\.conda\envs\VoiceClone\lib\site-packages\torch\nn\modules\rnn.py"", line 838, in forward
    self.dropout, self.training, self.bidirectional, self.batch_first)
RuntimeError: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 0; 8.00 GiB total capacity; 5.94 GiB already allocated; 0 bytes free; 6.11 GiB reserved in total by PyTorch)
````
Then I tried again with the --cpu argument.
Code said cpu = true, but the code after hparams stated: ""Synthesizer using device: cuda"", and it failed again on a CUDA out of memory error at 38%.

But it did run...
Time to hit the sack.
R/
TC2

",netman first thought issue look mine first line command line run see say mine go straight know would initialize check folder structure maybe postulate like synthesizer maybe incompatible model hence mat error ran solution ran complete got memory halt code python false false true true true true true true true false synthesizer device trainable loading loaded step found associated experimental feature subject change please use anything important stable triggered internally return input stride padding dilation recent call last file line module file line model file line return input file line forward file line return input file line forward file line return input file line forward memory tried allocate mib gib total capacity gib already free gib reserved total tried argument code said true code stated synthesizer device memory error run time hit sack,issue,positive,positive,positive,positive,positive,positive
908854720,"Thanks @blue-fish . 
Re suggestions:
**More training data:** I used every utterance of that voice that the sim has (~1600  .wav files). Most are very short and I had to combine several together to get utterances for training between 2 and 11 seconds (I generally joined 5 short .wavs into a single .wav to get the training utterances) . Not much more I can do there. In toolbox, I can try running a lot more (maybe 100) of the voice for embeds before going after the output.
**Improve quality of training data for finetuning (remove low-quality and overly complex utterances from training set)** I can see what can be done there. In the sim,  this is an Air traffic Controller speaking over a radio so the quality is not great to begin with. 16bit mono PCM .wav files at 8k (telephony) quality. Perhaps I can clean them up using Goldwave or Audacity
**Start with a better baseline model:** Which baseline pretrained model would you recommend over LibriSpeech 295k? Perhaps LibriTTS taco_pretrained from #437? Any others available?

I know you mentioned in #437 that you did not have much additional improvement pretraining the vocoder. I am going give that a try to see if I can get any better results - I haven't much to lose but some time. 

In toolbox, I am using Enhance Vocoder and Griffin-Lim, which sounds much better than the pretrained vocoder. Any suggestions for an alternate vocoder?
Thanks,
TC2
",thanks training data used every utterance voice short combine several together get training generally short single get training much toolbox try running lot maybe voice going output improve quality training data remove overly complex training set see done air traffic controller speaking radio quality great begin bit mono telephony quality perhaps clean audacity start better model model would recommend perhaps available know much additional improvement pretraining going give try see get better much lose time toolbox enhance much better alternate thanks,issue,positive,positive,positive,positive,positive,positive
908853668,"TC2, if the vocoder_preprocess runs successfully now, I would be interested to know. I have reached an impasse with a different problem. I am running a slightly different dataset and am getting this error:

initializing synthesizer/synthesize
Arguments:
    datasets_root:   C:\Users\tsquare\source\repos\RealTimeVoiceClone-blufsh447\toolbox\datasets
    model_dir:       synthesizer/saved_models/pretrained/
    hparams:
    no_trim:         False
    cpu:             False

{'allow_clipping_in_normalization': True,
 'allow_pickle': True,
 'clip_mels_length': True,
 'fmax': 7600,
 'fmin': 55,
 'griffin_lim_iters': 60,
 'hop_size': 200,
 'max_abs_value': 4.0,
 'max_mel_frames': 900,
 'min_level_db': -100,
 'n_fft': 800,
 'num_mels': 80,
 'power': 1.5,
 'preemphasis': 0.97,
 'preemphasize': True,
 'ref_level_db': 20,
 'rescale': True,
 'rescaling_max': 0.9,
 'sample_rate': 16000,
 'signal_normalization': True,
 'silence_min_duration_split': 0.4,
 'speaker_embedding_size': 256,
 'symmetric_mels': True,
 'synthesis_batch_size': 16,
 'trim_silence': True,
 'tts_cleaner_names': ['english_cleaners'],
 'tts_clip_grad_norm': 1.0,
 'tts_decoder_dims': 128,
 'tts_dropout': 0.5,
 'tts_embed_dims': 512,
 'tts_encoder_K': 5,
 'tts_encoder_dims': 256,
 'tts_eval_interval': 500,
 'tts_eval_num_samples': 1,
 'tts_lstm_dims': 1024,
 'tts_num_highways': 4,
 'tts_postnet_K': 5,
 'tts_postnet_dims': 512,
 'tts_schedule': [(1, 0.001, 20000, 12),
                  (2, 0.0005, 40000, 12),
                  (2, 0.0002, 80000, 12),
                  (2, 0.0001, 160000, 12),
                  (2, 3e-05, 320000, 12),
                  (2, 1e-05, 640000, 12)],
 'tts_stop_threshold': -3.4,
 'use_lws': False,
 'utterance_min_duration': 1.6,
 'win_size': 800}
Synthesizer using device: cuda
Trainable Parameters: 30.870M

Loading weights at synthesizer\saved_models\pretrained\pretrained.pt
Tacotron weights loaded from step 295000
Using inputs from:
        C:\Users\tsquare\source\repos\RealTimeVoiceClone-blufsh447\toolbox\datasets\SV2TTS\synthesizer\train.txt
        C:\Users\tsquare\source\repos\RealTimeVoiceClone-blufsh447\toolbox\datasets\SV2TTS\synthesizer\mels
        C:\Users\tsquare\source\repos\RealTimeVoiceClone-blufsh447\toolbox\datasets\SV2TTS\synthesizer\embeds
Found 25164 samples
Length of dataloader is:  1573
  0%|                                                 | 0/1573 [00:57<?, ?it/s]
Traceback (most recent call last):
  File ""C:\Users\tsquare\source\repos\TomTRTVC\vocoder_preprocess.py"", line 65, in <module>
    run_synthesis(args.in_dir, args.out_dir, args.model_dir, modified_hp)
  File ""C:\Users\tsquare\source\repos\TomTRTVC\synthesizer\synthesize.py"", line 89, in run_synthesis
    _, mels_out, _ , _= model(texts, mels, embeds) #added addl. _ per blufish
  File ""C:\Users\tsquare\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\module.py"", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File ""C:\Users\tsquare\source\repos\TomTRTVC\synthesizer\models\tacotron.py"", line 390, in forward
    encoder_seq_proj = self.encoder_proj(encoder_seq)
  File ""C:\Users\tsquare\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\module.py"", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File ""C:\Users\tsquare\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\linear.py"", line 87, in forward
    return F.linear(input, self.weight, self.bias)
  File ""C:\Users\tsquare\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\functional.py"", line 1612, in linear
    output = input.matmul(weight.t())
RuntimeError: size mismatch, m1: [2640 x 1024], m2: [512 x 128] at C:/w/b/windows/pytorch/aten/src\THC/generic/THCTensorMathBlas.cu:283
Press any key to continue . . .
In the past, this matmul error meant that I was trying to run incompatible models or a synthesizer with an encoder or vocoder with synthesizer. But in this case, I am using the pretrained encoder and synthesizer. My suspicion is that the 2nd factor of m1, should be 512 which represents a concatenation of speaker_embedding_size with the encoder output. Instead, somehow the speaker embedding becomes 768 and the concatenation results in 1024.
Any ideas? ",successfully would interested know impasse different problem running slightly different getting error false false true true true true true true true true false synthesizer device trainable loading loaded step found length recent call last file line module file line model added per file line result input file line forward file line result input file line forward return input file line linear output size mismatch press key continue past error meant trying run incompatible synthesizer synthesizer case synthesizer suspicion factor concatenation output instead somehow speaker becomes concatenation,issue,positive,positive,positive,positive,positive,positive
908846282,"Thanks @netman789 and @blue-fish . I will try the #729 solution and test. If everything runs properly, I will then submit pull requests to change train.py (in synthesizer, and vocoder) to fix pickle errors in win10, a pull request to fix synthesize.py for print(hparams_debug_string()) and collate_synthesizer issues and add the #729 fix as a pull request also.
Another potential issue:
...\embed\train.py also has num_workers = 8 in line 24. Should this also receive the Win10 pickle workaround fix? If so, I will add a pull request for that fix also.
Appreciate the help.
R/,
TC2",thanks netman try solution test everything properly submit pull change synthesizer fix pickle win pull request fix print add fix pull request also another potential issue also line also receive win pickle fix add pull request fix also appreciate help,issue,positive,positive,positive,positive,positive,positive
908784480,"Regarding the latest problem, please see: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/729#issuecomment-816901953

If you don't mind, please submit a pull request containing the modifications needed to make the vocoder preprocess code work.",regarding latest problem please see mind please submit pull request make code work,issue,negative,positive,positive,positive,positive,positive
908782649,"> The README instructed use of --summary_interval 125 --checkpoint_interval 100 as arguments for synthesizer_train.py.
> pytorch would not accept --summary_interval 125 --checkpoint_interval 100 arguments.

As you found, the instructions in #437 are for use with the old tensorflow synthesizer that was in use at the time. The new code does not take an explicit command line argument to evaluate every X steps. Instead that is set with `hparams.tts_eval_interval`, which can be overridden at the command line.

The equivalent new command line arguments are:
```
--hparams ""tts_eval_interval=125"" --save_every 100
```

> Any recommendations on improving the voice quality (especially of V13M) would be appreciated.

* More training data
* Improve quality of training data for finetuning (remove low-quality and overly complex utterances from training set)
* Start with a better baseline model",instructed use would accept found use old synthesizer use time new code take explicit command line argument evaluate every instead set command line equivalent new command line improving voice quality especially would training data improve quality training data remove overly complex training set start better model,issue,positive,positive,neutral,neutral,positive,positive
908767043,"Per earlier comment by blufish, line 87 should read: _, mels_out, _, _ = model(texts, mels,embeds)",per comment line read model,issue,negative,neutral,neutral,neutral,neutral,neutral
908687802,"OK, I did a bit more tracing. Based on the above error, in synthesizer\synthesize.py, line 69, I changed the line from:

`69   collate_fn=lambda batch: collate_synthesizer(batch, r)`

to:

`69   collate_fn=lambda batch: collate_synthesizer(batch, r, hparams)`

and ran vocoder.preprocess.py using the command line:

`python vocoder_preprocess.py datasets_root --model_dir synthesizer/saved_models/V13M_LS_pretrained`

**This cleared the collate_synthesizer error, but still failed to run the preprocess**. Here is the output I received:

```
(VoiceClone) C:\Utilities\SV2TTS>python vocoder_preprocess.py datasets_root --model_dir synthesizer/saved_models/V13M_LS_pretrained
Arguments:
    datasets_root:   datasets_root
    model_dir:       synthesizer/saved_models/V13M_LS_pretrained
    hparams:
    no_trim:         False
    cpu:             False

{'allow_clipping_in_normalization': True,
 'clip_mels_length': True,
 'fmax': 7600,
 'fmin': 55,
 'griffin_lim_iters': 60,
 'hop_size': 200,
 'max_abs_value': 4.0,
 'max_mel_frames': 900,
 'min_level_db': -100,
 'n_fft': 800,
 'num_mels': 80,
 'power': 1.5,
 'preemphasis': 0.97,
 'preemphasize': True,
 'ref_level_db': 20,
 'rescale': True,
 'rescaling_max': 0.9,
 'sample_rate': 16000,
 'signal_normalization': True,
 'silence_min_duration_split': 0.4,
 'speaker_embedding_size': 256,
 'symmetric_mels': True,
 'synthesis_batch_size': 16,
 'trim_silence': True,
 'tts_cleaner_names': ['english_cleaners'],
 'tts_clip_grad_norm': 1.0,
 'tts_decoder_dims': 128,
 'tts_dropout': 0.5,
 'tts_embed_dims': 512,
 'tts_encoder_K': 5,
 'tts_encoder_dims': 256,
 'tts_eval_interval': 500,
 'tts_eval_num_samples': 1,
 'tts_lstm_dims': 1024,
 'tts_num_highways': 4,
 'tts_postnet_K': 5,
 'tts_postnet_dims': 512,
 'tts_schedule': [(2, 0.001, 20000, 12),
                  (2, 0.0005, 40000, 12),
                  (2, 0.0002, 80000, 12),
                  (2, 0.0001, 160000, 12),
                  (2, 3e-05, 320000, 12),
                  (2, 1e-05, 640000, 12)],
 'tts_stop_threshold': -3.4,
 'use_lws': False,
 'utterance_min_duration': 1.6,
 'win_size': 800}
Synthesizer using device: cuda
Trainable Parameters: 30.870M

Loading weights at synthesizer\saved_models\V13M_LS_pretrained\V13M_LS_pretrained.pt
Tacotron weights loaded from step 297000
Using inputs from:
        datasets_root\SV2TTS\synthesizer\train.txt
        datasets_root\SV2TTS\synthesizer\mels
        datasets_root\SV2TTS\synthesizer\embeds
Found 325 samples
  0%|                                                                                           | 0/21 [00:00<?, ?it/s]C:\Users\Colt_\.conda\envs\VoiceClone\lib\site-packages\torch\nn\functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\c10/core/TensorImpl.h:1156.)
  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)
  0%|                                                                                           | 0/21 [00:03<?, ?it/s]
Traceback (most recent call last):
  File ""vocoder_preprocess.py"", line 58, in <module>
    run_synthesis(args.in_dir, args.out_dir, args.model_dir, modified_hp)
  File ""C:\Utilities\SV2TTS\synthesizer\synthesize.py"", line 87, in run_synthesis
    _, mels_out, _ = model(texts, mels, embeds)
ValueError: too many values to unpack (expected 3)

```
Here are the relevant lines from synthesize.py:

`83 # Parallelize model onto GPUS using workaround due to python bug
84           if device.type == ""cuda"" and torch.cuda.device_count() > 1:
85               _, mels_out, _ = data_parallel_workaround(model, texts, mels, embeds)
86           else:
87               _, mels_out, _ = model(texts, mels, embeds)`

Not sure where to go with this one...I am using a GPU, CUDA 11.1, num_workers=0 (because of Win10 pickle error).
Could it be that the mels_out assignment should really be to the data_parallel_workaround rather than to model(text, mels, embed)?  
Regards,
TC2",bit tracing based error line line batch batch batch batch ran command line python error still run output received python false false true true true true true true true false synthesizer device trainable loading loaded step found associated experimental feature subject change please use anything important stable triggered internally return input stride padding dilation recent call last file line module file line model many unpack relevant parallelize model onto due python bug model else model sure go one win pickle error could assignment really rather model text embed,issue,positive,positive,positive,positive,positive,positive
908458333,"You need to install Sklearn module. Then try.
pip install -U scikit-learn

Best practice is to create a separate environment and do all the
requirements installation for the project there.

On Mon, 30 Aug, 2021, 8:49 pm Atomdax, ***@***.***> wrote:

> ModuleNotFoundError: No module named 'sklearn.utils'
>
> on windows
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/834>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AQB63DMVSQ44NOVN6SRMBBLT7OORPANCNFSM5DCDHRLA>
> .
> Triage notifications on the go with GitHub Mobile for iOS
> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>
> or Android
> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.
>
>
",need install module try pip install best practice create separate environment installation project mon wrote module thread reply directly view triage go mobile android,issue,positive,positive,positive,positive,positive,positive
906306547,"> I can't download the pretrained models, can someone upload to a different file sharing service?

Use this website  with mortix https://baidu.kinh.cc/",ca someone different file service use,issue,negative,neutral,neutral,neutral,neutral,neutral
906025019,"I just found issue #669 and implemented blue-fish solution to change synthesizer train.py to check for windows system and if so, set numworkers =0. This worked, and synthesizer is now happily training away. Closing this issue.
Regards,
Tomcattwo.",found issue solution change synthesizer check system set worked synthesizer happily training away issue,issue,positive,positive,positive,positive,positive,positive
905380474,"Please follow the directions here to set up the repo without Anaconda: https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542

",please follow set without anaconda,issue,negative,neutral,neutral,neutral,neutral,neutral
905378974,"Loading an audio in the toolbox does not train or alter the model in any way. It just changes the speaker embedding (an array of size (256,)) input to the synthesizer. Only the embedding from the last loaded audio is used during generation. Due to dropout, the synthesizer output is never the same for a given set of inputs (speaker embedding + text) **unless** the ""random seed"" parameter is set to force deterministic generation.",loading audio toolbox train alter model way speaker array size input synthesizer last loaded audio used generation due dropout synthesizer output never given set speaker text unless random seed parameter set force deterministic generation,issue,negative,negative,negative,negative,negative,negative
905375966,"As you've noticed, the old Tensorflow 1.x repo is not compatible with latest hardware. We tried upgrading to TF2 (see #370) but the automatic conversion tool failed.

There are 2 options:
1. Train a model with LibriTTS on the new repo (either using CPU, or your old GPU if you still have it, or ask for help)
2. Rewrite the old synthesizer model in Pytorch

I would be willing to do the Pytorch rewrite if someone out there wants to sponsor the work. It would also include a conversion utility to make TF model checkpoint files usable.",old compatible latest hardware tried see automatic conversion tool train model new either old still ask help rewrite old synthesizer model would willing rewrite someone sponsor work would also include conversion utility make model usable,issue,negative,positive,positive,positive,positive,positive
905363236,The internal architecture of the Tacotron TTS model is not compatible with known VC algorithms. You need to start from scratch or use someone else's VC codebase.,internal architecture model compatible known need start scratch use someone else,issue,negative,neutral,neutral,neutral,neutral,neutral
905356815,"Our code always passes the speaker_embedding argument so the default value is never used. However, it is there for a reason.

The synthesizer is Tacotron 1 (from fatchord's repo). When I was working on #472, I set the default speaker_embedding=None so I could test the code using fatchord's pretrained **single-speaker** synthesizer model. This was necessary until I could train a working model from scratch.",code always argument default value never used however reason synthesizer working set default could test code synthesizer model necessary could train working model scratch,issue,negative,neutral,neutral,neutral,neutral,neutral
905351303,The GPU may not be the bottleneck for encoder training. Check the GPU utilization and diagnostic output to find out. The main advantage of multiple GPUs is ability to use a larger batch size.,may bottleneck training check utilization diagnostic output find main advantage multiple ability use batch size,issue,positive,positive,neutral,neutral,positive,positive
905349446,"Encoder training does not support multiple GPUs currently. It is possible but requires some updates to the code. Please submit a pull request if you figure this out.

Closing this issue as a duplicate of #664 .",training support multiple currently possible code please submit pull request figure issue duplicate,issue,positive,neutral,neutral,neutral,neutral,neutral
905346097,"Here is a brief overview. If you need more information, please refer to the [SV2TTS paper](https://arxiv.org/pdf/1806.04558.pdf) and the code in this repo.

### Encoder model
* Input = mel spectrogram (2-D array with 40 channels and other dimension equal to number of timesteps)
* Output = speaker embedding (1-D array with 256 elements)

### Synthesizer model
* Input 1 = speaker embedding
* Input 2 = text (represented as 1-D array)
* Output = mel spectrogram (2-D array with 80 channels and other dimension equal to number of timesteps)

### Vocoder model
* Input = mel spectrogram
* Output = audio (16000 samples per second)",brief overview need information please refer paper code model input mel spectrogram array dimension equal number output speaker array synthesizer model input speaker input text array output mel spectrogram array dimension equal number model input mel spectrogram output audio per second,issue,negative,neutral,neutral,neutral,neutral,neutral
905340778,"Hi, thank you for submitting such a detailed issue with screenshots. What is the performance difference when vocoding in `--cpu` mode compared to GPU?

There are some operations in the vocoder model that need to be performed on CPU, so it is plausible that is the bottleneck. You can step through the model and refer to the pytorch documentation to see if a particular operation is performed using the CPU or GPU.",hi thank detailed issue performance difference mode model need plausible bottleneck step model refer documentation see particular operation,issue,negative,positive,positive,positive,positive,positive
905335172,Maybe your spectrogram or wav data contains a NaN.,maybe spectrogram data nan,issue,negative,neutral,neutral,neutral,neutral,neutral
905333436,This appears to be a duplicate of #767. Try the instructions in #615 to set up a virtual environment.,duplicate try set virtual environment,issue,negative,neutral,neutral,neutral,neutral,neutral
905331530,This seems specific to your Python installation. Are there any other users that experience this problem?,specific python installation experience problem,issue,negative,neutral,neutral,neutral,neutral,neutral
905328772,Please submit a pull request.,please submit pull request,issue,negative,neutral,neutral,neutral,neutral,neutral
905326146,"For a small number of voices, you don't need an encoder. You can use a numeric speaker ID (1, 2, 3, 4, ...) as the embedding. If you do this, the synthesizer model will need to be trained from scratch. You will need hours of speech data to train a good model.

If you have a limited amount of training data, use the finetuning method I propose in #437. Train a separate model to each speaker.",small number need use speaker id synthesizer model need trained scratch need speech data train good model limited amount training data use method propose train separate model speaker,issue,negative,positive,positive,positive,positive,positive
905321817,"You can't do 1 or 2 with this repo.

There is a limited ability to do 3, by defining a [text cleaner](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/synthesizer/utils/cleaners.py) to replace words in your input text. However, all you can do is substitute words in the input. It cannot be used to change the language.",ca limited ability text cleaner replace input text however substitute input used change language,issue,negative,negative,neutral,neutral,negative,negative
905318043,"Please share the traceback accompanying the error message. Also check that you're using a supported version of Python (currently 3.6, 3.7, 3.8).",please share error message also check version python currently,issue,negative,neutral,neutral,neutral,neutral,neutral
905315858,You can also run demo_cli.py and demo_toolbox.py with the `--cpu` option to bypass the GPU's limited memory.,also run option bypass limited memory,issue,negative,negative,neutral,neutral,negative,negative
905314050,You are correct. The `utterances_per_speaker` setting in [encoder/params_model.py](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/95adc699c1deb637f485e85a5107d40da0ad94fc/encoder/params_model.py) only affects the batch size for training. It has no effect for inference.,correct setting batch size training effect inference,issue,negative,neutral,neutral,neutral,neutral,neutral
905312093,"I can't download the pretrained models, can someone upload to a different file sharing service? ",ca someone different file service,issue,negative,neutral,neutral,neutral,neutral,neutral
905303425,"> I think vocoder also don't need training (NOT SURE ?).

Try using the pretrained vocoder, trained on English speech. It should also work for Spanish since the input is a mel spectrogram.

> Should I change the symbols.py and replace english characters with spanish alphabets

Yes. You will need to train a new model, using a dataset with Spanish audio.",think also need training sure try trained speech also work since input mel spectrogram change replace yes need train new model audio,issue,positive,positive,positive,positive,positive,positive
905294814,"@ishaghodgaonkar Our open-source SV2TTS implementation already performs nearly as well as Google’s published samples, when it comes to similarity for unseen voices. However, Google still has the better TTS which results in higher overall quality.

I’ve tried numerous things to improve the TTS, with limited success. Some of the remaining low-hanging fruit includes:
1. Train the synthesizer with more voices
2. Improve quality of training audios (synthesizer + vocoder)
3. Change attention paradigm
4. Better vocoder model
5. Switching the synthesizer model back to Tacotron 2 is also an obvious thing to try, but not trivial.

To further improve voice similarity, you need to look beyond SV2TTS and consider different model architectures. One example is Attentron ([paper](https://arxiv.org/pdf/2005.08484), [audio samples](https://hyperconnect.github.io/Attentron/)). However, this involves a lot of effort and experimentation, and is not the high ROI activity you are looking for.",implementation already nearly well come similarity unseen however still better higher overall quality tried numerous improve limited success fruit train synthesizer improve quality training synthesizer change attention paradigm better model switching synthesizer model back also obvious thing try trivial improve voice similarity need look beyond consider different model one example paper audio however lot effort experimentation high roi activity looking,issue,positive,positive,positive,positive,positive,positive
903908035,"Basically, yes. The model is pretty much at its best, so if you would train it for long enough (like 2mil. steps at largest possible batch size) it would sound better.",basically yes model pretty much best would train long enough like mil possible batch size would sound better,issue,positive,positive,positive,positive,positive,positive
903840464,Do you mean fine-tuning the existing model? Is this worth the effort/ do you think there will be a significant performance improvement?,mean model worth think significant performance improvement,issue,positive,positive,positive,positive,positive,positive
903752578,"> Is it support chinese input or do we need to input in English like in your video?

now you can input in chinese",support input need input like video input,issue,positive,neutral,neutral,neutral,neutral,neutral
903653450,Is it support chinese input or do we need to input in English like in your video?,support input need input like video,issue,positive,neutral,neutral,neutral,neutral,neutral
903592012,"@Jetical You mentioned training, that's why I thought you would need help with it. Make sure that the audio you feed into the program is good as it can be, it doesn't have to be necessarily 5 second long (or short) and you don't really need more than one audio. The pretrained models were trained mainly on audiobooks, that's why your ""Big the Cat"" audio won't work as expected. Text you're trying to synthesize should be around 40 - 50 characters per sentence.",training thought would need help make sure audio feed program good necessarily second long short really need one audio trained mainly big cat audio wo work text trying synthesize around per sentence,issue,positive,positive,positive,positive,positive,positive
903508344,"Honestly, I'm not even sure if I'm following the right thing.
""Real Time"" isn't a necessity for me. I'm looking for a FOSS implementation of Adobe's project VoCo because I run Linux as my main OS now, and Adobe has implemented a pricing structure I find offensive.
I already do decent imitations of some people. As an Blender animator, I can do anything visually. The place I am weakest is in needing to hire voice talent. Otherwise I'm a one man movie studio.

I would love to see this as a feature on Tenacity (Audacity fork with no telemetry).
Maybe a YouTube channel for us artists would help get some support.
I would Patreon a Foss VoCo in a heartbeat.",honestly even sure following right thing real time necessity looking implementation adobe project run main o adobe structure find offensive already decent people blender animator anything visually place needing hire voice talent otherwise one man movie studio would love see feature tenacity audacity fork telemetry maybe channel u would help get support would heartbeat,issue,positive,positive,positive,positive,positive,positive
903495679,"Yeah, not gonna lie don't know what that means, i just jumped head first into this without a parachute and tried cloning ""big the cat"" voice, instead i get some out of breath creeper reading my synthesized text, the clip isn 5 seconds long(i can make it that short if need be) , i thought this program of cloning worked better the more times the voice was synthesized. Anywho if can be pointed in the right direction im certain i can make a fix. My skill is low when it comes to running programs like these, but my bag is full of comedic gold to come. and i have patience to learn. so that being said, if your willing to help or googlearn me, what do you mean, dataset, batch size,and  what do you mean by ""steps?""",yeah gon na lie know head first without parachute tried big cat voice instead get breath creeper reading text clip long make short need thought program worked better time voice pointed right direction certain make fix skill low come running like bag full comedic gold come patience learn said willing help mean batch size mean,issue,positive,positive,neutral,neutral,positive,positive
902950368,You can simply enlarge the dataset (or enhance it with better recordings) and train the model longer.,simply enlarge enhance better train model longer,issue,negative,positive,positive,positive,positive,positive
902713767,"> > RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 2.00 GiB total capacity; 939.96 MiB already allocated; 0 bytes free; 962.00 MiB reserved in total by PyTorch) This error occurs when i'm trying synthesize and vocode.
> 
> you can try to decrease the **synthesis_batch_size** in synthesizer\hparams.py, my case is 2, and it works

also same error using my 940MX,it doesn't work,any other way?",memory tried allocate mib gib total capacity mib already free mib reserved total error trying synthesize try decrease case work also error work way,issue,negative,positive,positive,positive,positive,positive
902685005,"> Unfortunately, training is not available for Google Colab, and there are no plans to support it in this repo.
> 
> My recommendation is to get proper hardware if you're serious about training a model. You'll need to assess whether you have sufficient enthusiasm and computer skill to justify the expense.

um, gpu shortage? its kinda uh, impossible to ""get proper hardware"" right now?",unfortunately training available support recommendation get proper hardware serious training model need ass whether sufficient enthusiasm computer skill justify expense um shortage impossible get proper hardware right,issue,negative,negative,negative,negative,negative,negative
901462450,"Not sure what dataset you're using. But with default batch size, you should hear understable sentences at around 20k steps in wavs directory.",sure default batch size hear around directory,issue,negative,positive,positive,positive,positive,positive
901459463,The current pretrained models output english audio sounding like input audio. There's only one way to make it sound better and that is to use a larger dataset and train the models as much as possible. The problem is that the quality would increase only a little in an awfully long time.,current output audio sounding like input audio one way make sound better use train much possible problem quality would increase little awfully long time,issue,positive,positive,positive,positive,positive,positive
900803675,@blue-fish Could you please share your thoughts on this one if possible ?,could please share one possible,issue,positive,neutral,neutral,neutral,neutral,neutral
899930828,"I managed to boot it up, however all my training attempts from audio clips, make my voices come out out of breath, terrified and scratchy, the more i synthesize, the weirder the voices become, so yeah, i'm somewhere at least lol",boot however training audio clip make come breath scratchy synthesize become yeah somewhere least,issue,negative,negative,negative,negative,negative,negative
898978295,Run `visdom` in seperate console window. Then run `encoder_train.py`. Also you can check which files to edit #819 and only other guide is [here](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training).,run console window run also check edit guide,issue,negative,neutral,neutral,neutral,neutral,neutral
898977696,"Make sure you're running the command properly `python demo_toolbox.py -d <datasets_root>` where datasets_root is folder above dataset_name
",make sure running command properly python folder,issue,negative,positive,positive,positive,positive,positive
898898844,"> * VoxCeleb1: Dev A - D as well as the metadata file (extract as `VoxCeleb1/wav` and `VoxCeleb1/vox1_meta.csv`)
> * VoxCeleb2: Dev A - H (extract as `VoxCeleb2/dev`)

Hmmm what kind of that i must to download ? 
*The site has a metadata VoxCeleb and audiofiles",dev well file extract dev extract kind must site,issue,positive,positive,positive,positive,positive,positive
897493213,"> All links to KuangDD's projects now are no longer accessible. I'm currently working on latest fork of this repo to support mandarin and if anyone want to use as reference, please be free to folk and train: https://github.com/babysor/Realtime-Voice-Clone-Chinese

i can not even synthesize ''ni hao' by your repo

",link longer accessible currently working latest fork support mandarin anyone want use reference please free folk train even synthesize ni hao,issue,positive,positive,positive,positive,positive,positive
895215799,"I got a same error

Preprocessing librispeech_other
LibriSpeech/train-other-500: Preprocessing data for 1166 speakers.
LibriSpeech/train-other-500: 100%|██████████████████████████████████████████| 1166/1166 [00:33<00:00, 35.21speakers/s]
Done preprocessing LibriSpeech/train-other-500.

Preprocessing voxceleb1
VoxCeleb1: using samples from 1123 (presumed anglophone) speakers out of 1251.
VoxCeleb1: found 0 anglophone speakers on the disk, 1123 missing (this is normal).
VoxCeleb1: Preprocessing data for 0 speakers.
VoxCeleb1: 0speakers [00:00, ?speakers/s]
Done preprocessing VoxCeleb1.

Preprocessing voxceleb2
VoxCeleb2: Preprocessing data for 0 speakers.
VoxCeleb2: 0speakers [00:00, ?speakers/s]
Done preprocessing VoxCeleb2.

Please help me !!",got error data done found disk missing normal data done data done please help,issue,negative,negative,neutral,neutral,negative,negative
895160498,"You can check the file diff in my repo for reference. Mine works for Chinese and I think you can do the similar modification.
https://github.com/babysor/Realtime-Voice-Clone-Chinese",check file reference mine work think similar modification,issue,negative,neutral,neutral,neutral,neutral,neutral
894915940,"I strongly recommend you use the venv, which is a much better practice unless you want to mess up the projects with others and your system.",strongly recommend use much better practice unless want mess system,issue,positive,positive,positive,positive,positive,positive
894913186,"None, the terminal is just stuck doing nothing. For some reason, the problem only happens on regular Python instead of venv (https://levelup.gitconnected.com/how-to-create-a-voice-clone-with-the-real-time-voice-cloning-toolbox-on-windows-7b8609438001).",none terminal stuck nothing reason problem regular python instead,issue,negative,neutral,neutral,neutral,neutral,neutral
894602905,"> @mennatallah644 The list is up-to-date. There are no Arabic pretrained models at this time.

It seems the link to the Chinese model is now broken. ",list time link model broken,issue,negative,negative,negative,negative,negative,negative
894602218,"All links to KuangDD's projects now are no longer accessible. I'm currently working on latest fork of this repo to support mandarin and if anyone want to use as reference, please be free to folk and train: https://github.com/babysor/Realtime-Voice-Clone-Chinese",link longer accessible currently working latest fork support mandarin anyone want use reference please free folk train,issue,positive,positive,positive,positive,positive,positive
894601870,"Some projects of chinese pretrain model now are no longer accessible. I'm currently working on latest fork of this repo to support mandarin and if anyone want to use as reference, please be free to folk and train: https://github.com/babysor/Realtime-Voice-Clone-Chinese",pretrain model longer accessible currently working latest fork support mandarin anyone want use reference please free folk train,issue,positive,positive,positive,positive,positive,positive
894601419," I'm currently working on latest fork of this repo to support mandarin and if anyone want to use as reference, please be free to folk and train: https://github.com/babysor/Realtime-Voice-Clone-Chinese",currently working latest fork support mandarin anyone want use reference please free folk train,issue,positive,positive,positive,positive,positive,positive
894601360,"> Thank you for sharing the zhrtvc pretrained models @windht ! It will not be as obvious in the future, so for anyone else who wants to try, the models work flawlessly with this commit: https://github.com/KuangDD/zhrtvc/tree/932d6e334c54513b949fea2923e577daf292b44e
> 
> What I like about zhrtvc:
> 
> * Display alignments for synthesized spectrograms
> * Option to preprocess wavs for making the speaker embedding.
> * Auto-save generated wavs (though I prefer our solution in [Export and replay generated wav #402](https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/402))
> 
> Melgan is integrated but it doesn't work well with the default synthesizer model, so I ended up using Griffin-Lim most of the time for testing. WaveRNN quality is not that good either so it might be an issue on my end.
> 
> I'm trying to come up with ideas for this repo to support other languages without having to edit files.

All links to KuangDD's projects now are no longer accessible. I'm currently working on latest fork of this repo to support mandarin and if anyone want to use as reference, please be free to folk and train: https://github.com/babysor/Realtime-Voice-Clone-Chinese",thank obvious future anyone else try work flawlessly commit like display option making speaker though prefer solution export replay work well default synthesizer model ended time testing quality good either might issue end trying come support without edit link longer accessible currently working latest fork support mandarin anyone want use reference please free folk train,issue,positive,positive,positive,positive,positive,positive
894385661,"> RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 2.00 GiB total capacity; 939.96 MiB already allocated; 0 bytes free; 962.00 MiB reserved in total by PyTorch) This error occurs when i'm trying synthesize and vocode.

you can try to decrease the **synthesis_batch_size** in synthesizer\hparams.py, my case is 2, and it works",memory tried allocate mib gib total capacity mib already free mib reserved total error trying synthesize try decrease case work,issue,negative,positive,positive,positive,positive,positive
892483083,"You might need to use a better GPU. I got the same error using my 940MX after 1min training, but I ran finely with 2060GTX after hours.",might need use better got error min training ran finely,issue,negative,positive,positive,positive,positive,positive
889024330,"Guys, if you couldn't solve the problem, write the name of that module/package with ""install"" to Google. Install it, then try to write ""demo_toolbox.py"" again.",could solve problem write name install install try write,issue,negative,neutral,neutral,neutral,neutral,neutral
886031235,"I came across [this](https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542) that they never end up merging the new docs, however there are some stuff missing for windows...

Follow that then it works (missing pytorch and several others)",came across never end new however stuff missing follow work missing several,issue,negative,negative,neutral,neutral,negative,negative
885897610,"YO I found the solution go to Anacando (or whatever command thing ur using) and type. ""C:\Users\(UR USERNAME)\Desktop\Real-Time-Voice-Cloning-master> ls"" (Or the directory you saved it in but this will work if u are using the default one aka u ignored everything and just pressed accept during installation)

Once that, you should see something like this
![Screenshot 2021-07-23 212349](https://user-images.githubusercontent.com/87876721/126837686-10b866dd-5293-4530-9317-3588bc226db0.png)

Type ""pip install -r .\requirements.txt"" and press enter it should look something like this
![Screenshot 2021-07-23 212546](https://user-images.githubusercontent.com/87876721/126837941-9b28878f-977a-4347-ad9e-637890a03c3f.png)

After that finishes installing give the proagram another go. It should work.

",yo found solution go whatever command thing ur type ur directory saved work default one aka everything accept installation see something like type pip install press enter look something like give another go work,issue,positive,neutral,neutral,neutral,neutral,neutral
885861659,Same. but it says No module named 'matplotlib' I think we need to download something,module think need something,issue,negative,neutral,neutral,neutral,neutral,neutral
885529269,"yes you can , you just need to do some code modification /get inspired by the demo_cli.py for example and u will be fine !",yes need code modification inspired example fine,issue,positive,positive,positive,positive,positive,positive
884564367,"@MGSousa you're brazilian right? Can we talk a lil more about it? 
se for brasileiro podemos ver alguma forma de conversarmos sobre isso? Queria saber mais sobre o dataset de vozes para o treinamento",right talk se de saber de para,issue,negative,positive,positive,positive,positive,positive
884408801,"@Rainer2465 I changed outputs/step to four ( r=4 ~91 ) somewhat got decent results for now. 
But with r=2 the results remain the same (42 ~ 43 steps /s).",rainer four somewhat got decent remain,issue,negative,positive,positive,positive,positive,positive
884273685,"another question , if i ihad intention to only use the tool for 5 voices, does training the models help to get better results  or  ican work the current ones (pretrained.pt) ?",another question intention use tool training help get better work current,issue,positive,positive,positive,positive,positive,positive
882210202,"as I tried to run the demo_toolbox.py I was getting the ""No module named 'vocoder.display'"" error, comming from the Syntheziser/interface.py line 7 ""from vocoder.display import simple_table"". it only worked as i created the /vocoder/__init__.py file",tried run getting module error line import worked file,issue,negative,neutral,neutral,neutral,neutral,neutral
882023326,What exactly are you trying to run? Because that file isn't a problem.,exactly trying run file problem,issue,negative,positive,positive,positive,positive,positive
881995475,"Solved, ended up having to reinstall most of the libraries that were supposed to be installed by requirements.txt. I don't know how this problem came about, but reinstalling various libraries solved this error and every error after it.",ended reinstall supposed know problem came various error every error,issue,negative,neutral,neutral,neutral,neutral,neutral
881884300,@pilnyjakub thanks. Somehow I overlooked that alignments are also mandatory.,thanks somehow also mandatory,issue,negative,positive,positive,positive,positive,positive
881697701,"Image of terminal because I cant figure out this formatting
![terminalVoiceClone](https://user-images.githubusercontent.com/39639229/126004511-8debb5f2-ae63-4eb7-84fc-5e3847fd16b6.png)
",image terminal cant figure,issue,negative,neutral,neutral,neutral,neutral,neutral
880179209,"@blue-fish 
Hey, is there still anything going towards voice-to-voice? I would be super interested to know more!",hey still anything going towards would super interested know,issue,positive,positive,positive,positive,positive,positive
879514558,ok i managed to get the preprocess to do all 30 gigs.. phew.  now to figure out the rest ecoder_train is next?,get phew figure rest next,issue,negative,neutral,neutral,neutral,neutral,neutral
879179179,"As we're talking about using and not training the model, there is no saving at this stage. When using a model on a voice, the result is unpredicteble unless you set random seed to specific value.",talking training model saving stage model voice result unless set random seed specific value,issue,negative,negative,negative,negative,negative,negative
879157268,"the saving of a ""single voice model"" would interest me too",saving single voice model would interest,issue,negative,negative,neutral,neutral,negative,negative
878869479,"Sorry, my bad, I should have shown it better on the dataset you are using. `E:\[datasets_root]\LibriSpeech\train-clean-100` is the structure you should have. Then just run `python encoder_preprocess.py E:\[datasets_root]`. I'd also suggest not having the datasets_root in the save directory as this application.",sorry bad shown better structure run python also suggest save directory application,issue,negative,negative,negative,negative,negative,negative
877782114,i think this is just a cuda bug because i changed from torch 1.6 to torch 1.3 and it worked. ,think bug torch torch worked,issue,negative,neutral,neutral,neutral,neutral,neutral
877639236,Models are trained on multiple voices. They can then be used on any voice.,trained multiple used voice,issue,negative,neutral,neutral,neutral,neutral,neutral
877638713,"Models are trained on multiple voices. They can then be used on any voice. When they are used, they are unpredictable unless you set the random seed to specific value. There is no ""saving"" at this stage (already using the model).",trained multiple used voice used unpredictable unless set random seed specific value saving stage already model,issue,negative,negative,negative,negative,negative,negative
877637269,"How can collaborate?

El sáb., 10 de jul. de 2021, 8:12 a. m., pilnyjakub <
***@***.***> escribió:

> There is no Spanish model yet.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/789#issuecomment-877635873>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AADIWFGK5WVUWHPXCBCYIG3TXBBL3ANCNFSM473UA5FA>
> .
>
",collaborate el de de model yet thread reply directly view,issue,negative,positive,neutral,neutral,positive,positive
877635080,This is full path to training audio `E:\Speech\[my_dataset]\audio\[speaker_id]\[book_id]\[audio]` that means my datasets_root is `E:\Speech`,full path training audio audio,issue,negative,positive,positive,positive,positive,positive
877627689,"Hi , sorry for asking any further .Are models described by pilnyjakub , ML models applied on a specific voice ? I m am 100% sure that Anas-eldili means to recreate a specific voice without having to reload everything. Is a pt model trained on a special voice or just pretained and ready to be applied on a specific voice ?",hi sorry applied specific voice sure recreate specific voice without reload everything model trained special voice ready applied specific voice,issue,positive,positive,neutral,neutral,positive,positive
877164033,I am pretty sure it's a windows issue. I'm trying to get it working on a virtual Box. (kali) ,pretty sure issue trying get working virtual box kali,issue,positive,positive,positive,positive,positive,positive
874591468,"There's information on other people's attempts at this on existing issue [Support for other languages #30](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/30#issuecomment-831992614).

",information people issue support,issue,negative,neutral,neutral,neutral,neutral,neutral
872721231,"> You might have to downgrade to `torch==1.4.0` to get DataParallel to work.
> 
> * [How to use multi-GPU? fatchord/WaveRNN#189](https://github.com/fatchord/WaveRNN/issues/189)
> * [Pytorch 1.5 DataParallel huggingface/transformers#3936](https://github.com/huggingface/transformers/issues/3936)

Hey, I'm having issues with this as well..
I feel it's just something stupid-simple I'm overlooking and is an easy fix though if you are able to get it working.. :)

The original repo worked fine with 1 GPU using Torch 1.6.0 before I installed a 2nd GPU to speed up training..
I am using torch 1.7.1 like you said was working for you.
Torch version 1.4.0 does not actually run.


I have tried using both the CorentinJ and your blue-fish repo forks (yours being the one you had suggested which was the branch for multi-GPU support).
The main repo does not run with Torch 1.4.0, 1.6.0, nor 1.7.1 unless I remove the second GPU from the system.
Your repo branch I mentioned does work with the enviroment path override added and using Torch 1.7.1.. however it does not actually utilize the second GPU.


Is there a requirements.txt that you can provide for testing? Perhaps I have some other library installed which breaks this functionality? I'm grasping at straws at this point.. 
I have been working at it for days but to no avail.. Didn't want to post here until I felt that I **needed** assistance.


Kind regards.",might downgrade get work use hey well feel something easy fix though able get working original worked fine torch speed training torch like said working torch version actually run tried one branch support main run torch unless remove second system branch work path override added torch however actually utilize second provide testing perhaps library functionality grasping point working day avail want post felt assistance kind,issue,positive,positive,positive,positive,positive,positive
870788708,"The models are saved automatically, you can change the frequency in encoder_train.py/synthesizer_train.py or by passing the parrameters.",saved automatically change frequency passing,issue,negative,neutral,neutral,neutral,neutral,neutral
868202782,For another language in  single speaker ?,another language single speaker,issue,negative,negative,neutral,neutral,negative,negative
867942948,"Hello i am trying to train the  system in spanish 
The first thing i need is train the encoder ,what i need to change in the code or what are the step by step for make the training someone can help me ?",hello trying train system first thing need train need change code step step make training someone help,issue,negative,positive,positive,positive,positive,positive
867466575,"Hm?? strange i got this error 

 Could not find any synthesizer weights under synthesizer\saved_models\pretrained\taco_pretrained

Can someone help me?? ",strange got error could find synthesizer someone help,issue,negative,negative,neutral,neutral,negative,negative
866434962,"Thanks blue-fish, let me take a look at the mel-spectrograms. ",thanks let take look,issue,negative,positive,positive,positive,positive,positive
866160734,"Yes, the additional symbols are needed. But you can't add new symbols when finetuning. If using the pretrained English model, you're stuck with the original symbol set.

For best results you should train a new model on the expanded symbol set.",yes additional ca add new model stuck original symbol set best train new model expanded symbol set,issue,positive,positive,positive,positive,positive,positive
866144038,"had modified, expanded synthesizer/utils/symbols.py for the Spanish alphabet
#_characters = ""AÁBCDEÉFGHIÍJKLMNÑOÓPQRSTUÚÜVWXYZaábcdeéfghiíjklmnñoópqrstuúüvwxyz!,.:;?¡¿-|\'()\""° ""
_characters = ""ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz!\'\""(),-.:;? ""
I made a fine adjustment and thought that I should leave the alphabet of my language.
If I put the preset symbols.py it no longer gives that error.
It is not necessary to use the symbols in Spanish when executing the toolbox, so that the pronunciation is good for another language, and detects well the ""Ñ"" for example?",expanded alphabet made fine adjustment thought leave alphabet language put preset longer error necessary use toolbox pronunciation good another language well example,issue,negative,positive,positive,positive,positive,positive
865610871,"Something is not right there, but I haven't experienced this issue before. Make sure there is 1 folder per speaker in `<datasets_root>/SV2TTS/encoder`. If you understand the preprocessing, inspect the spectrograms in the .npy files. You can also try preprocessing again. I'm out of ideas if that doesn't solve it.",something right experienced issue make sure folder per speaker understand inspect also try solve,issue,negative,positive,positive,positive,positive,positive
865410124,"This is not something that we support or plan to implement, but you may find this helpful: https://github.com/pytorch/pytorch/issues/2733",something support plan implement may find helpful,issue,positive,neutral,neutral,neutral,neutral,neutral
865404772,The list of symbols in `synthesizer/utils/symbols.py` needs to match the version used to train the model file.,list need match version used train model file,issue,negative,neutral,neutral,neutral,neutral,neutral
865359992,"> Do I need to edit some configuration file in order to the list of character of my language or I can follow the same training step described here?

@andreafiandro  - ""Considerations - languages other than English"" in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/431#issuecomment-673555684",need edit configuration file order list character language follow training step,issue,negative,neutral,neutral,neutral,neutral,neutral
864959430,"> @andreafiandro Check the attention graphs from your synthesizer model training. You should get diagonal lines that look like this if attention has been learned. (This is required for inference to work) https://github.com/Rayhane-mamah/Tacotron-2/wiki/Spectrogram-Feature-prediction-network#tacotron-2-attention
> 
> If it does not look like that, you'll need additional training for the synthesizer, check the preprocessing for problems, and/or clean your dataset.

Thank you, I have something really different from expected diagonal line:
![attention_step_30000_sample_1](https://user-images.githubusercontent.com/18190274/122754442-f6593900-d293-11eb-94fd-4f257f6f9026.png)

Probably I made some mistake in the data preprocessing or the dataset is too poor. I will try again, checking the results using the plots.

Do I need to edit some configuration file in order to the list of character of my language or I can follow the same training step described here?

@VitoCostanzo I can share the file if you want but it isn't working for the moment",check attention synthesizer model training get diagonal look like attention learned inference work look like need additional training synthesizer check clean thank something really different diagonal line probably made mistake data poor try need edit configuration file order list character language follow training step share file want working moment,issue,positive,negative,neutral,neutral,negative,negative
864652401,"Hi @blue-fish :
Sorry for late reply, I started a training job with lr1e-4and the loss hits 0.0000 after 250k iterations, here is the umap illustrations at various iterations. 
![trial_lr1e-4_umap_070000](https://user-images.githubusercontent.com/18693148/122835335-2584a000-d2be-11eb-8845-acc92909be38.png)

![trial_lr1e-4_umap_249000](https://user-images.githubusercontent.com/18693148/122694214-ae420400-d20a-11eb-817c-c33232c0f661.png)
![trial_lr1e-4_umap_150000](https://user-images.githubusercontent.com/18693148/122835361-2fa69e80-d2be-11eb-933e-26015054886d.png)

Would you be so kind and point out what could be wrong in this scenario? 

Best


",hi sorry late reply training job loss various would kind point could wrong scenario best,issue,negative,positive,neutral,neutral,positive,positive
864340065,"> Can someone explain step 2 (Pretrained files) in excruciating detail?

@qoevak 
### Detailed instructions for step 12 (pretrained files)
1. Open Windows Explorer and navigate to the downloaded pretrained.zip file.
2. Double-click to look inside the zip file.
3. You will see 3 folders, named `encoder`, `synthesizer` and `vocoder`.
    * This mirrors the directory structure of the repo.
4. Go inside the `encoder` folder. You will see a folder named `saved_models`. 
5. Open a second Windows Explorer and navigate to `C:\Real-Time-Voice-Cloning-master\encoder`
6. Drag and drop the `saved_models` folder into `C:\Real-Time-Voice-Cloning-master\encoder`.
7. Repeat 4-6 for ""synthesizer"" and ""vocoder"":
    * Drag and drop the `saved_models` folder **from the zip file's `synthesizer` folder** into `C:\Real-Time-Voice-Cloning-master\synthesizer`.
    * Drag and drop the `saved_models` folder **from the zip file's `vocoder` folder** into `C:\Real-Time-Voice-Cloning-master\vocoder`.

### Notes
* All of the pretrained model files in the zip file are named `pretrained.pt`, which can be super confusing.
    * The one inside ""encoder"" can only be used with the encoder.
    * The one inside ""synthesizer"" can only be used with the synthesizer.
    * The one inside ""vocoder"" can only be used with the vocoder.
* You can use the filesizes to tell them apart.
    * The encoder `pretrained.pt` is 17 MB.
    * The synthesizer `pretrained.pt` is 370 MB.
    * The vocoder `pretrained.pt` is 53 MB.",someone explain step excruciating detail detailed step open explorer navigate file look inside zip file see synthesizer directory structure go inside folder see folder open second explorer navigate drag drop folder repeat synthesizer drag drop folder zip file synthesizer folder drag drop folder zip file folder model zip file super one inside used one inside synthesizer used synthesizer one inside used use tell apart synthesizer,issue,negative,positive,neutral,neutral,positive,positive
863797398,After 2 days of training I can confirm that the behavior is the result of not trimming silences.,day training confirm behavior result trimming,issue,negative,neutral,neutral,neutral,neutral,neutral
863737160,Can someone explain step 2 (Pretrained files) in excruciating detail?,someone explain step excruciating detail,issue,negative,neutral,neutral,neutral,neutral,neutral
863566647,"The loss for a good model is 0.01 or lower. You'll also want to inspect the UMAP projections during training to verify that the model is able to distinguish between speakers. From https://matheo.uliege.be/handle/2268.2/6801

![image](https://user-images.githubusercontent.com/67130644/122472587-a3227600-cf75-11eb-92f0-901ddcd701c2.png)
",loss good model lower also want inspect training verify model able distinguish image,issue,negative,positive,positive,positive,positive,positive
863009775,"Hi @blue-fish, thanks for your reply. That was exactly my assumption, I found out that in my older scripts the silence trimming was not used. I started the training for the synthesizer again yesterday, let's see if results improve.",hi thanks reply exactly assumption found older silence trimming used training synthesizer yesterday let see improve,issue,positive,positive,positive,positive,positive,positive
862973224,"Perhaps your synthesizer is trying to predict the silence that sometimes occurs at the start of your training utterances. In your screenshot, the spectrogram at top has some silence at the beginning. This seems a plausible explanation if it is representative of your training dataset.

After experiencing this issue myself, I added a ""trim silence"" feature to the synthesizer preprocessing. It is discussed somewhat in #501.

https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/95adc699c1deb637f485e85a5107d40da0ad94fc/synthesizer/preprocess.py#L206-L208",perhaps synthesizer trying predict silence sometimes start training spectrogram top silence beginning plausible explanation representative training issue added trim silence feature synthesizer somewhat,issue,negative,positive,positive,positive,positive,positive
862969534,You can use [`demo_cli.py`](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/demo_cli.py) as a template. The comments explain how to synthesize texts.,use template explain synthesize,issue,negative,neutral,neutral,neutral,neutral,neutral
862968371,"Yes. After attention is learned, you can use demo_toolbox to synthesize new speech.",yes attention learned use synthesize new speech,issue,negative,positive,positive,positive,positive,positive
861415726,"> Hello,
> I tried to train the model for the italian languages but I still have some issues.
> The steps I followed are:
> 
> * Preprocessing of the dataset http://www.openslr.org/94/
> * Training of the synthetizer
> * Using the synthetizer to generate the input data for the vocoder
> * Train of the vocoder
> 
> After a long training (especially for the vocoder) the output generated by means of the toolbox is really poor (it can't ""speak"" italian).
> 
> Did I do something wrong or I missed some steps?
> 
> Thank you in advance

@andreafiandro please, can you share your file trained for italian language? (pretrained.pt of synthetizer)",hello tried train model still training synthetizer synthetizer generate input data train long training especially output toolbox really poor ca speak something wrong thank advance please share file trained language synthetizer,issue,negative,negative,negative,negative,negative,negative
861354798,"Thank you very much for your help.I'm currently training on my own dataset and it predicts very well.When it finds Attention,like the predicted sounds,Is it understandable readable in the text synthesized in demo_toolbox?",thank much currently training attention like understandable readable text,issue,positive,neutral,neutral,neutral,neutral,neutral
861231666,"> Also, there should be a data line in the second image but it doesn't show up in this image. I still don't understand where I went wrong.

The absence of the line means attention has not been learned yet. (Attention is needed for inference to work.) In my experience, it takes anywhere between 10000 and 30000 steps to learn attention using the LibriSpeech dataset. Keep training. If you still don't see the line by 50000 steps, verify the quality of your dataset and double-check the model configuration.

> When I try to do vocoder training, I get an error message that it cannot find the file in it. 

It looks like Python doesn't have permission to make the folder called `Veri\SV2TTS\vocoder\mels_gta`. Try making the folder yourself and try the preprocess again.

There is a known bug with vocoder preprocess. You may need to edit a file to make it work: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/729#issuecomment-816901953",also data line second image show image still understand went wrong absence line attention learned yet attention inference work experience anywhere learn attention keep training still see line verify quality model configuration try training get error message find file like python permission make folder try making folder try known bug may need edit file make work,issue,negative,negative,negative,negative,negative,negative
861222894,"> 1: Do you think I can achieve decent results with this low batch size [of 6]?

It is possible. I have previously trained a model with a batch size of 2, with good results.

> 2: How many utterances should I use from every speaker.

As many as you can while preserving the overall balance of the dataset.

> 3: When training, ...

I'm not familiar with that error message. Anyway, we cannot answer questions for unsupported hardware configurations. Please consider upgrading your GPU.",think achieve decent low batch size possible previously trained model batch size good many use every speaker many overall balance training familiar error message anyway answer unsupported hardware please consider,issue,negative,positive,positive,positive,positive,positive
860802376,"solved  placing the dataset in the project folder and using this command: `python demo_toolbox.py -d .` the path must be without the dataset directory.
For example if do you have dataset on desktop you run `python demo_toolbox.py -d C:\Users\Vito\Desktop` and NOT `C:\Users\Vito\Desktop\LibriSpeech\train-clean-100`",project folder command python path must without directory example run python,issue,negative,neutral,neutral,neutral,neutral,neutral
860080413,"Error
spyder 4.2.5 requires pyqt5<5.13, but you have pyqt5 5.15.4 which is incompatible.

Kindly assist me in this 
Gowtham",error incompatible kindly assist,issue,negative,positive,positive,positive,positive,positive
858225778,"I am interested in this system, if it's still happening",interested system still happening,issue,negative,positive,positive,positive,positive,positive
858031441,"With ""random seed"" enabled, you will always get the same output for a given combination of speaker embedding and input text.",random seed always get output given combination speaker input text,issue,negative,negative,negative,negative,negative,negative
856278580,"Thanks for the answer. So i understood right, random sid shouldnt affect the input at all, neither in negative or positive way?",thanks answer understood right random shouldnt affect input neither negative positive way,issue,negative,negative,neutral,neutral,negative,negative
855911304,"Hi @rajuc110 , sorry for the delayed response. No, I couldn't reproduce the results in hindi and had to shift to another task meanwhile.",hi sorry response could reproduce shift another task meanwhile,issue,negative,negative,negative,negative,negative,negative
855420031,"> > should i split data to each folder for each speaker for encoder, synthesizer and vocoder?
> 
> For encoder training only, it is necessary to make separate folders for each speaker.
> It doesn't matter for the synthesizer or vocoder.

thank, i will try",split data folder speaker synthesizer training necessary make separate speaker matter synthesizer thank try,issue,negative,neutral,neutral,neutral,neutral,neutral
855388416,"thank you very much, it worked. I assumed it was to add encoding = 'utf-8', but I didn't know exactly where. When you learn well and have a decent speaking in Spanish, I will share it with you. Thanks.",thank much worked assumed add know exactly learn well decent speaking share thanks,issue,positive,positive,positive,positive,positive,positive
855356586,"@andreafiandro Check the attention graphs from your synthesizer model training. You should get diagonal lines that look like this if attention has been learned. (This is required for inference to work) https://github.com/Rayhane-mamah/Tacotron-2/wiki/Spectrogram-Feature-prediction-network#tacotron-2-attention

If it does not look like that, you'll need additional training for the synthesizer, check the preprocessing for problems, and/or clean your dataset.",check attention synthesizer model training get diagonal look like attention learned inference work look like need additional training synthesizer check clean,issue,positive,positive,positive,positive,positive,positive
855352177,"Unfortunately, training is not available for Google Colab, and there are no plans to support it in this repo.

My recommendation is to get proper hardware if you're serious about training a model. You'll need to assess whether you have sufficient enthusiasm and computer skill to justify the expense.",unfortunately training available support recommendation get proper hardware serious training model need ass whether sufficient enthusiasm computer skill justify expense,issue,negative,negative,negative,negative,negative,negative
855348685,"> should i split data to each folder for each speaker for encoder, synthesizer and vocoder?

For encoder training only, it is necessary to make separate folders for each speaker.
It doesn't matter for the synthesizer or vocoder.",split data folder speaker synthesizer training necessary make separate speaker matter synthesizer,issue,negative,neutral,neutral,neutral,neutral,neutral
855339225,"@ddlafouine any luck running this on your Mac? I have a MacBook Air with BIg Sur and I am not sure it this script will run on it.

Can you comment on that?",luck running mac air big sur sure script run comment,issue,positive,positive,positive,positive,positive,positive
854934807,"> The same issues happened to me. I created a virtual environment with Python 3.6 and it works without issues. See #615

Thanks, that sorted the issue. Now I just need to figure out how to use it :P",virtual environment python work without see thanks sorted issue need figure use,issue,negative,positive,positive,positive,positive,positive
854914594,The same issues happened to me. I created a virtual environment with Python 3.6 and it works without issues. See #615 ,virtual environment python work without see,issue,negative,neutral,neutral,neutral,neutral,neutral
854887319,"Running Manjaro Arch Linux

version numbers
Python 3.9.5
pip 21.0.1
numpy 1.20.3 (received errors* with 1.19.4)
scipy 1.6.3
torch 1.8.1


*`ValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject`


",running arch version python pip received torch size may indicate binary incompatibility header got,issue,negative,neutral,neutral,neutral,neutral,neutral
854266663,"To solve `EOFError: Ran out of input` on Windows, try the suggestion in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/669#issuecomment-781130738",solve ran input try suggestion,issue,negative,neutral,neutral,neutral,neutral,neutral
851005976,"I also get an error like this in the train part of the synthesizer ->EOFError:Ran out of input .I would also like to mention that I got this for LibriSpeech in the synthesizer both in German and the original git clone. I would appreciate it if you could help me on how to solve this problem. Thank you in advance :)
![image](https://user-images.githubusercontent.com/81026327/120107131-76debb00-c168-11eb-879a-31af9fed3f10.png)


",also get error like train part synthesizer ran input would also like mention got synthesizer german original git clone would appreciate could help solve problem thank advance image,issue,positive,positive,positive,positive,positive,positive
850997070,"> Klasör ve dosya yapısının LibriSpeech gibi görünmesi için Mozilla CommonVoice veri kümesini işleyerek başlayın. Bunun gibi bir şey: [#437 (yorum)](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-666099538)
> 
> O zaman ön işleme araçlarımız çalışmalıdır.

First of all, thank you for your reply.I started to design the dataset as you said. Right now I have close to 400 sounds, but this is not enough. Estimated how much more sound should I add and is there a place where I should add the txt file I prepared in the code?",veri zaman first thank design said right close enough much sound add place add file prepared code,issue,positive,positive,positive,positive,positive,positive
850957503,"This appears to be a problem with your system configuration. Most likely the operating system, memory, or GPU if you have one. Unfortunately we are not able to help troubleshoot. I can only recommend this:

1. Follow these instructions to set up the toolbox: https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542
2. If the problem still occurs, add breakpoints to find out specifically which line of code is responsible for the program freezing. Then troubleshoot from there.",problem system configuration likely operating system memory one unfortunately able help recommend follow set toolbox problem still add find specifically line code responsible program freezing,issue,negative,positive,positive,positive,positive,positive
850956875,"Start by processing the Mozilla CommonVoice dataset so the folder and file structure looks like LibriSpeech. Something like this: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-666099538

Then our preprocessing tools should work.",start folder file structure like something like work,issue,positive,neutral,neutral,neutral,neutral,neutral
850956596,"Duplicate of #700, let's collect all the information about slow training and possible fixes there. Thanks for reporting the issue.",duplicate let collect information slow training possible thanks issue,issue,negative,negative,neutral,neutral,negative,negative
850956076,"1. The sentence length should not depend on the duration of the audio clip used to make the embedding. You will have to work around this by generating shorter sentences.
2. The vocoder is trained to output at 16 kHz. Without retraining the model, the best one can do is to edit the code to save the wav at a higher sample rate. However, that will not improve the sound quality.",sentence length depend duration audio clip used make work around generating shorter trained output without model best one edit code save higher sample rate however improve sound quality,issue,negative,positive,positive,positive,positive,positive
850955197,Webrtcvad is optional. If the toolbox window does not appear that problem is caused by something else.,optional toolbox window appear problem something else,issue,negative,neutral,neutral,neutral,neutral,neutral
850955031,"Try following these instructions to the letter instead: https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542

Also use Python 3.7.",try following letter instead also use python,issue,negative,neutral,neutral,neutral,neutral,neutral
850954893,You need to run the `python demo_toolbox.py` command from the folder where `demo_toolbox.py` is located. You need to use the command line for this. The paths will be incorrect when clicking an icon in a file browser.,need run python command folder need use command line incorrect icon file browser,issue,negative,neutral,neutral,neutral,neutral,neutral
850954068,"1. ""Random seed"" makes the generation deterministic. Mostly used for troubleshooting and benchmarking.
2. No. Whenever an utterance is loaded, the embedding is recalculated using that utterance only. It makes no difference what order the utterances are loaded. Only the last one loaded matters.",random seed generation deterministic mostly used whenever utterance loaded utterance difference order loaded last one loaded,issue,negative,neutral,neutral,neutral,neutral,neutral
850953754,"There is not a Spanish model at this time.

If you have a GPU and wish to contribute:
1. Learn synthesizer training by following the tutorial to train an English model using the LibriSpeech dataset. https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training
2. Find a Spanish dataset. https://openslr.org/resources.php
3. Preprocess the dataset. Arrange the files so it looks like LibriSpeech. You do not need alignment files. https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-666099538
4. Train the synthesizer model from scratch. It may take multiple attempts to get a good model.",model time wish contribute learn synthesizer training following tutorial train model find arrange like need alignment train synthesizer model scratch may take multiple get good model,issue,positive,positive,positive,positive,positive,positive
850176099,"Hello,
I tried to train the model for the italian languages but I still have some issues.
The steps I followed are:

- Preprocessing of the dataset [http://www.openslr.org/94/](http://www.openslr.org/94/)
- Training of the synthetizer
- Using the synthetizer to generate the input data for the vocoder
- Train of the vocoder 

After a long training (especially for the vocoder) the output generated by means of the toolbox is really poor (it can't ""speak"" italian).

Did I do something wrong or I missed some steps?

Thank you in advance",hello tried train model still training synthetizer synthetizer generate input data train long training especially output toolbox really poor ca speak something wrong thank advance,issue,negative,negative,negative,negative,negative,negative
850139983,@blue-fish Any progress on this? I need a project like this,progress need project like,issue,positive,neutral,neutral,neutral,neutral,neutral
849914094,"@KeithYJohnson which celebreties voice you tried to use? From my experience this can happen if one or more of these circumstances are given.
* the voice is high. With voices of children or voices that reach that level it is almost completely impossible to get a good clone. For example i once tried to clone an earlier voice off mine just for the fun. It didn't sound like the original, had this windy noise (it is probably the vocoder or the synthesizer trying to generate a breathing of some sorts) and also you suddenly had these moments sometimes for entire sentences and sometimes starting at some point and then just geting worse through the entire generation. It was that the voice suddenly got much deeper then it was soposed to but it was still kind off mixed to the original which made a weirdish and grumpy output happen. It can also happen that it speaks but speaks giberish like ababababa. But i only had that happen like 1 time.
* You give it a to long sentence. You should keep them arround 5 till 12 seconds and then make a break by placing a blanc line. Unfortunately some of the going arround google colab books do not allow this, effectively making it impossible to generate long sentences, at some point you hear only breathing.
* the sample provided is not english. It actually works some sort but much much worse then english sample. Some times you get a pretty decend output actually (only happened 1 out off 5 times when i was messing with that though), sometiems outputs that do not sound like the original and are filled with breating and sometimes just pure breathing or clippy sounds.

Can you maybe give info whom you tried to generate from and which text?",voice tried use experience happen one given voice high reach level almost completely impossible get good clone example tried clone voice mine fun sound like original windy noise probably synthesizer trying generate breathing also suddenly sometimes entire sometimes starting point worse entire generation voice suddenly got much still kind mixed original made weirdish grumpy output happen also happen like happen like time give long sentence keep till make break blanc line unfortunately going allow effectively making impossible generate long point hear breathing sample provided actually work sort much much worse sample time get pretty output actually time messing though sound like original filled sometimes pure breathing maybe give tried generate text,issue,positive,positive,neutral,neutral,positive,positive
849532473,"> You know, I just decreased the sample rate and it worked for me pretty well!!!
> `sf.write(filename, generated_wav.astype(np.float32), round(synthesizer.sample_rate / 1.1))`
> 
> At least it reduced the noise =)

The voice doesn't sound anything at all like mine :(",know sample rate worked pretty well round least reduced noise voice sound anything like mine,issue,positive,positive,neutral,neutral,positive,positive
849528482,"The colab notebook gives following error

```
Synthesizing the waveform:
{| ████████████████ 85500/86400 | Batch Size: 9 | Gen Rate: 11.7kHz | }generated_wav.dtype:  float64
Caught exception: AttributeError(""module 'librosa' has no attribute 'output'"")
Restarting
```

Fixed by - in the translate.py line 131 replace this

```
# librosa.output.write_wav(fpath, generated_wav.astype(np.float32), synthesizer.sample_rate)
import soundfile as sf
sf.write('output.wav', generated_wav.astype(np.float32), synthesizer.sample_rate, 'PCM_24')
```",notebook following error batch size gen rate float caught exception module attribute fixed line replace import,issue,negative,positive,neutral,neutral,positive,positive
848965256,"Just realized it's right there in the log ""python3.9"". Let me know if you would like my steps for setting up a seperate 3.7 install along side your current 3.9(Did this on manjaro).",right log let know would like setting install along side current,issue,negative,positive,positive,positive,positive,positive
848961332,@jasonkhanlar just had a very similar issue trying to use python 3.9 with this project. Fixed it by install 3.7 and running the setup on that. What version is your python?,similar issue trying use python project fixed install running setup version python,issue,negative,positive,neutral,neutral,positive,positive
848104882,I've installed ffmpeg and added its path to my environment variables but I still see this error. Any suggestions?,added path environment still see error,issue,negative,neutral,neutral,neutral,neutral,neutral
846647533,"You know, I just decreased the sample rate and it worked for me pretty well!!!
`sf.write(filename, generated_wav.astype(np.float32), round(synthesizer.sample_rate / 1.1))`

At least it reduced the noise =)",know sample rate worked pretty well round least reduced noise,issue,positive,negative,neutral,neutral,negative,negative
846644918,"your first command already failed... for me it was including the directory before the files are in place as it seems to not parse multi directory depth...

python synthesizer_preprocess_audio.py E:\DeepFakes\cleanRTVC --subfolders de_DE\\***by_book\male\karlsson\*** --dataset """" --no_alignments --no_trim -n 4

this did work on my machine fine

and then I ran

python synthesizer_preprocess_embeds.py E:\DeepFakes\cleanRTVC\SV2TTS\synthesizer

while here you used instead of \ also forward slash / which is for linux..

in case you still wonder, or anyone else is in trouble like me the last days",first command already directory place parse directory depth python work machine fine ran python used instead also forward slash case still wonder anyone else trouble like last day,issue,negative,positive,positive,positive,positive,positive
846577831,"> Hi, I'm trying to train synthesizer for another language with a few training data to test a pipeline. The result of mel-spectrogram and audio file while training the synthesizer looks fine, but when I try to inference , the output keep generating fixed length output around 40-50 second with noise even though the input is just a one word. I also try to train the synthesizer with english with a few training data, but the result is the same. Did anyone stuck with this kind of stuff ?

I have the same problem as you, the evaluation is hearable but when I inference that sample with the same ref and text input, but the output is terrible, How have you solved this, do I need to keep on training?",hi trying train synthesizer another language training data test pipeline result audio file training synthesizer fine try inference output keep generating fixed length output around second noise even though input one word also try train synthesizer training data result anyone stuck kind stuff problem evaluation hearable inference sample ref text input output terrible need keep training,issue,negative,positive,neutral,neutral,positive,positive
846437195,"Can I take up this issue??
Try using **pydub** for audio enhancements",take issue try audio,issue,negative,neutral,neutral,neutral,neutral,neutral
846436671,"Can I take up this issue?
Try doing **pip install matplotlib**
",take issue try pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
846289096,"I encountered the same problem when I tried to run it on CPU.
And I solved this problem by uninstalling the zip file and trained data and reinstalling.First, I moved the downloaded zip file to C: In order to switch between directories easily. Then I copied the 3 files in the pretrained model I downloaded and pasted it into the file where I will perform the whole function.Then I pasted ffmeg.exe into the same folder. Then I added matplotlib> = 3.2.2 and webrtcvad-wheels in requirements.txt and saved it.All that remains is to proceed through anaconda prompt. So I proceeded from there.conda create -n sound python == 3.7. Then activate it and install , 
=>conda install pytorch torchvision torchaudio cpuonly -c pytorch (pytorch run for conda da cpu),
I went to the directory where I will run demo_toolbox.py. I ran pip install -r requirements.txt and conda install -c conda-forge tensorflow = 1.14 respectively. After that I ran demo_toolbox.py without errors. I hope I can understand and help you",problem tried run problem zip file trained data zip file order switch easily copied model pasted file perform whole pasted folder added saved remains proceed anaconda prompt create sound python activate install install run da went directory run ran pip install install respectively ran without hope understand help,issue,negative,positive,positive,positive,positive,positive
844565198,"took me super long, but the solution is:
download any version of visual studio installer
make sure to select following extra tick: windows 10 sdk

if it still does not work, add to the systems variable PATH following path C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\bin\amd64 and add as new entry lib C:\Program Files (x86)\Windows Kits\10\Lib\10.0.18362.0\um\x64 and C:\Program Files (x86)\Windows Kits\10\Lib\10.0.18362.0\ucrt\x64 and C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\lib\amd64. Having another entry called include with C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\include and C:\Program Files (x86)\Windows Kits\10\Include\10.0.18362.0\um and C:\Program Files (x86)\Windows Kits\10\Include\10.0.18362.0\ucrt aaand C:\Program Files (x86)\Windows Kits\10\Include\10.0.18362.0\shared got it then working for me

ENJOY


< works on latest windows 10 with visual studio 2019 installer and python 3.6 for me btw >",took super long solution version visual studio installer make sure select following extra tick still work add variable path following path visual studio add new entry visual studio another entry include visual studio got working enjoy work latest visual studio installer python,issue,positive,positive,positive,positive,positive,positive
841741851,"Whenever I try to run 

pip install webrtcvad

I get an error sayin

```
ERROR: Command errored out with exit status 1:
     command: 'c:\users\robert\appdata\local\programs\python\python37\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\Robert\\AppData\\Local\\Temp\\pip-install-6sn2p345\\webrtcvad_805b21f71bdf4aaca9265e94cf666bb5\\setup.py'""'""'; __file__='""'""'C:\\Users\\Robert\\AppData\\Local\\Temp\\pip-install-6sn2p345\\webrtcvad_805b21f71bdf4aaca9265e94cf666bb5\\setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'C:\Users\Robert\AppData\Local\Temp\pip-record-drp1f51x\install-record.txt' --single-version-externally-managed --compile --install-headers 'c:\users\robert\appdata\local\programs\python\python37\Include\webrtcvad'
         cwd: C:\Users\Robert\AppData\Local\Temp\pip-install-6sn2p345\webrtcvad_805b21f71bdf4aaca9265e94cf666bb5\
    Complete output (19 lines):
    running install
    running build
    running build_py
    creating build
    creating build\lib.win-amd64-3.7
    copying webrtcvad.py -> build\lib.win-amd64-3.7
    running build_ext
    building '_webrtcvad' extension
    creating build\temp.win-amd64-3.7
    creating build\temp.win-amd64-3.7\Release
    creating build\temp.win-amd64-3.7\Release\cbits
    creating build\temp.win-amd64-3.7\Release\cbits\webrtc
    creating build\temp.win-amd64-3.7\Release\cbits\webrtc\common_audio
    creating build\temp.win-amd64-3.7\Release\cbits\webrtc\common_audio\signal_processing
    creating build\temp.win-amd64-3.7\Release\cbits\webrtc\common_audio\vad
    C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.26.28801\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MT -D_WIN32 -Icbits -Ic:\users\robert\appdata\local\programs\python\python37\include -Ic:\users\robert\appdata\local\programs\python\python37\include ""-IC:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.26.28801\include"" ""-IC:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.26.28801\include"" /Tccbits\pywebrtcvad.c /Fobuild\temp.win-amd64-3.7\Release\cbits\pywebrtcvad.obj
    pywebrtcvad.c
    c:\users\robert\appdata\local\programs\python\python37\include\pyconfig.h(59): fatal error C1083: Cannot open include file: 'io.h': No such file or directory
    error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.26.28801\\bin\\HostX86\\x64\\cl.exe' failed with exit status 2
    ----------------------------------------
ERROR: Command errored out with exit status 1: 'c:\users\robert\appdata\local\programs\python\python37\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\Robert\\AppData\\Local\\Temp\\pip-install-6sn2p345\\webrtcvad_805b21f71bdf4aaca9265e94cf666bb5\\setup.py'""'""'; __file__='""'""'C:\\Users\\Robert\\AppData\\Local\\Temp\\pip-install-6sn2p345\\webrtcvad_805b21f71bdf4aaca9265e94cf666bb5\\setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'C:\Users\Robert\AppData\Local\Temp\pip-record-drp1f51x\install-record.txt' --single-version-externally-managed --compile --install-headers 'c:\users\robert\appdata\local\programs\python\python37\Include\webrtcvad' Check the logs for full command output.
```",whenever try run pip install get error error command exit status command io o open else import setup setup code compile code install record compile complete output running install running build running build running building extension visual visual visual fatal error open include file file directory error command visual exit status error command exit status io o open else import setup setup code compile code install record compile check full command output,issue,negative,positive,neutral,neutral,positive,positive
841574935,"Hey, I got this exact message! I ran 

pip install webrtcvad

and that message is gone!  However it shouldn't be needed.
",hey got exact message ran pip install message gone however,issue,negative,positive,positive,positive,positive,positive
840861619,"The largest clip is 60 seconds, ive loaded up 10 min clips with no issue in the past ",clip loaded min clip issue past,issue,negative,negative,negative,negative,negative,negative
840307401,"@GauriDhande and @thehetpandya were you guys able to generate the model for cloning Hindi sentences? Please reply.

Thanks.",able generate model please reply thanks,issue,positive,positive,positive,positive,positive,positive
839367578,"it seems like shorter phrases seem to work better too, and for some reason when i train with shorter recordings it works better. i thought if you train with more data it works better, because training is more accurate
",like shorter seem work better reason train shorter work better thought train data work better training accurate,issue,positive,positive,positive,positive,positive,positive
839354566,"5425557 is the last commit before Tensorflow support was removed in b5ba6d03
",last commit support removed,issue,positive,neutral,neutral,neutral,neutral,neutral
839237516,Also I was wondering why sometimes if i upload a file to use it will only generate a few words and then trail off into noise - is there something I am doing wrong? When I recorded my own voice I could type longer sentences and it worked fine...,also wondering sometimes file use generate trail noise something wrong voice could type longer worked fine,issue,negative,negative,neutral,neutral,negative,negative
833574356,"> Does it also happen if you change this line:
> https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/95adc699c1deb637f485e85a5107d40da0ad94fc/demo_cli.py#L46
> 
> to:
> 
> ```
> os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1"" 
> ```

Apologies for the late response. Just saw this now. I made the aforementioned change and --cpu seems to be working now. Thanks for the assistance.",also happen change line late response saw made change working thanks assistance,issue,negative,negative,neutral,neutral,negative,negative
832109490,You will need a GPU with more memory to use acceleration. Use CPU if this is not available.,need memory use acceleration use available,issue,negative,positive,positive,positive,positive,positive
832108825,"I may have misunderstood the intent of your question.

What you ask is technically quite feasible, but requires integrating the code of the new vocoder into this repo, and possibly training a new model. It has not been done before, so if you will need to do this yourself if the capability is needed. This may or may not be possible depending on your level of experience with Python and machine learning.",may misunderstood intent question ask technically quite feasible code new possibly training new model done need capability may may possible depending level experience python machine learning,issue,negative,positive,neutral,neutral,positive,positive
831992614,"> > Any progress on Spanish dataset training?
> > Algún avance en la formación del conjunto de datos español?
> 
> Same here! let me know if any news or any help for Spanish

Hey, I ended up using tacotron2 implementation by NVIDIA. If you train it in spanish, it speaks spanish; so I guess it will 
work just as good in any language https://github.com/NVIDIA/tacotron2",progress training en la de let know news help hey ended implementation train guess work good language,issue,positive,positive,positive,positive,positive,positive
831592603,"> Yes, it is possible. Both our WaveRNN vocoder and HiFi-GAN take a mel spectrogram as input.

How can I get a hifi gan model for sv2tts?",yes possible take mel spectrogram input get gan model,issue,negative,neutral,neutral,neutral,neutral,neutral
830922972,Not enough information to troubleshoot issue. Would like to understand why `torch.cuda.is_available()` is unreliable here.,enough information issue would like understand unreliable,issue,negative,neutral,neutral,neutral,neutral,neutral
830921401,"You should follow the advice here, it will point you to the line of code that is causing the error. https://github.com/pytorch/pytorch/issues/32921#issuecomment-581734069",follow advice point line code causing error,issue,negative,neutral,neutral,neutral,neutral,neutral
830921057,"Try using the toolbox in CPU mode:
```
python demo_toolbox.py --cpu
```",try toolbox mode python,issue,negative,neutral,neutral,neutral,neutral,neutral
830920945,"You will want to use a Python virtual environment (venv) to mitigate the path issues. I wrote up the Windows 10 setup instructions here: https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542 . Sorry, it doesn't have screenshots. Maybe someone will make a video of it.

",want use python virtual environment mitigate path wrote setup sorry maybe someone make video,issue,negative,negative,negative,negative,negative,negative
830920492,"Yes, it is possible. Both our WaveRNN vocoder and HiFi-GAN take a mel spectrogram as input.",yes possible take mel spectrogram input,issue,negative,neutral,neutral,neutral,neutral,neutral
830919679,This is happening because the resulting list is empty. Your audios might be too short or long.,happening resulting list empty might short long,issue,negative,negative,neutral,neutral,negative,negative
830919145,"Yes there will be because the model architectures are different. The old Tensorflow is Tacotron 2 ([1712.05884](https://arxiv.org/pdf/1712.05884.pdf)) and the new PyTorch is Tacotron 1 ([1703.10135](https://arxiv.org/pdf/1703.10135.pdf)).

Pytorch and Tensorflow use the same building blocks, so that is not the source of any difference you may notice.",yes model different old new use building source difference may notice,issue,negative,positive,neutral,neutral,positive,positive
830863361,"> Any progress on Spanish dataset training?
> Algún avance en la formación del conjunto de datos español?

Same here! let me know if any news or any help for Spanish",progress training en la de let know news help,issue,positive,neutral,neutral,neutral,neutral,neutral
826152627,"The relevant part of the error message:
```
Librosa will be unable to open mp3 files if additional software is not installed.
Please install ffmpeg or add the '--no_mp3_support' option to proceed without support for mp3 files.
```",relevant part error message unable open additional please install add option proceed without support,issue,negative,negative,neutral,neutral,negative,negative
825690463,"Does it also happen if you change this line:
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/95adc699c1deb637f485e85a5107d40da0ad94fc/demo_cli.py#L46

to:
```
os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1"" 
```",also happen change line,issue,negative,neutral,neutral,neutral,neutral,neutral
825158633,Reopen this issue if you still experience this problem after following the above setup instructions.,reopen issue still experience problem following setup,issue,negative,neutral,neutral,neutral,neutral,neutral
825156885,Reopen this issue if you've followed the above instructions and you still have this problem.,reopen issue still problem,issue,negative,neutral,neutral,neutral,neutral,neutral
825155492,See #57 . It also depends on the voice to be cloned. The speaker encoder is trained with noisy recordings from VoxCeleb so a perfectly clean recording is not needed. You'll need to do some exploration to get an answer.,see also voice speaker trained noisy perfectly clean recording need exploration get answer,issue,positive,positive,positive,positive,positive,positive
825151670,"Please try with a normal python installation, not anaconda. Instructions here for Windows. https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542",please try normal python installation anaconda,issue,negative,positive,positive,positive,positive,positive
823643634,"> I will be trying to do the same with spanish. Wish me luck. Any suggestions about compute power?

did you get around to train the model. I found these datasets in spanish (and many other languages) https://commonvoice.mozilla.org/es/datasets",trying wish luck compute power get around train model found many,issue,positive,positive,positive,positive,positive,positive
822935852,"> Is there words limit to how much i can synthesize and vocode at once?

You'll need to do some exploration to find the limits. It may also depend on your system hardware.",limit much synthesize need exploration find may also depend system hardware,issue,negative,positive,positive,positive,positive,positive
822933996,Please try following these Windows 10 setup instructions verbatim and see if you still get this issue. https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542,please try following setup verbatim see still get issue,issue,negative,neutral,neutral,neutral,neutral,neutral
822933212,What is your operating system version? You can try these instructions for Ubuntu 20.04: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/615#issuecomment-740270185,operating system version try,issue,negative,neutral,neutral,neutral,neutral,neutral
822932458,Issue author has closed the pull request. https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/720#issuecomment-822210508 ,issue author closed pull request,issue,negative,negative,neutral,neutral,negative,negative
822931320,You just let the encoder continue training until you're satisfied with the results. Then stop the training and use the last saved checkpoint. There is no need to set a target number of steps. ,let continue training satisfied stop training use last saved need set target number,issue,positive,positive,positive,positive,positive,positive
822930418,"Between the two, LibriTTS is the better dataset. It has higher sample rate (24 khz vs. 16 khz), shorter utterances, punctuation in transcripts, and removes some speakers with noisy recordings.

However, it's essentially the same data as LibriSpeech. If you want to train a better model, you'll need to curate the data, or switch to a better dataset. For reducing gaps, I provide some ideas in #501 which have been implemented in the code.",two better higher sample rate shorter punctuation noisy however essentially data want train better model need curate data switch better reducing provide code,issue,positive,positive,positive,positive,positive,positive
822925762,"You need to run `demo_cli.py` out of the directory where it exists.

For your specific case:
```
j:
cd Real-Time-Voice-Cloning-master
python demo_cli.py
```

See these instructions if you need more help: https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542",need run directory specific case python see need help,issue,negative,neutral,neutral,neutral,neutral,neutral
822497156,"Is there words limit to how much i can synthesize and vocode at once? I notice it happened because large .flac input or huge amount of words to vocode

> Try restarting your computer and running only the `demo_toolbox.py`. If that doesn't help, you might need to pass `--cpu` parameters to `demo_toolbox.py`

",limit much synthesize notice large input huge amount try computer running help might need pas,issue,positive,positive,positive,positive,positive,positive
822480399,"Try running this command `pip install torch -f https://download.pytorch.org/whl/torch_stable.html` from https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542
",try running command pip install torch,issue,negative,neutral,neutral,neutral,neutral,neutral
822473453,"Try restarting your computer and running only the `demo_toolbox.py`. If that doesn't help, you might need to pass `--cpu` parameters to `demo_toolbox.py`",try computer running help might need pas,issue,negative,neutral,neutral,neutral,neutral,neutral
822210508,"I am closing this pull request.

This work is done under [HertzAI](https://github.com/hertz-ai)

The team of HertzAI will complete this project and will be maintained. ",pull request work done team complete project,issue,negative,positive,neutral,neutral,positive,positive
822041083,@ireneu  @blue-fish  you can check [this link](https://github.com/yui-mhcp/text_to_speech) for my pretrained models and [this link](https://github.com/yui-mhcp/siamese_networks) for experiments on siamese networks for speaker verification :),check link link speaker verification,issue,negative,neutral,neutral,neutral,neutral,neutral
821254574,"I'm trying to do the same and as @blue-fish said (if I got it correct) I just need to train the synthesizer so I have to skip the first steps in https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#datasets until I reach the: </br> ""Begin with the audios and the mel spectrograms:</br>_python synthesizer_preprocess_audio.py <datasets_root>_"".</br>Is it right? If so, how have I to structure my dataset? I have downloaded the italian one from  http://www.openslr.org/94/ but I don't know if I have to preprocess it before running the instruction above (in other words I don't know what it is expected in _<datasets_root>_)
Thanks in advance ",trying said got correct need train synthesizer skip first reach begin mel right structure one know running instruction know thanks advance,issue,negative,positive,positive,positive,positive,positive
819389138,"try to run this: 
python3 -m pip install torch",try run python pip install torch,issue,negative,neutral,neutral,neutral,neutral,neutral
818603311,"Hey guys, is there anyone who used this for German male voices?",hey anyone used german male,issue,negative,neutral,neutral,neutral,neutral,neutral
818386106,"Thank you!

Cause I solved this problem, I close this issue.",thank cause problem close issue,issue,negative,neutral,neutral,neutral,neutral,neutral
816921860,Why are you using `tar` instead of `unzip`? Please direct related questions to the VoxCeleb authors.,tar instead please direct related,issue,negative,positive,neutral,neutral,positive,positive
816920566,"It is documented here: https://github.com/Rayhane-mamah/Tacotron-2/wiki/Spectrogram-Feature-prediction-network#training

Our old Tensorflow code in this repo modified the Rayhane-mamah code to add L1 loss of decoder output. That code is obsolete and no longer supported. Please file an issue in https://github.com/Rayhane-mamah/Tacotron-2 if you have more questions about the Tensorflow synthesizer.",old code code add loss output code obsolete longer please file issue synthesizer,issue,negative,positive,neutral,neutral,positive,positive
816914523,"Transfer learning is not really possible. You will need to train a new model, or finetune the pretrained model with your own audio data.",transfer learning really possible need train new model model audio data,issue,negative,positive,neutral,neutral,positive,positive
816909999,This is not supported by our repo but I have links to other repos here in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/434#issuecomment-661294839 . There may have been others added since then.,link may added since,issue,negative,neutral,neutral,neutral,neutral,neutral
816906553,"You will need pytorch>=1.1.0 for `repeat_interleave` according to https://stackoverflow.com/a/64153313

I'll leave this issue open until the README is updated. Thanks for reporting the problem.",need according leave issue open thanks problem,issue,negative,positive,neutral,neutral,positive,positive
816901953,"Thanks for reporting this bug. After I added the stop token prediction, I should have updated the GTA synthesis, which I don't normally use.

Change this line:
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/95adc699c1deb637f485e85a5107d40da0ad94fc/synthesizer/synthesize.py#L83

To:
```
_, mels_out, _, _ = model(texts, mels, embeds)
```",thanks bug added stop token prediction synthesis normally use change line model,issue,negative,positive,positive,positive,positive,positive
816898362,"See https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/400#issuecomment-653830346

You will need commit `5425557` from this repo (tensorflow 1.15) which is old and no longer supported. ",see need commit old longer,issue,negative,positive,neutral,neutral,positive,positive
816813517,"> to clarify, installing python-sounddevice fixed the issue or were there other steps involved? i have that package installed and still get the same error


I got no problem after that. 


> Solved with [python-sounddevice](https://anaconda.org/conda-forge/python-sounddevice)

",clarify fixed issue involved package still get error got problem,issue,negative,positive,neutral,neutral,positive,positive
815538096,"I am using Unix as well and cat works completely fine. I get the zipped folder but I am unable to unzip it. Anyway if I do not concatenate the files. How can I use them simply?
",well cat work completely fine get folder unable anyway concatenate use simply,issue,negative,negative,neutral,neutral,negative,negative
815377272,"i saw a python task suspended once when troubleshooting, but was not able to reproduce it. however, every time i run the process, the windows problem reporting task ramps up in cpu and power usage in task manager",saw python task suspended able reproduce however every time run process problem task power usage task manager,issue,negative,positive,positive,positive,positive,positive
815333754,"  File ""demo_toolbox.py"", line 2, in <module>
    from toolbox import Toolbox
  File ""C:\Users\dacia\Desktop\Real-Time-Voice-Cloning-master\toolbox\__init__.py"", line 1, in <module>
    from toolbox.ui import UI
  File ""C:\Users\dacia\Desktop\Real-Time-Voice-Cloning-master\toolbox\ui.py"", line 2, in <module>
    from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
  File ""C:\Users\dacia\AppData\Roaming\Python\Python37\site-packages\matplotlib\backends\backend_qt5agg.py"", line 11, in <module>
    from .backend_qt5 import (
  File ""C:\Users\dacia\AppData\Roaming\Python\Python37\site-packages\matplotlib\backends\backend_qt5.py"", line 13, in <module>
    import matplotlib.backends.qt_editor.figureoptions as figureoptions
  File ""C:\Users\dacia\AppData\Roaming\Python\Python37\site-packages\matplotlib\backends\qt_editor\figureoptions.py"", line 11, in <module>
    from matplotlib.backends.qt_compat import QtGui
  File ""C:\Users\dacia\AppData\Roaming\Python\Python37\site-packages\matplotlib\backends\qt_compat.py"", line 179, in <module>
    raise ImportError(""Failed to import any qt binding"")
ImportError: Failed to import any qt binding
PS C:\Users\dacia\Desktop\Real-Time-Voice-Cloning-master>",file line module toolbox import toolbox file line module import file line module import file line module import file line module import file line module import file line module raise import binding import binding,issue,negative,neutral,neutral,neutral,neutral,neutral
815285053,"> i also have the same problem apv96 is having. no errors are logged other than the webrtcvad one but the process doesnt start either, making it hard to troubleshoot

I think the problem is caused by the task appearing as suspended in the the taskbar, i don't know how to solve it so i can't help more.",also problem logged one process doesnt start either making hard think problem task suspended know solve ca help,issue,negative,negative,negative,negative,negative,negative
815256157,"Hey @pacifist-dev @blue-fish @apv96 ,
Sorry for my extended absence.

Perhaps the error is created by a `try`/`except` block with `pass`?",hey sorry extended absence perhaps error try except block pas,issue,negative,negative,negative,negative,negative,negative
814837731,"Any progress on Spanish dataset training?
Algún avance en la formación del conjunto de datos español?",progress training en la de,issue,negative,neutral,neutral,neutral,neutral,neutral
814699033,"to clarify, installing python-sounddevice fixed the issue or were there other steps involved? i have that package installed and still get the same error",clarify fixed issue involved package still get error,issue,negative,positive,neutral,neutral,positive,positive
814690465,"i also have the same problem apv96  is having. no errors are logged other than the webrtcvad one but the process doesnt start either, making it hard to troubleshoot",also problem logged one process doesnt start either making hard,issue,negative,negative,negative,negative,negative,negative
814007203,"Just to share my experience. 
I downloaded the file from the Google drive of VoxCeleb directly. 
You may also need to check if the ""cat"" command is available in your environment. 
As I know, this command is for Unix. 
[https://en.wikipedia.org/wiki/Cat_(Unix)](url)
If your OS does not accept this, maybe please check if there is any equivalent command in your system. ",share experience file drive directly may also need check cat command available environment know command o accept maybe please check equivalent command system,issue,positive,positive,positive,positive,positive,positive
812985801,"Those look like in-development versions of the original vocoder model released [here](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models/2cd3887f379d4921b193214973b463043efa5c23).

I [asked about it before](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/162#issuecomment-653872830) and the response is that the vocoder model is identical to what we have. Actually, the current vocoder model has been improved as briefly described in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/126#issuecomment-650531957 .",look like original model response model identical actually current model briefly,issue,positive,positive,positive,positive,positive,positive
812946572,"For me, I needed some env variables you can get from `brew info tcl-tk`

```
If you need to have tcl-tk first in your PATH run:
  echo 'export PATH=""/usr/local/opt/tcl-tk/bin:$PATH""' >> ~/.zshrc

For compilers to find tcl-tk you may need to set:
  export LDFLAGS=""-L/usr/local/opt/tcl-tk/lib""
  export CPPFLAGS=""-I/usr/local/opt/tcl-tk/include""

For pkg-config to find tcl-tk you may need to set:
  export PKG_CONFIG_PATH=""/usr/local/opt/tcl-tk/lib/pkgconfig""
```",get brew need first path run echo path find may need set export export find may need set export,issue,negative,positive,positive,positive,positive,positive
812929874,"I'll see if I can contact Shopify to get it taken down before anyone gets scammed

Edit: The license does say that it could be sold as long as the original license is included, but I'm not about to pay to find out if they kept the license",see contact get taken anyone edit license say could sold long original license included pay find kept license,issue,negative,positive,positive,positive,positive,positive
812460829,"This will be a nice contribution, keep us posted on your progress.

If you need inspiration, look at [ForwardTacotron](https://github.com/as-ideas/ForwardTacotron) as it is somewhat related to NAT and has a similar codebase as our repo. It uses [phonemizer](https://github.com/bootphon/phonemizer) for the grapheme to phoneme conversion.",nice contribution keep u posted progress need inspiration look somewhat related nat similar phoneme conversion,issue,positive,positive,positive,positive,positive,positive
812458493,"> How can I use sv2tts with directml ?
> I have an amd gpu with 8gb vram so I want it run the SV2TTS with directml

Just run it on CPU using Pytorch CPU",use want run run,issue,negative,neutral,neutral,neutral,neutral,neutral
811561114,"I think this is a problem with matplotlib or pyqt5, not really with our code. Try some of these:
https://stackoverflow.com/a/32078408 (if you are running from IDLE and not the terminal)
https://github.com/MTG/sms-tools/issues/36#issuecomment-296493101
https://github.com/matplotlib/matplotlib/issues/13414#issuecomment-490721594

If you figure it out please let us know what solved it.
",think problem really code try running idle terminal figure please let u know,issue,negative,positive,positive,positive,positive,positive
811558471,"Thanks for the suggestion @ranshaa05 . However, I'm not going to submit a pull request this unless more people ask for it. I hope the feature is working well for you.",thanks suggestion however going submit pull request unless people ask hope feature working well,issue,positive,positive,positive,positive,positive,positive
811557947,"Closing due to lack of developer interest at this time.
Please comment and reopen if you would like to work on this.",due lack developer interest time please comment reopen would like work,issue,positive,negative,negative,negative,negative,negative
811557120,Thanks for submitting the PR @ankitdobhal . Not going to merge because these are untested changes. We'll pick up some of these code improvements the next time we have a reason to update these files.,thanks going merge untested pick code next time reason update,issue,negative,positive,neutral,neutral,positive,positive
811552830,"@RAVANv2 We do not provide support for colab, it can be done but you'll have to figure it out on your own.",provide support done figure,issue,negative,neutral,neutral,neutral,neutral,neutral
811551572,"@MGSousa @StElysse Thanks for your observations in #711 which confirm the issue. Reopening this, no idea where to start since I don't have Windows to test.",thanks confirm issue idea start since test,issue,negative,positive,positive,positive,positive,positive
811550725,"If your source audio is low quality, that may be the problem. Do you have this problem if you use the audio files bundled with the repo in the `samples` folder?",source audio low quality may problem problem use audio folder,issue,negative,neutral,neutral,neutral,neutral,neutral
811545924,"You need to switch away from the ""main"" visdom environment to the one that is started when the training run is launched.

Visdom only works for encoder training. We do not make visualizations for the synthesizer or vocoder.",need switch away main environment one training run work training make synthesizer,issue,negative,positive,positive,positive,positive,positive
811541680,"```
python demo_toolbox.py -d <datasets_root>
```

Replace `<datasets_root>` with the path to the directory containing the dataset.",python replace path directory,issue,negative,neutral,neutral,neutral,neutral,neutral
811090996,@blue-fish I would like to retrain all models. Is there any problem if I use google colab GPU for training purpose. Is it sufficient for training?,would like retrain problem use training purpose sufficient training,issue,negative,neutral,neutral,neutral,neutral,neutral
811035109,@ireneu  you can find the data processing code I used at this [link](https://github.com/yui-mhcp/data_processing) ,find data code used link,issue,negative,neutral,neutral,neutral,neutral,neutral
810957393,"I am trying to use the new Pytorch version repo to do the same thing. 
I would close this issue until I have the result from new repo. 
Hope the new repo could give better audio. ",trying use new version thing would close issue result new hope new could give better audio,issue,positive,positive,positive,positive,positive,positive
808955657,"I've got the same problem as @StElysse even without Anaconda, using Python v3.8.
With RTX 2080, only 0.42 steps per second...",got problem even without anaconda python per second,issue,negative,neutral,neutral,neutral,neutral,neutral
807957197,"Check your numba version by running below
`pip show numba`

I had a older version of numba, Solved it by installing 0.53.0
`pip install numba==0.53.0`",check version running pip show older version pip install,issue,negative,positive,positive,positive,positive,positive
806557894,"Hi @ireneu  and thank you for your interest in my work !
Sorry but in fact I did not make a fork but implemented all models in a personal project code and the model was inspired from the NVIDIA’s implementation so it is not really compatible with the implementation of this repo... 

However, during the summer holidays I will create a personal repository with some of my codes, models and weights so that you will be able to use, retraind and improve them as you want ! :)
Maybe if I have time (and motivation lol) will try to create the git during easter holidays but so many course projects it’s time consuming haha :’)

As a note, my cloning model is quite poor in similarity (due to the poor quality of Fr TTS multi-speaker datasets) but it is possible to fine-tune it with around 10-20min on a single-voice and result is really impressive ! :)
Till now I used CommonVoice, VoxForge and SIWIS dataset but if you know other datasets it can be helpfull too (I also tried to use MLS but audios of more than 10sec is too big for my 6Go-memory GPU)

Have a nice week and will notify you when my git is created !",hi thank interest work sorry fact make fork personal project code model inspired implementation really compatible implementation however summer create personal repository able use improve want maybe time motivation try create git easter many course time consuming note model quite poor similarity due poor quality possible around result really impressive till used know also tried use sec big nice week notify git,issue,positive,positive,neutral,neutral,positive,positive
806550601,"Hi @Ananas120, looks like you've made a lot of progress with the french models. Would you be willing to share your models + fork? I'd love to play around with it and see if I can take it further (rather than redoing your work 😄 )",hi ananas like made lot progress would willing share fork love play around see take rather work,issue,positive,positive,positive,positive,positive,positive
806311086,"The output does not contain ""Anaconda"". 

I also clearly recall installing Python from python.org. The Anaconda.com site is completely new to me.",output contain anaconda also clearly recall python site completely new,issue,negative,positive,positive,positive,positive,positive
806290418,"It's a matter of how you installed Python. The download from python.org provides a normal installation. Another way to install Python is through Anaconda (anaconda.com). Some users experience problems when using an Anaconda-based Python with this repo, which is why I ask.

To find out, type this in the command prompt and see if the output contains ""Anaconda"".
```
python -c ""import sys; print sys.version""
```",matter python normal installation another way install python anaconda experience python ask find type command prompt see output anaconda python import print,issue,negative,positive,positive,positive,positive,positive
806268560,"I'm not sure if I am using anaconda or not. How would I find this out? If so, how do I disable it/run the program without anaconda?",sure anaconda would find disable program without anaconda,issue,negative,positive,positive,positive,positive,positive
806207969,Will reopen issue if slow training speed is confirmed with a normal Python installation.,reopen issue slow training speed confirmed normal python installation,issue,negative,positive,neutral,neutral,positive,positive
806207433,"Try without Anaconda and see if the training speed improves.

Although this repo works with Anaconda, we don't have enough developer interest to support it.",try without anaconda see training speed although work anaconda enough developer interest support,issue,positive,neutral,neutral,neutral,neutral,neutral
806205730,"For a more precise display of training step, change this line
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/95adc699c1deb637f485e85a5107d40da0ad94fc/vocoder/train.py#L121
to:
```
f""steps/s | Step: {step} | ""
```",precise display training step change line step step,issue,negative,positive,positive,positive,positive,positive
806204325,"@StElysse Slow training on Windows is also reported in #700. Are you also using anaconda?

@chankl3579 We provide a pretrained model for pytorch. Instructions for getting it are in the readme.",slow training also also anaconda provide model getting,issue,negative,negative,negative,negative,negative,negative
806181849,"> `RuntimeWarning: invalid value encountered in true_divide`

Check for audio files that are completely silent.",invalid value check audio completely silent,issue,negative,neutral,neutral,neutral,neutral,neutral
806095384,"Also, I want to share for those who have similar problems:

- ` Can't pickle <class 'Memory Error'>: it's not the same object as builtins. MemoryError`  when running `synthesizer_preprocess_audio.py`  is solved using `--n_processes 1`
- Instead of the `webrtcvad` library, I use `webrtcvad-wheels` on Windows10",also want share similar ca pickle class error object running instead library use,issue,negative,neutral,neutral,neutral,neutral,neutral
806092109,"What can be related to
```
RuntimeWarning: invalid value encountered in true_divide
wav = wav / np. abs(wav). max () * params.rescaling_max 
```
when running the script `synthesizer_preprocess_audio.py` ?",related invalid value running script,issue,negative,neutral,neutral,neutral,neutral,neutral
805966227,"I am still using the old Tensorflow repo. 
Does it mean that if I change to the PyTorch version, all saved models, including pretrained models, are no longer usable? 
Also if I want to do the single speaker finetune in #437 , do I need to train new ""pretrained models"" so that I can use them as  a starting point of finetune? ",still old mean change version saved longer usable also want single speaker need train new use starting point,issue,negative,negative,neutral,neutral,negative,negative
805899667,"Thank you again for the insight.

I got the new repo to work with the num_workers=0 workaround from #669, because I was encountering the 'pickle' error. 

My step time seems to hover around 0.3-4 steps per second. Do you know what steps I could take to improve my training time other than reducing `max_mel_frames` for preprocessing? The sounds in my dataset range from 0-11 seconds, and I would like to see if it is possible to keep the longer sounds while increasing my training time. 

I have an i7-7700 CPU, and a GTX 1080 GPU, if that is helpful.",thank insight got new work error step time hover around per second know could take improve training time reducing range would like see possible keep longer increasing training time helpful,issue,positive,positive,neutral,neutral,positive,positive
805503601,"> Also, are my old tensorflow models in anyway compatible with the new repo? If so, how can I do that? If not, will I have to train my models again?

Realistically speaking, you will need to start over. The TF taco2 and the PT taco1 model architectures are far too different to transfer pretrained weights.

For completeness, I will add that with knowledge of ML and a lot of patience, it is possible to write an equivalent PT taco2 model in such a way that allows conversion of pretrained weights between the old and new repos.

> And also, there's a line of code that indicates the lack of an ability to train saved models?

You can resume training on saved models with the new repo. If the specified model name exists, it will automatically continue the training. `--force_restart` is used to overwrite existing models and start from scratch.",also old anyway compatible new train realistically speaking need start model far different transfer completeness add knowledge lot patience possible write equivalent model way conversion old new also line code lack ability train saved resume training saved new model name automatically continue training used overwrite start scratch,issue,positive,positive,neutral,neutral,positive,positive
805374178,"@Daikath thanks for help, i found the python task suspended in the task manager but i'm not sure how to change its state. Luckily i did the setup on my secondary laptop and it works perfectly. I will try to fix the task problem to execute it on my main pc.
",thanks help found python task suspended task manager sure change state luckily setup secondary work perfectly try fix task problem execute main,issue,positive,positive,positive,positive,positive,positive
805237874,"@apv96 A problem I had. Was that it opened the toolbox. But that windows didnt select it so I didn't see it.

After clicking on it on the taskbar I got to work with it.

It may be a very simple thing. But if I missed it. Maybe it could be it.",problem toolbox didnt select see got work may simple thing maybe could,issue,negative,neutral,neutral,neutral,neutral,neutral
805233695,"> @apv96 The webrtcvad message is a warning, not an error. The toolbox is able to work without it. Installing webrtcvad on Windows can be difficult so we make it optional.
> 
> If a toolbox window is not appearing, then that's caused by something else.

Thanks a lot, I thought the toolbox wasnt appearing cause of that warning. It must be something else wrong but I don't get any error message. Sorry for asking something that was already explained and thanks again",message warning error toolbox able work without difficult make optional toolbox window something else thanks lot thought toolbox wasnt cause warning must something else wrong get error message sorry something already thanks,issue,negative,negative,negative,negative,negative,negative
805053877,"> @apv96 The webrtcvad message is a warning, not an error. The toolbox is able to work without it. Installing webrtcvad on Windows can be difficult so we make it optional.
> 
> If a toolbox window is not appearing, then that's caused by something else.

Why do programmers keep smoking despite all the health warnings on the packaging??

They are warnings. Not errors.",message warning error toolbox able work without difficult make optional toolbox window something else keep smoking despite health,issue,negative,neutral,neutral,neutral,neutral,neutral
805050465,"@apv96 The webrtcvad message is a warning, not an error. The toolbox is able to work without it. Installing webrtcvad on Windows can be difficult so we make it optional.

If a toolbox window is not appearing, then that's caused by something else. ",message warning error toolbox able work without difficult make optional toolbox window something else,issue,negative,neutral,neutral,neutral,neutral,neutral
805048806,"Make a dataset where the original voice and cloned voice are treated as different speakers. Then train the encoder on this dataset.

[Resemblyzer](https://github.com/resemble-ai/Resemblyzer) has code for evaluating similarity (demo01_similarity.py) if you need it.",make original voice voice different train code similarity need,issue,negative,positive,positive,positive,positive,positive
805039079,"I remember getting the same error, and mentioned it in #647. I think `webrtcvad` only works with Python 3.7 (64 bit), but I could be wrong",remember getting error think work python bit could wrong,issue,negative,negative,negative,negative,negative,negative
805035434,"> Good afternoon, i followed the instructiona in #642 to setup in windows 10 but i still getting the following error:
> 
> UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended.
> 
> I would like to know if someone has been able to solve this problem and how.
> Thanks in advance

@blue-fish

Is it normal to get this error using the new instructions? any guess on what could be wrong?",good afternoon setup still getting following error unable import package noise removal would like know someone able solve problem thanks advance normal get error new guess could wrong,issue,negative,positive,neutral,neutral,positive,positive
803654571,"@blue-fish 

I've got it working, much thanks putting in the extra time to help me.

I did not have the pretrained.pt file in my original pretrained.zip folder,, but luckily I was able to find out one thing for myself and managed to find an updated one.

",got working much thanks extra time help file original folder luckily able find one thing find one,issue,positive,positive,positive,positive,positive,positive
803648275,@Daikath You accidentally switched the encoder and vocoder models. The information here will help resolve your problem: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/692#issuecomment-792226190,accidentally switched information help resolve problem,issue,negative,neutral,neutral,neutral,neutral,neutral
803642283,"@blue-fish  I didn't notice the 2nd window opening, I got the toolbox to open just fine,, but when I try to load in a sample mp3 to emulate,,, I get a long list of warnings..

I tried looking through this thread, but didn't see these errros in other posts. I really just hope it wasn't me doing something stupid... You have any idea what could be causing this??

`Microsoft Windows [Version 6.1.7601]
Copyright (c) 2009 Microsoft Corporation.  All rights reserved.

C:\Windows\system32>cd C:\Real-Time-Voice-Cloning-master

C:\Real-Time-Voice-Cloning-master>venv\Scripts\activate.bat

(venv) C:\Real-Time-Voice-Cloning-master>python demo_toolbox.py
C:\Real-Time-Voice-Cloning-master\encoder\audio.py:13: UserWarning: Unable to im
port 'webrtcvad'. This package enables noise removal and is recommended.
  warn(""Unable to import 'webrtcvad'. This package enables noise removal and is
recommended."")
Arguments:
    datasets_root:    None
    enc_models_dir:   encoder\saved_models
    syn_models_dir:   synthesizer\saved_models
    voc_models_dir:   vocoder\saved_models
    cpu:              False
    seed:             None
    no_mp3_support:   False

Warning: you did not pass a root directory for datasets as argument.
The recognized datasets are:
        LibriSpeech/dev-clean
        LibriSpeech/dev-other
        LibriSpeech/test-clean
        LibriSpeech/test-other
        LibriSpeech/train-clean-100
        LibriSpeech/train-clean-360
        LibriSpeech/train-other-500
        LibriTTS/dev-clean
        LibriTTS/dev-other
        LibriTTS/test-clean
        LibriTTS/test-other
        LibriTTS/train-clean-100
        LibriTTS/train-clean-360
        LibriTTS/train-other-500
        LJSpeech-1.1
        VoxCeleb1/wav
        VoxCeleb1/test_wav
        VoxCeleb2/dev/aac
        VoxCeleb2/test/aac
        VCTK-Corpus/wav48
Feel free to add your own. You can still use the toolbox by recording samples yo
urself.
Traceback (most recent call last):
  File ""C:\Real-Time-Voice-Cloning-master\toolbox\__init__.py"", line 180, in rec
ord
    self.add_real_utterance(wav, name, speaker_name)
  File ""C:\Real-Time-Voice-Cloning-master\toolbox\__init__.py"", line 189, in add
_real_utterance
    self.init_encoder()
  File ""C:\Real-Time-Voice-Cloning-master\toolbox\__init__.py"", line 329, in ini
t_encoder
    encoder.load_model(model_fpath)
  File ""C:\Real-Time-Voice-Cloning-master\encoder\inference.py"", line 34, in loa
d_model
    _model.load_state_dict(checkpoint[""model_state""])
  File ""C:\Real-Time-Voice-Cloning-master\venv\lib\site-packages\torch\nn\module
s\module.py"", line 1224, in load_state_dict
    self.__class__.__name__, ""\n\t"".join(error_msgs)))
RuntimeError: Error(s) in loading state_dict for SpeakerEncoder:
        Missing key(s) in state_dict: ""similarity_weight"", ""similarity_bias"", ""l
stm.weight_ih_l0"", ""lstm.weight_hh_l0"", ""lstm.bias_ih_l0"", ""lstm.bias_hh_l0"", ""l
stm.weight_ih_l1"", ""lstm.weight_hh_l1"", ""lstm.bias_ih_l1"", ""lstm.bias_hh_l1"", ""l
stm.weight_ih_l2"", ""lstm.weight_hh_l2"", ""lstm.bias_ih_l2"", ""lstm.bias_hh_l2"", ""l
inear.weight"", ""linear.bias"".
        Unexpected key(s) in state_dict: ""step"", ""upsample.resnet.conv_in.weight
"", ""upsample.resnet.batch_norm.weight"", ""upsample.resnet.batch_norm.bias"", ""upsa
mple.resnet.batch_norm.running_mean"", ""upsample.resnet.batch_norm.running_var"",
""upsample.resnet.batch_norm.num_batches_tracked"", ""upsample.resnet.layers.0.conv
1.weight"", ""upsample.resnet.layers.0.conv2.weight"", ""upsample.resnet.layers.0.ba
tch_norm1.weight"", ""upsample.resnet.layers.0.batch_norm1.bias"", ""upsample.resnet
.layers.0.batch_norm1.running_mean"", ""upsample.resnet.layers.0.batch_norm1.runni
ng_var"", ""upsample.resnet.layers.0.batch_norm1.num_batches_tracked"", ""upsample.r
esnet.layers.0.batch_norm2.weight"", ""upsample.resnet.layers.0.batch_norm2.bias"",
 ""upsample.resnet.layers.0.batch_norm2.running_mean"", ""upsample.resnet.layers.0.
batch_norm2.running_var"", ""upsample.resnet.layers.0.batch_norm2.num_batches_trac
ked"", ""upsample.resnet.layers.1.conv1.weight"", ""upsample.resnet.layers.1.conv2.w
eight"", ""upsample.resnet.layers.1.batch_norm1.weight"", ""upsample.resnet.layers.1
.batch_norm1.bias"", ""upsample.resnet.layers.1.batch_norm1.running_mean"", ""upsamp
le.resnet.layers.1.batch_norm1.running_var"", ""upsample.resnet.layers.1.batch_nor
m1.num_batches_tracked"", ""upsample.resnet.layers.1.batch_norm2.weight"", ""upsampl
e.resnet.layers.1.batch_norm2.bias"", ""upsample.resnet.layers.1.batch_norm2.runni
ng_mean"", ""upsample.resnet.layers.1.batch_norm2.running_var"", ""upsample.resnet.l
ayers.1.batch_norm2.num_batches_tracked"", ""upsample.resnet.layers.2.conv1.weight
"", ""upsample.resnet.layers.2.conv2.weight"", ""upsample.resnet.layers.2.batch_norm
1.weight"", ""upsample.resnet.layers.2.batch_norm1.bias"", ""upsample.resnet.layers.
2.batch_norm1.running_mean"", ""upsample.resnet.layers.2.batch_norm1.running_var"",
 ""upsample.resnet.layers.2.batch_norm1.num_batches_tracked"", ""upsample.resnet.la
yers.2.batch_norm2.weight"", ""upsample.resnet.layers.2.batch_norm2.bias"", ""upsamp
le.resnet.layers.2.batch_norm2.running_mean"", ""upsample.resnet.layers.2.batch_no
rm2.running_var"", ""upsample.resnet.layers.2.batch_norm2.num_batches_tracked"", ""u
psample.resnet.layers.3.conv1.weight"", ""upsample.resnet.layers.3.conv2.weight"",
""upsample.resnet.layers.3.batch_norm1.weight"", ""upsample.resnet.layers.3.batch_n
orm1.bias"", ""upsample.resnet.layers.3.batch_norm1.running_mean"", ""upsample.resne
t.layers.3.batch_norm1.running_var"", ""upsample.resnet.layers.3.batch_norm1.num_b
atches_tracked"", ""upsample.resnet.layers.3.batch_norm2.weight"", ""upsample.resnet
.layers.3.batch_norm2.bias"", ""upsample.resnet.layers.3.batch_norm2.running_mean""
, ""upsample.resnet.layers.3.batch_norm2.running_var"", ""upsample.resnet.layers.3.
batch_norm2.num_batches_tracked"", ""upsample.resnet.layers.4.conv1.weight"", ""upsa
mple.resnet.layers.4.conv2.weight"", ""upsample.resnet.layers.4.batch_norm1.weight
"", ""upsample.resnet.layers.4.batch_norm1.bias"", ""upsample.resnet.layers.4.batch_
norm1.running_mean"", ""upsample.resnet.layers.4.batch_norm1.running_var"", ""upsamp
le.resnet.layers.4.batch_norm1.num_batches_tracked"", ""upsample.resnet.layers.4.b
atch_norm2.weight"", ""upsample.resnet.layers.4.batch_norm2.bias"", ""upsample.resne
t.layers.4.batch_norm2.running_mean"", ""upsample.resnet.layers.4.batch_norm2.runn
ing_var"", ""upsample.resnet.layers.4.batch_norm2.num_batches_tracked"", ""upsample.
resnet.layers.5.conv1.weight"", ""upsample.resnet.layers.5.conv2.weight"", ""upsampl
e.resnet.layers.5.batch_norm1.weight"", ""upsample.resnet.layers.5.batch_norm1.bia
s"", ""upsample.resnet.layers.5.batch_norm1.running_mean"", ""upsample.resnet.layers
.5.batch_norm1.running_var"", ""upsample.resnet.layers.5.batch_norm1.num_batches_t
racked"", ""upsample.resnet.layers.5.batch_norm2.weight"", ""upsample.resnet.layers.
5.batch_norm2.bias"", ""upsample.resnet.layers.5.batch_norm2.running_mean"", ""upsam
ple.resnet.layers.5.batch_norm2.running_var"", ""upsample.resnet.layers.5.batch_no
rm2.num_batches_tracked"", ""upsample.resnet.layers.6.conv1.weight"", ""upsample.res
net.layers.6.conv2.weight"", ""upsample.resnet.layers.6.batch_norm1.weight"", ""upsa
mple.resnet.layers.6.batch_norm1.bias"", ""upsample.resnet.layers.6.batch_norm1.ru
nning_mean"", ""upsample.resnet.layers.6.batch_norm1.running_var"", ""upsample.resne
t.layers.6.batch_norm1.num_batches_tracked"", ""upsample.resnet.layers.6.batch_nor
m2.weight"", ""upsample.resnet.layers.6.batch_norm2.bias"", ""upsample.resnet.layers
.6.batch_norm2.running_mean"", ""upsample.resnet.layers.6.batch_norm2.running_var""
, ""upsample.resnet.layers.6.batch_norm2.num_batches_tracked"", ""upsample.resnet.l
ayers.7.conv1.weight"", ""upsample.resnet.layers.7.conv2.weight"", ""upsample.resnet
.layers.7.batch_norm1.weight"", ""upsample.resnet.layers.7.batch_norm1.bias"", ""ups
ample.resnet.layers.7.batch_norm1.running_mean"", ""upsample.resnet.layers.7.batch
_norm1.running_var"", ""upsample.resnet.layers.7.batch_norm1.num_batches_tracked"",
 ""upsample.resnet.layers.7.batch_norm2.weight"", ""upsample.resnet.layers.7.batch_
norm2.bias"", ""upsample.resnet.layers.7.batch_norm2.running_mean"", ""upsample.resn
et.layers.7.batch_norm2.running_var"", ""upsample.resnet.layers.7.batch_norm2.num_
batches_tracked"", ""upsample.resnet.layers.8.conv1.weight"", ""upsample.resnet.laye
rs.8.conv2.weight"", ""upsample.resnet.layers.8.batch_norm1.weight"", ""upsample.res
net.layers.8.batch_norm1.bias"", ""upsample.resnet.layers.8.batch_norm1.running_me
an"", ""upsample.resnet.layers.8.batch_norm1.running_var"", ""upsample.resnet.layers
.8.batch_norm1.num_batches_tracked"", ""upsample.resnet.layers.8.batch_norm2.weigh
t"", ""upsample.resnet.layers.8.batch_norm2.bias"", ""upsample.resnet.layers.8.batch
_norm2.running_mean"", ""upsample.resnet.layers.8.batch_norm2.running_var"", ""upsam
ple.resnet.layers.8.batch_norm2.num_batches_tracked"", ""upsample.resnet.layers.9.
conv1.weight"", ""upsample.resnet.layers.9.conv2.weight"", ""upsample.resnet.layers.
9.batch_norm1.weight"", ""upsample.resnet.layers.9.batch_norm1.bias"", ""upsample.re
snet.layers.9.batch_norm1.running_mean"", ""upsample.resnet.layers.9.batch_norm1.r
unning_var"", ""upsample.resnet.layers.9.batch_norm1.num_batches_tracked"", ""upsamp
le.resnet.layers.9.batch_norm2.weight"", ""upsample.resnet.layers.9.batch_norm2.bi
as"", ""upsample.resnet.layers.9.batch_norm2.running_mean"", ""upsample.resnet.layer
s.9.batch_norm2.running_var"", ""upsample.resnet.layers.9.batch_norm2.num_batches_
tracked"", ""upsample.resnet.conv_out.weight"", ""upsample.resnet.conv_out.bias"", ""u
psample.up_layers.1.weight"", ""upsample.up_layers.3.weight"", ""upsample.up_layers.
5.weight"", ""I.weight"", ""I.bias"", ""rnn1.weight_ih_l0"", ""rnn1.weight_hh_l0"", ""rnn1
.bias_ih_l0"", ""rnn1.bias_hh_l0"", ""rnn2.weight_ih_l0"", ""rnn2.weight_hh_l0"", ""rnn2
.bias_ih_l0"", ""rnn2.bias_hh_l0"", ""fc1.weight"", ""fc1.bias"", ""fc2.weight"", ""fc2.bi
as"", ""fc3.weight"", ""fc3.bias"".
`",notice window opening got toolbox open fine try load sample emulate get long list tried looking thread see really hope something stupid idea could causing version copyright corporation reserved python unable port package noise removal warn unable import package noise removal none false seed none false warning pas root directory argument feel free add still use toolbox recording yo recent call last file line name file line add file line file line loa file line error loading missing key unexpected key step ba ked eight batch batch tracked weight weight,issue,negative,negative,negative,negative,negative,negative
803559869,"Much thanks for the speedy answer!

Now it stays stuck at one point.. but thats likely my old computer. I'll just leave it running a while.

The help with the instructions and followup help is greatly appreciated.",much thanks speedy answer stay stuck one point thats likely old computer leave running help help greatly,issue,positive,positive,positive,positive,positive,positive
803489619,"> In this line, change `0.15` to the desired number of seconds to add between line breaks.
> https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/95adc699c1deb637f485e85a5107d40da0ad94fc/toolbox/__init__.py#L272

You are a star man! thank you!!! :)",line change desired number add line star man thank,issue,positive,neutral,neutral,neutral,neutral,neutral
803477655,"In this line, change `0.15` to the desired number of seconds to add between line breaks.
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/95adc699c1deb637f485e85a5107d40da0ad94fc/toolbox/__init__.py#L272",line change desired number add line,issue,negative,neutral,neutral,neutral,neutral,neutral
803432314,"@Daikath Thank you for reporting the issue. There was a typo in the instructions, which I have corrected. The location of the synthesizer checkpoint should be:
```
C:\Real-Time-Voice-Cloning-master\synthesizer\saved_models\pretrained\pretrained.pt
```",thank issue typo corrected location synthesizer,issue,negative,neutral,neutral,neutral,neutral,neutral
803298244,"Hey, much thanks for this... I  followed [this](https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542) for Windows 7, Everything seemed to work fine,, the tutorial worked right till the end... Then I saw this.. Am I missing anything obvious? Help will be greatly appreciated. 

```
(venv) C:\Real-Time-Voice-Cloning-master>python demo_toolbox.py
C:\Real-Time-Voice-Cloning-master\encoder\audio.py:13: UserWarning: Unable to im
port 'webrtcvad'. This package enables noise removal and is recommended.
  warn(""Unable to import 'webrtcvad'. This package enables noise removal and is
recommended."")
Arguments:
    datasets_root:    None
    enc_models_dir:   encoder\saved_models
    syn_models_dir:   synthesizer\saved_models
    voc_models_dir:   vocoder\saved_models
    cpu:              False
    seed:             None
    no_mp3_support:   False

Warning: you did not pass a root directory for datasets as argument.
The recognized datasets are:
        LibriSpeech/dev-clean
        LibriSpeech/dev-other
        LibriSpeech/test-clean
        LibriSpeech/test-other
        LibriSpeech/train-clean-100
        LibriSpeech/train-clean-360
        LibriSpeech/train-other-500
        LibriTTS/dev-clean
        LibriTTS/dev-other
        LibriTTS/test-clean
        LibriTTS/test-other
        LibriTTS/train-clean-100
        LibriTTS/train-clean-360
        LibriTTS/train-other-500
        LJSpeech-1.1
        VoxCeleb1/wav
        VoxCeleb1/test_wav
        VoxCeleb2/dev/aac
        VoxCeleb2/test/aac
        VCTK-Corpus/wav48
Feel free to add your own. You can still use the toolbox by recording samples yo
urself.
Traceback (most recent call last):
  File ""demo_toolbox.py"", line 43, in <module>
    Toolbox(**vars(args))
  File ""C:\Real-Time-Voice-Cloning-master\toolbox\__init__.py"", line 72, in __in
it__
    self.reset_ui(enc_models_dir, syn_models_dir, voc_models_dir, seed)
  File ""C:\Real-Time-Voice-Cloning-master\toolbox\__init__.py"", line 140, in res
et_ui
    self.ui.populate_models(encoder_models_dir, synthesizer_models_dir, vocoder_
models_dir)
  File ""C:\Real-Time-Voice-Cloning-master\toolbox\ui.py"", line 348, in populate_
models
    raise Exception(""No synthesizer models found in %s"" % synthesizer_models_dir
)
Exception: No synthesizer models found in synthesizer\saved_models

(venv) C:\Real-Time-Voice-Cloning-master>
```",hey much thanks everything work fine tutorial worked right till end saw missing anything obvious help greatly python unable port package noise removal warn unable import package noise removal none false seed none false warning pas root directory argument feel free add still use toolbox recording yo recent call last file line module toolbox file line seed file line file line raise exception synthesizer found exception synthesizer found,issue,positive,positive,neutral,neutral,positive,positive
801561661,"We do not support Google Colab. Try asking your question on another platform, like Stack Overflow.",support try question another platform like stack overflow,issue,positive,neutral,neutral,neutral,neutral,neutral
801561432,"See this page for details on training, including datasets used.
https://blue-fish.github.io/experiments/RTVC-7.html",see page training used,issue,negative,neutral,neutral,neutral,neutral,neutral
801560681,"Advice
* I do not recommend adding English, but it is something you can try if you need a model that works for both languages.
* Train a new synthesizer model. Don't forget to edit `synthesizer/utils/symbols.py` to include all the letters of the Russian alphabet. Here is a good start for Russian: [`symbols.py`](https://github.com/vlomme/Multi-Tacotron-Voice-Cloning/blob/master/synthesizer/utils/symbols.py)
* Realistically, CPU is too slow. The model needs to learn attention before inference will work. This usually requires 10,000 to 20,000 steps. The training speed on CPU is anywhere from 1 to 4 steps per **minute**. So you will be waiting 1 to 2 weeks until you know whether your settings are correct. Even after attention is learned, you will be waiting another month or longer to train the 100,000 to 200,000 steps that it takes for the model to become usable.

If you do not have access to a GPU, try to set up this repo, which has a Russian pretrained model. Note: It uses tensorflow and you will need to apply the synthesizer changes in #366 to make it work on CPU. https://github.com/vlomme/Multi-Tacotron-Voice-Cloning

",advice recommend something try need model work train new synthesizer model forget edit include alphabet good start realistically slow model need learn attention inference work usually training speed anywhere per minute waiting know whether correct even attention learned waiting another month longer train model become usable access try set model note need apply synthesizer make work,issue,positive,positive,neutral,neutral,positive,positive
801163530,"@arianaglande I'm also looking for it. If you managed to do that, it would be very helpful sharing that with us. Thanks",also looking would helpful u thanks,issue,positive,positive,positive,positive,positive,positive
800765776,"@ErikaN322 The code now uses pytorch instead of tensorflow. Thus, you should not be seeing that error again. A suggestion is to clone this repo again and then start with the demo_cli.py using the new pretrained models.",code instead thus seeing error suggestion clone start new,issue,negative,positive,positive,positive,positive,positive
799471528,"  @arianaglande 
hello, i am looking  for  italian models.  let me know if i can help  to train the model.  i have a rtx2070  gpu.",hello looking let know help train model,issue,negative,neutral,neutral,neutral,neutral,neutral
799022619,"I'm attempting to install this on Windows 10 as well. I used Anaconda to install Pytorch and the rest of the requirements, but as far as downloading ffmpeg goes, I'm not sure where to place it for the executable to be found...

Nonetheless, the rest of the installation seemed to proceed without any issues. Upon running the demo however, I see this message:
```
C:\Users\Carvell\gitrepos\Real-Time-Voice-Cloning\encoder\audio.py:13: UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended.
  warn(""Unable to import 'webrtcvad'. This package enables noise removal and is recommended."")
Arguments:
    enc_model_fpath:   encoder\saved_models\pretrained.pt
    syn_model_fpath:   synthesizer\saved_models\pretrained\pretrained.pt
    voc_model_fpath:   vocoder\saved_models\pretrained\pretrained.pt
    cpu:               False
    no_sound:          False
    seed:              None
    no_mp3_support:    False

A s s e r t i o n   f a i l e d !

 P r o g r a m :   C : \ U s e r s \ C a r v e l l \ a n a c o n d a 3 \ e n v s \ v o i c e _ c l o n i n g \ p y t h o n . e x e
 F i l e :   s r c / h o s t a p i / w d m k s / p a _ w i n _ w d m k s . c ,   L i n e   1 0 6 1

 E x p r e s s i o n :   F A L S E
 ```
The `s p a c e d  o u t` text appears there as it does in my terminal. The python version in the conda env I'm using is 3.8.8.",install well used anaconda install rest far go sure place executable found nonetheless rest installation proceed without upon running however see message unable import package noise removal warn unable import package noise removal false false seed none false text terminal python version,issue,negative,negative,negative,negative,negative,negative
798915535,"> Let's suppose I got the Italian dataset from here (ASR one, flac) http://www.openslr.org/94/
> How am I supposed to create all the pretrained models from it (the .pt files, for vocoder, synthesizer and encoder)?

HI, can you release the Italian models you trained? How do I set it up? I want to clone voices in this language. ",let suppose got one supposed create synthesizer hi release trained set want clone language,issue,negative,neutral,neutral,neutral,neutral,neutral
798624089,"Good afternoon, i followed the instructiona in #642 to setup in windows 10 but i still getting the following error:

UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended.

I would like to know if someone has been able to solve this problem and how.
Thanks in advance",good afternoon setup still getting following error unable import package noise removal would like know someone able solve problem thanks advance,issue,positive,positive,positive,positive,positive,positive
796077998,"Thank you for your suggestions and detailed explanation; I appreciate it a lot. 

I think I'll stick to training a new vocoder from scratch; if the results aren't to my liking, I will stick with just using the improved pretrained 1159k step vocoder that you provided. I will sniff around the tacotron papers too to see where my synthesizer setup can improve.

So far, I'm training the new vocoder and the files named ""0k_steps_1_gen_batched_target8000_overlap400"" are generating what sounds like television static that matches the diction (profile? speaking?) of the target file associated with it. I'm about 60 epochs in; how can I measure my progress in a more precise way (as compared to the synthesizers' # steps), given that the 'step' counter is measured in 0k, 1k, 2k, etc.?

Thanks again!",thank detailed explanation appreciate lot think stick training new scratch liking stick step provided sniff around see synthesizer setup improve far training new generating like television static diction profile speaking target file associated measure progress precise way given counter measured thanks,issue,positive,positive,positive,positive,positive,positive
795016340,"> I figured that this 'downsampling' was part of the cause of lower audio quality of the audios I was generating; I'd like to mitigate that.

I don't think the resampling is responsible. The pretrained model is bad and that's limiting your quality.

Improving the output audio quality is not trivial. Bumping up the sample rate is not sufficient. You also need very clean datasets and some experimentation to find the optimal parameters. [Google](https://google.github.io/tacotron/publications/speaker_adaptation/index.html) has very good quality despite resampling VCTK/LibriSpeech to 16 khz. Read the Tacotron papers for hints on how it might be done.

> I'm given audio that's played super fast, like someone was pressing 'fast-forward' on a cassette tape player.

The pretrained vocoder model outputs a 16000 Hz waveform. When you change the sample rate hparam to 48000, it plays back the 16k data at 48k for a speedup of 3.0x. **To properly get the higher sample rate, you'll need to train a vocoder model from scratch.**

Here's some more detail that may be helpful for understanding this. The time resolution of the spectrogram is determined by the ""frame shift"" which is the hop size divided by sample rate. Using our defaults, hop=200 and sr=16000 gives a frame shift of (200/16000) = 0.0125 seconds. This is the amount of time that each frame represents. When we generate a spectrogram from an audio file, we run a FFT process that converts every 200 samples of the wav file into 1 spectrogram frame. This 200:1 ratio is set by the hop size. The vocoder model is the inverse of this process and converts each spectrogram frame into 200 waveform samples.

Now let's look at your numbers. After the synthesizer update, the frame shift is (hop/sample rate) = (600/48000) = 0.0125 sec , i.e. unchanged from before. You're still using the pretrained vocoder which outputs 200 samples for each spectrogram frame. So for every second of speech, your synth+vocoder combination outputs (200/0.0125) = 16000 samples. However, the playback rate is set to 48000 Hz which results in artificial speedup.

There might be something else going on with the other hparams but there's not enough detail about your setup to diagnose it.",figured part cause lower audio quality generating like mitigate think responsible model bad limiting quality improving output audio quality trivial bumping sample rate sufficient also need clean experimentation find optimal good quality despite read might done given audio super fast like someone pressing tape player model change sample rate back data properly get higher sample rate need train model scratch detail may helpful understanding time resolution spectrogram determined frame shift hop size divided sample rate frame shift amount time frame generate spectrogram audio file run process every file spectrogram frame ratio set hop size model inverse process spectrogram frame let look synthesizer update frame shift rate sec unchanged still spectrogram frame every second speech combination however playback rate set artificial might something else going enough detail setup diagnose,issue,positive,positive,neutral,neutral,positive,positive
794951719,How do I do it? I don't understand the thread. What do I have to type in command?,understand thread type command,issue,negative,neutral,neutral,neutral,neutral,neutral
794936076,"Please download the latest version of the code from this site. It no longer uses Tensorflow. We have setup instructions here: https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542

> Do I need a video card to use this?

A GPU is not required, but inference will run considerably slower.",please latest version code site longer setup need video card use inference run considerably,issue,negative,positive,positive,positive,positive,positive
794413981,"Hi @blue-fish ,

Sorry about that. I will keep that in mind for future discussions. And thank you for your help. ",hi sorry keep mind future thank help,issue,positive,negative,negative,negative,negative,negative
794203570,"@rishabhjain16 Please stay on topic. In the future, open a new issue to ask unrelated questions.

1. The synthesizer must be retrained if the encoder is changed.
2. I don't use MFA. Set up your dataset in [this format](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-666099538) and use the `--no_alignments` option for preprocessing.",please stay topic future open new issue ask unrelated synthesizer must use set format use option,issue,negative,positive,neutral,neutral,positive,positive
793649041,"Hey @blue-fish,
I have two things to ask you. 

1. Encoder, Synthesizer and Vocoder can be trained independently right? Or are they dependent on one another (in the code for training them). I needed to restart my encoder and change few of the parameters. So I was planning to train my synthesizer in parallel. I wanted to ask you if I can start this process in parallel with Encoder training. Or if running the scripts mentioned [here](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training) for synthesizer preprocessing and training linked with Encoder as well? 


2. I have collected my own dataset and I wanted to fine-tune the synthesizer for that single voice as mentioned [here](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437). I would need to generate the alignments for my dataset first before I can preprocess and train my dataset.. I have looked into **Montreal forced Aligner** for this but can't get it working right. So I wanted to take your advice for this. I am planning to use pretrained Librispeech model and fine tune that for my own dataset (around 15 minutes) and see that I get. But I am stuck with Montreal Forced Aligner. I have my audio files in a **.wav** format and text files in a **.txt** format. I have also looked at the **.Lab** format and the **.TextGrid** format as an input in Montreal Forced Aligner (MFA) as mentioned [here](https://montreal-forced-aligner.readthedocs.io/en/latest/data_prep.html) but can't really find a workaround. So any advice for me? Maybe I have the wrong format. So I need any other files apart from .wav and .txt for running the MFA? 

Example error for Montreal Forced Aligner: 

```
>mfa align mydataset/ lexicon.txt english ~/Documents/aligned

WARNING - WARNING: Some issues parsing the corpus were detected. Please run the validator to get more information.
Traceback (most recent call last):
  File ""/opt/conda/envs/aligner/bin/mfa"", line 8, in <module>
    sys.exit(main())
  File ""/opt/conda/envs/aligner/lib/python3.8/site-packages/montreal_forced_aligner/command_line/mfa.py"", line 339, in main
    run_align_corpus(args, unknown, acoustic_languages)
  File ""/opt/conda/envs/aligner/lib/python3.8/site-packages/montreal_forced_aligner/command_line/align.py"", line 159, in run_align_corpus
    align_corpus(args, unknown_args)
  File ""/opt/conda/envs/aligner/lib/python3.8/site-packages/montreal_forced_aligner/command_line/align.py"", line 85, in align_corpus
    logger.info(corpus.speaker_utterance_info())
  File ""/opt/conda/envs/aligner/lib/python3.8/site-packages/montreal_forced_aligner/corpus/base.py"", line 286, in speaker_utterance_info
    average_utterances = sum(len(x) for x in self.speak_utt_mapping.values()) / num_speakers
ZeroDivisionError: division by zero
```
I have also tried preparing the dataset in the same format as Librispeech and created a **mydataset.trans.txt** file containing all the transcription data and then run MFA but that didn't seems to be working as well. 

For reference, this is my directory structure: **mydataset->speaker_name(i.e. datarj)-> .wav_files, .txt_files** (multiple .wav and .txt files for speaker_name datarj)

Any help is appreciated. I am stuck here since last week. Thanks in advance. ",hey two ask synthesizer trained independently right dependent one another code training restart change train synthesizer parallel ask start process parallel training running synthesizer training linked well collected synthesizer single voice would need generate first train forced aligner ca get working right take advice use model fine tune around see get stuck forced aligner audio format text format also format format input forced aligner ca really find advice maybe wrong format need apart running example error forced aligner align warning warning corpus please run get information recent call last file line module main file line main unknown file line file line file line sum division zero also tried format file transcription data run working well reference directory structure multiple help stuck since last week thanks advance,issue,negative,positive,neutral,neutral,positive,positive
793509262,Thanks! Now both errors got solved... but it's really slow (the 20000 steps train command)... also idk why it says `Using device: cpu` even tho I installed the latest cuda toolkit and I got a GTX 1050 Ti...,thanks got really slow train command also device even tho latest got ti,issue,negative,positive,positive,positive,positive,positive
793089033,"I don't have time to fully troubleshoot issues, but this may help. If not, you'll need to figure it out yourself.

### Weird characters in train.txt
Problem may be coming from this line, which reads the transcripts:
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/b5ba6d0371882dbab595c48deb2ff17896547de7/synthesizer/preprocess.py#L77

Try adding utf-8 file encoding.
```
with text_fpath.open(""r"", encoding=""utf-8"") as text_file:
```


### Error running synthesizer_train.py

For a soluton to:
```
AttributeError: Can't pickle local object 'train.<locals>.<lambda>'
EOFError: Ran out of input
```

Please see https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/669#issuecomment-781130738 for a workaround. We set num_workers=0 on Windows. ",time fully may help need figure weird problem may coming line try file error running ca pickle local object lambda ran input please see set,issue,negative,negative,negative,negative,negative,negative
793047347,"> Please start by reading my advice on training. This contains the link to training documentation: [#431 (comment)](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/431#issuecomment-673555684)
> 
> If I were doing this, I would reuse the encoder and vocoder models. For the synthesizer, you have the option of training from scratch or finetuning the English model. Training from scratch should give better pronunciation and prosody. Finetuning will reduce training time and possibly have better voice similarity. If you finetune, modify the [text cleaner](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/95adc699c1deb637f485e85a5107d40da0ad94fc/synthesizer/utils/cleaners.py#L66-L70) to remove diacritics from vowels (change à to a, è and é to e, etc.). This is necessary since the English synthesizer does not include these characters in [symbols.py](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/synthesizer/utils/symbols.py).

So, I tried doing what you told me to do and everything was doing well until the synthesizer_train.py command...
Here is the execution of all the commands contained there https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training (till the train one ofc, which thrown the error)
Any idea? 🤔

I also noticed those weird symbols inside the SV2TTS/synthesizer/train.txt file...
![image](https://user-images.githubusercontent.com/26201373/110376359-616b6080-8053-11eb-9054-2e52f4bd21d3.png)
Is it normal? I tried to edit the symbols.py/cleaners.py files but doing that didn't fix it... but anyways this is probably not what's causing the crash of the train command...

```
C:\Users\Workspace\Desktop\Real-Time-Voice-Cloning>py -3.6 synthesizer_preprocess_audio.py datasets_root --datasets_name LibriTTS --subfolders testing --no_alignments
Arguments:
    datasets_root:   datasets_root
    out_dir:         datasets_root\SV2TTS\synthesizer
    n_processes:     None
    skip_existing:   False
    hparams:
    no_alignments:   True
    datasets_name:   LibriTTS
    subfolders:      testing

Using data from:
    datasets_root\LibriTTS\testing
LibriTTS: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.52s/speakers]
The dataset consists of 9 utterances, 7450 mel frames, 1488960 audio timesteps (0.03 hours).
Max input length (text chars): 140
Max mel frames length: 889
Max audio timesteps length: 177600





C:\Users\Workspace\Desktop\Real-Time-Voice-Cloning>python synthesizer_preprocess_embeds.py datasets_root/SV2TTS/synthesizer
Arguments:
    synthesizer_root:      datasets_root\SV2TTS\synthesizer
    encoder_model_fpath:   encoder\saved_models\pretrained.pt
    n_processes:           4

Embedding:   0%|                                                                         | 0/9 [00:00<?, ?utterances/s]Loaded encoder ""pretrained.pt"" trained to step 1564501
Loaded encoder ""pretrained.pt"" trained to step 1564501
Loaded encoder ""pretrained.pt"" trained to step 1564501
Loaded encoder ""pretrained.pt"" trained to step 1564501
Embedding: 100%|█████████████████████████████████████████████████████████████████| 9/9 [00:05<00:00,  1.73utterances/s]





C:\Users\Workspace\Desktop\Real-Time-Voice-Cloning>python synthesizer_train.py testing datasets_root/SV2TTS/synthesizer
Arguments:
    run_id:          testing
    syn_dir:         datasets_root/SV2TTS/synthesizer
    models_dir:      synthesizer/saved_models/
    save_every:      1000
    backup_every:    25000
    force_restart:   False
    hparams:

Checkpoint path: synthesizer\saved_models\testing\testing.pt
Loading training data from: datasets_root\SV2TTS\synthesizer\train.txt
Using model: Tacotron
Using device: cpu

Initialising Tacotron Model...

Trainable Parameters: 30.876M

Starting the training of Tacotron from scratch

Using inputs from:
        datasets_root\SV2TTS\synthesizer\train.txt
        datasets_root\SV2TTS\synthesizer\mels
        datasets_root\SV2TTS\synthesizer\embeds
Found 9 samples
+----------------+------------+---------------+------------------+
| Steps with r=2 | Batch Size | Learning Rate | Outputs/Step (r) |
+----------------+------------+---------------+------------------+
|   20k Steps    |     12     |     0.001     |        2         |
+----------------+------------+---------------+------------------+

Traceback (most recent call last):
  File ""synthesizer_train.py"", line 35, in <module>
    train(**vars(args))
  File ""C:\Users\Workspace\Desktop\Real-Time-Voice-Cloning\synthesizer\train.py"", line 158, in train
    for i, (texts, mels, embeds, idx) in enumerate(data_loader, 1):
  File ""C:\Users\Workspace\AppData\Local\Programs\Python\Python36\lib\site-packages\torch\utils\data\dataloader.py"", line 355, in __iter__
    return self._get_iterator()
  File ""C:\Users\Workspace\AppData\Local\Programs\Python\Python36\lib\site-packages\torch\utils\data\dataloader.py"", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File ""C:\Users\Workspace\AppData\Local\Programs\Python\Python36\lib\site-packages\torch\utils\data\dataloader.py"", line 914, in __init__
    w.start()
  File ""C:\Users\Workspace\AppData\Local\Programs\Python\Python36\lib\multiprocessing\process.py"", line 105, in start
    self._popen = self._Popen(self)
  File ""C:\Users\Workspace\AppData\Local\Programs\Python\Python36\lib\multiprocessing\context.py"", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File ""C:\Users\Workspace\AppData\Local\Programs\Python\Python36\lib\multiprocessing\context.py"", line 322, in _Popen
    return Popen(process_obj)
  File ""C:\Users\Workspace\AppData\Local\Programs\Python\Python36\lib\multiprocessing\popen_spawn_win32.py"", line 65, in __init__
    reduction.dump(process_obj, to_child)
  File ""C:\Users\Workspace\AppData\Local\Programs\Python\Python36\lib\multiprocessing\reduction.py"", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
AttributeError: Can't pickle local object 'train.<locals>.<lambda>'

C:\Users\Workspace\Desktop\Real-Time-Voice-Cloning>Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\Workspace\AppData\Local\Programs\Python\Python36\lib\multiprocessing\spawn.py"", line 105, in spawn_main
    exitcode = _main(fd)
  File ""C:\Users\Workspace\AppData\Local\Programs\Python\Python36\lib\multiprocessing\spawn.py"", line 115, in _main
    self = reduction.pickle.load(from_parent)
EOFError: Ran out of input
```",please start reading advice training link training documentation comment would reuse synthesizer option training scratch model training scratch give better pronunciation prosody reduce training time possibly better voice similarity modify text cleaner remove change necessary since synthesizer include tried told everything well command execution till train one thrown error idea also weird inside file image normal tried edit fix anyways probably causing crash train command testing none false true testing data mel audio input length text mel length audio length python loaded trained step loaded trained step loaded trained step loaded trained step python testing testing false path loading training data model device model trainable starting training scratch found batch size learning rate recent call last file line module train file line train enumerate file line return file line return self file line file line start self file line return file line return file line file line dump file protocol ca pickle local object lambda recent call last file string line module file line file line self ran input,issue,negative,positive,neutral,neutral,positive,positive
792646327,"Please start by reading my advice on training. This contains the link to training documentation: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/431#issuecomment-673555684

If I were doing this, I would reuse the encoder and vocoder models. For the synthesizer, you have the option of training from scratch or finetuning the English model. Training from scratch should give better pronunciation and prosody. Finetuning will reduce training time and possibly have better voice similarity. If you finetune, modify the [text cleaner](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/95adc699c1deb637f485e85a5107d40da0ad94fc/synthesizer/utils/cleaners.py#L66-L70) to remove diacritics from vowels (change à to a, è and é to e, etc.). This is necessary since the English synthesizer does not include these characters in [symbols.py](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/synthesizer/utils/symbols.py).",please start reading advice training link training documentation would reuse synthesizer option training scratch model training scratch give better pronunciation prosody reduce training time possibly better voice similarity modify text cleaner remove change necessary since synthesizer include,issue,positive,positive,positive,positive,positive,positive
792352612,"Python 3.9 is not supported. You need 3.6, 3.7 or 3.8.

Install the proper Python version, then follow steps 5-7 in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/615#issuecomment-740270185 . You should also install FFmpeg if you want to clone voices from mp3 files.",python need install proper python version follow also install want clone,issue,negative,neutral,neutral,neutral,neutral,neutral
792347121,"Update now to 3.9.2. Thank you. But a new error appear... Can you help me?

> matteo@MBP-di-matteo Real-Time-Voice-Cloning-master % python --version
> Python 3.9.2
> matteo@MBP-di-matteo Real-Time-Voice-Cloning-master % python demo_cli.py                             
> Traceback (most recent call last):
>   File ""/Users/matteo/Real-Time-Voice-Cloning-master/demo_cli.py"", line 2, in <module>
>     from utils.argutils import print_args
>   File ""/Users/matteo/Real-Time-Voice-Cloning-master/utils/argutils.py"", line 2, in <module>
>     import numpy as np
> ModuleNotFoundError: No module named 'numpy'",update thank new error appear help python version python python recent call last file line module import file line module import module,issue,negative,positive,neutral,neutral,positive,positive
792310227,"I'm sorry, but we do not support Google colab. You might want to try asking somewhere else like Stack Overflow.",sorry support might want try somewhere else like stack overflow,issue,positive,negative,negative,negative,negative,negative
792304177,"You need a newer version of Python. At this time, 3.6 to 3.8 work.",need version python time work,issue,negative,neutral,neutral,neutral,neutral,neutral
792299201,"Thanks @blue-fish , I check it out on another system to not mess up my current installation which works like a charm. Did here ever anybody popup with Brazilian Portuguese or Australian English? Sorry for all the questions, I am pretty new to this but plan to get deeper into the subject and try to add here some useful content later on. ",thanks check another system mess current installation work like charm ever anybody sorry pretty new plan get subject try add useful content later,issue,positive,positive,neutral,neutral,positive,positive
792240855,"@ranshaa05 Finally got around to this feature request. Please see my [`522_volume_bar`](https://github.com/blue-fish/Real-Time-Voice-Cloning/tree/522_volume_bar) branch for the code.

The volume bar replaces the ""Enhance vocoder output"" box, which is unnecessary since our newer models do not synthesize silence. After the change, the toolbox UI looks like this.

![image](https://user-images.githubusercontent.com/67130644/110234411-306a1f00-7edf-11eb-925e-9ba03373454e.png)

",finally got around feature request please see branch code volume bar enhance output box unnecessary since synthesize silence change toolbox like image,issue,positive,negative,negative,negative,negative,negative
792231873,"@blue-fish Thank you so much!  I can't believe I messed that up!  You were right, copying was errant.  Working great now!",thank much ca believe right errant working great,issue,positive,positive,positive,positive,positive,positive
792229052,@floripaoliver Pretrained german: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/571#issuecomment-716054792 . You will need an older version of the repo that uses the tensorflow synthesizer. Download commit [5425557](https://github.com/CorentinJ/Real-Time-Voice-Cloning/tree/5425557efe30863267f805851f918124191e0be0). Tensorflow 1.x is rather difficult to install and we do not support it anymore as it is obsolete.,german need older version synthesizer commit rather difficult install support obsolete,issue,negative,negative,negative,negative,negative,negative
792227666,"You might have to downgrade to `torch==1.4.0` to get DataParallel to work.

* https://github.com/fatchord/WaveRNN/issues/189
* https://github.com/huggingface/transformers/issues/3936",might downgrade get work,issue,negative,neutral,neutral,neutral,neutral,neutral
792226190,"This confusion was bound to happen at some point, because we use `pretrained.pt` as the filename for all the models.

Make sure your `synthesizer/saved_models/pretrained/pretrained.pt` file is from the same path in the zip file. You can also use the following information to ensure the files are located correctly.

```
File size    File path                                            md5 checksum
---------    ---------                                            ------------
 17090379    encoder/saved_models/pretrained.pt                   a2e2a0bf4cc3ac59053d3fe4d67b0c4b
370554559    synthesizer/saved_models/pretrained/pretrained.pt    7485ed2ae3f218819443841159e67b3c
 53845290    vocoder/saved_models/pretrained/pretrained.pt        417fd4c9b98e750764e8b466e777fd48
```",confusion bound happen point use make sure file path zip file also use following information ensure correctly file size file path,issue,negative,positive,positive,positive,positive,positive
792225208,"If you just want to run the toolbox and clone a voice, no training is required. The toolbox allows you to load your own audio files, click the ""browse"" button.

If following the training guide: All required scripts are in the [repository](https://github.com/CorentinJ/Real-Time-Voice-Cloning). The training guide contains the Python commands needed. It should be possible to reproduce the training of the pretrained models without needing to modify code. If you train with something besides LibriSpeech or LibriTTS, something is likely to break and you'll need to troubleshoot errors in the Python scripts.

You might be interested in my guide on finetuning the pretrained models to better match a single speaker. This is described in #437.",want run toolbox clone voice training toolbox load audio click browse button following training guide repository training guide python possible reproduce training without needing modify code train something besides something likely break need python might interested guide better match single speaker,issue,positive,positive,positive,positive,positive,positive
792042249,@Sadam1195 Awesome! worked like charm! thank you for your extremely fast reply too. ;),awesome worked like charm thank extremely fast reply,issue,positive,positive,positive,positive,positive,positive
792037724,"> > [https://drive.google.com/file/d/1ze3PHQtfpzCietKjyRT9Nd1t5jXKqtfS/view?usp=sharing](url)
> > 
> > * The provided link is for windows 10, python 3.
> > * Unzip this folder and place all the files in it to your directory where main.py file is.
> > 
> > @ZenithBerserker @rdrlima
> 
> I am new to this and got all working but when I try to use the encode_preprocess.py I got stuck because of the missing webrtvcd issue. I downloaded your files but I am not sure where to place the, I don't see anywhere the main.py? Sorry again for this question here but I just got this work yesterday and this is all new to me.

First download the folder, then unzip it.
After unzipping it, copy the content of that folder into the folder where code is, like where you need to import the webrtcvad
Hope it helps.
@floripaoliver 

Edit : You might not need webrtcvad in the latest version please pull the most recent code.",provided link python folder place directory file new got working try use got stuck missing issue sure place see anywhere sorry question got work yesterday new first folder copy content folder folder code like need import hope edit might need latest version please pull recent code,issue,positive,positive,positive,positive,positive,positive
792036383,"> [https://drive.google.com/file/d/1ze3PHQtfpzCietKjyRT9Nd1t5jXKqtfS/view?usp=sharing](url)
> 
> * The provided link is for windows 10, python 3.
> * Unzip this folder and place all the files in it to your directory where main.py file is.
> 
> @ZenithBerserker @rdrlima

I am new to this and got all working but when I try to use the encode_preprocess.py I got stuck because of the missing webrtvcd issue. I downloaded your files but I am not sure where to place the, I don't see anywhere the main.py? Sorry again for this question here but I just got this work yesterday and this is all new to me.",provided link python folder place directory file new got working try use got stuck missing issue sure place see anywhere sorry question got work yesterday new,issue,negative,positive,neutral,neutral,positive,positive
792028479,"When I run demo_cli.py, I get this error 16262 segmentation fault (core dumped)  python demo_cli.py

Reinstalling libqt5gui5 is working only once, and the issue comes back again. 
So, I'm reinstalling libqt5gui5 every time I run the demo.
",run get error segmentation fault core python working issue come back every time run,issue,negative,neutral,neutral,neutral,neutral,neutral
791930818,"Thanks! It works with LibriTTS! And I'm starting to figure out how everything works. I want to make a Russian model, I hope everything will work out",thanks work starting figure everything work want make model hope everything work,issue,positive,positive,neutral,neutral,positive,positive
791901300,"It's failing because it expects each audio file to have a text transcript when you specify `--no_alignments`. You'll need alignment files for LibriSpeech. This is metadata that relates each word in the transcript to a segment of audio. The LibriSpeech recordings are too long, so we split them into shorter recordings. The alignments are used to create a transcript for each segment. Follow the directions in our [training documentation](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training) to get the alignment files.

Another option is to download train-clean-100 from LibriTTS. That dataset does not require alignments because the utterances are already split and the transcript files are provided. Preprocess it with the `--datasets_name LibriTTS --no_alignments` options.",failing audio file text transcript specify need alignment word transcript segment audio long split shorter used create transcript segment follow training documentation get alignment another option require already split transcript provided,issue,negative,negative,neutral,neutral,negative,negative
791894970,"I just downloaded and unzipped LibriSpeech/train-clean-100, I didn't think there would be any problems with it. Perhaps the problem is also what is used there .flac instead .wav? 
![image](https://user-images.githubusercontent.com/52282778/110200322-5e634b00-7e6e-11eb-9f8e-6e3cf8759438.png)
",think would perhaps problem also used instead image,issue,negative,neutral,neutral,neutral,neutral,neutral
791838468,We should make the error message more informative if no audio files are detected.,make error message informative audio,issue,negative,neutral,neutral,neutral,neutral,neutral
791837775,"""The paging file is too small"" is not the relevant error message. The problem is that it is not finding any wav files. It looks like you're trying to use a custom dataset. Make sure your dataset folder has this hierarchy. You might be missing the ""book"" directory. https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-666099538",file small relevant error message problem finding like trying use custom make sure folder hierarchy might missing book directory,issue,negative,positive,positive,positive,positive,positive
791338844,No worries. It seems like a great tool. So I am happy to contribute however I can. ,like great tool happy contribute however,issue,positive,positive,positive,positive,positive,positive
790974580,"Add quotes around the dataset location if there are spaces in the path. Ensure that a supported dataset is in the specified location.
```
python demo_toolbox.py -d ""C:\Users\Scanner\Documents\Real Time TTS""
```",add around location path ensure location python time,issue,negative,neutral,neutral,neutral,neutral,neutral
790850197,"@chayan-agrawal I don't have a multiple GPU environment to troubleshoot. All I can suggest is to ensure that Python sees both of your GPUs. For example, add this to the beginning of `synthesizer_train.py` to have it use the first and second GPUs.

```
import os
os.environ[""CUDA_VISIBLE_DEVICES""] = ""0,1""
```",multiple environment suggest ensure python example add beginning use first second import o,issue,negative,positive,neutral,neutral,positive,positive
790714467,"Hi @blue-fish,

Just to give you an update, my encoder is still running, so couldn't make any changes so far. 

I have another question though. I noticed in my encoder training that the loss and EER calculated and plotted on Visdom is based on training data. Is there any way to also test the model maybe on a validation dataset or if we can provide a split in the dataloader for testing and training? I am curious if I can test my trained Speaker Verification model on an unseen or test dataset for **Loss** and **EER**.",hi give update still running could make far another question though training loss eer calculated plotted based training data way also test model maybe validation provide split testing training curious test trained speaker verification model unseen test loss eer,issue,negative,neutral,neutral,neutral,neutral,neutral
789357461,"> 
> 
> > I will be trying to do the same with spanish. Wish me luck. Any suggestions about compute power?
> 
> como te fue?

Si hay equipo y alguien toma el liderazgo, yo me sumo para el esfuerzo en español.",trying wish luck compute power te si hay el yo para el en,issue,positive,neutral,neutral,neutral,neutral,neutral
789116548,@blue-fish I have multiple GPUs on my system but it is working for only single GPU. Any help on how can I use multiple GPUs,multiple system working single help use multiple,issue,negative,negative,neutral,neutral,negative,negative
789100836,"Before we even think of merging this code, we'll need to consider these issues:
* Parallel training on multiple GPUs introduces a lot of overhead, slowing training: https://github.com/as-ideas/ForwardTacotron/issues/9
* Using DataParallel may result in incorrect gradient computation on multiple GPUs: https://github.com/pytorch/pytorch/issues/15716 (might be fixed in torch >= 1.4.0, but issue is still open)",even think code need consider parallel training multiple lot overhead training may result incorrect gradient computation multiple might fixed torch issue still open,issue,negative,positive,neutral,neutral,positive,positive
789094262,"To help others with a similar problem, can you clarify which suggestion worked? Was it updating command line developer tools to get webrtcvad to build? Or installing `webrtcvad-wheels` instead of webrtcvad?",help similar problem clarify suggestion worked command line developer get build instead,issue,negative,neutral,neutral,neutral,neutral,neutral
789050671,You may need to update your command line developer tools. You can also try replacing webrtcvad with `webrtcvad-wheels`.,may need update command line developer also try,issue,negative,neutral,neutral,neutral,neutral,neutral
789047301,"@chayan-agrawal Thanks for suggesting that change. I have updated the code with your suggestion: https://github.com/blue-fish/Real-Time-Voice-Cloning/commit/a90d2340c0d0c416bcec4da089a2c9ce3e4ed7d4

DataParallel also works in CPU and single GPU environment, so it is not necessary to check for multiple GPUs. It would be nice to get feedback on whether it works for multiple GPUs.",thanks suggesting change code suggestion also work single environment necessary check multiple would nice get feedback whether work multiple,issue,positive,positive,positive,positive,positive,positive
789031403,"Does your Macbook have an Intel or Apple silicon processor? It may not work on the latter.

You can also remove or comment out webrtvcad in requirements.txt, it is optional.",apple silicon processor may work latter also remove comment optional,issue,negative,neutral,neutral,neutral,neutral,neutral
789016068,@blue-fish I am also using torch==1.7.1. Instead of model.load if used model.module.load it works on single GPU. Other GPUs are not in use. ,also instead used work single use,issue,negative,negative,neutral,neutral,negative,negative
789009195,@chayan-agrawal Which version of torch are you using? I'm using torch==1.7.1 and don't get that error.,version torch get error,issue,negative,neutral,neutral,neutral,neutral,neutral
788854616,"@blue-fish In the above mentioned code, we get another error at Line no 110 in synthesizer/train.py. 
torch.nn.modules.module.ModuleAttributeError: 'DataParallel' object has no attribute 'load'
",code get another error line object attribute,issue,negative,neutral,neutral,neutral,neutral,neutral
788431082,"> I will be trying to do the same with spanish. Wish me luck. Any suggestions about compute power?

como te fue?",trying wish luck compute power te,issue,positive,neutral,neutral,neutral,neutral,neutral
788403938,"@blue-fish great, thanks for the tip. I'll update the synthesizer and train a new model with this option enabled. I hope it will be significantly better quality.",great thanks tip update synthesizer train new model option hope significantly better quality,issue,positive,positive,positive,positive,positive,positive
788394246,@wm1511 I describe a way to automatically remove silence in #501. It's also built into the latest version of this repo (set `trim_silence = True` in synthesizer/hparams.py).,describe way automatically remove silence also built latest version set true,issue,negative,positive,positive,positive,positive,positive
788391328,"@StElysse Not possible with this repo currently. The Tacotron model would need to be modified to support ""Global Style Tokens"" (#230). A multi-speaker dataset with labeled emotions is also lacking.

@dorigom06 For better accent matching, finetune the model with the process in #437.",possible currently model would need support global style also better accent matching model process,issue,positive,positive,positive,positive,positive,positive
788385666,"In the literature, real-time means that the generation time is less than the audio's duration.
Change the vocoder model to achieve real-time performance, by your definition.

Here are some recommended neural vocoders which offer a good balance of speed and audio quality.
* https://github.com/kan-bayashi/ParallelWaveGAN
* https://github.com/seungwonpark/melgan
* https://github.com/descriptinc/melgan-neurips",literature generation time le audio duration change model achieve performance definition neural offer good balance speed audio quality,issue,negative,positive,positive,positive,positive,positive
787644009,You can try implementing the code changes in https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/527 . It allows for a folder called `UserAudio` in datasets_root to be used as a custom dataset.,try code folder used custom,issue,negative,neutral,neutral,neutral,neutral,neutral
787599868,"I'm also having a similar problem. I'm using my friends to train the model, and they both have accents. The code isn't particularly good at modeling certain accents. 
",also similar problem train model code particularly good modeling certain,issue,negative,positive,positive,positive,positive,positive
787532366,"Sorry this piggy backs off of those who've mentioned that the dataset GUI is ""greyed out"". I'm just wondering if there's a way so that I can cycle back through datasets that I have under -d ""dataset"" in the RTVC folder? ",sorry piggy wondering way cycle back folder,issue,negative,negative,negative,negative,negative,negative
787516239,"I trained on common voice also, but most of them was too silent or had parts of silence and results without alignments (which aren't made) were worse than before training on that dataset. So from nearly 100 000 utterances from commmon voice around 7000 made by 35 speakers have left (set ""train"").",trained common voice also silent silence without made worse training nearly voice around made left set train,issue,negative,negative,negative,negative,negative,negative
787514954,you can train new on mozilla common voice ,train new common voice,issue,negative,negative,neutral,neutral,negative,negative
787482536,"Uploaded. I'll pass if anyone needs. This model is trained for the previous version of synthesizer (Tensorflow). It's not a very good quality, because I couldn't find sufficient amount of speakers for the databases, but I will do my best with the new synthesizer. ;)
",pas anyone need model trained previous version synthesizer good quality could find sufficient amount best new synthesizer,issue,positive,positive,positive,positive,positive,positive
787423598,"> @PaYo90 I have all the datasets prepared for training polish and polish model trained to 226k steps. If you want any help, tell me.

Yeah sure, please: skorpss[AT]gmail.com : ) Send me, if you could tell me couple of tips to do that i appreacie it. Id kile to know the difference between this synthetizer, vodoer and the third one",prepared training polish polish model trained want help tell yeah sure please send could tell couple id know difference synthetizer third one,issue,positive,positive,positive,positive,positive,positive
787406380,@zxmanxz When will you be able to test the multi-GPU training code? https://github.com/blue-fish/Real-Time-Voice-Cloning/tree/664_multi_gpu_training,able test training code,issue,negative,positive,positive,positive,positive,positive
787406225,"Going back to the original problem, I don't think this problem is widespread enough on Windows to change the default to `num_workers=0`. But here is a potential solution in case this gets reported again. https://github.com/blue-fish/Real-Time-Voice-Cloning/commit/89a99647a9e1a5b122305773610e1a4c38e41329 (not tested)

I am going to close this issue. @arstropica , you're invited to open an issue for the training speed problem. I do not have any ideas, but someone else might.",going back original problem think problem widespread enough change default potential solution case tested going close issue open issue training speed problem someone else might,issue,negative,positive,neutral,neutral,positive,positive
787405161,"Problem is that it can't find a file called `samples/1320_00000.mp3` relative to the location from where you're running the Python command.

Based on your error message, the easiest solution might be to do this:
```
cd C:\Users\Horus50\Desktop\Coding\Real-Time-Voice-Cloning-master\Real-Time-Voice-Cloning-master
python demo_cli.py
```

To prevent this from happening to other users, we should make the default paths relative to the location of demo_cli.py or demo_toolbox.py.",problem ca find file relative location running python command based error message easiest solution might python prevent happening make default relative location,issue,negative,neutral,neutral,neutral,neutral,neutral
787404317,"Thanks for reporting the issue, but I can't provide support for installations using Anaconda. It's recommended to use a normal Python environment. See the Windows setup instructions in: https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542",thanks issue ca provide support anaconda use normal python environment see setup,issue,positive,positive,positive,positive,positive,positive
787000449,"I was missing one of the requirements, but now when I run it I get this:
Arguments:
    enc_model_fpath:   encoder\saved_models\pretrained.pt
    syn_model_fpath:   synthesizer\saved_models\pretrained\pretrained.pt
    voc_model_fpath:   vocoder\saved_models\pretrained\pretrained.pt
    cpu:               False
    no_sound:          False
    seed:              None
    no_mp3_support:    False

C:\Users\Horus50\AppData\Local\Programs\Python\Python37\lib\site-packages\librosa\core\audio.py:162: UserWarning: PySoundFile failed. Trying audioread instead.
  warnings.warn(""PySoundFile failed. Trying audioread instead."")
Traceback (most recent call last):
  File ""C:\Users\Horus50\AppData\Local\Programs\Python\Python37\lib\site-packages\librosa\core\audio.py"", line 146, in load
    with sf.SoundFile(path) as sf_desc:
  File ""C:\Users\Horus50\AppData\Local\Programs\Python\Python37\lib\site-packages\soundfile.py"", line 629, in __init__
    self._file = self._open(file, mode_int, closefd)
  File ""C:\Users\Horus50\AppData\Local\Programs\Python\Python37\lib\site-packages\soundfile.py"", line 1184, in _open
    ""Error opening {0!r}: "".format(self.name))
  File ""C:\Users\Horus50\AppData\Local\Programs\Python\Python37\lib\site-packages\soundfile.py"", line 1357, in _error_check
    raise RuntimeError(prefix + _ffi.string(err_str).decode('utf-8', 'replace'))
RuntimeError: Error opening 'samples/1320_00000.mp3': System error.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Horus50\Desktop\Coding\Real-Time-Voice-Cloning-master\Real-Time-Voice-Cloning-master\demo_cli.py"", line 50, in <module>
    librosa.load(""samples/1320_00000.mp3"")
  File ""C:\Users\Horus50\AppData\Local\Programs\Python\Python37\lib\site-packages\librosa\core\audio.py"", line 163, in load
    y, sr_native = __audioread_load(path, offset, duration, dtype)
  File ""C:\Users\Horus50\AppData\Local\Programs\Python\Python37\lib\site-packages\librosa\core\audio.py"", line 187, in __audioread_load
    with audioread.audio_open(path) as input_file:
  File ""C:\Users\Horus50\AppData\Local\Programs\Python\Python37\lib\site-packages\audioread\__init__.py"", line 111, in audio_open
    return BackendClass(path)
  File ""C:\Users\Horus50\AppData\Local\Programs\Python\Python37\lib\site-packages\audioread\rawread.py"", line 62, in __init__
    self._fh = open(filename, 'rb')
FileNotFoundError: [Errno 2] No such file or directory: 'samples/1320_00000.mp3'

It seems like I'm missing some files somehow, but I downloaded the entire project earlier today and the only change that I have made is sticking the pretrained models into it.",missing one run get false false seed none false trying instead trying instead recent call last file line load path file line file file line error opening file line raise prefix error opening system error handling exception another exception recent call last file line module file line load path offset duration file line path file line return path file line open file directory like missing somehow entire project today change made sticking,issue,negative,negative,negative,negative,negative,negative
786942232,"Please make sure you are running `python demo_toolbox.py` from the folder where demo_toolbox.py exists. If that still doesn't work, try this: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/680#issuecomment-786452068",please make sure running python folder still work try,issue,positive,positive,positive,positive,positive,positive
786939776,"@blue-fish  Thanks for your observation.  I am not sure how to address the cuda performance issue. 

The GPU is definitely working but only at a minimum rate.  Benchmark tests show my SSD performance within expected values.

Perhaps it is due to my GPU driver being out of sync with the cuda toolkit. Here is the output from nvidia-smi.
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 465.42       Driver Version: 465.42       CUDA Version: 11.3     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|

```
Not sure why performance is so bad.  I have tried installing different toolkit versions:  10.0, 11.1 and 11.2 with the same result. Do I need to downgrade my driver to match the toolkit?
I ",thanks observation sure address performance issue definitely working minimum rate show performance within perhaps due driver sync output driver version version name volatile fan temp compute mig sure performance bad tried different result need downgrade driver match,issue,positive,positive,neutral,neutral,positive,positive
786691561,Those problems in inference are caused by failures of the attention mechanism. We currently use Location-Sensitive Attention. There is a comparison of attention methods in 1910.10288 and LSA is one of the worse ones.,inference attention mechanism currently use attention comparison attention one worse,issue,negative,negative,negative,negative,negative,negative
786675290,Thanks @woodrow73 and @blue-fish I think as I've set up using the old documents/files I'm going to start from scratch with the new install instructions and updates. Appreciate the help!,thanks woodrow think set old going start scratch new install appreciate help,issue,positive,positive,positive,positive,positive,positive
786553519,"@PaYo90 I have all the datasets prepared for training polish and polish model trained to 226k steps. If you want any help, tell me.
",prepared training polish polish model trained want help tell,issue,positive,neutral,neutral,neutral,neutral,neutral
786547499,"Hi @blue-fish ,

I am training my encoder model at the moment. But I can take a look at it next week (depending on the encoder results if I don't need to retrain it). Will let you know if I do make any changes. ",hi training model moment take look next week depending need retrain let know make,issue,negative,neutral,neutral,neutral,neutral,neutral
786527292,"> Development of the pytorch synthesizer is complete. Please review the changes.
> 
> The [pretrained model release](https://github.com/blue-fish/Real-Time-Voice-Cloning/releases/tag/v1.0) consists of the pretrained encoder, along with synthesizer and vocoder models I have developed. Audio samples and model details: https://blue-fish.github.io/experiments/RTVC-7.html

Hi, thanks for the amazing work, may I know what does Google rows mean in your demo page? Thanks",development synthesizer complete please review model release along synthesizer audio model hi thanks amazing work may know mean page thanks,issue,positive,positive,positive,positive,positive,positive
786475986,Off-topic but I found that after long phrases (> 128 symbols) RNN (I guess) get some UNremovable trash in its state and the speaker starts to say some indistinguishable noise. So with this model actually I have to split texts to relatively small portions of texts.,found long guess get unremovable trash state speaker say indistinguishable noise model actually split relatively small,issue,negative,negative,neutral,neutral,negative,negative
786456794,"Thanks for sharing your observations @arstropica . `num_workers` is for the data loader. If the CPU is the bottleneck for training speed, performance may improve with a higher setting. If the bottleneck is the GPU or disk, then num_workers is not relevant.

Not sure GPU acceleration is working properly there. I would expect 5-10x faster training speed with a 2080ti.

",thanks data loader bottleneck training speed performance may improve higher setting bottleneck disk relevant sure acceleration working properly would expect faster training speed ti,issue,positive,positive,positive,positive,positive,positive
786452068,"Please try this command. Also check the permissions and ownership on the folders.
```
python demo_toolbox.py --enc_models_dir C:\full\path\to\encoder\saved_models --syn_models_dir C:\full\path\to\synthesizer\saved_models --voc_models_dir C:\full\path\to\vocoder\saved_models
```

You can also bypass the model check by commenting out these lines.
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/95adc699c1deb637f485e85a5107d40da0ad94fc/demo_toolbox.py#L38-L40",please try command also check ownership python also bypass model check,issue,negative,neutral,neutral,neutral,neutral,neutral
786445701,"We do not support Mozilla Common Voice directly. Here is my advice.
1. Process the dataset to look like LibriTTS: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-666099538 Then you can use our preprocessing scripts.
2. Don't forget to edit [symbols.py](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/synthesizer/utils/symbols.py) to include all the letters of the Polish alphabet.
3. Try training the synthesizer only for now. Our pretrained English encoder and vocoder models should work well enough.

This is all the help I can provide. Good luck.",support common voice directly advice process look like use forget edit include polish alphabet try training synthesizer work well enough help provide good luck,issue,positive,positive,positive,positive,positive,positive
786443718,"> synthesizer/inference.py:
>
> len(inputs) returns you 1 (if demo_cli.py used), but inputs is a list which potentially can have any size. So batched_inputs is not ""batched"" in any sense.

I don't quite understand the concern. We use batching in the toolbox. There is also an example of how to use batching in demo_cli.py. It's appropriate to call it ""batched"" in synthesizer/inference.py.

https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/95adc699c1deb637f485e85a5107d40da0ad94fc/toolbox/__init__.py#L224-L229

https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/95adc699c1deb637f485e85a5107d40da0ad94fc/demo_cli.py#L104-L109

> Also there should be some way to keep states and continue to inference for new batches to ensure connectivity of the speech.

With batching, it's hard because everything is processed in parallel. One way to get around this is to synthesize segments with overlap and then blend them together. See the below figure from Corentin's thesis.

Synthesizing one sentence at a time is good enough for me, so I find this unnecessary. For better flow between sentences, you should improve or remove the attention mechanism. Then there would be no limit on the length of the text input. These papers can be used as a starting point if you want to pursue this.

* https://arxiv.org/pdf/1910.10288.pdf (Battenberg et. al, ""Location-Relative Attention Mechanisms for Robust Long-Form Speech Synthesis"")
* https://arxiv.org/pdf/2010.04301 (Shen et. al, ""Non-Attentive Tacotron: Robust and Controllable Neural TTS Synthesis Including Unsupervised Duration Modeling"")

![image](https://user-images.githubusercontent.com/67130644/109261945-8ccd9000-77b5-11eb-895b-5c02151bd94f.png)",used list potentially size sense quite understand concern use toolbox also example use appropriate call also way keep continue inference new ensure connectivity speech hard everything parallel one way get around synthesize overlap blend together see figure thesis one sentence time good enough find unnecessary better flow improve remove attention mechanism would limit length text input used starting point want pursue al attention robust speech synthesis al robust controllable neural synthesis unsupervised duration modeling image,issue,positive,positive,positive,positive,positive,positive
786266274,"This is an issue that affects everyone training an encoder model. Some ideas to fix it:
1. Modify encoder preprocessing to delete empty folders in `datasets_root/SV2TTS/encoder` before exiting.
2. Fix the random cycler or the encoder training code so it doesn't break when empty folders are present.",issue everyone training model fix modify delete empty fix random cycler training code break empty present,issue,negative,negative,negative,negative,negative,negative
785923102,"I may have spoken too soon.

Using dill instead of pickle prevents the serialization error, but there doesn't seem to be any performance change with the value of `num_workers`.  

With my NVIDIA GeForce RTX 2080 Ti:

`num_workers`: 0
```
+----------------+------------+---------------+------------------+
| Steps with r=2 | Batch Size | Learning Rate | Outputs/Step (r) |
+----------------+------------+---------------+------------------+
|   15k Steps    |     12     |     0.001     |        2         |
+----------------+------------+---------------+------------------+

{| Epoch: 1/3750 (4/4) | Loss: 0.1510 | 0.40 steps/s | Step: 5k | }
{| Epoch: 2/3750 (4/4) | Loss: 0.1501 | 0.44 steps/s | Step: 5k | }
{| Epoch: 3/3750 (4/4) | Loss: 0.1488 | 0.45 steps/s | Step: 5k | }
{| Epoch: 4/3750 (4/4) | Loss: 0.1493 | 0.45 steps/s | Step: 5k | }
{| Epoch: 5/3750 (4/4) | Loss: 0.1507 | 0.46 steps/s | Step: 5k | }
{| Epoch: 6/3750 (4/4) | Loss: 0.1494 | 0.45 steps/s | Step: 5k | }
{| Epoch: 7/3750 (4/4) | Loss: 0.1475 | 0.45 steps/s | Step: 5k | }

```
`num_workers`: 2

```
+----------------+------------+---------------+------------------+
| Steps with r=2 | Batch Size | Learning Rate | Outputs/Step (r) |
+----------------+------------+---------------+------------------+
|   15k Steps    |     12     |     0.001     |        2         |
+----------------+------------+---------------+------------------+

{| Epoch: 1/3750 (4/4) | Loss: 0.1506 | 0.40 steps/s | Step: 5k | }
{| Epoch: 2/3750 (4/4) | Loss: 0.1577 | 0.45 steps/s | Step: 5k | }
{| Epoch: 3/3750 (4/4) | Loss: 0.1549 | 0.48 steps/s | Step: 5k | }
{| Epoch: 4/3750 (4/4) | Loss: 0.1569 | 0.49 steps/s | Step: 5k | }
{| Epoch: 5/3750 (4/4) | Loss: 0.1536 | 0.50 steps/s | Step: 5k | }
{| Epoch: 6/3750 (4/4) | Loss: 0.1517 | 0.50 steps/s | Step: 5k | }
{| Epoch: 7/3750 (4/4) | Loss: 0.1520 | 0.49 steps/s | Step: 5k | }
{| Epoch: 8/3750 (4/4) | Loss: 0.1504 | 0.48 steps/s | Step: 5k | }
{| Epoch: 9/3750 (4/4) | Loss: 0.1496 | 0.48 steps/s | Step: 5k | }
```

Not sure multiprocessing is working at all with my fix.",may spoken soon dill instead pickle serialization error seem performance change value ti batch size learning rate epoch loss step epoch loss step epoch loss step epoch loss step epoch loss step epoch loss step epoch loss step batch size learning rate epoch loss step epoch loss step epoch loss step epoch loss step epoch loss step epoch loss step epoch loss step epoch loss step epoch loss step sure working fix,issue,negative,positive,positive,positive,positive,positive
785812241,Also there should be some way to keep states and continue to inference for new batches to ensure connectivity of the speech.,also way keep continue inference new ensure connectivity speech,issue,negative,positive,positive,positive,positive,positive
785546078,"My workaround for this issue of lambda objects not being pickled on Windows was to install and use [multiprocessing_on_dill](https://pypi.org/project/multiprocessing_on_dill/) instead of python's multiprocessing module. This meant replacing two references to the native multiprocessing module in the `__init__.py` file for torch.multiprocessing.  

There should be a better way of doing this via the `multiprocessing_context` argument of the Dataloader class constructor that would involve extending torch.multiprocessing, replacing the reference in the extended class, and passing that to the constructor.  Anyone more familiar with python and/or torch should be able to do a PR on this. Also conditionally installing `multiprocessing_on_dill` on windows would have to be a prerequisite.",issue lambda install use instead python module meant two native module file better way via argument class constructor would involve extending reference extended class passing constructor anyone familiar python torch able also conditionally would prerequisite,issue,negative,positive,positive,positive,positive,positive
785478906,"There shouldn't be a dependency on tensorflow as of 10 days ago? - https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/652#issuecomment-778798757

Are you using the latest windows install instructions - https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542",dependency day ago latest install,issue,negative,positive,positive,positive,positive,positive
785345330,"Update: I've reinstalled umap and the TF error goes away but still same ""Model files not found"" error at bottom",update error go away still model found error bottom,issue,negative,neutral,neutral,neutral,neutral,neutral
784983248,"I was stuck at the same place getting the same error. So I have written this helper script to figure out where the empty folders are and save them in a .txt file. I am taking length<2 as empty folders does contain source file.
```
import glob
import os

Dir = 'D:\mstts\SV2TTS\encoder'  #Source directory containing encoder preprocessed files 
OUTPUTFILE = r'D:\mstts\SV2TTS\folder.txt' #.txt destination directory

path = os.path.join(Dir, '*')
for folder in glob.glob(path):
    with open(OUTPUTFILE, 'a') as f:
        if len(os.listdir(folder)) < 2:
            f.write(""{0} is a directory with No files'\n' "".format(folder))
        else:
            f.write(""{0} is a directory with files'\n' "".format(folder))
```

Usage:
1. Use this script to create a txt file with required information
2. Look for folders where 'No files' is output
3. Delete the folders from step 2 from your <dataset_root>/SVTTS/encoder

That's it, you are good to go. ",stuck place getting error written helper script figure empty save file taking length empty contain source file import import o source directory destination directory path folder path open folder directory folder else directory folder usage use script create file information look output delete step good go,issue,positive,positive,positive,positive,positive,positive
784836076,Nothing more to suggest here.  We have to improve technology for the better.,nothing suggest improve technology better,issue,negative,positive,positive,positive,positive,positive
784212196,"Hello, guys.

I solved this issue.
Please check this pull request.

#678

Hope that my solution be helpful.",hello issue please check pull request hope solution helpful,issue,positive,neutral,neutral,neutral,neutral,neutral
784211399,"@CorentinJ 

It seems that most of users who are using this repository have been suffered from this issue.

Hope that my solution be helpful to everyone who uses this repo.

Thank you.",repository issue hope solution helpful everyone thank,issue,positive,neutral,neutral,neutral,neutral,neutral
782822038,"@blue-fish  have you ever tested your idea of fine-tuning the model with new voice without labelled text ? 
I just trained new SE in 22050hz and 256 embedding-dim (1 on raw audio and 1 on mel) so compatible with my model output and will try to make a synthesizer with their embeddings (hope it will work well)
The results metrics for these 2 SE are 0.07 for EER and 0.97 for AUC and 0.925 for binary accuracy (test-set) for a 12h training (quite funny, the SE on raw audio was the best (test-acc) at epoch 2 so only 2h training for a quite powerful Speaker-Verification system x) )
Both were trained by siamese approach, 960 speakers (100 same-pairs and 100 not-same-pairs for each) and tested on 100 totally unknown speakers (100 same / not-same also)

If synthesizer produces good quality voice (understandable / good quality), I will try your idea with my mel-based SE but don’t know if it will not decrease the understandability by improving it’s similarity... So if you already tried it I am curious to see your results ;) ",ever tested idea model new voice without text trained new se raw audio mel compatible model output try make synthesizer hope work well metric se eer binary accuracy training quite funny se raw audio best epoch training quite powerful system trained approach tested totally unknown also synthesizer good quality voice understandable good quality try idea se know decrease understandability improving similarity already tried curious see,issue,positive,positive,positive,positive,positive,positive
782751232,">  If I'm understanding this correctly, having max_mel_frames = 900 will still cut off Wav-Files at 11.25 seconds?

Wav files that are too long are not truncated. Instead, they are dropped from the training set entirely. This can be avoided by using alignment data to split the wavs.",understanding correctly still cut long truncated instead training set entirely alignment data split,issue,negative,negative,neutral,neutral,negative,negative
782745670,"error running synthesizer_preprocess_audio.py (and almost identical one to running synthesizer_preprocess_embeds.py):
```
  File ""C:\Users\username\AppData\Roaming\Python\Python37\site-packages\torch\__init__.py"", line 117, in <module>
    from torch.nn.utils import clip_grad_norm_
  File ""C:\Users\username\AppData\Roaming\Python\Python37\site-packages\torch\__init__.py"", line 117, in <module>
    raise err
OSError: [WinError 1455] The paging file is too small for this operation to complete. Error loading ""C:\Users\username\AppData\Roaming\Python\Python37\site-packages\torch\lib\cusolverMg64_10.dll"" or one of its dependencies.
    raise err
OSError: [WinError 1455] The paging file is too small for this operation to complete. Error loading ""C:\Users\username\AppData\Roaming\Python\Python37\site-packages\torch\lib\cudnn_adv_infer64_8.dll"" or one of its dependencies.
LibriSpeech: 100%|█████████████████████████| 1/1 [00:11<00:00, 11.28s/speakers]
The dataset consists of 32 utterances, 10936 mel frames, 2183520 audio timesteps (0.04 hours).
Max input length (text chars): 330
Max mel frames length: 586
Max audio timesteps length: 117120
```
[Pastebin of the full error logs](https://pastebin.com/3W584m9n) from both synthesizer_preprocess_audio.py and synthesizer_preprocess_embeds.py
[Workaround](https://github.com/Spandan-Madan/Pytorch_fine_tuning_Tutorial/issues/10#issuecomment-768135516)

Another error I encountered afterwards was a Cuda memory error, which I traced to synthesizer_preprocess_embeds.py, where I found the helpful information needed for the workaround coded in the \help command:
```
parser.add_argument(""-n"", ""--n_processes"", type=int, default=1, help= \
        ""Number of parallel processes. An encoder is created for each, so you may need to lower ""
        ""this value on GPUs with low memory. Set it to 1 (default is 4) if CUDA is unhappy."")
```",error running almost identical one running file line module import file line module raise err file small operation complete error loading one raise err file small operation complete error loading one mel audio input length text mel length audio length full error another error afterwards memory error found helpful information command number parallel may need lower value low memory set default unhappy,issue,negative,negative,neutral,neutral,negative,negative
782742215,"> Notice that lowering max_mel_frames results in your longer samples being discarded due to length. This shrinks your dataset. For example, each mel frame is 0.0125 seconds, so setting max_mel_frames = 300 will keep wav files up to 3.75 sec.

Interesting - aside from being smaller, I imagine there may be additional noise issues as a result of abruptly cutting off words from the Wav-Files.  If I'm understanding this correctly, having max_mel_frames = 900 will still cut off Wav-Files at 11.25 seconds?  Well, this is definitely yet another motivation to unlock the potential of my hardware via an Ubuntu installation.

Thanks for the info; I'll also post the errors / workarounds here since it's on topic for synthesizer training on windows.",notice lowering longer due length example mel frame setting keep sec interesting aside smaller imagine may additional noise result abruptly cutting understanding correctly still cut well definitely yet another motivation unlock potential hardware via installation thanks also post since topic synthesizer training,issue,positive,negative,neutral,neutral,negative,negative
782710830,"You posted output for these cases (corresponding to max_mel_frames = 900, 600 and 300 respectively):
1. 48 samples, batch size 5, 25k steps remaining until training stage is completed.
2. 32 samples, batch size 8, 20k steps remaining.
3. 14 samples, batch size 12, 20k steps remaining.

* In the first case, your dataset has 48 samples. With batch=5, it takes 10 steps to complete an epoch. (Every epoch will have 9 steps with 5 samples, followed by 1 final step with 2 samples.) The training stage has 25k steps remaining, so it will take (25000/10) = 2500 epochs to complete the training stage.
* In the 2nd case, dataset=32 and batch=8 so it takes 4 steps to complete an epoch. Since the training stage has 20k steps remaining, you need to complete (20000/4) = 5000 epochs to finish the training stage.
* In the 3rd case, dataset=14 and batch=12 so it only takes 2 steps to complete an epoch. Since the training stage has 20k steps remaining, you need to complete (20000/2) = 10000 epochs to finish the training stage.

Notice that lowering `max_mel_frames` results in your longer samples being discarded due to length. This shrinks your dataset. For example, each mel frame is 0.0125 seconds, so setting max_mel_frames = 300 will keep wav files up to 3.75 sec.

This diagram may be helpful to understand the display:
```
                 ___ total epochs to complete training stage
                |
                |        ___ total steps to complete epoch
                |       |
{| Epoch: 100/2500 (10/10) | Loss: 0.3283 | 0.35 steps/s | Step: 296k | }
           |        |                                             |
           |        |___ steps completed on current epoch         |
           |             (increments when step completed)         |
           |                                                      |
           |___ epochs completed on current training stage        |
                (increments when epoch is started)                |
                                                                  |
              total steps completed across all training stages ___|
```",posted output corresponding respectively batch size training stage batch size batch size first case complete epoch every epoch final step training stage take complete training stage case complete epoch since training stage need complete finish training stage case complete epoch since training stage need complete finish training stage notice lowering longer due length example mel frame setting keep sec diagram may helpful understand display total complete training stage total complete epoch epoch loss step current epoch step current training stage epoch total across training,issue,negative,positive,neutral,neutral,positive,positive
782555477,"No trouble, thanks for the pointers.  I hadn't considered the storage medium a variable, but makes sense with a lot of reading & writing; however, it is already on an internal SSD (850 evo).

Yes, my max_mel_frames are 900 - after preprocessing the data with `max_mel_frames = 600`, I was able to adjust the batch size to 8.  Here's the output after 5min of running:
```
Found 32 samples
+----------------+------------+---------------+------------------+
| Steps with r=2 | Batch Size | Learning Rate | Outputs/Step (r) |
+----------------+------------+---------------+------------------+
|   20k Steps    |      8     |     0.001     |        2         |
+----------------+------------+---------------+------------------+

{| Epoch: 1/5000 (4/4) | Loss: 12.23 | 0.39 steps/s | Step: 0k | }
.
.
{| Epoch: 34/5000 (4/4) | Loss: 1.010 | 0.47 steps/s | Step: 0k | }
```

After preprocessing the data with `max_mel_frames = 300`, I could change the batch sizes to the default of 12 (didn't try higher); and here's the result of running for 5min:
```
Found 14 samples
+----------------+------------+---------------+------------------+
| Steps with r=2 | Batch Size | Learning Rate | Outputs/Step (r) |
+----------------+------------+---------------+------------------+
|   20k Steps    |      12    |     0.001     |        2         |
+----------------+------------+---------------+------------------+

{| Epoch: 1/10000 (2/2) | Loss: 9.556 | 0.76 steps/s | Step: 0k | }
.
.
{| Epoch: 125/10000 (2/2) | Loss: 0.8832 | 1.0 steps/s | Step: 0k | }
```
This has me curious what the cost on the output is as a result of altering both max_mel_frames & batch size - as the Epoch description in the link would suggest, a smaller batch size likely creates more noise as it is too small to properly represent all the data.  I'm not ML savvy enough (yet) to understand what exactly the console output is communicating in the first part - should 1/10000 (2/2) be alarming?

I'll try it with Ubuntu within the week, after I post my 3-17 second utterance extractor from a single Wav-File (already finished, just wanna clean & document it more).  I'm also gonna rig something up to help automate the process of writing down the timestamp of when each word is said (nothing fancy, like playing the audio clip slowly, then pressing a button at the start of each word) - or maybe I'll give in and try a forced aligner like you showed [here](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/547#issuecomment-706139555).",trouble thanks considered storage medium variable sense lot reading writing however already internal yes data able adjust batch size output min running found batch size learning rate epoch loss step epoch loss step data could change batch size default try higher result running min found batch size learning rate epoch loss step epoch loss step curious cost output result batch size epoch description link would suggest smaller batch size likely noise small properly represent data savvy enough yet understand exactly console output communicating first part alarming try within week post second utterance extractor single already finished wan na clean document also gon na rig something help process writing word said nothing fancy like audio clip slowly pressing button start word maybe give try forced aligner like,issue,negative,positive,neutral,neutral,positive,positive
782478951,"Are you double-clicking demo_cli.py in the Finder? Make sure you are launching Python from the terminal.
```
python demo_cli.py
```",finder make sure python terminal python,issue,negative,positive,positive,positive,positive,positive
782458315,"Try running on CPU instead of GPU:
```
python demo_toolbox.py --cpu
```",try running instead python,issue,negative,neutral,neutral,neutral,neutral,neutral
782354886,"It's 

> Your GPU is too old to use the latest pytorch.
> 
> First uninstall pytorch:
> 
> ```
> pip uninstall pytorch
> ```
> 
> Then install an older pytorch:
> 
> ```
> pip install torch==1.2 -f https://download.pytorch.org/whl/torch_stable.html
> ```

It's still the same error message",old use latest first pip install older pip install still error message,issue,negative,positive,positive,positive,positive,positive
782299868,"Your GPU is too old to use the latest pytorch.

First uninstall pytorch:
```
pip uninstall pytorch
```
Then install an older pytorch:
```
pip install torch==1.2 -f https://download.pytorch.org/whl/torch_stable.html
```",old use latest first pip install older pip install,issue,negative,positive,positive,positive,positive,positive
781994209,"Sorry, we're unable to provide support for this issue. We removed the colab notebook from the repo because no one was maintaining it. (#656, #657)",sorry unable provide support issue removed notebook one,issue,negative,negative,negative,negative,negative,negative
781953493,"Thanks for the update. The training speed seems slow to me. Since neither your CPU nor GPU are at 100%, I think the bottleneck is the storage medium. Try moving your datasets_root/SV2TTS folder to a SSD. Is `max_mel_frames=900`? You can preprocess with a lower number like 600 which will reduce memory consumption and training time per step.

Your GPU should be capable of 1 step/sec and handle a batch size between 16 and 24 with reduction factor r=2. See if it is faster on Ubuntu.

A quick read you may find helpful: https://docs.paperspace.com/machine-learning/wiki/epoch",thanks update training speed slow since neither think bottleneck storage medium try moving folder lower number like reduce memory consumption training time per step capable handle batch size reduction factor see faster quick read may find helpful,issue,positive,positive,positive,positive,positive,positive
781942149,"Yes, I'll try to use multi GPU's later.",yes try use later,issue,negative,neutral,neutral,neutral,neutral,neutral
781927674,"I've noticed this too when training the old Tensorflow synthesizer, but don't know what causes it. We don't support that code anymore. However, I'd be curious to know if you find out.

This repo has switched to using a PyTorch synthesizer. Try downloading the latest code and pretrained models. The new training script does not pause to generate training batches.",training old synthesizer know support code however curious know find switched synthesizer try latest code new training script pause generate training,issue,positive,positive,positive,positive,positive,positive
781747292,"Thanks for the workaround - just got around to trying it out & initially got a memory error:
```
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 6.00 GiB total capacity; 4.25 GiB already allocated; 10.70 MiB free; 4.45 GiB reserved in total by PyTorch)
```

So I edited synthesizer/hparams.py to decrease batch size.  synthesis_batch_size didn't seem to have an effect, so I left that at 16, and I changed the batch_size values in tts_schedule to 5 - which seems to be the highest value I can give it without running out of memory.

I timed the training duration for the first 10 epochs(steps?), which took 5 minutes 3 seconds - here's what the console prints for me:
```
Found 48 samples
+----------------+------------+---------------+------------------+
| Steps with r=2 | Batch Size | Learning Rate | Outputs/Step (r) |
+----------------+------------+---------------+------------------+
|   25k Steps    |      5     |     3e-05     |        2         |
+----------------+------------+---------------+------------------+

{| Epoch: 1/2500 (10/10) | Loss: 0.5695 | 0.33 steps/s | Step: 295k | }
.
.
{| Epoch: 100/2500 (10/10) | Loss: 0.3283 | 0.35 steps/s | Step: 296k | }
```
The program appears to be using between 20-50% of my Nvidia GeForce GTX 1060 6GB & 19-35% of my i5-6600k.  Thanks for the help - this approach seems to result in training at the same speed as [ori-pixel's](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-666851411) CPU approach with a i5-4690k, if each step = an epoch.  I can easily get 400 epochs overnight- I'm thrilled that it's working now; I think I'll still give Ubuntu a try.",thanks got around trying initially got memory error memory tried allocate mib gib total capacity gib already mib free gib reserved total decrease batch size seem effect left highest value give without running memory timed training duration first took console found batch size learning rate epoch loss step epoch loss step program thanks help approach result training speed approach step epoch easily get working think still give try,issue,positive,positive,positive,positive,positive,positive
781576176,@zxmanxz Can you let me know if the multi-GPU branch above works for you?,let know branch work,issue,negative,neutral,neutral,neutral,neutral,neutral
781302741,"Please try a normal Python install instead of Anaconda.
The Windows setup instructions are here: https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542",please try normal python install instead anaconda setup,issue,negative,positive,positive,positive,positive,positive
781199632,"It has been over a year with no real progress on this item. I'm closing it. A binary release is still a good idea, but it is not a goal for this repo at this time.

In the meantime, we've made numerous improvements which improve access and make setup much easier:
1. NVIDIA GPU no longer required (#366)
2. Make webrtcvad optional (#375)
3. Remove Tensorflow as dependency (#472)
4. Detailed Windows install instructions (https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542)",year real progress item binary release still good idea goal time made numerous improve access make setup much easier longer make optional remove dependency detailed install,issue,positive,positive,positive,positive,positive,positive
781130738,"@woodrow73 As a workaround, can you try setting `num_workers=0` in this part of train.py? This makes the DataLoader use the main python process, which will be slower.

https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/9a35b3e022d184e7ed2e7ce612413d765f50eb67/synthesizer/train.py#L146-L151

References:
* https://github.com/fatchord/WaveRNN/issues/207
* https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813",woodrow try setting part use main python process,issue,negative,positive,positive,positive,positive,positive
780970989,"Confirming an identical error without a virtual environment.  For a workaround I attempted to coerce CPU usage over GPU - short of reinstalling CUDA (since changing file names & environment variables didn't do the trick - maybe it's due to the native pytorch files), I did try manually changing the 10 instances in the repository where torch.cuda.is_available() to False, but .pyc files might make that approach moot; same error message.

Gonna try running with your Ubuntu 20.04 instructions",confirming identical error without virtual environment coerce usage short since file environment trick maybe due native try manually repository false might make approach moot error message gon na try running,issue,negative,negative,negative,negative,negative,negative
780919075,"You're getting that error message because Anaconda does not include the VC runtime DLL, and it's needed to use Pytorch. Get it from the download link here: https://github.com/pytorch/pytorch/issues/32916#issuecomment-611868780",getting error message anaconda include use get link,issue,negative,neutral,neutral,neutral,neutral,neutral
780898977,"Same issue reported here, but without Anaconda. https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/472#issuecomment-721874292

@rallandr Did you ever find a solution to the pickle problem when training on Windows?",issue without anaconda ever find solution pickle problem training,issue,negative,neutral,neutral,neutral,neutral,neutral
780838687,Closing due to inactivity. #472 has been merged and this project no longer depends on an outdated Tensorflow.,due inactivity project longer outdated,issue,negative,negative,negative,negative,negative,negative
780838048,Closing stale pull request. I invite a new PR to be opened which provides good step-by-step instructions like those suggested in https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542 .,stale pull request invite new good like,issue,positive,positive,positive,positive,positive,positive
780836969,"The model cannot perfectly predict spectrograms without overfitting, so the loss will converge to a nonzero number. In my experience it is around 0.4 for LibriSpeech.

0.4 is the summation of L2 loss from decoder and postnet mels, and L1 loss of the decoder mels. You can see it in the code here: https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/10ca8f7c785707f21c78cfe858a97841c1d875ba/synthesizer/train.py#L179-L184

Results will get better with: more data + less noise + consistent prosody + consistent volume + consistent recording conditions

You can also experiment with adjusting the network size but that will require training a model from scratch.",model perfectly predict without loss converge nonzero number experience around summation loss loss see code get better data le noise consistent prosody consistent volume consistent recording also experiment network size require training model scratch,issue,positive,positive,positive,positive,positive,positive
780833255,"Thanks for reporting this issue @woodrow73 . Please try and see if this is reproducible with a normal Python installation, instead of Anaconda. Reference the [Windows install instructions](https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542) if needed. There are several bugs which seem specific to conda (#644, #646) and we don't have enough developer support to squash them.",thanks issue woodrow please try see reproducible normal python installation instead anaconda reference install several seem specific enough developer support squash,issue,positive,positive,neutral,neutral,positive,positive
780830541,"@albert3-3 Did you solve the issue? If yes, what fixed the problem for you?",solve issue yes fixed problem,issue,negative,positive,neutral,neutral,positive,positive
780357539,"Here is a brief overview of the 3 parts that make up SV2TTS. The repo is organized around this structure.
1. **Speaker encoder.** This converts a short audio into a mathematical representation of the voice. We call this an utterance embedding. In this repo, it's a numpy array of shape (256,) composed of floats between 0 and 1.
2. **Modified Tacotron synthesizer.** The utterance embedding from the speaker encoder is concat with the output of Tacotron's text encoder as it enters the decoder. For SV2TTS, Tacotron is trained on a large multi-speaker dataset that helps it learn the relationship between utterance embedding and features in the mel spectrogram that it generates as output. Once the model is trained, it can perform zero-shot voice cloning on voices that are unseen during training.
3. **Vocoder.** This converts the mel spectrograms to audio waveforms that we perceive as speech.

I don't have time to answer further questions, but for more information read the papers and Corentin's thesis linked in README.md. I have some audio samples on my [webpage](https://blue-fish.github.io/experiments/RTVC-7.html) so you can hear how well it works on our pretrained models.",brief overview make organized around structure speaker short audio mathematical representation voice call utterance array shape composed synthesizer utterance speaker output text trained large learn relationship utterance mel spectrogram output model trained perform voice unseen training mel audio perceive speech time answer information read thesis linked audio hear well work,issue,negative,positive,neutral,neutral,positive,positive
780243476,"Thanks for the feedback. We added the model check in July 2020 (#416) so they wouldn't see the error message if they were using an older version of the software.

Please ask for help if you're still stuck on what needs to be done next.",thanks feedback added model check would see error message older version please ask help still stuck need done next,issue,negative,positive,positive,positive,positive,positive
780241273,"@zxmanxz Try this branch for multi-GPU training. If it works I will submit a pull request.
https://github.com/blue-fish/Real-Time-Voice-Cloning/tree/664_multi_gpu_training",try branch training work submit pull request,issue,negative,neutral,neutral,neutral,neutral,neutral
780239757,"I think that would help the overall design and be more user friendly if that's what you're asking.

However, I watched a tutorial, did everything they did exactly with no errors and then they ran the demo and it worked and opened the GUI then afterwards they moved the models.
",think would help overall design user friendly however watched tutorial everything exactly ran worked afterwards,issue,positive,positive,positive,positive,positive,positive
780233790,"There is an error but we might need to draw more attention to it. Would it help if the output looked like this instead?
```
(voice-clone) S:\path\path\path\path\Real-Time-Voice-Cloning-master>python demo_toolbox.py
S:\path\path\path\path\Real-Time-Voice-Cloning-master\encoder\audio.py:13: UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended.
warn(""Unable to import 'webrtcvad'. This package enables noise removal and is recommended."")
Arguments:
datasets_root: None
enc_models_dir: encoder\saved_models
syn_models_dir: synthesizer\saved_models
voc_models_dir: vocoder\saved_models
cpu: False
seed: None
no_mp3_support: False

********************************************************************************
Error: Model files not found. Follow these instructions to get and install the models:
https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models
********************************************************************************
```",error might need draw attention would help output like instead python unable import package noise removal warn unable import package noise removal none false seed none false error model found follow get install,issue,negative,negative,negative,negative,negative,negative
780101635,"Thank you, if there would be the way to parallel computation between many GPU, it would be great. ",thank would way parallel computation many would great,issue,positive,positive,positive,positive,positive,positive
780089294,"To increase VRAM usage, adjust the batch size parameter (far right number) in hparams. https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/10ca8f7c785707f21c78cfe858a97841c1d875ba/synthesizer/hparams.py#L52-L57",increase usage adjust batch size parameter far right number,issue,negative,positive,positive,positive,positive,positive
780078318,"It works fine with single GPU, mb you can give me an advise how to get full GPU usage (e.g. now it just using 4 GB and the other 7 are available)",work fine single give advise get full usage available,issue,negative,positive,positive,positive,positive,positive
780049344,"If you get an identical message on a single GPU, then something is wrong because it shouldn't be executing the multi-GPU code.

Why don't you try setting CUDA_VISIBLE_DEVICES inside `synthesizer_train.py`? (This file is in the root of the repo, unlike train.py) See https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/489#issuecomment-673358005 . Then run the original, unmodified code. Paste the error message if you get one.",get identical message single something wrong code try setting inside file root unlike see run original unmodified code paste error message get one,issue,negative,negative,neutral,neutral,negative,negative
780011227,"And paste where?
Also I tried to set CUDA_VISIBLE_DEVICES=0 (to get only one GPU) but problem was the same...",paste also tried set get one problem,issue,negative,neutral,neutral,neutral,neutral,neutral
779970910,"My ability to help with this is limited, since I don't have a server with multiple GPUs to test.

Let's see if the data_parallel_workaround is not required. In synthesizer/train.py, try copying the code from line 177 over to line 174.
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/10ca8f7c785707f21c78cfe858a97841c1d875ba/synthesizer/train.py#L172-L177",ability help limited since server multiple test let see try code line line,issue,negative,negative,neutral,neutral,negative,negative
779942749,"This was first reported in #658. Are you using Python 3.9? If so, try switching to Python 3.8.",first python try switching python,issue,negative,positive,positive,positive,positive,positive
779799051,"Tacotron has separate modes for training and inference. Recall that the decoder is autoregressive: the previous mel frame is used to predict the next one. The decoder runs in teacher-forcing mode for training, where the previous frames come from the ground truth. This ensures the predicted mel has the same shape as the ground truth, allowing loss to be calculated. The use of ground truth in teacher forcing makes the model appear better than it really is. The audio in `<model-dir>/wavs` comes from teacher forcing.

When synthesizing unseen text, Tacotron runs in inference mode. Since you don't have a ground truth, Tacotron feeds back its own mel outputs to the decoder. In the early stages of training a model, the decoder is not good. Any errors in output propagate to future frames and it eventually breaks down. This explains your 50/50 speech and silence that you observe for eval. It will get better with more steps.

For training, the lines in attention plots usually stop somewhere in the middle. This is because the text sequences and target spectrograms are padded to make use of batching. If the input text is padded, the line does not go to the top. If the spectrogram is padded, the line becomes horizontal before reaching the right side.

I am unavailable to answer further questions on this subject. Feel free to ask anyway in case someone else from the community can help. You are encouraged to read the papers (1712.05884, 1806.45558) and study the code.",separate training inference recall previous mel frame used predict next one mode training previous come ground truth mel shape ground truth loss calculated use ground truth teacher forcing model appear better really audio come teacher forcing unseen text inference mode since ground truth back mel early training model good output propagate future eventually speech silence observe get better training attention usually stop somewhere middle text target make use input text line go top spectrogram line becomes horizontal reaching right side unavailable answer subject feel free ask anyway case someone else community help read study code,issue,positive,positive,positive,positive,positive,positive
779763864,"I just found one interesting thing:
The alignment plots I posted here are from `<model-dir>/eval-dir/plots`. But when I look into the `<model-dir>/plots` there are plots created with the model backup, the plots saved there look much better and the loss is below 0.5. Even the wavs in `<model-dir>/wavs` sound fairly normally.

What is the difference between these folders, and what is the difference between the plots and wavs in those folders?",found one interesting thing alignment posted look model backup saved look much better loss even sound fairly normally difference difference,issue,positive,positive,positive,positive,positive,positive
779750075,"Hi @blue-fish , following your advice I managed to get the attention plots to show the desired line, however it still does not reach both corners and the loss started to diverge again:

<img width=""1356"" alt=""Screenshot 2021-02-16 at 11 31 42"" src=""https://user-images.githubusercontent.com/60513760/108050960-984ce880-704a-11eb-91e4-5fc8fc090884.png"">

`Step   41645 [1.985 sec/step, loss=0.42938, avg_loss=0.38850]`

I stripped the dataset of other languages, and left Czech only. Afterwards I tried to remove silences from the clips. I removed the silence using VAD, and I tried to strip everything before 95% of frames contained audio, and everything after 30% of frames were silent again. This way I might have spit some sentences in half if there was longer pause between words or because of a comma.

The eval-generated wavs contain huge amount of silence at the end, almost 50/50 speech/silence at the end. Is that normal?
Also, the speech sounds very much robotic.

So I am wondering whether the 40k steps are not enough to get things right and I should continue the training or it's just a waste of time?
Why even after the silence removal (and quite aggressive one) the attention line suddenly ends in 3/4 of the plot?
Might the encoder model be the key to this disaster ... with only 32k trained steps?",hi following advice get attention show desired line however still reach loss diverge step stripped left afterwards tried remove clip removed silence tried strip everything audio everything silent way might spit half longer pause comma contain huge amount silence end almost end normal also speech much wondering whether enough get right continue training waste time even silence removal quite aggressive one attention line suddenly plot might model key disaster trained,issue,negative,positive,neutral,neutral,positive,positive
779618981,"It is working now. Issue was with python 3.9, It is working fine with python 3.8 

Thanks..",working issue python working fine python thanks,issue,positive,positive,positive,positive,positive,positive
779617988,"You need to install visual c++ 14. Then install webrtcvad and ffmpeg audio. After all this, it works fine. it worked for me.  ",need install visual install audio work fine worked,issue,negative,positive,positive,positive,positive,positive
779522957,"Doesn't seem related to webrtcvad. Looks like an audio issue, resolution is https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/420#issuecomment-657154844",seem related like audio issue resolution,issue,negative,neutral,neutral,neutral,neutral,neutral
779493442,"I'll have to refer you to the detailed setup instructions for Windows: https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542

If you follow the instructions to the letter and it still doesn't work, feel free to ask for help there.",refer detailed setup follow letter still work feel free ask help,issue,positive,positive,positive,positive,positive,positive
779374030,Make sure you are running the command from the folder that contains demo_toolbox.py.,make sure running command folder,issue,negative,positive,positive,positive,positive,positive
779347597,"The following instructions pertain to the latest repo code, which uses the pytorch synthesizer in #472. We no longer support the tensorflow synth.

Training occurs in a nested loop in [synthesizer/train.py](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/synthesizer/train.py). You can break out of it when the loss is below your threshold.

```
for epoch in range(1, epochs+1):
    for i, (texts, mels, embeds, idx) in enumerate(data_loader, 1):
        # Loss calculation (line 184)
        loss = m1_loss + m2_loss + stop_loss

        # Loss window (line 197), tracks past 100 loss values
        loss_window.append(loss.item())

        """"""
        Add these lines to the end of the inside loop
        """"""
        if loss_window.average < training_stop_threshold:
            model.save(weights_fpath, optimizer)
            break
    """"""
    Add these lines to the end of the outside loop
    """"""
    if loss_window.average < training_stop_threshold:
        break
```",following pertain latest code synthesizer longer support training loop break loss threshold epoch range enumerate loss calculation line loss loss window line past loss add end inside loop break add end outside loop break,issue,negative,positive,neutral,neutral,positive,positive
779038099,Closing this issue. Colab notebook is no longer part of this repo. See #656 and #657,issue notebook longer part see,issue,negative,neutral,neutral,neutral,neutral,neutral
779036521,"Need more information. What is your operating system? Is there an error message in the terminal?

You can try our detailed setup guides for [Windows](https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542) and [Linux](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/615#issuecomment-740270185).",need information operating system error message terminal try detailed setup,issue,negative,positive,positive,positive,positive,positive
779033918,"Unable to reproduce issue on Ubuntu 20.04.
For reference, this is how I setup the toolbox from a fresh install: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/615#issuecomment-740270185
(It should work with the preinstalled Python 3.8 but I haven't had time to update and retest)

I think the issue is with your system configuration, but let us know if you have evidence that it's our code.",unable reproduce issue reference setup toolbox fresh install work python time update retest think issue system configuration let u know evidence code,issue,negative,negative,neutral,neutral,negative,negative
778804143,"I'm going to blame this on conda. Solution is to not use conda.
Please reopen this issue if the problem occurs in a normal Python environment.",going blame solution use please reopen issue problem normal python environment,issue,negative,positive,positive,positive,positive,positive
778710426,"I have gotten the avg_loss to 0.435 with a few thousand steps, but it is starting to just hover at that point. Have I hit the limit for refining this tool, or will adding more datasets from the same speaker improve results in other ways?  ",gotten thousand starting hover point hit limit refining tool speaker improve way,issue,negative,neutral,neutral,neutral,neutral,neutral
778672881,"More data is better. There is no harm in leaving the batch size at 32. More utterances means it requires more steps to complete the epoch.

If your speech data is high quality and you have at least 8 hours, consider training your synthesizer and vocoder models from scratch for best results. Finetuning is a shortcut for those who don't enough data, time, or audio quality to train a better model.",data better harm leaving batch size complete epoch speech data high quality least consider training synthesizer scratch best enough data time audio quality train better model,issue,positive,positive,positive,positive,positive,positive
778623893,"Thank you.

At the risk of straying slightly off topic, I have another question: The dataset I am currently using has about 450 utterances. I have more, but I am running into pesky Out Of Memory issues. A successful workaround was changing the training batches from 36 to 32.

I have far, far more utterances from the same speaker that I want to train the model on for single-speaker fine-tuning. Should I create another dataset and swap it out with the first one, and training the model on that? What other suggestions might you have, in the context of the OoM issue I have mentioned?",thank risk slightly topic another question currently running pesky memory successful training far far speaker want train model create another swap first one training model might context issue,issue,positive,positive,positive,positive,positive,positive
778604197,"Okay, thanks for sharing the knowledge. Will post an update about achieved results.",thanks knowledge post update,issue,negative,positive,positive,positive,positive,positive
778603944,"The model will be usable with 300 speakers. You can go to my webpage and compare results for RTVC-4 (1172 speakers) and RTVC-5 (109 speakers). https://blue-fish.github.io/experiments/RTVC-5.html

You can resume training on 1 language, but I suggest restarting from scratch and observing the process from the beginning.",model usable go compare resume training language suggest scratch observing process beginning,issue,negative,neutral,neutral,neutral,neutral,neutral
778601313,"Thanks for the reply! Did not know that, will try.
Unfortunately, the single language dataset contains only 36hrs of speech with 302 speakers, which is from my observations not enough to achieve good results thus training from scratch seems as a waste of time..?
I'm thinking of resuming the training but just with one language.. is there a chance this approach will work?",thanks reply know try unfortunately single language speech enough achieve good thus training scratch waste time thinking training one language chance approach work,issue,negative,positive,positive,positive,positive,positive
778599237,"The quality will improve somewhat if you clean your dataset for finetuning. But recognize that the base model is trained on imperfect data, and some of that will transfer. https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/364#issuecomment-660075042",quality improve somewhat clean recognize base model trained imperfect data transfer,issue,negative,negative,negative,negative,negative,negative
778598709,Train the synthesizer on a single language dataset. The different languages are confusing the attention mechanism. Alignment plots will look like that until attention has been learned (10-25k steps). It is important to use clean audio with consistent quality between speakers.,train synthesizer single language different attention mechanism alignment look like attention learned important use clean audio consistent quality,issue,positive,positive,positive,positive,positive,positive
778215970,"I see, thank you. 

I notice on Audacity that several of my utterances contain inhalation after some words or phrases. Some also have long periods of silence between phrases or words. 

To avoid awkward silences and other potential sources of noise and artifacts, should I clean these things up from the audio files before creating and training the datasets? Thank you!",see thank notice audacity several contain inhalation also long silence avoid awkward potential noise clean audio training thank,issue,positive,negative,neutral,neutral,negative,negative
777310430,"@Garvit-32
The main reason to use Taco1 is that it uses the same codebase with the vocoder (fatchord/WaveRNN). The commonality makes it a lot easier to write the training script and integrate it with the rest of the repo. Now that the supporting infrastructure is in place, the model can be switched with relative ease.

I've already shared my thoughts on Taco1 vs Taco2 in https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/472#issuecomment-769914574 . They're close in performance. I prefer Taco2.",main reason use commonality lot easier write training script integrate rest supporting infrastructure place model switched relative ease already close performance prefer,issue,positive,positive,positive,positive,positive,positive
777256627,"Hi @blue-fish Amazing Work !!
Can you tell me why you are using tacotron1 instead of tacotron2 ?  Have you tested tacotron2 ?",hi amazing work tell instead tested,issue,positive,positive,positive,positive,positive,positive
777245954,"Probably not. The vocoder is responsible for most of the noise and artifacts. (Switch to Griffin-Lim and you'll get clean output, though distorted.)",probably responsible noise switch get clean output though distorted,issue,negative,positive,positive,positive,positive,positive
777217036,"Hey there,

First of all, thanks for putting all of this together :D 

I'm currently trying to install on Windows, and I'll let you know if I run into errors.

So far I only got a bit confused by `Ignoring numpy: markers 'platform_system != ""Windows""' don't match your environment` during requirement.txt installation. After a bit of useless googling, I actually read the requirements.txt file and realized that this is expected haha.

Cheers!

Edit: Got it all installed and working, no hiccups 😄 I didn't try setting up GPU. Thanks",hey first thanks together currently trying install let know run far got bit confused match environment installation bit useless actually read file edit got working try setting thanks,issue,negative,negative,neutral,neutral,negative,negative
776917605,"That was it, I tried on my roommate's pc with a GTX 1060 and it worked with no problems, thanks",tried roommate worked thanks,issue,negative,positive,positive,positive,positive,positive
776738157,"Thank you so much for your help! I now know I have an option when deciding how to go about the vocoder. The 1159k model you've shared is better.

I have one last question: Will hardcoding all utterances to a single embedding reduce noise and artifacts? If so, how do I do that?",thank much help know option go model better one last question single reduce noise,issue,positive,positive,positive,positive,positive,positive
776492528,"I wrote #437 and do not recommend finetuning the vocoder. The quality doesn't seem to improve. Instead, try using my 1159k vocoder model for slightly better quality. https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/126#issuecomment-650531957

But if you insist, here's an easy way to work around your problem. Copy `SV2TTS/synthesizer/train.txt` to `SV2TTS/vocoder/synthesized.txt`. Also copy the `SV2TTS/synthesizer/mels` folder to `SV2TTS/vocoder/mels_gta`. This copies your ground truth (GT) mels to the area where the vocoder expects ground truth-aligned (GTA) mels.

",wrote recommend quality seem improve instead try model slightly better quality insist easy way work around problem copy also copy folder ground truth area ground,issue,positive,positive,positive,positive,positive,positive
776484852,"Tensorflow 1.x is not compatible with NVIDIA 30xx GPUs.
I have a pull request to remove this dependency, it is in the final stages of review.
It's already released in my fork if you don't want to wait.
https://github.com/blue-fish/Real-Time-Voice-Cloning
",compatible pull request remove dependency final review already fork want wait,issue,negative,neutral,neutral,neutral,neutral,neutral
776369749,"@iFreddie Thanks for reporting the issue. Most likely your GPU is too old to use the latest pytorch.
First uninstall pytorch:
```
pip uninstall pytorch
```
Then install an older pytorch:
```
pip install torch==1.2 -f https://download.pytorch.org/whl/torch_stable.html
```

If it still doesn't work, then reinstall a CPU-only pytorch using the pip command in https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-774537958",thanks issue likely old use latest first pip install older pip install still work reinstall pip command,issue,negative,positive,positive,positive,positive,positive
776317470,"Hi! I've followed your precious guide...i've also followed other guides before but everytime I upload my track I have this error 
""RuntimeError: CUDA error: no kernel image is available for execution on the device""
Can you please help me?",hi precious guide also track error error kernel image available execution device please help,issue,positive,positive,positive,positive,positive,positive
775063427,I haven't changed the default batch size but taking an even larger batch size is causing memory error resulting in process killed. Could you suggest something to improve the results as it is generating same output for every input?,default batch size taking even batch size causing memory error resulting process could suggest something improve generating output every input,issue,negative,neutral,neutral,neutral,neutral,neutral
774984599,@blue-fish Thanks a lot! It was the RAM. Now it is working.,thanks lot ram working,issue,negative,positive,positive,positive,positive,positive
774963380,"Wow, amazing work. I'll do my best to find the time to review within this week.",wow amazing work best find time review within week,issue,positive,positive,positive,positive,positive,positive
774716482,"Development of the pytorch synthesizer is complete. Please review the changes.

The [pretrained model release](https://github.com/blue-fish/Real-Time-Voice-Cloning/releases/tag/v1.0) consists of the pretrained encoder, along with synthesizer and vocoder models I have developed. Audio samples and model details: https://blue-fish.github.io/experiments/RTVC-7.html",development synthesizer complete please review model release along synthesizer audio model,issue,negative,positive,neutral,neutral,positive,positive
774537958,"@mrmscmike Thanks for testing. It looks like a memory problem. How much RAM do you have? You can try an older version of torch. (For a smaller download, use the +cpu version of the below command if you don't have a GPU.)

```
pip install torch==1.2 -f https://download.pytorch.org/whl/torch_stable.html
pip install torch==1.2+cpu -f https://download.pytorch.org/whl/torch_stable.html
```",thanks testing like memory problem much ram try older version torch smaller use version command pip install pip install,issue,negative,positive,positive,positive,positive,positive
774531555,"@blue-fish Thank you for the #642 instructions.
I get the following error on a clean win 10:
```
(venv) C:\Real-Time-Voice-Cloning-master>pip install torch -f https://download.pytorch.org/whl/torch_stable.html
Looking in links: https://download.pytorch.org/whl/torch_stable.html
Collecting torch
  Downloading https://download.pytorch.org/whl/cu110/torch-1.7.1%2Bcu110-cp37-cp37m-win_amd64.whl (2050.2 MB)
     |████████████████████████████████| 2050.2 MB 6.4 MB/s eta 0:00:01ERROR: Exception:
Traceback (most recent call last):
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_vendor\resolvelib\resolvers.py"", line 171, in _merge_into_criterion
    crit = self.state.criteria[name]
KeyError: 'torch'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_internal\cli\base_command.py"", line 189, in _main
    status = self.run(options, args)
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_internal\cli\req_command.py"", line 178, in wrapper
    return func(self, options, args)
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_internal\commands\install.py"", line 317, in run
    reqs, check_supported_wheels=not options.target_dir
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_internal\resolution\resolvelib\resolver.py"", line 122, in resolve
    requirements, max_rounds=try_to_avoid_resolution_too_deep,
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_vendor\resolvelib\resolvers.py"", line 453, in resolve
    state = resolution.resolve(requirements, max_rounds=max_rounds)
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_vendor\resolvelib\resolvers.py"", line 318, in resolve
    name, crit = self._merge_into_criterion(r, parent=None)
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_vendor\resolvelib\resolvers.py"", line 173, in _merge_into_criterion
    crit = Criterion.from_requirement(self._p, requirement, parent)
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_vendor\resolvelib\resolvers.py"", line 82, in from_requirement
    if not cands:
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_vendor\resolvelib\structs.py"", line 124, in __bool__
    return bool(self._sequence)
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_internal\resolution\resolvelib\found_candidates.py"", line 143, in __bool__
    return any(self)
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_internal\resolution\resolvelib\found_candidates.py"", line 38, in _iter_built
    candidate = func()
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_internal\resolution\resolvelib\factory.py"", line 169, in _make_candidate_from_link
    name=name, version=version,
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_internal\resolution\resolvelib\candidates.py"", line 306, in __init__
    version=version,
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_internal\resolution\resolvelib\candidates.py"", line 144, in __init__
    self.dist = self._prepare()
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_internal\resolution\resolvelib\candidates.py"", line 226, in _prepare
    dist = self._prepare_distribution()
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_internal\resolution\resolvelib\candidates.py"", line 312, in _prepare_distribution
    self._ireq, parallel_builds=True,
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_internal\operations\prepare.py"", line 457, in prepare_linked_requirement
    return self._prepare_linked_requirement(req, parallel_builds)
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_internal\operations\prepare.py"", line 482, in _prepare_linked_requirement
    self.download_dir, hashes,
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_internal\operations\prepare.py"", line 234, in unpack_url
    hashes=hashes,
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_internal\operations\prepare.py"", line 108, in get_http_url
    from_path, content_type = download(link, temp_dir.path)
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_internal\network\download.py"", line 163, in __call__
    for chunk in chunks:
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_internal\cli\progress_bars.py"", line 159, in iter
    for x in it:
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_internal\network\utils.py"", line 88, in response_chunks
    decode_content=False,
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_vendor\urllib3\response.py"", line 576, in stream
    data = self.read(amt=amt, decode_content=decode_content)
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_vendor\urllib3\response.py"", line 519, in read
    data = self._fp.read(amt) if not fp_closed else b""""
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_vendor\cachecontrol\filewrapper.py"", line 65, in read
    self._close()
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_vendor\cachecontrol\filewrapper.py"", line 52, in _close
    self.__callback(self.__buf.getvalue())
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_vendor\cachecontrol\controller.py"", line 309, in cache_response
    cache_url, self.serializer.dumps(request, response, body=body)
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_vendor\cachecontrol\serialize.py"", line 72, in dumps
    return b"","".join([b""cc=4"", msgpack.dumps(data, use_bin_type=True)])
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_vendor\msgpack\__init__.py"", line 35, in packb
    return Packer(**kwargs).pack(o)
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_vendor\msgpack\fallback.py"", line 960, in pack
    self._pack(obj)
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_vendor\msgpack\fallback.py"", line 944, in _pack
    len(obj), dict_iteritems(obj), nest_limit - 1
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_vendor\msgpack\fallback.py"", line 1045, in _pack_map_pairs
    self._pack(v, nest_limit - 1)
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_vendor\msgpack\fallback.py"", line 944, in _pack
    len(obj), dict_iteritems(obj), nest_limit - 1
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_vendor\msgpack\fallback.py"", line 1044, in _pack_map_pairs
    self._pack(k, nest_limit - 1)
  File ""c:\real-time-voice-cloning-master\venv\lib\site-packages\pip\_vendor\msgpack\fallback.py"", line 896, in _pack
    return self._buffer.write(obj)
MemoryError
```
",thank get following error clean win pip install torch looking link torch eta exception recent call last file line name handling exception another exception recent call last file line status file line wrapper return self file line run file line resolve file line resolve state file line resolve name file line requirement parent file line file line return bool file line return self file line candidate file line file line file line file line file line file line return file line file line file line link file line chunk file line iter file line file line stream data file line read data amt else file line read file line file line request response file line return data file line return packer file line pack file line file line file line file line file line return,issue,positive,positive,positive,positive,positive,positive
774503569,@likith1337 please do! It would help the install process greatly.,please would help install process greatly,issue,positive,positive,positive,positive,positive,positive
774485092,"Just began to fix some issues by downloading necessary files to run datasets and Found out that only my GPU memory is being used when I tested the toolbox, Maybe that is what the GPU is for (GTX 1650)? Here is a screenshot from task manager

![gpu](https://user-images.githubusercontent.com/69638063/107120427-79e63080-6885-11eb-98fb-05fb7d696b0c.png)

Anyway, I just want to say THANK YOU all Contributors and helpers for making such a fantastic project, and the fact that the community is still alive gives me some hope this will get better.

-Liam
",fix necessary run found memory used tested toolbox maybe task manager anyway want say thank making fantastic project fact community still alive hope get better,issue,positive,positive,positive,positive,positive,positive
774444110,I can do a complete run down video of the [#642 ](https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542) instructions mentioned on a fresh windows virtual machine. If @Tomodachi94  agrees,complete run video fresh virtual machine,issue,negative,positive,positive,positive,positive,positive
774315976,"The Poorly Documented guide is obsolete. Use this guide for Windows 10: https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542

Unless you're training a model, it's not worth the trouble to install Tensorflow 1.x with GPU support. Synthesizer inference is fast even on CPU.",poorly guide obsolete use guide unless training model worth trouble install support synthesizer inference fast even,issue,negative,negative,neutral,neutral,negative,negative
774294775,I'd suggest to follow this guide instead: https://poorlydocumented.com/2019/11/installing-corentinjs-real-time-voice-cloning-project-on-windows-10-from-scratch/,suggest follow guide instead,issue,negative,neutral,neutral,neutral,neutral,neutral
774249821,"Depends on the quality of training data. The number of steps of the pretrained models is a good target. (278k synthesizer, 428k vocoder) There is a learning curve to training. I recommend getting some experience with a proven dataset like LibriSpeech before changing the language.",quality training data number good target synthesizer learning curve training recommend getting experience proven like language,issue,positive,positive,positive,positive,positive,positive
774220974,"> 
> 
> Reduce the batch size:
> 
> https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/5425557efe30863267f805851f918124191e0be0/synthesizer/hparams.py#L243

one question. Do you know how much trained synthesizer and vocoder are ?",reduce batch size one question know much trained synthesizer,issue,negative,positive,positive,positive,positive,positive
774219719,"Thank you. I set the maximum values for 6 gigabytes of GPU memory
`tacotron_batch_size=20`",thank set maximum memory,issue,negative,neutral,neutral,neutral,neutral,neutral
774158622,"Hi, This is really really helpful! and I found out that maybe I don't have python at all because it now says this: `python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases.`  thanks.",hi really really helpful found maybe python python found run without install store disable manage execution thanks,issue,positive,positive,positive,positive,positive,positive
773873436,You need much more steps with reasonable batch size to get understandable speech.,need much reasonable batch size get understandable speech,issue,negative,positive,positive,positive,positive,positive
773870866,"You may need bigger dataset, but it's not really important. The main problem is probably that your batch size is quite low. If your batch size is lower than default then it will take longer to train understandable model.",may need bigger really important main problem probably batch size quite low batch size lower default take longer train understandable model,issue,negative,positive,positive,positive,positive,positive
773707524,"Yeah, screenshots are a good idea, or at least text examples of input/output.",yeah good idea least text,issue,positive,positive,positive,positive,positive,positive
773497694,"@blue-fish You might want to add some screenshots for step 6 on editing the environment variables to add additional clarification.  You might want to add a step 14 at the end to show users how to create a shortcut on their desktop for the demo_toolbox so they can launch from there without having to open up a cmd prompt.  Other than that, it worked great!",might want add step environment add additional clarification might want add step end show create launch without open prompt worked great,issue,positive,positive,positive,positive,positive,positive
773446548,"@bigdog3604 
Before you leave, please provide feedback on the setup process.
Are there any pain points?
Do the instructions need to be more clear?",leave please provide feedback setup process pain need clear,issue,negative,positive,positive,positive,positive,positive
773445777,"@bigdog3604
Yes, what you're asking is possible. It is why we recommend a virtual environment.

First, type `deactivate` to stop your venv if it is active.
Then follow Step 7 to make a separate virtual environment for the new repo.
Now you can safely install the requirements for the new repository.",yes possible recommend virtual environment first type deactivate stop active follow step make separate virtual environment new safely install new repository,issue,positive,positive,positive,positive,positive,positive
773428521,"One last question - is it possible to download and utilize another Voice Cloning project such as: 

https://github.com/NVIDIA/tacotron2

without screwing up the existing Corentin3 program?  I don't want to install tacotron2 only to find out it ""breaks"" my working program due to different python versions and/or other module requirements.",one last question possible utilize another voice project without screwing program want install find working program due different python module,issue,negative,negative,neutral,neutral,negative,negative
772959357,"I restarted the PC and reset my Internet connection and still couldn't download the archive version of the RTVC - stops at about 90%.  I don't see Norton blocking the site so I'm not sure what's going on there.  I was able to move the file to Google Drive and grab it from there.  **_I couldn't believe it when the program actually worked for me!_**  I was able to use the CLI and toolbox to make some clones.  I don't know how many hours I've spent trying to get the old version to work only to give up after many attempts and having it always error out for some unknown reason.  **_@blue-fish and @Tomodachi94 deserve a lot of credit for getting this thing to work._**  I'm sure there are many other users who spent many hours messing with this only to give up and consider literally banging their head against the wall.  Great job, guys!  Now, if you could make some of the other Tacotron GitHubs actually install and work, you would be amazing.  **_It seems every one of these real time voice cloning GitHub projects looks great on paper, but, many people like myself, without a real python background, can't ever get them to install without many incompatibilities between the various pieces of software._**   ",reset connection still could archive version see blocking site sure going able move file drive grab could believe program actually worked able use toolbox make know many spent trying get old version work give many always error unknown reason deserve lot credit getting thing sure many spent many messing give consider literally banging head wall great job could make actually install work would amazing every one real time voice great paper many people like without real python background ca ever get install without many various,issue,positive,positive,positive,positive,positive,positive
772891846,"Will do.

On Wed, Feb 3, 2021 at 3:13 PM blue-fish <notifications@github.com> wrote:

> The download link completes for me, so I think it's an issue with your
> internet connection.
> Try restarting your computer, and waiting a while if it still doesn't work.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-772891261>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AQ7F7S77MCUGCFLIN7UFYETS5HKAPANCNFSM4WQIJB6Q>
> .
>
-- 
-Ed-
",wed wrote link think issue connection try computer waiting still work reply directly view,issue,negative,positive,neutral,neutral,positive,positive
772891261,"The download link completes for me, so I think it's an issue with your internet connection.
Try restarting your computer, and waiting a while if it still doesn't work.",link think issue connection try computer waiting still work,issue,negative,neutral,neutral,neutral,neutral,neutral
772888628,"I tried to download the archive version of the RTVC via the above GitHub link but the download progress bar stops at around 90% and never goes any farther.  I tried the link on two different computers and get the same result from both of them.

![Snag_142e958](https://user-images.githubusercontent.com/71196619/106821080-5cb92400-6631-11eb-926d-9ff9bb20b313.png)
",tried archive version via link progress bar around never go farther tried link two different get result,issue,negative,neutral,neutral,neutral,neutral,neutral
772822168,"Sorry. 
Maybe I made a mistake and the GPU does everything much faster than the CPU .
And the load on the CPU is due to reading and writing .
[this is a snapshot of my system](https://drive.google.com/file/d/1jj_nRANPfeNE5Ytsg5lVEwwiVJ3D5ax7/view?usp=sharing)
",sorry maybe made mistake everything much faster load due reading writing snapshot system,issue,negative,negative,negative,negative,negative,negative
772781186,"I have not received an error message. 
I double-checked what I saved
`device = torch.device (""cuda"")`
after launch
`python encoder_train.py my_ru <datasets_root> / SV2TTS / encoder`
 only CPU works.
",received error message saved device launch python work,issue,negative,neutral,neutral,neutral,neutral,neutral
772774312,"Glad you figured it out, but I recommend using the latest version of the repo. There are a bunch of improvements like CPU support and saving audio files. It also does not require `webrtcvad` which should make setup easier.",glad figured recommend latest version bunch like support saving audio also require make setup easier,issue,positive,positive,positive,positive,positive,positive
772771968,"Change this line: https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/5425557efe30863267f805851f918124191e0be0/encoder/train.py#L30

To: `device = torch.device(""cuda"")`

And let me know which error message you get.",change line device let know error message get,issue,negative,neutral,neutral,neutral,neutral,neutral
772770492,"![Snag_83bb82](https://user-images.githubusercontent.com/71196619/106799511-33d66600-6614-11eb-98ad-1da0650bdebc.png)

I got the -r requirements.txt to run by eliminating the instp at the front of the tensorflow_gpu entry (top line).",got run front entry top line,issue,negative,positive,positive,positive,positive,positive
772769038,"

> To check if you have GPU support, start Python and type:
> 
> ```
> import torch
> torch.cuda.is_available()
> ```
> 
> If it returns True, you have GPU support and it should work automatically. We check `torch.cuda.is_available()` at runtime and use the GPU if Pytorch says it is available.
> 
> If it returns False, then you do not have GPU support. If that is the case you'll have to go elsewhere for help setting up Pytorch for CUDA support.

thank you.
`torch.cuda.is_available ()` outputs True. But when teaching the encoder, my GPU does not work, but the CPU works. Probably `python encoder_train.py my_ru <datasets_root> / SV2TTS / encoder` does not have the function to use GPU",check support start python type import torch true support work automatically check use available false support case go elsewhere help setting support thank true teaching work work probably python function use,issue,positive,positive,positive,positive,positive,positive
772759625,"@bigdog3604
That error message indicates you have an old version of this repo.
Get the latest Real-Time-Voice-Cloning code in step 1.
You should not need to repeat steps 3-7.",error message old version get latest code step need repeat,issue,negative,positive,positive,positive,positive,positive
772757889,"To check if you have GPU support, start Python and type:
```
import torch
torch.cuda.is_available()
```

If it returns True, you have GPU support and it should work automatically. We check `torch.cuda.is_available()` at runtime and use the GPU if Pytorch says it is available.

If it returns False, then you do not have GPU support. If that is the case you'll have to go elsewhere for help setting up Pytorch for CUDA support.",check support start python type import torch true support work automatically check use available false support case go elsewhere help setting support,issue,positive,positive,positive,positive,positive,positive
772744548,"If you'd like to use
```webrtcvad```, I think it requires the 64 bit version of Python 3.7, but I could be confusing that with something else...",like use think bit version python could something else,issue,negative,neutral,neutral,neutral,neutral,neutral
772740744,"The ""Poorly Documented"" guide is obsolete given the recent usability improvements. It is hurting users more than it is helping now. We should stop linking to it in the README.",poorly guide obsolete given recent usability hurting helping stop linking,issue,negative,negative,negative,negative,negative,negative
772739665,"@bigdog3604 
Since #376, `webrtcvad` is optional and the toolbox will run without it.
Make sure you have downloaded the latest version of Real-Time-Voice-Cloning from this site.
Here's a download link: [master.zip](https://github.com/CorentinJ/Real-Time-Voice-Cloning/archive/master.zip)",since optional toolbox run without make sure latest version site link,issue,negative,positive,positive,positive,positive,positive
772735939,"> 
> 
> ParametricUMAP needs Tensorflow 2.0 or higher, but when the ImportError occurs it just tells the user to install Tensorflow.
> 
> This is fixed in [lmcinnes/umap@d599e59](https://github.com/lmcinnes/umap/commit/d599e59fe6a6fd5939a3fd7e868658edca1afff0) . With the latest code, I do not get a warning with Tensorflow 1.15 installed. For more information: [lmcinnes/umap#551](https://github.com/lmcinnes/umap/issues/551)
> 
> However, users will continue to see the warning until a new release of umap-learn is pushed to PyPI.

Hello. 
Sorry to bother you, I was wondering. 
How to run `python encoder_train.py my_ru <datasets_root>/SV2TTS/encoder` on the GPU ?",need higher user install fixed de latest code get warning information however continue see warning new release hello sorry bother wondering run python,issue,negative,positive,neutral,neutral,positive,positive
772722721,"I have a virgin Windows 10 laptop and would love to be one of your subjects to test out the new installation process.  Please let me know if that is possible.  I've spent two days trying to install this Github without success.  I believe it doesn't have anything to do with the way I downloaded the files, etc.  Let me know!",virgin would love one test new installation process please let know possible spent two day trying install without success believe anything way let know,issue,positive,positive,positive,positive,positive,positive
772717738,"I have been trying to install this program for over two days without any luck.  I've followed the ""Poorly Documented"" guide and believe I have followed all the steps.  I try to launch the CLI toolbox and get an error:  ModuleNotFound:  No module name 'webrtcvad'.  I go to the Python 3.7 lib folder and the module is there.  I would love to work with @tomodachi94 to test out his new process for Windows 10 users.  The existing process sucks.",trying install program two day without luck poorly guide believe try launch toolbox get error module name go python folder module would love work test new process process,issue,negative,positive,neutral,neutral,positive,positive
772690358,"I will start fine tuning the encoder in Arabic Language, what should i do after gathering data ( the data i have is in the form of wav files ) how can i make it in the shape so that i can preprocess it with the script here and what changes should i make in the pre processing script?",start fine tuning language gathering data data form make shape script make script,issue,negative,positive,positive,positive,positive,positive
772220419,@Tomodachi94 Are you still going to work on this? Please see my suggestions for the Windows install instructions if you haven't already.,still going work please see install already,issue,negative,neutral,neutral,neutral,neutral,neutral
771443122,"### Get files for Real-Time-Voice-Cloning
1. Download this file to get the latest version of Real-Time-Voice-Cloning: https://github.com/CorentinJ/Real-Time-Voice-Cloning/archive/master.zip
2. Extract it to `C:\` The files will be in a folder called Real-Time-Voice-Cloning-master

### Install Python
3. Download Python from here: https://www.python.org/ftp/python/3.7.9/python-3.7.9-amd64.exe
    * Warning: Other versions of Python may not work.
    * Warning: Do not install Python from the Windows Store.
4. Run the installer. Click the ""Install Now"" button. Do not change the defaults.
5. Exit the installer when complete. Do not disable the path length limit.
6. Edit the environment variables for your account. Add these locations to PATH. First is for python.exe, second is for pip.exe. Move these to the top to take precedence over any pre-existing Python installations.
```
%USERPROFILE%\AppData\Local\Programs\Python\Python37
%USERPROFILE%\AppData\Roaming\Programs\Python\Python37\Scripts
```

### Set up virtual environment and install required packages
7. Open Windows command prompt and run these commands to set up and activate a virtual environment. Real-Time-Voice-Cloning needs this dedicated environment as it uses obsolete packages that could cause conflicts with other Python programs.
```
cd C:\Real-Time-Voice-Cloning-master
python -m venv venv
venv\Scripts\activate.bat
```
8. Next, run these commands to install Python packages.
```
pip install --upgrade pip
pip install torch -f https://download.pytorch.org/whl/torch_stable.html
pip install -r requirements.txt
```

### Get FFmpeg
9. Download FFmpeg from here: https://github.com/BtbN/FFmpeg-Builds/releases/download/autobuild-2021-10-31-12-23/ffmpeg-N-104454-gd92fdc7144-win64-lgpl.zip
10. Extract the zip file. Move `ffmpeg.exe` to `C:\Real-Time-Voice-Cloning-master` (the same folder as demo_toolbox.py)

### Pretrained models
11. Download pretrained models from: https://github.com/blue-fish/Real-Time-Voice-Cloning/releases/download/v1.0/pretrained.zip
12. Extract pretrained.zip. Move the included files to the below locations. See [here](https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-864340065) if you need more help on this step.
```
C:\Real-Time-Voice-Cloning-master\encoder\saved_models\pretrained.pt
C:\Real-Time-Voice-Cloning-master\synthesizer\saved_models\pretrained\pretrained.pt
C:\Real-Time-Voice-Cloning-master\vocoder\saved_models\pretrained\pretrained.pt
```

### Launching the toolbox
13. Open Windows command prompt and type:
```
cd C:\Real-Time-Voice-Cloning-master
venv\Scripts\activate.bat
python demo_toolbox.py
```",get file get latest version extract folder install python python warning python may work warning install python store run installer click install button change exit installer complete disable path length limit edit environment account add path first second move top take precedence python set virtual environment install open command prompt run set activate virtual environment need environment obsolete could cause python python next run install python pip install upgrade pip pip install torch pip install get extract zip file move folder extract move included see need help step toolbox open command prompt type python,issue,negative,positive,positive,positive,positive,positive
771043077,Conda may be causing this problem. Try the latest Python 3.7 from https://www.python.org/downloads/windows/,may causing problem try latest python,issue,negative,positive,positive,positive,positive,positive
770895214,"Someone else is going to need to help with the collab notebook if you're going that route.

It's not easy, but you will probably have a better experience running this code from a normal Python environment. It doesn't require a GPU so most hardware is supported. Here are setup instructions for [Windows](https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542), [macOS](https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-766312100), and [Linux](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/615#issuecomment-740270185).",someone else going need help notebook going route easy probably better experience running code normal python environment require hardware setup,issue,positive,positive,positive,positive,positive,positive
770846345,"![image](https://user-images.githubusercontent.com/75932124/106463086-fce34180-64a7-11eb-9c96-948cd6c7746e.png)
I copied all the code from there and this error came up:

> The code changes needed for librosa 0.8.0 are in #572.
> However, the collab notebook is not officially maintained or supported, so you will need to make the updates yourself. Please submit a pull request if you get it working.

",image copied code error came code however notebook officially need make please submit pull request get working,issue,negative,neutral,neutral,neutral,neutral,neutral
770620789,"Same issue reported in #641. You'll need to seek help in a different support channel (e.g. Stack Overflow) as the problem is not originating from this repo.

Easiest workaround is to install my version of the toolbox which does not use tensorflow: https://github.com/blue-fish/Real-Time-Voice-Cloning",issue need seek help different support channel stack overflow problem easiest install version toolbox use,issue,positive,neutral,neutral,neutral,neutral,neutral
770618155,"The code changes needed for librosa 0.8.0 are in #572.
However, the collab notebook is not officially maintained or supported, so you will need to make the updates yourself. Please submit a pull request if you get it working.",code however notebook officially need make please submit pull request get working,issue,negative,neutral,neutral,neutral,neutral,neutral
769914574,"Training is still in progress. Here are some observations:
* Tacotron 1 learns faster than Tacotron 2. Training time per step is about the same, but the model is usable in fewer steps. Voice quality is similar. Attention is more robust in Taco2.
* Voice quality improves when training on the full LibriSpeech dataset instead of the no-gaps subset described in #538. It sounds quite similar to Corentin's model even with a batch size of 12 instead of 144.
* Silence in the middle of training data causes long gaps, skipped words, and stuttering. I tried solving this by trimming silences with webrtcvad. This trims long gaps into very short gaps, which results in random pauses between words.",training still progress faster training time per step model usable voice quality similar attention robust voice quality training full instead subset quite similar model even batch size instead silence middle training data long stuttering tried trimming long short random,issue,negative,negative,neutral,neutral,negative,negative
769815524,"ParametricUMAP needs Tensorflow 2.0 or higher, but when the ImportError occurs it just tells the user to install Tensorflow.

This is fixed in https://github.com/lmcinnes/umap/commit/d599e59fe6a6fd5939a3fd7e868658edca1afff0 . With the latest code, I do not get a warning with Tensorflow 1.15 installed. For more information: https://github.com/lmcinnes/umap/issues/551 

However, users will continue to see the warning until a new release of umap-learn is pushed to PyPI.",need higher user install fixed latest code get warning information however continue see warning new release,issue,negative,positive,positive,positive,positive,positive
769786690,"> 
> 
> Does `encoder_train.py` run? This repo does not use ParametricUMAP, so the warning can be ignored. If the program is stopping, it is for a different reason.

thanks. 
The program works, I just don't understand why ""Tensorflow not installed"" .
 I installed Tensorflow this is visible in conda list",run use warning program stopping different reason thanks program work understand visible list,issue,negative,positive,neutral,neutral,positive,positive
769387799,"Does `encoder_train.py` run? This repo does not use ParametricUMAP, so the warning can be ignored. If the program is stopping, it is for a different reason.",run use warning program stopping different reason,issue,negative,neutral,neutral,neutral,neutral,neutral
768585756,"> Please Mention it as Python 3.7 and below
> As tensorflow=1.15 is not supported in 3.8 and higher

@likith1337 done. Anything else?",please mention python higher done anything else,issue,negative,positive,positive,positive,positive,positive
768316667,"That's a very good work, congrats. 
I don't know if I'm a the good place to post this but it give an american accent to the cloned voice although the speaker I want to clone have a British accent, is it the encoder, the synthesizer, the vocoder or the three ? Is there a way to change this without having a Nvidia Gpu to train the models ? Or is there already models trained with British accent available ? 
Also I noticed the pronunciation is wrong sometimes and it even miss totally some words, is there a way to change this ? Maybe it's due to the ponctuation no taken in account ?",good work know good place post give accent voice although speaker want clone accent synthesizer three way change without train already trained accent available also pronunciation wrong sometimes even miss totally way change maybe due taken account,issue,negative,positive,positive,positive,positive,positive
766630964,Tentatively closing this since this issue doesn't require attention at this time.,tentatively since issue require attention time,issue,negative,neutral,neutral,neutral,neutral,neutral
766630386,"@vinamramunot-tech It worked, and I expect to have a model for release within a week.

If you are available to help, it would be nice to add mixed precision training support. That will allow us to train better models on consumer GPUs with limited memory. If you have any interest in helping, leave a message in #487 and I'll let you know how you can get started.",worked expect model release within week available help would nice add mixed precision training support allow u train better consumer limited memory interest helping leave message let know get,issue,positive,positive,positive,positive,positive,positive
766535349,@blue-fish thank you!! Let me know if I can be of any help.,thank let know help,issue,positive,neutral,neutral,neutral,neutral,neutral
766438044,"May you add some maintainers to the repo, create an announcement and ask for help. It happened before with others repositories ",may add create announcement ask help,issue,positive,neutral,neutral,neutral,neutral,neutral
766434516,"If you want to hear the difference, we've made some comparisons: https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/472#issuecomment-687651582 , and https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/472#issuecomment-704377514

The current pytorch model will perform worse mainly because the network is smaller (15M parameters vs 28M for tensorflow). The next iteration of pytorch synthesizer will have comparable size and voice quality.",want hear difference made current model perform worse mainly network smaller next iteration synthesizer comparable size voice quality,issue,negative,negative,neutral,neutral,negative,negative
766429654,"Thank you for the help, your version works for me. Since it does not use TensorFlow, are there any differences in the performance of the engine compared to the other one? I figured this because it tells me
`UserWarning: Tensorflow not installed; ParametricUMAP will be unavailable`.",thank help version work since use performance engine one figured unavailable,issue,positive,neutral,neutral,neutral,neutral,neutral
766335841,"Never mind, I just had to install one of the libraries. I think it was the Librosa library, and just a simple pip install did the trick!
",never mind install one think library simple pip install trick,issue,negative,neutral,neutral,neutral,neutral,neutral
766324029,"Okay so I accidentally closed the environment and I made a new one pretty much the same way, but now I get these errors:
```
(voice-clone) PS C:\Users\anon\Desktop\Real-Time-Voice-Cloning-master> python .\demo_toolbox.py
C:\Users\anon\Desktop\Real-Time-Voice-Cloning-master\encoder\audio.py:13: UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended.
  warn(""Unable to import 'webrtcvad'. This package enables noise removal and is recommended."")
2021-01-24 03:19:15.272827: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found
2021-01-24 03:19:15.275594: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
C:\Users\anon\miniconda3\envs\voice-clone\lib\site-packages\umap\__init__.py:9: UserWarning: Tensorflow not installed; ParametricUMAP will be unavailable
  warn(""Tensorflow not installed; ParametricUMAP will be unavailable"")
Arguments:
    datasets_root:    None
    enc_models_dir:   encoder\saved_models
    syn_models_dir:   synthesizer\saved_models
    voc_models_dir:   vocoder\saved_models
    low_mem:          False
    seed:             None
    no_mp3_support:   False

Librosa will be unable to open mp3 files if additional software is not installed.
Please install ffmpeg or add the '--no_mp3_support' option to proceed without support for mp3 files.
(voice-clone) PS C:\Users\anon\Desktop\Real-Time-Voice-Cloning-master> print(""hello world"")
Unable to initialize device PRN
(voice-clone) PS C:\Users\anon\Desktop\Real-Time-Voice-Cloning-master> python
Python 3.7.9 (default, Aug 31 2020, 17:10:11) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from pathlib import Path
>>> Path(""encoder\saved_models"").is_dir()
True
>>> Path(""synthesizer\saved_models"").is_dir()
True
>>> Path(""vocoder\saved_models"").is_dir()
True
>>> exit()
```",accidentally closed environment made new one pretty much way get python unable import package noise removal warn unable import package noise removal could load dynamic library found ignore set machine unavailable warn unavailable none false seed none false unable open additional please install add option proceed without support print hello world unable initialize device python python default bit anaconda win type help copyright license information import path path true path true path true exit,issue,positive,negative,neutral,neutral,negative,negative
766312304,Let's write the install guides for the repo in its current state. Releases will come later.,let write install current state come later,issue,negative,neutral,neutral,neutral,neutral,neutral
766312100,"> Can someone contribute some MacOS X instructions?

The basic outline is:

1. Install Python 3.7 from https://www.python.org/downloads/mac-osx/
2. Install [homebrew](https://brew.sh)
3. `brew install ffmpeg`
4. Everything else is almost identical to Linux after that",someone contribute basic outline install python install brew install everything else almost identical,issue,negative,neutral,neutral,neutral,neutral,neutral
766309412,"@blue-fish most certainly publish ""releases"" in GitHub for every stable release.

~~Not sure about the legality of distributing the executable file though.~~

And don't worry about the install instructions too much, I'll get it mostly taken care of, I'll let you know if I need your assistance.",certainly publish every stable release sure legality executable file worry install much get mostly taken care let know need assistance,issue,positive,positive,positive,positive,positive,positive
766309000,"Awesome, thanks for helping write the install guide! This code gets downloaded a lot (over 1000 times/week) so any improvement to the install process will have a big impact.

If it would make the guide easier to write, I have the power to do things that simplify the install process. For example: we can publish a ""release"" zip file that bundles the code along with the model checkpoints and a standalone ffmpeg.exe. That eliminates the need to ""install ffmpeg"" or ""download pretrained models"". Let me know if interested.",awesome thanks helping write install guide code lot improvement install process big impact would make guide easier write power simplify install process example publish release zip file code along model need install let know interested,issue,positive,positive,positive,positive,positive,positive
766308530,"Whoops, meant to commit ""Remove Installation Instructions from README.md"" not ""Remove insta"".",whoop meant commit remove installation remove,issue,negative,neutral,neutral,neutral,neutral,neutral
766306936,"@blue-fish I would be pleased to help you with this.

I have access to both Windows and Ubuntu (via WSL), however testing on multiple environments will prevent error, so testing would be helpful.

TLDR: Yes, I am willing to help write instructions for Windows and could help with  Ubuntu instructions if needed.

Thank you for taking the time to respond!",would help access via however testing multiple prevent error testing would helpful yes willing help write could help thank taking time respond,issue,positive,positive,positive,positive,positive,positive
766305830,"> I personally think we should move install instructions to another file

Excellent suggestion! Let's replace the minimal instructions in the readme with a link to INSTALL.md. Are you willing to help write the instructions for Windows? I can provide feedback and testing.",personally think move install another file excellent suggestion let replace minimal link willing help write provide feedback testing,issue,positive,positive,positive,positive,positive,positive
766304551,"> I think # 639 is a less common situation where the person uses Python for multiple projects, but doesn't already utilize virtual environments. Usually users seem to fall into one of two categories:
Experienced enough with python to use virtualenv
Installing python for the first time to run this project only

This is true. 

> So now I'm questioning if the README needs to include instructions on setting up the virtual environment. Eventually, we'll have a binary release (.exe) that will greatly simplify setup. Considering everything here, I think we don't need the pull request. What do you think?

I personally think we should move install instructions to another file (as stated in my previous comment) but I think it should be **merged** as the install instructions should be current and working, not for the future or the past.

Again, apologies for the lengthly reply.",think le common situation person python multiple already utilize virtual usually seem fall one two experienced enough python use python first time run project true need include setting virtual environment eventually binary release greatly simplify setup considering everything think need pull request think personally think move install another file stated previous comment think install current working future past reply,issue,negative,positive,neutral,neutral,positive,positive
766304221,"Thanks, my computer has 8 GB ram ill try both of those options out.",thanks computer ram ill try,issue,negative,negative,negative,negative,negative,negative
766303654,"> ...the rest of the setup instructions is rather minimal. 

This is sad but true. I wouldn’t mind adding more details to the existing documentation.

————————————————————

> And this makes me think that we should be writing with a different audience in mind. The typical user doesn't have enough computer experience to use the README effectively, turning a 5-step process into a nightmare that lasts hours. 

This is true. Most people are used to a GUI, and find the Command Line scary and hard to use without experience.

————————————————————

> Maybe we should rewrite it to be more like: Windows users follow these friendly step-by-step instructions to set up the repo
Everyone else go to Issue 615 or see the wiki.

I disagree. Shouldn’t all instructions be in one place (file), perhaps an INSTALL.md?

————————————————————

> But it may not be possible to make Windows instructions that are robust like what we have for Ubuntu.

Most commands on Ubuntu have some variation on Windows (a major exception being Git).

Apologies for the long comment!",rest setup rather minimal sad true mind documentation think writing different audience mind typical user enough computer experience use effectively turning process nightmare true people used find command line scary hard use without experience maybe rewrite like follow friendly set everyone else go issue see disagree one place file perhaps may possible make robust like variation major exception git long comment,issue,positive,positive,neutral,neutral,positive,positive
766303470,"I don't know enough about your computer's configuration to provide next steps for resolving the tensorflow issue. If you're unable to solve it yourself, these are your remaining options:

1. Install Ubuntu 20.04 (to a virtual machine, or ""Try Ubuntu"" from a live USB if you have >= 16 GB RAM) and follow these [step-by-step instructions](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/615#issuecomment-740270185) which I've tested, or
2. Install my version of the toolbox which does not use tensorflow: https://github.com/blue-fish/Real-Time-Voice-Cloning
    * However, this is an unofficial version and I cannot provide any help for it.",know enough computer configuration provide next issue unable solve install virtual machine try live ram follow tested install version toolbox use however unofficial version provide help,issue,positive,negative,neutral,neutral,negative,negative
766301101,"I think #639 is a less common situation where the person uses Python for multiple projects, but doesn't already utilize virtual environments. Usually users seem to fall into one of two categories:
1. Experienced enough with python to use virtualenv
2. Installing python for the first time to run this project only

So now I'm questioning if the README needs to include instructions on setting up the virtual environment. Eventually, we'll have a [binary release](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/267) (.exe) that will greatly simplify setup. Considering everything here, I think we don't need the pull request. What do you think?",think le common situation person python multiple already utilize virtual usually seem fall one two experienced enough python use python first time run project need include setting virtual environment eventually binary release greatly simplify setup considering everything think need pull request think,issue,negative,positive,positive,positive,positive,positive
766300000,"@Tomodachi94 Thank you for submitting the PR. Your style is helpful and explanatory while the rest of the setup instructions is rather minimal. And this makes me think that we should be writing with a different audience in mind.

The typical user doesn't have enough computer experience to use the README effectively, turning a 5-step process into a nightmare that lasts hours. Maybe we should rewrite it to be more like:
1. Windows users follow these friendly step-by-step instructions to set up the repo
2. Everyone else go to Issue 615 or see the wiki.

But it may not be possible to make Windows instructions that are robust like what we have for Ubuntu: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/615#issuecomment-740270185",thank style helpful explanatory rest setup rather minimal think writing different audience mind typical user enough computer experience use effectively turning process nightmare maybe rewrite like follow friendly set everyone else go issue see may possible make robust like,issue,positive,positive,positive,positive,positive,positive
766296891,"Try running this command:
```
python -c ""import sys; print(sys.maxsize > 2**32)""
```

The result should be True, but if it returns False then you are on a 32-bit version of Python, and need to install a 64-bit version.

Some other troubleshooting scenarios are mentioned here: https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156
* This might be helpful if your CPU does not support AVX instructions: https://stackoverflow.com/questions/49932993

If none of this helps, you'll need to seek support elsewhere for this issue. You can also try [my fork](https://github.com/blue-fish/Real-Time-Voice-Cloning) for a tensorflow-free version of this repo.",try running command python import print result true false version python need install version might helpful support none need seek support elsewhere issue also try fork version,issue,positive,negative,neutral,neutral,negative,negative
766296198,"blue-fish, let me know if anything needs changing!",let know anything need,issue,negative,neutral,neutral,neutral,neutral,neutral
766290805,"~~Sure thing, want me to fork and PR @blue-fish?~~ Forking, opening PR.",thing want fork opening,issue,negative,neutral,neutral,neutral,neutral,neutral
766282201,"@Tomodachi94 This is a good suggestion, can you please write up a quick tutorial on how to set up a virtual environment in Windows?

If we can keep it simple, we'll add this to the project's README.md.",good suggestion please write quick tutorial set virtual environment keep simple add project,issue,positive,positive,positive,positive,positive,positive
766281866,"If you do any other work with Python you should use a virtual environment as suggested in #640. The setup instructions for Linux on the wiki and in #615 do this.

Look for a tutorial on the internet (""python virtualenv"") or watch #640 for updates, in case instructions are added.",work python use virtual environment setup look tutorial python watch case added,issue,negative,neutral,neutral,neutral,neutral,neutral
766281215,"Could be a permissions issue with the files or the directories. Also make sure capitalization matches the wiki page.

Try launching python from the Real-Time-Voice-Cloning folder and see if the [model checks](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/utils/modelutils.py) pass when manually run.

```
from pathlib import Path
Path(""encoder\saved_models"").is_dir()      # Should return True
Path(""synthesizer\saved_models"").is_dir()  # Should return True
Path(""vocoder\saved_models"").is_dir()      # Should return True
```",could issue also make sure capitalization page try python folder see model pas manually run import path path return true path return true path return true,issue,positive,positive,positive,positive,positive,positive
766280024,"I've already installed the pretrained models, and double checked to make sure all the files were in the right directories, so I'm not sure what's up with that... And thanks for responding so quickly!",already double checked make sure right sure thanks quickly,issue,positive,positive,positive,positive,positive,positive
766279800,"This is the key part of the error message:
```
Error: Model files not found. Follow these instructions to get and install the models:
https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models
```",key part error message error model found follow get install,issue,negative,neutral,neutral,neutral,neutral,neutral
765876017,@vinamramunot-tech Actually I just found something that should improve my pytorch model. I'll report back in a few days if it worked. https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/472#issuecomment-765875497,actually found something improve model report back day worked,issue,negative,neutral,neutral,neutral,neutral,neutral
765875497,"I notice that Rayhane's attention implementation masks text padding, while fatchord's does not. This seems like a plausible explanation for the attention difficulties. I'll try adding a mask and see if it improves the results.

Edit: This fixed it, attention is much more robust with the mask. Now training a model for release.",notice attention implementation text padding like plausible explanation attention try mask see edit fixed attention much robust mask training model release,issue,positive,positive,positive,positive,positive,positive
765860232,@blue-fish Although I am not knowledgeable but I can give it a try. Is there something that we can start on?,although knowledgeable give try something start,issue,negative,neutral,neutral,neutral,neutral,neutral
765806990,"The information above is still current. [My fork](https://github.com/blue-fish/Real-Time-Voice-Cloning) works with Python 3.8, but it's not the long-term solution to this problem. Someone else needs to step up and contribute a better model.",information still current fork work python solution problem someone else need step contribute better model,issue,negative,positive,positive,positive,positive,positive
765800053,"If you just want to try out the toolbox, run it on CPU with this workaround: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/431#issuecomment-664429238

Sorry but I can't answer questions relating to GPU support.",want try toolbox run sorry ca answer support,issue,negative,negative,negative,negative,negative,negative
765790958,"I’m showing PyTorch only supports Cuda 10.0 up to 1.4.0.  Can I use a Cuda
other than 10.0 on this configuration with PyTorch 1.7?

On Fri, Jan 22, 2021 at 3:51 PM blue-fish <notifications@github.com> wrote:

> Other people had a similar issue in #600
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/600>.
> (NVIDIA 30xx cards, resolved by using pytorch >= 1.7)
>
> Your toolbox code is very old. If still experiencing problems, pull the
> latest code from this repo, and make sure you can run python demo_cli.py
> successfully before trying the toolbox.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/637#issuecomment-765769095>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AQ7F7S4A2ZOH7OVV72YBLALS3IFPDANCNFSM4WPC75TQ>
> .
>
-- 
-Ed-
",showing use configuration wrote people similar issue resolved toolbox code old still pull latest code make sure run python successfully trying toolbox thread reply directly view,issue,positive,positive,positive,positive,positive,positive
765769095,"Other people had a similar issue in #600. (NVIDIA 30xx cards, resolved by using pytorch >= 1.7)

Your toolbox code is very old. If still experiencing problems, pull the latest code from this repo, and make sure you can run `python demo_cli.py` successfully before trying the toolbox.",people similar issue resolved toolbox code old still pull latest code make sure run python successfully trying toolbox,issue,positive,positive,positive,positive,positive,positive
765709075,"Including snapshot of the actual toolbox display.  I highlighted the info referenced above...

![toolbox_snapshot](https://user-images.githubusercontent.com/71196619/105553469-4f5b7b80-5cba-11eb-9341-30e385267b9d.png)
",snapshot actual toolbox display,issue,negative,neutral,neutral,neutral,neutral,neutral
765684747,"> @xuexidi If gaps are a problem, you can also remove them from synthesized utterances with voice activation detection (VAD). That's what the ""Enhance vocoder output"" button does. I'm not aware of any other tricks at this time. Corentin has these suggestions for preprocessing and training:
> 
> 1. [#364 (comment)](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/364#issuecomment-653996443)
> 2. [#364 (comment)](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/364#issuecomment-660075042)

@blue-fish I've been beating my head against the wall for 2 days trying to get the toolbox to work.  Today, I finally got the GUI to launch...but here's my problem:  GUI launches, I can import a wav file into the toolbox but, then, the system attempts to load the encoder from encoder\saved_models\pretrained.pt and just sits there until I finally exit out of the program.  I've verified pretrained.pt is at C:Real-Time-Voice-Cloning-master/encoder/saved_models.  Any help would greatly be appreciated!",problem also remove voice activation detection enhance output button aware time training comment comment beating head wall day trying get toolbox work today finally got launch problem import file toolbox system load finally exit program help would greatly,issue,negative,positive,positive,positive,positive,positive
765240076,I am also facing the same issue. I tried to see if there was packages in tensorflow_addon however couldn't find it. Is there an update to this? @blue-fish ,also facing issue tried see however could find update,issue,negative,neutral,neutral,neutral,neutral,neutral
765070340,"Thank you so much! The alternative model does get the short sentences right (also gets rid of weird pauses).
Re the long ones--I've got a working method of handling them (basically considering them in chunks of 20 words and seeing if I can split on commas).",thank much alternative model get short right also rid weird long got working method handling basically considering seeing split,issue,negative,negative,neutral,neutral,negative,negative
765016333,"Poor performance on short inputs results from bad training data. You can try the alternative model in #538 and see if it gets better.

Problems with long inputs are caused by a failure of the attention mechanism. Solution is to implement a better one and retrain the model. Much easier said than done.",poor performance short bad training data try alternative model see better long failure attention mechanism solution implement better one retrain model much easier said done,issue,negative,negative,neutral,neutral,negative,negative
764486041,"> @xuexidi If gaps are a problem, you can also remove them from synthesized utterances with voice activation detection (VAD). That's what the ""Enhance vocoder output"" button does. I'm not aware of any other tricks at this time. Corentin has these suggestions for preprocessing and training:
> 
> 1. [#364 (comment)](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/364#issuecomment-653996443)
> 2. [#364 (comment)](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/364#issuecomment-660075042)

@blue-fish 
I got it!
Thanks a lot, it seems the only way to reduce ""gaps""  is to improve the quality of training speech data.",problem also remove voice activation detection enhance output button aware time training comment comment got thanks lot way reduce improve quality training speech data,issue,negative,positive,positive,positive,positive,positive
764468682,"Here are a dozen of samples from both models
You will see the encoder from this repo is slightly better than mine but have 1-2 ""bugs"" (1 infinite prediction stopped at 10sec and 1 long silence in the middle)
You will also see the bad quality of majority of audios (original) so if the quality of prediction is strange, it's mainly because the original one is bad quality ^^'

[samples_test.zip](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/5847756/samples_test.zip)

If you want more information about my progress, you can contact me by mail but as this topic finally works, I will no longer use it

As a conclusion for this tensorflow 2.x implementation topic : 
- It is possible to use tensorflow to code the SV2TTS model (either basedon TFTTS implementation (in tf2) or the NVIDIA one (in pytorch))
- You can use transfer learning (from a single-speaker model) to accelerate training (you can use my convertion code to make weights convertion from pt to tf or make partial transfert learning)
- Pay attention to clean the dataset (trim silence at the beginning / end) and maybe reduce the noise (you can use noisereduce library)
- You can use my siamese approach as encoder and it is working, maybe use it with mel as input or embedding-dim 256 (maybe it can improve performance ?) but the training is much faster (15h)
",dozen see slightly better mine infinite prediction stopped sec long silence middle also see bad quality majority original quality prediction strange mainly original one bad quality want information progress contact mail topic finally work longer use conclusion implementation topic possible use code model either implementation one use transfer learning model accelerate training use code make make partial learning pay attention clean trim silence beginning end maybe reduce noise use library use approach working maybe use mel input maybe improve performance training much faster,issue,positive,positive,neutral,neutral,positive,positive
763852959,"@blue-fish  it is working !!
I tested 2 models : 
1) 128 embedding-dim (my siamese) trained on 20k batches (18 epochs)
2) 256 embedding-dim (the encoder from this repo) trained on 11k batches (10 epochs) (4 more during this night)

Results : 
- Inference : no « infinite » prediction with the 1st one and quite rare with the 2nd one (mostly on strange voice-embedding I think)
- Voice quality : quite good and similar in both (if working for the 2nd)
- Voice similarity : slightly better with the 2nd one

I will post a dozen of prediction (with target) from both models tomorrow so that you can see the quality / similarity
However you will also see the bad quality of my dataset but I just found a new dataset extracted from LibriVox (free audiobooks) it can be really interesting to improve my models with better quality of voice !

Now I have 11 days of vacation so I will try : 
- Download, embed and train my model on the new dataset
- Use my model for fun :D 
- Fine-tune it on a specific speaker with different intonations so will use utterance-embedding to see if the model can learn emotion !

If I have time : 
- Test a new metric : voice similarity (based on my siamese)
- Train new siamese-encoder based on the same mel config as the Tacotron-2
- Test to use this similarity metric in a training and use it as a similarity-loss ",working tested trained trained night inference infinite prediction st one quite rare one mostly strange think voice quality quite good similar working voice similarity slightly better one post dozen prediction target tomorrow see quality similarity however also see bad quality found new extracted free really interesting improve better quality voice day vacation try embed train model new use model fun specific speaker different use see model learn emotion time test new metric voice similarity based train new based mel test use similarity metric training use,issue,positive,positive,positive,positive,positive,positive
763810836,@Garvit-32 Pretrained model is linked in first post of #472.,model linked first post,issue,negative,positive,positive,positive,positive,positive
763265475,The pytorch model has attention failures when the speaker embedding is generated from a synthetic utterance or low-quality recording. I still don't understand why this is the case. ,model attention speaker synthetic utterance recording still understand case,issue,negative,neutral,neutral,neutral,neutral,neutral
763260683,"@xuexidi If gaps are a problem, you can also remove them from synthesized utterances with voice activation detection (VAD). That's what the ""Enhance vocoder output"" button does. I'm not aware of any other tricks at this time. Corentin has these suggestions for preprocessing and training:
1. https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/364#issuecomment-653996443
2. https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/364#issuecomment-660075042",problem also remove voice activation detection enhance output button aware time training,issue,negative,positive,positive,positive,positive,positive
763258055,"When installing dependencies to a virtual environment, you'll need to ensure it is activated before launching the toolbox. You'll know it's active because `(venv)` shows up in your command prompt. If not, run `activate.bat` using the command above.",virtual environment need ensure toolbox know active command prompt run command,issue,positive,negative,negative,negative,negative,negative
763256914,"Python 3.8 is not supported: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/431#issuecomment-665165023

We can't help with the EnvironmentError message in the 1st screenshot, that's not our code.",python ca help message st code,issue,negative,neutral,neutral,neutral,neutral,neutral
763256117,"Set up a python 3.7 virtual environment. If the command doesn't work, you need to add it to your system path.
```
cd Real-Time-Voice-Cloning
python3.7 -m venv venv
venv\Scripts\activate.bat
```

Then `pip install torch` and `pip install -r requirements.txt` should work.",set python virtual environment command work need add system path python pip install torch pip install work,issue,negative,neutral,neutral,neutral,neutral,neutral
763216311,"When attempting to install tensorflow with the ""pip install -r requirements.txt"" file i get this:
![image](https://user-images.githubusercontent.com/36093760/105107711-a711a100-5a86-11eb-848f-a45f1a23096f.png)
",install pip install file get image,issue,negative,neutral,neutral,neutral,neutral,neutral
762584311,"> The gaps are also due to long pauses between words and sentences in the training data. It's improved with my alternative synthesizer model in #538.
> 
> Weird noises are an artifact of the training data, which has a lot of mic bumps. It may be slightly better with my 002 vocoder model in #400 . Those go away when you vocode with Griffin-Lim, but you get different quality problems.

@blue-fish 
**Hi，thanks for your pretrained models! It performed perfect!**

**I would like to ask you some questions:**
you mentioned: ""The gaps are also due to long pauses between words and sentences in the training data. ""
Is that means the only way to reduce the ""gaps"" is to clean up the sentences with "" long pauses"" in traning data?  is there any other tricks in ""trainig"" or in ""inference""？

Looking forward to your reply. Thank you!",also due long training data alternative synthesizer model weird artifact training data lot may slightly better model go away get different quality perfect would like ask also due long training way reduce clean long data inference looking forward reply thank,issue,positive,positive,neutral,neutral,positive,positive
762477924,"The gaps are also due to long pauses between words and sentences in the training data. It's improved with my alternative synthesizer model in #538.

Weird noises are an artifact of the training data, which has a lot of mic bumps. It may be slightly better with my 002 vocoder model in #400 . Those go away when you vocode with Griffin-Lim, but you get different quality problems.",also due long training data alternative synthesizer model weird artifact training data lot may slightly better model go away get different quality,issue,negative,negative,neutral,neutral,negative,negative
762350065,"The use of a dataset is optional. You can ignore that message and load your own audio files (.wav, .mp3, .flac).

If you want to do some exploration with a dataset, get one from here: https://openslr.org/12

The readme recommends `train-clean-100`, but any of the smaller ""dev"" or ""test"" datasets will work too. Make a folder called `datasets_root` in your Real-Time-Voice-Cloning folder. Extract the .tar.gz file and move the `LibriSpeech` folder inside datasets_root. Then launch the toolbox with this command:

```
python demo_toolbox.py -d datasets_root
```

Also, if you see `.DS_Store` in the dropdown menus (as in your screenshot), you will have to switch them to `pretrained` for proper operation.",use optional ignore message load audio want exploration get one smaller dev test work make folder folder extract file move folder inside launch toolbox command python also see switch proper operation,issue,negative,neutral,neutral,neutral,neutral,neutral
761929412,"> There's no file with that name

That explains it. Try extracting the .zip file again. You can also try using git to clone the repository:
```
git clone --depth 1 https://github.com/CorentinJ/Real-Time-Voice-Cloning
```",file name try file also try git clone repository git clone depth,issue,negative,neutral,neutral,neutral,neutral,neutral
761928406,"This line seems to be the culprit of the attention issues: [synthesizer/models/tacotron.py#L227](https://github.com/blue-fish/Real-Time-Voice-Cloning/blob/44bb76561499c16222b9636167322e286de978b1/synthesizer/models/tacotron.py#L227)

I've updated it to match Rayhane's LSA, which only uses the cumulative attention score in the location convolution. I don't know why fatchord does the concat with the previous attention output. It learns faster, but is less stable for unseen speakers.

Edit: Disregard this suggestion. The attention concat gets better results with fatchord's tacotron.",line culprit attention match cumulative attention score location convolution know previous attention output faster le stable unseen edit disregard suggestion attention better,issue,negative,positive,positive,positive,positive,positive
761927026,"> To eliminate obvious issues, please check the following:
> 
> 1. Does the file `inference.py` exist in the folder: `/Users/luke/Documents/Real-Time-Voice-Cloning-master/encoder` ?
> 2. Are you running the `python demo_toolbox.py` command from the terminal, in this folder? `/Users/luke/Documents/Real-Time-Voice-Cloning-master`

There's no file with that name and yes i am running it from the terminal with that command in the folder",eliminate obvious please check following file exist folder running python command terminal folder file name yes running terminal command folder,issue,positive,neutral,neutral,neutral,neutral,neutral
761907524,"To eliminate obvious issues, please check the following:
1. Does the file `inference.py` exist in the folder: `/Users/luke/Documents/Real-Time-Voice-Cloning-master/encoder` ?
2. Are you running the `python demo_toolbox.py` command from the terminal, in this folder? `/Users/luke/Documents/Real-Time-Voice-Cloning-master`",eliminate obvious please check following file exist folder running python command terminal folder,issue,negative,neutral,neutral,neutral,neutral,neutral
761907069,"I'm unable to reproduce this problem in macOS. Which version of python are you using, and how did you install it?",unable reproduce problem version python install,issue,negative,negative,negative,negative,negative,negative
761690658,"You need a different repo for voice conversion. I have a list in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/434#issuecomment-661294839

If you just want to get an idea of the output quality, upload or point me to an audio file and I'll convert it with one of my pretrained voices. These repos are not user-friendly and take some time to set up.",need different voice conversion list want get idea output quality point audio file convert one take time set,issue,negative,neutral,neutral,neutral,neutral,neutral
761113101,"thanks for this .
edit: time for me to learn python .",thanks edit time learn python,issue,negative,positive,positive,positive,positive,positive
760713530,"Attention is problematic when the speaker embedding is generated from a synthetic utterance. The current synth model is more robust in this regard. I will experiment and see if this can be improved by replacing the attention GRU with a 2-layer LSTM as in Tacotron2.

Edit: The LSTM is only marginally better. I reverted to the GRU, and added zoneout to try to get it to generalize better. Also tried regularizing the weights. These approaches didn't fix the problem.",attention problematic speaker synthetic utterance current model robust regard experiment see attention edit marginally better added try get generalize better also tried fix problem,issue,negative,positive,positive,positive,positive,positive
760654201,"Try downloading the precompiled sounddevice linked from: https://github.com/spatialaudio/python-sounddevice/issues/7#issuecomment-751341488
You'll need to match your python version and CPU architecture.
```
pip uninstall sounddevice
pip install sounddevice‑0.4.1‑cp37‑cp37m‑win_amd64.whl
```",try linked need match python version architecture pip pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
760569896,"> Recent similar projects:
> https://github.com/Tomiinek/Multilingual_Text_to_Speech
> https://github.com/espnet/espnet

Can I also clone voices with these repo's using a small audio clip of 3-5 minutes? This repo needs a 5 second audio clip, but for resemable.ai a larger sample with voice is better. Now resemable ask voice verification, something I can't do.

Are there repo's that can also use a longer voice sample of, for example, 5 minutes, that sound better than this repo? if so, which ones have the best result?

I would like to pay the person who can help me make good voice clones from 3-5 minute samples. really need it. blue-fish, I see you're very active here. Help me? :)",recent similar also clone small audio clip need second audio clip sample voice better ask voice verification something ca also use longer voice sample example sound better best result would like pay person help make good voice minute really need see active help,issue,positive,positive,positive,positive,positive,positive
760234018,"Plots are made from 15 speakers, 10 utterances on voxforge and common voice (1 plot per dataset) (same spk / utterances for the 3 plots)

Plots for the model from this repo :
![mel_ge2e_embeddings](https://user-images.githubusercontent.com/45139111/104604191-7f34ce80-567d-11eb-9129-350cbf36b56d.png)
![mel_ge2e_embeddings_vox](https://user-images.githubusercontent.com/45139111/104604194-7fcd6500-567d-11eb-92c4-4aecc4dd427c.png)


Plots for my siamese 128
![siamese_embeddings_128](https://user-images.githubusercontent.com/45139111/104604356-8cea5400-567d-11eb-90a1-d168dcadf354.png)
![siamese_embeddings_128_vox](https://user-images.githubusercontent.com/45139111/104604357-8d82ea80-567d-11eb-97b5-da860495e73a.png)


Plots for my siamese 512 :
![siamese_embeddings_512](https://user-images.githubusercontent.com/45139111/104604415-a095ba80-567d-11eb-8722-3bf6e99e4236.png)
![siamese_embeddings_512_vox](https://user-images.githubusercontent.com/45139111/104604422-a12e5100-567d-11eb-9770-1dc25aee7725.png)

Finally seems to be quivalent, maybe a few better for the one of this repo, not really clear... In general mine are more closer but some are little bit merged so...
",made common voice plot per model finally maybe better one really clear general mine closer little bit,issue,positive,positive,neutral,neutral,positive,positive
760197951,"Oh yes I see what you mean, in fact you train Tacotron-2 based on its own generated audio but with different embeddings (the one from original audio to produce target-mel and the one from generated mel to compute loss compared to target-mel)... Nice trick but is it really stable and gives interesting results ? 

My datasets contains (after removing non-France pronounciations), more than 3k speakers so I hope it is enough
The SE I trained was trained on LibriSpeech (en) + CommonVoice (fr) + VoxForge (fr) on  more than 100 utterances for 1200 speakers and tested on 300 totally different speakers (speakers not in training set) and achieved 97.5% binary-accuracy (training and testing) so really nice generalization for the SE part

For TTS model, I use CommonVoice (> 160k samples), VoxForge (25k samples) and SIWIS (9.5k samples, single speaker) datasets so quite nice ! but CV and VoxForge are (I think) STT datasets and not TTS sot it needs a little more cleanings (it is most probably why my noise-reduction and better trim-silence helped so much)",oh yes see mean fact train based audio different one original audio produce one mel compute loss nice trick really stable interesting removing hope enough se trained trained en tested totally different training set training testing really nice generalization se part model use single speaker quite nice think sot need little probably better much,issue,positive,positive,positive,positive,positive,positive
760184496,"> but as I understand the mels inputs and the mel produced by Tacotron2 are not the same (40 vs 80 mels channels etc) so you cannot compare them no ?

Yes, that is the case for this repo. I had in mind to make a new SE that is compatible with the output mels from Tacotron2 as to make the loss calculation possible. The reason for doing this is because unseen voices do not clone well. The output voice does not sound like the voice used to create the embedding. To overcome this problem, we need a larger training set, but TTS datasets usually have about 100 to 1000 speakers (not enough!).

Training the SE involves different datasets with thousands of speakers, which are normally unusable for TTS training since they are not transcribed. However, that is not a problem for us! First, run the audio through the SE to generate an input embedding. Next, feed this to Tacotron2 with some short text inputs to generate some mels. The specific text doesn't matter because the speaker embedding should be invariant to the content of the speech. Finally, run the output mels through the SE to get the output embedding. Then calculate the loss of the output embed using the input embed as a target. The loss can be used to finetune the decoder LSTM and/or the postnet convolutional layers in Tacotron.

> but maybe a raw-audio encoder can encode also this type of information or something else that can be useful to conditionn Tacotron-decoding ?
I really don’t know what kind of information the SE part can encode especially in raw audio to have as good results :’)

The reason for using mels in the SE is for speed. If the SE takes raw audio as input, you will have to vocode the output mels to get the audio. And that is much too slow for the training loop.",understand mel produced compare yes case mind make new se compatible output make loss calculation possible reason unseen clone well output voice sound like voice used create overcome problem need training set usually enough training se different normally unusable training since however problem u first run audio se generate input next feed short text generate specific text matter speaker invariant content speech finally run output se get output calculate loss output embed input embed target loss used convolutional maybe encode also type information something else useful really know kind information se part encode especially raw audio good reason se speed se raw audio input output get audio much slow training loop,issue,positive,positive,neutral,neutral,positive,positive
760140168,"> Excellent progress @Ananas120! Please share some plots and wavs when able.

Yes, normally the 21th I will be free to test and share some results
I hope my trainings will improve the stability because I re-listened to the results and the end of the sentence was understandable but a bit noisy or flappy and the voice not really similar to the original one but it was only 5 epochs so already impressive 
However I continued training and after 500 steps more the results were less good than before so will see if I can stabilize it

> That's interesting and I'm curious to know how well it works. I prefer a speaker encoder based on mels. This makes it possible to condition Tacotron's output mels to sound more like the target voice, by using an encoder loss term based on mels. After training the TTS part of the model, you can finetune it with additional voices unseen during TTS training. The finetuning does not require teacher forcing since the loss is calculated from an embedding and not a mel.

Not sure to understand what you would say, the SE part is trained with mels from speakers-voice as input and compute loss based on embeddings (same as mine except that I use raw audio instead of embeddings as input) but as I understand the mels inputs and the mel produced by Tacotron2 are not the same (40 vs 80 mels channels etc) so you cannot compare them no ? 
So if you want to fine-tune the Tacotron part on a specific speaker, you have to make a « normal » training (text and embedding and mel as input) and mel as output target to compute the loss MSE ?

The only difference between the 2 SE are the input (raw vs mel) and it’s loss training but the embeddings produced are of the same format (1D vector of dim 256 / 128 / 512) and this embedding is concatenated with the Tacotron-encoder as input to the Tacotron-decoder so not sure to understand how you can « condition » the Tacotron-model ? 
Except if you mean the SE « encodes » frequencies’ information in it’s embedding but maybe a raw-audio encoder can encode also this type of information or something else that can be useful to conditionn Tacotron-decoding ?
I really don’t know what kind of information the SE part can encode especially in raw audio to have as good results :’)

Did you already made experiments by conditioning Tacotron with embeddings from « pre-fabricated » mels as SE-input ? Is it possible to « force » it to use some frequencies or something like that ? 
",excellent progress ananas please share able yes normally th free test share hope improve stability end sentence understandable bit noisy voice really similar original one already impressive however continued training le good see stabilize interesting curious know well work prefer speaker based possible condition output sound like target voice loss term based training part model additional unseen training require teacher forcing since loss calculated mel sure understand would say se part trained input compute loss based mine except use raw audio instead input understand mel produced compare want part specific speaker make normal training text mel input mel output target compute loss difference se input raw mel loss training produced format vector dim input sure understand condition except mean se information maybe encode also type information something else useful really know kind information se part encode especially raw audio good already made possible force use something like,issue,positive,positive,positive,positive,positive,positive
760133750,"It will run on Windows 10, but you need to install python and required python packages. Expect it to be difficult if you're not already a python user. Someone put together a video explaining how to set it up: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/614#issuecomment-748032599",run need install python python expect difficult already python user someone put together video explaining set,issue,negative,negative,negative,negative,negative,negative
760130217,"Excellent progress @Ananas120! Please share some plots and wavs when able.

> I think the really important modification was the trim_silence and noise_reduction and not the embeddings

That's likely the case. Not trimming silence makes it harder to learn attention well, and leads to problems in inference.

> Mine are trained from raw audio, here from spectrogram so maybe it gives more information on frequencies from the speaker ?

That's interesting and I'm curious to know how well it works. I prefer a speaker encoder based on mels. This makes it possible to condition Tacotron's output mels to sound more like the target voice, by using an encoder loss term based on mels. After training the TTS part of the model, you can finetune it with additional voices unseen during TTS training. The finetuning does not require teacher forcing since the loss is calculated from an embedding and not a mel.
",excellent progress ananas please share able think really important modification likely case trimming silence harder learn attention well inference mine trained raw audio spectrogram maybe information speaker interesting curious know well work prefer speaker based possible condition output sound like target voice loss term based training part model additional unseen training require teacher forcing since loss calculated mel,issue,positive,positive,positive,positive,positive,positive
760094200,"The attention issues can be resolved by matching the settings of `LocationSensitiveAttention` in Rayhane's tacotron. It's also necessary to use a stop token prediction since attention is less stable for multispeaker. Silent mel frames are common, which causes fatchord's Taco1 to prematurely stop generation. Adding a stop token forces it to finish. With attention fixed, training on the full LibriSpeech dataset yields a usable model with voice quality comparable to tensorflow.

I'll push the code changes when the pretrained model is ready, to avoid disrupting those actively using this branch.",attention resolved matching also necessary use stop token prediction since attention le stable silent mel common prematurely stop generation stop token finish attention fixed training full usable model voice quality comparable push code model ready avoid actively branch,issue,negative,positive,neutral,neutral,positive,positive
760015377,"@blue-fish my model made 2 of 3 sentences nearly perfect (perfectly understandable) ! 
I have exams till the 20th so I can’t test it on other speakers / sentences now to test the similarity between produced voice and original one but it’s really promising / encouraging to have these results !

What I changed : 
- I embedded my datasets with the encoder of this repo
- I fairly improved my `trim_silence` method and use the `noisereduce` library to reduce noise and better trim silence 

The 1st modification was not really interesting first (I already tried it in the past and here in 9 epochs, nothing interesting)
I therefore made 1 epoch more (so the 10th) after improving my method and the model made quite good inference in only 1 epoch 
So I retrained a model from 0 for 5 epochs (1100 batchs, size 32 per epoch) and the result is as described, really promising (I will post some samples next week after my last exam) !

Next steps  : 
- Continue training (will do it during learning for exams)
- Test inference on other speakers / sentences to test the similarity / pronounciations on new words
- Test to train with my embeddings (with the siamese approach) instead of the one of this repo : I think the really important modification was the trim_silence and noise_reduction and not the embeddings because just the embeddings did not improve performances

Furthermore, when plotting embeddings from my siamese and the one from this repo, I find (really subjectif) that mine are much closer (for same speakers) than the one predicted from this model (I can also share plots next week if you want)
However, 2 differences can affect for the prediction quality : 
- The embedding-dim : here is 256 but the one I tested was 128 (I have also trained a 512 so can also test this one)
- Mine are trained from raw audio, here from spectrogram so maybe it gives more information on frequencies from the speaker ? I also trained a sieamese on mel so can also try it (but only size 128)... 
Will see next week ! quite exciting haha :D 

ps : if you want I can also share my `trim_silence` method, I have to optimize it in tensorflow instead of numpy but the main principle is to use a moving window (with  np.convolve) to check where the mean is higher than a threshold (0.1 in my case) and it allows to remove some noise and end / beginning artefacts from opening / closing the mic
Note : for the filter in np.convolve, I tested a simple mean mask and a triangular mask and best results are with window of 0.25 sec with triangular mask (with ‘valid’ padding)",model made nearly perfect perfectly understandable till th test test similarity produced voice original one really promising encouraging fairly method use library reduce noise better trim silence st modification really interesting first already tried past nothing interesting therefore made epoch th improving method model made quite good inference epoch model size per epoch result really promising post next week last exam next continue training learning test inference test similarity new test train approach instead one think really important modification improve furthermore plotting one find really mine much closer one model also share next week want however affect prediction quality one tested also trained also test one mine trained raw audio spectrogram maybe information speaker also trained mel also try size see next week quite exciting want also share method optimize instead main principle use moving window check mean higher threshold case remove noise end beginning opening note filter tested simple mean mask triangular mask best window sec triangular mask valid padding,issue,positive,positive,positive,positive,positive,positive
757685546,"The researchers who came up with this impressive concept also had very good execution and results: https://google.github.io/tacotron/publications/speaker_adaptation/index.html

Unfortunately the Google implementation is not open-source so we are still trying to replicate the results of that paper. My best results so far: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/624#issuecomment-756020854",came impressive concept also good execution unfortunately implementation still trying replicate paper best far,issue,positive,positive,positive,positive,positive,positive
757680317,What amazes me is the idea and the technology. I really don't care about the quality 👍 ,idea technology really care quality,issue,positive,positive,positive,positive,positive,positive
757677732,"Thanks for the feedback. Corentin did a really good job making neural TTS a lot more accessible to the general public. The GUI and command-line utilities make it very user-friendly.

I'm glad that you appreciate it for what it is. Most people have very high expectations. The output will not fool a human, but is still impressive given it can be trained from scratch on consumer hardware, using free datasets, in a matter of days.",thanks feedback really good job making neural lot accessible general public make glad appreciate people high output fool human still impressive given trained scratch consumer hardware free matter day,issue,positive,positive,positive,positive,positive,positive
757644141,"Yes i have it there and i can see pretrained in the list. I tought it was called something else.

Really amazing job. :)",yes see list tought something else really amazing job,issue,positive,positive,positive,positive,positive,positive
757642289,"You can enable the WaveRNN vocoder by copying `vocoder/saved_models/pretrained/pretrained.pt` from the [zip file](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models) to that same location in your repo. It will show up in the vocoder list as ""pretrained"".",enable zip file location show list,issue,negative,neutral,neutral,neutral,neutral,neutral
757635499,"Oh i see.  can onl see Griffin-Lim in the list btw.
Actually, this is an amazing job 👍 ",oh see see list actually amazing job,issue,positive,positive,positive,positive,positive,positive
757582159,"Yes, whenever text is updated you need to synthesize and vocode. You can use Griffin-Lim vocoder to check the quality before running the pretrained WaveRNN, which is slow on CPU.

The pretrained model is quite bad. I have a method in #437 to improve quality, but it involves recording a dataset.",yes whenever text need synthesize use check quality running slow model quite bad method improve quality recording,issue,negative,negative,negative,negative,negative,negative
757579364,"I am getting the error message when i try to record my voice. Exception: Audio buffer is not finite everywhere
I have recorded using a different application this time it works. But i am not sure if i am doing the correct thing. Voice is too noisy most of the time and when i change the text, do i have to click Synthesize and vocode and wait each time?",getting error message try record voice exception audio buffer finite everywhere different application time work sure correct thing voice noisy time change text click synthesize wait time,issue,negative,positive,positive,positive,positive,positive
757575587,What is the error message and traceback in the terminal when you try to record?,error message terminal try record,issue,negative,neutral,neutral,neutral,neutral,neutral
757575233,"yes to launch the toolbox. I launched toolbox. When i browse and select from samples, it works but when i try to record i am getting the error i mention earlier.",yes launch toolbox toolbox browse select work try record getting error mention,issue,negative,neutral,neutral,neutral,neutral,neutral
757574942,"To launch the toolbox? Let me know if you meant installation.
```
python demo_toolbox.py
```",launch toolbox let know meant installation python,issue,negative,neutral,neutral,neutral,neutral,neutral
757572577,Can you please tell me what command did you execute?,please tell command execute,issue,negative,neutral,neutral,neutral,neutral,neutral
757568297,"It's working for me on Intel Mac, no GPU. Just did a fresh toolbox install in Python 3.7.9 with pristine venv.
* Running `conda list` and `pip freeze` would be helpful to know which package versions you are running.
* If you are using an external microphone could you try unplugging it? Then use the built-in microphone and see if you still get the same error.",working mac fresh toolbox install python pristine running list pip freeze would helpful know package running external microphone could try unplugging use microphone see still get error,issue,negative,positive,positive,positive,positive,positive
757564457,"I ran the toolbox. After recording, getting the following error:
Exception: Audio buffer is not finite everywhere",ran toolbox recording getting following error exception audio buffer finite everywhere,issue,negative,neutral,neutral,neutral,neutral,neutral
757563818,"That was librosa. I have just updated librosa and the error is gone. This may help others as well.. However now i am getting the following error and trying to fix it.

[3] Call to cuInit results in CUDA_ERROR_NOT_INITIALIZED:

I'm running on MAC and no GPU here.",error gone may help well however getting following error trying fix call running mac,issue,negative,neutral,neutral,neutral,neutral,neutral
757563576,"Looks like it could be an issue with anaconda (similar to: https://github.com/dereneaton/ipyrad/issues/180). Try setting up a clean environment with miniconda or pip.

If using Linux you might find https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/615#issuecomment-740270185 or the [wiki setup instructions](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/615) helpful.",like could issue anaconda similar try setting clean environment pip might find setup helpful,issue,positive,positive,positive,positive,positive,positive
756600418,"They trained their model during 400h on 8 GPU better than mine so I think I will never achieve as good performances, it is why I use my transfert and partial transfert learning for my single / multi speaker models ;) 

What do you mean by « add the speaker embedding as part of the context vector » ? 
In the implementation I concat the encoder-output with the speaker-embedding and it is passed as input to the attention’s memory layer and is also part of the energies’ computation
Where do you suggest to add the speaker-embedding ?",trained model better mine think never achieve good use partial learning single speaker mean add speaker part context vector implementation input attention memory layer also part computation suggest add,issue,negative,positive,positive,positive,positive,positive
756396542,"SV2TTS is not that good for cloning unseen voices. For better results, collect 10+ minutes of transcribed speech samples, [make a dataset](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-666099538), and finetune the pretrained synthesizer model (see #437). A GPU is not required for finetuning.

The VCTK model has its own problems, and does not make a good TTS or voice cloning engine. I don't plan to release it. If you find this interesting and want to do your own exploration, get a GPU and start with the tutorial on [training models from scratch](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training). After that, you'll have enough experience to train your own models using the VCTK dataset.",good unseen better collect speech make synthesizer model see model make good voice engine plan release find interesting want exploration get start tutorial training scratch enough experience train,issue,positive,positive,positive,positive,positive,positive
756337828,"@blue-fish thanks a lot for the update. Are the models you used to generate these results uploaded somewhere? If not can you add them at https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/400#issuecomment-653830346 . It will be very helpful.
I was under the impression that I could use one of these pre-trained models for cloning unseen voices(voices not part of the dataset on which they are trained) but the results are much worse for that case. 
Results on audios that do not belong to any datasets:
https://drive.google.com/drive/folders/1zFqKqkdnvG9c2c-7i7x_XHuh0YfBcr_H?usp=sharing


",thanks lot update used generate somewhere add helpful impression could use one unseen part trained much worse case belong,issue,positive,negative,neutral,neutral,negative,negative
756314118,"You should try training the single-speaker model from scratch on LJSpeech. See if that performs as well as NVIDIA's pretrained model.

I also wonder if it is necessary to include the speaker embedding as part of the context vector, in the attention input. My pytorch synthesizer in #472 has fewer attention failures for single-speaker (without speaker embedding) compared to multi-speaker.",try training model scratch see well model also wonder necessary include speaker part context vector attention input synthesizer attention without speaker,issue,negative,neutral,neutral,neutral,neutral,neutral
756068787,"Thank you for the link for LJSpeech
Yes in fact my model is really powerful in Single Speaker, I used the pretrained model from NVIDIA and used my weights convertor to convert pretrained model in tensorflow (the convertion is perfect (output are exactly the same), I tested it) and when training on SIWIS (single speaker), I achieve really good performance in only 5 steps with good inference at epoch 2-3 so I thought multi-speaker could also work but seems not
Now I am using a trick inspired from TFTTS implementation and seems to have inference working sometimes but it’s quite rare and unstable and not understandable result... 
I am testing with harder value for teacher-forcing ratio and will see if it helps
If it does not work, another step would be to test on LibriSpeech instead of my French dataset to see if it is the model or the datasets I use",thank link yes fact model really powerful single speaker used model used convertor convert model perfect output exactly tested training single speaker achieve really good performance good inference epoch thought could also work trick inspired implementation inference working sometimes quite rare unstable understandable result testing harder value ratio see work another step would test instead see model use,issue,positive,positive,positive,positive,positive,positive
756056135,"Hi @Ananas120 , nice to read about your progress! Hopefully your latest model works out. But if not, you should try training a single-speaker dataset and see if you can get inference to work, before moving onto multi-speaker. Here is a link for LJSpeech: https://keithito.com/LJ-Speech-Dataset/",hi ananas nice read progress hopefully latest model work try training see get inference work moving onto link,issue,positive,positive,positive,positive,positive,positive
756020854,"Hi @vishal16babu , I take it you are using Corentin's pretrained models? Use LibriSpeech for a fair comparison with the examples.

If you want to match the VCTK results, you need to follow the procedure outlined in the paper. This means training the synthesizer and vocoder on the VCTK dataset. Here are my results when I do that using this repo with default settings. The cloned voice is comparable to Google's results, though their audio quality is much better.

Encoder: Pretrained (1.56M steps)
Synthesizer: VCTK (242k steps)
Vocoder: VCTK (300k steps)

#### P240 validation samples: [validation.zip](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/5780908/validation.zip) (492KB, 7 .wav files)",hi take use fair comparison want match need follow procedure outlined paper training synthesizer default voice comparable though audio quality much better synthesizer validation,issue,positive,positive,positive,positive,positive,positive
756004824,"Colab is unsupported, but feel free to submit a pull request if you make any improvements to the notebook.",unsupported feel free submit pull request make notebook,issue,negative,positive,positive,positive,positive,positive
755777595,This is popular things. but until now not a single release available. I hope somebody handle it. Thank you ,popular single release available hope somebody handle thank,issue,positive,positive,positive,positive,positive,positive
754062829,"> @CorentinJ @yaguangtang @tail95 @zbloss @HumanG33k I am finetuning the encoder model by Chhinese data of 3100 persons. I want to know how to judge whether the train of finetune is OK. In Figure0, The blue line is based on 2100 persons , the yellow line is based on 3100 persons which is trained now.
> Figure0:
> ![image](https://user-images.githubusercontent.com/40649244/63139015-3f446480-c00f-11e9-9ec2-3eadebd6023f.png)
> 
> Figure1:(finetune 920k , from 1565k to 1610k steps, based on 2100 persons)
> ![image](https://user-images.githubusercontent.com/40649244/63139038-5aaf6f80-c00f-11e9-85ca-f54fdd81431e.png)
> 
> Figure2:(finetune 45k from 1565k to 1610k steps, based on 3100 persons)
> ![image](https://user-images.githubusercontent.com/40649244/63139112-a8c47300-c00f-11e9-9097-fb65452df9fd.png)
> 
> I also what to know how mang steps is OK , in general. Because, I only know to train the synthesizer model and vocoder mode oneby one to judge the effect. But it will cost very long time. How about my EER or Loss ? Look forward your reply!

@UESTCgan Hi, I would like to ask you if you do finetuning with Chinese dataset on the encoder trained with English dataset Librispeec? In addition, what Chinese datasets are you using？And how about your final eer value with your training?

",tail model data want know judge whether train figure blue line based yellow line based trained figure image figure based image figure based image also know mang general know train synthesizer model mode one judge effect cost long time eer loss look forward reply hi would like ask trained addition final eer value training,issue,negative,positive,neutral,neutral,positive,positive
753639861,"Yes ! It seems to work ! Not understandable yet but only the 1st epoch with a non-teacher-forcing ratio (after 8 epochs in teacher-forcing mode) so really promising ! 
The model seems to use attention, can make nearly understandable voice (the end of the sentence is a little bit flappy but the beginning is quite understandable (but noisy)) and he does not go to the « maximum-iterations » (so it predicts well a end-of-sentence gate) and it seems to use different voice according to the sample (but difficult to see it because only 3 test-samples and noisy result)
I hope after 5 steps more on 5 times more data will help and give usable result !",yes work understandable yet st epoch ratio mode really promising model use attention make nearly understandable voice end sentence little bit beginning quite understandable noisy go well gate use different voice according sample difficult see noisy result hope time data help give usable result,issue,positive,negative,neutral,neutral,negative,negative
753020805,My understanding is that it runs until the user terminates the training loop. You monitor progress with visdom and stop training when satisfied.,understanding user training loop monitor progress stop training satisfied,issue,positive,positive,positive,positive,positive,positive
752998439,"I achieved to code my model with `dynamic_decode` However I have an OOM but do not really understand why because I changed only 1 thing compared to the TFTTS model (no OOM with it) x)
Tomorrow I will try to remove the modification to see if it causes the OOM... I hope not because if it is the cause I will not be able to reuse my old model-weights so will see

I had a question for English training : which dataset is used classically ? I thought LibriSpeech but it is a multi-speaker dataset with at maximum 5k samples for 1 speaker so it seems not enough no ? I also heard about LJSpeech (I think) but can’t find it... 
In French I use the SIWIS dataset with 9.5k samples (around 10h) and CommonVoice for multi-speaker (for SV2TTS training)

The 2 differences I found between the NVIDIA implementation and the TFTTS one is : 
- The attention conv-layer takes as input cumulative weiths (int TFTTS) and concatenation of cumulative weights and previous weights (in NVIDIA one) (it’s the modification I will try to change tomorrow to avoid OOM)
- The TFTTS predicts stop-token based on mel-output and decoder-rnn-output but NVIDIA computes based on attention-context and decoder-rnn-output (same input as for the mel-prediction)

In my implementation I made all options (with hyperparameters) so I can test all implementations and compare them haha :D 
I also implemented the encoder with speaker-id, speaker-embedding or simple version 

If you want I implemented a funny HParams class allowing to have a powerful class to configure my models and get config from command-line arguments etc :) ",code model however really understand thing model tomorrow try remove modification see hope cause able reuse old see question training used classically thought maximum speaker enough also think find use around training found implementation one attention input cumulative concatenation cumulative previous one modification try change tomorrow avoid based based input implementation made test compare also simple version want funny class powerful class configure get,issue,positive,positive,positive,positive,positive,positive
752871070,"after some google,is seams that:
in train.py Line19:os.environ[""CUDA_VISIBLE_DEVICES""] = ""3""
there's some wrong with it
so I change it to ""cpu"" as:
`device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')`
It worked , but no gpu or tpu
:sad 
",line wrong change device else worked sad,issue,negative,negative,negative,negative,negative,negative
752852097,"!nvidia-smi
---
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |
| N/A   38C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |
|                               |                      |                 ERR! |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+",driver version version name volatile fan temp compute mig mib mib default err type process name memory id id usage running found,issue,negative,neutral,neutral,neutral,neutral,neutral
752851861,"import torch
print('Torch', torch.__version__, 'CUDA', torch.version.cuda)
print('Device:', torch.device('cuda:0'))
---
Torch 1.7.0+cu101 CUDA 10.1
Device: cuda:0",import torch print print torch device,issue,negative,neutral,neutral,neutral,neutral,neutral
752834878,"> Please ensure the files are extracted to these locations within your local copy of the repository:
> 
> ```
> encoder\saved_models\pretrained.pt
> synthesizer\saved_models\logs-pretrained\taco_pretrained\checkpoint
> synthesizer\saved_models\logs-pretrained\taco_pretrained\tacotron_model.ckpt-278000.data-00000-of-00001
> synthesizer\saved_models\logs-pretrained\taco_pretrained\tacotron_model.ckpt-278000.index
> synthesizer\saved_models\logs-pretrained\taco_pretrained\tacotron_model.ckpt-278000.meta
> vocoder\saved_models\pretrained\pretrained.pt
> ```

thx for all.I have solved . ",please ensure extracted within local copy repository,issue,positive,neutral,neutral,neutral,neutral,neutral
752152349,"Hello @blue-fish  (I don’t know how to contact you so I do it here ;) )
I finished my courses and my exams are in 10 days but already finished to study so I will continue tests during 1 week and have 2 free weeks (after the 20jan) so I hope to achieve better results...

What I already did since last time : 
- Integrate the TFTTS Tacotron2 model to my project and train it : really impressive results in teacher-forcing mode but no attention learned and nothing in inference (not a « noise » but cyclic voice)... Also after only a few hours of training on my 9k samples dataset I achieved a loss of 0.5 so 0.15 postnet loss, it seems really « too » impressive
Also I don’t understand why but with this implementation I can’t train it and call my evaluation function after epochs, even my prediction function does not work, really strange... 
Furthermore, the 0.5 loss is achieved in less than 15 epochs but without transfer-learning ! with pretrained english model, the loss is 3 times higher but seems to converge...
- With my old implementation, always same results : good in teacher-forcing but bad in inference and no attention learned... 

I think I will try to add a loss on the attention to « force » it to learn a linear-increasing attention (for some steps) and remove it after it is learned to continue training without this alignement-forcing (if you have some code to do it, it is welcome :D )
This week I will also try to recode my old model with the `dynamic_decode` function in order to run it on complete samples (instead of my trick by splitting a sample in sub-frames) (the TFTTS model uses it and I achieve a 2 times faster training-step). Maybe with a more classical training procedure, the model will better learn the attention (I hope !)",hello know contact finished day already finished study continue week free hope achieve better already since last time integrate model project train really impressive mode attention learned nothing inference noise cyclic voice also training loss loss really impressive also understand implementation train call evaluation function even prediction function work really strange furthermore loss le without model loss time higher converge old implementation always good bad inference attention learned think try add loss attention force learn attention remove learned continue training without code welcome week also try recode old model function order run complete instead trick splitting sample model achieve time faster maybe classical training procedure model better learn attention hope,issue,positive,positive,positive,positive,positive,positive
751869149,@blue-fish I have some GPU resources that could be put to work on this; happy to join the Slack for further details.,could put work happy join slack,issue,positive,positive,positive,positive,positive,positive
751386130,"win 13 error is an i/o error. 
Do you have all the proper files in the folder?
Are they writable? 
Maybe you need to remove read only on folder and files in folder?
(right click on folder and remove read only)",win error error proper folder writable maybe need remove read folder folder right click folder remove read,issue,negative,positive,positive,positive,positive,positive
751384405,What is the python command you are using to launch the toolbox?,python command launch toolbox,issue,negative,neutral,neutral,neutral,neutral,neutral
751320193,"> 
> 
> I will be trying to do the same with spanish. Wish me luck. Any suggestions about compute power?

Did you have any luck?",trying wish luck compute power luck,issue,positive,neutral,neutral,neutral,neutral,neutral
750521259,"https://github.com/fuwiak/medium/tree/master/5minutes 
this requirements solve my problem, it works totally fine. And use 
""https://github.com/shawwn/Real-Time-Voice-Cloning"" this branch if you are only a CPU user.",solve problem work totally fine use branch user,issue,negative,positive,positive,positive,positive,positive
748410014,"How can I make a picture like this， X  is frame and Y is mel channel? 
THX :) !",make picture like frame mel channel,issue,negative,neutral,neutral,neutral,neutral,neutral
748395138,"@Langstra The deadsnakes ppa only supports Ubuntu LTS releases (20.04). Try the [Ubuntu setup wiki page](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Installation---Ubuntu-20.04), it uses miniconda to install python 3.7. The CUDA/cuDNN steps are unnecessary in my experience: without them, synthesizer training (tensorflow) still has GPU acceleration.",try setup page install python unnecessary experience without synthesizer training still acceleration,issue,negative,negative,negative,negative,negative,negative
748393502,"X (horizontal) axis is time.
Y (vertical) axis is Hz, with lower freq on top and higher on bottom.",horizontal axis time vertical axis lower top higher bottom,issue,negative,positive,positive,positive,positive,positive
748372966,"Any suggestions for Ubuntu 20.10, there is no version 3.6 or 3.7 of python for this version. Only option is to download Ubuntu?",version python version option,issue,negative,neutral,neutral,neutral,neutral,neutral
748202491,"> @Xnessax check out my video : https://www.youtube.com/watch?v=_4ySXTe5iyQ&feature=youtu.be

the last step would be like ...: c://users/documents/Freixas_voices.mp3 (""I gave up on discussing on politics."") 
Is that right? 
Hope to know about it. 
@justinjohn0306
",check video last step would like gave politics right hope know,issue,positive,positive,positive,positive,positive,positive
744888643,"@Ctibor67 Step-by-step instructions: https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training

Practice on LibriSpeech and then adapt the code to your dataset. No guide available but I provide some hints in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/431#issuecomment-673555684

Your checkpoint.pt from NVIDIA/tacotron2 is not compatible with this repo's tacotron, which uses tensorflow.",practice adapt code guide available provide compatible,issue,negative,positive,positive,positive,positive,positive
744851860,Can someone please advise me (step by step) how to start synthesizer training? And what should be the structure of the csv file? I trained Tacotron2 here (https://github.com/NVIDIA/tacotron2) - is it possible to use such a pre-trained checkpoint.pt?,someone please advise step step start synthesizer training structure file trained possible use,issue,negative,neutral,neutral,neutral,neutral,neutral
743914835,"@blue-fish 
Thanks, It is very helpful.
 I recommend addin it to the README.
It might help another people",thanks helpful recommend might help another people,issue,positive,positive,positive,positive,positive,positive
742510392,"> Hi @amrahsmaytas no I couldn't land to good results. And then I had to shift to another task. Still I'd be glad if I cloud be of any help. 

Thanks for the reply,het! 
I need your help in training, could you please check your mail (send from greetsatyamsharma@gmail.com) and connect me over there for further discussions! 

Thanks ✌, 
Awaiting for your response, dude
Satyam. ",hi could land good shift another task still glad cloud help thanks reply het need help training could please check mail send connect thanks response dude,issue,positive,positive,positive,positive,positive,positive
742505083,Hi @amrahsmaytas no I couldn't land to good results. And then I had to shift to another task. Still I'd be glad if I cloud be of any help. ,hi could land good shift another task still glad cloud help,issue,positive,positive,positive,positive,positive,positive
742454244,@mennatallah644 It is possible to train a single-speaker model with this repository. You should only need to train a synthesizer. The existing encoder and vocoder should work.,possible train model repository need train synthesizer work,issue,negative,neutral,neutral,neutral,neutral,neutral
742405279,"@boydydoydy
1. Learn to train synthesizer with LibriSpeech dataset: [\[wiki page\]](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training)
2. My guide on finetuning the model for better quality: [\[writeup\]](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437), [\[VCTK example\]](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-663308789)

Reorganize your files into this folder structure: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-666099538",learn train synthesizer guide model better quality reorganize folder structure,issue,negative,positive,positive,positive,positive,positive
742364315,"Greetings @thehetpandya 

Are you able to do a real time voice cloning for the given english text, with your experiment in indian accent?

Could you please help/guide me with Voice cloning of english Text In My voice with indian accent?

Thanks",able real time voice given text experiment accent could please voice text voice accent thanks,issue,positive,positive,positive,positive,positive,positive
742146854,"* If you need GPU support, here are instructions for [setup using Ubuntu 20.04 + Miniconda](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Installation---Ubuntu-20.04)
* If you don't need GPU support, this is easier: [setup using Ubuntu 20.04 (no Anaconda)](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/615#issuecomment-740270185)",need support setup need support easier setup anaconda,issue,positive,neutral,neutral,neutral,neutral,neutral
742009816,"I am trying to train encoder, synthesizer and vocoder in Arabic can i train synthesizer with single dataset not multi ?",trying train synthesizer train synthesizer single,issue,negative,negative,neutral,neutral,negative,negative
741671724,"I'd like to echo DeeGeeTill's congratulation - thanks for creating this amazing programme!

I am not a programmer. I am a sound artist and have managed to get the RTVC programme up and running. For the purposes I hope to use it for using British accents would be ideal. As per your response above is retraining the dataset using the VCTK dataset something a programming  noob like me will be able to do? Are there any instructions on how to train using a new dataset that you can point me towards? 

My apologies if all this exists somewhere obvious already. This whole journey into voice cloning has been a slow learner for me. 

Thanks again. ",like echo congratulation thanks amazing programmer sound artist get running hope use would ideal per response something like able train new point towards somewhere obvious already whole journey voice slow learner thanks,issue,positive,positive,positive,positive,positive,positive
740270185,"### Ubuntu 20.04 install instructions (tested)

#### 1. Add repositories
```
sudo add-apt-repository universe
sudo add-apt-repository ppa:deadsnakes/ppa
```
#### 2. Install software
```
snap install ffmpeg
sudo apt install python3.6 python3.6-dev python3 python3-pip git
pip3 install virtualenv
```
Additional steps are needed to overcome bugs with portaudio and QT:
* Portaudio bugfix: https://stackoverflow.com/a/60824906
```
sudo apt install libasound2-dev
git clone -b alsapatch https://github.com/gglockner/portaudio
cd portaudio
./configure && make
sudo make install
sudo ldconfig
cd ..
```
* QT bugfix: https://askubuntu.com/a/1069502
```
sudo apt install --reinstall libxcb-xinerama0
```
#### 3. Make a virtual environment and activate it
```
~/.local/bin/virtualenv --python python36 rtvc
source rtvc/bin/activate
```
#### 4. Download RTVC
```
git clone --depth 1 https://github.com/CorentinJ/Real-Time-Voice-Cloning.git
```
#### 5. Install requirements
```
cd Real-Time-Voice-Cloning
pip install torch
pip install -r requirements.txt
```
#### 6. Get pretrained models
```
wget https://github.com/blue-fish/Real-Time-Voice-Cloning/releases/download/v1.0/pretrained.zip -O pretrained.zip
unzip pretrained.zip
```
#### 7. Launch toolbox
```
python demo_toolbox.py
```",install tested add universe install snap install apt install python python git pip install additional overcome apt install git clone make make install apt install reinstall make virtual environment activate python python source git clone depth install pip install torch pip install get launch toolbox python,issue,negative,positive,positive,positive,positive,positive
740211472,"Nice guide @FreddyFeuerstein! Just below this, I've improved on it to provide step-by-step instructions specific to Ubuntu 20.04.",nice guide provide specific,issue,negative,positive,positive,positive,positive,positive
740195453,"My tensorflow synth from scratch in #538 has similar voice quality issues as the [pretrained model](https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/472#issuecomment-695206377) shared here. So I think the lower quality is not due to the tacotron 1/2 difference, but instead by:
1. **Smaller batch size:** I used batch 12 instead of 144 due to limited vram, and/or
2. **Smaller training dataset:** I curated LibriSpeech aggressively to remove gaps

The model in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/449#issuecomment-665645153 used batch=36, with the full dataset. It has voice quality that is comparable to the incumbent model. So I think it is more likely the restricted dataset that is affecting the voice quality. This gives me some ideas for experiments to improve the pytorch pretrained model.",scratch similar voice quality model think lower quality due difference instead smaller batch size used batch instead due limited smaller training aggressively remove model used full voice quality comparable incumbent model think likely restricted affecting voice quality improve model,issue,negative,positive,neutral,neutral,positive,positive
739337975,"ERROR: Could not find a version that satisfies the requirement pyqt5<5.13 (from spyder) (from versions: 5.14.0, 5.14.1, 5.14.2, 5.15.0, 5.15.1, 5.15.2)
ERROR: No matching distribution found for pyqt5<5.13 (from spyder)
",error could find version requirement error matching distribution found,issue,negative,neutral,neutral,neutral,neutral,neutral
739316532,I tried making an encoder based on LibriSpeech `train-clean-100` (251 speakers). It trained to 60k steps without problems using default settings. Only thing I modified is batch size 64 --> 32 to fit in limited vram. So I think it has something to do with your dataset.,tried making based trained without default thing batch size fit limited think something,issue,negative,positive,positive,positive,positive,positive
739146639,Closing due to inactivity. Reopen if you have more results to share. Thanks for noting that Pytorch 1.7 is compatible with the new 30xx series of GPUs.,due inactivity reopen share thanks compatible new series,issue,positive,positive,neutral,neutral,positive,positive
739144111,"Yes. The synthesizer takes 2 inputs: text and embedding. The embedding is a numerical representation of the target voice. Once you have the embedding output from the encoder (a 256-element array), it can be saved and reused.

https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/5425557efe30863267f805851f918124191e0be0/demo_cli.py#L156",yes synthesizer text numerical representation target voice output array saved,issue,positive,neutral,neutral,neutral,neutral,neutral
739142418,See #411 and #590 for additional explanation of low quality.,see additional explanation low quality,issue,negative,neutral,neutral,neutral,neutral,neutral
739142141,"The encoder is deterministic and doesn't learn when multiple samples are loaded. Set ""random seed"" to a fixed number and your results will be consistent.

You can combine embeddings (256-element array) to make a composite voice. A similar idea is explored in the SV2TTS paper, for audio samples see ""fictitious speakers"" section of: https://google.github.io/tacotron/publications/speaker_adaptation/",deterministic learn multiple loaded set random seed fixed number consistent combine array make composite voice similar idea paper audio see fictitious section,issue,negative,negative,neutral,neutral,negative,negative
738461524,"leaving this here in case anyone else runs into these problems

firstly to get mp3 support you have to add ffmpeg to your PATH, you cant just pip install it

secondly, the reason those dropdowns were unclickable was simply because i was dumb and not configuring my master directory for the pretrained models correctly, which is not a step included in the ""better guide"" but is one included on the main git page.",leaving case anyone else firstly get support add path cant pip install secondly reason simply dumb master directory correctly step included better guide one included main git page,issue,negative,positive,positive,positive,positive,positive
735774682,"This is mainly because your speakers have different accent, than the pretrained model.",mainly different accent model,issue,negative,neutral,neutral,neutral,neutral,neutral
735471720,go for A100 for Training if available or else use P100. I believe the cost is worth it. you can check this link https://towardsdatascience.com/choosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86 for further information,go training available else use believe cost worth check link information,issue,negative,positive,positive,positive,positive,positive
734626791,"Thank you for your recommendation, definitely changing learning rate is one of the first option that comes in my mind, the other option could be gradient clipping. 

I also try reducing the learning rate from (default) 1e-4 to 1e-5, this helps my model survives longer than 25k steps, but then the same issue occurred. 

IMO, the reason could be the size of the data is not large enough. When I used the pre-trained model and fine-tune on my little data, everything works just fine! 

I will leave this issue open for a while in case any other people have a more proper answer to this :) ",thank recommendation definitely learning rate one first option come mind option could gradient clipping also try reducing learning rate default model longer issue reason could size data large enough used model little data everything work fine leave issue open case people proper answer,issue,positive,positive,neutral,neutral,positive,positive
734386997,Thanks for reporting. @blue-fish is correct: they're within their rights given that I put an MIT license. I'll report the vimeo video but the rest is technically fair game.,thanks correct within given put license report video rest technically fair game,issue,positive,positive,positive,positive,positive,positive
733184959,"Thank you for the suggestion, but we want the user to install pytorch using the instructions at https://pytorch.org and not with pip.",thank suggestion want user install pip,issue,negative,neutral,neutral,neutral,neutral,neutral
732637957,Thank you very much for your answer. I will refer to @blue-fish  experiments and try to improve the quality of single-speaker TTS model.,thank much answer refer try improve quality model,issue,positive,positive,positive,positive,positive,positive
732437625,"Closing this PR without merging as it is not a feature many users need. However, it is good to know that it can be accomplished with a minimal code change.",without feature many need however good know accomplished minimal code change,issue,positive,positive,positive,positive,positive,positive
732322381,Got it to work.. Thanks a lot for your suggestions. It is much better now. Broke the longer text into smaller segments and that worked!,got work thanks lot much better broke longer text smaller worked,issue,negative,positive,positive,positive,positive,positive
732250930,Thanks for your reply. I tried with the pretrained model but the speech is still not good for longer texts.  What changes need to be done to break the inputs into smaller chunks? Is this possible to do when running demo_cli.py?,thanks reply tried model speech still good longer need done break smaller possible running,issue,positive,positive,positive,positive,positive,positive
731855686,"Known issue, discussed somewhat in #347. What I think occurs is the attention drops off for very long inputs and the model struggles. You can try my alternative pretrained model in #538, but the usual workaround is to break up your inputs into smaller chunks (add line breaks in the toolbox).",known issue somewhat think attention long model try alternative model usual break smaller add line toolbox,issue,negative,negative,neutral,neutral,negative,negative
731782751,That's something completely different from the SV2TTS paradigm this repo implements. Check out [autovc](https://github.com/auspicious3000/autovc) for an example of a repo that would do it. It's not as user-friendly though.,something completely different paradigm check example would though,issue,negative,neutral,neutral,neutral,neutral,neutral
731639341,"Considering they have an embedded vimeo reupload of the YouTube demo video, I'm pretty sure they are breaking YouTube's terms.

From a quick nslookup, I'm pretty sure they're using Shopify. I can see shopify in the DOM as well, so pretty sure. And if they are. they are most likely breaking their terms of service. Specifically point 3 about general conditions. As an end user, you are not allowed to use shopify for any illegal or sketchy purposes. Could be worth giving shopify a heads up on their abuse portal.",considering video pretty sure breaking quick pretty sure see dom well pretty sure likely breaking service specifically point general end user use illegal sketchy could worth giving abuse portal,issue,positive,positive,positive,positive,positive,positive
731479282,"> And i use pytorch 1.7

That fixed it for me. I installed 1.7 in my test environment and now it loads up fine. Thank you!",use fixed test environment fine thank,issue,positive,positive,positive,positive,positive,positive
731421964,"I uninstalled webrtcvad (your missing noise removal component) and performed the following test in demo_toolbox.py with Corentin's pretrained models:
1. Use `samples/p240_00000.mp3` to generate a speaker embedding.
2. Text: `Welcome to the toolbox! To begin, load an utterance from your datasets or record one yourself.`
3. Random seed box checked, value = 0
4. Expected output (wav file inside zip): [p240_sample.zip](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/5576210/p240_sample.zip)

If your result sounds similar then your toolbox installation is working.

Please set your expectations very low, especially for voice samples that don't sound like a normal person reading an audiobook.",uninstalled missing noise removal component following test use generate speaker text welcome toolbox begin load utterance record one random seed box checked value output file inside zip result similar toolbox installation working please set low especially voice sound like normal person reading,issue,positive,positive,neutral,neutral,positive,positive
731132544,"> Haven't tried that. But are u able to train the encoder?

I haven't actually, I wanted to use the pretrained one and fine tune the synthesizer. I've just seen your post on your new encoder, very nice work, it's great to see people working on using newer GPUs for this project, I'll be following your thread with great interest! 
I went back to an old GTX1070 but I'll switch to my 3080 soon to try everything with CUDA 11.0 and Torch 1.7
",tried able train actually use one fine tune synthesizer seen post new nice work great see people working project following thread great interest went back old switch soon try everything torch,issue,positive,positive,positive,positive,positive,positive
731131604,But i have also multiprocessing issues: if i use num_workers other then 0 i run into a pipeline error. But that doesn't bother me too much. ,also use run pipeline error bother much,issue,negative,positive,positive,positive,positive,positive
731130432,"> > Yes, i'm training the encoder on a rtx 3070. U need to install the latest nvidia drivers and cuda version 11.0. Ampere Graphic cards can only use CUDA Version 11.0 +. But since Pytorch isn't compatible with 11.1 u need 11.0. Newer CUDA Versions are backward compatible, so it's irrelevant, if u use CUDA 11 or 10.
> 
> Have you tried to train the synthesizer and vocoder? I am unable to do it. And even with @blue-fish Pytorch based code, I run into a multiprocessing error in Torch 1.8 - AttributeError: Can't pickle local object - (because I'm on windows I think)

Haven't tried that. But are u able to train the encoder?",yes training need install latest version ampere graphic use version since compatible need backward compatible irrelevant use tried train synthesizer unable even based code run error torch ca pickle local object think tried able train,issue,negative,neutral,neutral,neutral,neutral,neutral
731120911,"> Yes, i'm training the encoder on a rtx 3070. U need to install the latest nvidia drivers and cuda version 11.0. Ampere Graphic cards can only use CUDA Version 11.0 +. But since Pytorch isn't compatible with 11.1 u need 11.0. Newer CUDA Versions are backward compatible, so it's irrelevant, if u use CUDA 11 or 10.

Have you tried to train the synthesizer and vocoder? I am unable to do it. And even with @blue-fish Pytorch based code, I run into a multiprocessing error in Torch 1.8 - AttributeError: Can't pickle local object -  (because I'm on windows I think)
",yes training need install latest version ampere graphic use version since compatible need backward compatible irrelevant use tried train synthesizer unable even based code run error torch ca pickle local object think,issue,negative,negative,neutral,neutral,negative,negative
731116552,"Yes, i'm training the encoder on a rtx 3070. U need to install the latest nvidia drivers and cuda version 11.0.  Ampere Graphic cards can only use CUDA Version 11.0 +.  But since Pytorch isn't compatible with 11.1 u need 11.0. Newer CUDA Versions are backward compatible, so it's irrelevant, if u use CUDA 11 or 10.  ",yes training need install latest version ampere graphic use version since compatible need backward compatible irrelevant use,issue,negative,neutral,neutral,neutral,neutral,neutral
730728480,There might be some copyright violations involved in reuploading Corentin's youtube video and using images from [this video](https://www.youtube.com/watch?v=pKoOw5a74XU) on the webpage.,might copyright involved video video,issue,negative,neutral,neutral,neutral,neutral,neutral
730726628,"The [license](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/LICENSE.txt) gives permission to sell copies of the software. It would be nice if those who take from this project also give back, but it is not a requirement.",license permission sell would nice take project also give back requirement,issue,negative,positive,positive,positive,positive,positive
730658434,Still locking up when I try and select an audio file.,still locking try select audio file,issue,negative,neutral,neutral,neutral,neutral,neutral
730605368,"Same here, the code is based on CUDA 10 / tensorflow 1.15 which are not compatible with ampere achitecure (30x0 series)... 
If anyone has a clue on how to make the code compatible with CUDA 11 and tensorflow 2 it would be great",code based compatible ampere series anyone clue make code compatible would great,issue,positive,positive,positive,positive,positive,positive
730260332,"Make sure you placed the correct pretrained.pt file in `encoder/saved_models/pretrained.pt` , filesize should be 17090379. See instructions here: https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models",make sure correct file see,issue,negative,positive,positive,positive,positive,positive
729945377,"Make sure to set your environment variables as in: https://video.stackexchange.com/a/20496

Thank you for reporting the numpy issue, we have updated requirements.txt in #597 so other users will not experience the error.

",make sure set environment thank issue experience error,issue,negative,positive,positive,positive,positive,positive
729898976,"> Try `pip install numpy==1.19.3` as suggested in [numpy/numpy#17746 (comment)](https://github.com/numpy/numpy/issues/17746#issuecomment-724782963)

thank you! now i am one step away from being able to use this. but there is one issue. it keeps saying i need ffmpeg even though i already have ffmpeg, ive done pip install ffmpeg but i still get the error. have any tips?",try pip install comment thank one step away able use one issue saying need even though already done pip install still get error,issue,negative,positive,positive,positive,positive,positive
728717742,"The short answer: it's an industry-grade project.

There's 5 machine learning engineers (myself included) and 4 non-ML engineers in the Resemble team. We've been iterating over the project for 2 years and we have contracts with other companies that are expecting strong results from us. We're far away from this student project which I built alone in 8 months while studying other courses; even though ML has its way of making every small incremental improvement harder to get than the last.

Our TTS engine at Resemble contains hundreds of improvements over this one, as well as a truckload of features that allow for more applications and more customization. We also indeed have a different routine for cloning a voice than what is done here. SV2TTS has its limitations (as noted by the authors themselves) and while it serves a nice purpose, we are aiming for a higher overall synthesis quality. What @mbdash suggests should already be a good step forward.",short answer project machine learning included resemble team project strong u far away student project built alone even though way making every small incremental improvement harder get last engine resemble one well truckload allow also indeed different routine voice done noted nice purpose aiming higher overall synthesis quality already good step forward,issue,positive,positive,positive,positive,positive,positive
726080278,"See @blue-fish experiments here:
Single speaker fine-tuning process and results https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437

> Summary
A relatively easy way to improve the quality of the toolbox output is through fine-tuning of the multispeaker pretrained models on a dataset of a single target speaker. Although it is no longer voice cloning, it is a shortcut for obtaining a single-speaker TTS model with less training data needed relative to training from scratch. This idea is not original, but a sample single-speaker model is presented along with a process and data for replicating the model.

> Improvement in quality is obtained by taking the pretrained synthesizer model and training a few thousand steps on a single-speaker dataset. This amount of training can be done in less than a day on a CPU, and even faster with a GPU.",see single speaker process summary relatively easy way improve quality toolbox output single target speaker although longer voice model le training data relative training scratch idea original sample model along process data model improvement quality taking synthesizer model training thousand amount training done le day even faster,issue,positive,positive,positive,positive,positive,positive
726054272,"Thanks for your answer.
So, using this model, how to improve the quality of cloned voice? Collect 50 vocal sentences and fine-tune the model?",thanks answer model improve quality voice collect vocal model,issue,positive,positive,positive,positive,positive,positive
725286855,"I believe resemble.ai uses distributed cloud buckets to store and iterate on user-specific models. That is, *I think* it has a huge training set for its base models, then specifically trains account-centric models separately using a much stronger set of computers than a normal human being would be able to afford. I might be wrong, of course. ",believe distributed cloud store iterate think huge training set base specifically separately much set normal human would able afford might wrong course,issue,negative,negative,neutral,neutral,negative,negative
724234082,"Sorry but we no longer use the issues board for general technical support questions, for reasons similar to [`tensorflow/ISSUES.md`](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md). If you have evidence that this is caused by a bug in the repo code, please share and we'll troubleshoot.",sorry longer use board general technical support similar evidence bug code please share,issue,positive,negative,negative,negative,negative,negative
724111989,We don't have a guide at this time. Try making your custom dataset with a similar structure as LibriTTS: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-666099538,guide time try making custom similar structure,issue,negative,neutral,neutral,neutral,neutral,neutral
723523011,Hello I am interested in the voice to voice system,hello interested voice voice system,issue,negative,positive,positive,positive,positive,positive
723016688,"Look forward to your answer, thanks.",look forward answer thanks,issue,negative,positive,positive,positive,positive,positive
722555090,"@237sankalp Check and see if you have a hidden folder in `train-other-500`. You can also check the value of `speaker_dirs` when this line is executed.

https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/9ee277332a8ef049c14bd7298a08e9adf20cb9bc/encoder/preprocess.py#L63",check see hidden folder also check value line executed,issue,negative,negative,negative,negative,negative,negative
722503344,"@blue-fish thanks,
![doubt](https://user-images.githubusercontent.com/66549964/98270784-385b9600-1fb5-11eb-8062-a6735dbf7692.png)
for just experimenting I am providing the model with 1 speaker input but I am confused as to why the system is showing 2 speakers when I am only providing it with 1 speaker named ""20"".  I think that the extra addition of the speaker is giving me the problem to train the encoder, as it is a null set.",thanks doubt providing model speaker input confused system showing providing speaker think extra addition speaker giving problem train null set,issue,negative,negative,neutral,neutral,negative,negative
721981605,"This error message is encountered when there a subfolder in SV2TTS/encoder does not contain any .npy files. This happens because in preprocessing, folders are created for each speaker, but in some cases there are no valid files to populate the folder.

We should improve the preprocess code so the speaker folder is only created if it contains audio files. This is the line in question that should be moved:

https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/9ee277332a8ef049c14bd7298a08e9adf20cb9bc/encoder/preprocess.py#L73",error message contain speaker valid populate folder improve code speaker folder audio line question,issue,negative,neutral,neutral,neutral,neutral,neutral
721874292,"Just so you know, there is an issue which I think is specific to Windows when trying to train the synthesizer, I think it's related to multiprocessing and for some reason it doesn't work on Windows...

Traceback (most recent call last):
  **File ""synthesizer_train.py"", line 35, in <module>**
    train(\*\*vars(args))
  **File ""D:\dfvoc\Real-Time-Voice-Cloning-447_pytorch_synthesizer\synthesizer\train.py"", line 158, in train
    for i, (texts, mels, embeds, idx) in enumerate(data_loader, 1):**
  File ""C:\Users\Remi\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\utils\data\dataloader.py"", line 359, in __iter__
    return self._get_iterator()
  File ""C:\Users\Remi\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\utils\data\dataloader.py"", line 301, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File ""C:\Users\Remi\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\utils\data\dataloader.py"", line 885, in __init__
    w.start()
  File ""C:\Users\Remi\AppData\Local\Programs\Python\Python37\lib\multiprocessing\process.py"", line 112, in start
    self._popen = self._Popen(self)
  File ""C:\Users\Remi\AppData\Local\Programs\Python\Python37\lib\multiprocessing\context.py"", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File ""C:\Users\Remi\AppData\Local\Programs\Python\Python37\lib\multiprocessing\context.py"", line 322, in _Popen
    return Popen(process_obj)
  File ""C:\Users\Remi\AppData\Local\Programs\Python\Python37\lib\multiprocessing\popen_spawn_win32.py"", line 89, in __init__
    reduction.dump(process_obj, to_child)
  File ""C:\Users\Remi\AppData\Local\Programs\Python\Python37\lib\multiprocessing\reduction.py"", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
**AttributeError: Can't pickle local object 'train.\<locals>.\<lambda>'**
PS D:\dfvoc\Real-Time-Voice-Cloning-447_pytorch_synthesizer> Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\Remi\AppData\Local\Programs\Python\Python37\lib\multiprocessing\spawn.py"", line 105, in spawn_main
    exitcode = _main(fd)
  File ""C:\Users\Remi\AppData\Local\Programs\Python\Python37\lib\multiprocessing\spawn.py"", line 115, in _main
    self = reduction.pickle.load(from_parent)
**EOFError: Ran out of input**

I'll try yo find a way around it ",know issue think specific trying train synthesizer think related reason work recent call last file line module train file line train enumerate file line return file line return self file line file line start self file line return file line return file line file line dump file protocol ca pickle local object lambda recent call last file string line module file line file line self ran input try yo find way around,issue,negative,neutral,neutral,neutral,neutral,neutral
721826704,"Try a relative path:
```
synthesizer = Synthesizer(Path(""synthesizer/saved_models/logs-pretrained/taco_pretrained""))
```

Or this command which may be closer to what you intended:
```
synthesizer = Synthesizer(Path(project_name + ""/synthesizer/saved_models/logs-pretrained/taco_pretrained""))
```

Most users starting out with the toolbox use either of these commands. You can study the code in `demo_cli.py` and adapt it for your specific purposes.
```
python demo_cli.py
python demo_toolbox.py
```

Also, I apologize but the github issues board is not for technical support. It is for coordinating development of the software and reporting bugs. This is a research code, not a finished product. For these types of problems you'll need to seek help somewhere else.",try relative path synthesizer synthesizer path command may closer intended synthesizer synthesizer path starting toolbox use either study code adapt specific python python also apologize board technical support development research code finished product need seek help somewhere else,issue,positive,neutral,neutral,neutral,neutral,neutral
721816202,@rallandr You need to download the code from my fork to use this model: [`blue-fish:447_pytorch_synthesizer`](https://github.com/blue-fish/Real-Time-Voice-Cloning/tree/447_pytorch_synthesizer),need code fork use model,issue,negative,neutral,neutral,neutral,neutral,neutral
721805388,"Thanks @blue-fish

However I keep running in a CUDNN error while trying to train the vocoder.
""RuntimeError: Unable to find a valid cuDNN algorithm to run convolution"" and it doesn't start the training.

With smaller batch size it starts but stops after few seconds with the following cudnn error : 
""{| Epoch: 1 (11/52) | Loss: 4.1256 | 0.8 steps/s | Step: 436k | }Traceback (most recent call last):
  File ""vocoder_train.py"", line 55, in <module>
    train(**vars(args))
  File ""D:\dfvoc\Real-Time-Voice-Cloning-master\vocoder\train.py"", line 106, in train
    running_loss += loss.item()
RuntimeError: CUDA error: unspecified launch failure""


I guess this is because CUDA 11 works with cudnn 8 and not cudnn7...",thanks however keep running error trying train unable find valid algorithm run convolution start training smaller batch size following error epoch loss step recent call last file line module train file line train error unspecified launch failure guess work,issue,negative,negative,neutral,neutral,negative,negative
721803171,"> Here is a pretrained model that can be used with the synthesizer: https://www.dropbox.com/s/gemy4hlm3fdf76u/librispeech_130k.zip?dl=0
> 
> 1. Place `pretrained.pt` in `synthesizer/saved_models/pretrained/pretrained.pt`
> 2. Ignore the hparams.py in the zip file. The required changes were merged into the branch in commit [`39f90da`](https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/472/commits/39f90daa6ceb853657dace53a4449ffaf66cf349)
> 
> Would anyone testing this please take the time to leave a comment with your thoughts on how well this pretrained model performs?

Thanks for the material to start a Pytorch synthesizer, however I dont get how you make the toolbox run with this new format (.pt) for the syntesizer model. When using the toolbox it expects synthesizer weights... Thanks for any insight you could give",model used synthesizer place ignore zip file branch commit would anyone testing please take time leave comment well model thanks material start synthesizer however dont get make toolbox run new format model toolbox synthesizer thanks insight could give,issue,positive,positive,positive,positive,positive,positive
721758018,"> 
> 
> > @shawwn I've uploaded the models to my dropbox. The vocoder is still training and will be for another 24-48 hours. Please share whatever you end up making with them!
> > #### Encoder
> > https://www.dropbox.com/s/xl2wr13nza10850/encoder.zip?dl=0
> > #### Synthesizer (Tacotron)
> > https://www.dropbox.com/s/t7qk0aecpps7842/tacotron.zip?dl=0
> > #### Vocoder
> > https://www.dropbox.com/s/bgzeaid0nuh7val/vocoder.zip?dl=0
> 
> Dear All,
> i've downloaded the models from @sberryman and adapted the hyper parameters accordingly.
> I created a few examples with them. I observe the following:
> 
>     1. the sound quality is pretty good (clearly understandable, no bleeps or blops etc.)
> 
>     2. the voice does not resemble the reference embedding. it's like a 'generic' voice.
> 
> 
> I wonder why that is. Did anybody else experience this?
> Thanks!

i also experience that
did you solve this issue?",still training another please share whatever end making synthesizer dear hyper accordingly observe following sound quality pretty good clearly understandable voice resemble reference like voice wonder anybody else experience thanks also experience solve issue,issue,positive,positive,positive,positive,positive,positive
721545163,"> > I seem to solve this problem. It turned out that my original data has an illegal speaker folder. Thank you for helping me debug.
> 
> @sunnnnnnnny How did you manage to find this illegal speaker folder

The same problem, I am also not getting it how did you find an illegal speaker.",seem solve problem turned original data illegal speaker folder thank helping manage find illegal speaker folder problem also getting find illegal speaker,issue,negative,negative,negative,negative,negative,negative
721237876,"It's most likely due to the configuration of your system changing over that time.

However, it could also be one of these two things:
1. Dropout. The synthesizer (tacotron) has a prenet in the encoder and decoder where some elements are randomly set to zero. This is mainly done to prevent model overfitting during training, but is also used in inference to get some variation in the output. If you click the ""random seed"" button on the UI, it will make generation deterministic.
2. The file used to generate the speaker embedding must be identical. Any variation may lead to a different embedding and the synthesizer output is sometimes sensitive to small differences in the embedding.

If you study this issue further and have evidence of a bug, please submit it and we'll troubleshoot it further. However, I spent a lot of time on deterministic generation (#432) and think it's unlikely there is a bug (at least not one that affects all users).",likely due configuration system time however could also one two dropout synthesizer randomly set zero mainly done prevent model training also used inference get variation output click random seed button make generation deterministic file used generate speaker must identical variation may lead different synthesizer output sometimes sensitive small study issue evidence bug please submit however spent lot time deterministic generation think unlikely bug least one,issue,negative,negative,negative,negative,negative,negative
720707936,"Tacotron uses ""teacher forcing"" training which means the decoder cheats and uses the ground truth mel to predict subsequent frames. It's necessary to ensure the input and output have same length which facilitates a loss calculation. For inference you do not have a ground truth, so the model has to rely on what it learned during training. Toolbox output with your new model will not be intelligible until about 20-50k steps. You need 150-250k steps not to have serious bugs during inference. 

A few words about dataset quality: if your dataset is good, it may converge in fewer steps. And if it is bad, your inference will never be good. Many free datasets are bad quality.

I will recommend that you modify [synthesizer/utils/symbols.py](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/synthesizer/utils/symbols.py) to contain all letters of your alphabet. Anything that is not in that list will be ignored by the model, and make it very hard to generalize.

There's a learning curve to this. Since you are training a new model, it will be very helpful to **study the code carefully and figure out what it's doing.** If there is something you don't understand, read the papers linked in the README for context.",teacher forcing training ground truth mel predict subsequent necessary ensure input output length loss calculation inference ground truth model rely learned training toolbox output new model intelligible need serious inference quality good may converge bad inference never good many free bad quality recommend modify contain alphabet anything list model make hard generalize learning curve since training new model helpful study code carefully figure something understand read linked context,issue,positive,negative,neutral,neutral,negative,negative
720692292,"I think im importing it wrong because the wavs folder gives me some what understandable audio files.
for importing the synthesizer to the toolbox it is just selecting the name from the dropdown menu thingie right?",think wrong folder understandable audio synthesizer toolbox name menu right,issue,negative,negative,negative,negative,negative,negative
720686996,"but the plots I received are looking way better than in the toolbox
![step-4000-mel-spectrogram](https://user-images.githubusercontent.com/57623355/97912065-8de13a00-1d4c-11eb-9231-35b40cb976dd.png)
that one is after 4K steps, the toolbox plots are just blue or completely green",received looking way better toolbox step one toolbox blue completely green,issue,negative,positive,neutral,neutral,positive,positive
720685183,"I preproccesed and embedded with 'with metadata_fpath.open(""r"", encoding=""latin-1"") as metadata_file:'
because that recognized all my characters, do i have to change somthing in the hparams.py for the training because right now with a loss of 0.9 i have random synthesizer results, just white noice.",change training right loss random synthesizer white,issue,negative,negative,neutral,neutral,negative,negative
720672070,"Because tensorflow 1.x only supports CUDA 10, you'll need a new synthesizer model. I have a pytorch-only synthesizer in #472 ([pretrained model](https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/472#issuecomment-695206377)). No modifications should be needed for vocoder training.

I don't expect bugs with pytorch 1.8 but please report them if they are encountered.",need new synthesizer model synthesizer model training expect please report,issue,negative,positive,positive,positive,positive,positive
720667271,"Thank you very much!
its training a dutch model right now.
thank you for all the support!",thank much training dutch model right thank support,issue,positive,positive,positive,positive,positive,positive
720659260,"All of the utterances in the training batch are padded to match the length of the longest one. To reduce memory consumption you can also reduce `hparams.max_mel_frames`. I've had good results with 500 and 600. You can use [this code](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/449#issuecomment-671071445) to modify your SV2TTS/synthesizer/train.txt to remove utterances that are too long. I have successfully trained a synth model with a batch size of 12 (see #538), so you can go much lower if needed.

You can stop the training at every time, but you will lose all progress since the last saved checkpoint. Use `--checkpoint_interval` to save more frequently if needed. Restart training with the same python command, if it finds a saved checkpoint it resumes by default.",training batch match length one reduce memory consumption also reduce good use code modify remove long successfully trained model batch size see go much lower stop training every time lose progress since last saved use save frequently restart training python command saved default,issue,positive,positive,positive,positive,positive,positive
720632991,"what is the best way of stopping the training so you can test if the model is any good?
and how van you continue to train on it again later",best way stopping training test model good van continue train later,issue,positive,positive,positive,positive,positive,positive
720616802,"It's either the attention mechanism or the stop token prediction. Some users work around this by padding the input with words to be trimmed in post-processing.

You can [return the alignments](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/47a9093f901469486fca5b83babeb26ca98fa85d/synthesizer/inference.py#L104) during inference and [plot them](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/47a9093f901469486fca5b83babeb26ca98fa85d/synthesizer/utils/plot.py#L15-L38). For the input `Welcome to the toolbox!` I get [this plot](https://user-images.githubusercontent.com/67130644/97897271-18914d00-1ceb-11eb-9655-cbf879d48249.png). The ""encoder timestep"" refers to the input characters (don't forget to account for [text normalization](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/47a9093f901469486fca5b83babeb26ca98fa85d/synthesizer/utils/text.py#L56-L62)), for each ""decoder timestep"" Tacotron will generate `hparams.outputs_per_step=2` frames of mel spectrogram output (each equal to frame_shift_ms or hop_size/sample_rate, default 0.0125 sec).

By scrutinizing this output you can figure out when Tacotron is working on each letter of your input sequence, and if there's an attention failure towards the end. Since it consistently happens at the end, it's more likely an issue with the stop token being predicted too early. If this is the case, then it could be a problem of the training data or the algorithm itself.

Nowadays attention is being replaced with feedforward schemes where Tacotron is trained to predict phoneme duration which leads to more stable and predictable output from inference. For a repo you can see [ForwardTacotron](https://github.com/as-ideas/ForwardTacotron) and for a paper you can check out [FastSpeech](https://arxiv.org/abs/1905.09263) or [Non-Attentive Tacotron](https://arxiv.org/abs/2010.04301).

I'll leave this open for a few days to give others a chance to comment and discuss, but this is ""wontfix"" unless you are going to work on it.",either attention mechanism stop token prediction work around padding input return inference plot input welcome toolbox get plot input forget account text normalization generate mel spectrogram output equal default sec output figure working letter input sequence attention failure towards end since consistently end likely issue stop token early case could problem training data algorithm nowadays attention trained predict phoneme duration stable predictable output inference see paper check leave open day give chance comment discus unless going work,issue,negative,positive,neutral,neutral,positive,positive
720587409,"Please open an issue in the pytorch repo, they will be able to help you a lot better than we can. https://github.com/pytorch/pytorch/",please open issue able help lot better,issue,positive,positive,positive,positive,positive,positive
720117831,"You might need to change the character encoding to utf-8 in this line:
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/47a9093f901469486fca5b83babeb26ca98fa85d/synthesizer/preprocess.py#L77

```
with text_fpath.open(""r"", encoding=""utf-8"") as text_file:
```

Also, I apologize but the github issues board is not for technical support. It is for coordinating development of the software and reporting bugs. This is a research code, not a finished product. You'll have to get help somewhere else if there's a problem that you're unable to solve.",might need change character line also apologize board technical support development research code finished product get help somewhere else problem unable solve,issue,positive,negative,negative,negative,negative,negative
720075427,"Same error asgain...

`(voice2) E:\Python\Voice\Real-Time-Voice-Cloning>python synthesizer_preprocess_audio.py E:\Python\ADATA --datasets_name LibriTTS --subfolders train-clean-100 --no_alignments
2020-11-01 11:36:43.492077: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
Arguments:
    datasets_root:   E:\Python\ADATA
    out_dir:         E:\Python\ADATA\SV2TTS\synthesizer
    n_processes:     None
    skip_existing:   False
    hparams:
    no_alignments:   True
    datasets_name:   LibriTTS
    subfolders:      train-clean-100

Using data from:
    E:\Python\ADATA\LibriTTS\train-clean-100
LibriTTS:   0%|                                                                                                                                                                                                                                                                                                              | 0/1 [00:00<?, ?speakers/s]2020-11-01 11:36:49.384569: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-11-01 11:36:49.387587: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-11-01 11:36:49.394051: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-11-01 11:36:49.397403: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-11-01 11:36:49.408732: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-11-01 11:36:49.434932: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-11-01 11:36:49.449949: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-11-01 11:36:49.453167: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-11-01 11:36:49.459298: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-11-01 11:36:49.459375: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-11-01 11:36:49.488964: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-11-01 11:36:49.490559: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
C:\Users\quiri\anaconda3\envs\voice2\lib\site-packages\librosa\core\audio.py:162: UserWarning: PySoundFile failed. Trying audioread instead.
  warnings.warn(""PySoundFile failed. Trying audioread instead."")
C:\Users\quiri\anaconda3\envs\voice2\lib\site-packages\librosa\core\audio.py:162: UserWarning: PySoundFile failed. Trying audioread instead.
  warnings.warn(""PySoundFile failed. Trying audioread instead."")
LibriTTS:   0%|                                                                                                                                                                                                                                                                                                            | 0/1 [1:03:16<?, ?speakers/s]
multiprocessing.pool.RemoteTraceback:
""""""
Traceback (most recent call last):
  File ""C:\Users\quiri\anaconda3\envs\voice2\lib\multiprocessing\pool.py"", line 121, in worker
    result = (True, func(*args, **kwds))
  File ""E:\Python\Voice\Real-Time-Voice-Cloning\synthesizer\preprocess.py"", line 78, in preprocess_speaker
    text = """".join([line for line in text_file])
  File ""E:\Python\Voice\Real-Time-Voice-Cloning\synthesizer\preprocess.py"", line 78, in <listcomp>
    text = """".join([line for line in text_file])
  File ""C:\Users\quiri\anaconda3\envs\voice2\lib\encodings\cp1252.py"", line 23, in decode
    return codecs.charmap_decode(input,self.errors,decoding_table)[0]
UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 42: character maps to <undefined>
""""""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""synthesizer_preprocess_audio.py"", line 59, in <module>
    preprocess_dataset(**vars(args))
  File ""E:\Python\Voice\Real-Time-Voice-Cloning\synthesizer\preprocess.py"", line 35, in preprocess_dataset
    for speaker_metadata in tqdm(job, datasets_name, len(speaker_dirs), unit=""speakers""):
  File ""C:\Users\quiri\anaconda3\envs\voice2\lib\site-packages\tqdm\std.py"", line 1171, in __iter__
    for obj in iterable:
  File ""C:\Users\quiri\anaconda3\envs\voice2\lib\multiprocessing\pool.py"", line 748, in next
    raise value
UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 42: character maps to <undefined>`

do you know what to do",error voice python successfully dynamic library none false true data successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library trying instead trying instead trying instead trying instead recent call last file line worker result true file line text line line file line text line line file line decode return input ca decode position character undefined exception direct cause following exception recent call last file line module file line job file line iterable file line next raise value ca decode position character undefined know,issue,positive,positive,neutral,neutral,positive,positive
720064455,"**Already fixed this by uninstalling tensorflow & tensorflow-gpu and installing again**

I removed the "" from all txt files and all other weird characters, now i get this error

`(voice2) E:\Python\Voice\Real-Time-Voice-Cloning>python synthesizer_preprocess_audio.py E:\Python\ADATA --datasets_name LibriTTS --subfolders train-clean-100 --no_alignments
2020-11-01 11:13:45.064517: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
Traceback (most recent call last):
  File ""synthesizer_preprocess_audio.py"", line 1, in <module>
    from synthesizer.preprocess import preprocess_dataset
  File ""E:\Python\Voice\Real-Time-Voice-Cloning\synthesizer\preprocess.py"", line 2, in <module>
    from synthesizer import audio
  File ""E:\Python\Voice\Real-Time-Voice-Cloning\synthesizer\audio.py"", line 4, in <module>
    import tensorflow as tf
  File ""C:\Users\quiri\anaconda3\envs\voice2\lib\site-packages\tensorflow\__init__.py"", line 99, in <module>
    from tensorflow_core import *
  File ""C:\Users\quiri\anaconda3\envs\voice2\lib\site-packages\tensorflow_core\__init__.py"", line 36, in <module>
    from tensorflow._api.v1 import compat
  File ""C:\Users\quiri\anaconda3\envs\voice2\lib\site-packages\tensorflow_core\_api\v1\compat\__init__.py"", line 23, in <module>
    from tensorflow._api.v1.compat import v1
  File ""C:\Users\quiri\anaconda3\envs\voice2\lib\site-packages\tensorflow_core\_api\v1\compat\v1\__init__.py"", line 674, in <module>
    [_module_util.get_parent_dir(estimator)] + _current_module.__path__)
  File ""C:\Users\quiri\anaconda3\envs\voice2\lib\site-packages\tensorflow_core\python\tools\module_util.py"", line 24, in get_parent_dir
    return os.path.abspath(os.path.join(os.path.dirname(module.__file__), ""..""))
  File ""C:\Users\quiri\anaconda3\envs\voice2\lib\ntpath.py"", line 221, in dirname
    return split(p)[0]
  File ""C:\Users\quiri\anaconda3\envs\voice2\lib\ntpath.py"", line 183, in split
    p = os.fspath(p)
TypeError: expected str, bytes or os.PathLike object, not NoneType`",already fixed removed weird get error voice python successfully dynamic library recent call last file line module import file line module synthesizer import audio file line module import file line module import file line module import file line module import file line module estimator file line return file line return split file line split object,issue,negative,negative,neutral,neutral,negative,negative
719984690,"i think i foudn the problem

> Ik heb net alle seizoenen van “Friends” in één ruk uitgekeken.	

It is porbbably the """".",think problem net van,issue,negative,neutral,neutral,neutral,neutral,neutral
719983943,"I have around 40K.wav and txt files but the /SV2TTS/synnthesizer/audio ends with 12K files is there something wrong
I get a long output:

`  File ""C:\Users\quiri\anaconda3\envs\voice1\lib\runpy.py"", line 96, in _run_module_code
  File ""E:\Python\Voice\Real-Time-Voice-Cloning\synthesizer_preprocess_audio.py"", line 1, in <module>
    exec(code, run_globals)
    mod_name, mod_spec, pkg_name, script_name)
    from synthesizer.preprocess import preprocess_dataset
  File ""E:\Python\Voice\Real-Time-Voice-Cloning\synthesizer_preprocess_audio.py"", line 1, in <module>
  File ""C:\Users\quiri\anaconda3\envs\voice1\lib\runpy.py"", line 85, in _run_code
  File ""E:\Python\Voice\Real-Time-Voice-Cloning\synthesizer\preprocess.py"", line 5, in <module>
    from synthesizer.preprocess import preprocess_dataset
  File ""E:\Python\Voice\Real-Time-Voice-Cloning\synthesizer\preprocess.py"", line 5, in <module>
    from encoder import inference as encoder
    exec(code, run_globals)
    from encoder import inference as encoder
  File ""E:\Python\Voice\Real-Time-Voice-Cloning\encoder\inference.py"", line 2, in <module>
  File ""E:\Python\Voice\Real-Time-Voice-Cloning\synthesizer_preprocess_audio.py"", line 1, in <module>
  File ""E:\Python\Voice\Real-Time-Voice-Cloning\encoder\inference.py"", line 2, in <module>
    from encoder.model import SpeakerEncoder
  File ""E:\Python\Voice\Real-Time-Voice-Cloning\encoder\model.py"", line 5, in <module>
    from encoder.model import SpeakerEncoder
  File ""E:\Python\Voice\Real-Time-Voice-Cloning\encoder\model.py"", line 5, in <module>
    from torch.nn.utils import clip_grad_norm_
    from synthesizer.preprocess import preprocess_dataset
    from torch.nn.utils import clip_grad_norm_
  File ""C:\Users\quiri\anaconda3\envs\voice1\lib\site-packages\torch\__init__.py"", line 117, in <module>
  File ""E:\Python\Voice\Real-Time-Voice-Cloning\synthesizer\preprocess.py"", line 5, in <module>
  File ""C:\Users\quiri\anaconda3\envs\voice1\lib\site-packages\torch\__init__.py"", line 117, in <module>
    from encoder import inference as encoder
    raise err
OSError: [WinError 1455] The paging file is too small for this operation to complete. Error loading ""C:\Users\quiri\anaconda3\envs\voice1\lib\site-packages\torch\lib\caffe2_detectron_ops_gpu.dll"" or one of its dependencies.
    raise err
OSError: [WinError 1455] The paging file is too small for this operation to complete. Error loading ""C:\Users\quiri\anaconda3\envs\voice1\lib\site-packages\torch\lib\caffe2_detectron_ops_gpu.dll"" or one of its dependencies.
  File ""E:\Python\Voice\Real-Time-Voice-Cloning\encoder\inference.py"", line 2, in <module>
    from encoder.model import SpeakerEncoder
  File ""E:\Python\Voice\Real-Time-Voice-Cloning\encoder\model.py"", line 5, in <module>
    from torch.nn.utils import clip_grad_norm_
  File ""C:\Users\quiri\anaconda3\envs\voice1\lib\site-packages\torch\__init__.py"", line 117, in <module>
    raise err
OSError: [WinError 1455] The paging file is too small for this operation to complete. Error loading ""C:\Users\quiri\anaconda3\envs\voice1\lib\site-packages\torch\lib\caffe2_detectron_ops_gpu.dll"" or one of its dependencies.
C:\Users\quiri\anaconda3\envs\voice1\lib\site-packages\librosa\core\audio.py:162: UserWarning: PySoundFile failed. Trying audioread instead.
  warnings.warn(""PySoundFile failed. Trying audioread instead."")
2020-10-31 20:15:40.735238: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-10-31 20:15:40.775576: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-10-31 20:15:40.805657: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-10-31 20:15:40.885325: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
C:\Users\quiri\anaconda3\envs\voice1\lib\site-packages\librosa\core\audio.py:162: UserWarning: PySoundFile failed. Trying audioread instead.
  warnings.warn(""PySoundFile failed. Trying audioread instead."")
2020-10-31 20:15:41.728081: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-10-31 20:15:41.800110: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-10-31 20:15:41.815603: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
LibriTTS:   0%|                                                                            | 0/1 [46:28<?, ?speakers/s]
multiprocessing.pool.RemoteTraceback:
""""""
Traceback (most recent call last):
  File ""C:\Users\quiri\anaconda3\envs\voice1\lib\multiprocessing\pool.py"", line 121, in worker
    result = (True, func(*args, **kwds))
  File ""E:\Python\Voice\Real-Time-Voice-Cloning\synthesizer\preprocess.py"", line 78, in preprocess_speaker
    text = """".join([line for line in text_file])
  File ""E:\Python\Voice\Real-Time-Voice-Cloning\synthesizer\preprocess.py"", line 78, in <listcomp>
    text = """".join([line for line in text_file])
  File ""C:\Users\quiri\anaconda3\envs\voice1\lib\encodings\cp1252.py"", line 23, in decode
    return codecs.charmap_decode(input,self.errors,decoding_table)[0]
UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 42: character maps to <undefined>
""""""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""synthesizer_preprocess_audio.py"", line 59, in <module>
    preprocess_dataset(**vars(args))
  File ""E:\Python\Voice\Real-Time-Voice-Cloning\synthesizer\preprocess.py"", line 35, in preprocess_dataset
    for speaker_metadata in tqdm(job, datasets_name, len(speaker_dirs), unit=""speakers""):
  File ""C:\Users\quiri\anaconda3\envs\voice1\lib\site-packages\tqdm\std.py"", line 1171, in __iter__
    for obj in iterable:
  File ""C:\Users\quiri\anaconda3\envs\voice1\lib\multiprocessing\pool.py"", line 748, in next
    raise value
UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 42: character maps to <undefined>`",around something wrong get long output file line file line module code import file line module file line file line module import file line module import inference code import inference file line module file line module file line module import file line module import file line module import import import file line module file line module file line module import inference raise err file small operation complete error loading one raise err file small operation complete error loading one file line module import file line module import file line module raise err file small operation complete error loading one trying instead trying instead successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library trying instead trying instead successfully dynamic library successfully dynamic library successfully dynamic library recent call last file line worker result true file line text line line file line text line line file line decode return input ca decode position character undefined exception direct cause following exception recent call last file line module file line job file line iterable file line next raise value ca decode position character undefined,issue,positive,negative,neutral,neutral,negative,negative
719980103,"> the synthesizer_preprocess_audio.py uses a lot of cpu?

You're welcome. The high CPU usage is expected.",lot welcome high usage,issue,negative,positive,positive,positive,positive,positive
719976064,"Thanks, that fixed it! is it correct even tough i installed the tensorflow gpu version, the synthesizer_preprocess_audio.py uses a lot of cpu?",thanks fixed correct even tough version lot,issue,negative,negative,neutral,neutral,negative,negative
719972524,"You might need to put the audio files inside a ""book"" folder that is contained in the speaker folder. See this: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-666099538

Also, make sure to use the `--no_alignments` option for preprocess since you have separate text files for each utterance.",might need put audio inside book folder speaker folder see also make sure use option since separate text utterance,issue,negative,positive,positive,positive,positive,positive
719965773,"I apologize, I am not available to provide consultation outside of the issues board here. For now, my priorities are 1) code development and 2) bug fixes. I answer support questions as time permits but that is not my purpose here.

_Originally posted by @blue-fish in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/486#issuecomment-673397231_",apologize available provide consultation outside board code development bug answer support time purpose posted,issue,negative,positive,positive,positive,positive,positive
719928606,"Hi, i separeted all the transcripts into common_voice_nl_17691033.txt and then i have the file common_voice_nl_17691033.mp4 repeated for all the files i have all in one folder, but i get this error:

LibriTTS: 100%|██████████████████████████████████████████████████████████████████████| 83648/83648 [00:20<00:00, 4128.04speakers/s]
The dataset consists of 0 utterances, 0 mel frames, 0 audio timesteps (0.00 hours).
Traceback (most recent call last):
  File ""synthesizer_preprocess_audio.py"", line 59, in <module>
    preprocess_dataset(**vars(args))
  File ""E:\Python\Voice\Real-Time-Voice-Cloning\synthesizer\preprocess.py"", line 49, in preprocess_dataset
    print(""Max input length (text chars): %d"" % max(len(m[5]) for m in metadata))
ValueError: max() arg is an empty sequence

it recognizes all the files but what is going wrong?
",hi file repeated one folder get error mel audio recent call last file line module file line print input length text empty sequence going wrong,issue,negative,negative,negative,negative,negative,negative
719866457,"I recommend skipping straight to synthesizer training, using Corentin's pretrained encoder. You will not need to separate the files for each speaker which will simplify things considerably. It will be easiest if you make a separate transcript file per mp3/wav. ",recommend skipping straight synthesizer training need separate speaker simplify considerably easiest make separate transcript file per,issue,positive,positive,positive,positive,positive,positive
719602039,"Currently my dataset is structured like this:

> FOLDER:

    
    clips (folder)

             common_voice_nl_17691032.mp4
             common_voice_nl_17691033.mp4
             common_voice_nl_17691034.mp4 (and so on)
     trainNL.txt (transcript)

where transcript.tsv contains:
client_id	namefile	    sentence (all separeted with a tab):

037cfe75436b3f535ccd1035df41847b8d0706c31b0d451151dae5b8e9eccc64404fe870ce14553dd286b13d7687ea2503608248bf8d1f62523b182d1cbf6906	common_voice_nl_18054747.mp3	Nucleaire energie is goedkoop, maar niet zo duurzaam

My clips arent sorted on speaker, do you know any code that can sort them according to the transcript.tsv
And do you need to have a separet transcript file per mp3/wav?

Here you have the transcipt file:

[trainNL.txt](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/5465909/trainNL.txt)

",currently structured like folder clip folder transcript sentence tab zo clip arent sorted speaker know code sort according need transcript file per file,issue,negative,neutral,neutral,neutral,neutral,neutral
719095587,"> This should fix it: https://github.com/blue-fish/Real-Time-Voice-Cloning/commit/69a074cb732190cb34e2d4f4d944be69e4cd09ff
> 
> Would you please let me know if you're able to export audio files after making that change? You can modify the file directly, or download it from my branch: [`blue-fish:580_export_after_portaudio_error`](https://github.com/blue-fish/Real-Time-Voice-Cloning/tree/580_export_after_portaudio_error)

Thank you a *lot* for the quick replies and helping me, it works, I can export it now.",fix would please let know able export audio making change modify file directly branch thank lot quick helping work export,issue,positive,positive,positive,positive,positive,positive
719091780,"This should fix it: https://github.com/blue-fish/Real-Time-Voice-Cloning/commit/69a074cb732190cb34e2d4f4d944be69e4cd09ff

Would you please let me know if you're able to export audio files after making that change? You can modify the file directly, or download it from my branch: [`blue-fish:580_export_after_portaudio_error`](https://github.com/blue-fish/Real-Time-Voice-Cloning/tree/580_export_after_portaudio_error)",fix would please let know able export audio making change modify file directly branch,issue,negative,positive,positive,positive,positive,positive
719075225,"You'll need something like #417 because the exception is preventing execution of the other code which adds it to history and allows export. I'll make a branch for this, it should be available for testing shortly.",need something like exception execution code history export make branch available testing shortly,issue,negative,positive,positive,positive,positive,positive
719072169,"> Can you confirm that the exception is not fatal? I think it should just get logged, but the toolbox should not close as a result.
> 
> After vocoding, you can click ""export"" to save the generated waveform to an audio file. Try that.

I can't export it afaik.
I made a video, maybe that can help.
https://drive.google.com/file/d/11Rplua8mOzfjgGXQl6Hnv0f0tmKYn7D9/view?usp=drivesdk",confirm exception fatal think get logged toolbox close result click export save audio file try ca export made video maybe help,issue,negative,neutral,neutral,neutral,neutral,neutral
719070631,"Are you familiar with the folder structure of the [LibriSpeech dataset](https://openslr.org/12)? Make your dataset look like that. For each speaker, you should have a separate folder that collects audio files of that speaker.

Otherwise, you will need to edit [encoder/preprocess.py](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/47a9093f901469486fca5b83babeb26ca98fa85d/encoder/preprocess.py) to preprocess a dataset in a different format. We aren't able to help with your custom dataset, but would welcome a pull request if you make any code improvements while doing this.",familiar folder structure make look like speaker separate folder audio speaker otherwise need edit different format able help custom would welcome pull request make code,issue,positive,positive,positive,positive,positive,positive
719066022,"Can you confirm that the exception is not fatal? I think it should just get logged, but the toolbox should not close as a result.

After vocoding, you can click ""export"" to save the generated waveform to an audio file. Try that.",confirm exception fatal think get logged toolbox close result click export save audio file try,issue,negative,neutral,neutral,neutral,neutral,neutral
719065650,"> Have you tried changing the ""audio output"" device?
> 
> If you don't have any usable devices then we may need to do something similar to #417 which adds code to ignore that exception so it is not fatal.

Thank you a lot for the quick reply, I can't select any audio output devices, it says that it didn't detect and input or output devices, but I don't really care about that, I just want the output file. I'll look into #417 and try to figure out what to do.",tried audio output device usable may need something similar code ignore exception fatal thank lot quick reply ca select audio output detect input output really care want output file look try figure,issue,negative,positive,positive,positive,positive,positive
719064190,"Have you tried changing the ""audio output"" device?

If you don't have any usable devices then we may need to do something similar to #417 which adds code to ignore that exception so it is not fatal.",tried audio output device usable may need something similar code ignore exception fatal,issue,negative,neutral,neutral,neutral,neutral,neutral
719046546,"Currently i have tried importing it as one of the example models but my dutch dataset has just one transcript.txt which states what file has what text, but how do i implement this data correctly?",currently tried one example dutch one file text implement data correctly,issue,negative,neutral,neutral,neutral,neutral,neutral
718006548,"FAQ entry for your error message: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/431#issuecomment-665165023

Another alternative you can consider, this branch of my fork does not require tensorflow: [`blue-fish:447_pytorch_synthesizer`](https://github.com/blue-fish/Real-Time-Voice-Cloning/tree/447_pytorch_synthesizer) Follow the setup instructions in README.md and take the additional step of downloading and installing the [pytorch synthesizer model](https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/472#issuecomment-695206377).",entry error message another alternative consider branch fork require follow setup take additional step synthesizer model,issue,negative,neutral,neutral,neutral,neutral,neutral
717402157,"Pytorch 1.7 just released, so it is time to try again. If we add AMP, the implementation must be clean while preserving support for lower versions of PT.",time try add implementation must clean support lower,issue,negative,positive,positive,positive,positive,positive
716917128,"@Erfie 

I'm sorry, but we no longer allow the issues board to be used for technical support questions. If you have evidence that this is a bug of our software, please share and we will reopen the issue and investigate.

Most likely you have an incompatibility between your CUDA, cuDNN and pytorch versions. You can try reinstalling pytorch using the instructions at https://pytorch.org and see if that helps.",sorry longer allow board used technical support evidence bug please share reopen issue investigate likely incompatibility try see,issue,positive,negative,negative,negative,negative,negative
716774808,"Hi @AyushExel , thank you for taking the time to prepare and submit this PR. Support for W&B is something that has been proposed before, but it's not a feature that we wish to add at this time. I've reviewed your changes and it's much cleaner than the last W&B PR (#159). If someone asks for W&B then I'll point them to your fork.",hi thank taking time prepare submit support something feature wish add time much cleaner last someone point fork,issue,positive,positive,neutral,neutral,positive,positive
716232742,This PR resolves #461. It is no longer necessary to pin the numba version (#373) so that has been fixed too. ,longer necessary pin version fixed,issue,negative,positive,neutral,neutral,positive,positive
716171737,"I'm cleaning up the issues list, and there hasn't been much discussion on this topic so I'm closing it due to inactivity. Thank you for the contribution @rustygentile .",cleaning list much discussion topic due inactivity thank contribution,issue,negative,positive,neutral,neutral,positive,positive
716171424,"For over 2 months, there has been nothing asked frequently enough to update the FAQ and the support questions have subsided, so there may not be a need for this anymore. Though I am closing this, the contents are still searchable.",nothing frequently enough update support may need though content still searchable,issue,negative,neutral,neutral,neutral,neutral,neutral
716169205,It's going to be a while before we get back to encoder training. I'm going to close this issue for now. Will reopen when we restart.,going get back training going close issue reopen restart,issue,negative,neutral,neutral,neutral,neutral,neutral
716168656,"> How exactly would I concatenate the specs?

https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/bfb6002a94c1142b3ca431aefc7ef1489818d2af/demo_cli.py#L106-L108

Recall my other suggestion of batching the text inputs to the synthesizer for extra speed. That trick only works on a GPU. From the generation times mentioned in your original post, it sounds like you are using a CPU, so batching will not make synthesis any faster.

However, synthesis is still much faster than WaveRNN vocode, so total time should be 5 min or less once you concatenate the specs and reduce it to a single vocode.",exactly would concatenate spec recall suggestion text synthesizer extra speed trick work generation time original post like make synthesis faster however synthesis still much faster total time min le concatenate spec reduce single,issue,positive,positive,positive,positive,positive,positive
716131259,"Hello, thank you very much for replying to my question.

How exactly would I concatenate the specs?

Do I just put them into a list?


>     for textGenerate in cloningText:
        counter += 1
        print(str(counter)+""/""+str(len(textGenerate)))
        print(textGenerate)
        try:
            # Get the reference audio filepath
            message = ""Reference voice: enter an audio filepath of a voice to be cloned (mp3, "" \
                      ""wav, m4a, flac, ...):\n""
            soundFilePath = ""voiceClone/rawaudio.mp3""
            in_fpath = Path(soundFilePath.replace(""\"""", """").replace(""\'"", """"))
            #in_fpath = ""trump5.mp3""
            if in_fpath.suffix.lower() == "".mp3"" and args.no_mp3_support:
                print(""Can't Use mp3 files please try again:"")
                continue
            ## Computing the embedding
            # First, we load the wav using the function that the speaker encoder provides. This is 
            # important: there is preprocessing that must be applied.
            
            # The following two methods are equivalent:
            # - Directly load from the filepath:
            preprocessed_wav = encoder.preprocess_wav(in_fpath)
            # - If the wav is already loaded:
            original_wav, sampling_rate = librosa.load(str(in_fpath))
            preprocessed_wav = encoder.preprocess_wav(original_wav, sampling_rate)
            print(""Loaded file succesfully"")
            
            # Then we derive the embedding. There are many functions and parameters that the 
            # speaker encoder interfaces. These are mostly for in-depth research. You will typically
            # only use this function (with its default parameters):
            embed = encoder.embed_utterance(preprocessed_wav)
            print(""Created the embedding"")
            
            ## Generating the spectrogram
            #text = input(""Write a sentence (+-20 words) to be synthesized:\n"")
            text = textGenerate
            # The synthesizer works in batch, so you need to put your data in a list or numpy array
            texts = [text]
            embeds = [embed]
            # If you know what the attention layer alignments are, you can retrieve them here by
            # passing return_alignments=True
            specs = synthesizer.synthesize_spectrograms(texts, embeds)
            spec = specs[0]
            print(""Created the mel spectrogram"")
            
            
            ## Generating the waveform
            print(""Synthesizing the waveform:"")

            # If seed is specified, reset torch seed and reload vocoder
            if args.seed is not None:
                torch.manual_seed(args.seed)
                vocoder.load_model(args.voc_model_fpath)

            # Synthesizing the waveform is fairly straightforward. Remember that the longer the
            # spectrogram, the more time-efficient the vocoder.
            generated_wav = vocoder.infer_waveform(spec)
            
            
            ## Post-generation
            # There's a bug with sounddevice that makes the audio cut one second earlier, so we
            # pad it.
            generated_wav = np.pad(generated_wav, (0, synthesizer.sample_rate), mode=""constant"")

            # Trim excess silences to compensate for gaps in spectrograms (issue #53)
            generated_wav = encoder.preprocess_wav(generated_wav)
            
            # Play the audio (non-blocking)
            if not args.no_sound:
                try:
                    sd.stop()
                    sd.play(generated_wav, synthesizer.sample_rate)
                except sd.PortAudioError as e:
                    print(""\nCaught exception: %s"" % repr(e))
                    print(""Continuing without audio playback. Suppress this message with the \""--no_sound\"" flag.\n"")
                except:
                    raise
                
            # Save it on the disk
            filename = ""voiceClone/demo_output_%02d.wav"" % num_generated
            wavFiles.append(filename)
            print(generated_wav.dtype)
            sf.write(filename, generated_wav.astype(np.float32), synthesizer.sample_rate)
            num_generated += 1
            print(""\nSaved output as %s\n\n"" % filename)
            

Thank you very much in advance.",hello thank much question exactly would concatenate spec put list counter print counter print try get reference audio message reference voice enter audio voice path print ca use please try continue first load function speaker important must applied following two equivalent directly load already loaded print loaded file derive many speaker mostly research typically use function default embed print generating spectrogram text input write sentence text synthesizer work batch need put data list array text embed know attention layer retrieve passing spec spec spec print mel spectrogram generating print seed reset torch seed reload none fairly straightforward remember longer spectrogram spec bug audio cut one second pad constant trim excess compensate issue play audio try except print exception print without audio playback suppress message except raise save disk print print output thank much advance,issue,positive,positive,positive,positive,positive,positive
716054792,"Model is here: https://github.com/padmalcom/Real-Time-Voice-Cloning-German/releases/tag/0.1
Please read the README.md for help.",model please read help,issue,positive,neutral,neutral,neutral,neutral,neutral
716037969,"Thanks, filesize is now 372mb which is fantastic. Here are the most important points I learned during training. Will upload the archive soon.

# First things first
- Trained on female German voices of m-ailabs
- Tests show that the vocoder training is not neccessary, so focus on encoder and synthesizer
- I had to edit some parts of the implementation to make it work with m-ailabs
- m-ailabs requires a lot of cleaning (at least for German voices). I added a script to do most of the work.
	- mailab_normalize_text.py: Creates text files besides each wav file of the m-ailabs dataset which apparently is required.

# How to run?
Please note that all changes I did were made for windows. You might want to adapt it for linux.

## Download
If you just want to apply the model, download the bin.zip from the following release and unzip it to your repository.
https://github.com/padmalcom/Real-Time-Voice-Cloning-German/releases/tag/0.1

## Training
To train the model replace all code files in the repositlry by the code files from my release. You can leave out the bin.zip file from the release.

### Encoder
- python encoder_preprocess.py E:\Datasets\
- python encoder_train.py my_run E:\Datasets\SV2TTS\encoder

### Synthesizer
- Make sure you updated synthesizer/utils/symbols.py for your language
- python synthesizer_preprocess_audio.py E:\Datasets\ --subfolders de_DE\by_book\female\ --dataset """" --no_alignments --wav_dir
- python synthesizer_preprocess_embeds.py E:\Datasets\SV2TTS\synthesizer
- python synthesizer_train.py my_run E:\Datasets\SV2TTS\synthesizer

### Vocoder (You really do not need to train the vocoder, it works well as the pretrained model is - at least for German)
- pip install librosa==0.7.2
- python vocoder_preprocess.py E:\Datasets\ --model_dir=synthesizer/saved_models/logs-my_run/
- python vocoder_train.py my_run E:\Datasets\

### Toolbox
- pip install matplotlib==3.2.2 #required for toolbox to work

## Todos and Learnings
- Getting the Toolbox to run with a specific voice of the four voices does not seem to work properly. I still have to figure out why (not).
- Next experiment will be to merge all voices into one voice package and see if the result improves.",thanks fantastic important learned training archive soon first first trained female german show training focus synthesizer edit implementation make work lot cleaning least german added script work text besides file apparently run please note made might want adapt want apply model following release repository training train model replace code code release leave file release python python synthesizer make sure language python python python really need train work well model least german pip install python python toolbox pip install toolbox work getting toolbox run specific voice four seem work properly still figure next experiment merge one voice package see result,issue,positive,positive,neutral,neutral,positive,positive
716028645,I added a dropbox link to the [pretrained models](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models) wiki page to provide a second option.,added link page provide second option,issue,negative,neutral,neutral,neutral,neutral,neutral
715991971,"Yes, you should be able to speed this up considerably. What is happening is you have your vocode inside your loop:
```
for texts in paragraph:
    spec = synthesizer.synthesize_spectrograms(texts, embeds)   # This is fast
    wav = vocoder.infer_waveform(spec)                          # This is slow
# **concat the wavs**
```

Because WaveRNN works more efficiently for longer inputs, you should only vocode once, when all your specs have been generated:
```
for texts in paragraph:
    spec = synthesizer.synthesize_spectrograms(texts, embeds)   # This is fast

# **concat the specs**
wav = vocoder.infer_waveform(concat_spec)                       # This is slow
```

To speed it up further, you should make use of batch generation for spectograms (put multiple texts into a list when calling `synthesize_spectrograms`).",yes able speed considerably happening inside loop paragraph spec fast spec slow work efficiently longer spec paragraph spec fast spec slow speed make use batch generation put multiple list calling,issue,positive,positive,neutral,neutral,positive,positive
715980473,"Hi @padmalcom , congratulations on completing full stack training and thank you for sharing your German models with the community! For the pretrained models, this is the minimum set of files we distribute. If this is not enough guidance on which files should be selected, please ask.

```
encoder\saved_models\pretrained.pt
synthesizer\saved_models\logs-pretrained\taco_pretrained\checkpoint
synthesizer\saved_models\logs-pretrained\taco_pretrained\tacotron_model.ckpt-278000.data-00000-of-00001
synthesizer\saved_models\logs-pretrained\taco_pretrained\tacotron_model.ckpt-278000.index
synthesizer\saved_models\logs-pretrained\taco_pretrained\tacotron_model.ckpt-278000.meta
vocoder\saved_models\pretrained\pretrained.pt
```

If you modified any of the following files, please also include them:
```
encoder\params_model.py
encoder\params_data.py
synthesizer\hparams.py
synthesizer\utils\cleaners.py
synthesizer\utils\symbols.py
synthesizer\utils\text.py
vocoder\hparams.py
```",hi full stack training thank german community minimum set distribute enough guidance selected please ask following please also include,issue,positive,positive,neutral,neutral,positive,positive
714935775,@CodingRox82 Sent you a github invitation to discuss further. And deleted your comment as requested.,sent invitation discus comment,issue,negative,neutral,neutral,neutral,neutral,neutral
714906114,"@CodingRox82 I don't have any trouble running the toolbox, but I use Linux where setup is very straightforward. I thought you were going to attempt a Windows install, and then compile it with PyInstaller if things went well.",trouble running toolbox use setup straightforward thought going attempt install compile went well,issue,negative,positive,neutral,neutral,positive,positive
714778583,@blue-fish were you able to get it working on your machine?,able get working machine,issue,negative,positive,positive,positive,positive,positive
714595223,"All community-developed models are listed here: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/400
At the time of this writing, models are only available in English, Chinese and Swedish.

If you are extremely motivated and want to train your own model, you can look at https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/431#issuecomment-673555684 for an idea of the process but we cannot help with it. 

You would also need to find datasets, based on a quick search I found:
* https://lionbridge.ai/datasets/12-best-turkish-language-datasets-for-machine-learning/
* https://github.com/JRMeyer/open-speech-corpora",listed time writing available extremely want train model look idea process help would also need find based quick search found,issue,positive,positive,positive,positive,positive,positive
714345290,"> It sounds pretty good. I tried cloning Rachel McAdams voice: https://soundcloud.com/arun-ghontale-235763095/rachel-mcadams-rachel-output?in=arun-ghontale-235763095/sets/name-synthesis

Hi @arun-ghontale ,the samples in your soundcloud sound pretty good. You mind enlightening me on what you used for [samuel L. Jackson](https://soundcloud.com/arun-ghontale-235763095/samuel-jackson),[Johnny Depp](https://soundcloud.com/arun-ghontale-235763095/johnny-depp) and [Scarlett Johansson](https://soundcloud.com/arun-ghontale-235763095/scarlett-johansson)? What models did you use for tts and vocoder? Dataset size? Time it took to train/fine-tune? There is a big difference with https://soundcloud.com/arun-ghontale-235763095/sets/voice-synthesis-output which were created by this toolbox if I am not wrong.",pretty good tried voice hi sound pretty good mind enlightening used use size time took big difference toolbox wrong,issue,positive,positive,positive,positive,positive,positive
712351929,"Sorry, but a port to C++ is not planned. There are not enough developers working on the project to make that happen. Also, the voice cloning quality is bad and improving that is our top priority.

However you are welcome to contribute a C++ version. Steps are as follows:
1. Start with [tacotron-tts-cpp](https://github.com/syoyo/tacotron-tts-cpp) (the only C++ tacotron on github?). Implement the SV2TTS changes, preferably after upgrading it to tacotron2.
    * Matching Rayhane's tacotron2 would allow you to transfer weights from our pretrained model, otherwise you'll also need to find a way to train (either implement training in C++ or make an equivalent python implementation to train)
2. Reimplement the speaker encoder in C++
3. You'll need some kind of vocoder. I didn't find any C++ implementations but didn't look very hard. As a last resort you can use Griffin-Lim.
4. Connect everything together once you have all the pieces.",sorry port enough working project make happen also voice quality bad improving top priority however welcome contribute version start implement preferably matching would allow transfer model otherwise also need find way train either implement training make equivalent python implementation train speaker need kind find look hard last resort use connect everything together,issue,positive,positive,neutral,neutral,positive,positive
712120855,"alright, but why not just upload the models to mega.nz (their new domain) and put it there instead? i personally prefer it to google drive.",alright new domain put instead personally prefer drive,issue,negative,positive,neutral,neutral,positive,positive
712113945,"Thank you for reporting this @ranshaa05 , I have removed the old link from the pretrained models wiki page since it is down. (For reference, it was: `https://megaupload.nz/F982Z8x6n5/pretrained_12_06_19_zip` ) Google drive is currently the only option for pretrained models. ",thank removed old link page since reference drive currently option,issue,negative,positive,neutral,neutral,positive,positive
710669712,"https://datashare.is.ed.ac.uk/handle/10283/3443
You must download the whole collection to get p240 and p260.",must whole collection get,issue,negative,positive,positive,positive,positive,positive
710234777,I prefer windows as well. As GPU does that need something that supports Cuda? I don't got Nvidia. And how is it going? ,prefer well need something got going,issue,negative,neutral,neutral,neutral,neutral,neutral
710139899,"> I'd expect the speech rate is a hard-to-untangle function of the speaker embedding + the specific input sequence + which elements get affected by the random dropout during inference. This is because the prosody is inconsistent between speakers (and even between utterances for a given speaker) in the LibriSpeech dataset that is used for training.
> 
> The easiest way is to postprocess and resample the wav to a different length, in such a way that the pitch is preserved. It is a common operation but if you find a way of doing it automatically in Python, please share the code. It would enable a feature where we add a slider to the toolbox UI that can adjust the output rate.

@blue-fish Thanks for your advice. I will try the idea you provided. If it's successful, I will share with you.
",expect speech rate function speaker specific input sequence get affected random dropout inference prosody inconsistent even given speaker used training easiest way resample different length way pitch common operation find way automatically python please share code would enable feature add slider toolbox adjust output rate thanks advice try idea provided successful share,issue,positive,positive,neutral,neutral,positive,positive
710126709,"Hi @him2him2 , we fixed this in #563. Thank you for bringing this to our attention.",hi fixed thank attention,issue,negative,positive,neutral,neutral,positive,positive
710124930,Closing this since prosody transfer is outside the scope of SV2TTS.,since prosody transfer outside scope,issue,negative,neutral,neutral,neutral,neutral,neutral
710123988,"Please read through #411 for some clues to why this is the case. Can also read #41 and #364. Some voices will clone better than others due to their representation by the speaker encoder and the training data used.

If you want better similarity with your own voice, you can generate your own training data and finetune a single-speaker model. See #437 for information on how to do it (no support available).",please read case also read clone better due representation speaker training data used want better similarity voice generate training data model see information support available,issue,positive,positive,positive,positive,positive,positive
710119640,"Decided to take the approach in https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/527#issuecomment-693227456 and update the documentation to reflect the deleted feature.

@ai-are-better-than-humans I do appreciate you taking the initiative to write a fix for a problem that you found. We would welcome additional contributions, though it would be preferable to first open an issue and validate the need to fix something before going ahead with it.",decided take approach update documentation reflect feature appreciate taking initiative write fix problem found would welcome additional though would preferable first open issue validate need fix something going ahead,issue,positive,positive,positive,positive,positive,positive
709795135,"Updated documentation: https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models
Also submitted a pull request to improve the error message: #562
You should be able to figure it out now.",documentation also pull request improve error message able figure,issue,negative,positive,positive,positive,positive,positive
709774751,"I read your question again, and I think you are wanting the speech rate of the output to match the sample used for cloning. The toolbox doesn't work this way as the speaker encoder is not designed to capture any information about prosody.

Please take a look at [mellotron](https://github.com/NVIDIA/mellotron) or [SpeechSplit](https://github.com/auspicious3000/SpeechSplit) for ideas on how to transfer speech rate from a reference sample.",read question think wanting speech rate output match sample used toolbox work way speaker designed capture information prosody please take look transfer speech rate reference sample,issue,negative,neutral,neutral,neutral,neutral,neutral
709687958,"> @CorentinJ @yaguangtang @tail95 @zbloss @HumanG33k I am finetuning the encoder model by Chhinese data of 3100 persons. I want to know how to judge whether the train of finetune is OK. In Figure0, The blue line is based on 2100 persons , the yellow line is based on 3100 persons which is trained now.
> Figure0:
> ![image](https://user-images.githubusercontent.com/40649244/63139015-3f446480-c00f-11e9-9ec2-3eadebd6023f.png)
> 
> Figure1:(finetune 920k , from 1565k to 1610k steps, based on 2100 persons)
> ![image](https://user-images.githubusercontent.com/40649244/63139038-5aaf6f80-c00f-11e9-85ca-f54fdd81431e.png)
> 
> Figure2:(finetune 45k from 1565k to 1610k steps, based on 3100 persons)
> ![image](https://user-images.githubusercontent.com/40649244/63139112-a8c47300-c00f-11e9-9097-fb65452df9fd.png)
> 
> I also what to know how mang steps is OK , in general. Because, I only know to train the synthesizer model and vocoder mode oneby one to judge the effect. But it will cost very long time. How about my EER or Loss ? Look forward your reply!

@UESTCgan Hello, I would like to ask you if you do finetuning with Chinese dataset on the encoder trained with English dataset Librispeec? In addition, what Chinese datasets are you using？",tail model data want know judge whether train figure blue line based yellow line based trained figure image figure based image figure based image also know mang general know train synthesizer model mode one judge effect cost long time eer loss look forward reply hello would like ask trained addition,issue,negative,positive,neutral,neutral,positive,positive
709519429,"@blue-fish  i achieved a 0.008 gate-loss in the third epoch !! it’s the first time my gate-loss decreases below 0.01 with the SV2TTS model it’s so interesting ! Now my inference is still so bad, just noise and no attention but it’s only epoch 3 and attention seems to improve in prediction progressively so really encouraging :D 

It’s the proof that a better processing of my dataset can solve my inference problem because the gate loss is a really good sign of the attention learning and inference improvments (as i saw in my old tests with the single-speaker model, when the gate loss decreases, the inference becomes better and the attention too)",third epoch first time model interesting inference still bad noise attention epoch attention improve prediction progressively really encouraging proof better solve inference problem gate loss really good sign attention learning inference saw old model gate loss inference becomes better attention,issue,positive,positive,positive,positive,positive,positive
709431770,Please reopen the issue when able to provide the requested information.,please reopen issue able provide information,issue,negative,positive,positive,positive,positive,positive
709430649,"I'd expect the speech rate is a hard-to-untangle function of the speaker embedding + the specific input sequence + which elements get affected by the random dropout during inference. This is because the prosody is inconsistent between speakers (and even between utterances for a given speaker) in the LibriSpeech dataset that is used for training.

The easiest way is to postprocess and resample the wav to a different length, in such a way that the pitch is preserved. It is a common operation but if you find a way of doing it automatically in Python, please share the code. It would enable a feature where we add a slider to the toolbox UI that can adjust the output rate.",expect speech rate function speaker specific input sequence get affected random dropout inference prosody inconsistent even given speaker used training easiest way resample different length way pitch common operation find way automatically python please share code would enable feature add slider toolbox adjust output rate,issue,positive,negative,negative,negative,negative,negative
709289565,"Silly me, I found the folder in the encoder folder of the git repo. The synthesizer model is stored elsewhere, that confused me.",silly found folder folder git synthesizer model elsewhere confused,issue,negative,negative,negative,negative,negative,negative
708441984,"Many thanks blue-fish
I have taken that on board and have read your work in #437, I will be referring back to that often. I’m enjoying playing with this Tool Box. 
I have just one final question is it not possible to save the output from “demo_toolbox.py” maybe I’m missing something?
",many thanks taken board read work back often enjoying tool box one final question possible save output maybe missing something,issue,positive,positive,positive,positive,positive,positive
708093197,This is too niche for inclusion in README.md but as of now we still have #398 open. That will give visibility to your work for those who are still unable to install the toolbox on their system for whatever reason.,niche inclusion still open give visibility work still unable install toolbox system whatever reason,issue,negative,negative,negative,negative,negative,negative
708085872,"If anyone is wondering, training is still paused while @mbdash is denoising datasets and @steven850 is doing trial runs to determine best hparams for the encoder. It's a slow process. We plan to swap out the ReLU for a Tanh activation, but will try to match the model structure of the updated Resemblyzer encoder if it is released.",anyone wondering training still steven trial determine best slow process plan swap tanh activation try match model structure,issue,positive,positive,positive,positive,positive,positive
708084058,"> Am I wasting my time trying to get a voice that sounds a bit like the original this way?

@Jindalee-AUS Because the synthesizer is trained on audiobook data, the output voice sounds just like that. There is no control of emotion, only the randomness of the synthesizer. With all the problems of the current synth model, it's more of a novelty and not suited for production use. If you must try it, set your expectations very low.

Being new to python you will likely find it quite frustrating, but don't let that discourage you if you're determined and willing to put in many many hours of work. This being said, I'm going to allocate my limited time to development and not provide consultation on side projects anymore, so I cannot really help you further. You can see my work in #437 which includes tutorials and single-speaker mini-datasets that have been preprocessed.",wasting time trying get voice bit like original way synthesizer trained data output voice like control emotion randomness synthesizer current model novelty production use must try set low new python likely find quite let discourage determined willing put many many work said going allocate limited time development provide consultation side really help see work,issue,negative,positive,positive,positive,positive,positive
708081224,"@arceuss I hope that worked for you. With the repo in its current state, the result you get through the finetuning process is as good as it will get. To further improve the quality is a huge task, one I've been working on for months now. Reopen the issue if you're still stuck.",hope worked current state result get process good get improve quality huge task one working reopen issue still stuck,issue,positive,positive,positive,positive,positive,positive
708080398,"If you would like support with this, please copy and paste the full error message (the ""traceback"") from the terminal. From what you've provided there's not enough information to determine which file is erroring out and why.

I'm interested because it seems like it could be a bug. Please also post the output of `pip freeze` so I know which versions of packages you are running.",would like support please copy paste full error message terminal provided enough information determine file interested like could bug please also post output pip freeze know running,issue,positive,positive,positive,positive,positive,positive
707876533,"If it’s ok for you you can still add me, it can be really interesting for me just to discuss / exchange idea and also share codes etc, it’s just that i will not be really active in code but maybe just sharing experiments and model architecture / code can help me to improve my experiments and maybe some of my utility-function can help you too ;) ",still add really interesting discus exchange idea also share really active code maybe model architecture code help improve maybe help,issue,positive,positive,positive,positive,positive,positive
707850580,"No problem @Ananas120 , I don't want to be a distraction when you have other important things to be working on. Let us know when you have more time and we'll tie you into the project. Best wishes for your coursework!",problem ananas want distraction important working let u know time tie project best,issue,negative,positive,positive,positive,positive,positive
707749140,"Ah okay I had it almost exactly backwards.

So following blue-fish's instructions in #538 to retrain tensorflow on libri-tts/libri-Speech should resolve the long pauses and also won't have the stop token issue?",ah almost exactly backwards following retrain resolve long also wo stop token issue,issue,negative,positive,neutral,neutral,positive,positive
707736544,"The stop token prediction (whether the model knows when to end the generation) on the tensorflow model is usually good, the long pauses is more of a dataset/data representation and attention mechanism issue.

The pytorch model is the one to fail at predicting stop tokens - indeed due to its attention mechanism - and hence why it stops during generation.",stop token prediction whether model end generation model usually good long representation attention mechanism issue model one fail stop indeed due attention mechanism hence generation,issue,negative,positive,neutral,neutral,positive,positive
707722303,"Let me know if I'm up to date on this- 

- blue-fish finished the effort to implement and train in pytorch in his fork.

- on review it was decided that the quality of the tensorflow model was still better overall quality.

- sometimes with the tensorflow model the stop token prediction fails and results in large gaps in the synthesis.

- sometimes with pytorch model will quit in the middle of synthesis, something to do with the attention model?",let know date finished effort implement train fork review decided quality model still better overall quality sometimes model stop token prediction large synthesis sometimes model quit middle synthesis something attention model,issue,negative,positive,positive,positive,positive,positive
707546401,"@blue-fish Thank you for your proposition, it’s really interesting ! 

The problem is that I will be busy this year with all course and course’s project so i don’t know how many time i can take to code on private projects... 

I can make trainings during nights or days and test differents hyperparameters (what i do now) but i think i can’t make really more than that now but the project interests me a lot !",thank proposition really interesting problem busy year course course project know many time take code private make night day test think make really project lot,issue,negative,positive,positive,positive,positive,positive
707504906,"Nothing as detailed for Windows is available, but the instructions in README.md have worked for me.",nothing detailed available worked,issue,negative,positive,positive,positive,positive,positive
707491502,"> Windows is not too difficult if GPU support is not required. The toolbox also runs on CPU, though somewhat slower.

@blue-fish  I'd prefer Windows (the OS of my personal computer), otherwise my options are an Ubuntu USB or buy a new laptop and install Ubuntu on it. Do we have an instruction set for Windows 10 installation?",difficult support toolbox also though somewhat prefer o personal computer otherwise buy new install instruction set installation,issue,negative,negative,negative,negative,negative,negative
707384052,"Sorry, I mistakenly assumed that you were an existing user of the toolbox. Why don't you try the toolbox first and then decide whether you want to compile it for the benefit of the community. You should use Ubuntu 20.04, I recommend the [setup guide in the wiki](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Installation---Ubuntu-20.04). Open an issue if you get stuck.

Windows is not too difficult if GPU support is not required. The toolbox also runs on CPU, though somewhat slower.",sorry mistakenly assumed user toolbox try toolbox first decide whether want compile benefit community use recommend setup guide open issue get stuck difficult support toolbox also though somewhat,issue,negative,negative,negative,negative,negative,negative
707372318,"I was trying to import an mp3, recording will return this error as well.",trying import recording return error well,issue,negative,neutral,neutral,neutral,neutral,neutral
707357696,"@blue-fish  I'll give it a shot. My long term goal for this is to use it on a smartphone, but I'll settle on running this server side for now. Can you point me in the right direction on which OS to use to start off? I read through some of the other comments and Windows does not seem like the way to go.",give shot long term goal use settle running server side point right direction o use start read seem like way go,issue,negative,positive,positive,positive,positive,positive
707355482,"@blue-fish Thanks a lot for your valuable help and time. I did come to the same conclusions as you. A lot of the users coming through are highly unexperienced.

I have been wanting to make things simpler just for the sake of reducing the number of technical support requests, but my awkward position makes it hard for me to stay involved.",thanks lot valuable help time come lot coming highly unexperienced wanting make simpler sake reducing number technical support awkward position hard stay involved,issue,positive,negative,negative,negative,negative,negative
707354600,"Hi @CodingRox82, thank you for expressing willingness to help make a compiled version of the toolbox. [PyInstaller](https://www.pyinstaller.org/) seems to be the way to go, but I have not used it before.",hi thank willingness help make version toolbox way go used,issue,positive,neutral,neutral,neutral,neutral,neutral
707352480,Sorry for the late reply @blue-fish . I'm definitely interested in using this. I like your idea of creating a pre-compiled version to give people to test out. I'm going to start tinkering around with this to try to get it to work and if I find the time to learn how to create a distributable precompiled version I'll give it a shot. ,sorry late reply definitely interested like idea version give people test going start around try get work find time learn create distributable version give shot,issue,positive,negative,negative,negative,negative,negative
707330342,"@CorentinJ Thanks for providing the statement of direction in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/543#issuecomment-705988630

In that context it's not worth my time continuing to provide technical support as I have the last few months. It was initially helpful to identify common pain points but now it's mainly down to getting rid of tensorflow, and people asking for an exe. To help potential developers I suggest disallowing the use of the issues board for tech support and requests for help with projects since it dilutes the development effort. I've donated a lot of my time trying to build some sense of community, but unfortunately it is not attracting and retaining the type of people who can push this project forward.

Tensorflow has this [issue policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), and it could help to implement something similar. I realize this will be unpopular because a lot of individuals want help and tech support, but it needs to be understood that you get what you pay for with open source.
> If you open a GitHub Issue, here is our policy: 1. It must be a bug/performance issue or a feature request or a build issue or a documentation issue (for small doc fixes please send a PR instead). 2. Make sure the Issue Template is filled out. 3. The issue should be related to the repo it is created in.
>
> **Here's why we have this policy:** We want to focus on the work that benefits the whole community, e.g., fixing bugs and adding features. Individual support should be sought on Stack Overflow or other non-GitHub channels. It helps us to address bugs and feature requests in a timely manner.
",thanks providing statement direction context worth time provide technical support last initially helpful identify common pain mainly getting rid people help potential suggest use board tech support help since development effort donated lot time trying build sense community unfortunately retaining type people push project forward issue policy could help implement something similar realize unpopular lot want help tech support need understood get pay open source open issue policy must issue feature request build issue documentation issue small doc please send instead make sure issue template filled issue related policy want focus work whole community fixing individual support sought stack overflow u address feature timely manner,issue,positive,positive,neutral,neutral,positive,positive
707321228,"Please post the command you were trying to run, and the full traceback",please post command trying run full,issue,negative,positive,positive,positive,positive,positive
707317072,"Some of us here have formed a small group focused on improving the RTVC code base and more importantly the quality of audio generation. Our interests include neural text to speech as well as voice conversion. We're also developing preprocessing techniques and finding better values of hyperparameters.

@Ananas120 You're invited to join us if you have similar interests. Our code base is currently pytorch, but if you want to code with us, I wouldn't mind switching to tensorflow 2.x. If you're willing to dedicate a few hours of your time every week to advancing the state of the art for open source, we would welcome your involvement.

What we can offer is help denoising your datasets, and help/feedback/advice for training. I've also found it a very stimulating environment to discuss ideas and try out new things.

Leave a message here if interested and I'll send you an invite to our slack where we've been collaborating.",u formed small group improving code base importantly quality audio generation include neural text speech well voice conversion also finding better ananas join u similar code base currently want code u would mind switching willing dedicate time every week advancing state art open source would welcome involvement offer help training also found environment discus try new leave message interested send invite slack,issue,positive,positive,neutral,neutral,positive,positive
707199115,When I upload a voice it doesn't work very well. But when I record the voice it's almost spot on. ,voice work well record voice almost spot,issue,negative,neutral,neutral,neutral,neutral,neutral
706976290,Thanks for trying @thehetpandya . If you decide to work on this later please reopen the issue and I'll try to help.,thanks trying decide work later please reopen issue try help,issue,positive,positive,neutral,neutral,positive,positive
706970906,If you continue your experiments and achieve good performance (or just interesting inference and not only noise like for me) let me know please it can maybe help me in the tensorflow implementation !,continue achieve good performance interesting inference noise like let know please maybe help implementation,issue,positive,positive,positive,positive,positive,positive
706970098,"Yes no problem you’re right, i will not be really active on my experiments during the end of the year... 
I made some tests but nothing really concluent...
- Try my single-speaker model on the 8k utterances of a speaker from CommonVoice : prediction good but bad inference —> this confirm my hypothesis that this is the fault of the dataset and maybe not the one of my model
- Try another dataset (VoxForge) : same result, good prediction but bad inference (and no attention learned)
- Try with a (maybe) better audio processing : nothing has changed

But for the 2 last tests i made only 1 experiment so i can’t conclude it’s over, i will continue experiments with concat() layer instead of my add() variant, i will also recompute the embeddings with my new audio processing 

Now I just tested my 512 embeddings with add() layer on VoxForge but i think my 128 embeddings with concat() can be interesting to test (I will do this when my embeddings are recomputed so maybe this night)

Now that all my processing functions (dataset and audio) are done it’s easier for me to make experiments because it’s just hyperparameters to change in my notebook

I will tell you if i have interesting results ;) i think if the model learns the attention, it can be really more interesting but now i have nothing interesting for the attention mechanism...",yes problem right really active end year made nothing really try model speaker prediction good bad inference confirm hypothesis fault maybe one model try another result good prediction bad inference attention learned try maybe better audio nothing last made experiment conclude continue layer instead add variant also recompute new audio tested add layer think interesting test maybe night audio done easier make change notebook tell interesting think model attention really interesting nothing interesting attention mechanism,issue,positive,positive,positive,positive,positive,positive
706968789,"Since there's no interest in this particular bug, I'm closing this issue. For anyone interested in fixing it, you can try this:
![image](https://user-images.githubusercontent.com/67130644/95723936-93a29e80-0c2a-11eb-9bfb-ece40206451d.png)
",since interest particular bug issue anyone interested fixing try image,issue,positive,positive,positive,positive,positive,positive
706965737,"Closing this issue due to inactivity. @rlutsyshyn I think you know as much about this repo as I do now. My recommendation is to avoid finetuning the vocoder, since it will not improve the quality that much. If you need a better vocoder train it from scratch.
",issue due inactivity think know much recommendation avoid since improve quality much need better train scratch,issue,positive,positive,positive,positive,positive,positive
706963626,"Hi @Ananas120 , I'm going to close this issue as well as #484 because it's not really pertinent to the codebase of this repo. Trying to clean up the issues list so it will be more useful to those who browse it.

However, I find your experiments quite interesting. Please keep me updated, I'm subscribed to this issue.",hi ananas going close issue well really pertinent trying clean list useful browse however find quite interesting please keep issue,issue,positive,positive,positive,positive,positive,positive
706958836,Please see https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/282#issuecomment-706946711 . Try running on a different computer or platform (e.g. Linux) and see if you still have the issue.,please see try running different computer platform see still issue,issue,negative,neutral,neutral,neutral,neutral,neutral
706956536,"We are still working actively on this, but collaborating elsewhere. If you are interested in contributing time towards the development of better models please leave a message in #474 .",still working actively elsewhere interested time towards development better please leave message,issue,positive,positive,positive,positive,positive,positive
706953859,The list is still being maintained at this time. It is closed to avoid cluttering the issues list.,list still time closed avoid list,issue,negative,negative,neutral,neutral,negative,negative
706952080,"I think this is improved somewhat with the new pretrained model in #538 . Selected training utterances a bit more deliberately and the speaking rate is more consistent in inference. Again, if this is a problem, it is with the training data. If it is too fast you can postprocess to suit your needs.",think somewhat new model selected training bit deliberately speaking rate consistent inference problem training data fast suit need,issue,negative,positive,positive,positive,positive,positive
706949867,A pull request to add a helpful error message would be welcome if someone is willing to write it. Closing as this is a lower priority issue.,pull request add helpful error message would welcome someone willing write lower priority issue,issue,negative,positive,positive,positive,positive,positive
706946711,This is most likely an issue with your system configuration and not the toolbox code. An inferior codec is being used to write wav files. You can analyze the header of the wav file to determine which. Please submit a pull request if you identify code changes that will fix this issue on your end.,likely issue system configuration toolbox code inferior used write analyze header file determine please submit pull request identify code fix issue end,issue,negative,neutral,neutral,neutral,neutral,neutral
706671891,"im getting the following error when i hit synthesize and vocode ,""Could not find any synthesizer weights under C:\Users\jack\PycharmProjects\pythonProject\Real-Time-Voice-Cloning\synthesizer\saved_models\logs-pretrained\taco_pretrained\checkpoint\taco_pretrained"",someone help please",getting following error hit synthesize could find synthesizer someone help please,issue,negative,neutral,neutral,neutral,neutral,neutral
706592732,"Hi Guys
I have been following several threads for a couple of weeks. I’m making a animated movie I have a couple of hundred utterances (but can get more) from the voice over actors, rather than getting them to rerecord every time I make a change to the script. With that in mind I have been trying to run vocoder_preprocess.py before running the vocoder_train.py.
1.	I’m new to python, but have managed to install and run the demo_cli.py and the demo_toolbox.py
2.	Am I wasting my time trying to get a voice that sounds a bit like the original this way?
3.	Can’t get past the first hurdle. I can’t get my head around the folder structure, I have run the –h and read all that I think I understand all that the dataset root should have folders for LibriSpeech/TTS and VoxCeleb1/VoxCeleb2 datasets, which I have but what is the code expecting to find in them? From the down loaded pertained the readme.txt shows a structure with 4 txt files plus the train-clean-100/ and its sub folders. 
demo_toolbox.py -d C:\Users\Administrator\anaconda3 this works for the demo_toolbox  So what is wrong with vocoder_preprocess.py -d C:\Users\Administrator\anaconda3
(base) C:\Users\Administrator\anaconda3>encoder_preprocess.py
usage: encoder_preprocess.py [-h] [-o OUT_DIR] [-d DATASETS] [-s] [--no_trim]
                             datasets_root
encoder_preprocess.py: error: the following arguments are required: datasets_root
",hi following several couple making animated movie couple hundred get voice rather getting every time make change script mind trying run running new python install run wasting time trying get voice bit like original way get past first hurdle get head around folder structure run read think understand root code find loaded structure plus sub work wrong base usage error following,issue,negative,negative,neutral,neutral,negative,negative
706588814,"Thanks for the help, I will try to play around a bit and train and test a few models :). ",thanks help try play around bit train test,issue,positive,positive,positive,positive,positive,positive
706561781,"Hi @blue-fish , no I coudn't find progress on this one. I tried fine-tuning https://github.com/Kyubyong/dc_tts instead, which gave clearer pronunciation of hindi words.
Edit - I tried fine-tuning https://github.com/Kyubyong/dc_tts on Source 1 i.e. https://cvit.iiit.ac.in/research/projects/cvit-projects/text-to-speech-dataset-for-indian-languages",hi find progress one tried instead gave clearer pronunciation edit tried source,issue,negative,neutral,neutral,neutral,neutral,neutral
706522645,"well another problem https://i.imgur.com/lpG547R.png
edit yes I have a gpu a gtx 1050 mobile",well another problem edit yes mobile,issue,negative,neutral,neutral,neutral,neutral,neutral
706428514,@CodingRox82 We're experimenting with voice to voice but it will be a few months before we will have something usable to share.,voice voice something usable share,issue,negative,neutral,neutral,neutral,neutral,neutral
706416881,"Also, in synthesizer/hparams.py, there are 2 settings of interest:
```
max_mel_frames = 900            # frames, or 11.25 seconds. The preprocess script discards anything longer than this.
utterance_min_duration = 1.6    # seconds. Anything less than this is discarded by preprocess.
```",also interest script anything longer anything le,issue,negative,neutral,neutral,neutral,neutral,neutral
706414195,"You get that error message when it didn't find anything to process. Make sure that when you add wavs, they are at the right level in that folder structure (see https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-666099538 for guidance) and there is a corresponding text file for each wav.",get error message find anything process make sure add right level folder structure see guidance corresponding text file,issue,negative,positive,positive,positive,positive,positive
706412731,"`python synthesizer_preprocess_audio.py  synthesizer/saved_models/logs-singlespeaker/datasets_root
2020-10-09 15:35:30.305837: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
Arguments:
    datasets_root:   synthesizer\saved_models\logs-singlespeaker\datasets_root
    out_dir:         synthesizer\saved_models\logs-singlespeaker\datasets_root\SV2TTS\synthesizer
    n_processes:     None
    skip_existing:   False
    hparams:
    no_alignments:   False
    datasets_name:   LibriSpeech
    subfolders:      train-clean-100, train-clean-360

Using data from:
    synthesizer\saved_models\logs-singlespeaker\datasets_root\LibriSpeech\train-clean-100
    synthesizer\saved_models\logs-singlespeaker\datasets_root\LibriSpeech\train-clean-360
LibriSpeech:   0%|                                                                                                                                                                                              | 0/1 [00:00<?, ?speakers/s]2020-10-09 15:35:36.352287: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-10-09 15:35:36.354889: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-10-09 15:35:36.387177: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-10-09 15:35:36.408799: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-10-09 15:35:36.413293: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-10-09 15:35:36.444144: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
LibriSpeech: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.86s/speakers]
The dataset consists of 0 utterances, 0 mel frames, 0 audio timesteps (0.00 hours).
Traceback (most recent call last):
  File ""synthesizer_preprocess_audio.py"", line 59, in <module>
    preprocess_dataset(**vars(args))
  File ""C:\Users\arceus\Real-Time-Voice-Cloning\synthesizer\preprocess.py"", line 49, in preprocess_dataset
    print(""Max input length (text chars): %d"" % max(len(m[5]) for m in metadata))
ValueError: max() arg is an empty sequence`
huh",python successfully dynamic library none false false data successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library mel audio recent call last file line module file line print input length text empty sequence huh,issue,positive,negative,neutral,neutral,negative,negative
706411138,oh i didnt do a custom dataset yet im smart lol,oh didnt custom yet smart,issue,negative,positive,positive,positive,positive,positive
706409100,"ok i did all the processing stuff what now
it still doesnt sound like scout",stuff still doesnt sound like scout,issue,negative,positive,positive,positive,positive,positive
706405422,"If you get an error message, please also share the command you were attempting to use. Try removing SV2TTS/synthesizer from the command you were using.",get error message please also share command use try removing command,issue,negative,neutral,neutral,neutral,neutral,neutral
706404137,"You either need to:
1. Downgrade your Python to 3.7 to get tensorflow==1.15, or 
2. Use the experimental [pytorch-only repo](https://github.com/blue-fish/Real-Time-Voice-Cloning/tree/447_pytorch_synthesizer) + [pretrained model](https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/472#issuecomment-695206377)",either need downgrade python get use experimental model,issue,negative,positive,neutral,neutral,positive,positive
706399581,"Start here: https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training

Here I break it down into some bite-sized steps to help you get started. You have to really want it because it's not a simple task. But if you are sufficiently motivated it is doable.

1. Download the datasets for the synthesizer
2. Preprocess the data
3. Start training a model
    * Either from scratch or continuing the training of the pretrained model.
    * Confirm you can run the scripts. Get the training to run for a small number of steps.
    * Look at the outputs it produces.
4. Gather and preprocess your own dataset
    * Assemble your wavs and texts in this format: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-666099538
5. Train your own model (usually more productive to finetune the pretrained model)",start break help get really want simple task sufficiently doable synthesizer data start training model either scratch training model confirm run get training run small number look gather assemble format train model usually productive model,issue,positive,negative,neutral,neutral,negative,negative
706395708,i dont really know how to train since im new at this lol,dont really know train since new,issue,negative,positive,positive,positive,positive,positive
706322617,im in school right now but ill make sure to try it when im home,school right ill make sure try home,issue,negative,positive,neutral,neutral,positive,positive
706211680,"Great, then you'll have everything you need if you decide to pursue this. Ask questions if you get stuck",great everything need decide pursue ask get stuck,issue,negative,positive,positive,positive,positive,positive
706174164,"If you have a lot of samples, and can make transcripts, it is possible to finetune a single-speaker model with about 20 minutes of data. That works a lot better. See #437 if interested.",lot make possible model data work lot better see interested,issue,positive,positive,positive,positive,positive,positive
706173626,I'm not surprised. Open-source voice cloning is bad quality at present.,voice bad quality present,issue,negative,negative,negative,negative,negative,negative
706144293,"This is a tensorflow issue, you can browse the issues there and look for a solution. Different people report different things solve it for them. For example here's a similar problem reported: https://github.com/tensorflow/tensorflow/issues/23715",issue browse look solution different people report different solve example similar problem,issue,negative,neutral,neutral,neutral,neutral,neutral
706141764,"The models are not that great, especially when the input file is nothing like an audiobook (which is where the training data came from). Highly emotional speech does not clone well in my experience. Try a few different voice samples and see if you get better results. Also certain voices are too dissimilar to the training voices so it's unable to generalize.",great especially input file nothing like training data came highly emotional speech clone well experience try different voice see get better also certain dissimilar training unable generalize,issue,positive,positive,positive,positive,positive,positive
706139555,"1. You can have a few errors in your transcripts but the overall error rate needs to be very low.
2. Don't bother with it. The pretrained vocoder is already quite good and does not benefit that much from additional training.
3. If you have alignment files, the synthesizer preprocess script can do it for you automatically. You can generate alignments with [Montreal Forced Aligner] or any of the tools on [this list](https://github.com/pettarin/forced-alignment-tools). Be warned: the setup is about as hard as this repo, if not harder.

It is possible to do machine learning on AMD with ROCm , but if you value your time just find a way to use CUDA. You won't get much acceleration from a Vega 8. If needed, rent a GPU in the cloud. See #398.

Regarding that comment, you need to realize that it's possible to take Corentin's pretrained synth model and continue the training where it left off. By pointing the training script to the checkpoint (`python synthesizer_train.py pretrained ...`) it will load the weights from the pretrained model, and update them as training progresses.

Do not train your dataset from scratch, it will not yield a usable model because the dataset is too small to generalize.",overall error rate need low bother already quite good benefit much additional training alignment synthesizer script automatically generate forced aligner list setup hard harder possible machine learning value time find way use wo get much acceleration rent cloud see regarding comment need realize possible take model continue training left pointing training script python load model update training train scratch yield usable model small generalize,issue,negative,positive,neutral,neutral,positive,positive
706050920,"Thank you, I tried to read everything I could find, but wasn't sure if I got it right :). 

I trained a few additional steps since the 300 steps were indeed not enough and the spectrograms were all noisy. Now I have 1200 training steps and made another test. For this, I used my my_run synthesizer with the pretrained encoder and vocoder. Does not reproduce the text very well but the voice is already clearly hearable. So I guess, more training is required ;). 

I have a few additional questions if you don't mind: 

1. I used a very rough cut dataset for now. That is, in some samples, words are cut off and the end. At the moment I have roughly 20 minutes of speech as training data. And I have done the transcripts automatically, so there might be some errors in it the text-to-speech translation. So my question is, how robust the net is against such noise in your experience? In other words, is it important to input an absolutely clean dataset? I imagine that with more noise, more data is required? 
2. Do you have any idea, why the vocoder training does not work? 
3. Is there a tool that creates audio training data by cutting it well without cut words and text-to-speech? 

As for your comment [here  ](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/547#issuecomment-705840054), was it with regard to training a new model from scratch? That might take a long time though since I do have an AMD graphics card, so I can't use CUDA, but is there an alternative I do not know of and which I can use for AMD?  

I have AMD Radeon Vega 8 Graphics and AMD Ryzen 3 2200G CPU. ",thank tried read everything could find sure got right trained additional since indeed enough noisy training made another test used synthesizer reproduce text well voice already clearly hearable guess training additional mind used rough cut cut end moment roughly speech training data done automatically might translation question robust net noise experience important input absolutely clean imagine noise data idea training work tool audio training data cutting well without cut comment regard training new model scratch might take long time though since graphic card ca use alternative know use graphic,issue,positive,positive,neutral,neutral,positive,positive
705993410,"ok, I got it running, but now it spits out the following error in the command prompt:

 0:   N
2020-10-09 01:10:17.358266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5680 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:29:00.0, compute capability: 7.5)
2020-10-09 01:10:18.018608: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2020-10-09 01:10:18.252081: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-10-09 01:10:18.254978: E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.2.1 but source was compiled with: 7.6.0.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.
2020-10-09 01:10:18.262729: E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.2.1 but source was compiled with: 7.6.0.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.
Traceback (most recent call last):
  File ""C:\Users\lk00d\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\client\session.py"", line 1365, in _do_call
    return fn(*args)
  File ""C:\Users\lk00d\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\client\session.py"", line 1350, in _run_fn
    target_list, run_metadata)
  File ""C:\Users\lk00d\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\client\session.py"", line 1443, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.
  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
         [[{{node Tacotron_model/inference/encoder_convolutions/conv_layer_1_encoder_convolutions/conv1d/conv1d}}]]
         [[Tacotron_model/inference/add/_269]]
  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
         [[{{node Tacotron_model/inference/encoder_convolutions/conv_layer_1_encoder_convolutions/conv1d/conv1d}}]]
0 successful operations.
0 derived errors ignored.",got running following error command prompt device memory physical device name bus id compute capability successfully dynamic library successfully dynamic library loaded library source library major minor version need match higher minor version case later version binary install upgrade library building make sure library loaded compatible version compile configuration loaded library source library major minor version need match higher minor version case later version binary install upgrade library building make sure library loaded compatible version compile configuration recent call last file line return file line file line root error found unknown get convolution algorithm probably initialize try looking see warning log message printed node unknown get convolution algorithm probably initialize try looking see warning log message printed node successful derived,issue,positive,positive,neutral,neutral,positive,positive
705988630,"> but due to not having a ready to use executable I like many others I'm sure of, have decided it isn't even worth messing with.

And you wouldn't entirely be wrong. You would have had an easier experience with a team of dedicated developers maintaining the repo at least 20 hours a week, but that is just not the case. The generation quality is poor and hasn't improved one bit since I made the project public, and there is little motivation to make the project easier of access before that is worked on. Hence the disclaimer in the readme.",due ready use executable like many sure decided even worth messing would entirely wrong would easier experience team least week case generation quality poor one bit since made project public little motivation make project easier access worked hence disclaimer,issue,positive,negative,neutral,neutral,negative,negative
705909539,I'm interested in a voice to voice system.,interested voice voice system,issue,negative,positive,positive,positive,positive,positive
705891010,eh as I feared. that brings me back to my original post. not worth the trouble sorry.,eh feared back original post worth trouble sorry,issue,negative,negative,neutral,neutral,negative,negative
705840054,"Make sure you downloaded the [pretrained models](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models) and are continuing the training of the synthesizer which is already at 278,000 steps. To do this, make the first argument to synthesizer_train.py XXXX, which represents the logs-XXXX folder where the existing checkpoint is located. So instead of `my_run` you can call it `pretrained`. Although the output might sound intelligible at 300 steps when training from scratch, that's because it uses teacher forcing and uses the ground truth as input to the decoder. That is not available during inference.",make sure training synthesizer already make first argument folder instead call although output might sound intelligible training scratch teacher forcing ground truth input available inference,issue,positive,positive,positive,positive,positive,positive
705645796,"Wow, you got really far considering this is your first time using the repo. It may not be necessary to finetune the vocoder model. Try using your finetuned synthesizer model in the toolbox. If it doesn't match your expectations then train a few more steps or add more training data.

Do not fret about the tensorflow warnings, it is normal. (We had an open issue to remove the warnings #444 but no one wanted to work on it...)  If you have the time and a GPU, it would be helpful to actually go through the motions of training a synthesizer and vocoder from scratch so you have some context and experience.",wow got really far considering first time may necessary model try synthesizer model toolbox match train add training data fret normal open issue remove one work time would helpful actually go training synthesizer scratch context experience,issue,positive,positive,neutral,neutral,positive,positive
705497551,"Also, since python is interpreted, there's no need to compile the code in order to use it. You do have to track down the dependencies, but it should not be too hard with pip and requirements.txt.

If you succeed in compiling the toolbox, then it would save others the trouble of installing and configuring python. Thanks for giving it a shot.",also since python need compile code order use track hard pip succeed toolbox would save trouble python thanks giving shot,issue,positive,negative,neutral,neutral,negative,negative
705492763,"Thank you. I tried to follow the steps, but something does not seem to work properly. I never used the tool before and am a python beginner, so I might have done something wrong :(. I am using windows 10. 

1. based on [this](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-666099538) comment, I prepared my audio and text files accordingly. I did not have webrtc
2. I used `python synthesizer_preprocess_audio.py datasets_root --datasets_name LibriTTS --subfolders train-clean-100 --no_alignments ` to preprocess the audio files, which seemed to work.
3. I then created the embeddings as describe in the [Training wiki ](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training): `python synthesizer_preprocess_embeds.py <datasets_root>/SV2TTS/synthesizer`
4. This resulted in a SV2TTS\synthesizer\audio\audio-utterance-001.npy, SV2TTS\synthesizer\embeds\embed-utterance-001.npy and SV2TTS\synthesizer\mels\mel-utterance-001.npy files for each utterance which should be fine.
5. I then started training the synthesizer with  `synthesizer_train.py my_run <datasets_root>\SV2TTS\synthesizer --checkpoint_interval 50 `
6. The plots of predicted and target spectrogram looked reasonably well and the wav files already sounded similar to the target voice, so I aborted after 300 steps 
7. ![step-300-mel-spectrogram](https://user-images.githubusercontent.com/72363021/95448718-0e866380-0964-11eb-9abe-6654c53c4a00.png)
8. I then tried to generate data for the vocoder with `python vocoder_preprocess.py <dataset_dir> --model_dir synthesizer\saved_models\logs-my_run --no_trim`, which stopped right after starting.

I got this message:

>  Loaded metadata for 104 examples (0.12 hours)
> Starting Synthesis
> 0it [00:00, ?it/s]

and no training data was created in <datasets_dir>\SV2TTS\vocoder\mels_gta and the synthesized.txt is also empty. 

At this point I'm stuck and do not know how to further proceed and find the problem. 

I also get a lot of warning about deprecated functions in tensorflow and wonder if I have the correct version of SV2TTs or my setup is wrong? I did it based on [this tutorial](https://poorlydocumented.com/2019/11/installing-corentinjs-real-time-voice-cloning-project-on-windows-10-from-scratch/)
 

",thank tried follow something seem work properly never used tool python beginner might done something wrong based comment prepared audio text accordingly used python audio work describe training python utterance fine training synthesizer target spectrogram reasonably well already similar target voice aborted step tried generate data python stopped right starting got message loaded starting synthesis training data also empty point stuck know proceed find problem also get lot warning wonder correct version setup wrong based tutorial,issue,negative,negative,neutral,neutral,negative,negative
705320243,"Can you post the file you are using to create the embedding, and the output you get from the toolbox? You can attach zip files here.",post file create output get toolbox attach zip,issue,negative,neutral,neutral,neutral,neutral,neutral
705319934,"You'll need to install the requirements to get the modules.
```
python3 -m pip install pip --upgrade
python3 -m pip install torch
python3 -m pip install -r requirements.txt
```",need install get python pip install pip upgrade python pip install torch python pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
705251505,"eh, seems like that isnt the issue. pyinstall seems to have missed a bunch of modules. 
`
This file lists modules PyInstaller was not able to find. This does not
necessarily mean this module is required for running you program. Python and
Python 3rd-party packages include a lot of conditional or optional modules. For
example the module 'ntpath' only exists on Windows, whereas the module
'posixpath' only exists on Posix systems.

Types if import:
* top-level: imported at the top-level - look at these first
* conditional: imported within an if-statement
* delayed: imported from within a function
* optional: imported within a try-except-statement

IMPORTANT: Do NOT post this list to the issue-tracker. Use it as a basis for
           yourself tracking down the missing module. Thanks!

missing module named _scproxy - imported by urllib.request (conditional)
missing module named termios - imported by tty (top-level), getpass (optional)
missing module named pwd - imported by posixpath (delayed, conditional), shutil (optional), tarfile (optional), pathlib (delayed, conditional, optional), http.server (delayed, optional), webbrowser (delayed), netrc (delayed, conditional), getpass (delayed), distutils.util (delayed, conditional, optional)
missing module named org - imported by copy (optional)
missing module named _posixshmem - imported by multiprocessing.resource_tracker (conditional), multiprocessing.shared_memory (conditional)
missing module named asyncio.DefaultEventLoopPolicy - imported by asyncio (delayed, conditional), asyncio.events (delayed, conditional)
missing module named posix - imported by os (conditional, optional), shutil (conditional)
missing module named resource - imported by posix (top-level), test.support (optional)
missing module named grp - imported by shutil (optional), tarfile (optional), pathlib (delayed)
missing module named vms_lib - imported by platform (delayed, conditional, optional)
missing module named 'java.lang' - imported by platform (delayed, optional), xml.sax._exceptions (conditional)
missing module named java - imported by platform (delayed)
missing module named _winreg - imported by platform (delayed, optional)
excluded module named _frozen_importlib - imported by importlib (optional), importlib.abc (optional), zipimport (top-level)
missing module named _frozen_importlib_external - imported by importlib._bootstrap (delayed), importlib (optional), importlib.abc (optional), zipimport (top-level)
missing module named readline - imported by cmd (delayed, conditional, optional), code (delayed, conditional, optional), pdb (delayed, optional)
missing module named 'org.python' - imported by pickle (optional), xml.sax (delayed, conditional)
missing module named multiprocessing.get_context - imported by multiprocessing (top-level), multiprocessing.pool (top-level), multiprocessing.managers (top-level), multiprocessing.sharedctypes (top-level)
missing module named multiprocessing.TimeoutError - imported by multiprocessing (top-level), multiprocessing.pool (top-level)
missing module named multiprocessing.BufferTooShort - imported by multiprocessing (top-level), multiprocessing.connection (top-level)
missing module named multiprocessing.AuthenticationError - imported by multiprocessing (top-level), multiprocessing.connection (top-level)
missing module named win32evtlog - imported by logging.handlers (delayed, optional)
missing module named win32evtlogutil - imported by logging.handlers (delayed, optional)
missing module named _posixsubprocess - imported by subprocess (optional), multiprocessing.util (delayed)
missing module named multiprocessing.set_start_method - imported by multiprocessing (top-level), multiprocessing.spawn (top-level)
missing module named multiprocessing.get_start_method - imported by multiprocessing (top-level), multiprocessing.spawn (top-level)
missing module named numpy - imported by encoder.model (top-level), encoder.audio (top-level), encoder.inference (top-level), toolbox.ui (top-level), synthesizer.models.helpers (top-level), synthesizer.models.tacotron (top-level), synthesizer.utils.plot (top-level), synthesizer.audio (top-level), synthesizer.tacotron2 (top-level), synthesizer.inference (top-level), vocoder.distribution (top-level), vocoder.display (top-level), vocoder.audio (top-level), toolbox (top-level), utils.argutils (top-level)
missing module named webrtcvad - imported by encoder.audio (optional), toolbox (delayed, optional)
missing module named audioread - imported by toolbox (top-level)
missing module named librosa - imported by encoder.audio (top-level), synthesizer.audio (top-level), synthesizer.inference (top-level), vocoder.audio (top-level), toolbox (top-level)
missing module named torch - imported by encoder.model (top-level), encoder.inference (top-level), synthesizer.models.modules (top-level), vocoder.models.fatchord_version (top-level), vocoder.distribution (top-level), vocoder.inference (top-level), toolbox (top-level)
missing module named lws - imported by synthesizer.audio (delayed)
missing module named 'scipy.io' - imported by synthesizer.audio (top-level)
missing module named scipy - imported by encoder.model (top-level), synthesizer.audio (top-level)
missing module named tensorflow - imported by synthesizer.models.tacotron (top-level), synthesizer.models.helpers (top-level), synthesizer.models.modules (top-level), synthesizer.models.architecture_wrappers (top-level), synthesizer.models.attention (top-level), synthesizer.models.custom_decoder (top-level), synthesizer.audio (top-level), synthesizer.tacotron2 (top-level), synthesizer.inference (top-level)
missing module named 'librosa.filters' - imported by synthesizer.audio (top-level)
missing module named 'tensorflow.contrib' - imported by synthesizer.models.helpers (top-level), synthesizer.models.tacotron (top-level), synthesizer.models.attention (top-level), synthesizer.models.architecture_wrappers (top-level), synthesizer.models.custom_decoder (top-level), synthesizer.hparams (top-level)
missing module named 'scipy.signal' - imported by vocoder.audio (top-level)
missing module named 'matplotlib.pyplot' - imported by encoder.inference (top-level), synthesizer.utils.plot (top-level), vocoder.display (top-level)
missing module named 'torch.nn' - imported by vocoder.models.fatchord_version (top-level), vocoder.distribution (top-level)
missing module named numba - imported by synthesizer.inference (top-level)
missing module named 'multiprocess.context' - imported by synthesizer.inference (top-level)
missing module named multiprocess - imported by synthesizer.inference (top-level)
missing module named matplotlib - imported by toolbox.ui (top-level), encoder.inference (top-level), synthesizer.utils.plot (top-level)
missing module named inflect - imported by synthesizer.utils.numbers (top-level)
missing module named unidecode - imported by synthesizer.utils.cleaners (top-level)
missing module named 'tensorflow.python' - imported by synthesizer.models.attention (top-level), synthesizer.models.architecture_wrappers (top-level), synthesizer.models.custom_decoder (top-level)
missing module named 'scipy.ndimage' - imported by encoder.audio (top-level)
missing module named 'scipy.optimize' - imported by encoder.model (top-level)
missing module named sklearn - imported by encoder.model (top-level)
missing module named umap - imported by toolbox.ui (top-level)
missing module named soundfile - imported by toolbox.ui (top-level)
missing module named sounddevice - imported by toolbox.ui (top-level)
missing module named 'PyQt5.QtWidgets' - imported by toolbox.ui (top-level)
missing module named PyQt5 - imported by toolbox.ui (top-level)
missing module named 'matplotlib.figure' - imported by toolbox.ui (top-level)
missing module named 'matplotlib.backends' - imported by toolbox.ui (top-level)
`

which may or may not bring me back to my original post.",eh like issue bunch file able find necessarily mean module running program python python include lot conditional optional example module whereas module import look first conditional within within function optional within important post list use basis missing module thanks missing module conditional missing module optional missing module conditional optional optional conditional optional optional conditional conditional optional missing module copy optional missing module conditional conditional missing module conditional conditional missing module o conditional optional conditional missing module resource optional missing module optional optional missing module platform conditional optional missing module platform optional conditional missing module platform missing module platform optional module optional optional missing module optional optional missing module conditional optional code conditional optional optional missing module pickle optional conditional missing module missing module missing module missing module missing module optional missing module optional missing module optional missing module missing module missing module toolbox missing module optional toolbox optional missing module toolbox missing module toolbox missing module torch toolbox missing module missing module missing module missing module missing module missing module missing module missing module missing module missing module missing module missing module missing module missing module inflect missing module missing module missing module missing module missing module missing module missing module missing module missing module missing module missing module missing module may may bring back original post,issue,negative,negative,negative,negative,negative,negative
705236898,"@azamet90 Is this still a problem? If you press ctrl+c while it is stuck, what is the traceback that is printed to the screen?",still problem press stuck printed screen,issue,negative,neutral,neutral,neutral,neutral,neutral
705236371,"Open up python and type the following. Does it return True? If not you'll have to troubleshoot your installation.
```
import torch
torch.cuda.is_available()
```",open python type following return true installation import torch,issue,negative,positive,positive,positive,positive,positive
705235712,@drats666 You might be close! #546 was merged yesterday and fixes a bug where the toolbox opens and instantly closes. Please make sure you have the latest code and try again.,might close yesterday bug toolbox instantly please make sure latest code try,issue,positive,positive,positive,positive,positive,positive
705225174,"Well, I  tried to compile it using pyinstaller. however the dist it creates opens then instantly closes.",well tried compile however instantly,issue,negative,neutral,neutral,neutral,neutral,neutral
704883532,"@jay-1104 Toolbox is a class, it's defined here in `__init.py__` in the toolbox folder: https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/e32cf8f4ddb63d9a7603eeb31f1855b54926aee6/toolbox/__init__.py#L42

If you're missing the toolbox folder then your copy of the repository is damaged and you should download it again.",toolbox class defined toolbox folder missing toolbox folder copy repository,issue,negative,negative,negative,negative,negative,negative
704867723,"Hello @blue-fish , i have checked everything and i found that there is line in the demo_toolbox stating ""import toolbox from TOOLBOX"" . 
But there is no TOOLBOX folder in the code , resulting in the import error !!

Please check it",hello checked everything found line import toolbox toolbox toolbox folder code resulting import error please check,issue,negative,neutral,neutral,neutral,neutral,neutral
704690811,"I didn't notice anything unusual in the pip freeze output, except you'll also need torch. However, installing torch is not going to solve your PyQt5 problem, which I don't know how to troubleshoot.

If you're looking for something to try, you could download [Anaconda 64-bit](https://www.anaconda.com/products/individual#Downloads) and follow the [wiki instructions](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Installation-Ubuntu-20.04---Installing-RTVC#setup-a-conda-workspace) for setup. If you still experience PyQt issues with Anaconda see this: https://stackoverflow.com/questions/42497689",notice anything unusual pip freeze output except also need torch however torch going solve problem know looking something try could anaconda follow setup still experience anaconda see,issue,negative,positive,positive,positive,positive,positive
704673650,"hey @blue-fish , i am using windows 10 OS.

Output of pip freeze : 
C:\Users\jayas>pip freeze
absl-py==0.10.0
astor==0.8.1
astroid==2.4.2
certifi==2020.6.20
colorama==0.4.3
cycler==0.10.0
gast==0.2.2
google-pasta==0.2.0
grpcio==1.32.0
h5py==2.10.0
importlib-metadata==2.0.0
isort==5.5.4
joblib==0.17.0
Keras-Applications==1.0.8
Keras-Preprocessing==1.1.2
kiwisolver==1.2.0
lazy-object-proxy==1.4.3
llvmlite==0.31.0
Markdown==3.3
matplotlib==3.3.2
mccabe==0.6.1
numba==0.48.0
numpy==1.19.2
opt-einsum==3.3.0
Pillow==7.2.0
protobuf==3.13.0
pylint==2.6.0
pyparsing==2.4.7
PyQt5==5.15.1
PyQt5-sip==12.8.1
python-dateutil==2.8.1
scipy==1.5.2
six==1.15.0
tensorboard==1.15.0
tensorflow==1.15.0
tensorflow-estimator==1.15.1
termcolor==1.1.0
threadpoolctl==2.1.0
toml==0.10.1
typed-ast==1.4.1
Werkzeug==1.0.1
wrapt==1.12.1
zipp==3.3.0

Please do needfull, i require it urgently
",hey o output pip freeze pip freeze please require urgently,issue,negative,neutral,neutral,neutral,neutral,neutral
704619170,"This solved a big question for me as to why the structure of the pre-trained model for the synthesizer had a completely different format than that for the encoder or vocoder.
",big question structure model synthesizer completely different format,issue,negative,neutral,neutral,neutral,neutral,neutral
704565602,"Hi @thehetpandya , have you made any progress on this recently?",hi made progress recently,issue,negative,neutral,neutral,neutral,neutral,neutral
704559221,Which operating system are you using? Can you provide the output of `pip freeze` so we know the version of packages you are using?,operating system provide output pip freeze know version,issue,negative,neutral,neutral,neutral,neutral,neutral
704556410,Please see #437 for information on how to do this. You should also refer to the training documentation here: https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training,please see information also refer training documentation,issue,negative,neutral,neutral,neutral,neutral,neutral
704524966,Not every reference audio will clone well. The quality depends on whether it is similar to other utterances seen by the encoder and synthesizer during training.,every reference audio clone well quality whether similar seen synthesizer training,issue,negative,neutral,neutral,neutral,neutral,neutral
704517695,"Closing this issue due to inactivity. Anyone is welcome to reopen it if taking this project on, or if a dataset is located that isn't VCTK. It is possible to train a voice cloning model on VCTK, but the result doesn't meet my standards.",issue due inactivity anyone welcome reopen taking project possible train voice model result meet,issue,negative,positive,positive,positive,positive,positive
704513074,Closing this issue due to inactivity. Please open an issue in https://github.com/resemble-ai/Resemblyzer if you have other questions on speaker verification.,issue due inactivity please open issue speaker verification,issue,negative,negative,neutral,neutral,negative,negative
704511837,Closing this issue due to inactivity. Would like to know what is causing the noise if you find out.,issue due inactivity would like know causing noise find,issue,negative,negative,negative,negative,negative,negative
704508323,Closing this issue due to inactivity. Feel free to reopen the issue if you would like to discuss further.,issue due inactivity feel free reopen issue would like discus,issue,positive,positive,positive,positive,positive,positive
704508133,Closing this issue due to inactivity. Feel free to reopen the issue if you have further questions.,issue due inactivity feel free reopen issue,issue,positive,positive,positive,positive,positive,positive
704504988,"> It would still be a good idea to change the order of the imports on our end to prevent unnecessary frustration for those not using matplotlib>=3.3.2.

Resolved in #546",would still good idea change order end prevent unnecessary frustration resolved,issue,negative,positive,positive,positive,positive,positive
704436858,"I also concluded TF is still better than PT, based on a test of 6 speakers in https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/472#issuecomment-687651582 . I'm a fan of fatchord's tacotron1 for its simplicity, but have yet to get good results for multispeaker despite a lot of experimentation. I plan to abandon it given the troubles I've had in getting it to train well.

The quality should improve by switching to tacotron2, however I do not have a timeline for when that might be completed.",also still better based test fan simplicity yet get good despite lot experimentation plan abandon given getting train well quality improve switching however might,issue,positive,positive,positive,positive,positive,positive
704377514,"That fix works, thanks.

Both models are pretty bad so it's not so easy to judge, although I'd say the TF one is still doing better...
[Tensorflow](https://puu.sh/GA6fz.flac)
[PyTorch](https://puu.sh/GA6fm.flac)
[Reference audio](https://puu.sh/GA6fI.wav)

It's is annoying, I'd really like to get this merged in, but we can't afford to sacrifice more quality. What are your next steps for improving the model? Also, I can't remember whether the pretrained vocoder is trained on GT or GTA mels. GT would be best, to avoid depending on a given synthesizer.
",fix work thanks pretty bad easy judge although say one still better reference audio annoying really like get ca afford sacrifice quality next improving model also ca remember whether trained would best avoid depending given synthesizer,issue,positive,positive,positive,positive,positive,positive
704343772,Also pushed a fix for the toolbox crashing on startup (presuming https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/504#issuecomment-678733034 is the cause of the crash).,also fix toolbox presuming cause crash,issue,negative,neutral,neutral,neutral,neutral,neutral
704336510,"> The toolbox crashes on start for me

Sounds like #504 , try using matplotlib<=3.2.2 for now. Also, I pushed a new hparams.py, please use that instead of the one bundled with the pretrained model.",toolbox start like try also new please use instead one model,issue,positive,positive,positive,positive,positive,positive
704172498,"Hey sorry for the delay. I got around testing a small sample with the demo but not with the toolbox. The toolbox crashes on start for me, that should be fixed. The sample I got was pretty bad unfortunately, but I'd like to test again with the toolbox working.",hey sorry delay got around testing small sample toolbox toolbox start fixed sample got pretty bad unfortunately like test toolbox working,issue,negative,negative,negative,negative,negative,negative
704045949,"ERROR: spyder 4.1.4 requires pyqtwebengine<5.13; python_version >= ""3"", which is not installed.                                                                             ERROR: spyder 4.1.4 has requirement pyqt5<5.13; python_version >= ""3"", but you'll have pyqt5 5.15.1 which is incompatible.",error error requirement incompatible,issue,negative,neutral,neutral,neutral,neutral,neutral
703869531,I'm glad it's working for you now @varungolusupudi. Did you resolve it with `--no_mp3_support` or by adding ffmpeg to your path?,glad working resolve path,issue,positive,positive,positive,positive,positive,positive
703456581,"Hi there @drats666 , thank you for taking the time to share your perspective. I can relate to this because there are a great deal of interesting repos but there's usually some obstacle and I decide that it's not worth hours of my time to set up.

We actually have an issue for this: #267 and it's the only one marked ""help wanted"" to draw special attention to it. I don't have Windows, but someone who does can run [PyInstaller](https://www.pyinstaller.org/) on demo_toolbox.py and upload the result.",hi thank taking time share perspective relate great deal interesting usually obstacle decide worth time set actually issue one marked help draw special attention someone run result,issue,positive,positive,positive,positive,positive,positive
703392382,"@halswift Would you please export a wav file (preferably using one of the included [samples](https://github.com/CorentinJ/Real-Time-Voice-Cloning/tree/master/samples) to make the embedding), zip it up and attach it here?

I'd like to compare the encoding of the wav file with what is generated on my system. I think it's a system configuration or dependency issue.",would please export file preferably one included make zip attach like compare file system think system configuration dependency issue,issue,positive,neutral,neutral,neutral,neutral,neutral
702937853,"> I've had a great experience following Corentin's instructions for reproducing the synthesizer and vocoder training: https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training Once you get the hang of it, try modifying the parameters and using different datasets with it. Do some experimentation and use the papers in the README for reference.
> 
> Next, pick some kind of a project, like training a synthesizer model in a different language, and you will learn from solving the problems as they come up.
> 
> If you want to skip straight to voice style transfer, check out [AutoVC](https://github.com/auspicious3000/autovc) or [SpeechSplit](https://github.com/auspicious3000/SpeechSplit). That community is not as developed as TTS so you may have a harder time getting help.

Thank you very much! ",great experience following synthesizer training get try different experimentation use reference next pick kind project like training synthesizer model different language learn come want skip straight voice style transfer check community may harder time getting help thank much,issue,positive,positive,positive,positive,positive,positive
702935778,"I've had a great experience following Corentin's instructions for reproducing the synthesizer and vocoder training: https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training Once you get the hang of it, try modifying the parameters and using different datasets with it. Do some experimentation and use the papers in the README for reference.

Next, pick some kind of a project, like training a synthesizer model in a different language, and you will learn from solving the problems as they come up.

If you want to skip straight to voice style transfer, check out [AutoVC](https://github.com/auspicious3000/autovc) or [SpeechSplit](https://github.com/auspicious3000/SpeechSplit). That community is not as developed as TTS so you may have a harder time getting help.",great experience following synthesizer training get try different experimentation use reference next pick kind project like training synthesizer model different language learn come want skip straight voice style transfer check community may harder time getting help,issue,positive,positive,positive,positive,positive,positive
701667787,"Feedback is appreciated @Choons , it's always helpful to hear from those who are using the software and models.",feedback always helpful hear,issue,negative,neutral,neutral,neutral,neutral,neutral
701662985,"Understood. I'm glad you have taken on improving this voice project. I have tried to use other voice cloning implementations, but could never get them working as well as this one, even with the gap problem. I will experiment with both of your solutions and report back in this post how well they work for me.",understood glad taken improving voice project tried use voice could never get working well one even gap problem experiment report back post well work,issue,negative,positive,positive,positive,positive,positive
701655731,"> Can you clarify-- do we need to add BOTH the code from #538 and #472 , or do we choose just one of either? ie a tensorflow solution versus a pytorch solution.

Most users today will want #538 because we haven't formally switched to the pytorch synthesizer. Once #472 is merged we will update the [pretrained models](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models) wiki page to point to pytorch.

> And if it's a choice between the two solutions, which one do you recommend as best performing?

They're about the same in performance. They have different quirks since the tacotron is different (tacotron 1 vs 2). In tensorflow (Rayhane-taco2) the stop token prediction sometimes fails and it synthesizes a huge silence until the decoder limit is reached. In pytorch (fatchord-taco1) the attention may get stuck on a certain character and making inference quit suddenly. Pick your poison. The attention mechanism needs to be improved.",clarify need add code choose one either ie solution versus solution today want formally switched synthesizer update page point choice two one recommend best performance different since different stop token prediction sometimes huge silence limit attention may get stuck certain character making inference quit suddenly pick poison attention mechanism need,issue,positive,positive,positive,positive,positive,positive
701536699,"Wow, bluefish, you have done some incredible work on this! Can you clarify-- do we need to add BOTH the code from #538 and #472 , or do we choose just one of either? ie a tensorflow solution versus a pytorch solution.

And if it's a choice between the two solutions, which one do you recommend as best performing?",wow bluefish done incredible work clarify need add code choose one either ie solution versus solution choice two one recommend best,issue,positive,positive,positive,positive,positive,positive
701519361,Thanks for reporting this @padmalcom . After #517 we check that librosa can load a sample audio on startup. If NoBackendError occurs while loading a file we can be fairly confident that the audio file is damaged. I'll consider adding a more helpful error message. ,thanks check load sample audio loading file fairly confident audio file consider helpful error message,issue,positive,positive,positive,positive,positive,positive
701392396,"@blue-fish I completely agree, running the experiment is the best option. ",completely agree running experiment best option,issue,positive,positive,positive,positive,positive,positive
701299815,"Glad to hear it, please feel free to open another issue if you experience another problem or have questions about the repo.",glad hear please feel free open another issue experience another problem,issue,positive,positive,positive,positive,positive,positive
701271086,"I plan to leave the pull request unmerged and see if anyone else requests this. To improve maintainability of the code we need to avoid adding rarely-used features. In the meantime, UserAudio is working for you and anyone else who stumbles across this pull request.",plan leave pull request unmerged see anyone else improve code need avoid working anyone else across pull request,issue,negative,neutral,neutral,neutral,neutral,neutral
701267442,Thanks @CorentinJ . We can delete the `low_mem_fix` branch since it is now in master.,thanks delete branch since master,issue,negative,positive,positive,positive,positive,positive
701263350,"I tried training on VCTK, but there is not enough data to make a good voice cloning model. Many of the speakers in that dataset have unsuitable accents (American, Canadian, Australian, Indian). Furthermore, the vocabulary in that corpus lacks the variety that we have in LibriSpeech/TTS which makes for a better synthesizer.

If anyone wants this, please provide the dataset and transcripts in a usable format (preferably the one in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-666099538 ). Commonvoice is a potential dataset as you can use the metadata to filter by accent. You could also explore the speakers in the Spoken Wikipedia Corpora (https://nats.gitlab.io/swc/).",tried training enough data make good voice model many unsuitable furthermore vocabulary corpus variety better synthesizer anyone please provide usable format preferably one potential use filter accent could also explore spoken corpus,issue,positive,positive,positive,positive,positive,positive
701258025,"> @Kurokawa-san Is this resolved?

Yes, it works perfectly now, thanks",resolved yes work perfectly thanks,issue,positive,positive,positive,positive,positive,positive
701242408,"Would like to highlight this again:
> The presence of gaps depends on the training data.

Trained a new synthesizer with a curated dataset, in #538 (tensorflow) and https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/472#issuecomment-695206377 (pytorch). This fixes the issue with gaps.",would like highlight presence training data trained new synthesizer issue,issue,positive,positive,positive,positive,positive,positive
701239948,"This model still has the occasional attention failure. However, this is not caused by Corentin's modifications to Rayhane's taco2. I have studied the differences line by line and concluded that there is no error introduced. Rather, I think the attention problems are inherent to the SV2TTS architecture, particularly because the speaker embedding is input to the attention mechanism.

![image](https://user-images.githubusercontent.com/67130644/94659641-73352480-02b9-11eb-9f25-8e3bc09297a3.png)

Attention is problematic even in single-speaker tacotrons, and it gets worse in multispeaker due to the speaker embedding concat. This highlights the need to use a better attention mechanism for SV2TTS.
",model still occasional attention failure however studied line line error rather think attention inherent architecture particularly speaker input attention mechanism image attention problematic even worse due speaker need use better attention mechanism,issue,negative,negative,neutral,neutral,negative,negative
701226443,"If you have no further questions at this time, please close it. You're welcome to open another issue whenever you have something to discuss.",time please close welcome open another issue whenever something discus,issue,positive,positive,positive,positive,positive,positive
701202451,"> This might be related? [#229 (comment)](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/229#issuecomment-557975932)
> 
> Regarding your configuration, you should consider running tensorflow 1.15.2 with CUDA 10.0 and cuDNN 7.4 per the [compatibility chart](https://www.tensorflow.org/install/source_windows#tested_build_configurations). Do you get a tensorflow warning about CUDA and cuDNN mismatch?

I've found a solution! I installed pytorch using:
`conda install pytorch torchvision cudatoolkit=10.1 -c pytorch
`
and used this tensorflow wheel: https://github.com/fo40225/tensorflow-windows-wheel/blob/master/1.14.0/py37/GPU/cuda101cudnn76sse2/tensorflow_gpu-1.14.0-cp37-cp37m-win_amd64.whl

and just left the cudnn64_7.dll there, and it works now! It synthesises and vocodes just fine. Thank you!",might related comment regarding configuration consider running per compatibility chart get warning mismatch found solution install used wheel left work fine thank,issue,negative,positive,positive,positive,positive,positive
701198558,"> This might be related? [#229 (comment)](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/229#issuecomment-557975932)
> 
> Regarding your configuration, you should consider running tensorflow 1.15.2 with CUDA 10.0 and cuDNN 7.4 per the [compatibility chart](https://www.tensorflow.org/install/source_windows#tested_build_configurations). Do you get a tensorflow warning about CUDA and cuDNN mismatch?

I'd switch to TF 1.15.2 and CUDA 10.0, but I can't find TF 1.15.2 that doesn't use AVX and uses CUDA 10.0, the closest one I can find is 1.15.0 with CUDA 10.1
Also, tried solution from 229, but now this error appears on startup:
`Traceback (most recent call last):
  File "".\demo_toolbox.py"", line 2, in <module>
    from toolbox import Toolbox
  File ""C:\Users\ABLPHA\Desktop\mods\Source SDK\Real-Time-Voice-Cloning-master\toolbox\__init__.py"", line 1, in <module>
    from toolbox.ui import UI
  File ""C:\Users\ABLPHA\Desktop\mods\Source SDK\Real-Time-Voice-Cloning-master\toolbox\ui.py"", line 5, in <module>
    from encoder.inference import plot_embedding_as_heatmap
  File ""C:\Users\ABLPHA\Desktop\mods\Source SDK\Real-Time-Voice-Cloning-master\encoder\inference.py"", line 2, in <module>
    from encoder.model import SpeakerEncoder
  File ""C:\Users\ABLPHA\Desktop\mods\Source SDK\Real-Time-Voice-Cloning-master\encoder\model.py"", line 5, in <module>
    from torch.nn.utils import clip_grad_norm_
  File ""C:\ProgramData\Anaconda3\envs\deep-voice-cloning-no-avx\lib\site-packages\torch\__init__.py"", line 81, in <module>
    from torch._C import *
ImportError: DLL load failed: The specified module could not be found.`",might related comment regarding configuration consider running per compatibility chart get warning mismatch switch ca find use one find also tried solution error recent call last file line module toolbox import toolbox file line module import file line module import file line module import file line module import file line module import load module could,issue,negative,neutral,neutral,neutral,neutral,neutral
701189381,"This might be related? https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/229#issuecomment-557975932

Regarding your configuration, you should consider running tensorflow 1.15.2 with CUDA 10.0 and cuDNN 7.4 per the [compatibility chart](https://www.tensorflow.org/install/source_windows#tested_build_configurations). Do you get a tensorflow warning about CUDA and cuDNN mismatch?",might related regarding configuration consider running per compatibility chart get warning mismatch,issue,negative,neutral,neutral,neutral,neutral,neutral
701186697,I'd like to stress that this error also occurs when an audio file can not be read. Took me a day to figure out in my environment.,like stress error also audio file read took day figure environment,issue,negative,neutral,neutral,neutral,neutral,neutral
701177744,"> Because only one file changed, you may find it easier to replace the contents of your `synthesizer/inference.py` with this: https://raw.githubusercontent.com/blue-fish/Real-Time-Voice-Cloning/491_fix_lowmem_seed/synthesizer/inference.py

Now it went much further, but another error appeared:
`Traceback (most recent call last):
  File ""C:\Users\ABLPHA\Desktop\mods\Source SDK\Real-Time-Voice-Cloning-master\toolbox\__init__.py"", line 229, in synthesize
    specs = self.synthesizer.synthesize_spectrograms(texts, embeds)
  File ""C:\Users\ABLPHA\Desktop\mods\Source SDK\Real-Time-Voice-Cloning-master\synthesizer\inference.py"", line 102, in synthesize_spectrograms
    [(self.checkpoint_fpath, embeddings, texts, self._seed)])[0]
  File ""C:\ProgramData\Anaconda3\envs\deep-voice-cloning-no-avx\lib\site-packages\multiprocess\pool.py"", line 276, in starmap
    return self._map_async(func, iterable, starmapstar, chunksize).get()
  File ""C:\ProgramData\Anaconda3\envs\deep-voice-cloning-no-avx\lib\site-packages\multiprocess\pool.py"", line 657, in get
    raise self._value
multiprocess.pool.MaybeEncodingError: Error sending result: '<multiprocess.pool.ExceptionWithTraceback object at 0x0000025FA21D69E8>'. Reason: 'TypeError(""can't pickle SwigPyObject objects"")'`",one file may find easier replace content went much another error recent call last file line synthesize spec file line file line return iterable file line get raise error sending result object reason ca pickle,issue,negative,positive,neutral,neutral,positive,positive
701159897,"It's possible to use do machine learning with an AMD GPU by using ROCm (I believe there are unofficial pytorch and tensorflow releases). But why make this any harder than it needs to be?

Presuming you have a NVIDIA GPU, @mbdash has a nice wiki guide for [installing the toolbox on Ubuntu 20.04](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Installation---Ubuntu-20.04) .",possible use machine learning believe unofficial make harder need presuming nice guide toolbox,issue,negative,positive,positive,positive,positive,positive
701157865,"In the SV2TTS paper it is stated that ""the audio quality [for encoder training] can be lower than for TTS training"" but between this and the GE2E paper I have not seen a statement that it **should** be of lower quality. The noise might train the network to distinguish based on features that humans can perceive as opposed to subtler differences that can be found in clean audio. But I think it's worth running the experiment to see if this is truly the case.",paper stated audio quality training lower training gee paper seen statement lower quality noise might train network distinguish based perceive opposed found clean audio think worth running experiment see truly case,issue,negative,positive,positive,positive,positive,positive
701151705,I guess these things require Good GPU and AMD won't be supported right? I'm running Ubuntu 20.04,guess require good wo right running,issue,negative,positive,positive,positive,positive,positive
701149680,"It would involve first training a multi-speaker model from scratch for the Tamil language, then you could clone your mom's voice from individual recordings the toolbox. We don't have any guides, but https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/431#issuecomment-673555684 and #492 can give you an idea of what it involves. The project will require several weeks of your time.

You'll need some datasets to train the base text-to-speech model, a quick search yielded the following:
* https://openslr.org/65 , ""Crowdsourced high-quality Tamil multi-speaker speech data set""
* https://msropendata.com/datasets/7230b4b1-912d-400e-be58-f84e0512985e , ""Microsoft Speech Corpus (Indian Languages)""

If you're successful, you have the option of finetuning your multi-speaker model into a higher-quality single-speaker model. The process in #437 requires about 20 minutes of recordings.",would involve first training model scratch language could clone voice individual toolbox give idea project require several time need train base model quick search following speech data set speech corpus successful option model model process,issue,positive,positive,neutral,neutral,positive,positive
701109601,"@mbdash I believe corentin was referring to low quality audio (background noise, static, etc) being important while training the encoder. Clean audio is important for synthesis. 
",believe low quality audio background noise static important training clean audio important synthesis,issue,negative,positive,positive,positive,positive,positive
701019423,"Because only one file changed, you may find it easier to replace the contents of your `synthesizer/inference.py` with this: https://raw.githubusercontent.com/blue-fish/Real-Time-Voice-Cloning/491_fix_lowmem_seed/synthesizer/inference.py",one file may find easier replace content,issue,negative,neutral,neutral,neutral,neutral,neutral
701006631,"A small update for anyone watching this thread.
@steven and @blue-fish are doing some experimentation with training.
I am currently cleaning up datasets to remove noise and artifacts from the source data used to train the models.

I saw @CorentinJ 's comment. 
> > VoxCeleb is rejected due to it's horrible quality.
> 
> The idea is to have a dataset with low quality though

We are just playing around, putting our resources in common and experimenting to improve audio output quality as well as adding some punctuation support.

I am done with LibriTTS60/train-clean-100
and progressing through 360",small update anyone watching thread steven experimentation training currently cleaning remove noise source data used train saw comment due horrible quality idea low quality though around common improve audio output quality well punctuation support done,issue,negative,negative,negative,negative,negative,negative
700998750,"> @mineLdiver Please see #536 for the fix, please report if it works.

I'll check it out in about 10 hours, can't do it right now, but will definitely. Thanks ",please see fix please report work check ca right definitely thanks,issue,positive,positive,positive,positive,positive,positive
700997197,"@mineLdiver Please see #536 for the fix, please report if it works.",please see fix please report work,issue,positive,neutral,neutral,neutral,neutral,neutral
700996196,"Please visit the new Wiki.
https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki

I highly recommend Ubuntu 18 or 20 and miniconda3.
cheers.",please visit new highly recommend,issue,positive,positive,positive,positive,positive,positive
700987781,"> I have been overly optimistic about how quickly we could switch over to the pytorch synthesizer (which does not have this issue). Since you're the third person who has asked (others in #491, #529), I'll submit a pull request to fix this bug.

Thank you! So all I need to do is just wait and download updated version after?",overly optimistic quickly could switch synthesizer issue since third person submit pull request fix bug thank need wait version,issue,positive,positive,positive,positive,positive,positive
700983358,"I have been overly optimistic about how quickly we could switch over to the pytorch synthesizer (which does not have this issue). Since you're the third person who has asked (others in #491, #529), I'll submit a pull request to fix this bug.",overly optimistic quickly could switch synthesizer issue since third person submit pull request fix bug,issue,negative,positive,positive,positive,positive,positive
700956972,Couldn't we train other languages like german from the data of audiobooks?,could train like german data,issue,negative,neutral,neutral,neutral,neutral,neutral
700845415,I feel so dumb... i spelt it wrong.. But i definetly did have a 32-bit version so thank you very much for informing me of that!,feel dumb spelt wrong version thank much,issue,negative,negative,negative,negative,negative,negative
700820673,"@TwashMan It's highly likely you are using a 32-bit python, and tensorflow only supports 64-bit. Please install a 64-bit python and try again.",highly likely python please install python try,issue,negative,neutral,neutral,neutral,neutral,neutral
700043672,"@CodingRox82 Hi, 
if you are seriously interested in Voice to Voice / Voice changer / Voice Transfer / ""insert any other description that involves converting the audio from 1 speaker to another without passing through TTS"";

Would you be interested in joining a small group with common interest?
We are currently working on creating a polished dataset.
Our small group have different but overlapping interests for the good of this repo and others that can provide voice to voice, bypassing TTS.

If you are interested, leave a comment in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/474
",hi seriously interested voice voice voice changer voice transfer insert description converting audio speaker another without passing would interested joining small group common interest currently working polished small group different good provide voice voice interested leave comment,issue,positive,positive,neutral,neutral,positive,positive
699757298,"I want to implement something like this for voice-to-voice. Basically, I want to record a voice and then use this as a basis for masking N voices, where N >> 1. Some questions:
1. _If you're planning to work on a serious project, my strong advice: find another TTS repo._: @CorentinJ , would this comment still apply if I don't need the part that reads and creates audio from a given text?
2. I understand that the impressive part of this repo is that it can clone a voice given only 5 seconds of audio, but in general does the output improve with training on more (and more diverse) data? What I wanted to have a professional speaker record hours of data to serve as input audio - would the output improve in quality?",want implement something like basically want record voice use basis work serious project strong advice find another would comment still apply need part audio given text understand impressive part clone voice given audio general output improve training diverse data professional speaker record data serve input audio would output improve quality,issue,positive,positive,positive,positive,positive,positive
699705804,"I just saw your update now, I'll be comparing your model with the base one sometime this week. It's been a while I haven't run the repo.",saw update model base one sometime week run,issue,negative,negative,negative,negative,negative,negative
699561615,"You can either disable mp3 support with:
```
python demo_cli.py --low_mem --no_mp3_support
```

Or if you want mp3 support, you likely need to add ffmpeg to your path (see the ""edit environment variables"" section of: https://video.stackexchange.com/a/20496 ).",either disable support python want support likely need add path see edit environment section,issue,positive,neutral,neutral,neutral,neutral,neutral
699131301,"final fix:

`sudo apt install --reinstall libqt5gui5`",final fix apt install reinstall,issue,negative,positive,positive,positive,positive,positive
699111613,"`
Python 3.6.9 (default, Jul 17 2020, 12:50:27) 
[GCC 8.4.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import matplotlib
>>> matplotlib.use('Qt5Agg')
>>> from PyQt5 import QtCore, QtWidgets
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: /usr/lib/x86_64-linux-gnu/libQt5Core.so.5: version `Qt_5.15' not found (required by /usr/local/lib/python3.6/dist-packages/PyQt5/QtCore.abi3.so)

`",python default type help copyright license information import import recent call last file line module version found,issue,negative,neutral,neutral,neutral,neutral,neutral
698261114,"Thanks @blue-fish, I've already applied for Source 1. Will also check out the second one. Your efforts on this project are much appreciated!",thanks already applied source also check second one project much,issue,negative,positive,positive,positive,positive,positive
698151174,"@blue-fish Yes, I think that I'm satisfied on the synthesizer model quality. But when I try to fine tune vocoder (on 16kHz data) on the output I listen just simple noise... ",yes think satisfied synthesizer model quality try fine tune data output listen simple noise,issue,positive,positive,positive,positive,positive,positive
697858971,"I need feedback from the community on whether this implementation is suitable, or if we need to try something different.

Personally, I find the attention failures annoying when they occur. Better attention mechanism and switching to tacotron2 may resolve this. I'm also experimenting with the so-called [Forward Tacotron](https://github.com/as-ideas/ForwardTacotron) which does not use any attention, but requires alignments for training.

Reproducible training of the pretrained model is also a goal. This tacotron1 is hard to train multispeaker without clean data, and until I gain more experience it's hard to tell if it's specific to this implementation or a more general issue with neural TTS.",need feedback community whether implementation suitable need try something different personally find attention annoying occur better attention mechanism switching may resolve also forward use attention training reproducible training model also goal hard train without clean data gain experience hard tell specific implementation general issue neural,issue,negative,positive,neutral,neutral,positive,positive
697833424,"@rlutsyshyn Yes, though you'll want to make sure you are satisfied with the synthesizer before moving on to vocoder training.",yes though want make sure satisfied synthesizer moving training,issue,positive,positive,positive,positive,positive,positive
697514530,"You might be able the combine the two sources below. First train a single-speaker model on source 1, then tune the voice cloning aspect on source 2. Some effort and experimentation will be required.

Source 1 (24 hours single-speaker): https://cvit.iiit.ac.in/research/projects/cvit-projects/text-to-speech-dataset-for-indian-languages
Source 2 (100 voices, 6 utterances each, untranscribed): https://github.com/shivam-shukla/Speech-Dataset-in-Hindi-Language",might able combine two first train model source tune voice aspect source effort experimentation source source untranscribed,issue,negative,positive,positive,positive,positive,positive
697356947,@blue-fish Can the vocoder fine tuning improve output audio quality?,fine tuning improve output audio quality,issue,positive,positive,positive,positive,positive,positive
697167337,"@rlutsyshyn That's something that I continue to work on now. I am experimenting with different synthesizer models and settings, but I still have not surpassed the pretrained models from Corentin.",something continue work different synthesizer still,issue,negative,neutral,neutral,neutral,neutral,neutral
696966318,"There isn't any complete process, but to get an idea of what it involves, refer to https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/431#issuecomment-673555684 and #492 .

It would be nice if someone would write up a guide.",complete process get idea refer would nice someone would write guide,issue,negative,positive,positive,positive,positive,positive
696548313,Was going to ask the same thing. I didn't find the Hindi open speech dataset on the internet yet.,going ask thing find open speech yet,issue,negative,neutral,neutral,neutral,neutral,neutral
696048648,"So for the news, i continue training but with harder trimming : 
- All frames with max_value < 0.6 * range_of_value (= max_val - min_val)
- All frames with a mean value < 0.33 * range_of_value
I also try by using utterance embedding instead of speaker-embedding and it seems to give better results  (in term of loss)

The resulting audios in prediction (teacher-forcing) are really similar but inference is really bad (not intelligible) and i don’t know how to improve my inference because the attention seems ok in teacher forcing and my loss is better than my single-speaker model which is really good at prediction and inference so it’s not a problem of loss so...

I think the next step (when i have time) would be to create a training function without teacher-forcing to force improving inference but i never tried inference-training so i don’t know if it’s as good as teacher-forcing training",news continue training harder trimming mean value also try utterance instead give better term loss resulting prediction really similar inference really bad intelligible know improve inference attention teacher forcing loss better model really good prediction inference problem loss think next step time would create training function without force improving inference never tried know good training,issue,positive,positive,positive,positive,positive,positive
695347889,"@blue-fish thanks, now it works good. But how can I improve the quality of the output ? ",thanks work good improve quality output,issue,positive,positive,positive,positive,positive,positive
695280283,"Your train.txt is improperly formatted. Here is an example line:
```
audio-p240_001.npy|mel-p240_001.npy|embed-p240_001.npy|38921|195|Please call Stella.
```",improperly example line call stella,issue,negative,neutral,neutral,neutral,neutral,neutral
695231861,"@blue-fish I tried to do what you said but I have still same results... bla bla bla. I checked  datasets/SV2TTS/synthesizer/train.txt file and all is good there e.g.:
`audio-Track 1 - 218.npy|mel-Track 1 - 218.npy|embed-Track 1 - 218.npy|113367|567|Track 1 - 218|You humans who listened to the low notes from the tuba rated it as bittersweet.|You humans who listened to the low notes from the tuba rated it as bittersweet`

I used first embedding for fine tuning model, and same embedding for inference in toolbox or demo_cli.py While I fine tuned the model loss was +-0.5 and won't fall more.",tried said still checked file good low tuba rated low tuba rated bittersweet used first fine tuning model inference toolbox fine tuned model loss wo fall,issue,negative,positive,positive,positive,positive,positive
695206377,"Here is a pretrained model that can be used with the synthesizer: https://www.dropbox.com/s/gemy4hlm3fdf76u/librispeech_130k.zip?dl=0

1. Place `pretrained.pt` in `synthesizer/saved_models/pretrained/pretrained.pt`
2. Ignore the hparams.py in the zip file. The required changes were merged into the branch in commit [`39f90da`](https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/472/commits/39f90daa6ceb853657dace53a4449ffaf66cf349)

Would anyone testing this please take the time to leave a comment with your thoughts on how well this pretrained model performs?",model used synthesizer place ignore zip file branch commit would anyone testing please take time leave comment well model,issue,negative,neutral,neutral,neutral,neutral,neutral
695203346,"@rlutsyshyn After https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/492#issuecomment-695072250 , your entire dataset is using the embedding from file 1. The embedding corresponds to a specific audio file, let's call it file1.wav. When you test your new synthesizer in the toolbox (or demo_cli.py), you must remember to load file1.wav to generate the embedding.",entire file specific audio file let call test new synthesizer toolbox must remember load generate,issue,negative,positive,neutral,neutral,positive,positive
695195810,"@blue-fish 

> For inference, make sure you load the same audio file used to generate your embeds for finetuning.

what did you mean? ",inference make sure load audio file used generate mean,issue,negative,positive,neutral,neutral,positive,positive
695118456,I will be trying to do the same with spanish. Wish me luck. Any suggestions about compute power?,trying wish luck compute power,issue,positive,neutral,neutral,neutral,neutral,neutral
695072250,"@rlutsyshyn You have 400 wav files in your training set for finetuning. When you run `synthesizer_preprocess_embeds.py` it will make embed-file1.npy, ... , embed-file400.npy, in `SV2TTS/synthesizer/embeds`. Copy the contents of file 1 to files 2-400, so that they are all the same.

@Ananas120 I use the embedding of a real utterance so I can load the audio file in the toolbox to get the desired embedding. The mean or L2-norm is technically better but with a good encoder model it shouldn't make much of a difference.",training set run make copy content file ananas use real utterance load audio file toolbox get desired mean technically better good model make much difference,issue,positive,positive,positive,positive,positive,positive
695053108,"At the moment i use a « speaker-embedding » (the mean of all utterances embeddings), is it more interesting or is it better to user 1 single « real » utterance embedding for all ?",moment use mean interesting better user single real utterance,issue,positive,positive,positive,positive,positive,positive
695051439,"@blue-fish Can you explain this approach with same embedding more accurate, please?",explain approach accurate please,issue,negative,positive,positive,positive,positive,positive
695050827,"When finetuning, use the same embedding for all of your samples for faster convergence. I take the embedding of the first audio file and use it to overwrite all the others. For inference, make sure you load the same audio file used to generate your embeds for finetuning.

If it still doesn't work, check your preprocessing and also make sure the transcripts in `train.txt` matches what is spoken in the audio files.",use faster convergence take first audio file use overwrite inference make sure load audio file used generate still work check also make sure spoken audio,issue,positive,positive,positive,positive,positive,positive
695044015,@blue-fish @Ananas120 I used english synthesizer and try to fine rune on english data but recorded by my self. I collected 400 samples of utterances and try to fine tune synthesizer on them but had bla bla bla . ,ananas used synthesizer try fine rune data self collected try fine tune synthesizer,issue,negative,positive,positive,positive,positive,positive
695020272,"For the classic Tacotron-2 model, training from En to another language work (in Fr for me) but En and Fr sounds are not as far as that so i suppose mapping slightly differs but not as much
For this model it doesn’t work but i think it’s not the fault of the pretrained weights but of my encoder or my dataset or my preprocessing",classic model training en another language work en far suppose slightly much model work think fault,issue,negative,positive,positive,positive,positive,positive
695018279,@rlutsyshyn Are you taking the pretrained synthesizer (English) and finetuning on your Ukrainian data? That's not going to work because the mapping of letters to sounds will not match. You need to start the synthesizer training from scratch when working with a new language.,taking synthesizer data going work match need start synthesizer training scratch working new language,issue,negative,positive,positive,positive,positive,positive
694847400,@GauriDhande I'm still looking for a good hindi speech dataset. Do you have any sources?,still looking good speech,issue,negative,positive,positive,positive,positive,positive
694691382,"@blue-fish Hey! Can you give me an advice? When I used data for fine tuning (16kHz english speaker) and fine tune only sysnthesizer  after testing I had similar voice but words are like bla bla bla ... bla bla bla Is that problem with synthesizer or I have to train (fine tune) vocoder for that voice?
Thnx",hey give advice used data fine tuning speaker fine tune testing similar voice like problem synthesizer train fine tune voice,issue,negative,positive,positive,positive,positive,positive
694446230,"I should have been a little more clear, you should be able to make the above change on Corentin's master and it should also resolve the ""self"" issue by not referencing it.

Since you're using my fork as the starting point, try replacing `seed=self._seed` with `seed=None` in this line:
https://github.com/blue-fish/Real-Time-Voice-Cloning/blob/8e4a0565544d0ae6ee3d22518961a28ed88eba5e/synthesizer/inference.py#L100-L101

If that doesn't work then try the CPU mode since that bypasses the low_mem code that is giving you issues.",little clear able make change master also resolve self issue since fork starting point try line work try mode since code giving,issue,positive,positive,positive,positive,positive,positive
694362284,"The issue is that ""self"" does not appear to be referenced in this python. That is why i used your other branch. How can i fix that error?",issue self appear python used branch fix error,issue,negative,neutral,neutral,neutral,neutral,neutral
694361109,"> I have tried the main repository, and i get the error that ""self"" is not defined. There was an issue about this, so i used the branch from that fix. Now i am getting:
> **raceback (most recent call last): File ""demo_cli.py"", line 93, in mels = synthesizer.synthesize_spectrograms(texts, embeds) File ""D:\RealTimeDeepFake\voice\Real-Time-Voice-Cloning\Real-Time-Voice-Cloning\synthesizer\inference.py"", line 101, in synthesize_spectrograms [(self.checkpoint_fpath, embeddings, texts, self._seed)])[0] File ""C:\Users\fnafm\AppData\Local\Programs\Python\Python37\lib\site-packages\multiprocess\pool.py"", line 276, in starmap return self._map_async(func, iterable, starmapstar, chunksize).get() File ""C:\Users\fnafm\AppData\Local\Programs\Python\Python37\lib\site-packages\multiprocess\pool.py"", line 657, in get raise self._value multiprocess.pool.MaybeEncodingError: Error sending result: '<multiprocess.pool.ExceptionWithTraceback object at 0x00000250FDB07DC8>'. Reason: 'TypeError(""can't pickle tensorflow_core.python._tf_stack.Stack objects"")'**
> 
> I have tried using conda environment with python 3.7, and also, plain python, following this procedure : https://poorlydocumented.com/2019/11/installing-corentinjs-real-time-voice-cloning-project-on-windows-10-from-scratch/
> 
> My hardware really sucks and i can't run linux. I am stuck with live sessions and windows.
> How can i fix the issue?

_Originally posted by @AntINFINAit in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/529#issue-703594168_",tried main repository get error self defined issue used branch fix getting recent call last file line file line file line return iterable file line get raise error sending result object reason ca pickle tried environment python also plain python following procedure hardware really ca run stuck live session fix issue posted,issue,negative,positive,neutral,neutral,positive,positive
694357816,"Hi @AntINFINAit , thank you for reporting this issue. As a workaround, remove the `seed` argument from this line:
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/8f71d678d2457dffc4d07b52e75be11433313e15/synthesizer/inference.py#L109

That is, change it to:
```
model = Tacotron2(checkpoint_fpath, hparams) 
```

You will be not have the ability to force deterministic synthesis, but at least it should run. If you need that feature, try the [`423_add_cpu_mode`](https://github.com/blue-fish/Real-Time-Voice-Cloning/tree/423_add_cpu_mode) branch of my fork, and change the `--low_mem` argument to `--cpu`.",hi thank issue remove seed argument line change model ability force deterministic synthesis least run need feature try branch fork change argument,issue,negative,negative,negative,negative,negative,negative
693667012,"The encoder only needs 5 to 10 seconds of audio to generate an accurate embedding, any more than this and it just averages out the characteristics of the speaker over time. Try using a shorter clip and see if it the results are still good.

To avoid having to process the file again, you can save the embedding into a numpy file after it is created on [line 192 of toolbox/\_\_init\_\_.py](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/8f71d678d2457dffc4d07b52e75be11433313e15/toolbox/__init__.py#L192). Something like this would work: `np.save(""embed_filename.npy"", embed, allow_pickle=False)`. You would also need to hack a method to load the embed directly from the .npy. One idea is to add .npy as a supported extension for the browse button, and bypass the normal wav and spectrogram processing if the file is .npy.

It's definitely not ideal but it beats waiting for the encoder to process the long files. If you find a better way to do it that's not a hack, please submit a pull request.",need audio generate accurate speaker time try shorter clip see still good avoid process file save file line something like would work embed would also need hack method load embed directly one idea add extension browse button bypass normal spectrogram file definitely ideal waiting process long find better way hack please submit pull request,issue,positive,positive,positive,positive,positive,positive
693477063,"Yes, so for larger datasets you might want to leverage the 'auto select next next' feature for bigger datasets.",yes might want leverage select next next feature bigger,issue,negative,neutral,neutral,neutral,neutral,neutral
693473487,"No, i meant the problem is that I cant load 400 immediately. loading them all takes around 3 hours because they are each around 22 minutes each. I want to be able to save the loaded models so that next time I do not have to load them.",meant problem cant load immediately loading around around want able save loaded next time load,issue,negative,positive,positive,positive,positive,positive
693467089,"> How would someone provide custom audio files if `UserAudio` was removed?

The ""browse"" button in the toolbox opens up a file browser to select any audio file that's available on the computer. Though that is most efficient with small numbers of files and not the 400 files you mentioned in #528.",would someone provide custom audio removed browse button toolbox file browser select audio file available computer though efficient small,issue,negative,positive,neutral,neutral,positive,positive
693462572," > However, the dataset I'm working with is a modest ~400 wav files. Which in total takes around 3 hours to load.

Can you please elaborate? Are you selecting each file individually and clicking the ""load"" button in the toolbox? Please be aware that loading an audio file does not train the model. You get the same result by loading file 400 immediately, and loading files 1 to 400 in order.

The voice similarity from the SV2TTS zero-shot cloning approach is usually quite mediocre. However, it is a really good starting point for training a single-speaker model. If you have a dataset of 400 files you might be interested in my work in #437 to finetune the model to your dataset. This results in much better voice similarity.",however working modest total around load please elaborate file individually load button toolbox please aware loading audio file train model get result loading file immediately loading order voice similarity approach usually quite mediocre however really good starting point training model might interested work model much better voice similarity,issue,positive,positive,positive,positive,positive,positive
693420050,"How would someone provide custom audio files if `UserAudio` was removed? either remove it and add the description on how to do this specifically in the readme, or keep it, add this function, and maybe also update the readme to mention `UserAudio`

The only mention of custom audio implementation currently is this:

> For playing with the toolbox alone, I only recommend downloading LibriSpeech/train-clean-100. Extract the contents as
> <datasets_root>/LibriSpeech/train-clean-100 where <datasets_root> is a directory of your choosing. Other datasets are supported in the toolbox, see here. You're free not to download any dataset, **but then you will need your own data as audio files** or you will have to record it with the toolbox.

which, if I might say, is very vague.

EDIT: I updated the readme in my patch to include this information. And I realize that in commit [41c37b2](https://github.com/CorentinJ/Real-Time-Voice-Cloning/commit/41c37b273fc1c150ceeab0906a6c1282163d9d5b), @CorentinJ  mentions 'modifying the recognized dataset' to include custom audio, but I feel as if it would be nicer to keep standard and custom datasets separate, and that it would also be easier to just allow for the creation of a custom dataset than to modify a standard one.",would someone provide custom audio removed either remove add description specifically keep add function maybe also update mention mention custom audio implementation currently toolbox alone recommend extract content directory choosing toolbox see free need data audio record toolbox might say vague edit patch include information realize commit include custom audio feel would keep standard custom separate would also easier allow creation custom modify standard one,issue,positive,negative,neutral,neutral,negative,negative
693240572,"> btw, i modified a script from adueck a bit. This script will convert video/audio with srt to audio with script for training. I'm not quite sure the format for sv2tts though, but i think u may find it useful if u are trying to get some more date set to train on.
> 
> https://github.com/adueck/split-video-by-srt
> 
> [srt-split.zip](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/5219166/srt-split.zip)

@lawrence124 thanks I shall take a look at it since I might need more data if I cannot find any public dataset",script bit script convert audio script training quite sure format though think may find useful trying get date set train thanks shall take look since might need data find public,issue,positive,positive,positive,positive,positive,positive
693227456,"Hi @ai-are-better-than-humans , thank you for your interest in contributing to the project. I reviewed the commit history and noticed that UserAudio is added in ec9658aad21178c90c5f673778c2c0051e33ad35 and then removed shortly thereafter in 41c37b273fc1c150ceeab0906a6c1282163d9d5b . It seems @CorentinJ forgot to update the documentation in demo_toolbox.py along with the second commit.

https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/8f71d678d2457dffc4d07b52e75be11433313e15/demo_toolbox.py#L14-L18

My recommendation is to remove the mention of UserAudio in demo_toolbox.py, instead of adding the feature back in. What do you think?",hi thank interest project commit history added removed shortly thereafter forgot update documentation along second commit recommendation remove mention instead feature back think,issue,positive,neutral,neutral,neutral,neutral,neutral
692697951,"@blue-fish yea, as with other data analysis....getting the good/clean dataset is always difficult.  (the prelim result of adding youtube clips is not good)

[20200915-204053_melgan_10240ms.zip](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/5224938/20200915-204053_melgan_10240ms.zip)

This is an example of using ""mandarin + cantonese"" as synthesizer, along with Melgan vocoder. I dont know if it is my ear or not, i dont really like the Griffin-Lim from zhrtvc, it has the ""robotic"" noise in the background. 

btw, seems like u are updating the synthesizer of sv2tts ?? the backbone is still tacotron ??  ",yea data analysis getting always difficult prelim result clip good example mandarin synthesizer along dont know ear dont really like noise background like synthesizer backbone still,issue,negative,positive,positive,positive,positive,positive
692644595,"@lawrence124 That website demo uses an different algorithm that probably does not involve machine learning. It sounds like a concatenative method of synthesis where prerecorded sounds are joined together. Listening closely, it is unnatural and obviously computer-generated. To their credit, they do use high-quality audio samples to build the output.

Here's a [wav of the demo text](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/5224362/zhrtvc_wav.zip) synthesized by zhrtvc, using Griffin-Lim as the vocoder. Tacotron speech flows a lot more smoothly than their demo. zhrtvc could sound better than the demo TTS if 1) it is trained on higher quality audio, and 2) a properly configured vocoder is available.",different algorithm probably involve machine learning like method synthesis together listening closely unnatural obviously credit use audio build output text speech lot smoothly could sound better trained higher quality audio properly available,issue,positive,positive,positive,positive,positive,positive
692539870,"@blue-fish 

would like to ask a rather random question...have u tried using the demo TTS from https://www.readspeaker.com/ ?? 

from my point of view, the result in Chinese/Cantonese is pretty good and i would like to discuss...is that their proprietary algorithm is simply superior ?? or they simply has the resources to build a better dataset to train on ?? 

based on the job description, what they are doing is not too different from tacotron / sv2tts

https://www.isca-speech.org/iscapad/iscapad.php?module=article&id=17363&back=p,250 ",would like ask rather random question tried point view result pretty good would like discus proprietary algorithm simply superior simply build better train based job description different,issue,positive,positive,positive,positive,positive,positive
692171306,"> @ranshaa05 I'll gladly work this when the pytorch PR, bugs and dependency issues are complete. Or possibly earlier if I have a free moment.
> 
> Since it could be a while I'll flag this as an issue for others to help out.

thanks! :)",gladly work dependency complete possibly free moment since could flag issue help thanks,issue,positive,positive,positive,positive,positive,positive
692105519,"btw, i modified a script from adueck a bit. This script will convert video/audio with srt to audio with script for training. I'm not quite sure the format for sv2tts though, but i think u may find it useful if u are trying to get some more date set to train on. 

https://github.com/adueck/split-video-by-srt 


[srt-split.zip](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/5219166/srt-split.zip)
",script bit script convert audio script training quite sure format though think may find useful trying get date set train,issue,positive,positive,positive,positive,positive,positive
691942732,"Once more, my bad... 
I forgot that during these predictions, my model was in train mode so all dropouts were active (and not only the prenet’s dropout) so it can explain why they are so bad especially for inference

Actually the model is trained on 5 epochs on 1k batches with encoder-part non-trainable and has a loss around 0.565
Now i try making a prediction in evaluation mode and it’s really better ! 
Really not perfect, noisy, metallic but the attention is interesting, the voice seems to be similar to the target and some words (and also sentences) are intelligible so really much interesting than before !
And on prediction (with teacher-forcing) in this mode, the result is really good for a majority 

Now i relaunch a training for 4 epochs on 1.5k batches with encoder trainable and will see ! I hope the loss will decreases below 0.5, if so, results should be really interesting",bad forgot model train mode active dropout explain bad especially inference actually model trained loss around try making prediction evaluation mode really better really perfect noisy metallic attention interesting voice similar target also intelligible really much interesting prediction mode result really good majority relaunch training trainable see hope loss really interesting,issue,positive,positive,positive,positive,positive,positive
691892507,"Okay, thanks @lawrence124 ! Seems like using the pretrained encoder is good to go for now. ",thanks like good go,issue,positive,positive,positive,positive,positive,positive
691884289,"i'm using the pretrained encoder from Kuangdd, but according to the file size and date...seems like it is the same as the pretrained encoder from here",according file size date like,issue,negative,neutral,neutral,neutral,neutral,neutral
691844225,"@blue-fish Thanks for your fast response, will try this :)",thanks fast response try,issue,negative,positive,positive,positive,positive,positive
691837318,"Hi @rlutsyshyn, you don't need to use the same datasets for synth and vocoder. You can preprocess a different 48khz dataset (even English) and it should generalize to Ukrainian if it has enough voices (several hundred or more). Use `synthesizer_preprocess_audio.py` , then copy `SV2TTS/synthesizer/mels` to `SV2TTS/vocoder/mels_gta` and `SV2TTS/synthesizer/train.txt` to `SV2TTS/vocoder/synthesized.txt`.

The downside to this approach is your trained vocoder will not compensate for any deficiencies of your synthesizer model. It is a missed opportunity to make the final output better.

1. You can continue to use Corentin's pretrained vocoder if your synthesizer hparams satisfy the following conditions.
    * num_mels = 80
    * (hop_size / sample_rate) = 0.0125
    * win_size = hop_size * 4
    * fmin = 55 and fmax = 7600

For proper vocoder inference, you either need to edit `synthesizer/hparams.py` or `vocoder/hparams.py` to set hop_size, win_size, and sample_rate to the old values (200, 800, 16khz). I don't know if it matters but you may also want to set `n_fft=800`. The toolbox uses the synthesizer's sampling rate, so easier to edit that hparams file (otherwise you need to resample the wav after getting it back from the vocoder).

The reason this works is because the vocoder just sees a 2d array of shape (num_mels, frames) as input. There is no sample rate information contained in the mel spectrogram. You can even go the other direction, and take a synthesizer trained at 16khz and use the mels on a vocoder trained at 24 khz :)

2. I've personally tried `(4, 4, 4, 4)` for 256, and `(5, 6, 10)` for 300 and the results were good. Have not read the WaveRNN paper so I don't know how to select the upsampling factor. Maybe try `(4, 5, 5, 6)` for 600? An extra element does not add that many trainable parameters, or affect inference speed significantly. ",hi need use different even generalize enough several hundred use copy downside approach trained compensate synthesizer model opportunity make final output better continue use synthesizer satisfy following proper inference either need edit set old know may also want set toolbox synthesizer sampling rate easier edit file otherwise need resample getting back reason work array shape input sample rate information mel spectrogram even go direction take synthesizer trained use trained personally tried good read paper know select factor maybe try extra element add many trainable affect inference speed significantly,issue,positive,positive,positive,positive,positive,positive
691822722,"Thank for your response @blue-fish , but for training vocoder I need a lot of 48kHz data, what could be a problem. By the way:
1. If I want to fine tune voice clonner on 22 or 24 kHz  I also need to retrain vocoder from scratch?
2. 5 * 5 * 8 = 200 but how can I split it for 600? 5 * 5 * 24 ? or smthing else? ",thank response training need lot data could problem way want fine tune voice also need retrain scratch split else,issue,negative,positive,positive,positive,positive,positive
691811046,@lawrence124 Glad to see your results! Did you have to train the encoder from scratch? Or using the pre-trained decoder/synthesizer worked for you?,glad see train scratch worked,issue,negative,positive,positive,positive,positive,positive
691810258,Thanks @blue-fish I went through the issues you mentioned. You gave me a good amount of resources for a start. Much appreciated!,thanks went gave good amount start much,issue,positive,positive,positive,positive,positive,positive
691717931,"Finally i deleted the model and retrain it with trimming during all training process because the old made too much attention on padding character and it makes training slower (i think) and inference more bad than the prediction

Another strange thing is that when i make the encoder part trainable, the inference seems to be less good than before (loss still decreases but the resulting plots (made on validation) are bad (more than before))

I retrain it on 5 epochs on 1k samples with the encoder part non-trainable and actually at epoch 2 step 200, loss is 0.61 so really better than before (for this step) ! (my best loss before was around 0.55) Will see tomorrow if inference is interesting or not 
My principal problem is that i don’t have enough time / power / GPU memory to train it on enough steps... Hopefully transfer-learning helps a lot to safe time and with the new approach it’s really more powerful with transfert-learning",finally model retrain trimming training process old made much attention padding character training think inference bad prediction another strange thing make part trainable inference le good loss still resulting made validation bad retrain part actually epoch step loss really better step best loss around see tomorrow inference interesting principal problem enough time power memory train enough hopefully lot safe time new approach really powerful,issue,positive,positive,positive,positive,positive,positive
691716301,"To see progress on this approach and results of Tacotron-2 using this siamese encoder, you can see the #507 where i describe my training results and procedure with a tf2.0 implementation of the synthesizer (slightly different from the Tacotron of this repo)",see progress approach see describe training procedure implementation synthesizer slightly different,issue,negative,neutral,neutral,neutral,neutral,neutral
691562724,"@lawrence124 Interesting, thanks for sharing that result! Occasionally the model fails to learn attention, you might try restarting the training from scratch with a different random seed. It might also help to trim the starting and ending silences. If your data is at 16 kHz then webrtcvad can do that for you (see the trim_long_silences function in encoder/audio.py).",interesting thanks result occasionally model learn attention might try training scratch different random seed might also help trim starting ending data see function,issue,positive,positive,neutral,neutral,positive,positive
691561579,"@lawrence124 A vocoder trained on enough speakers can generalize to unseen speakers and even other languages. https://github.com/mozilla/TTS/issues/221#issuecomment-506269011

The vocoders work within very narrow parameters and will fail if input does not meet the specification. To avoid incompatibility, the vocoder in this repo gets some of its hparams from the synthesizer. The relevant parameters are:
* sample_rate
* num_mels
* n_fft, hop_length, window_length
* fmin, fmax
* pre-emphasize

With default settings, I got very mediocre results on zhrtvc and I suspect that some of the vocoder parameters are not set correctly. Griffin-Lim performed the best. You should try this repo with the English models, the pretrained WaveRNN works quite well.",trained enough generalize unseen even work within narrow fail input meet specification avoid incompatibility synthesizer relevant default got mediocre suspect set correctly best try work quite well,issue,negative,positive,neutral,neutral,positive,positive
691504554,"@blue-fish 
i have not tried the waveRNN vocoder on RTVC, but the pretrained waveRNN vocoder can't give me anything useful. The melgan multi speaker (or other melgan vocoder) yield far better result. I'm not too sure why though.  (do we need to train vocoder for a specific language ??)",tried ca give anything useful speaker yield far better result sure though need train specific language,issue,positive,positive,positive,positive,positive,positive
691503557,"@thehetpandya 

I'm working on a forked version of sv2tts to train local dialect of chinese. Using the dataset from Common Voice (about 22k of utterances) , i couldn't get the data to converge. But if I add the local dialect on top of a pre-trained model (the main dialect of chinese), seems like the result is actually quite good.  Fyi, the local dialect and the main dialect have different, but similar alphabet romanization system (for example, the main has 4 tones, but the local dialect has 8)

using common voice data only:
![image](https://user-images.githubusercontent.com/29505986/92998458-7061d200-f54c-11ea-910d-d96bd952a4a5.png)

using pre-trained and then add local dataset:
![image](https://user-images.githubusercontent.com/29505986/92998277-76a37e80-f54b-11ea-8b23-8254a6b5b539.png)

@blue-fish 
not sure if i'm abusing the model, but at least it works :)
",working forked version train local dialect common voice could get data converge add local dialect top model main dialect like result actually quite good local dialect main dialect different similar alphabet system example main local dialect common voice data image add local image sure model least work,issue,positive,positive,neutral,neutral,positive,positive
691470100,"Here are plots of inference and prediction at step 6.5k
The resulting audio is quite good for prediction but not really intelligible in inference (only 1 / 4 i can understand something) but it's not bad for only 6.5k steps ! 
I think tomorrow after 3k steps more the result can be really interesting, i hope !

Inference : 

![pred_step-006500_3_infer](https://user-images.githubusercontent.com/45139111/92994109-b476b780-f4f7-11ea-85bc-50dfa48afddc.png)

Prediction : 

![pred_step-006500_3_pred](https://user-images.githubusercontent.com/45139111/92994112-b5a7e480-f4f7-11ea-9f33-283305e617bb.png)

Training history : 

![training_hist](https://user-images.githubusercontent.com/45139111/92994151-fe5f9d80-f4f7-11ea-907d-bc0c4e937b5b.png)

Edit : oups, my text on plots are in white so not readable, i have to find how to put the background of the whole plot in black and not only background of graph... i will test it for next plots !
",inference prediction step resulting audio quite good prediction really intelligible inference understand something bad think tomorrow result really interesting hope inference prediction training history edit text white readable find put background whole plot black background graph test next,issue,positive,positive,neutral,neutral,positive,positive
691287217,"Yes i think but it’s a model from torch hub so not mine and i don’t really know pytorch so i am waiting to find a tensorflow implementation of waveglow to convert weights (like the tacotron) but not found yet...

Now step 4k it’s really so funny, before for all inference the attention were only on padded-character and now i have an inference as in prediction ! with a plateau at the beginning and only after some frames, an increasing attention 😂
It’s so fun, i have a lazy model x) he has to wait some frames before prediction x)
Now i want to hear the result ! the training will ends only tomorrow 11 am i think, it will be interesting i think !
But if it continue the same way, it can be really interesting because now prediction seems to be good and inference seems to become interesting and this only after 4k steps ! (around 20h training with only 15 frames / optimization step so it’s around 17sec/batch)

Edit : i stopped training and continue with a hard trimming (i trim all frames that doesn’t have any value higher than min_val + (0.8 * (max_val - min_val)) where min and max val are the min and max of the whole spectrogram and with that trimming, i saw that the generated audio is the same as the original but with the additionnal « tic » that i can easily remove by removing the first x ms of the audio 
When stopping the training, the loss was decreasing from 0.62 to 0.605 approximately and now, after only 200 steps, the loss is at 0.573 so really promising i find !

Edit 2 : i have my first prediction with trimming and result seems to be really interesting, attention is as expected (linearly increasing) and inference seems similar for 3 / 4 of predicted text so now i want to hear the resulting audio but it will wait the end of the training (in 2 or 3 hours)
The loss is actually at 0.561 which is quite good too so i am really optimistic about this approach ! ",yes think model torch hub mine really know waiting find implementation convert like found yet step really funny inference attention inference prediction plateau beginning increasing attention fun lazy model wait prediction want hear result training tomorrow think interesting think continue way really interesting prediction good inference become interesting around training optimization step around edit stopped training continue hard trimming trim value higher min min whole spectrogram trimming saw audio original tic easily remove removing first audio stopping training loss decreasing approximately loss really promising find edit first prediction trimming result really interesting attention linearly increasing inference similar text want hear resulting audio wait end training loss actually quite good really optimistic approach,issue,positive,positive,positive,positive,positive,positive
691252906,"> i saw that too much trimming, the vocoder creates a « tic » noise at the beginning (like when you open your mic) and to avoid that, i trim not really hard so there is « dark frames » at the beginning of the spectrograms

You should be able to remove that sound artifact by training a new vocoder or finetuning your vocoder model. Nice update!",saw much trimming tic noise beginning like open avoid trim really hard dark beginning able remove sound artifact training new model nice update,issue,negative,positive,positive,positive,positive,positive
691232686,"Finally my old single speaker model doesn’t work because i have only 1 hour of training data (around 1k samples) and it’s not enough
So i re-test my approach (with the Add layer instead of a Concatenate) with my embeddings (siamese network) and have really fun results at step 3.5k

So my training procedure is as follow : 
- 5 epochs on 500 batches (size 64) with all encoder layers non-trainable (except the additional Dense layer that expands the 64-embeddings to a 512 vector to match the encoder size)
- 5 epochs on 1000 batches (also size 64) (still training)

Note : i use this approach because i used transfer-learning with a pretrained model trained on the same language so my intuition is that an encoder should be universal (because same language) and then fine-tune the decoder at first is more interesting like that it learns only to generalyse to multi-speaker instead of re-learning to encode text
And the intuition seems to be correct because loss decreases really fast and achieves a 0.64 score after the 5 epochs which is really not bad because the final loss of the pretrained model used for transfer-learning was around 0.35 (with same loss calculation, trained on 8 epochs on 8k single-speaker samples)

And an interesting thing is that the avg loss of the first 50 batches on the 1000 new samples (because i think they are not the same of the 500 first samples because of shuffling) was 0.625 so it supposes that the model is good on multi-speaker generalization !

At step 3.5k (so the 1st epoch on the 1000 batches), i have a really funny plot of spectrogram (i will post them tomorrow) but as my audios have some silence at the beginning, i wanted to trim them but after some experiment, i saw that too much trimming, the vocoder creates a « tic » noise at the beginning (like when you open your mic) and to avoid that, i trim not really hard so there is « dark frames » at the beginning of the spectrograms
And during these « dark frames », the model makes attention at a « padding character » so the attention is linearly increasing (like it should be) but not at the beginning but only when these « dark frames » are over and during these frames the attention is where the attention ends at the end of the spectrogram ! 

It’s really fun but i think it’s the cause of my bad inference so if after the 5 epochs the inference is still bad, i will trim harder and hope to have a classic linearly increasing attention beginning at the beginning of the spectrograms and to remove the « tic » noise i can simply cut a part of the audio generated",finally old single speaker model work hour training data around enough approach add layer instead concatenate network really fun step training procedure follow size except additional dense layer vector match size also size still training note use approach used model trained language intuition universal language first interesting like instead encode text intuition correct loss really fast score really bad final loss model used around loss calculation trained interesting thing loss first new think first shuffling model good generalization step st epoch really funny plot spectrogram post tomorrow silence beginning trim experiment saw much trimming tic noise beginning like open avoid trim really hard dark beginning dark model attention padding character attention linearly increasing like beginning dark attention attention end spectrogram really fun think cause bad inference inference still bad trim harder hope classic linearly increasing attention beginning beginning remove tic noise simply cut part audio,issue,positive,positive,neutral,neutral,positive,positive
691224523,Mozilla TTS is also developing a speaker encoder in https://github.com/mozilla/TTS/issues/512. I am inviting Mozilla TTS contributors to this discussion to see if we can decide on a common model structure. Also share thoughts on datasets and preprocessing techniques. In a best case situation we could even share the model.,also speaker inviting discussion see decide common model structure also share best case situation could even share model,issue,positive,positive,positive,positive,positive,positive
691220702,"@ranshaa05 I'll gladly work this when the pytorch PR, bugs and dependency issues are complete. Or possibly earlier if I have a free moment.

Since it could be a while I'll flag this as an issue for others to help out.",gladly work dependency complete possibly free moment since could flag issue help,issue,positive,positive,positive,positive,positive,positive
691217788,"With that last change the code revisions are complete, just need to train a model. My GPU is tied up performing other experiments for the next week, then I'll get back to it. I'll request another code review when testing is done and a good pretrained model is available.",last change code complete need train model tied next week get back request another code review testing done good model available,issue,negative,positive,positive,positive,positive,positive
691214989,"@rlutsyshyn You need to train a vocoder from scratch, the good news is that it trains relatively fast and you should only need to do it once. Most people choose sampling rates of 22.05 or 24 kHz for faster inference but that's your call.

In synthesizer hparams, you should modify `hop_length` to be 0.0125 * sample_rate , and `win_length` and `n_fft` to be 4 times that number. The vocoder automatically picks up those hparams from the synthesizer. You'll also need to edit the upsampling factors in this line of code, to match your new hop length. For example, 5\*5\*8 = 200 (the default hop length for 16 kHz).

https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/8f71d678d2457dffc4d07b52e75be11433313e15/vocoder/hparams.py#L26

When preprocessing data, the `fmax` can be adjusted. You can go as high as 0.5*sample rate (the [Nyquist rate](https://en.wikipedia.org/wiki/Nyquist_rate)). Higher is not necessarily better, because we only have 80 mel channels and each channel needs to represent a wider range of frequencies. If you don't want to experiment, it is safe to leave fmax untouched at 7600 Hz.
",need train scratch good news relatively fast need people choose sampling faster inference call synthesizer modify time number automatically synthesizer also need edit line code match new hop length example default hop length data go high sample rate rate higher necessarily better mel channel need represent range want experiment safe leave untouched,issue,positive,positive,positive,positive,positive,positive
691206288,"I found that the alignments are more robust when we don't project the encoder output when computing the attention scores. With commit https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/472/commits/39c5943275f2a57009962e3689d58bcfdd9ec648 there is much less quitting during inference.

I still think the speaker embedding should be excluded from the inputs to the attention mechanism. This would be a difference from the SV2TTS paper, but if it completely solves the quitting problem then I am all for it.",found robust project output attention commit much le inference still think speaker attention mechanism would difference paper completely problem,issue,negative,positive,positive,positive,positive,positive
691172107,"I agree with that suggestion. Encoder training requires a lot of data, time and effort. You can see #126 and #458 to get an idea. If your results are good enough without it, best to avoid that hassle.",agree suggestion training lot data time effort see get idea good enough without best avoid hassle,issue,negative,positive,positive,positive,positive,positive
691163160,"> VoxCeleb is rejected due to it's horrible quality.

The idea is to have a dataset with low quality though
",due horrible quality idea low quality though,issue,negative,negative,negative,negative,negative,negative
691160915,Anyone who wants to contribute in some way to the RTVC project is welcome to join the Slack. Leave a comment in #474 and we will provide an invite link.,anyone contribute way project welcome join slack leave comment provide invite link,issue,positive,positive,positive,positive,positive,positive
691086998,"Hi, we are making a group effort on building a new dataset curated and cleaned, quality over quantity.

VoxCeleb is rejected due to it's horrible quality.
VCTK might eventually have some bits in it.
CommonVoice  will be part of it.
LibriTTS 100 / 360 / 500 will mostly be the base. (1st iteration)

Join the Slack for more info.",hi making group effort building new quality quantity due horrible quality might eventually part mostly base st iteration join slack,issue,negative,negative,negative,negative,negative,negative
690997680,"Honnestly, i don’t know, i think blue-fish can help you better for this
If the audio only seems to go to fast but seems good, it can only be a problem with the audio player rate and the no matter the rate of spectrogram for the vocoder (because i don’t know if it changes something for the vocoder if the spectrogram is a 16khz or 48khoz)
So you could search where the toolbox uses something like sounddevice.play (sd.play) or something like that
You could also check when the vocoder generates an audio and play it yourself with 48khz parameter (with IPython.display.Audio for example if you use jupyter notebook)",know think help better audio go fast good problem audio player rate matter rate spectrogram know something spectrogram could search toolbox something like something like could also check audio play parameter example use notebook,issue,positive,positive,positive,positive,positive,positive
690994672,"@Ananas120 For synthesizer in hparams.py I can modify win_size, hop... etc, but in vocoder/hparams.py I don't see something like that, so waht sould I modify to fine tune my vocoder for 48kHz data?
Thnx :)",ananas synthesizer modify hop see something like modify fine tune data,issue,positive,positive,positive,positive,positive,positive
690969027,"@blue-fish Thanks a lot for the response! Yes, I have begun exploring the issues for now for a better understanding of the workflow before beginning with the training process. 

I also read on [#492 (comment) ](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/492#issuecomment-673670822) that beginning with training the synthesizer would be a good start and then only if the encoder doesn't seem to give proper results, one can proceed with training/fine-tuning the encoder. Does that same apply in case for a totally different language too, like in my case i.e. hindi?",thanks lot response yes begun exploring better understanding beginning training process also read comment beginning training synthesizer would good start seem give proper one proceed apply case totally different language like case,issue,positive,positive,positive,positive,positive,positive
690967463,"Hey guys, just went through this thread quickly.

I indeed removed the ReLU layer in the voice encoder we use at Resemble.AI. I think the model on Resemblyzer still has it. I planned to release a new one which, among other things, wouldn't be trained on data that would have silences at the start and end of each clip.

I don't think I'll update the code in this repo, but I should update the code on Resemblyzer when the new model's released.",hey went thread quickly indeed removed layer voice use think model still release new one among would trained data would start end clip think update code update code new model,issue,negative,positive,positive,positive,positive,positive
690948762,"Hi @thehetpandya , please start by reading this: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/431#issuecomment-673555684

It is not possible to finetune the English model to another language. A new model needs to be trained from scratch. This is because the model relates the letters of the alphabet to their associated sounds, so what the model knows about English does not transfer over to Hindi. At a minimum, you will need a total of 50 hours of transcribed speech from at least 100 speakers. For a better model get 10 times this number.

This is what you need to do. Good luck and have fun!
1. Replicate the training of the English synthesizer to learn how to use the data processing and training scripts.
    * I have no idea how to do this with Google colab, but it should be possible.
2. Assemble and preprocess your dataset
3. Train a synthesizer model
4. Troubleshoot problems with the model.
5. Repeat steps 3 and 4 until satisfied",hi please start reading possible model another language new model need trained scratch model alphabet associated model transfer minimum need total speech least better model get time number need good luck fun replicate training synthesizer learn use data training idea possible assemble train synthesizer model model repeat satisfied,issue,positive,positive,positive,positive,positive,positive
690905464,"Just to be sure, if you train the synthesizer to create 48khz melspectrogram, you should also train the vocoder to generate 48khz audio (because it’s trained on 16khz audio)
Also you should check if the parameters for the audio player etc are well modified according your 48khz rate

Good luck !",sure train synthesizer create also train generate audio trained audio also check audio player well according rate good luck,issue,positive,positive,positive,positive,positive,positive
690865772,"Hi @EthanZoneCoding , I take it you are running the PyTorch synthesizer from #472? The pretrained model in `logs-pretrained` is Tensorflow-based and will not work.

Take the file linked in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/449#issuecomment-674420484 and save it as `synthesizer/saved_models/pretrained/pretrained.pt` It is not a very good model. I've made some progress towards better models but still trying to improve the similarity and quality of the output voices.

You will get a better result with the toolbox if you use Corentin's repo and set up tensorflow, but it can be hard on some systems.",hi take running synthesizer model work take file linked save good model made progress towards better still trying improve similarity quality output get better result toolbox use set hard,issue,positive,positive,positive,positive,positive,positive
690715955,You can just install Python <= 3.7 and don't have any problems or can refactored code to get transfer from tf.v.1.X to v.2.X. In my opinion is more efficient to create a new virtual env with python 3.7... ,install python code get transfer opinion efficient create new virtual python,issue,positive,positive,positive,positive,positive,positive
690713269,"I have Python 3.8.5
I think that tensorflow version 1.15 was removed.

""pip install tensorflow==1.15""
I can install tensorflow > 2.2 but not 1.15

If there are some alternatives to build project with version >2, that would be fantastic or others ways to install tensorflow 1.15.",python think version removed pip install install build project version would fantastic way install,issue,positive,positive,positive,positive,positive,positive
690707613,"@blue-fish have issue with synthesizer now :) I mean, that when I use 48kHz audio and calculate parameters in synthesizer/hparams.py - after fine tuning my voice is like in Alvin and the Chipmunks (very very fast) ... mb you have some advices on this case? 
What are the main parameters to configure to have normal voice in the output?",issue synthesizer mean use audio calculate fine tuning voice like fast case main configure normal voice output,issue,positive,positive,positive,positive,positive,positive
690685399,"Closing issue due to inactivity, please reopen and provide additional information if this is still an issue.",issue due inactivity please reopen provide additional information still issue,issue,negative,negative,negative,negative,negative,negative
690685052,"Closing due to inactivity. Hope this has been resolved, if not go ahead and reopen the issue.",due inactivity hope resolved go ahead reopen issue,issue,positive,negative,negative,negative,negative,negative
689979385,"Thank you for sharing the zhrtvc pretrained models @windht ! It will not be as obvious in the future, so for anyone else who wants to try, the models work flawlessly with this commit: https://github.com/KuangDD/zhrtvc/tree/932d6e334c54513b949fea2923e577daf292b44e

What I like about zhrtvc:
* Display alignments for synthesized spectrograms
* Option to preprocess wavs for making the speaker embedding.
* Auto-save generated wavs (though I prefer our solution in #402)

Melgan is integrated but it doesn't work well with the default synthesizer model, so I ended up using Griffin-Lim most of the time for testing. WaveRNN quality is not that good either so it might be an issue on my end.

I'm trying to come up with ideas for this repo to support other languages without having to edit files.",thank obvious future anyone else try work flawlessly commit like display option making speaker though prefer solution work well default synthesizer model ended time testing quality good either might issue end trying come support without edit,issue,positive,positive,positive,positive,positive,positive
689799718,"@blue-fish 
I made an upload to google drive from @KuangDD 's baidu links download.

Zhrtvc Sample data:
https://drive.google.com/file/d/1qjaS8TbRhUSToW1uSFgZfdqsFb0MYyz4/view?usp=sharing

Zhrtvc Sample models:
https://drive.google.com/file/d/1QU7LhehlMzFE8f-B6y9E7Hw8bd_AOl5H/view?usp=sharing",made drive link sample data sample,issue,negative,neutral,neutral,neutral,neutral,neutral
688638589,"> Our encoder output (size 256) is concat with the speaker embedding (size 256) as it goes into the decoder, which sees an input of size 512. The decoder prenet projects this down to 256, then 128 dimensions before passing it to the attention module. That might work for single-speaker (256 --> 128) but it's too much of an information bottleneck for voice cloning (512 --> 128). I'll increase the second linear projection to 256 to match this repo, and put it in hparams so it is not hardcoded.

That actually made things worse. The information bottleneck is needed for the model to learn attention.",output size speaker size go input size passing attention module might work much information bottleneck voice increase second linear projection match put actually made worse information bottleneck model learn attention,issue,negative,negative,neutral,neutral,negative,negative
687942132,"@steven850 I need to take some time to decide on the specific parameters for the vocoder, I have in mind to make the parameters for 22,050 Hz compatible with some other vocoders, like WaveGlow, MelGAN or Parallel WaveGAN. If you have any thoughts or ideas on this let me know.

I invited you to a repo with a link to the slack that I've set up for this. I've also been using that to communicate with mbdash on encoder training.",steven need take time decide specific mind make compatible like parallel let know link slack set also communicate training,issue,negative,neutral,neutral,neutral,neutral,neutral
687762124,"@blue-fish I grabbed the datasets. I also still have the datasets that corentin trained on originally. 
what about a discord server?",also still trained originally discord server,issue,negative,positive,positive,positive,positive,positive
687750391,"> The toolbox is not intended to be a full-featured text-to-speech solution, but I can understand the desire for this capability since the output volume is not consistent for different speakers.
> 
> Are you able to develop this feature and submit a pull request? Or are you asking for someone else to take on this task?

Unfortunately i am not skilled enough in python to develop this feature, so yeah, im asking for somebody else to do it.",toolbox intended solution understand desire capability since output volume consistent different able develop feature submit pull request someone else take task unfortunately skilled enough python develop feature yeah somebody else,issue,positive,positive,positive,positive,positive,positive
687743843,"When inspecting the TFTTS tacotron-2 model, i found something really interesting that i’m now trying and it seems to be really interesting !

Instead of concatenating the encoder embeddings with the speaker embedding, i « expand » the speaker embedding with a Dense layer and i sum them so with this approach, the only new weights are from the new Dense layer but all attention and encore are the same so transfert-learning is more powerful here and attention seems to be interesting really fast !

Actually i’m only at step 4 with 1k batchs per epoch so not enough to have good results (loss is around 1.3) but i think it’s the most promising approach to use transfert-learning and have fast results 

I will continue this approach for 1 or 2 days but after that, i need to finish my initial project and then i will reuse my old single-speaker model but if anyone can try it more, it can be really interesting
@blue-fish  maybe you can test it with your pytorch approach if you find a pretrained pytorch Tacotron-2 (like Mozilla or NVIDIA) ? ",model found something really interesting trying really interesting instead speaker expand speaker dense layer sum approach new new dense layer attention encore powerful attention interesting really fast actually step per epoch enough good loss around think promising approach use fast continue approach day need finish initial project reuse old model anyone try really interesting maybe test approach find like,issue,positive,positive,positive,positive,positive,positive
687741693,"Dear @mbdash , any updates? If you find the time to share the current model, it'd be much appreciated! :)",dear find time share current model much,issue,positive,positive,neutral,neutral,positive,positive
687726687,"This might help? https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/494#issuecomment-674123036

> Please make sure the files can be found at the locations that the toolbox is expecting.
> 
> ```
> enc_model_fpath: encoder\saved_models\pretrained.pt
> syn_model_dir: synthesizer\saved_models\logs-pretrained
> voc_model_fpath: vocoder\saved_models\pretrained\pretrained.pt
> ```
> 
> If the files are in the correct location then make sure you are running python out of the Real-Time-Voice-Cloning folder since it is evaluating those paths relative to the location from which python is launched.

If this is not helpful then please describe what you are trying to do. Add an alternative pretrained model and use it with the toolbox?",might help please make sure found toolbox correct location make sure running python folder since relative location python helpful please describe trying add alternative model use toolbox,issue,positive,positive,positive,positive,positive,positive
687725845,@ZorvShadow Please provide an update. I recommend downloading the [`423_add_cpu_mode`](https://github.com/blue-fish/Real-Time-Voice-Cloning/tree/423_add_cpu_mode) branch of my fork and running the toolbox on CPU instead of GPU.,please provide update recommend branch fork running toolbox instead,issue,positive,neutral,neutral,neutral,neutral,neutral
687725548,"The toolbox is not intended to be a full-featured text-to-speech solution, but I can understand the desire for this capability since the output volume is not consistent for different speakers.

Are you able to develop this feature and submit a pull request? Or are you asking for someone else to take on this task?",toolbox intended solution understand desire capability since output volume consistent different able develop feature submit pull request someone else take task,issue,positive,positive,positive,positive,positive,positive
687725094,"> I'm still stymied by the inability to create custom datasets from scratch. Are you still working on this ""custom dataset"" tool that you mention here?

For those recording their own utterances, this is a useful tool: https://github.com/MycroftAI/mimic-recording-studio",still inability create custom scratch still working custom tool mention recording useful tool,issue,negative,positive,positive,positive,positive,positive
687722580,"Hi @steven850 , thank you for offering to contribute your time and hardware! Please download the LibriTTS train-clean-100 and train-clean-360 datasets (available at https://openslr.org/60 ) and we'll put your amazing CPU to work. I have in mind training a new vocoder model with higher sampling rate (22,050 Hz instead of the current 16,000 Hz). Should have the logistics for communication figured out shortly.",hi steven thank offering contribute time hardware please available put amazing work mind training new model higher sampling rate instead current logistics communication figured shortly,issue,positive,positive,positive,positive,positive,positive
687674744,"Hi blue-fish I see that you are actively working on this repo, I would love to help with testing and training. I can offer my 3990x to do some heavy lifting. How can i get in touch? ",hi see actively working would love help testing training offer heavy lifting get touch,issue,positive,positive,neutral,neutral,positive,positive
687662497,"> @ranshaa05 Interesting suggestion. Have you tried selecting the ""enhance vocoder output"" button on the toolbox UI? We added that to trim silences, but it also normalizes the volume of the generated wav.

It doesn't work for me at the moment (looks like some cudnn shannanigans) but i do remember it making the output a bit louder, but it certainly wasnt perfect. Even the ""normalize"" option in audacity doesnt work 100% of the time. That's why im suggesting a simple volume bar to have a bit more control over the actual volume.",interesting suggestion tried enhance output button toolbox added trim also volume work moment like remember making output bit certainly wasnt perfect even normalize option audacity doesnt work time suggesting simple volume bar bit control actual volume,issue,positive,positive,positive,positive,positive,positive
687651582,"### Samples and brief discussion of postnet

Voice quality is similar to the current synthesizer using 256 postnet_dims, but 512 has the potential for more depth. In this [**samples.zip**](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/5178474/samples.zip) you will find a bunch of pytorch samples at 256, and one at 512. The tensorflow samples are generated with the original pretrained models using default hparams. Tensorflow has somewhat better voice similarity, but the pytorch model only trained for 6 hours on a basic GPU so it's not a fair comparison.

Can confirm the prosody issue (frequent pauses for certain speaker embeddings) goes away when I train on a curated subset of LibriSpeech with no silences.

### An explanation for premature stop to inference?

The premature stop to inference is bugging me. Since no one else opened an issue for it in fatchord's repo, I started troubleshooting and think this is responsible:

https://github.com/blue-fish/Real-Time-Voice-Cloning/blob/14c3646e1737d0a71b7f12077fba38dcd1eee4ef/synthesizer/models/tacotron.py#L167

Our encoder output (size 256) is concat with the speaker embedding (size 256) as it goes into the decoder, which sees an input of size 512. The decoder prenet projects this down to 256, then 128 dimensions before passing it to the attention module. That might work for single-speaker (256 --> 128) but it's too much of an information bottleneck for voice cloning (512 --> 128). I'll increase the second linear projection to 256 to match this repo, and put it in hparams so it is not hardcoded.

We can also try leaving out the speaker embedding when calculating attention scores, since those should not depend on the speaker.",brief discussion voice quality similar current synthesizer potential depth find bunch one original default somewhat better voice similarity model trained basic fair comparison confirm prosody issue frequent certain speaker go away train subset explanation premature stop inference premature stop inference since one else issue think responsible output size speaker size go input size passing attention module might work much information bottleneck voice increase second linear projection match put also try leaving speaker calculating attention since depend speaker,issue,positive,positive,positive,positive,positive,positive
687629111,"@ranshaa05 Interesting suggestion. Have you tried selecting the ""enhance vocoder output"" button on the toolbox UI? We added that to trim silences, but it also normalizes the volume of the generated wav.",interesting suggestion tried enhance output button toolbox added trim also volume,issue,negative,positive,positive,positive,positive,positive
687593763,"Hey, I have new issue while tried to run vocoder_preprocess. Preprocessing ""starts"" but it had 0 iterations (without any error)
I have datasets/SV2TTS/vocoder/mels_gta but it is empty and datasets/SV2TTS/vocoder/synthesized.txt is also empty...
Mb I missed something? I just fine tune pretrained model on my own data (with synthesizer there was no problems)",hey new issue tried run without error empty also empty something fine tune model data synthesizer,issue,positive,positive,neutral,neutral,positive,positive
687585023,"Yes, lowering max mel frames is good, 500 or 700 is a good standard, 300 might be a little too small.

I'd still recommend another attention mechanism if you manage to get it working",yes lowering mel good good standard might little small still recommend another attention mechanism manage get working,issue,positive,positive,positive,positive,positive,positive
687358830,"> @javaintheuk In #472, I settled on this implementation: [blue-fish/Real-Time-Voice-Cloning@1d0d650...blue-fish:d692584](https://github.com/blue-fish/Real-Time-Voice-Cloning/compare/1d0d650...blue-fish:d692584)
> 
> Trimming silences from training data is very important when working with fatchord's tacotron1 model so I am going to bundle it with the pytorch synthesizer.

Thanks for the update! :)
",settled implementation trimming training data important working model going bundle synthesizer thanks update,issue,positive,positive,positive,positive,positive,positive
687334422,"Voice quality improves by increasing the postnet dims from 128 to 512. It really bloats the model size, so I might run a test to see if 256 is enough. Similarity of the generated voice to the reference audio is on par with the current synthesizer.

Refer to https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/472#discussion_r479706745 . Also changing the postnet CBHG projection from a hardcoded `[256, 80]` to `[postnet_dims*2, fft_bins] = [512*2, 80]` adds a lot of trainable parameters and its value is uncertain. I'll study that code again, and possibly make that first element its own hparam.

I'm fairly satisfied with fatchord's taco1 now that I've figured out how to best work with it. It trains very quickly. Due to my limited memory I cut `max_mel_frames` from 900 to 300 (that is, 11.25 to 3.75 seconds) and found it still generalizes to longer inputs.

Still trying to work out solutions to these 2 problems.
1. Occasionally, inference stops prematurely. It correlates with words like `and` or `or`, or highly nonstandard text inputs (sequences of numbers mixed with text). This seems to be a problem of the training data and not the tacotron, but it's annoying. I'll run an experiment to see if switching from location-sensitive attention to forward attention improves it. Next thing to try is to increase the max_mel_frames.
2. Possibly due to bad prosody or silences in the training data, some embeddings result in synthesized utterances that contain numerous and unexpected pauses every few words.

In my opinion, these are less serious than the problems with the existing synthesizer (inability to synthesize short utterances, random long gaps). So if that's the bar we need to clear, then we could have a releasable pretrained model regardless of whether these problems get solved.",voice quality increasing really model size might run test see enough similarity voice reference audio par current synthesizer refer also projection lot trainable value uncertain study code possibly make first element fairly satisfied figured best work quickly due limited memory cut found still longer still trying work occasionally inference prematurely like highly nonstandard text mixed text problem training data annoying run experiment see switching attention forward attention next thing try increase possibly due bad prosody training data result contain numerous unexpected every opinion le serious synthesizer inability synthesize short random long bar need clear could releasable model regardless whether get,issue,negative,negative,neutral,neutral,negative,negative
687262123,"The model (with siamese) seems to converge at 0.94 loss without attention learned, not really good so...

Now i try training with 256 embeddings but for 6 epochs on 500 batches only (24k utterances), i hope that with more epoch on same audios, it will learn better the attention mechanism (following my intuition) and if so, i can train on all dataset after that (to have better generalization if needed)

Tomorrow i will also try to implement a non-teacher-forcing training, it can be interesting too to learn attention during inference (which was the main issue in an old trained model)",model converge loss without attention learned really good try training hope epoch learn better attention mechanism following intuition train better generalization tomorrow also try implement training interesting learn attention inference main issue old trained model,issue,positive,positive,positive,positive,positive,positive
687249573,"Permission denied: https://drive.google.com/uc?id=1n1sPXvT34yXFLT47QZA6FIRGrwMeSsZc
Maybe you need to change permission over 'Anyone with the link'?
unzip:  cannot find or open pretrained.zip, pretrained.zip.zip or pretrained.zip.ZIP.

I can manually download the file though

Edit: Works now without any changes. Google is weird.",permission maybe need change permission link find open manually file though edit work without weird,issue,negative,negative,negative,negative,negative,negative
686914054,"I preprocessed LibriSpeech using `silence_min_duration_split` of 0.2 seconds instead of the default 0.4. This breaks up all utterances with long pauses (0.4 seconds is still quite long), which might make the VAD hack unnecessary. However, this setting is only effective when alignments are available for the dataset.

Edit: I did not notice a difference in the model when splitting at 0.2 and 0.4 seconds if VAD is applied. Now trying splitting on silences of 0.05 seconds without VAD.",instead default long still quite long might make hack unnecessary however setting effective available edit notice difference model splitting applied trying splitting without,issue,negative,positive,neutral,neutral,positive,positive
686911515,Closing as this issue is resolved. Thank you for sharing what worked for you @afantasialiberal .,issue resolved thank worked,issue,negative,neutral,neutral,neutral,neutral,neutral
686911296,"Presumed resolved. @therealjr reopen this issue, or start a new issue if you are still having trouble setting up the toolbox.",resolved reopen issue start new issue still trouble setting toolbox,issue,negative,negative,neutral,neutral,negative,negative
686910671,"@ZorvShadow I suggest opening an issue with tensorflow, they can provide much better help than we can. Here's an example of a similar issue: https://github.com/tensorflow/tensorflow/issues/37186

There is not enough information to troubleshoot. It could be a driver incompatibility issue, insufficient GPU memory (the [`423_add_cpu_mode`](https://github.com/blue-fish/Real-Time-Voice-Cloning/tree/423_add_cpu_mode) branch of my fork can help there, download it and launch the toolbox with `--cpu`), or something else.

If you're on linux you might be interested in this guide @mbdash wrote on installing the toolbox with GPU support on Ubuntu 20.04.",suggest opening issue provide much better help example similar issue enough information could driver incompatibility issue insufficient memory branch fork help launch toolbox something else might interested guide wrote toolbox support,issue,positive,positive,positive,positive,positive,positive
686823691,"Actually this is not the case. I was using a different venv which had `umap` installed (in addition to the correct package `umap-learn`) , clearing pycache fixed it.",actually case different addition correct package clearing fixed,issue,negative,positive,neutral,neutral,positive,positive
686822537,Now I am getting this issue after merging #517. Something in this commit is responsible: https://github.com/blue-fish/Real-Time-Voice-Cloning/commit/14c3646e1737d0a71b7f12077fba38dcd1eee4ef ,getting issue something commit responsible,issue,positive,positive,positive,positive,positive,positive
686607549,"Ok i will see... here i try a custom method to remove « useless frames » (remove all frames at the beginning / end without any value bigger than 1/3 of the min value of the complete spectrogram) and will see if it works better

With the encoder of this repo (embeddings 256) and simple concatenation with tacotron-encoder, the model converges around 1.045  loss and seems to learn the attention in prediction but not in inference so inference is really bad (noise / silence)
I re-try with my siamese encoder with simple concatenation and now at step 2 step 300 i have a loss of 0.95 so normally better but audio seems to be less intelligible, it’s strange but here the attention isn’t learned yet (neither in inference, neither in prediction) but it’s only step 2 so will wait for 1-2 epoch more (the main problem is that 1 epoch is 14h so can’t train really fast 

If i have no interesting result after this week-end, i will stop this approach and terminate my initial project with my old single-speaker model (which i’m sure it will work for a single speaker model) and will continue to train more this model later

Another thing i would try with this approach is a single-speaker model but emotion-conditionned to produce speech with specific emotion, that can be really fun too ! (i think it can be done simply by replacing the speaker-embedding by a one-hot (or embedding layer) of the desired emotion)",see try custom method remove useless remove beginning end without value bigger min value complete spectrogram see work better simple concatenation model around loss learn attention prediction inference inference really bad noise silence simple concatenation step step loss normally better audio le intelligible strange attention learned yet neither inference neither prediction step wait epoch main problem epoch train really fast interesting result stop approach terminate initial project old model sure work single speaker model continue train model later another thing would try approach model produce speech specific emotion really fun think done simply layer desired emotion,issue,negative,positive,neutral,neutral,positive,positive
686340880,"@javaintheuk In #472, I settled on this implementation: https://github.com/blue-fish/Real-Time-Voice-Cloning/compare/1d0d650...blue-fish:d692584

Trimming silences from training data is very important when working with fatchord's tacotron1 model so I am going to bundle it with the pytorch synthesizer.",settled implementation trimming training data important working model going bundle synthesizer,issue,negative,positive,positive,positive,positive,positive
686322102,"a bit more info, it was a clean install that i followed closely with this guide:
https://poorlydocumented.com/2019/11/installing-corentinjs-real-time-voice-cloning-project-on-windows-10-from-scratch/",bit clean install closely guide,issue,negative,positive,positive,positive,positive,positive
686311724,"Thank you for sharing @lawrence124 , I do not know exactly what conditions are needed for this to be a problem.
This is also reported in #435 and more information is available here: https://github.com/lmcinnes/umap/issues/24",thank know exactly problem also information available,issue,negative,positive,positive,positive,positive,positive
686164911,"@CorentinJ 
> If tensorflow is entirely removed from this repo, I will change that message for sure.
> 
> I still get a lot of feedback from people who spent hours trying to set things up.

In my opinion,
It is actually not that hard to setup on Ubuntu.
On windows... well... good luck. (for now)

I hope this will help reduce complaints :
# [WIKI](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki)

## [Installation - Ubuntu-20.04](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Installation---Ubuntu-20.04)

## [Installation - Windows-10](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Installation---Windows-10) _TODO_",entirely removed change message sure still get lot feedback people spent trying set opinion actually hard setup well good luck hope help reduce installation installation,issue,positive,positive,positive,positive,positive,positive
686085997,"This is a known issue, please see https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/431#issuecomment-665165023",known issue please see,issue,negative,neutral,neutral,neutral,neutral,neutral
686020838,"> @blue-fish i tried some things but nothing work, the prediction is not so bad but inference is always bad (only noise or silence)...
> Do you think it may be because i don’t trim silence ?

_Originally posted by @Ananas120 in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/447#issuecomment-685345939_

Please use this issue for discussion pertaining to your synthesizer. I have found that trimming silence is very helpful but I can still get an intelligible model without it.

While working on #472, I found that there was a lot to troubleshoot even starting with a synthesizer known to work. It might be worthwhile to get the pytorch equivalent working and use that to help validate your TF implementation.",tried nothing work prediction bad inference always bad noise silence think may trim silence posted ananas please use issue discussion pertaining synthesizer found trimming silence helpful still get intelligible model without working found lot even starting synthesizer known work might get equivalent working use help validate implementation,issue,positive,negative,negative,negative,negative,negative
685995786,"Accepted, thank you.

@blue-fish I will try my best.
",accepted thank try best,issue,positive,positive,positive,positive,positive,positive
685970520,"I support this request since we need better documentation. Would you also agree to help maintain the installation guides that you create?

Setup will become a lot easier when we merge #472, and when that happens the documentation should also reflect that.",support request since need better documentation would also agree help maintain installation create setup become lot easier merge documentation also reflect,issue,positive,positive,positive,positive,positive,positive
685948497,"> The presence of gaps depends on the training data. I get no gaps when training with VCTK, and plenty of gaps with LibriTTS.

As mentioned in https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/472#issuecomment-685943601 the gaps in LibriSpeech/TTS can be resolved by using voice activation detection to trim silences. See #501 for the process.",presence training data get training plenty resolved voice activation detection trim see process,issue,negative,neutral,neutral,neutral,neutral,neutral
685943601,"> The LibriSpeech model prematurely stops synthesizing, but it is not because the decoder loop is stopped upon predicting an empty frame.

Solved this by trimming silences when preprocessing data, see #501 for the general idea. I am finding that fatchord's taco1 is more sensitive to silence than Rayhane's taco2, so I will make the modifications to the preprocessing scripts in this PR.

Still need to improve voice similarity. To get things to train faster, I cut the model's trainable parameters in half (text encoder dims = 256, decoder LSTM dims = 512, when they are double that in this repo). That may have been too much.",model prematurely loop stopped upon empty frame trimming data see general idea finding sensitive silence make still need improve voice similarity get train faster cut model trainable half text double may much,issue,negative,positive,neutral,neutral,positive,positive
685914447,"Here is a documentation sample:

[Ubuntu20.04-Setup-NvidiaDrivers-020920-1812.pdf](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/5164052/Ubuntu20.04-Setup-NvidiaDrivers-020920-1812.pdf)

My aim would be to document how to get setup with this library step by step for Ubuntu20.04 and MiniConda.

I plan on installing the lib on other VMs so I would tweak the documentation I already created for myself in the process and share is with the community.
",documentation sample aim would document get setup library step step plan would tweak documentation already process share community,issue,negative,neutral,neutral,neutral,neutral,neutral
685764933,"Here is some partial documentation I have written.
It is missing the Nvidia drivers setup instruction which I hope I will be allowed to add to this project's WIKI.


# On Ubuntu 20.04

## Create a Conda workspace
```
conda deactivate

conda create --name prj_rtvc_py373 python=3.7.3
conda activate prj_rtvc_py373 
```

## Create edited requirement file
Open requirements.txt and edit the file to match the content below.
Save as requirements_conda_anaconda.txt
```
tensorflow==1.15
umap-learn
visdom
librosa>=0.5.1
matplotlib>=2.0.2
numpy>=1.14.0
scipy>=1.0.0
tqdm
python-sounddevice
pySoundFile
Unidecode
inflect
# PyQt5
multiprocess
numba==0.48
```

## Install packages
```
conda install -c conda-forge pytorch=1.2.0 torchvision=0.4.0
conda install ffmpeg

conda install -c conda-forge --file requirements_conda_anaconda.txt

# requirements_gpu.txt will not install
# conda install --file requirements_gpu.txt
pip install webrtcvad
conda install tensorflow-gpu=1.15
```",partial documentation written missing setup instruction hope add project create deactivate create name activate create requirement file open edit file match content save inflect install install install install file install install file pip install install,issue,positive,negative,negative,negative,negative,negative
685394946,"Hi @ramalamadingdong , thank you for contributing a fix for #446.

If we are going to distribute a sample mp3 with the repo, I'd prefer the reference audios from the [SV2TTS audio samples page](https://google.github.io/tacotron/publications/speaker_adaptation/index.html) since they can also be used for trying out the toolbox and benchmarking. We can get all 6 ""speaker adaptation for unseen speakers"" reference audios for 100k total: [**samples.zip**](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/5160607/samples.zip)",hi thank fix going distribute sample prefer reference audio page since also used trying toolbox get speaker adaptation unseen reference total,issue,negative,neutral,neutral,neutral,neutral,neutral
685345939,"@blue-fish i tried some things but nothing work, the prediction is not so bad but inference is always bad (only noise or silence)...
Do you think it may be because i don’t trim silence ? 
",tried nothing work prediction bad inference always bad noise silence think may trim silence,issue,positive,negative,negative,negative,negative,negative
684978718,"Welcome back @shoegazerstella . I tried your model by loading it in the toolbox with random examples. Not surprisingly, it still had much of the issues as the model I trained at 16,000 Hz. Would you please continue training of the model using the schedule below?

You should also increase the batch size to fully utilize the memory of your (dual?) v100 GPUs. Start training, and monitor the GPU memory utilization for a minute with `watch -n 0.5 nvidia-smi`. Keep adjusting until you are at 80-90% memory.

```
        ### Tacotron Training
        tts_schedule = [(7,  1e-3,    20_000,  96),   # Progressive training schedule
                        (5,  3e-4,    50_000,  64),   # (r, lr, step, batch_size)
                        (2,  1e-4,   100_000,  32),   #
                        (2,  1e-5, 2_000_000,  32)],  # r = reduction factor (# of mel frames
                                                      #     synthesized for each decoder iteration)
                                                      # lr = learning rate
```",welcome back tried model loading toolbox random surprisingly still much model trained would please continue training model schedule also increase batch size fully utilize memory dual start training monitor memory utilization minute watch keep memory training progressive training schedule step reduction factor mel iteration learning rate,issue,positive,positive,positive,positive,positive,positive
684964152,"Trying the following:
1. Train LibriSpeech using the other attention mechanism included with fatchord's tacotron. (I think it is Bahdanau attention)
2. Start with a model trained on VCTK, then finetune on LibriSpeech. Do not allow update of weights associated with Location-Sensitive Attention and possibly the encoder as well. (Already tried it without the last part and it did not work well)

I was very careful about making changes to the model (see https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/472#issuecomment-677730453) and implementing the training script. Since it is possible to train a model with robust attention on VCTK, I would not blame fatchord's tacotron1 entirely. The issue could be the quality of the training data (LibriSpeech/TTS) which may require us to decay the learning rate even faster.",trying following train attention mechanism included think attention start model trained allow update associated attention possibly well already tried without last part work well careful making model see training script since possible train model robust attention would blame entirely issue could quality training data may require u decay learning rate even faster,issue,negative,negative,neutral,neutral,negative,negative
684923206,"Hi @blue-fish,
Did you have time for testing the model I sent?
If not, I could do it, but I just wanted to understand if there was a testing script (that is using the usual test set and compute cumulative error metrics - if so, could you point me to it?), or if I should test it on some random examples.
Thanks!",hi time testing model sent could understand testing script usual test set compute cumulative error metric could point test random thanks,issue,negative,negative,negative,negative,negative,negative
684139226,"step 750K reached we are still around 0.03 loss

![image](https://user-images.githubusercontent.com/32403586/91784889-fa677d80-ebd1-11ea-9184-dee33862baac.png)

![encoder_mdl_ls_cv_vctk_vc12_umap_757500](https://user-images.githubusercontent.com/32403586/91784906-06ebd600-ebd2-11ea-8c62-5d263ae86b39.png)
",step still around loss image,issue,negative,neutral,neutral,neutral,neutral,neutral
684001677,"A few weeks will be more than good enough, thank you blue-fish :-) And thanks for the recommendation mbdash.",good enough thank thanks recommendation,issue,positive,positive,positive,positive,positive,positive
683867908,"The LibriSpeech model prematurely stops synthesizing, but it is not because the decoder loop is stopped upon predicting an empty frame. Not sure if this is due to the attention mechanism (it uses Location-Sensitive Attention) because the alignment is robust right until it reaches a problem point in the input text and then the attention drops suddenly and completely. Will see if this gets better with additional training. (Update: 1 Sep 2020: It does not)",model prematurely loop stopped upon empty frame sure due attention mechanism attention alignment robust right problem point input text attention suddenly completely see better additional training update,issue,negative,positive,positive,positive,positive,positive
683833812,"Problem fixed!
Turns out i'm stupid and didn't download pytorch",problem fixed turn stupid,issue,negative,negative,negative,negative,negative,negative
683811274,"Try `python3 demo_toolbox.py`

On many systems `python` opens python 2.x which does not have torch.",try python many python python torch,issue,negative,positive,positive,positive,positive,positive
683592994,"Here are an audio sample for the model (trained with embedding 256) at step 13.5k, i don't put the inference because it's just a stupid silence

Here are the plots for spectrograms / attention : 

![pred_step-013500_2_pred](https://user-images.githubusercontent.com/45139111/91690166-c8203680-eb65-11ea-9f00-dab89cde8ddb.png)
![pred_step-013500_2_infer](https://user-images.githubusercontent.com/45139111/91690200-d53d2580-eb65-11ea-8c4c-87bdd73ee507.png)

[audio_samples.zip](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/5148691/audio_samples.zip)


I think i will retry one old model with my embedding because it trains much faster and i think gate loss was better (but not sure)",audio sample model trained step put inference stupid silence attention think retry one old model much faster think gate loss better sure,issue,negative,positive,neutral,neutral,positive,positive
683538427,"@satan17 Closing as the original question has been answered, reopen this issue if you have further questions.",satan original question reopen issue,issue,negative,positive,positive,positive,positive,positive
683454099,"Oooh maybe it’s because i pad mels with 0 that the vocoder is impacted by them, maybe i should pad with another value... i will invastigate this because it can really fast up my inference ! (actually i do all vocoder inference in 1 audio per batch so not really optimal)

Yes of course, my training ends tomorrow morning so i will share the 4 last predictions if you want (i will see if inference is interesting, if not i will post prediction with teacher forcing and target so you can compare original, prediction and inference)
I hope my gate-loss will dcreases below 0.01, it never goes below till now... (except in one old model where it decreases around 0.0095)

Edit : it’s strange, i just look at the latest prediction (step 12k) and the attention mechanism seems to work well when teacher forcing but during inference... nothing (and gate loss is still at 0.011)
I suppose the cause is my custom training procedure with many steps with gate at 0 and only some steps (the latest) with gate at 1 and then it just predicts 0 every times... but it doesn’t explain the so bad inference",maybe pad impacted maybe pad another value really fast inference actually inference audio per batch really optimal yes course training tomorrow morning share last want see inference interesting post prediction teacher forcing target compare original prediction inference hope never go till except one old model around edit strange look latest prediction step attention mechanism work well teacher forcing inference nothing gate loss still suppose cause custom training procedure many gate latest gate every time explain bad inference,issue,positive,positive,positive,positive,positive,positive
683453646,"In the GUI you can see this text: `Warning: you did not pass a root directory for datasets as argument`

You need to launch the toolbox with the following command, where datasets_root is the parent folder containing your LibriSpeech or other supported dataset.
```
python demo_toolbox.py -d path/to/datasets_root
```",see text warning pas root directory argument need launch toolbox following command parent folder python,issue,negative,neutral,neutral,neutral,neutral,neutral
683453358,"It's good you found the problem instead of throwing away your model! I had a similar experience where my in-work synthesizer was scaling to the wrong range [-1, 1] and not [-4, 4], as well as  and padding mels with the wrong value (0 instead of -4). When starting out it is helpful to invert with a basic algorithm like Griffin-Lim to work out these types of issues.

Please share some wavs of your synthesizer in #507 when you get a chance?",good found problem instead throwing away model similar experience synthesizer scaling wrong range well padding wrong value instead starting helpful invert basic algorithm like work please share synthesizer get chance,issue,positive,negative,neutral,neutral,negative,negative
683449421,"Oh shit ! i just discover that my model was not as bad as i thought... 
In fact when i made prediction during training, i forgot to remove the padding and then the output of my vocoder was bad but not because the spectrogram was bad but it was the padding that « damages » the whole output audio...

So i removed padding and in fact the quality of the audio is not so bad with a loss around 1.05 so... i think my old models with 0.9 loss could be really good (but can’t test, i deleted them...)

The main issue now (and also present in the old models) is the gate-loss and the attention mechanism, the gate-loss doesn’t decrease below 0.01 and the attention mechanism seems not to be learned so at the end, the model is bad for inference
If you have some ideas to learn faster the gate part, you are welcome !

Some ideas i will test this week : 
- increase loss weight for the « end-gate » loss
- multiply by the mel-loss the position where it should predict 1 but predicted 0 (maybe multiply by 1-gate_pred where gate_true is equal to 1)
- retry my old model with additional Dense layer
- retry my models with my embeddings (with the siamese approach)

My vakanties ends in less than 2 weeks so i will not be able to make many tests but i hope one of these ideas will work !",oh discover model bad thought fact made prediction training forgot remove padding output bad spectrogram bad padding damage whole output audio removed padding fact quality audio bad loss around think old loss could really good test main issue also present old attention mechanism decrease attention mechanism learned end model bad inference learn faster gate part welcome test week increase loss weight loss multiply position predict maybe multiply equal retry old model additional dense layer retry approach le able make many hope one work,issue,negative,negative,neutral,neutral,negative,negative
683383948,@afantasialiberal Please see https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/431#issuecomment-673555684 for a general outline of the process. There is no tutorial available at this time.,please see general outline process tutorial available time,issue,negative,positive,positive,positive,positive,positive
683381936,"Hello, i speak spanish, is there a tutorial for train it on my language? sorry i am a very noob with this but very fun project-",hello speak tutorial train language sorry fun,issue,negative,negative,neutral,neutral,negative,negative
683367527,"The code issues identified should be resolved.

Regarding a pretrained model, I just started training on LibriSpeech `train-clean-100` and `train-clean-360`. With only 4gb of vram, my GPU is likely to have trouble finding the gradient as training progresses.

As mentioned earlier, we might need to add stop token prediction, else the synthesizer will terminate prematurely as it sometimes predicts empty frames whenever there is a failure to align. I'm going to look into implementing tacotron2 and transferring the existing weights, but I no longer have as much time to devote to this project as I did in recent weeks.",code resolved regarding model training likely trouble finding gradient training might need add stop token prediction else synthesizer terminate prematurely sometimes empty whenever failure align going look transferring longer much time devote project recent,issue,negative,negative,neutral,neutral,negative,negative
683333653,"I think it will not be as big as you think because tf2.x is really similar to pytorch for the inference and for the actual code you should just modify the inference methods by replacing the `sess.run(...)`call by a `model(...)`call (like in pytorch), i think i can be good with just that changes (for inference)

For training it’s a little bit more complicated i think but you can inspire you from my github to see how an optimization step works in tf2.x, it’s not as difficult as that (it’s really similar to a pytorch optimization step in fact)

```python
def optimize_step(inputs, target):
    with tf.GradientTape() as tape:
        y_pred = model(inputs, training = True)

        loss = loss_fn(target, y_pred)

    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))

    return loss
```

For the loss function, you can copy the one from the NVIDIA repo (or i can share my loss if you want, it’s inspired from the NVIDIA’s one), it’s just the sum of MSE on mel and postnet and a BCE on gate

You can have something like this : 
```python
def loss_fn(y_true, y_pred):
    mel, gate = y_true
    mel_pred, mel_postnet_pred, gate_pred = y_pred

    mel_loss = tf.reduce_mean(tf.square(mel - mel_pred))
    mel_postnet_loss = tf.reduce_mean(tf.square(mel - mel_postnet_pred))
    gate_loss = tf.keras.losses.binary_crossentropy(gate, gate_pred)

    return mel_loss + mel_postnet_loss + gate_loss
```",think big think really similar inference actual code modify inference call model call like think good inference training little bit complicated think inspire see optimization step work difficult really similar optimization step fact python target tape model training true loss target loss zip return loss loss function copy one share loss want inspired one sum mel gate something like python mel gate mel mel gate return,issue,positive,negative,neutral,neutral,negative,negative
683332626,"Thank you for that suggestion. There is a tensorflow 2.x version of our synthesizer already (without the SV2TTS modifications for speaker embedding): https://github.com/TensorSpeech/TensorFlowTTS

That should be a better starting point. The model is actually the easy part, getting it properly interfaced with the rest of our code is a lot of work. I'm not up to the task of rewriting the supporting infrastructure in TF 2.x but I'll take a look to get a better understanding of how much work it would entail.",thank suggestion version synthesizer already without speaker better starting point model actually easy part getting properly rest code lot work task supporting infrastructure take look get better understanding much work would entail,issue,positive,positive,positive,positive,positive,positive
683329625,"Ok i see, i didn’t know you already tried to convert the model to tf 2.x
The easiest way if you want to do this is to copy the pytorch implementation you found and rewrite it in tf2.x because the 2.x version is really similar to pytorch (layer names, arguments, functions, subclassing model, ...) then it can be really easy to convert it (convert the original 1.x model is really hard because tf 1.x is really different)

The actual checkpoint should work for the 2.x implementation if you achieve to make same names for layers (or you can convert the checkpoint with my convertor code)",see know already tried convert model easiest way want copy implementation found rewrite version really similar layer model really easy convert convert original model really hard really different actual work implementation achieve make convert convertor code,issue,positive,positive,neutral,neutral,positive,positive
683328750,"> @blue-fish in fact why do you want to convert the model to a pytorch one ?

@Ananas120 The synthesizer is written in tensorflow 1.x . There are 2 big problems with this:
1. The tensorflow 1.15 released binaries require CUDA 10.0 for GPU acceleration. This is an old version and only a narrow range of NVIDIA drivers will support it.
2. TF 1.15 is not available or python 3.8+.

Toolbox setup is getting harder over time because of these 2 issues. We have the choice of upgrading to TF 2.x (#370) or using a pytorch synthesizer (this issue). There is a preference for pytorch since the encoder and vocoder already use it. The vocoder is the slowest part by far so speed doesn't matter as much for the synth.",fact want convert model one ananas synthesizer written big require acceleration old version narrow range support available python toolbox setup getting harder time choice synthesizer issue preference since already use part far speed matter much,issue,negative,positive,neutral,neutral,positive,positive
683325655,"I just think about that but my convertor tensorflow to pytorch is based on names (i think, or it’s really easy to adapt it to base on names) then you can just use the checkpoint, extract variables with my code above and pass it to the converter and pass the pytorch model of the repo you found and then it should be good ;)",think convertor based think really easy adapt base use extract code pas converter pas model found good,issue,positive,positive,positive,positive,positive,positive
683315152,"@blue-fish  in fact why do you want to convert the model to a pytorch one ? Just for facility or because it can run faster on GPU ? 
Also  when you run the toolbox, are the 3 models running on GPU or only those in pytorch (or tf) ?

Because when i want to run my model (tf) with my vocoder (pt), it always raises OOM but when i runs the 2 models in pytorch, no OOM so i suppose pt and tf can’t run in parallel or have i missed something ?

Ps : if you want to see the new results of my tf implementation, i update the first message in #507 every new epoch
Actually i am training the model with the embedding done by the encoder of this repo and have a loss of 1.14 at epoch 4 step 500 so not as good as i had with my encoder but it decreases slower because the embedding is size 264 (my embeddings were size 64) so i hope this model trains slower but can decreases lower",fact want convert model one facility run faster also run toolbox running want run model always suppose run parallel something want see new implementation update first message every new epoch actually training model done loss epoch step good size size hope model lower,issue,negative,positive,positive,positive,positive,positive
683227979,"Oh thanks, but i did install it and did nt work but seems as it is this bug here https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/491 because the --low_mem argument, i try without it and work, now i am playing with it, seems like my 4gb ram are enough by now, but now i note the ascent on the synthetized are from english, BTW it start to be amazing congratulation to you there and thanks for your work, i have this doubt about the ascents on the results, i want bring some audio books to the internet jeje maybe can this be posible?",oh thanks install work bug argument try without work like ram enough note ascent start amazing congratulation thanks work doubt want bring audio maybe,issue,positive,positive,positive,positive,positive,positive
683187958,"Another known issue, see #455. Workaround is to install matplotlib 3.2.2. I have been focused on the pytorch synthesizer so have not been keeping up with dependencies.",another known issue see install synthesizer keeping,issue,negative,neutral,neutral,neutral,neutral,neutral
683185362,"Oh this time it seems to work(thanks!), mmm maybe you should put that on the readme. 


now i am trying to Synthetize and vocode the default text on the box with that dataset 5561 utterance 41615  

python3.7 demo_toolbox.py --low_mem  -d ./
Arguments:
    datasets_root:    .
    enc_models_dir:   encoder/saved_models
    syn_models_dir:   synthesizer/saved_models
    voc_models_dir:   vocoder/saved_models
    low_mem:          True
    seed:             None

libEGL warning: DRI2: failed to authenticate
[AVAudioResampleContext @ 0x5586f987ab00] Invalid input channel layout: 0
[AVAudioResampleContext @ 0x5586f993ca80] Invalid input channel layout: 0
Expression 'ret' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 1736
Expression 'AlsaOpen( hostApi, parameters, streamDir, &pcm )' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 1768


ALSA lib pcm_dmix.c:1090:(snd_pcm_dmix_open) unable to open slave
Loaded encoder ""pretrained.pt"" trained to step 1564501
Traceback (most recent call last):
  File ""/home/cmkat/src/voiceclone/Real-Time-Voice-Cloning-master/toolbox/__init__.py"", line 74, in <lambda>
    self.ui.browser_load_button.clicked.connect(lambda: self.load_from_browser())
  File ""/home/cmkat/src/voiceclone/Real-Time-Voice-Cloning-master/toolbox/__init__.py"", line 158, in load_from_browser
    self.add_real_utterance(wav, name, speaker_name)
  File ""/home/cmkat/src/voiceclone/Real-Time-Voice-Cloning-master/toolbox/__init__.py"", line 187, in add_real_utterance
    self.ui.draw_embed(embed, name, ""current"")
  File ""/home/cmkat/src/voiceclone/Real-Time-Voice-Cloning-master/toolbox/ui.py"", line 74, in draw_embed
    plot_embedding_as_heatmap(embed, embed_ax)
  File ""/home/cmkat/src/voiceclone/Real-Time-Voice-Cloning-master/encoder/inference.py"", line 174, in plot_embedding_as_heatmap
    cbar.set_clim(*color_range)
AttributeError: 'Colorbar' object has no attribute 'set_clim'
Found synthesizer ""pretrained"" trained to step 278000
multiprocess.pool.RemoteTraceback:


and at the end do nothing.",oh time work thanks maybe put trying synthetize default text box utterance python true seed none warning authenticate invalid input channel layout invalid input channel layout expression line expression line unable open slave loaded trained step recent call last file line lambda lambda file line name file line embed name current file line embed file line object attribute found synthesizer trained step end nothing,issue,positive,positive,neutral,neutral,positive,positive
683144159,Let's reopen and see if anyone knows why this is the case. Are you using RTVC (this repo) or [zhrtvc](https://github.com/KuangDD/zhrtvc)?,let reopen see anyone case,issue,negative,neutral,neutral,neutral,neutral,neutral
682584099,"Thank you for your replay.  I have not got the reason yet. I will check the vocoder first then the acoustic model. If I found the reason I will share with you.  thank you------------------&nbsp;原始邮件&nbsp;------------------
发件人:&nbsp;&quot;blue-fish&quot;<notifications@github.com&gt;
发送时间:&nbsp;2020年8月25日(星期二) 上午10:04
收件人:&nbsp;&quot;CorentinJ/Real-Time-Voice-Cloning&quot;<Real-Time-Voice-Cloning@noreply.github.com&gt;;
抄送:&nbsp;&quot;Johnzxf&quot;<1240721730@qq.com&gt;;&quot;Mention&quot;<mention@noreply.github.com&gt;;
主题:&nbsp;Re: [CorentinJ/Real-Time-Voice-Cloning] To Much Noise on Mandarin (#498)",thank replay got reason yet check first acoustic model found reason share thank quot quot quot quot quot quot quot mention quot mention much noise mandarin,issue,positive,positive,positive,positive,positive,positive
682449648,"> Regarding pretrained models, what's your status? We can't merge unless we have a working synthesizer and a vocoder that are least on par with the previous model.

Still don't have a good model. Over the last 3 weeks I learned a little about what works and doesn't work.

VCTK trains fast and the attention mechanism works well with it. It's possible to synthesize very long texts without gaps. The downside to VCTK is the limited number of speakers which results in the cloned voice having lower similarity. In synthesized speech, phonemes are not pronounced consistently owing to the accent diversity within the dataset. VCTK also has a limited vocabulary which requires more input preprocessing (text cleaners) to achieve good pronunciation.

LibriSpeech/TTS results in gaps and is more likely to expose the weakness of fatchord's tacotron1 which is the lack of stop token prediction. It stops synthesizing whenever empty frames are generated, which occasionally leads to sentences being cut off prematurely when the speaker and/or words in the input text are unseen. After opening the PR I found a pytorch version of Rayhane's synthesizer (https://github.com/begeekmyfriend/tacotron2). We could try bringing it in and transferring the weights, and if that fails, train a tacotron2 model from scratch. 

> Since you're actively maintaining the repo, it could be less frustrating for you if you could contact me on an instant messaging app. Shout me [an email](corentin.jemine@gmail.com) if you want to do that.

The current arrangement works well enough for me, I'll let you know if that changes.

",regarding status ca merge unless working synthesizer least par previous model still good model last learned little work work fast attention mechanism work well possible synthesize long without downside limited number voice lower similarity speech pronounced consistently owing accent diversity within also limited vocabulary input text achieve good pronunciation likely expose weakness lack stop token prediction whenever empty occasionally cut prematurely speaker input text unseen opening found version synthesizer could try transferring train model scratch since actively could le could contact instant shout want current arrangement work well enough let know,issue,negative,positive,neutral,neutral,positive,positive
682404878,"This project is not affiliated with Resemble.AI although the creator is.

The current repo is not too hard to set up if you can live with CPU-based inference. The slow part is the vocoder and since you need this to perform in real time, your options are getting GPU support for pytorch (relatively easy), or swapping out the vocoder for a faster one (hard if you're new to this, and merely tedious if you're not).

My [pytorch fork](https://github.com/blue-fish/Real-Time-Voice-Cloning/tree/447_pytorch_synthesizer) (currently going through the PR process) does not require an old version of tensorflow. We still have some catch-up for compatibility with the latest librosa and matplotlib. However, there is no good pretrained model for the torch-based synthesizer at this time and if you want to wait for it, it will be a few weeks.",project although creator current hard set live inference slow part since need perform real time getting support relatively easy swapping faster one hard new merely tedious fork currently going process require old version still compatibility latest however good model synthesizer time want wait,issue,positive,positive,neutral,neutral,positive,positive
682398435,"Try `python3.7 demo_toolbox.py --low_mem -d ./`

It is looking for a folder called `LibriSpeech` within the directory that is passed to the toolbox. We need to make that warning more helpful. Let me know if you have any suggestions.",try python looking folder within directory toolbox need make warning helpful let know,issue,negative,neutral,neutral,neutral,neutral,neutral
682388056,"this is a one diierent try   

python3.7 demo_toolbox.py --low_mem  -d LibriSpeech/train-clean-100/
Arguments:
    datasets_root:    LibriSpeech/train-clean-100
    enc_models_dir:   encoder/saved_models
    syn_models_dir:   synthesizer/saved_models
    voc_models_dir:   vocoder/saved_models
    low_mem:          True
    seed:             None

libEGL warning: DRI2: failed to authenticate
Warning: you do not have any of the recognized datasets in LibriSpeech/train-clean-100.
The recognized datasets are:
	LibriSpeech/dev-clean
	LibriSpeech/dev-other
	LibriSpeech/test-clean
	LibriSpeech/test-other
	LibriSpeech/train-clean-100
	LibriSpeech/train-clean-360
	LibriSpeech/train-other-500
	LibriTTS/dev-clean
	LibriTTS/dev-other
	LibriTTS/test-clean
	LibriTTS/test-other
	LibriTTS/train-clean-100
	LibriTTS/train-clean-360
	LibriTTS/train-other-500
	LJSpeech-1.1
	VoxCeleb1/wav
	VoxCeleb1/test_wav
	VoxCeleb2/dev/aac
	VoxCeleb2/test/aac
	VCTK-Corpus/wav48
Feel free to add your own. You can still use the toolbox by recording samples yourself.
[AVAudioResampleContext @ 0x5655461e9100] Invalid input channel layout: 0
[AVAudioResampleContext @ 0x565545d2b4c0] Invalid input channel layout: 0
Expression 'ret' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 1736
Expression 'AlsaOpen( hostApi, parameters, streamDir, &pcm )' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 1768
Expression 'ret' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 1736
Expression 'AlsaOpen( hostApi, parameters, streamDir, &pcm )' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 1768
Expression 'ret' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 1736
Expression 'AlsaOpen( hostApi, parameters, streamDir, &pcm )' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 1768
Expression 'ret' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 1736
Expression 'AlsaOpen( hostApi, parameters, streamDir, &pcm )' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 1768
ALSA lib pcm_dmix.c:1090:(snd_pcm_dmix_open) unable to open slave
",one try python true seed none warning authenticate warning feel free add still use toolbox recording invalid input channel layout invalid input channel layout expression line expression line expression line expression line expression line expression line expression line expression line unable open slave,issue,positive,positive,neutral,neutral,positive,positive
682350509,"Ok perfect, thank you, i copied the code of these 2 functions in my code project to run it faster (multi-thread to load audio / process mel-spect) 
Actually loss decreases slower (around 1.45 at step 2.7k) so i hope it will continue to decrease (1 epoch is 3k step so not yet the 2nd epoch)",perfect thank copied code code project run faster load audio process actually loss around step hope continue decrease epoch step yet epoch,issue,positive,positive,positive,positive,positive,positive
682242172,"#### How to make an utterance embedding
You will need the files in the `encoder` directory of this repo. It is recommended to save embeds to a file during preprocessing to conserve GPU memory and speed up synthesizer training. (See how we do it in this repo in `synthesizer_preprocess_audio.py` and `synthesizer_preprocess_embeds.py`)

```
from encoder import inference as encoder
from encoder import params_data
from pathlib import Path

encoder_model_fpath = Path(""encoder/saved_models/pretrained.pt"")
wav_fpath = Path(""file.wav"")

if not encoder.is_loaded():
    encoder.load_model(encoder_model_fpath)

# Load the audio waveform
wav, _ = librosa.load(str(wav_fpath), params_data.sampling_rate)
wav = encoder.preprocess_wav(wav)

# Make the embedding
embed = encoder.embed_utterance(wav)
```",make utterance need directory save file conserve memory speed synthesizer training see import inference import import path path path load audio make embed,issue,negative,neutral,neutral,neutral,neutral,neutral
682212305,"Tensorflow 1.15 only supports CUDA 10.0, and you have 10.2. That setup is not trivial and I am unable to provide support for it. 

If you are training from scratch, try my pytorch fork [`472_pytorch_synthesizer`](https://github.com/blue-fish/Real-Time-Voice-Cloning/tree/447_pytorch_synthesizer). GPU support for torch is much easier in my experience if you download the correct version from pytorch.org. I am trying to get it merged but the main thing holding it up is lack of a pretrained model that is on par with the tensorflow one.",setup trivial unable provide support training scratch try fork support torch much easier experience correct version trying get main thing holding lack model par one,issue,positive,negative,neutral,neutral,negative,negative
682208182,"Closing this support request due to inactivity (it is presumed resolved). Please comment in #455 for anything related to this particular problem, or open a new issue if experiencing other difficulties.",support request due inactivity resolved please comment anything related particular problem open new issue,issue,negative,positive,neutral,neutral,positive,positive
682184255,"@mbdash training is looking good, still some overlap on clusters. Have you tried to plot cross similarity matrixes?

https://github.com/resemble-ai/Resemblyzer/issues/13#issuecomment-544716234

I also did a few plots for a much larger number of speakers here:
https://github.com/resemble-ai/Resemblyzer/issues/13#issuecomment-544729472

Default is the model included in this repository and 768 was the model I trained. It looks like my EER was 0.00392 at 2.38M steps.",training looking good still overlap tried plot cross similarity also much number default model included repository model trained like eer,issue,positive,positive,positive,positive,positive,positive
682158246,"Dev has not stopped.

see #437 

I recommend Unbuntu 20 and MiniConda for getting the project up and running.",dev stopped see recommend getting project running,issue,negative,neutral,neutral,neutral,neutral,neutral
682042809,"At step 600k, loss is still moving between 0.015 and 0.04
But more often hitting lower values

https://drive.google.com/drive/folders/1QBeC8_PKFn-ZpZzsWY3vIFZKd4dk3g9O?usp=sharing

![image](https://user-images.githubusercontent.com/32403586/91466679-79b92200-e85d-11ea-904e-51ac6266d21c.png)

![encoder_mdl_ls_cv_vctk_vc12_umap_600000](https://user-images.githubusercontent.com/32403586/91466792-a10fef00-e85d-11ea-91bb-bbf496de49e5.png)



",step loss still moving often lower image,issue,negative,neutral,neutral,neutral,neutral,neutral
682038440,"I continue my experiments and tested my model with the additionnal dense layer but results are not good so i stopped it

I then embed all my datasets with the encoder of this repo and just launch a training for 1 epoch (3k steps) with 48 batch_size and 40 frames / optimization step

Loss was at 0.8 for the first 100 steps but increases to 1.2 at step 150 but it’s only the beginning and pretrained model was single-speaker so i will see loss after the epoch (tomorrow morning)
If loss is interesting, i will relaunch training for all the week-end and see

@blue-fish just to be sure, to embed « correctly » 1 audio with this encoder, i just need to preprocess_wav() and then make the spectrogram with the librosa.feature.melspectrogram() function with correct arguments (in the file of the repo) or should i do something more ?

Edit : oups, my bad, loss was 0.8 because i didn’t shuffle my dataset and then the pretrained speaker was in the first part x) now loss is 2.8 and decreases fast (1.8 at step 500)",continue tested model dense layer good stopped embed launch training epoch optimization step loss first step beginning model see loss epoch tomorrow morning loss interesting relaunch training see sure embed correctly audio need make spectrogram function correct file something edit bad loss shuffle speaker first part loss fast step,issue,negative,positive,positive,positive,positive,positive
680780059,"@afantasialiberal You should be putting the pretrained models in the following locations so the toolbox can find them:

Encoder: `Real-Time-Voice-Cloning/encoder/saved_models/pretrained.pt`
Synthesizer: `Real-Time-Voice-Cloning/synthesizer/saved_models/logs-pretrained/*` (there will be a few files)
Vocoder: `Real-Time-Voice-Cloning/vocoder/saved_models/pretrained/pretrained.pt`

Then run the toolbox with:
```
python3.7 demo_toolbox.py
```",following toolbox find synthesizer run toolbox python,issue,negative,neutral,neutral,neutral,neutral,neutral
680777374,"This is a known issue. From https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/455#issuecomment-674410447

> Workaround is to install a matplotlib version that is lower than 3.3.
> 
> ```
> pip install matplotlib==3.2.2
> ```
",known issue install version lower pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
680712287,"Hello, i am trying to make it work but i get this message, i am doing something wrong here?

python3.7 demo_toolbox.py -d dataset_root/vocoder/saved_models/pretrained/
Arguments:
    datasets_root:    dataset_root/vocoder/saved_models/pretrained
    enc_models_dir:   encoder/saved_models
    syn_models_dir:   synthesizer/saved_models
    voc_models_dir:   vocoder/saved_models
    low_mem:          False
    seed:             None

Error: Model files not found. If needed, download them here:
https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models
",hello trying make work get message something wrong python false seed none error model found,issue,negative,negative,negative,negative,negative,negative
680416717,"With a good encoder, the utterance embeddings will have very high cosine similarity which means the speaker embedding will not be far off. Therefore it should make little difference whether an utterance or speaker embedding is used.

In practice the encoder does transfer prosody, as noted in Corentin's thesis:

> The speech generated by the synthesizer matches correctly the text, even in the presence of complex or fictitious words.  The prosody is however sometimes unnatural, with pauses at unexpected locations in the sentence, or the lack of pauses where they are expected.  This is particularly noticeable with the embedding of some speakers who talk slowly, showing that the speaker encoder does capture some form of prosody.  The lack of punctuation in LibriSpeech is partially responsible for this, forcing the model to infer punctuation from the text alone.  This issue was highlighted by the authors as well,  and can be heard on some of their samples of LibriSpeech speakers.

I'm going to close this as I don't expect to perform any experiments in the near future.",good utterance high cosine similarity speaker far therefore make little difference whether utterance speaker used practice transfer prosody noted thesis speech synthesizer correctly text even presence complex fictitious prosody however sometimes unnatural unexpected sentence lack particularly noticeable talk slowly showing speaker capture form prosody lack punctuation partially responsible forcing model infer punctuation text alone issue well going close expect perform near future,issue,negative,positive,neutral,neutral,positive,positive
680415452,@afantasialiberal Please open a new issue if you are still stuck with the installation.,please open new issue still stuck installation,issue,negative,positive,neutral,neutral,positive,positive
680307365,Is anyone training a model in Brazilian Portuguese at this time? If not I will go ahead and close this issue.,anyone training model time go ahead close issue,issue,negative,neutral,neutral,neutral,neutral,neutral
680297021,"Confirm that the issue of gaps in spectrograms will be resolved if we merge fatchord's tacotron1 in #472. The presence of gaps depends on the training data. I get no gaps when training with VCTK, and plenty of gaps with LibriTTS.",confirm issue resolved merge presence training data get training plenty,issue,negative,neutral,neutral,neutral,neutral,neutral
680295566,"There's a pytorch version of Rayhane's tacotron-2. Wish I found this earlier! https://github.com/begeekmyfriend/tacotron2

As a last resort we could add support for this tacotron2, and transfer over the pretrained weights from tensorflow.",version wish found last resort could add support transfer,issue,positive,neutral,neutral,neutral,neutral,neutral
680293088,"> Edit: I didn't do a good job documenting and checking in code throughout all the experiments so there is a chance the model I trained for 1M+ steps didn't have a final activation. @blue-fish probably knows that better than me at this point. 

There are no states associated with the activation so it's not possible to tell with just by looking at the checkpoint file. When I hacked the final linear layer in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/458#issuecomment-673341585 I noticed the loss came down very quickly using ReLU. The loss was already down to 0.01 within 10 steps of restarting training. So if I had to guess, that particular 768/768 English encoder was likely using ReLU.

@sberryman Thanks for digging up and sharing those additional links.",edit good job code throughout chance model trained final activation probably better point associated activation possible tell looking file hacked final linear layer loss came quickly loss already within training guess particular likely thanks digging additional link,issue,positive,positive,positive,positive,positive,positive
680283300,"A fix for this has been merged in matplotlib, to be released in 3.3.2. Since it's their bug, I am going to label this as dependency issue instead. We still have unresolved issues with matplotlib 3.3.x, see #455.

It would still be a good idea to change the order of the imports on our end to prevent unnecessary frustration for those not using matplotlib>=3.3.2.",fix since bug going label dependency issue instead still unresolved see would still good idea change order end prevent unnecessary frustration,issue,negative,positive,positive,positive,positive,positive
680281683,"@mueller91 I used tanh as the final activation to force the values between -1 and 1 as opposed to 0-1 with ReLU. I never tried training with no final activation so I'm not sure how that would turn out to be honest.

https://github.com/resemble-ai/Resemblyzer/issues/13#issuecomment-557269666

Edit: I didn't do a good job documenting and checking in code throughout all the experiments so there is a chance the model I trained for 1M+ steps didn't have a final activation. @blue-fish probably knows that better than me at this point. There is a chance tanh was only used on an experiment when I was trying to build an encoder model based on raw waveform

Edit 2: Corentin doesn't think it makes much of a difference though. https://github.com/resemble-ai/Resemblyzer/issues/15#issuecomment-555097662

Edit 3: This implementation doesn't use an activation function after the LSTM. https://github.com/HarryVolek/PyTorch_Speaker_Verification/blob/master/speech_embedder_net.py",used tanh final activation force opposed never tried training final activation sure would turn honest edit good job code throughout chance model trained final activation probably better point chance tanh used experiment trying build model based raw edit think much difference though edit implementation use activation function,issue,positive,positive,positive,positive,positive,positive
680280718,"@sberryman When I train a VCTK-based synthesizer to 100k steps on my basic GPU, I get a very similar result for voice cloning regardless of whether I use Corentin's model or @mbdash 315k model (LS+VCTK+CV) as the speaker encoder for training and inference. See: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/458#issuecomment-678747512

That result was completely unexpected and maybe I should delete my pycache just to be very sure that I performed the experiment properly. But because of the different hidden unit size it is impossible to use the wrong encoder with the wrong synthesizer.

Also, the pretrained encoder bundled with Resemblyzer is identical to the one in this repo, which I recall took ~20 days to train on a single 1080 TI.",train synthesizer basic get similar result voice regardless whether use model model speaker training inference see result completely unexpected maybe delete sure experiment properly different hidden unit size impossible use wrong wrong synthesizer also identical one recall took day train single ti,issue,negative,negative,negative,negative,negative,negative
680278710,@sberryman Could you elaborate why to chose Tanh as final activation? ,could elaborate chose tanh final activation,issue,negative,positive,positive,positive,positive,positive
680277487,"@blue-fish how does @mbdash's model work with the existing synth/vocoder? I would assume not very well and producing a generic voice?

If that is the case, @mbdash should stop training and start over with Tanh as the final activation. Then you'll use @mbdash's new model to train the synthesizer and vocoder from scratch (several weeks worth of GPU time.)

Replicating Corentin's work training from scratch would likely require well over 700 hours of training using two 1080 TI's. They are using much larger GPUs (40GB of memory I believe) at Resemble.ai to train more quickly. The 700 hours is a very rough estimate to illustrate the several weeks of training for each of the three models.",model work would assume well generic voice case stop training start tanh final activation use new model train synthesizer scratch several worth time work training scratch would likely require well training two ti much memory believe train quickly rough estimate illustrate several training three,issue,negative,positive,neutral,neutral,positive,positive
680273233,"@mueller91 I think your observations are explained by our continued use of ReLU (which is not a deliberate choice, we just used the repo code without modification). @sberryman removed ReLU in https://github.com/resemble-ai/Resemblyzer/issues/13 which causes inter-similarity to be centered around zero instead of 0.5 as in our case.

We will continue using ReLU as long as the encoder model needs to support Corentin's pretrained encoder which also uses it.",think continued use deliberate choice used code without modification removed centered around zero instead case continue long model need support also,issue,negative,negative,neutral,neutral,negative,negative
680269521,"I wanted to leave one last thing before closing, here's the link to the .whl file required for webrtcvad:
https://drive.google.com/file/d/1Sdg8iRwmjIrju-c7ikOZrkxRyPzAP0oH/view",leave one last thing link file,issue,negative,neutral,neutral,neutral,neutral,neutral
680260686,"Congratulations on completing the setup! It is far from a smooth process as you experienced. You can [`pip install webrtcvad-wheels`](https://pypi.org/project/webrtcvad-wheels/) to avoid having to build webrtcvad from source.

I am going to close this issue as I believe it is resolved, if you have any further issues you are welcome to reopen it, or start a new issue. Thanks for sharing the solutions to each of the problems that you encountered, they will help others.",setup far smooth process experienced pip install avoid build source going close issue believe resolved welcome reopen start new issue thanks help,issue,positive,positive,positive,positive,positive,positive
680250516,"@mueller91 I am not the guy you are looking for, the wise guy with the answers is @blue-fish.

Note that there is 2 encoder models. 
Be sure you are downloading the proper one.


""**LibriSpeech + CommonVoice + VCTK Only until step 315k**""
The 1st one was only ""**LibriSpeech + CommonVoice + VCTK until step 315k**""
Available here: https://drive.google.com/drive/folders/1OkHpeV3i5fGzI6shhjY3nkpN9jXGk7Ak?usp=sharing


""**LibriSpeech + CommonVoice + VCTK until step 315k + VoxCeleb1&2**""
The 2nd encoder model is the encoder above with VoxCeleb 1 & 2 added to the dataset after step 315k.
(I stopped the training at 315k, then added more datasets (VoxCeleb), and resumed training at step 315k)
I am constantly (daily) updating the new model ""**LibriSpeech + CommonVoice + VCTK until step 315k + VoxCeleb1&2**""
Available here: https://drive.google.com/drive/folders/1QBeC8_PKFn-ZpZzsWY3vIFZKd4dk3g9O?usp=sharing

The latest uploaded is 525k steps.
Currently I am locally at step 531k.

I would wait for me to reach 750k to do anything with this encoder if I were you, 
I see the loss bouncing from .026 to .04 non stop currently.

![image](https://user-images.githubusercontent.com/32403586/91223698-06919d80-e6ef-11ea-83f8-e80cbef9a0da.png)
",guy looking wise guy note sure proper one step st one step available step model added step stopped training added training step constantly daily new model step available latest currently locally step would wait reach anything see loss bouncing non stop currently image,issue,negative,positive,positive,positive,positive,positive
680237854,"Dear @mbdash 
thank you for providing the GPU and publishing the models. One curious observation, though: i use your model to embed a batch of utterances, and compute the inter- and intra class cosine similarity (i.e. the cosine similarity for all pairs s_i, s_j where the speakers are different, or the same, repectively).
i obtain mean inter-similarity of around 0.45, and intra-similarity of around 0.9
- i would expect an inter-sim of close to 0 (since this is the training objective)
- these values do not correspond to the very low loss you report
are you still using ReLu activation? Shouldn't relu be disadvantageous (since it limits the expressiveness of the model, or more intuitively, it limits 'where' on a hypersphere the model can map to, namely only to the positive domain)",dear thank providing one curious observation though use model embed batch compute class cosine similarity cosine similarity different obtain mean around around would expect close since training objective correspond low loss report still activation disadvantageous since expressiveness model intuitively hypersphere model map namely positive domain,issue,positive,negative,neutral,neutral,negative,negative
680213707,"@blue-fish Thank you for the suggestion. I downgraded the machine to be running Ubuntu 18.04 now with Python 3.7.5. I had completed installing PyTorch and FFMPEG, but I'm stuck on `pip3 install -r requirements_gpu.txt` Where webrtcvad is causing issues:
`    ERROR: Command errored out with exit status 1:
     command: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-gzlnmnxc/webrtcvad/setup.py'""'""'; __file__='""'""'/tmp/pip-install-gzlnmnxc/webrtcvad/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /tmp/pip-record-qzppu8_s/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers /home/rachnera/.local/include/python3.7m/webrtcvad
         cwd: /tmp/pip-install-gzlnmnxc/webrtcvad/
    Complete output (20 lines):
    running install
    running build
    running build_py
    creating build
    creating build/lib.linux-x86_64-3.7
    copying webrtcvad.py -> build/lib.linux-x86_64-3.7
    running build_ext
    building '_webrtcvad' extension
    creating build/temp.linux-x86_64-3.7
    creating build/temp.linux-x86_64-3.7/cbits
    creating build/temp.linux-x86_64-3.7/cbits/webrtc
    creating build/temp.linux-x86_64-3.7/cbits/webrtc/common_audio
    creating build/temp.linux-x86_64-3.7/cbits/webrtc/common_audio/signal_processing
    creating build/temp.linux-x86_64-3.7/cbits/webrtc/common_audio/vad
    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DWEBRTC_POSIX -Icbits -I/usr/include/python3.7m -c cbits/pywebrtcvad.c -o build/temp.linux-x86_64-3.7/cbits/pywebrtcvad.o
    cbits/pywebrtcvad.c:1:10: fatal error: Python.h: No such file or directory
     #include <Python.h>
              ^~~~~~~~~~
    compilation terminated.
    error: command 'x86_64-linux-gnu-gcc' failed with exit status 1
    ----------------------------------------
ERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-gzlnmnxc/webrtcvad/setup.py'""'""'; __file__='""'""'/tmp/pip-install-gzlnmnxc/webrtcvad/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /tmp/pip-record-qzppu8_s/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers /home/rachnera/.local/include/python3.7m/webrtcvad Check the logs for full command output.
`

Update: I managed to compile my own version of webrtcvad, but now I can't open the demo_toolbox.py program with this error appearing:
`qt.qpa.plugin: Could not load the Qt platform plugin ""xcb"" in """" even though it was found.`

Update 2: I figured it out, I just needed to install a dependency for Qt
`sudo apt-get install libxcb-xinerama0`",thank suggestion machine running python stuck pip install causing error command exit status command open compile code install record user compile complete output running install running build running build running building extension fatal error file directory include compilation error command exit status error command exit status open compile code install record user compile check full command output update compile version ca open program error could load platform even though update figured install dependency install,issue,negative,positive,neutral,neutral,positive,positive
680134246,@ustraymond Code for calculating EER is shared in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/61#issuecomment-514653922 and https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/126#issuecomment-530960239 . The code is identical but the context for discussion is slightly different.,code calculating eer code identical context discussion slightly different,issue,negative,neutral,neutral,neutral,neutral,neutral
680104169,"> 
> 
> @ustraymond This is one way of doing it:
> 
>     1. Make a folder `datasets_root/SV2TTS/encoder_test/` and move some folders over from encoder to make a test set.
> 
>     2. Modify [speaker_verification_dataset.py](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/a32962bb7b4827660646ac6dabf62309aea08a91/encoder/data_objects/speaker_verification_dataset.py#L11) to take 2 paths, one for training and test. Modify the DataLoader in the same file to return a training batch and test batch.
> 
>     3. In encoder/train.py, you would run forward/backward pass as normal on the training batch, and then follow that up with a forward pass on the test set to get loss and eer for display purposes. I think the test part should be wrapped with `with torch.no_grad()` and you might need to set `model.eval()` too.
> 
> 
> This would slow down training considerably if performed every step, so you might want to run this evaluation every 10 or 100 steps.

in the thesis, 
""In fact, we computed the test set EER to be4.5%.  ""
""We refer to GE2E and use 6 utterances for enrollment and compare those to 7 utterances."" ???

it seems there is no code here to do such calculation.

So I was trying to calculate the EER for whole set of test data, (40 speakers with 4000+ utterances), different speakers got different amount of utterance.

Assume I calculate the embedding of all utterance, what is the next step? 

repeat the step?
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/encoder/model.py#L80

assume a speaker got at least 13 utterances?

get the centroid of each speaker based on 6 utterances ?
centroids_incl 


for another 7+ utterances, find its distance to centroids_incl? and find the smallest centroid and use that as predicted label?

then reuse these lines in model.py?

           # Snippet from https://yangcha.github.io/EER-ROC/
            fpr, tpr, thresholds = roc_curve(labels.flatten(), preds.flatten())           
            eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)

wonder how to calculate the EER 4.5%.?

thx for advice.",one way make folder move make test set modify take one training test modify file return training batch test batch would run pas normal training batch follow forward pas test set get loss eer display think test part wrapped might need set would slow training considerably every step might want run evaluation every thesis fact test set eer refer gee use enrollment compare code calculation trying calculate eer whole set test data different got different amount utterance assume calculate utterance next step repeat step assume speaker got least get centroid speaker based another find distance find centroid use label reuse snippet eer lambda wonder calculate eer advice,issue,negative,negative,neutral,neutral,negative,negative
679977434,"Comparison of Griffin-Lim, MelGAN, and WaveRNN: [**wavfiles.zip**](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/5123483/wavfiles.zip)

I like how fast MelGAN synthesizes, it's even faster than Griffin-Lim. The codebase is also very lightweight, I used the descript implementation with their multispeaker pretrained model. But the quality is not good enough. [Parallel WaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN) is also fast and performs better, it is next on my list of vocoders to try. Also looking forward to whenever someone makes an open-source version of [HooliGAN](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/364#issuecomment-670367298).",comparison like fast even faster also lightweight used descript implementation model quality good enough parallel also fast better next list try also looking forward whenever someone version hooligan,issue,positive,positive,positive,positive,positive,positive
679962340,"@MarcoDotIO If you want to be a beta tester look at my pull request that eliminates the tensorflow requirement, see #472.

That will work on 20.04.1 (bug #504 applies, but has a solution). Unfortunately our pretrained models on the torch-based synthesizer are not that good and we are still developing them. See https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/449#issuecomment-674420484 for a model that works with the torch-based synth. You will still need the encoder and vocoder model from the [pretrained models](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models) wiki page.",want beta tester look pull request requirement see work bug solution unfortunately synthesizer good still see model work still need model page,issue,negative,positive,neutral,neutral,positive,positive
679592464,"Hi @afantasialiberal, tensorflow 1.15 is required. I do have a pull request that eliminates the tensorflow dependency (#472) but we don't have a very good pretrained model for use with it at this time.

https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/431#issuecomment-664038511
> ### No module named 'tensorflow.contrib'
> #### Summary
> 
> The toolbox requires Tensorflow 1.15 and this error message occurs when Tensorflow 2.x is installed.
> #### Solution
> 
> Install Tensorflow 1.15.
> 
> If you get a pip error `Could not find a version that satisfies the requirement tensorflow==1.15` then you are likely using Python 3.8+ which is incompatible with TF 1.15. To resolve that, you will need to switch to Python 3.6 or 3.7.",hi pull request dependency good model use time module summary toolbox error message solution install get pip error could find version requirement likely python incompatible resolve need switch python,issue,negative,positive,positive,positive,positive,positive
679458546,@Johnzxf I'm sorry you didn't get a response. Did you figure out what was causing the noise?,sorry get response figure causing noise,issue,negative,negative,negative,negative,negative,negative
679161035,"I solved it via adding the sip line right before pyqt5

On Sun, Aug 23, 2020 at 9:44 AM blue-fish <notifications@github.com> wrote:

> @explodersname <https://github.com/explodersname> Looks like this issue:
> https://stackoverflow.com/a/59797479
> For which the suggested resolution is one of the following:
>
>    - Upgrade pip: python3 -m pip install --upgrade pip , then try
>    installing requirements.txt again
>    - Or install version 5.14.0 of PyQt5: python3 -m pip install
>    PyQt5==5.14.0
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/505#issuecomment-678776321>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AKBJ6IM7BQUSPPDBOH6Y2SLSCEMM3ANCNFSM4QIVNM7Q>
> .
>
",via sip line right sun wrote like issue resolution one following upgrade pip python pip install upgrade pip try install version python pip install reply directly view,issue,negative,positive,positive,positive,positive,positive
678892265,"@appstore-redphoenix If you would like to get support, please open a new issue and post the error message and full traceback. Please check and make sure you are running Python 3.6 or 3.7, and have installed the requirements per the readme.",would like get support please open new issue post error message full please check make sure running python per,issue,positive,positive,positive,positive,positive,positive
678872771,It looks like this issue was opened in error. I am going to close it. @yurimickaelsp You are invited to update the description and reopen.,like issue error going close update description reopen,issue,negative,neutral,neutral,neutral,neutral,neutral
678872478,"@javaintheuk Right before checking hparams.rescale. I have noticed that for a few utterances in VCTK the preprocess result will be None or an empty wav which causes an error, so if you experience this you could follow up the preprocess_wav with `if wav is not None:` and `if wav.size > 0` to check for a valid wav before continuing.

```
# Load the audio waveform
wav, _ = librosa.load(str(wav_fpath), hparams.sample_rate)
wav = encoder.preprocess_wav(wav)
```

Thanks for expressing your interest in this idea, I will consider submitting a pull request once I've figured out a good implementation. Anyone in the community is also welcome to contribute ideas for better preprocessing or to submit a PR.",right result none empty error experience could follow none check valid load audio thanks interest idea consider pull request figured good implementation anyone community also welcome contribute better submit,issue,positive,positive,positive,positive,positive,positive
678802651,"Hi Bluefish, (big fan of your work here!)

Thank for the tip! please can you correct me if I'm wrong? you mean HERE?

`# Load the audio waveform 
             wav, _ = librosa.load(str(wav_fpath), hparams.sample_rate) 
             if hparams.rescale: 
                 wav = wav / np.abs(wav).max() * hparams.rescaling_max 


HERE? 
--->   wav = encoder.preprocess_wav(wav)

# Get the corresponding text 
`

Thanks in advance!",hi bluefish big fan work thank tip please correct wrong mean load audio get corresponding text thanks advance,issue,positive,negative,negative,negative,negative,negative
678791727,"Hi @halswift , thank you for taking the time to report the issue. It might be related to #282 . My headphones are not good and I cannot tell the difference between audio played within the toolbox and the saved file. However, I came up with a simple test which might help demonstrate the issue:

1. Synthesize a speech sample in the toolbox. Its spectrogram will be displayed.
2. Export the synthesized sample to a wav file
3. Click ""browse"" and load the new file back into the toolbox

<img width=""784"" alt=""screenshot_issue462"" src=""https://user-images.githubusercontent.com/67130644/90982843-f0cd7e00-e51e-11ea-97b5-044afe5b32fb.png"">

I am finding that the spectrograms are different. The one on the top appears visibly noisy. A good exercise would be to compare the numerical representation of the synthesized wav (a numpy array) to what is saved into the file and see what is causing it.",hi thank taking time report issue might related good tell difference audio within toolbox saved file however came simple test might help demonstrate issue synthesize speech sample toolbox spectrogram displayed export sample file click browse load new file back toolbox finding different one top visibly noisy good exercise would compare numerical representation array saved file see causing,issue,positive,positive,positive,positive,positive,positive
678785932,"This is a simple concept but difficult to implement in a user-friendly way (by which I mean being able to accomplish the desired result without having to edit code). Mozilla TTS asks the user to specify the prosody embedding directly, and then this is concatenated in an identical manner as the speaker embedding. That kind of implementation is only suitable for researchers and users with a technical background.

[1803.09047](https://arxiv.org/pdf/1803.09047.pdf) mentions the use of a ""prosody encoder"" for automatic classification. However, I think the architecture we have for the speaker encoder could work for this, except one would put the wav files into separate folders representing each desired prosody feature instead of by speaker.

Finetuning an existing synthesizer model in this way does not seem reasonable unless 1) the prosody dataset is sufficiently large, or 2) the baseline synthesizer has consistent prosody. I can see training a synth on LibriTTS and calling that ""American accent"", followed by finetuning on some subset of VCTK and defining that as ""UK accent"". Aside from accent, I do not see any reasonable use cases for finetuning which means the prosody encoder needs to be in use during initial training of models.

One idea I have is to have a generic ""encoder"" module that operates an encoder network consisting of a speaker encoder and one or more prosody encoders. These encoders could be trained independently. Something that might work well is to first train the prosody encoders, then concat the prosody embedding with the speaker embedding when evaluating GE2E loss during training of the speaker encoder. Doing this should make the speaker encoder's utterance embeddings more invariant with respect to prosody. 

In any event, having scoped out this issue, I have decided against working it due to a general lack of interest, and also because no simple, user-friendly implementation exists. If you want the feature, you could try Mozilla TTS as GST support is merged in their dev branch now.",simple concept difficult implement way mean able accomplish desired result without edit code user specify prosody directly identical manner speaker kind implementation suitable technical background use prosody automatic classification however think architecture speaker could work except one would put separate desired prosody feature instead speaker synthesizer model way seem reasonable unless prosody sufficiently large synthesizer consistent prosody see training calling accent subset accent aside accent see reasonable use prosody need use initial training one idea generic module network speaker one prosody could trained independently something might work well first train prosody prosody speaker gee loss training speaker make speaker utterance invariant respect prosody event issue decided working due general lack interest also simple implementation want feature could try support dev branch,issue,positive,positive,positive,positive,positive,positive
678776943,"@Jason3018 Due to the lack of a response I am going to presume this is resolved. It the issue is not resolved, please reopen the issue and please share what you have tried and the current error message that you are seeing.",due lack response going presume resolved issue resolved please reopen issue please share tried current error message seeing,issue,negative,negative,neutral,neutral,negative,negative
678776321,"@explodersname Looks like this issue: https://stackoverflow.com/a/59797479
For which the suggested resolution is one of the following:
* Upgrade pip: `python3 -m pip install --upgrade pip` , then try installing requirements.txt again
* Or install version 5.14.0 of PyQt5: `python3 -m pip install PyQt5==5.14.0`",like issue resolution one following upgrade pip python pip install upgrade pip try install version python pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
678747512,"The previous data is bad because I used the wrong encoder for testing with @mbdash 315k model. Here is a fair comparison (old = Corentin's encoder, new = mbdash 315k encoder): [**wav_comparison.zip**](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/5113713/wav_comparison.zip)

Now we have a different and unexpected issue where the synthesized voices are very similar regardless of the encoder used! They are indistinguishable from my point of view. (You will notice that the synth trained with the old encoder has an annoying sound artifact at the end. It does not appear when Griffin-Lim is used to invert the spectrogram, so it is an artifact of the vocoder.)

When the encoder is better trained on VoxCeleb I will repeat this experiment to see if the similarity of the cloned voice improves. The authors of SV2TTS obtained better results on this metric. Which parts of our model need to improve to match their quality?",previous data bad used wrong testing model fair comparison old new different unexpected issue similar regardless used indistinguishable point view notice trained old annoying sound artifact end appear used invert spectrogram artifact better trained repeat experiment see similarity voice better metric model need improve match quality,issue,negative,positive,neutral,neutral,positive,positive
678738496,"@blue-fish sure, will add a comment there, give me a minute.",sure add comment give minute,issue,negative,positive,positive,positive,positive,positive
678738245,"@ZeroCool940711 I opened https://github.com/matplotlib/matplotlib/issues/18326 to report this bug. Would you please add a comment with the following information to that issue, to aid in troubleshooting? Since it can occur on multiple OS and versions of python it would be helpful for them to know that.

```
**Matplotlib version**
<!--Please specify your platform and versions of the relevant libraries you are using:-->
  * Operating system:
  * Matplotlib version: 
  * Matplotlib backend (`print(matplotlib.get_backend())`):
  * Python version:
  * Jupyter version (if applicable):
  * Other libraries: 

<!--Please tell us how you installed matplotlib and python e.g., from source, pip, conda-->
<!--If you installed from conda, please specify which channel you used if not the default-->
```",report bug would please add comment following information issue aid since occur multiple o python would helpful know version please specify platform relevant operating system version print python version version applicable please tell u python source pip please specify channel used default,issue,positive,positive,positive,positive,positive,positive
678735886,"@blue-fish I think the problem with that error is that there are two versions of the QT library installed on the system and they are in conflict, seems like other people have reported the same on other software, some solutions are finding the two versions and removing one of them, installing and running the software inside a virtualenv and running as different user on ubuntu and other distro so it doesnt find both versions, I will test those things and let you know if anything works. Feel free to open a new issue for this but I think we should keep the discussion here and then reference the fix on another issue if we find a fix.",think problem error two library system conflict like people finding two removing one running inside running different user doesnt find test let know anything work feel free open new issue think keep discussion reference fix another issue find fix,issue,negative,positive,positive,positive,positive,positive
678734984,Please open a new issue and paste the traceback. I have some experience with that error but have not been successful in resolving it.,please open new issue paste experience error successful,issue,positive,positive,positive,positive,positive,positive
678734526,"@blue-fish sorry I was testing the `toolbox/ui` change so I could give you better feedback, I tried changing it as you said but now when I run the `demo_toolbox.py` I get another error related to QT `Cannot mix incompatible Qt library (5.9.7) with this library (5.15.0)`, this happens only when using the latest matplotlib version, on older versions I dont get that error, so, another error we have to figure out.",sorry testing change could give better feedback tried said run get another error related mix incompatible library library latest version older dont get error another error figure,issue,negative,positive,positive,positive,positive,positive
678734029,"@blue-fish dont worry, I didnt understand what you meant with a `minimum reproducible example` when you asked the first time and I went and just gave you a step-by-step explanation of what I did to run the code, I understand now that it was too much to do, I got the idea better when you shared the three lines of code to test the import order, will keep that in mind for the future. :)",dont worry didnt understand meant minimum reproducible example first time went gave explanation run code understand much got idea better three code test import order keep mind future,issue,positive,positive,positive,positive,positive,positive
678733585,"@ZeroCool940711 Can you also change the order of the imports to get the toolbox to work?

I realize my previous response was a little harsh and I did not explain sufficiently what I meant by ""minimum reproducible example"" when I asked you to do it... please know that I really appreciate the effort and your help in understanding what is going on here.",also change order get toolbox work realize previous response little harsh explain sufficiently meant minimum reproducible example please know really appreciate effort help understanding going,issue,positive,negative,neutral,neutral,negative,negative
678733434,"@blue-fish I tried your test code and indeed it seems like the order of the imports is what's creating the problem.

```
from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
import matplotlib.pyplot as plt

plt.plot()
```

works only with the older version of matplotlib and gives `TypeError: 'NoneType' object is not callable` on the latest version but
```
import matplotlib.pyplot as plt
from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas

plt.plot()
```

do work on the latest version as well as the older version.",tried test code indeed like order problem import import work older version object callable latest version import import work latest version well older version,issue,negative,positive,positive,positive,positive,positive
678733034,"@ZeroCool940711 That's much too involved to be a minimal reproducible example. Can you try this on your system and see if you get the same error message:
```
from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
import matplotlib.pyplot as plt

plt.plot()
```

Now on Ubuntu 20.04, I can change the order of the imports and there is no error. This was most unexpected but I think I have an idea of what is going on behind the scenes.

```
import matplotlib.pyplot as plt
from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas

plt.plot()
```

Go to [`toolbox/ui.py`](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/toolbox/ui.py#L11) and move the `import matplotlib.pyplot as plt` line to the very top of the file. With that change I am able to use the toolbox on my python3.8 machine.

If it also works for you I will submit a pull request.",much involved minimal reproducible example try system see get error message import import change order error unexpected think idea going behind import import go move import line top file change able use toolbox python machine also work submit pull request,issue,negative,positive,positive,positive,positive,positive
678729524,"Fixed experiment, please see below for the results or https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/458#issuecomment-678664495 for how it was conducted.

Please disregard what I said here initially, those conclusions are incorrect.",fixed experiment please see please disregard said initially incorrect,issue,negative,positive,neutral,neutral,positive,positive
678725644,"@blue-fish I installed Anaconda from scratch, meaning I had a clean Python 3.7 installation, I havent tried with Python 3.8 but as you said it probably wont work. I used [this old post](https://poorlydocumented.com/2019/11/installing-corentinjs-real-time-voice-cloning-project-on-windows-10-from-scratch/) to install it on Windows 10, here is the important things that need to be installed:

* Anaconda3-2020.02-Windows-x86_64 (Python 3.7 as newer versions have by default Python 3.8 and I was to lazy to mess with changing the default python version so I used an old installer).
  
* PyTorch for CUDA 10.0
         Not 9.2, not 10.1 – only CUDA 10.0 will work.
         This was done via Python’s pip.
         Full Command used was `pip install torch==1.2.0 torchvision==0.4.0 -f https://download.pytorch.org/whl/torch_stable.html`

       After this is installed you have to remove the bundled cuDNN DLL. You can go directly to this directory using ""%userprofile%\AppData\Local\Programs\Python\Python37\Lib\site-packages\torch\lib"" if you are on Windows. Find cudnn64_7.dll and either rename it (e.g., cudnn64_7.dll.bak) or delete it.
* [NVIDIA CUDA](https://developer.nvidia.com/cuda-10.0-download-archive) 10.0
         Not 9.x, not 10.1 – only 10.0 (This is because TensorFlow >=1.10.0, <=1.14 is required for the code if using themain repo, this can probably be skipped if using the Pytorch implementation).
         Download the network install since you won’t need all the components unless you have another need for CUDA
* [NVIDIA cuDNN for CUDA](https://developer.nvidia.com/cudnn) 10.0
        Not 9.x, not 10.1 – only 10.0 will work.
        You need an NVIDIA developer account for this.
* Cloned the code from [CorentinJ/Real-Time-Voice-Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning) or from [blue-fish/Real-Time-Voice-Cloning](https://github.com/blue-fish/Real-Time-Voice-Cloning/tree/447_pytorch_synthesizer) in my case I tried both and only the Pytorch version worked for me as my CPU and GPU have troubles dealing with tensorflow.

After cloning the repo I installed the dependencies using pip with the command `pip install -r requirements.txt`, this installed the latest version of `matplotlib` (3.3.1) which caused the error on the top of this issue, here is it again for comparison:

```
D:\ZeroCool\Projects\Python\Otros\Machine Learning\Voice Cloning\Real-Time-Voice-Cloning_pytorch>python demo_toolbox.py -d datasets --low_mem
Arguments:
    datasets_root:    datasets
    enc_models_dir:   encoder\saved_models
    syn_models_dir:   synthesizer\saved_models
    voc_models_dir:   vocoder\saved_models
    low_mem:          True
    seed:             None

Traceback (most recent call last):
  File ""demo_toolbox.py"", line 39, in <module>
    Toolbox(**vars(args))
  File ""D:\ZeroCool\Projects\Python\Otros\Machine Learning\Voice Cloning\Real-Time-Voice-Cloning_pytorch\toolbox\__init__.py"", line 63, in __init__
    self.ui = UI()
  File ""D:\ZeroCool\Projects\Python\Otros\Machine Learning\Voice Cloning\Real-Time-Voice-Cloning_pytorch\toolbox\ui.py"", line 455, in __init__
    fig, self.umap_ax = plt.subplots(figsize=(3, 3), facecolor=""#F0F0F0"")
  File ""D:\Python\lib\site-packages\matplotlib\cbook\deprecation.py"", line 451, in wrapper
    return func(*args, **kwargs)
  File ""D:\Python\lib\site-packages\matplotlib\pyplot.py"", line 1271, in subplots
    fig = figure(**fig_kw)
  File ""D:\Python\lib\site-packages\matplotlib\pyplot.py"", line 677, in figure
    **kwargs)
  File ""D:\Python\lib\site-packages\matplotlib\pyplot.py"", line 299, in new_figure_manager
    return _backend_mod.new_figure_manager(*args, **kwargs)
  File ""D:\Python\lib\site-packages\matplotlib\backend_bases.py"", line 3494, in new_figure_manager
    return cls.new_figure_manager_given_figure(num, fig)
  File ""D:\Python\lib\site-packages\matplotlib\backend_bases.py"", line 3499, in new_figure_manager_given_figure
    canvas = cls.FigureCanvas(figure)
TypeError: 'NoneType' object is not callable
Error in sys.excepthook:
Traceback (most recent call last):
  File ""D:\ZeroCool\Projects\Python\Otros\Machine Learning\Voice Cloning\Real-Time-Voice-Cloning_pytorch\toolbox\__init__.py"", line 70, in excepthook
    self.ui.log(""Exception: %s"" % exc_value)
AttributeError: 'Toolbox' object has no attribute 'ui'

Original exception was:
Traceback (most recent call last):
  File ""demo_toolbox.py"", line 39, in <module>
    Toolbox(**vars(args))
  File ""D:\ZeroCool\Projects\Python\Otros\Machine Learning\Voice Cloning\Real-Time-Voice-Cloning_pytorch\toolbox\__init__.py"", line 63, in __init__
    self.ui = UI()
  File ""D:\ZeroCool\Projects\Python\Otros\Machine Learning\Voice Cloning\Real-Time-Voice-Cloning_pytorch\toolbox\ui.py"", line 455, in __init__
    fig, self.umap_ax = plt.subplots(figsize=(3, 3), facecolor=""#F0F0F0"")
  File ""D:\Python\lib\site-packages\matplotlib\cbook\deprecation.py"", line 451, in wrapper
    return func(*args, **kwargs)
  File ""D:\Python\lib\site-packages\matplotlib\pyplot.py"", line 1271, in subplots
    fig = figure(**fig_kw)
  File ""D:\Python\lib\site-packages\matplotlib\pyplot.py"", line 677, in figure
    **kwargs)
  File ""D:\Python\lib\site-packages\matplotlib\pyplot.py"", line 299, in new_figure_manager
    return _backend_mod.new_figure_manager(*args, **kwargs)
  File ""D:\Python\lib\site-packages\matplotlib\backend_bases.py"", line 3494, in new_figure_manager
    return cls.new_figure_manager_given_figure(num, fig)
  File ""D:\Python\lib\site-packages\matplotlib\backend_bases.py"", line 3499, in new_figure_manager_given_figure
    canvas = cls.FigureCanvas(figure)
TypeError: 'NoneType' object is not callable
```",anaconda scratch meaning clean python installation havent tried python said probably wont work used old post install important need python default python lazy mess default python version used old installer work done via python pip full command used pip install remove go directly directory find either rename delete code probably implementation network install since need unless another need work need developer account code case tried version worked dealing pip command pip install latest version error top issue comparison python true seed none recent call last file line module toolbox file line file line fig file line wrapper return file line fig figure file line figure file line return file line return fig file line canvas figure object callable error recent call last file line exception object attribute original exception recent call last file line module toolbox file line file line fig file line wrapper return file line fig figure file line figure file line return file line return fig file line canvas figure object callable,issue,negative,positive,positive,positive,positive,positive
678723258,"@ZeroCool940711 When you have time can you please put together a minimal reproducible example that results in this error? Then either of us can open an issue with matplotlib and get their help with it. (I searched their issues on the error message, did not find anything applicable)",time please put together minimal reproducible example error either u open issue get help error message find anything applicable,issue,negative,negative,neutral,neutral,negative,negative
678722752,"@blue-fish I think the problem might be related to changes or differences between `matplotlib` and `pillow` as in order for them to work you need specific versions of those, I tried multiple versions and the latest version of matplotlib do not because it seems to have some incompatibilities with the version of Pillow I have, if I try to install another pillow version then `matplotlib` will fail, if any of them has a version that is not  completely compatible with the other then we get an error, that's what I found but you are right, we need more people to report this issue so we can get a better idea of what's causing it, right now it can be also related to just things working different on different platforms as I'm on Windows 10 and you are on Ubuntu 20.04",think problem might related pillow order work need specific tried multiple latest version version pillow try install another pillow version fail version completely compatible get error found right need people report issue get better idea causing right also related working different different,issue,negative,positive,neutral,neutral,positive,positive
678722471,@ZeroCool940711 Let's continue the discussion about this error in #504 as it is most likely unrelated to the pytorch synthesizer.,let continue discussion error likely unrelated synthesizer,issue,negative,neutral,neutral,neutral,neutral,neutral
678722145,"@ZeroCool940711 Thank you for the suggestion but matplotlib 3.1.2 doesn't work for me, and for some reason I am unable to install any version earlier than that.

I think we should troubleshoot the error and figure out what is causing it, instead of working around it with different matplotlib versions. If you have a working setup for the toolbox why don't you just go ahead and have fun with it, and come back to this later if you are still interested. I really appreciate your willingness to help out, but until several more people report this issue it might not be worth our time fixing it.",thank suggestion work reason unable install version think error figure causing instead working around different working setup toolbox go ahead fun come back later still interested really appreciate willingness help several people report issue might worth time fixing,issue,positive,positive,neutral,neutral,positive,positive
678715203,"you're right, thank you both, you were a great help",right thank great help,issue,positive,positive,positive,positive,positive,positive
678713676,"@blue-fish sorry for taking so long to answer, I tried multiple things and seems like the `matplotlib` problem is not limited to version 2.2.4, I found that any version before `3.1.2` will work, at least on my computer, maybe its different for Ubuntu, I will keep trying other stuff, and keep you inform, if you can try using `matplotlib` version `3.1.2` and let me know if that works.",sorry taking long answer tried multiple like problem limited version found version work least computer maybe different keep trying stuff keep inform try version let know work,issue,negative,negative,negative,negative,negative,negative
678703426,"You seem to be missing Unidecode, You should `pip install -r requirements.txt` to install Unicode and anything else you might be missing.",seem missing pip install install anything else might missing,issue,negative,negative,negative,negative,negative,negative
678701289,"
here is thank you very much for helping me I don't have much knowledge forgive my ignorance

(voice-clone) C:\Users\PC\Real-Time-Voice-Cloning-master> python .\demo_toolbox.py
C:\Users\PC\Real-Time-Voice-Cloning-master\encoder\audio.py:13: UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended.
  warn(""Unable to import 'webrtcvad'. This package enables noise removal and is recommended."")
Traceback (most recent call last):
  File "".\demo_toolbox.py"", line 2, in <module>
    from toolbox import Toolbox
  File ""C:\Users\PC\Real-Time-Voice-Cloning-master\toolbox\__init__.py"", line 3, in <module>
    from synthesizer.inference import Synthesizer
  File ""C:\Users\PC\Real-Time-Voice-Cloning-master\synthesizer\inference.py"", line 1, in <module>
    from synthesizer.tacotron2 import Tacotron2
  File ""C:\Users\PC\Real-Time-Voice-Cloning-master\synthesizer\tacotron2.py"", line 1, in <module>
    from synthesizer.utils.text import text_to_sequence
  File ""C:\Users\PC\Real-Time-Voice-Cloning-master\synthesizer\utils\text.py"", line 2, in <module>
    from . import cleaners
  File ""C:\Users\PC\Real-Time-Voice-Cloning-master\synthesizer\utils\cleaners.py"", line 14, in <module>
    from unidecode import unidecode
ModuleNotFoundError: No module named 'unidecode'",thank much helping much knowledge forgive ignorance python unable import package noise removal warn unable import package noise removal recent call last file line module toolbox import toolbox file line module import synthesizer file line module import file line module import file line module import file line module import module,issue,negative,negative,neutral,neutral,negative,negative
678675533,"Oh ok i understand better, it’s tacotron1 and me is version 2... 

I also understand your « no-gate » but it will not work with my vocoder (i think ?) which is based only on Conv and no RNN 
But this is not a problem, i will keep my idea to add Dense layer and train encoder without training decoder for a few steps, i think it can be really interesting to see if it works better !",oh understand better version also understand work think based problem keep idea add dense layer train without training think really interesting see work better,issue,positive,positive,positive,positive,positive,positive
678673395,"> One really strange thing in your model is that you have no « gate projection » which is really useful to predict when the model should stop during inference

@Ananas120 please see: https://github.com/fatchord/WaveRNN/issues/74 for an explanation. Predicting empty frames actually works well enough in practice with a good model. With a bad or insufficiently trained model, it results in premature synthesis termination which I have experienced during testing.

Edit:
> i just look at your pytorch model but it is so different than mine... i can’t convert it easily they are too much different 

fatchord's synthesizer is tacotron1 and not tacotron2. Take a look at this repo, it is written in tensorflow 1.x and might match closer: https://github.com/keithito/tacotron",one really strange thing model gate projection really useful predict model stop inference ananas please see explanation empty actually work well enough practice good model bad insufficiently trained model premature synthesis termination experienced testing edit look model different mine convert easily much different synthesizer take look written might match closer,issue,positive,positive,neutral,neutral,positive,positive
678669030,"@blue-fish  i conntinue my tests but the actual model is not really good... i have OOM after 1 epoch and don’t understand why so it makes training slower and less efficient because i have to re-launch it

I try with different parameters for my loss but i think the attention can’t be learned because of my 20-frames / optimization training (because i have so many times the optimization for a 0-gate) and then i suppose it learns to generate only 0 and never 1 (then bad audios, bad inference and no attention learned)

Another supposition is that is only a too small training time but i don’t know how many times it can take
I tried to embed my dataset with the encoder of this repo but it takes so many times that i stop it 

I think i will try to check your pytorch model and see if i could not recode it in tf2.0 and transfert your weights, i think it can be a better pretrained model than a partial transfert (like i do actually)

Edit : i just look at your pytorch model but it is so different than mine... i can’t convert it easily they are too much different ^^’
But you give me a really good idea to avoid my partial transfert learning ! I will create a Dense layer after the concatenation of the encoder and speaker embedding and then my attention mechanism will have the full pretrained weights so i hope it wil help !
I think i will also make a few steps (lilke 1 epoch or 1k steps) training only the encoder and the dense layer like that i will not « damage » the pretrained attention mechanism and pre-train the new Dense layer

One really strange thing in your model is that you have no « gate projection » which is really useful to predict when the model should stop during inference",actual model really good epoch understand training le efficient try different loss think attention learned optimization training many time optimization suppose generate never bad bad inference attention learned another supposition small training time know many time take tried embed many time stop think try check model see could recode think better model partial like actually edit look model different mine convert easily much different give really good idea avoid partial learning create dense layer concatenation speaker attention mechanism full hope help think also make epoch training dense layer like damage attention mechanism new dense layer one really strange thing model gate projection really useful predict model stop inference,issue,positive,positive,positive,positive,positive,positive
678668651,"Closing this issue due to inactivity. Thanks again for sharing @DereWah . Feel free to reopen the issue if you have anything new to discuss, or new results to share.",issue due inactivity thanks feel free reopen issue anything new discus new share,issue,positive,positive,positive,positive,positive,positive
678667806,"@ZeroCool940711 Thank you for reporting that, unfortunately I get an error when downgrading to 2.2.4. Please let us know if you find other solutions that work.

@Ananas120 
Do you have anything new to report for your model training? Would like to hear wavs at some point. I suggest you open a new issue (call it ""Tensorflow 2.x implementation of SV2TTS"" or similar) and you can use that for sharing results and discussion pertaining to your models. 

> have you continued to train your model or not ? And if yes, do you have better results ?

I am optimistic we can resolve the issues of gaps (#53) by training on a better dataset with fatchord's tacotron. At a low number of synthesizer steps I can synthesize the entire ""Welcome to the toolbox!..."" text without any gaps. This is training on a [curated VCTK](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/458#issuecomment-678427075) with [preprocessing to trim silences](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/501) and punctuation removed from transcripts.

If the latest experiment goes well (reverting to Corentin's encoder) then I will attempt to train a synthesizer on the full VCTK dataset. Then if that gets good results we can release it as a baseline model to get #472 merged.",thank unfortunately get error please let u know find work ananas anything new report model training would like hear point suggest open new issue call implementation similar use discussion pertaining continued train model yes better optimistic resolve training better low number synthesizer synthesize entire welcome toolbox text without training trim punctuation removed latest experiment go well attempt train synthesizer full good release model get,issue,positive,positive,positive,positive,positive,positive
678664495,"@mueller91 wrote in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/126#issuecomment-604999848
> Dear All,
> i've downloaded the models from @sberryman and adapted the hyper parameters accordingly.
> I created a few examples with them. I observe the following:
> 
> 1. the sound quality is pretty good (clearly understandable, no bleeps or blops etc.)
> 2. the voice does not resemble the reference embedding. it's like a 'generic' voice.
> 
> 
> I wonder why that is. Did anybody else experience this?
> Thanks!

Edit: For a while I thought I was also getting a ""generic voice"" using @mbdash 315k encoder trained on LibriSpeech, VCTK and CommonVoice. Please see https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/458#issuecomment-678747512 for the new results.

The parameters for this experiment are:
* Using pytorch synthesizer #472 
* Synthesizer trained on [curated VCTK](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/458#issuecomment-678427075) with [modified hparams](https://github.com/blue-fish/Real-Time-Voice-Cloning/commit/f1e1668d9b749db1de5147200bd25954c8705cf9)
* Only trained for 10 hours on a basic GPU

(If anyone's wondering why the original results were so bad, I accidentally used the hacked 768/256 encoder from https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/458#issuecomment-673341585 with a synth trained on embeds from mbdash 315k.)",wrote dear hyper accordingly observe following sound quality pretty good clearly understandable voice resemble reference like voice wonder anybody else experience thanks edit thought also getting generic voice trained please see new experiment synthesizer synthesizer trained trained basic anyone wondering original bad accidentally used hacked trained,issue,positive,positive,positive,positive,positive,positive
678650847,"@blue-fish here is something I tried before going to sleep, I downgraded `matplotlib` to the version `2.2.4` , it seems to work for me now, at least the GUI shows, give it a try and let me know if that works, I will still try more things later and see what else can fix that, of course after my beauty sleep, I actually need it, I've been scaring too many people recently.",something tried going sleep version work least give try let know work still try later see else fix course beauty sleep actually need many people recently,issue,positive,positive,neutral,neutral,positive,positive
678648309,@ZeroCool940711 Any information you can provide on this issue would be greatly appreciated. I get the same error on Python 3.8 and Ubuntu 20.04 but it seems specific to that computer.,information provide issue would greatly get error python specific computer,issue,negative,positive,positive,positive,positive,positive
678647208,"@blue-fish Im using Python 3.7 and on Windows 10, I got the `demo_toolbox.py` to run properly and test it for a while but then I stupidly decided to upgrade pip which broke my whole python installation, Im going to sleep soon as its late where I live but when I wake up I will test everything from scratch with a clean installation of Python 3.7 using anaconda, I will let you know how things go, this would probably be really useful to know how the code behaves with a clean installation.",python got run properly test stupidly decided upgrade pip broke whole python installation going sleep soon late live wake test everything scratch clean installation python anaconda let know go would probably really useful know code clean installation,issue,negative,positive,neutral,neutral,positive,positive
678646432,"@ZeroCool940711 Thanks for testing the pytorch branch and reporting the error, and a solution. I also noticed that when running the toolbox on Python 3.8, but I don't get that error on a different computer that has Python 3.7.

I added it to a list of known issues in #472 since I didn't want to get bogged troubleshooting while testing the changes:
> Toolbox doesn't launch on Python 3.8 due to error drawing the GUI. Let's address that in a different PR.

Searching on the error message suggests it may be a [matplotlib backend issue](https://stackoverflow.com/questions/53707217/typeerror-matplotlib-in-a-python-gui). I'm going to try setting up the current toolbox on the machine that has this issue and confirm that it is not related to the pytorch synthesizer changes.",thanks testing branch error solution also running toolbox python get error different computer python added list known since want get testing toolbox launch python due error drawing let address different searching error message may issue going try setting current toolbox machine issue confirm related synthesizer,issue,negative,positive,neutral,neutral,positive,positive
678645814,"@DillFrescott We have an entry for this in our FAQ: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/431#issuecomment-664038511

Please let us know if you have any further issues installing the toolbox.",entry please let u know toolbox,issue,negative,neutral,neutral,neutral,neutral,neutral
678645682,"It would be more helpful if you posted the full traceback along with the error message. Based on limited information, try reinstalling torch.

From https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/260#issuecomment-655039901

> After some quick research I figured out, that I was using the wrong PyTorch version. I was following along your suggested setup guide from [here](https://poorlydocumented.com/2019/11/installing-corentinjs-real-time-voice-cloning-project-on-windows-10-from-scratch/), which had an install command for PyTorch. I headed over so the [PyTorch site](https://pytorch.org/get-started/locally/) and downloaded the latest version with `pip install torch==1.5.1+cpu torchvision==0.6.1+cpu -f https://download.pytorch.org/whl/torch_stable.html` (as I don't have a GPU ready).
> 
> And this fixed it!",would helpful posted full along error message based limited information try torch quick research figured wrong version following along setup guide install command headed site latest version pip install ready fixed,issue,negative,positive,positive,positive,positive,positive
678619906,"@blue-fish Im trying to run your branch with Pytorch and im getting some errors when trying to run the `demo_toolbox.py`, not sure if its something on my computer that I messed up but would appreciate some help, the `demo_cli.py` seems to work but im not completely sure as I havent been able to test it fully, I'm still downloading the datasets to test it later but it does start which is more than what I can say about the original version using Tensorflow xD, this is the error im getting when running the `demo_toolbox.py`:

```
D:\ZeroCool\Projects\Python\Otros\Machine Learning\Voice Cloning\Real-Time-Voice-Cloning_pytorch>python demo_toolbox.py
Arguments:
    datasets_root:    None
    enc_models_dir:   encoder\saved_models
    syn_models_dir:   synthesizer\saved_models
    voc_models_dir:   vocoder\saved_models
    low_mem:          False
    seed:             None

Traceback (most recent call last):
  File ""demo_toolbox.py"", line 39, in <module>
    Toolbox(**vars(args))
  File ""D:\ZeroCool\Projects\Python\Otros\Machine Learning\Voice Cloning\Real-Time-Voice-Cloning_pytorch\toolbox\__init__.py"", line 63, in __init__
    self.ui = UI()
  File ""D:\ZeroCool\Projects\Python\Otros\Machine Learning\Voice Cloning\Real-Time-Voice-Cloning_pytorch\toolbox\ui.py"", line 455, in __init__
    fig, self.umap_ax = plt.subplots(figsize=(3, 3), facecolor=""#F0F0F0"")
  File ""D:\Python\lib\site-packages\matplotlib\cbook\deprecation.py"", line 451, in wrapper
    return func(*args, **kwargs)
  File ""D:\Python\lib\site-packages\matplotlib\pyplot.py"", line 1271, in subplots
    fig = figure(**fig_kw)
  File ""D:\Python\lib\site-packages\matplotlib\pyplot.py"", line 677, in figure
    **kwargs)
  File ""D:\Python\lib\site-packages\matplotlib\pyplot.py"", line 299, in new_figure_manager
    return _backend_mod.new_figure_manager(*args, **kwargs)
  File ""D:\Python\lib\site-packages\matplotlib\backend_bases.py"", line 3494, in new_figure_manager
    return cls.new_figure_manager_given_figure(num, fig)
  File ""D:\Python\lib\site-packages\matplotlib\backend_bases.py"", line 3499, in new_figure_manager_given_figure
    canvas = cls.FigureCanvas(figure)
TypeError: 'NoneType' object is not callable
Error in sys.excepthook:
Traceback (most recent call last):
  File ""D:\ZeroCool\Projects\Python\Otros\Machine Learning\Voice Cloning\Real-Time-Voice-Cloning_pytorch\toolbox\__init__.py"", line 70, in excepthook
    self.ui.log(""Exception: %s"" % exc_value)
AttributeError: 'Toolbox' object has no attribute 'ui'

Original exception was:
Traceback (most recent call last):
  File ""demo_toolbox.py"", line 39, in <module>
    Toolbox(**vars(args))
  File ""D:\ZeroCool\Projects\Python\Otros\Machine Learning\Voice Cloning\Real-Time-Voice-Cloning_pytorch\toolbox\__init__.py"", line 63, in __init__
    self.ui = UI()
  File ""D:\ZeroCool\Projects\Python\Otros\Machine Learning\Voice Cloning\Real-Time-Voice-Cloning_pytorch\toolbox\ui.py"", line 455, in __init__
    fig, self.umap_ax = plt.subplots(figsize=(3, 3), facecolor=""#F0F0F0"")
  File ""D:\Python\lib\site-packages\matplotlib\cbook\deprecation.py"", line 451, in wrapper
    return func(*args, **kwargs)
  File ""D:\Python\lib\site-packages\matplotlib\pyplot.py"", line 1271, in subplots
    fig = figure(**fig_kw)
  File ""D:\Python\lib\site-packages\matplotlib\pyplot.py"", line 677, in figure
    **kwargs)
  File ""D:\Python\lib\site-packages\matplotlib\pyplot.py"", line 299, in new_figure_manager
    return _backend_mod.new_figure_manager(*args, **kwargs)
  File ""D:\Python\lib\site-packages\matplotlib\backend_bases.py"", line 3494, in new_figure_manager
    return cls.new_figure_manager_given_figure(num, fig)
  File ""D:\Python\lib\site-packages\matplotlib\backend_bases.py"", line 3499, in new_figure_manager_given_figure
    canvas = cls.FigureCanvas(figure)
TypeError: 'NoneType' object is not callable
```

Edit: Just in case someone finds this comment because they are getting the same error this is caused by some problem with `matplotlib` when installing it using `pip install matplotlib`, it can be fixed by installing `matplotlib` using `conda install matplotlib`, it doesnt matter what version you try to install using `pip` it will always give you the same error but any version installed with `conda` will just work.",trying run branch getting trying run sure something computer would appreciate help work completely sure havent able test fully still test later start say original version error getting running python none false seed none recent call last file line module toolbox file line file line fig file line wrapper return file line fig figure file line figure file line return file line return fig file line canvas figure object callable error recent call last file line exception object attribute original exception recent call last file line module toolbox file line file line fig file line wrapper return file line fig figure file line figure file line return file line return fig file line canvas figure object callable edit case someone comment getting error problem pip install fixed install doesnt matter version try install pip always give error version work,issue,negative,positive,positive,positive,positive,positive
678612458,"> I seem to solve this problem. It turned out that my original data has an illegal speaker folder. Thank you for helping me debug.

 @sunnnnnnnny How did you manage to find this illegal speaker folder",seem solve problem turned original data illegal speaker folder thank helping manage find illegal speaker folder,issue,negative,negative,negative,negative,negative,negative
678531802,"Please see #501 everyone. Although LibriTTS wavs are trimmed so that there no leading or trailing silence, sometimes there are huge gaps in the middle of utterances and we can remove them by preprocessing the wavs. This should help improve the issue we see with gaps when synthesizing.",please see everyone although leading trailing silence sometimes huge middle remove help improve issue see,issue,positive,positive,positive,positive,positive,positive
678470516,@Jason3018 Did that solve your problem? If not please share the full traceback that comes with the error message.,solve problem please share full come error message,issue,negative,positive,positive,positive,positive,positive
678436360,"@ustraymond This is one way of doing it:
1. Make a folder `datasets_root/SV2TTS/encoder_test/` and move some folders over from encoder to make a test set.
2. Modify [speaker_verification_dataset.py](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/a32962bb7b4827660646ac6dabf62309aea08a91/encoder/data_objects/speaker_verification_dataset.py#L11) to take 2 paths, one for training and test. Modify the DataLoader in the same file to return a training batch and test batch.
3. In encoder/train.py, you would run forward/backward pass as normal on the training batch, and then follow that up with a forward pass on the test set to get loss and eer for display purposes. I think the test part should be wrapped with `with torch.no_grad()` and you might need to set `model.eval()` too.

This would slow down training considerably if performed every step, so you might want to run this evaluation every 10 or 100 steps.",one way make folder move make test set modify take one training test modify file return training batch test batch would run pas normal training batch follow forward pas test set get loss eer display think test part wrapped might need set would slow training considerably every step might want run evaluation every,issue,negative,negative,neutral,neutral,negative,negative
678427075,"@mbdash I am training VCTK with your 315k encoder. Deliberately avoided VoxCeleb as it doesn't need to perform well on celebrities or speech data in the wild. Trying to compare results with the SV2TTS authors, so I left out p240 and p260 from the training set. I spent an hour manually curating VCTK, throwing out about half of the speakers for various reasons (no UK or Irish accent - trying to help with #388, excessive unrelated sounds like fabric rustling or deep breaths before speaking each time). I'm also removing punctuation from the transcripts as I don't have the compute power to train that aspect of it well.

For preprocessing and training I had to bring the batch size down to make it fit in my GPU's limited memory (4 GB), since the bigger encoder model is loaded in memory. There's an advantage to having a lightweight encoder for TTS.",training deliberately need perform well speech data wild trying compare left training set spent hour manually throwing half various accent trying help excessive unrelated like fabric rustling deep speaking time also removing punctuation compute power train aspect well training bring batch size make fit limited memory since bigger model loaded memory advantage lightweight,issue,positive,negative,neutral,neutral,negative,negative
678421319,"Thank you @shoegazerstella ! Do you want feedback on the model now, or wait until August 31st?

For anyone else who would like to try the above synthesizer model: here is a **[synthesizer/hparams.py](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/5110020/hparams.txt)** that is compatible with the latest changes to my [`447_pytorch_synthesizer`](https://github.com/blue-fish/Real-Time-Voice-Cloning/tree/447_pytorch_synthesizer) branch.",thank want feedback model wait august st anyone else would like try synthesizer model compatible latest branch,issue,positive,positive,positive,positive,positive,positive
678399129,"@blue-fish  have you continued to train your model or not ? And if yes, do you have better results ? 

My loss is actually at 0.95 at the beginning of the epoch but the mean loss of the last epoch was 0.89 so for tomorrow (after 4k steps more) : 
- if loss decreases below 0.7, i continue training for few days
- If loss is between 0.7 and 0.8, i create the code to embed my dataset with the encoder of this repo and continue to train my model during the day and if not below 0.7, i embed my dataset during the night
- If greater than 0.8, i stop my training, embed my dataset with the encoder of this repo and train my synthesizer with this embeddings 

I hope it will decreases below 0.7, it can be really good !",continued train model yes better loss actually beginning epoch mean loss last epoch tomorrow loss continue training day loss create code embed continue train model day embed night greater stop training embed train synthesizer hope really good,issue,positive,positive,positive,positive,positive,positive
678386581,"To get a better idea on the model performance, should the EER be calculated on voxceleb ""test"" set / at least the whole training set?

Any hints on how to modify the codes (train.py?) to do so?
Thanks!
",get better idea model performance eer calculated test set least whole training set modify thanks,issue,positive,positive,positive,positive,positive,positive
678272295,"thank you for your feedback,

Here is the update on the encoder training, with voxceleb added.
The checkpoint is been uploaded to the google drive.
https://drive.google.com/drive/folders/1QBeC8_PKFn-ZpZzsWY3vIFZKd4dk3g9O?usp=sharing


![image](https://user-images.githubusercontent.com/32403586/90892115-cc409d00-e38a-11ea-887c-cce7ba71ea87.png)

![encoder_mdl_ls_cv_vctk_vc12_umap_367500](https://user-images.githubusercontent.com/32403586/90892162-df536d00-e38a-11ea-96a6-ada12a0e7cf2.png)
",thank feedback update training added drive image,issue,negative,neutral,neutral,neutral,neutral,neutral
678120958,"Hi @blue-fish 
[Here](https://drive.google.com/file/d/1Pk65KNSWc07C9J6c1_H3smvN08ZzVdo7/view?usp=sharing) the last synth checkpoint + hparams.py
I'm OOO till August 31st so I won't be able to test it or make adjustments for further trainings before that day.
Thank you!",hi last till august st wo able test make day thank,issue,negative,positive,positive,positive,positive,positive
678011592,"@mbdash For single-speaker finetuning, you should only retrain the synth. In #437, to make things converge faster we bypass the encoder and always feed the same speaker embedding input to the synth. This means there is no benefit to encoder finetuning on a single voice; in fact it would actually be harmful and increase the amount of synth training needed.

In https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/458#issuecomment-670248226 I made the observation that there are diminishing returns from improving the encoder when a single-speaker model is the goal.",retrain make converge faster bypass always feed speaker input benefit single voice fact would actually harmful increase amount training made observation improving model goal,issue,positive,negative,neutral,neutral,negative,negative
677958101,"For my personal project, if I wanted to clone my voice or a voice actor,
should i only re-train the synth, 
or both the encoder and synth 
(using only datasets from a single voice?)",personal project clone voice voice actor single voice,issue,negative,negative,neutral,neutral,negative,negative
677910109,"Maybe you want to wait for the new encoder i am training?

I should be done in a couple days training a new encoder using 

LibriSpeech + CommonVoice + VCTK 
for 315k steps (loss < 0.005)

then adding 

VoxCeleb 1& 2 to continue the training.
Loss is currently at <=0.1 at step 344k",maybe want wait new training done couple day training new loss continue training loss currently step,issue,negative,positive,neutral,neutral,positive,positive
677901760,"Hi @shoegazerstella ! The results look and sound great but we need to put the model to the test and see whether it generalizes well to new text.

During training of the synth, at every time step the Tacotron decoder is given the previous frame of the ground-truth mel spectrogram, and predicts the current frame using that info combined with the encoder output. When generating unseen speech, there is no ground truth spectrogram to rely upon, so the decoder has no choice but to use the previous predicted output. This may cause the synth to behave wildly for long or rarely seen input sequences. So testing is the only way to find out.

Would you please upload the current model checkpoint (.pt file) along with a copy of your `synthesizer/hparams.py`?

Edit: Until a vocoder is trained at 22,050 Hz you will have to use Griffin-Lim for testing. It will sound like garbage if you connect it to the original pretrained vocoder (trained at 16,000 Hz).",hi look sound great need put model test see whether well new text training every time step given previous frame mel spectrogram current frame combined output generating unseen speech ground truth spectrogram rely upon choice use previous output may cause behave wildly long rarely seen input testing way find would please current model file along copy edit trained use testing sound like garbage connect original trained,issue,positive,positive,positive,positive,positive,positive
677838560,"> @shoegazerstella How is synthesizer training coming along?

Hi @blue-fish,
I am sharing [plots and wavs](https://drive.google.com/file/d/1hGjeQnJYkG2EKm1WXFRQZANA14zsIsN8/view?usp=sharing). It seems it has far passed the 250k steps. 
What do you think of these results?",synthesizer training coming along hi far think,issue,negative,positive,neutral,neutral,positive,positive
677764559,"I am training a Tacotron-2 model with my embeddings right now (you can see the other issue « pytorch synthesizer » in this repo where i give some results / ...) 

But i am training a tacotron-2 which is not exactly the same as the model of this repo and training with embeddings of size 64 only so i don’t know if it’s not working (actually) because the model is (a little) different or because the embedding is too small or just because it will never work with my siamese embeddings

I have no RNN but i have a GlobalMaxPooling layer so it reduces the dimension to be usable and normally the maximum of a speaker should be similar in whole audios (because it should be similar between different audios) so i don’t really understand why it doesn’t work
Because if the GlobalMaxPool is similar between 2 samples, it should be similar for an audio of arbitrary length so... it should work in theory no ? ",training model right see issue synthesizer give training exactly model training size know working actually model little different small never work layer dimension usable normally maximum speaker similar whole similar different really understand work similar similar audio arbitrary length work theory,issue,negative,positive,neutral,neutral,positive,positive
677746264,@shoegazerstella How is synthesizer training coming along?,synthesizer training coming along,issue,negative,neutral,neutral,neutral,neutral,neutral
677730886,"Thank you so much for your answer!
- It would be interesting to see if a synthesizer conditioned on your siamese embeddings can produce better audios. Are planning on trying this any time soon?
- you're still computing euclidean distance (L2 distance) which is subject to the curse of dimensionality. Still, it might still work for n=64 or even n=256.
- you probably cannot use variable length audios because your architecture does not contain a RNN / LSTM / GRU (recurrent) component. (Sidenode: These tend not to work with raw audio because the temporal dependencies are too large)
I'm curious to learn more about the results from your experiments.",thank much answer would interesting see synthesizer conditioned produce better trying time soon still distance distance subject curse dimensionality still might still work even probably use variable length architecture contain recurrent component tend work raw audio temporal large curious learn,issue,positive,positive,positive,positive,positive,positive
677730453,"@CorentinJ I pushed and reverted a commit that overwrites the tacotron model with fatchord's version in WaveRNN. It lets you see the changes that were made to adapt it for SV2TTS: https://github.com/blue-fish/Real-Time-Voice-Cloning/compare/963aab8...blue-fish:7d00b0a

Edit: Updated link. Sorry about the additional commits. I understand no one wants to review a moving target and will stop making changes.",commit model version see made adapt edit link sorry additional understand one review moving target stop making,issue,negative,negative,negative,negative,negative,negative
677726299,"@mueller91 in the approach of siamese in the paper i follow, he passes raw audio as input of the model and then less processing is needed compared to this repo (where you have to process the spectrogram)

The other point is that i don’t use the L2 loss but a BinaryCrossentropy loss on the final layer of the siamese because the siamese has an encoder and a « decoder » part : the decoder takes the 2 embeddings, calculate distance (i use euclidian) and after that, a 1-neuron dense layer with sigmoid gives the distance (between 0 and 1) or the probability that they are same (also between 0 and 1) and then you can just use a BCE loss on that decision

Another drawback of my approach is that i can’t use unfixed length audio samples (i don’t understand why, maybe not enough training)

Yes the GE2E loss with cosine similarity is a good approach too and it can be interesting to compare them with real experiments (same models, training set, metrics, ...)
For me i use the siamese because i find it fun, more expressive and my GE2E loss is not very efficient (very memory efficient) and then training is much longer ",approach paper follow raw audio input model le process spectrogram point use loss loss final layer part calculate distance use dense layer sigmoid distance probability also use loss decision another drawback approach use unfixed length audio understand maybe enough training yes gee loss cosine similarity good approach interesting compare real training set metric use find fun expressive gee loss efficient memory efficient training much longer,issue,positive,positive,positive,positive,positive,positive
677717631,"Interesting approach! Some thoughts that popped into my head:
- It is not clear to my why the siamese model should require less preprocessing. I would feed it mel-specs, just as the speaker encoder in this repo. What would the advantage be of feeding in raw audio?
- The Encoder (SE) in this repo is very similar to a siamese network (both take an input and produce a latent representation), except that the SE does not use L2 distance to compute the loss, but cosine distance; and the SE does not compute it for only a single pair, but for 640 audios in a pairwise fashion.
- L2 suffers from the curse of dimensionality, whereas cos-sim does not; thus, i'd favour the SE.

In general, i'm having a hard time to see what the siamese network should produce better embeddings? Could you elaborate on this? What am i missing?
And as for better metrics, you can just measure the cosine similarity within all pairwise-similar audios (and pairwise dissimilar audios) and get an estimate how close / far these pairs are mapped on the hypersphere by the SE.",interesting approach head clear model require le would feed speaker would advantage feeding raw audio se similar network take input produce latent representation except se use distance compute loss cosine distance se compute single pair pairwise fashion curse dimensionality whereas thus se general hard time see network produce better could elaborate missing better metric measure cosine similarity within pairwise dissimilar get estimate close far hypersphere se,issue,positive,positive,positive,positive,positive,positive
677561091,"Got it, I'll do the review when I have time. I'll try to do it within a week",got review time try within week,issue,negative,neutral,neutral,neutral,neutral,neutral
677555337,"Ready for code review. There is a pretrained model for testing purposes: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/449#issuecomment-674420484

",ready code review model testing,issue,negative,positive,positive,positive,positive,positive
677430948,"> Not enough info provided. I can't really help with this but if you answer the following questions it will improve your chances of getting a helpful response.
> 
> 1. Did you use the code from this repo? If so what modifications did you make?
> 2. Which dataset did you train on?
> 3. What are your settings? Attach the synthesizer/hparams.py file
> 4. Which vocoder are you using?
> 
> Also you should make a zip file of your audio samples and attach them here, many of us here can't download from pan.baidu.com.

1: Yes, I use this repo. I did not change anything on the model. The most modifications is the preprocess of the dataset.
2:  The dataset I have trained on is internal, and the audio is clean. The audio in my dataset is not as longer as Libirspeech.Most of the audio is between 3-5s.
3：Attach is setting.
4:  The vocoder is WaveRNN.

During training synthesizer, I found the aligments has some gap,bteween encoder and decoder. Also see the attachment. Is it has inflence?

![a5b7a944a07fea7964d8c1830649507](https://user-images.githubusercontent.com/49888410/90731061-ba63e900-e2fb-11ea-8fd3-da85840aed86.jpg)

[SV2TTS.zip](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/5101374/SV2TTS.zip)
[hparams.txt](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/5101381/hparams.txt)

 
",enough provided ca really help answer following improve getting helpful response use code make train attach file also make zip file audio attach many u ca yes use change anything model trained internal audio clean audio longer audio setting training synthesizer found gap also see attachment,issue,positive,positive,positive,positive,positive,positive
677195813,"Not enough info provided. I can't really help with this but if you answer the following questions it will improve your chances of getting a helpful response.

1. Did you use the code from this repo? If so what modifications did you make?
2. Which dataset did you train on?
3. What are your settings? Attach the synthesizer/hparams.py file
4. Which vocoder are you using?

Also you should make a zip file of your audio samples and attach them here, many of us here can't download from pan.baidu.com.",enough provided ca really help answer following improve getting helpful response use code make train attach file also make zip file audio attach many u ca,issue,positive,positive,positive,positive,positive,positive
677173819,"Excellent! Could you share the 315k pre-VoxCeleb checkpoint? My hypothesis is that the (LibriSpeech+VCTK+CommonVoice) encoder should be good enough for utterances that are recorded under similar conditions. Adding VoxCeleb should make it perform better for voice recordings gathered in the wild.

While the speaker encoder should perform better for celebrities included in that dataset, unless the TTS is also trained on similar voices I don't think it will help voice cloning of celebrities.",excellent could share hypothesis good enough similar make perform better voice wild speaker perform better included unless also trained similar think help voice,issue,positive,positive,positive,positive,positive,positive
676881112,"![encoder_mdl_ls_cv_vctk_vc12_umap_315000](https://user-images.githubusercontent.com/32403586/90711043-b7221c00-e26d-11ea-8cbd-a0858c6945dc.png)

![image](https://user-images.githubusercontent.com/32403586/90711126-e769ba80-e26d-11ea-9c5a-6636fd495490.png)


alright we are way below the 0.01 loss target, i am going to add voxceleb",image alright way loss target going add,issue,negative,neutral,neutral,neutral,neutral,neutral
676606530,"> I also notice that you are using Python 3.8. Are you aware that the toolbox only supports 3.6 and 3.7? Anything torch-based will work (encoder, vocoder) but you will not be able to use the synthesizer until #472 is merged.

As far as preprocessing and training synthesizer, I didn't face any issue.",also notice python aware toolbox anything work able use synthesizer far training synthesizer face issue,issue,negative,positive,positive,positive,positive,positive
676606297,"> I had this error on empty dirs (no audio files)
> You have to make a python script to check all folders and if only source file, del source file and folder.
> 
> I can share mine later, I am currently not in front of my PC.

Thanks! Fixed it.",error empty audio make python script check source file source file folder share mine later currently front thanks fixed,issue,negative,positive,neutral,neutral,positive,positive
676532453,"It depends what you call « state »
For me the state is the internal state of the LSTMCell of the decoder so i use the state of the model
But for you the « state » refers (i suppose) to the decoder input (the spectrogram) and then yes of course i use the true spectrogram as input and not the last decoder output

Finally i d’ont copy the model of TensorflowTTS because their architecture looks like the architecture of this repo (and so different of mine so i can’t use transfer learning with my model) but i achieved to optimize my model to run it in graph mode which is much faster 

The drawback is that i have to reduce the maximum frames to 25 instead of 50 but also with that, the training time is divided by 2 
I just have an error i don’t understand but i will solve it tomorrow and relaunch the training 
I will also test to make mean of whole gradients (or sum) to make 1 optimization instead of n_frames / 25 which is not really good x)

I shared my model implementation with the training step on my github if you want to see it ^^

Edit : finally i didn’t achieve to make sum or mean of gradients because of multiple errors (like OOM) 
Si i run it as it with the model in graph mode (that multiplies the speed of call() method by 4) so i run training with these parameters : 
- Max_train_frames : 20 (nb of frames by optimization step)
- Batch_size : 64
- 2000 batch in train-set and 600 in valid-set
- 3 epochs (to test if loss decreases below 1., if yes i wil continue and if not i will see to change my model or train a new embedding model with size 258 or use the model of this repo)

Training step is around 14sec (like the old model) but with 4x bigger batch size so only around 8h/epoch so really much better 

Edit 2 : after around 4k steps i have a loss of 0.95 (mel_postnet_loss of 0.4 and gate_loss of 0.11) so attention is not yet learned and audio result is bad but it’s only 4k steps and loss always decreases each 50 steps so if it decreases below 0.9 today it can be really interesting after a few days training !
Just a very strange thing : i have an OOM error after more than 1 epoch so don’t understand why because i don’t cache data and it already does 1 epoch so no « new bigger length » in my data because it already make training on all datas x) but no problem i save the model every hour ",call state state internal state use state model state suppose input spectrogram yes course use true spectrogram input last output finally copy model architecture like architecture different mine use transfer learning model optimize model run graph mode much faster drawback reduce maximum instead also training time divided error understand solve tomorrow relaunch training also test make mean whole sum make optimization instead really good model implementation training step want see edit finally achieve make sum mean multiple like si run model graph mode speed call method run training optimization step batch test loss yes continue see change model train new model size use model training step around sec like old model bigger batch size around really much better edit around loss attention yet learned audio result bad loss always today really interesting day training strange thing error epoch understand cache data already epoch new bigger length data already make training problem save model every hour,issue,positive,positive,neutral,neutral,positive,positive
676530029,"See https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/431#issuecomment-673555684 for some information on how to get started with training. Please note:

> First thing, we do not provide any official support for training a model with your own data, it is expected that anyone who trains a model is capable of coding in Python and solving the inevitable error messages on their own. You are welcome to open an issue if you get stuck but no one will walk you through the entire process.

You need to find a suitable dataset. Mozilla CommonVoice currently has a total of 48 hours of Portuguese speech from 744 speakers, which is a start but probably not enough.

Mozilla TTS has a model in Brazilian Portuguese. See: https://github.com/mozilla/TTS/issues/160 It doesn't support voice cloning but you could finetune a voice given enough data. However, you would need to learn how to use their repo. They do have a discussion board where questions can be asked.

This is going to be a lot of work. Plan on 20 hours if you know what you're doing and 100 hours if you don't.",see information get training please note first thing provide official support training model data anyone model capable python inevitable error welcome open issue get stuck one walk entire process need find suitable currently total speech start probably enough model see support voice could voice given enough data however would need learn use discussion board going lot work plan know,issue,positive,positive,positive,positive,positive,positive
676463590,@mbdash Looking good! You can add the VoxCeleb sets whenever convenient.,looking good add whenever convenient,issue,negative,positive,positive,positive,positive,positive
676313206,"I had this error on empty dirs (no audio files)
You have to make a python script to check all folders and if only source file, del source file and folder.

I can share mine later, I am currently not in front of my PC.",error empty audio make python script check source file source file folder share mine later currently front,issue,negative,negative,neutral,neutral,negative,negative
676023422,"> Yes yes i use the previous state to generate next frame (i keep track of the new internal state during optimization iterations)

Just to be sure, are you getting this previous state from the model, or the ground-truth spectrogram? (It should be from the spectrogram)",yes yes use previous state generate next frame keep track new internal state optimization sure getting previous state model spectrogram spectrogram,issue,positive,positive,neutral,neutral,positive,positive
675993822,"Thank you i didn’t know this repo ! i think i will try to copy their implementation of Tacotron so it can work better i hope

Yes yes i use the previous state to generate next frame (i keep track of the new internal state during optimization iterations)",thank know think try copy implementation work better hope yes yes use previous state generate next frame keep track new internal state optimization,issue,positive,positive,neutral,neutral,positive,positive
675973410,"I also notice that you are using Python 3.8. Are you aware that the toolbox only supports 3.6 and 3.7? Anything torch-based will work (encoder, vocoder) but you will not be able to use the synthesizer until #472 is merged.",also notice python aware toolbox anything work able use synthesizer,issue,negative,positive,positive,positive,positive,positive
675961949,"@Ananas120 It is my understanding that the decoder uses its previous state in generating the output for the next frame. For training we typically override the actual state with the ground truth mel spectrogram. So the previous frame of the ground truth mel and encoder output are used to predict the mel for the current frame. Are you doing this?

I do not know the answer to your question about the RNN gradient calculation.

Are you aware of [TensorflowTTS](https://github.com/TensorSpeech/TensorFlowTTS)? You might want to try that out, or compare codes.",ananas understanding previous state generating output next frame training typically override actual state ground truth mel spectrogram previous frame ground truth mel output used predict mel current frame know answer question gradient calculation aware might want try compare,issue,positive,negative,neutral,neutral,negative,negative
675943045,"Please share updates about your encoder model in #458, would be interested to see how it is working. Did you modify any hparams?",please share model would interested see working modify,issue,positive,positive,positive,positive,positive,positive
675926206,I think the problem is that you have directories with _sources.txt and no other files. You need to remove those directories. See https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/458#issuecomment-674328951,think problem need remove see,issue,negative,neutral,neutral,neutral,neutral,neutral
675924573,"> Where does it take a lot of code? Is it problematic to have that code?

Figured out a solution to this. I implemented a simple hparams override which uses `ast.literal_eval()` to convert the string representation of the values into other data types. Unlike the tensorflow class, I do not check that the overrides have the correct data type or even match to a valid key.",take lot code problematic code figured solution simple override convert string representation data unlike class check correct data type even match valid key,issue,negative,neutral,neutral,neutral,neutral,neutral
675881533,Where does it take a lot of code? Is it problematic to have that code?,take lot code problematic code,issue,negative,neutral,neutral,neutral,neutral,neutral
675614371,"@blue-fish  so... my results are bad... 
After 1-2k steps my loss was around 1.0 and now after 5-6k steps my loss is around 1.2 so... i don’t really understand why it increases... I will train untill tomorrow and if loss is not below 1.0 after the first full epoch, i will stop training and try to optimize my model to run it much faster because 15s/step is really too slow 
I don’t know if the result is bad because i use another speaker encoder or because i am so unimpatient and i just need to wait more or because my splitting-training method works bad for this architecture 

I think it can impact because i make 6 or more optimization step on the same audio (different frames) but same speaker so perhaps it should be better if i do only 1 optimization on the whole spectrogram

So i have a question : how is computed gradients in RNN ? Because something i can try is to get gradients for each sub-part of the spectrogram and then sum them or make mean (don’t know what i should do) and then make optimization with that (and then only 1 optimization / step) by summing the sub-gradients 

What do you think about that ? 

Note : i don’t know if i can do this with tensorflow but i can try if it can help my training...

I will also try to optimize my decoder because i think i make too much single operations and could make them once so i hope it will help to run faster and use less memory (to run on more than 50 frames per block)

Edit : i will also share my tf2.0 tacotron-2 implementation so if you know a little about tensorflow, don’t hesitate to say me if you see something that can be optimized ! ;)",bad loss around loss around really understand train untill tomorrow loss first full epoch stop training try optimize model run much faster really slow know result bad use another speaker unimpatient need wait method work bad architecture think impact make optimization step audio different speaker perhaps better optimization whole spectrogram question something try get spectrogram sum make mean know make optimization optimization step think note know try help training also try optimize think make much single could make hope help run faster use le memory run per block edit also share implementation know little hesitate say see something,issue,positive,negative,neutral,neutral,negative,negative
675557303,@CorentinJ Do you mind if I eliminate the feature to override hparams at the command line? It takes a lot of code: https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/contrib/training/python/training/hparam.py,mind eliminate feature override command line lot code,issue,negative,neutral,neutral,neutral,neutral,neutral
675537187,"> @CorentinJ Is there any chance you can lend some compute power

Sorry but I can't",chance lend compute power sorry ca,issue,negative,negative,negative,negative,negative,negative
675224628,"Thanks for the data point, I'll suggest adding in VoxCeleb any time after the loss is consistently less than 0.01. Please share a few UMAP plots tomorrow so we can see if there's improvement in cluster separation.",thanks data point suggest time loss consistently le please share tomorrow see improvement cluster separation,issue,positive,positive,positive,positive,positive,positive
674649707,"I should have done some more research on machine learning and python before attempting. But it's working now, so yay",done research machine learning python working,issue,negative,neutral,neutral,neutral,neutral,neutral
674649524,Not really. I think the documentation is easy enough for people to follow if they understand what they are doing. In my case I had no idea what I was doing until the end of the process when i finally figured out how larger python programs worked.,really think documentation easy enough people follow understand case idea end process finally figured python worked,issue,negative,positive,positive,positive,positive,positive
674649180,If you have any suggestions for the documentation after you get the toolbox working I would really appreciate them.,documentation get toolbox working would really appreciate,issue,negative,positive,positive,positive,positive,positive
674649159,Nevermind. Figured it out. It's working now. I'll close the issue. Thanks for the help o/,figured working close issue thanks help,issue,positive,positive,positive,positive,positive,positive
674648871,"https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models
> The archive comes with the same directory structure as the repo, and you're expected to merge its contents with the root of the repository.

@NerdIt The documentation is not particularly clear but you are supposed to unzip everything to the top level of the repo, so it will make a `saved_models` folder in each of encoder, synthesizer, and vocoder. I explain this in a little more detail here:

https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/494#issuecomment-674123036",archive come directory structure merge content root repository documentation particularly clear supposed everything top level make folder synthesizer explain little detail,issue,positive,positive,positive,positive,positive,positive
674648160,@blue-fish or is there a directory I am meant to add the model to?,directory meant add model,issue,negative,neutral,neutral,neutral,neutral,neutral
674647092,"I downloaded and unzipped the pre-trained model but it gave me the same error. I am using 
`python demo_toolbox.py -d E:\USERNAME\Downloads\pretrained`
is this correct?",model gave error python correct,issue,negative,neutral,neutral,neutral,neutral,neutral
674644484,Great! I think you're getting close. If you are going to clone a voice by loading mp3 files please make sure to install FFmpeg (a link is provided in README.md). If your source voices are in .wav format then you don't need FFmpeg.,great think getting close going clone voice loading please make sure install link provided source format need,issue,positive,positive,positive,positive,positive,positive
674642496,Never mind. I found the thread where you already solved this error. I'm downloading the pre-trained model now,never mind found thread already error model,issue,negative,neutral,neutral,neutral,neutral,neutral
674633606,"Unless I just need a data set. Actually, ready the error and warnings, I think that's exactly what I need
",unless need data set actually ready error think exactly need,issue,negative,positive,positive,positive,positive,positive
674632858,"@blue-fish well the window opened for the first time, but only for a second before crashing.

I'm getting this error.
`Traceback (most recent call last):
  File ""demo_toolbox.py"", line 32, in <module>
    Toolbox(**vars(args))
  File ""E:\USER\Anaconda\deepvoice\RealTimeVoiceCloningmMaster\toolbox\__init__.py"", line 49, in __init__
    self.reset_ui(enc_models_dir, syn_models_dir, voc_models_dir)
  File ""E:\USER\Anaconda\deepvoice\RealTimeVoiceCloningmMaster\toolbox\__init__.py"", line 97, in reset_ui
    self.ui.populate_models(encoder_models_dir, synthesizer_models_dir, vocoder_models_dir)
  File ""E:\USER\Anaconda\deepvoice\RealTimeVoiceCloningmMaster\toolbox\ui.py"", line 275, in populate_models
    raise Exception(""No encoder models found in %s"" % encoder_models_dir)
Exception: No encoder models found in encoder\saved_models
`
",well window first time second getting error recent call last file line module toolbox file line file line file line raise exception found exception found,issue,negative,positive,neutral,neutral,positive,positive
674616918,"@NerdIt If you're following an installation guide, please be aware that they were written when this toolbox required GPU and it was a real pain to set up. Likely they will lead you down a path of installing NVIDIA drivers which is really not needed since we've added CPU support. We've also removed webrtcvad as a dependency which means you can start with Python 3.7 and for the most part be up and running with the toolbox after running the commands below. Our readme documentation is a little better now.
```
pip install pip --upgrade
pip install torch
pip install -r requirements.txt
```

Vocoding is slow with CPU but for the purposes of testing out the toolbox, it's much better than spending hours trying to get it working with the GPU. Only if you like the toolbox that much should you go down that route.",following installation guide please aware written toolbox real pain set likely lead path really since added support also removed dependency start python part running toolbox running documentation little better pip install pip upgrade pip install torch pip install slow testing toolbox much better spending trying get working like toolbox much go route,issue,positive,positive,positive,positive,positive,positive
674610653,"@blue-fish ok. I most likely will have to restart the installation process through because I made the error of tampering with one of the system variables and have lost the original value. At this point, I am doing a quick backup and restoring my Windows files to this morning.",likely restart installation process made error one system lost original value point quick backup morning,issue,negative,positive,positive,positive,positive,positive
674609092,"@NerdIt We've had this reported in another issue. Please see the suggestion here: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/421#issuecomment-657180528

Let me know if it doesn't work",another issue please see suggestion let know work,issue,negative,neutral,neutral,neutral,neutral,neutral
674604024,Oh and the tool box still doesn't open lol,oh tool box still open,issue,negative,neutral,neutral,neutral,neutral,neutral
674603767,"@blue-fish ok so I seem to no longer be having the errors I had last time, but now I am getting configuration messages. I don't understand what I am meant to do with them or what is causing them.

Warning! ***HDF5 library version mismatched error***
The HDF5 header files used to compile this application do not match
the version used by the HDF5 library to which this application is linked.
Data corruption or segmentation faults may occur if the application continues.
This can happen when an application was compiled by one version of HDF5 but
linked with a different version of static or shared HDF5 library.
You should recompile the application or check your shared library related
settings such as 'LD_LIBRARY_PATH'.
You can, at your own risk, disable this warning by setting the environment
variable 'HDF5_DISABLE_VERSION_CHECK' to a value of '1'.
Setting it to 2 or higher will suppress the warning messages totally.
Headers are 1.10.6, library is 1.10.5
        SUMMARY OF THE HDF5 CONFIGURATION
        =================================

General Information:
-------------------
                   HDF5 Version: 1.10.5
                  Configured on: 2019-03-04
                  Configured by: Visual Studio 15 2017 Win64
                    Host system: Windows-10.0.17763
              Uname information: Windows
                       Byte sex: little-endian
             Installation point: C:/Program Files/HDF5

Compiling Options:
------------------
                     Build Mode:
              Debugging Symbols:
                        Asserts:
                      Profiling:
             Optimization Level:

Linking Options:
----------------
                      Libraries:
  Statically Linked Executables: OFF
                        LDFLAGS: /machine:x64
                     H5_LDFLAGS:
                     AM_LDFLAGS:
                Extra libraries:
                       Archiver:
                         Ranlib:

Languages:
----------
                              C: yes
                     C Compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx86/x64/cl.exe 19.16.27027.1
                       CPPFLAGS:
                    H5_CPPFLAGS:
                    AM_CPPFLAGS:
                         CFLAGS:  /DWIN32 /D_WINDOWS /W3
                      H5_CFLAGS:
                      AM_CFLAGS:
               Shared C Library: YES
               Static C Library: YES

                        Fortran: OFF
               Fortran Compiler:
                  Fortran Flags:
               H5 Fortran Flags:
               AM Fortran Flags:
         Shared Fortran Library: YES
         Static Fortran Library: YES

                            C++: ON
                   C++ Compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx86/x64/cl.exe 19.16.27027.1
                      C++ Flags: /DWIN32 /D_WINDOWS /W3 /GR /EHsc
                   H5 C++ Flags:
                   AM C++ Flags:
             Shared C++ Library: YES
             Static C++ Library: YES

                            JAVA: OFF
                   JAVA Compiler:

Features:
---------
                   Parallel HDF5: OFF
Parallel Filtered Dataset Writes:
              Large Parallel I/O:
              High-level library: ON
                    Threadsafety: OFF
             Default API mapping: v110
  With deprecated public symbols: ON
          I/O filters (external):  DEFLATE DECODE ENCODE
                             MPE:
                      Direct VFD:
                         dmalloc:
  Packages w/ extra debug output:
                     API Tracing: OFF
            Using memory checker: OFF
 Memory allocation sanity checks: OFF
          Function Stack Tracing: OFF
       Strict File Format Checks: OFF
    Optimization Instrumentation:
Bye...
",seem longer last time getting configuration understand meant causing warning library version error header used compile application match version used library application linked data corruption segmentation may occur application happen application one version linked different version static library recompile application check library related risk disable warning setting environment variable value setting higher suppress warning totally library summary configuration general information version visual studio win host system information sex installation point build mode optimization level linking statically linked extra yes compiler visual library yes static library yes compiler library yes static library yes compiler visual library yes static library yes compiler parallel parallel large parallel library default public external deflate decode encode direct extra output tracing memory checker memory allocation sanity function stack tracing strict file format optimization instrumentation bye,issue,positive,positive,positive,positive,positive,positive
674592059,"@blue-fish in the process of re-installing everything so I am not left with any incorrect versions
",process everything left incorrect,issue,negative,neutral,neutral,neutral,neutral,neutral
674568229,"Thanks @mbdash . Still think it would be beneficial to run it to 200-250k steps before adding in VoxCeleb to get the loss down. It would also help answer whether a good encoder can be obtained without VoxCeleb since that's a monster of a dataset.

So let it run for another 2-3 days and add in VoxCeleb at some convenient time during that interval?",thanks still think would beneficial run get loss would also help answer whether good without since monster let run another day add convenient time interval,issue,positive,positive,positive,positive,positive,positive
674566712,"With the addition of vocoder preprocess, the pytorch synthesizer fork is fully functional. It also has a pretrained model that can be used for testing purposes: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/449#issuecomment-674420484

Rayhane has a lot of hparams! To simplify things I made assumptions for settings that I don't expect most users to change. I will provide a full list when I mark it ready for review, but the list includes:
* symmetric_mels = True
* signal_normalization = True
* allow_clipping_in_normalization = True
* Generate GTA mels for vocoder preprocess

The code should be stable now, but I won't mark it as ready for review until I've retested all the functions. I expect that the lack of a suitable pretrained model is going to hold up the merge. @CorentinJ Is there any chance you can lend some compute power to training a new model on LibriSpeech/TTS if we provide the settings?",addition synthesizer fork fully functional also model used testing lot simplify made expect change provide full list mark ready review list true true true generate code stable wo mark ready review expect lack suitable model going hold merge chance lend compute power training new model provide,issue,positive,positive,positive,positive,positive,positive
674545079,"> i will bring it to 100k before adding VoxCeleb1&2, except if you have a different opinion.

It has figured out how to group utterances from the same speaker but not so much how to separate different speakers. I think you can let this go until 200-250k before adding VoxCeleb.

Also there no need to upload any .pt files at this time, just the training .png files. Though the final .pt checkpoint before adding VoxCeleb would be a helpful data point.

Edit: What is the current loss value?",bring except different opinion figured group speaker much separate different think let go also need time training though final would helpful data point edit current loss value,issue,negative,positive,neutral,neutral,positive,positive
674536390,"LibriSpeech, CommonVoice & VCTK only
model_hidden_size = 768, 
72k steps

RTVC_encoder_mdl_ls_cv_vctk_vc12 
(currently 72k steps for LibriSpeech, CommonVoice & VCTK only)
https://drive.google.com/drive/folders/1hg65MdHOA_b20RzF5roA2pnoFDZy4oWQ?usp=sharing

![encoder_mdl_ls_cv_vctk_vc12_umap_072000](https://user-images.githubusercontent.com/32403586/90335828-84de9900-dfa5-11ea-93f1-df029ba46e24.png)


Edit: I used the flag `--no_visdom` since it was not working in the view
Edit2: i will bring it to 100k before adding VoxCeleb1&2, except if you have  a different opinion.
I will try your tutorial for weight transfer later, once we have a purely trained encoder based on the presets at the beginning of this thread.",currently edit used flag since working view edit bring except different opinion try tutorial weight transfer later purely trained based beginning thread,issue,negative,positive,neutral,neutral,positive,positive
674516886,Tensorflow 1.15 is not available for Python 3.8. Make sure you're using Python 3.6 or 3.7 (check it with `python --version`).,available python make sure python check python version,issue,negative,positive,positive,positive,positive,positive
674502483,"It seems to be good ! i downgrade tensorflow to version 2.1 and the error still not occur at step 660 ! (it’s logic because i already trained this model with tf2.1 3 months ago without any issue so...) but unfortunately i had to change my callbacks because the version 2.1 and 2.3 are not identical so i have no progressbar so i can’t know the loss before it ends... (i normally have the predictions every 1k steps to check the quality of the predicted spectrogram but it’s all)
To see progress i use the tqdm progress bar but no metrics shown so... i hope it improves well :3 (in last trys with bugs, loss decreases to 2.9 around step 200 so not so bad) 
Results in... 3 days i think !",good downgrade version error still occur step logic already trained model ago without issue unfortunately change version identical know loss normally every check quality spectrogram see progress use progress bar metric shown hope well last loss around step bad day think,issue,positive,negative,neutral,neutral,negative,negative
674498909,"> @NerdIt Any updates? Did you make any progress on the toolbox setup?

I'm having an issue when installing requirements, I get the following error message:
`ERROR: Could not find a version that satisfies the requirement tensorflow==1.15 (from -r requirements.txt (line 1)) (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2, 2.3.0)
ERROR: No matching distribution found for tensorflow==1.15 (from -r requirements.txt (line 1))`",make progress toolbox setup issue get following error message error could find version requirement line error matching distribution found line,issue,negative,neutral,neutral,neutral,neutral,neutral
674475361,@NerdIt Any updates? Did you make any progress on the toolbox setup?,make progress toolbox setup,issue,negative,neutral,neutral,neutral,neutral,neutral
674475306,Closing this request for setup support due to inactivity. @jcshamir please reopen this issue when you are ready to proceed with troubleshooting. Also see my question about your installed torch version above,request setup support due inactivity please reopen issue ready proceed also see question torch version,issue,positive,positive,neutral,neutral,positive,positive
674420484,"(Removed pretrained model, it is no longer compatible with the current pytorch synthesizer.)",removed model longer compatible current synthesizer,issue,negative,neutral,neutral,neutral,neutral,neutral
674420367,"Good ! me i always try to run it before bugs but i really don’t understand some strange things... 
For instance, tensorflow runs out of memory for some optimization process but not all so... don’t understant because all my frames are of size 45 max (because of my splitting)
Another thing is the bug that crashes all process but it appears randomly x) The error is « Internal error : failed to call ThenRNNBackward with model config ... » if you already faced this issue... 

Now i am trying with a try except and will see if it solves the problem but my training process code is so bad x) but if it can work i will be happy haha

Edit : most of times the error occurs at step 30, 50, ... and now i added the try except and want to test it to see if i t works or not, the error doesn’t appear ! step 188 now and nothing x)
Loss of 2.8 at step 288, not bad i think (my loss is the sum of BCE-gate loss, MSE-mel loss and  MSE-mel-postnet loss so in fact i only have a mel-loss of 1.4 ",good always try run really understand strange instance memory optimization process size splitting another thing bug process randomly error internal error call model already faced issue trying try except see problem training process code bad work happy edit time error step added try except want test see work error appear step nothing loss step bad think loss sum loss loss loss fact,issue,negative,negative,neutral,neutral,negative,negative
674416435,"> For dropout, make sure it will always be active (so look at the NVIDIA implementation to see how they do that, i can’t help you... in tensorflow i just call directly K.dropout without using the training argument so it’s active also in inference)

Thanks for the suggestion @Ananas120 , I pushed a couple of commits to always enable the dropout in the prenet, and it is working as expected. The ""random seed"" function can be used to get deterministic behavior.

I will push a pretrained model to #472 tomorrow. My GPU is down for the moment so I can't continue the training but it is good enough for evaluation purposes. It still has gaps (due to punctuation) and leaves a lot to be desired.",dropout make sure always active look implementation see help call directly without training argument active also inference thanks suggestion ananas couple always enable dropout working random seed function used get deterministic behavior push model tomorrow moment ca continue training good enough evaluation still due punctuation leaf lot desired,issue,positive,positive,neutral,neutral,positive,positive
674410524,"We are tracking the matplotlib incompatibility in another issue, here is the workaround for this problem: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/455#issuecomment-674410447",incompatibility another issue problem,issue,negative,neutral,neutral,neutral,neutral,neutral
674410447,"Workaround is to install a matplotlib version that is lower than 3.3.

```
pip install matplotlib==3.2.2
```",install version lower pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
674406076,"Nice!! It took a while to resolve all the dataset issues but the model is finally training 😄 

Please share some visdom screenshots from time to time: https://user-images.githubusercontent.com/324437/65079232-877add80-d953-11e9-921e-abe695803f53.png

If you want to try the tutorial to transfer weights from the 768/768 encoder, it can be done without the GPU. Just add the following to the top of `encoder_train.py` before you run it, to use the CPU.
```
import os
os.environ[""CUDA_VISIBLE_DEVICES""] = """"
```

We will learn more from training from scratch than trying to transfer encoder weights. The tutorial is there if you want to learn how to transfer weights from a pretrained model to another model. It should take about 30 minutes to complete (plus compute time for training).",nice took resolve model finally training please share time time want try tutorial transfer done without add following top run use import o learn training scratch trying transfer tutorial want learn transfer model another model take complete plus compute time training,issue,positive,positive,positive,positive,positive,positive
674394883,"I just launch my tacotron training but i have really strange issue with RNN and i don’t know why... i relaunch with a little modification and will see if it works

I launch with 120 000 training set and 40 000 in validation set with batch_size of 16 and 50 frames splitting (max 150 char in input text and no limitation ond mel)
1 step takes around 11 sec so it’s very slow... i then launch for only 3 epochs and will see results... 
As i used pretrained model, i have already a loss of 2.x at step 200 and the attention mechanism is already learned (i suppose) so i hope it will be ok with only 3 epochs

The GTA is not necessary, you can train your model without using it 
For dropout, make sure it will always be active (so look at the NVIDIA implementation to see how they do that, i can’t help you... in tensorflow i just call directly K.dropout without using the training argument so it’s active also in inference)

Good luck and will tell you my results when i have something... 
I also created a predictor_callback to make prediction and inference every 1000 steps but it will only generates mel and not the corresponding audio so not very mieaningful but i can still see the attention mechanism and the similarity between mels ^^

Also due to the pretraining, the gate loss is approximately 0 so it’s also a good thing and great advantage ",launch training really strange issue know relaunch little modification see work launch training set validation set splitting char input text limitation mel step around sec slow launch see used model already loss step attention mechanism already learned suppose hope necessary train model without dropout make sure always active look implementation see help call directly without training argument active also inference good luck tell something also make prediction inference every mel corresponding audio still see attention mechanism similarity also due pretraining gate loss approximately also good thing great advantage,issue,positive,positive,neutral,neutral,positive,positive
674370713,"Tacotron2 implementation is canceled for now. Started on it and it is too much work to convert a pretrained model with known issues.

Speaking of known issues, this is the list for the pytorch synthesizer:
* Dropout may not be working (synthesized utterances always identical), this may be hurting training too. The tacotron1 paper says ""The dropout in the pre-net is critical for the model to generalize, as it provides a noise source to resolve the multiple modalities in the output distribution.""
* Still haven't reimplemented the feature to generate ground truth-aligned mels for vocoder training. `vocoder_preprocess.py` is currently broken because of it
* May need a better attention mechanism: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/449#issuecomment-674362479",implementation much work convert model known speaking known list synthesizer dropout may working always identical may hurting training paper dropout critical model generalize noise source resolve multiple output distribution still feature generate ground training currently broken may need better attention mechanism,issue,negative,positive,neutral,neutral,positive,positive
674362479,"From https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/364#issuecomment-653996443
> * You can lower the upper bound I put on utterance duration, which I suspect has for effect of removing long utterances that are more likely to have more pauses (I formally evaluated models trained this way to generate less frequent long pauses). It also trains faster and does not have drawbacks (with a good attention paradigm, the model can generate sentences longer than seen in training).

Based on experience here, fixing the attention mechanism needs to be the first step. If we reduce the max utterance duration without fixing attention, then the resulting model will have trouble synthesizing long sentences. In other words, reducing duration of training utterances is the reward for implementing a better attention paradigm.

Some alternatives are discussed and evaluated in [1910.10288](https://arxiv.org/pdf/1910.10288.pdf). In the meantime we should go back to `max_mel_frames = 900` (and accept the gaps that come along with it).
* Pytorch implementation of Dynamic Convolution Attention here: https://github.com/mindslab-ai/cotatron/blob/master/modules/attention.py
* Mozilla TTS also has some other attention mechanisms implemented on the dev branch.",lower upper bound put utterance duration suspect effect removing long likely formally trained way generate le frequent long also faster good attention paradigm model generate longer seen training based experience fixing attention mechanism need first step reduce utterance duration without fixing attention resulting model trouble long reducing duration training reward better attention paradigm go back accept come along implementation dynamic convolution attention also attention dev branch,issue,positive,positive,neutral,neutral,positive,positive
674333572,"> I have a tutorial for you @mbdash (this was a good learning experience for me too).
> 
> 1. In [`encoder/params_model.py`](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/encoder/params_model.py#L3) update `model_hidden_size = 768` if you haven't already
> 2. Initialize a new model with the correct dimensions, save it after 1 step then ctrl+c to stop training
> 
> ```
> python encoder_train.py new_model datasets_root/SV2TTS/encoder/ -b 1
> ```
> 
> 1. Verify that it generated the file `encoder/saved_models/new_model_backups/new_model_bak_000001.pt`
> 2. Download `english_run.pt`, the 768/768 English model trained to 2,143,500 steps from [#458 (comment)](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/458#issuecomment-671675613)
> 3. Save this gist to `transfer_encoder_weights.py` . Pytorch model checkpoints are simple dictionaries and it is trivial to make edits.
>    
>    * https://gist.github.com/blue-fish/194cf931ecfec1a446c35487e0f95e27
> 4. Put the files from steps 3-5 in the same location
> 5. Run the script
> 
> ```
> python transfer_encoder_weights.py
> ```
> 
> 6. It saves a file called `modified_encoder.pt`, move it to `encoder/saved_models`
> 7. We only want to train the linear transformation at the end of the model that projects the final hidden layer (size 768) down to the desired embedding size (256) so we set `requires_grad=False` on the model elements that we don't want to update
>    
>    * See the modifications here: [blue-fish@30a8c7b](https://github.com/blue-fish/Real-Time-Voice-Cloning/commit/30a8c7b4734fa1de2fea45d94da063ab1cc1ffa0)
> 8. Now train the modified model that we created
> 
> ```
> python encoder_train.py modified_encoder datasets_root/SV2TTS/encoder/
> ```
> 
> 9. Let it run until you can go 1,000 steps without the loss spiking above 0.1. At this point we will know that the nn.Linear elements are properly set for the encoder weights that we imported
> 10. Revert the changes from step 9 to re-enable grad on all model elements
> 11. Continue training using the command in step 10

Mkay.... so i guess i need to look into this tomorrow, that is a lot of step when tired.",tutorial good learning experience update already initialize new model correct save step stop training python verify file model trained comment save gist model simple trivial make put location run script python file move want train linear transformation end model final hidden layer size desired size set model want update see train model python let run go without loss spiking point know properly set revert step grad model continue training command step guess need look tomorrow lot step tired,issue,positive,positive,neutral,neutral,positive,positive
674330665,"the 1st command returned me nothing ....

However, looking at it, I found a lot of CommonVoice folders with only a source.txt file.

The command above should be ran on the dataset prior to pretraining i guess.

Update:
Ok I got leazy and basically used Bitvise sftp to list and sort by size the folders and delete any folder <=34kb
And the training started.

So basically, It is running on LibriSpeech, CommonVoice and VCTK.
and VoxCeleb 1&2 are ready in another folder.",st command returned nothing however looking found lot file command ran prior pretraining guess update got basically used list sort size delete folder training basically running ready another folder,issue,negative,positive,neutral,neutral,positive,positive
674328951,"> Exception: Can't create RandomCycler from an empty collection

That got me several times while testing out encoder training (to demonstrate https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/458#issuecomment-673341585 would work).

Basically the issue is that training is crashing on nearly empty folders (containing only `_sources.txt`), which can be dealt with easily in Linux:

(Updated command based on feedback below)
```
find datasets_root/SV2TTS/encoder -type f -name _sources.txt -empty -exec rm {} \;  #delete empty _sources.txt files
find datasets_root/SV2TTS/encoder -type d -empty -exec rmdir {} \;  #delete the containing folders
```",exception ca create empty collection got several time testing training demonstrate would work basically issue training nearly empty dealt easily command based feedback find delete empty find delete,issue,negative,positive,neutral,neutral,positive,positive
674327220,"I did re-prerpocess the dataset which had broken log files,

but I currently am still stuck here:
![image](https://user-images.githubusercontent.com/32403586/90302250-e2be9400-de72-11ea-8ff3-3b4e2196b8ee.png)
",broken log currently still stuck image,issue,negative,negative,negative,negative,negative,negative
674283279,Looks like I missed a lot but you are on the right track. Anything you modify upstream requires all downstream modules to be retrained. I don't think you are going to get lucky trying to hack existing weights into the stream. Will be interesting to hear if your linear layer idea worked though.,like lot right track anything modify upstream downstream think going get lucky trying hack stream interesting hear linear layer idea worked though,issue,positive,positive,positive,positive,positive,positive
674244178,I think i've alredy fixed) just started it on linux with gpu support. Thanks for your help,think fixed support thanks help,issue,positive,positive,positive,positive,positive,positive
674177781,@shoegazerstella Please make the change in https://github.com/blue-fish/Real-Time-Voice-Cloning/commit/5ad081ca25f9276fac31417c0bfce54c59c2a98f before testing your trained models with the toolbox. We should be using the postnet output for best results. The mel spectrograms and sample wavs from training already use the correct output so your training outputs are unaffected.,please make change testing trained toolbox output best mel sample training already use correct output training unaffected,issue,positive,positive,positive,positive,positive,positive
674176235,"If I were to start again, I'd either keep punctuation or discard it entirely by switching back to LibriSpeech. Maybe increase the max mel frames (to 600 or 700) so the synth can train on slightly more complex sentences. So disregard the suggestions in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/449#issuecomment-671071445

Also, I am not noticing much improvement in voice quality when I increased the tacotron1 layer sizes in #447 to be more in line with what we have in the current tacotron2: https://github.com/blue-fish/Real-Time-Voice-Cloning/commit/8110552273afe6eb6093faaf701be5215a8285c9#diff-a20e5738bee4a9f617e9faabe4e7e17e

For my next model I will revert those changes and initialize my weights using fatchord's pretrained tacotron1 model in the [WaveRNN repo](https://github.com/fatchord/WaveRNN), which uses LJSpeech. My results with fatchord's hparams (https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/447#issuecomment-670673528) show that it is sufficient for voice cloning.",start either keep punctuation discard entirely switching back maybe increase mel train slightly complex disregard also much improvement voice quality layer size line current next model revert initialize model show sufficient voice,issue,negative,negative,neutral,neutral,negative,negative
674149297,"@Tombaysbot Did you try CPU mode as suggested in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/477#issuecomment-670929263 ?

Enabling GPU support is a complex and highly individual process. I have not seen anything that works flawlessly for everyone and we do not have the time and resources to help with it. GPU support should get a little easier once #472 is merged because it gets rid of tensorflow, which is your issue according to the error message.",try mode support complex highly individual process seen anything work flawlessly everyone time help support get little easier rid issue according error message,issue,positive,positive,positive,positive,positive,positive
674126205,"Yeah, my inexperience is really showing here. Everything was saved in the right place but I was running python out of the wrong folder. Once I corrected this it launched perfectly. Thanks so much and sorry for the repeat issue! ",yeah inexperience really showing everything saved right place running python wrong folder corrected perfectly thanks much sorry repeat issue,issue,positive,negative,neutral,neutral,negative,negative
674123036,"@malewpro Please make sure the files can be found at the locations that the toolbox is expecting.
```
enc_model_fpath: encoder\saved_models\pretrained.pt
syn_model_dir: synthesizer\saved_models\logs-pretrained
voc_model_fpath: vocoder\saved_models\pretrained\pretrained.pt
```

If the files are in the correct location then make sure you are running python out of the Real-Time-Voice-Cloning folder since it is evaluating those paths relative to the location from which python is launched.

We really need to improve the error message, as #463, #476, #482, and now this issue all had the same problem. Perhaps also print out what files are expected where.

Before I added this check in #416 we got a lot of issues opened by people who never downloaded the pretrained models.",please make sure found toolbox correct location make sure running python folder since relative location python really need improve error message issue problem perhaps also print added check got lot people never,issue,positive,positive,positive,positive,positive,positive
674062170,@Crazyjoedevola You should see #437. The process is described there and I provide two examples with all files needed to replicate my results. Refer to https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-666099538 when you are making your custom dataset.,see process provide two replicate refer making custom,issue,negative,neutral,neutral,neutral,neutral,neutral
674029173,"Hi.

This is probably a really noob question, so Im sorry about that.
Could I take youtube video audio to use to train my model for a specific voice given that I transcribe each phrase?
This means i could not get the voice to speak specific sentences for training.

I would be super greatful if someone could describe the process in an easy to understand way ( i know everything how to prepare the audio etc, but have a hard time understanding how to proceed with training and testing of the generated voice).",hi probably really question sorry could take video audio use train model specific voice given transcribe phrase could get voice speak specific training would super someone could describe process easy understand way know everything prepare audio hard time understanding proceed training testing voice,issue,positive,positive,neutral,neutral,positive,positive
674027977,"I just launch the embedding process for my dataset but it takes very long time... the most surprising is that this is not the embedding part which is long but the audio loading (just the librosa.load function call) which is limited to around 10 load / sec (and for 260 000 audios, it takes many time...) so i think i can run my first tacotron-2 training only tomorrow

I will also share my embeddings  on my github if it can help (it’s for datasets CommonVoice (fr part) and SIWIS (fr))
I embed with the final model which has 97.5% accuracy (training and test set) and BCE-loss of 0.02 on validation set 

For the tacotron-2, i will use my pretrained model with partial transfert learning, i hope it can save me lot of time (no need to learn the attention mechanism for example, i hope all will work correctly !)",launch process long time surprising part long audio loading function call limited around load sec many time think run first training tomorrow also share help part embed final model accuracy training test set validation set use model partial learning hope save lot time need learn attention mechanism example hope work correctly,issue,positive,positive,positive,positive,positive,positive
673944854,"For guys who are trying to preprocess VoxCeleb2:
once you download the dataset, the audio files are in "".m4a"" format. You guys need to reformat the audio files into "".wav"".
Just put the following code snippet `convert.sh` (need to save it as `.sh`) in the root directory of the data (e.g. `<path-to-VoxCeleb2>/raw/dev/aac`)
[convert.txt](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/5073347/convert.txt)

then run 
`./convert.sh`

Also, make sure you have ffmpeg installed on you machine.
You also need to modify the function  `preprocess_voxceleb2` in `encode/preprocess.py`, and change the extension to "".wav"".

I'm training a new encoder with more datasets other than Libri/Vox1, 2 and will update everyone in a few days.",trying audio format need audio put following code snippet need save root directory data run also make sure machine also need modify function change extension training new update everyone day,issue,positive,positive,positive,positive,positive,positive
673889442,"> It looks like the synth needs to be retrained.

To make the synth more portable we could run the speaker embedding through a linear projection before the concat with the encoder output. Then to make the synth compatible with a new encoder, we can use the same trick where we `requires_grad=False` on all model elements except the linear projection to train it. After the loss comes down we can re-enable grad to finetune the synth.",like need make portable could run speaker linear projection output make compatible new use trick model except linear projection train loss come grad,issue,negative,positive,positive,positive,positive,positive
673770916,"A bug to fix with librosa 0.8.0 (this may not be the only one):
> When I try to train vocoder I have this issue:
> 
> ```
> {| Epoch: 1 (24/24) | Loss: 5.4869 | 1.9 steps/s | Step: 0k | }
> | Generating: 1/5
> Traceback (most recent call last):
>   File ""vocoder_train.py"", line 55, in <module>
>     train(**vars(args))
>   File ""/home/roma/Real-Time-Voice-Cloning/vocoder/train.py"", line 126, in train
>     hp.voc_target, hp.voc_overlap, model_dir)
>   File ""/home/roma/Real-Time-Voice-Cloning/vocoder/gen_wavernn.py"", line 23, in gen_testset
>     save_wav(x, save_path.joinpath(""%dk_steps_%d_target.wav"" % (k, i)))
>   File ""/home/roma/Real-Time-Voice-Cloning/vocoder/audio.py"", line 23, in save_wav
>     librosa.output.write_wav(path, x.astype(np.float32), sr=hp.sample_rate)
> AttributeError: module 'librosa' has no attribute 'output'
> ```

_Originally posted by @rlutsyshyn in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/486#issuecomment-673451741_",bug fix may one try train issue epoch loss step generating recent call last file line module train file line train file line file line path module attribute posted,issue,negative,neutral,neutral,neutral,neutral,neutral
673690870,"> the encoder is trained in english so don’t know if it is portable for other languages 

The English encoder works all right for Swedish. There's info on setting it up and samples in #257 . Since encoder training is very intensive, you should just try it (either jump straight to synth preprocess and training, or do some speaker verification with Ukrainian utterances to see how well it performs).",trained know portable work right setting since training intensive try either jump straight training speaker verification see well,issue,negative,positive,positive,positive,positive,positive
673670822,"Hello, i will also try to train a voice cloning model in another language (in Fr for me) and i have some tricks for you if it can help your : 
- First : use transfert-learning (load the checkpoint with your custom model, normally the last layer doesn’t have same shape but all other yes so load weights for other layers), it will really speed up training
- Secondly, like @blue-fish  says, the encoder is trained in english so don’t know if it is portable for other languages (instead you can use my approach of the siamese network, i trained it on a mixt of En and Fr dataset and have awesome results so it’s compatible with multi-language) 
- Another thing you can do is to test the encoder with your dataset to see if results are good or not (if not, you can retrain-it on your data)
- Last thing, normally the vocoder is a universal vocoder and so it should be trained to convert mel-spect to wav form (no matter the language) so i think it’s only a bonus to fine tune it on your language but normally it will give good results as it

Good luck for training !",hello also try train voice model another language help first use load custom model normally last layer shape yes load really speed training secondly like trained know portable instead use approach network trained en awesome compatible another thing test see good data last thing normally universal trained convert form matter language think bonus fine tune language normally give good good luck training,issue,positive,positive,positive,positive,positive,positive
673595086,"Update : 97% accuracy with 0.02 BCE val-loss for the best model (which i used for the embedding plot)
![training](https://user-images.githubusercontent.com/45139111/90164218-660ab780-dd97-11ea-8f80-d0a23da84e76.png)

![embedding_plot](https://user-images.githubusercontent.com/45139111/90164205-60ad6d00-dd97-11ea-93d4-6027a6a97202.png)
",update accuracy best model used plot training,issue,positive,positive,positive,positive,positive,positive
673579098,"I am closing this issue due to inactivity, please feel free to reopen when ready to continue to the discussion.",issue due inactivity please feel free reopen ready continue discussion,issue,positive,positive,positive,positive,positive,positive
673573664,"Which version of torch do you have? You can check it by launching Python and typing these commands. For example I have 1.5.1:
```
>>> import torch
>>> torch.__version__
'1.5.1'
>>>
```",version torch check python example import torch,issue,negative,neutral,neutral,neutral,neutral,neutral
673556216,I added an entry to the FAQ since most users are going to have questions about this: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/431#issuecomment-673555684,added entry since going,issue,negative,neutral,neutral,neutral,neutral,neutral
673555684,"### How to train your own models

First thing, we do not provide any official support for training a model with your own data, it is expected that anyone who trains a model is capable of coding in Python and solving the inevitable error messages on their own. You are welcome to open an issue if you get stuck but no one will walk you through the entire process.

Most users will want to train a synthesizer model. In most cases the pretrained encoder and vocoder can be reused.

### 1. Practice training with LibriSpeech

Corentin has a wiki page for replicating the training of the pretrained models: https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training

I recommend that you work through the preprocessing and training steps for the synthesizer, using the LibriSpeech `train-clean-100` and `train-clean-360` datasets. It is not necessary to wait until the model is fully trained but you should verify that the code works on your platform before switching to your own dataset.

### 2. Dataset preparation

Assembling the dataset is perhaps the hardest part for most users. For training with your own data, you will need to get your dataset into this format: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-666099538

Once you successfully preprocess the data then the training commands will work as before.

### Additional notes
#### Finetuning a single-speaker model
If you do not require a multi-speaker model, use the process in #437 to finetune the existing models to a single speaker. This can be done in a reasonable amount of time on CPU.

#### Considerations - languages other than English
1. Update `synthesizer/utils/symbols.py` to contain all valid characters in your text transcripts (the characters you want to train on). This is an example for Swedish: https://github.com/blue-fish/Real-Time-Voice-Cloning/commit/3eb96df1c6b4b3e46c28c6e75e699bffc6dd43be However, be aware that in order for someone to run the model you've created, they will also need to make the same changes to the symbols file.
2. For best results with multi-speaker models (for voice cloning)
    * The speaker encoder is trained on English and may not work well for other languages. If you have a large number of voice samples in your target language, you may wish to train a new encoder or at least finetune an existing one. Data preprocessing for encoder is not a smooth process, so set your expectations accordingly.
    * There are some very good speaker encoders shared in #126 but the model size of 768 is too big to be practical for cloning. You can use this process to import the relevant weights from the model and finetune to a more useful dimension: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/458#issuecomment-673341585",train first thing provide official support training model data anyone model capable python inevitable error welcome open issue get stuck one walk entire process want train synthesizer model practice training page training recommend work training synthesizer necessary wait model fully trained verify code work platform switching preparation perhaps part training data need get format successfully data training work additional model require model use process single speaker done reasonable amount time update contain valid text want train example however aware order someone run model also need make file best voice speaker trained may work well large number voice target language may wish train new least one data smooth process set accordingly good speaker model size big practical use process import relevant model useful dimension,issue,positive,positive,positive,positive,positive,positive
673545051,"Yes, you can resume training on a pretrained model using a different dataset. The main use for this is single-speaker finetuning (process and examples in #437) but you could also finetune multi-speaker using the same process.

One more thing to add, the speaker encoder is trained on English and may not work well for other languages. If you have a large number of voice samples in your target language, you may wish to train a new encoder or at least finetune an existing one. (Data preprocessing for encoder is not a smooth process so set your expectations accordingly).

There are some very good speaker encoders shared in #126 but the model size of 768 is too big to be practical for cloning. You can use this process to import the relevant weights from the model and finetune to a more useful dimension: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/458#issuecomment-673341585",yes resume training model different main use process could also process one thing add speaker trained may work well large number voice target language may wish train new least one data smooth process set accordingly good speaker model size big practical use process import relevant model useful dimension,issue,positive,positive,positive,positive,positive,positive
673540454,Can you also tell me - can I somehow fine tune pretrained model on some new voice samples without full retraining?,also tell somehow fine tune model new voice without full,issue,negative,positive,positive,positive,positive,positive
673527262,"Thank you for reporting this bug, we should make `seed` an argument to `_one_shot_synthesize_spectrograms`. I have pushed a fix for this in the [`491_lowmem_synthesize_error`](https://github.com/blue-fish/Real-Time-Voice-Cloning/tree/491_lowmem_synthesize_error) branch of my fork, but I am unable to test it as I do not have a GPU available.

You can get this fix with the following git command, or download a [zip file](https://github.com/blue-fish/Real-Time-Voice-Cloning/archive/491_lowmem_synthesize_error.zip):
```
git clone -b 491_lowmem_synthesize_error --depth 1 https://github.com/blue-fish/Real-Time-Voice-Cloning
```

I do not plan to submit a PR because we will migrate to a torch-based synthesizer shortly.",thank bug make seed argument fix branch fork unable test available get fix following git command zip file git clone depth plan submit migrate synthesizer shortly,issue,negative,negative,neutral,neutral,negative,negative
673511541,"Update `synthesizer/utils/symbols.py` to contain all valid characters in your text transcripts (the characters you want to train on). This is an example for Swedish: https://github.com/blue-fish/Real-Time-Voice-Cloning/commit/3eb96df1c6b4b3e46c28c6e75e699bffc6dd43be

However, be careful: in order for someone to run the model you've created they will also need to make the same changes to the file. I spent hours learning this the hard way trying to use the model in #257 because the creator was unavailable to help.",update contain valid text want train example however careful order someone run model also need make file spent learning hard way trying use model creator unavailable help,issue,negative,negative,negative,negative,negative,negative
673506440,"No problem, i am still learning too ;) 

I updated my converter code to make it more robusts 
I also write all the pipeline and the model so, in theory, the only things i need to add is the speaker embedding and i can run it !
My training hyperparameters were 50 frames / step (splitting) with batch_size of 16 and max text length of 150 (char) but i have a 6GB GPU memory so... i suppose my model have more weights... i have a trining time of 2-4 sec/step if i remember (1 step = 1 full audio so 8 optimization step on 50 frames for a 400 frames mel spectrogram)",problem still learning converter code make also write pipeline model theory need add speaker run training step splitting text length char memory suppose model time remember step full audio optimization step mel spectrogram,issue,negative,positive,positive,positive,positive,positive
673474616,"I restarted the training from scratch with the correct number of samples, I am now at step 8k.
I will share later some spectrogram plots + wavs.",training scratch correct number step share later spectrogram,issue,negative,neutral,neutral,neutral,neutral,neutral
673416870,"@shoegazerstella If restricting the max mel frames length to 500, I have 76,153 samples. (The other number is using the default of 900) So everything seems to be working well now!",mel length number default everything working well,issue,negative,neutral,neutral,neutral,neutral,neutral
673414329,"You are right, now I see:
`Found 76052 samples`

I should also mention this log from the preprocessing:

```
The dataset consists of 76052 utterances, 21949820 mel frames, 6025708370 audio timesteps (75.91 hours).
Max input length (text chars): 158
Max mel frames length: 500
Max audio timesteps length: 137374
```

So this seems to be in line with it!",right see found also mention log mel audio input length text mel length audio length line,issue,negative,positive,positive,positive,positive,positive
673410935,"Does it help if you change line 13 synthesizer_dataset.py to:

```
with metadata_fpath.open(""r"", encoding=""utf-8"") as metadata_file:
```

I think your system locale causes files to be saved as utf-8 by default so certain characters are out of range when loading them as ascii.",help change line think system locale saved default certain range loading ascii,issue,positive,positive,positive,positive,positive,positive
673407661,"This is in your traceback: `assert text_fpath.exists()`

Please check that for every `filename.wav` in your folder, there is a corresponding `filename.txt` in the same location.",assert please check every folder corresponding location,issue,negative,neutral,neutral,neutral,neutral,neutral
673406691,"And @sberryman is right! It looks like the synth needs to be retrained. I finetuned the 768/256 encoder to 2,144,100 steps (added 600 steps) and get garbage out when I try to synthesize text. This result makes sense in context of how the encoder is optimized; for a given utterance the loss function doesn't care about the specific values of the embedding as long as it is close to other embeds derived from the same speaker, and far from utterance embeds of other speakers.",right like need added get garbage try synthesize text result sense context given utterance loss function care specific long close derived speaker far utterance,issue,positive,positive,neutral,neutral,positive,positive
673405719,"Okay, understood. I have created dataset for new training (just for testing used one speaker), when I start synthesizer_preprocess_audio.py first it seems good, but after I have an error like that:
```
Arguments:
    datasets_root:   datasets
    out_dir:         datasets/SV2TTS/synthesizer
    n_processes:     None
    skip_existing:   False
    hparams:         
    no_alignments:   True
    datasets_name:   Ukrainian
    subfolders:      female

Using data from:
    datasets/Ukrainian/female
Ukrainian:   0%|                                                         | 0/1 [01:03<?, ?speakers/s]
multiprocessing.pool.RemoteTraceback: 
""""""
Traceback (most recent call last):
  File ""/home/roma/miniconda3/envs/work/lib/python3.7/multiprocessing/pool.py"", line 121, in worker
    result = (True, func(*args, **kwds))
  File ""/home/roma/Стільниця/Work/NMT/Real-Time-Voice-Cloning/synthesizer/preprocess.py"", line 76, in preprocess_speaker
    assert text_fpath.exists()
AssertionError
""""""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""synthesizer_preprocess_audio.py"", line 59, in <module>
    preprocess_dataset(**vars(args))
  File ""/home/roma/Стільниця/Work/NMT/Real-Time-Voice-Cloning/synthesizer/preprocess.py"", line 35, in preprocess_dataset
    for speaker_metadata in tqdm(job, datasets_name, len(speaker_dirs), unit=""speakers""):
  File ""/home/roma/miniconda3/envs/work/lib/python3.7/site-packages/tqdm/std.py"", line 1129, in __iter__
    for obj in iterable:
  File ""/home/roma/miniconda3/envs/work/lib/python3.7/multiprocessing/pool.py"", line 748, in next
    raise value
AssertionError
```
",understood new training testing used one speaker start first good error like none false true female data recent call last file line worker result true file line assert exception direct cause following exception recent call last file line module file line job file line iterable file line next raise value,issue,negative,positive,positive,positive,positive,positive
673403569,"```
2020-08-12 15:33:52 - INFO - b'Starting the training of Tacotron from scratch\n'
2020-08-12 15:33:52 - INFO - b'\n'
2020-08-12 15:33:52 - INFO - b'Using inputs from:\n'
2020-08-12 15:33:52 - INFO - b'\t/opt/ml/input/data/train/train.txt\n'
2020-08-12 15:33:52 - INFO - b'\t/opt/ml/input/data/train/mels\n'
2020-08-12 15:33:52 - INFO - b'\t/opt/ml/input/data/train/embeds\n'
2020-08-12 15:33:52 - INFO - b'Traceback (most recent call last):\n'
2020-08-12 15:33:52 - INFO - b'  File ""synthesizer_train.py"", line 33, in <module>\n'
2020-08-12 15:33:52 - INFO - b'    train(**vars(args))\n'
2020-08-12 15:33:52 - INFO - b'  File ""/root/voicecloning/synthesizer/train.py"", line 112, in train\n'
2020-08-12 15:33:52 - INFO - b'    dataset = SynthesizerDataset(metadata_fpath, mel_dir, embed_dir)\n'
2020-08-12 15:33:52 - INFO - b'  File ""/root/voicecloning/synthesizer/synthesizer_dataset.py"", line 14, in __init__\n'
2020-08-12 15:33:52 - INFO - b'    metadata = [line.split(""|"") for line in metadata_file]\n'
2020-08-12 15:33:52 - INFO - b'  File ""/root/voicecloning/synthesizer/synthesizer_dataset.py"", line 14, in <listcomp>\n'
2020-08-12 15:33:52 - INFO - b'    metadata = [line.split(""|"") for line in metadata_file]\n'
2020-08-12 15:33:52 - INFO - b'  File ""/opt/conda/lib/python3.6/encodings/ascii.py"", line 26, in decode\n'
2020-08-12 15:33:52 - INFO - b'    return codecs.ascii_decode(input, self.errors)[0]\n'
2020-08-12 15:33:52 - INFO - b""UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 1481: ordinal not in range(128)\n""
```",training recent call last file line module train file line file line line file line line file line return input ca decode position ordinal range,issue,negative,neutral,neutral,neutral,neutral,neutral
673397231,"I apologize, I am not available to provide consultation outside of the issues board here. For now, my priorities are 1) code development and 2) bug fixes. I answer support questions as time permits but that is not my purpose here.",apologize available provide consultation outside board code development bug answer support time purpose,issue,negative,positive,positive,positive,positive,positive
673396909,"Hi @blue-fish thanks a lot for your help! 
Training is now in progress, the configuration follows the parameters you suggested above.

I had another little issue similar to [https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/439#issuecomment-673349904](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/439#issuecomment-673349904), thus it seems it is processing 24353 samples only. Is that correct? thanks!

```
Initialising Tacotron Model...

Trainable Parameters: 24.888M

Starting the training of Tacotron from scratch

Using inputs from:
        DATA/SV2TTS/synthesizer/train.txt
        DATA/SV2TTS/synthesizer/mels
        DATA/SV2TTS/synthesizer/embeds

Found 24353 samples
+----------------+------------+---------------+------------------+
| Steps with r=7 | Batch Size | Learning Rate | Outputs/Step (r) |
+----------------+------------+---------------+------------------+
|   10k Steps    |     32     |     0.001     |        7         |
+----------------+------------+---------------+------------------+

/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:211: RuntimeWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
  self.dropout, self.training, self.bidirectional, self.batch_first)
| Epoch: 1/14 (762/762) | Loss: 0.8026 | 1.0 steps/s | Step: 0k | [[B
| Epoch: 2/14 (762/762) | Loss: 0.7637 | 1.0 steps/s | Step: 1k |
| Epoch: 3/14 (476/762) | Loss: 0.7511 | 1.0 steps/s | Step: 2k | Input at step 2000: my dear child, i said grandly, do you really suppose i am afraid of that poor wretch?~__________________________
| Epoch: 3/14 (762/762) | Loss: 0.7460 | 1.0 steps/s | Step: 2k |
| Epoch: 4/14 (361/762) | Loss: 0.7274 | 1.0 steps/s | Step: 2k | 
```

",hi thanks lot help training progress configuration another little issue similar thus correct thanks model trainable starting training scratch found batch size learning rate module part single contiguous chunk memory need compacted every call possibly greatly increasing memory usage compact call epoch loss step epoch loss step epoch loss step input step dear child said grandly really suppose afraid poor wretch epoch loss step epoch loss step,issue,negative,positive,neutral,neutral,positive,positive
673391861,@shoegazerstella You might want to run synthesizer_train.py with `-s 500` to save the model every 500 steps (that way you do not lose too much progress when stopping and restarting),might want run save model every way lose much progress stopping,issue,negative,positive,positive,positive,positive,positive
673389520,"Going to close this issue, please share updates in #449. Thanks @shoegazerstella !",going close issue please share thanks,issue,positive,positive,positive,positive,positive,positive
673389303,"Nice! I pushed the fix. About the warning, you should compare training speed for single GPU and multi-GPU to make sure it is not adding too much overhead.

Next, monitor the GPU and memory usage with `nvidia-smi`, you can `watch -n 0.5 nvidia-smi` to constantly refresh. You can adjust batch size until your GPU memory is filled (leave 20% as mem usage increases during training). It is safe to stop and resume training.

I would recommend not getting too attached to your first few models, the time is better spent learning what works and adjusting the training schedule for maximum efficiency. Train it to 20k steps, listen to the wavs and look at the plots, and try it in the toolbox (set your expectations low until it gets to 100k steps).",nice fix warning compare training speed single make sure much overhead next monitor memory usage watch constantly refresh adjust batch size memory filled leave mem usage training safe stop resume training would recommend getting attached first time better spent learning work training schedule maximum efficiency train listen look try toolbox set low,issue,positive,positive,positive,positive,positive,positive
673380193,"Thank you so much for the code! I am still learning so that saves me a lot of time.

> 2 questions to be sure before my training :
> 
> * CommonVoice fr has many accents, i think it could be better to use only french accent no ?

You're talking about spoken accent (and not text accents like è)? Ideally you would label the various accents using a style token (#230) but we do not have the feature yet. That would enable you to synthesize a specific accent later on. I think it is preferable to not mix accents in your training data. But the training process should be robust and still produce a usable result even if you have multiple accents in your data.

As for text accents make sure your `synthesizer/utils/symbols.py` has all of the symbols that are used in your transcripts (at least the ones you want to train on). This is what I'm talking about (other tacotron implementations also use the file): https://github.com/blue-fish/Real-Time-Voice-Cloning/commit/3eb96df1c6b4b3e46c28c6e75e699bffc6dd43be

> * In fact, the only difference with the original Tacotron-2 model is that the output of the encoder is concatenated with the speaker embedding, no other changes in architecture itself ?

Yes, this is correct. That's how it is in the SV2TTS paper. I also painstakingly diffed Rayhane's tacotron2 and this one and came to the same conclusion.",thank much code still learning lot time sure training many think could better use accent talking spoken accent text like ideally would label various style token feature yet would enable synthesize specific accent later think preferable mix training data training process robust still produce usable result even multiple data text make sure used least want train talking also use file fact difference original model output speaker architecture yes correct paper also painstakingly one came conclusion,issue,positive,positive,positive,positive,positive,positive
673377918,"Awesome! It seems it's working. There's just a new warning I am reporting if you need it for reference:

```
    models_dir:      DATA/models
    save_every:      1000
    backup_every:    25000
    force_restart:   False

Checkpoint path: DATA/models/synthesizer_model/pretrained.pt
Loading training data from: DATA/SV2TTS/synthesizer/train.txt
Using model: Tacotron
Using device: cuda

Initialising Tacotron Model...

Trainable Parameters: 24.888M

Starting the training of Tacotron from scratch

Using inputs from:
        DATA/SV2TTS/synthesizer/train.txt
        DATA/SV2TTS/synthesizer/mels
        DATA/SV2TTS/synthesizer/embeds
LEN METADATA 24353
Found 24353 samples
+----------------+------------+---------------+------------------+
| Steps with r=7 | Batch Size | Learning Rate | Outputs/Step (r) |
+----------------+------------+---------------+------------------+
|   10k Steps    |     32     |     0.001     |        7         |
+----------------+------------+---------------+------------------+

/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:211: RuntimeWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
  self.dropout, self.training, self.bidirectional, self.batch_first)
| Epoch: 1/14 (41/762) | Loss: 1.793 | 0.92 steps/s | Step: 0k | 
```

Note that if I load the model created with the old code I have this error, I had to train from scratch in order to make it work.

```
Loading weights at DATA/models/synthesizer_model/synthesizer_model.pt
Traceback (most recent call last):
  File ""synthesizer_train.py"", line 35, in <module>
    train(**vars(args))
  File ""/root/voicecloning/synthesizer/train.py"", line 105, in train
    model.load(weights_fpath, optimizer)
  File ""/root/voicecloning/synthesizer/models/tacotron.py"", line 497, in load
    optimizer.load_state_dict(checkpoint[""optimizer_state""])
  File ""/opt/conda/lib/python3.6/site-packages/torch/optim/optimizer.py"", line 115, in load_state_dict
    raise ValueError(""loaded state dict contains a parameter group ""
ValueError: loaded state dict contains a parameter group that doesn't match the size of optimizer's group
```",awesome working new warning need reference false path loading training data model device model trainable starting training scratch found batch size learning rate module part single contiguous chunk memory need compacted every call possibly greatly increasing memory usage compact call epoch loss step note load model old code error train scratch order make work loading recent call last file line module train file line train file line load file line raise loaded state parameter group loaded state parameter group match size group,issue,negative,positive,positive,positive,positive,positive
673376034,"Hey, how can I contact you? I have some more questions but here is not comfortable to ask it.",hey contact comfortable ask,issue,positive,positive,positive,positive,positive,positive
673373603,"@shoegazerstella I'd like to try fixing this. It will also make your training faster if it works. When you get a chance, could you try changing these lines in synthesizer/model/tacotron.py. Then comment out the `os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""` in synthesizer_train.py and see if multi-GPU training will work?

If it doesn't fix the problem you should revert the change because I noticed a slight speed improvement with the current code.

### Old
```
self.step = nn.Parameter(torch.zeros(1).long(), requires_grad=False)
self.stop_threshold = nn.Parameter(torch.tensor(stop_threshold).float32(), requires_grad=False)
```
### New
```
self.register_buffer('step', torch.zeros(1, dtype=torch.long))
self.register_buffer('stop_threshold', torch.tensor(stop_threshold, dtype=torch.float32))
```

I made this change since Corentin did something similar when he converted fatchord's vocoder, but now I am wondering if it breaks multi-GPU.

https://github.com/blue-fish/Real-Time-Voice-Cloning/commit/7760081087b57b1a953525ac0bca6213879d2cea#diff-aae6b44cd4ebc2321fee5d9ef4c851ef",like try fixing also make training faster work get chance could try comment see training work fix problem revert change slight speed improvement current code old new made change since something similar converted wondering,issue,positive,positive,neutral,neutral,positive,positive
673362112,"Here is the code (it's just a modified version of my get_tf_layer to use checkpoint variables instead of model variables)
```python
import tensorflow as tf

def get_ckpt_layers(ckpt):
    layers = {}
    for name, shape in ckpt:
        layer_name = '/'.join(name.split('/')[:-1])
        # To remove duplicata variables layer of the checkpoint
        if len(layer_name.split('/')) > 3 and layer_name.split('/')[2] == 'inference': continue
        # print(name) # uncomment to show the full name of variable
        if layer_name not in layers: layers[layer_name] = []
        layers[layer_name].append(shape)
    return layers

path = 'your_path_to_ckpt_dir'

ckpt = tf.train.list_variables(path) # get variables in the checkpoint as list of tuple [(name, shape), ...]

layers = get_ckpt_layers(ckpt)
for name, shapes in layers.items():
    print(""{} : {}"".format(name, shapes))

### To load variables
variables = {n : tf.train.load_variable(path, n) for n, _ in ckpt]
```

I am adapting my model to accept embedding vector, i think i will launch first training only tomorrow or this week-end because i also have to create all embeddings vectors for my 2 datasets and before that i would like to train more my siamese 

2 questions to be sure before my training : 
- CommonVoice fr has many accents, i think it could be better to use only french accent no ? 
- In fact, the only difference with the original Tacotron-2 model is that the output of the encoder is concatenated with the speaker embedding, no other changes in architecture itself ?

Good luck for your convertion to pytorch !",code version use instead model python import name shape remove layer continue print name show full name variable shape return path path get list name shape name print name load path model accept vector think launch first training tomorrow also create would like train sure training many think could better use accent fact difference original model output speaker architecture good luck,issue,positive,positive,positive,positive,positive,positive
673359956,"Thanks a lot! 
This solves the issue and now I am able to start the training phase.",thanks lot issue able start training phase,issue,negative,positive,positive,positive,positive,positive
673358005,"@shoegazerstella I was not able to test parallel GPU training during development since I don't have that kind of hardware. You can add this code to the top of synthesizer_train.py to make it only run on a single GPU for now.

```
import os
os.environ[""CUDA_VISIBLE_DEVICES""] = ""0"" # Set to the GPU you want to use
```",able test parallel training development since kind hardware add code top make run single import o set want use,issue,positive,positive,positive,positive,positive,positive
673355768,"> Main differences :
>     * Some bias are used in mine but not the other (or in the other sense)
>     * Order and names are not the same (really problematic to convert them...)
>     * He uses a 2-frames by step (me 1 by step)
>     * He uses a linear layer as projection for postnet (me is a convolution)
>     * I don't find the ""value layer"" in the attention layer but instead i have this :
>       """"""
>       Tacotron_model/inference/decoder/Location_Sensitive_Attention : [[128], [128]]
>       // Full name of the variables are :
>       Tacotron_model/inference/decoder/Location_Sensitive_Attention/attention_bias
>       Tacotron_model/inference/decoder/Location_Sensitive_Attention/attention_variable_projection
>       """"""
>       And i don't have the equivalent in my layer, very strange
> 
> 
> So... they are a lot of changes in architecture, do you think it is interesting to try to convert weights to my model (if i make the changes) or not ? Because i can make changes in term of architecture but i can't be sure the forward pass will be the same...

Thank you for that analysis. The differences are too significant for a model conversion to work at this time. It is going to be a 3-step process:
1. Integrate a pytorch-based tacotron2 with this repo.
2. Modify tacotron2 to match the tensorflow-based implementation in this repo to the extent possible.
3. Convert the weights.

Or a 2-step process:
1. Rewrite this repo's tacotron2 in pytorch
2. Convert the weights

Since you found significant differences, the 2-step process is likely the better approach (a lot like https://medium.com/huggingface/from-tensorflow-to-pytorch-265f40ef2a28 ).

> i could post my code to get variables of the checkpoint and load them, it can help you to have idea of variables to create)

It would be very helpful if you could share the code. You can make code blocks by wrapping the code with ```",main bias used mine sense order really problematic convert step step linear layer projection convolution find value layer attention layer instead full name equivalent layer strange lot architecture think interesting try convert model make make term architecture ca sure forward pas thank analysis significant model conversion work time going process integrate modify match implementation extent possible convert process rewrite convert since found significant process likely better approach lot like could post code get load help idea create would helpful could share code make code wrapping code,issue,positive,positive,positive,positive,positive,positive
673349904,"I am having the same issue but appearing in `synthesizer/synthesizer_dataset.py` line 13 which I tried to solve like:

```
        metadata = []
        with metadata_fpath.open(""r"", encoding=""ascii"") as metadata_file:
            #metadata = [line.split(""|"") for line in metadata_file]
            try:
                for line in metadata_file:
                    metadata.append(line.split(""|""))
            except Exception as e:
                ex = e
```

But now I have this much samples in the training dataset and I am not sure it is correct:
`Found 24353 samples`",issue line tried solve like ascii line try line except exception ex much training sure correct found,issue,positive,positive,positive,positive,positive,positive
673341585,"I have a tutorial for you @mbdash (this was a good learning experience for me too).

1. In [`encoder/params_model.py`](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/encoder/params_model.py#L3) update `model_hidden_size = 768` if you haven't already
2. Initialize a new model with the correct dimensions, save it after 1 step then ctrl+c to stop training
```
python encoder_train.py new_model datasets_root/SV2TTS/encoder/ -b 1
```
3. Verify that it generated the file `encoder/saved_models/new_model_backups/new_model_bak_000001.pt`
4. Download `english_run.pt`, the 768/768 English model trained to 2,143,500 steps from https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/458#issuecomment-671675613
5. Save this gist to `transfer_encoder_weights.py` . Pytorch model checkpoints are simple dictionaries and it is trivial to make edits.
    * https://gist.github.com/blue-fish/194cf931ecfec1a446c35487e0f95e27
6. Put the files from steps 3-5 in the same location
7. Run the script
```
python transfer_encoder_weights.py
```
8. It saves a file called `modified_encoder.pt`, move it to `encoder/saved_models`
9. We only want to train the linear transformation at the end of the model that projects the final hidden layer (size 768) down to the desired embedding size (256) so we set `requires_grad=False` on the model elements that we don't want to update
    * See the modifications here: https://github.com/blue-fish/Real-Time-Voice-Cloning/commit/30a8c7b4734fa1de2fea45d94da063ab1cc1ffa0
10. Now train the modified model that we created
```
python encoder_train.py modified_encoder datasets_root/SV2TTS/encoder/
```
11. Let it run until you can go 1,000 steps without the loss spiking above 0.1. At this point we will know that the nn.Linear elements are properly set for the encoder weights that we imported
12. Revert the changes from step 9 to re-enable grad on all model elements
13. Continue training using the command in step 10",tutorial good learning experience update already initialize new model correct save step stop training python verify file model trained save gist model simple trivial make put location run script python file move want train linear transformation end model final hidden layer size desired size set model want update see train model python let run go without loss spiking point know properly set revert step grad model continue training command step,issue,positive,positive,positive,positive,positive,positive
673332014,"@blue-fish Here is a list of variables in the checkpoint and a list of my variables 

TF1.x checkpoint (this repo) : 
Tacotron_model/inference/decoder/Location_Sensitive_Attention : [[128], [128]]
Tacotron_model/inference/decoder/Location_Sensitive_Attention/location_features_convolution : [[32], [31, 1, 32]]
Tacotron_model/inference/decoder/Location_Sensitive_Attention/location_features_layer : [[32, 128]]
Tacotron_model/inference/decoder/Location_Sensitive_Attention/query_layer : [[1024, 128]]
Tacotron_model/inference/decoder/decoder_LSTM/multi_rnn_cell/cell_0/decoder_LSTM_1 : [[4096], [2048, 4096]]
Tacotron_model/inference/decoder/decoder_LSTM/multi_rnn_cell/cell_1/decoder_LSTM_2 : [[4096], [2048, 4096]]
Tacotron_model/inference/decoder/decoder_prenet/dense_1 : [[256], [80, 256]]
Tacotron_model/inference/decoder/decoder_prenet/dense_2 : [[256], [256, 256]]
Tacotron_model/inference/decoder/linear_transform_projection/projection_linear_transform_projection : [[160], [1792, 160]]
Tacotron_model/inference/decoder/stop_token_projection/projection_stop_token_projection : [[2], [1792, 2]]
Tacotron_model/inference/encoder_LSTM/bidirectional_rnn/bw/encoder_bw_LSTM : [[1024], [768, 1024]]
Tacotron_model/inference/encoder_LSTM/bidirectional_rnn/fw/encoder_fw_LSTM : [[1024], [768, 1024]]
Tacotron_model/inference/encoder_convolutions/conv_layer_1_encoder_convolutions/batch_normalization : [[512], [512], [512], [512]]
Tacotron_model/inference/encoder_convolutions/conv_layer_1_encoder_convolutions/conv1d : [[512], [5, 512, 512]]
Tacotron_model/inference/encoder_convolutions/conv_layer_2_encoder_convolutions/batch_normalization : [[512], [512], [512], [512]]
Tacotron_model/inference/encoder_convolutions/conv_layer_2_encoder_convolutions/conv1d : [[512], [5, 512, 512]]
Tacotron_model/inference/encoder_convolutions/conv_layer_3_encoder_convolutions/batch_normalization : [[512], [512], [512], [512]]
Tacotron_model/inference/encoder_convolutions/conv_layer_3_encoder_convolutions/conv1d : [[512], [5, 512, 512]]
Tacotron_model/inference : [[66, 512]]
Tacotron_model/inference/memory_layer : [[768, 128]]
Tacotron_model/inference/postnet_convolutions/conv_layer_1_postnet_convolutions/batch_normalization : [[512], [512], [512], [512]]
Tacotron_model/inference/postnet_convolutions/conv_layer_1_postnet_convolutions/conv1d : [[512], [5, 80, 512]]
Tacotron_model/inference/postnet_convolutions/conv_layer_2_postnet_convolutions/batch_normalization : [[512], [512], [512], [512]]
Tacotron_model/inference/postnet_convolutions/conv_layer_2_postnet_convolutions/conv1d : [[512], [5, 512, 512]]
Tacotron_model/inference/postnet_convolutions/conv_layer_3_postnet_convolutions/batch_normalization : [[512], [512], [512], [512]]
Tacotron_model/inference/postnet_convolutions/conv_layer_3_postnet_convolutions/conv1d : [[512], [5, 512, 512]]
Tacotron_model/inference/postnet_convolutions/conv_layer_4_postnet_convolutions/batch_normalization : [[512], [512], [512], [512]]
Tacotron_model/inference/postnet_convolutions/conv_layer_4_postnet_convolutions/conv1d : [[512], [5, 512, 512]]
Tacotron_model/inference/postnet_convolutions/conv_layer_5_postnet_convolutions/batch_normalization : [[512], [512], [512], [512]]
Tacotron_model/inference/postnet_convolutions/conv_layer_5_postnet_convolutions/conv1d : [[512], [5, 512, 512]]
Tacotron_model/inference/postnet_projection/projection_postnet_projection : [[80], [512, 80]]
 : [[]]


My implementation (based on NVIDIA) : 
encoder_embeddings : [(148, 512)]
encoder_conv_1 : [(5, 512, 512), (512,)]
batch_normalization : [(512,), (512,), (512,), (512,)]
encoder_conv_2 : [(5, 512, 512), (512,)]
batch_normalization_1 : [(512,), (512,), (512,), (512,)]
encoder_conv_3 : [(5, 512, 512), (512,)]
batch_normalization_2 : [(512,), (512,), (512,), (512,)]
encoder_lstm/forward_lstm/lstm_cell_1 : [(512, 1024), (256, 1024), (1024,)]
encoder_lstm/backward_lstm/lstm_cell_2 : [(512, 1024), (256, 1024), (1024,)]
tacotron2/decoder/prenet/prenet_layer_1 : [(80, 256)]
tacotron2/decoder/prenet/prenet_layer_2 : [(256, 256)]
tacotron2/decoder/attention_rnn : [(768, 4096), (1024, 4096), (4096,)]
tacotron2/decoder/location_attention/query_layer : [(1024, 128)]
tacotron2/decoder/memory_layer : [(512, 128)]
tacotron2/decoder/location_attention/value_layer : [(128, 1)]
tacotron2/decoder/location_attention/location_layer/location_conv : [(31, 2, 32)]
tacotron2/decoder/location_attention/location_layer/location_dense : [(32, 128)]
tacotron2/decoder/decoder_rnn : [(1536, 4096), (1024, 4096), (4096,)]
tacotron2/decoder/linear_projection : [(1536, 80), (80,)]
tacotron2/decoder/gate_output : [(1536, 1), (1,)]
postnet_conv_1 : [(5, 80, 512), (512,)]
batch_normalization_3 : [(512,), (512,), (512,), (512,)]
postnet_conv_2 : [(5, 512, 512), (512,)]
batch_normalization_4 : [(512,), (512,), (512,), (512,)]
postnet_conv_3 : [(5, 512, 512), (512,)]
batch_normalization_5 : [(512,), (512,), (512,), (512,)]
postnet_conv_4 : [(5, 512, 512), (512,)]
batch_normalization_6 : [(512,), (512,), (512,), (512,)]
postnet_conv_6 : [(5, 512, 80), (80,)]
batch_normalization_7 : [(80,), (80,), (80,), (80,)]

Main differences : 
- Some bias are used in mine but not the other (or in the other sense)
- Order and names are not the same (really problematic to convert them...) 
- He uses a 2-frames by step (me 1 by step)
- He uses a linear layer as projection for postnet (me is a convolution)
- I don't find the ""value layer"" in the attention layer but instead i have this : 
"""""" 
Tacotron_model/inference/decoder/Location_Sensitive_Attention : [[128], [128]] 
// Full name of the variables are : 
Tacotron_model/inference/decoder/Location_Sensitive_Attention/attention_bias
Tacotron_model/inference/decoder/Location_Sensitive_Attention/attention_variable_projection
"""""" 
And i don't have the equivalent in my layer, very strange

So... they are a lot of changes in architecture, do you think it is interesting to try to convert weights to my model (if i make the changes) or not ? Because i can make changes in term of architecture but i can't be sure the forward pass will be the same... 

I think i will not try to convert the model and directly retrain one from my existing model (because, at the end, the target for me is to retrain it so retrain one model or another, at the end, it's the same)
But if you want to try, i can share my tf2.0 implementation and you can modify it to have same architecture... after that, rearranging variables is borring but not difficult

Note : to convert from tensorflow to pytorch, it's much easier because state dict is vased on name (and not a list based on the structure) so if you name your modules and weights like the chekpoint, it's very easy to match them so you just need to create a model with these modules / sub-modules and the corresponding shapes (how do you create 'code block' in github ? like that i could post my code to get variables of the checkpoint and load them, it can help you to have idea of variables to create)

Note 2 : my siamese encoder achieves 96% binary accuracy with 0.1 BCE loss at step 20, it awesome ! (and i think it can improve more) and the plot is really nice too",list list implementation based main bias used mine sense order really problematic convert step step linear layer projection convolution find value layer attention layer instead full name equivalent layer strange lot architecture think interesting try convert model make make term architecture ca sure forward pas think try convert model directly retrain one model end target retrain retrain one model another end want try share implementation modify architecture difficult note convert much easier state name list based structure name like easy match need create model corresponding create block like could post code get load help idea create note binary accuracy loss step awesome think improve plot really nice,issue,positive,positive,positive,positive,positive,positive
673297946,"No problem, if you want to compare your model and the checkpoint, you can try to load tf checkpoint variables individually (with a combination of tf.train.list_variables(ckpt_path) and tf.load_variable(ckpt_path, name) or something like that) and inspect variables by name and by shape and also by order to see where your variables differs from the checkpoint (my code get_tf_layers extract « layers variables » from a list of all variables based on name so you can also compare based on layers)",problem want compare model try load individually combination name something like inspect name shape also order see code extract list based name also compare based,issue,negative,neutral,neutral,neutral,neutral,neutral
673292326,"@Ananas120 If you do that, it would be greatly appreciated. In that case I'll try to get the NVIDIA tacotron2 working with our repo. Supposing it works, I will still try to converge to Mozilla TTS because it is actively developed.",ananas would greatly case try get working supposing work still try converge actively,issue,negative,positive,positive,positive,positive,positive
673288067,"I will invastiguate the tf1.x checkpoint today and see if it can be loaded by my tf2.0 model, if so, i can convert weights to the nvidia implementation and send you the pytorch checkpoint 
Note : results will not be exacly same because of the non-use of Zoneout-LSTM (i don’t know if it will change something in fact because i think it’s just a difference in training-dropout but we never know) but for the rest of the model, i think they are really similar",today see loaded model convert implementation send note know change something fact think difference never know rest model think really similar,issue,negative,neutral,neutral,neutral,neutral,neutral
673287144,"@Coastchb If you still need an answer please try asking your question in #364 . I am closing this issue due to inactivity, you are welcome to reopen it at any time.",still need answer please try question issue due inactivity welcome reopen time,issue,positive,positive,positive,positive,positive,positive
673286335,"Thank you @Ananas120 . Currently I am trying to get the tacotron2 in Mozilla TTS to work, there will be easier integration for global style tokens (#230) since they have already implemented the feature. They also have a more active community where I can request help if needed. The tacotron2 you linked is my next choice.",thank ananas currently trying get work easier integration global style since already feature also active community request help linked next choice,issue,positive,negative,neutral,neutral,negative,negative
673284229,"Here is the pytorch-based implementation i used to recode it in tensorflow 
https://github.com/NVIDIA/tacotron2

I already trained the model (pretrained) to retrain it in french but forgot my training parameters... i will say you when i refind my training code...
Good luck ! you can also look at the issues in the nvidia or other tacotron github, i already seen issues where the model speaks « too fast »",implementation used recode already trained model retrain forgot training say refind training code good luck also look already seen model fast,issue,positive,positive,positive,positive,positive,positive
673283274,Closing issue due to inactivity. We are training a new encoder (#458) and will be vigilant for this problem.,issue due inactivity training new vigilant problem,issue,negative,positive,neutral,neutral,positive,positive
673282237,Closing due to inactivity. Start a new issue to continue this discussion.,due inactivity start new issue continue discussion,issue,negative,positive,neutral,neutral,positive,positive
673281535,"I'm also noticing this in my tacotron1 model that I'm training in #447 and I mentioned there,

> Rate of speech (#347) may be an issue but it at least seems to be consistent and affects short text inputs too. (Edit: I also think this is from the training data which has similar speech rate.) We might add an option to the toolbox UI to postprocess the output with a speed multiplier that doesn't affect pitch.",also model training rate speech may issue least consistent short text edit also think training data similar speech rate might add option toolbox output speed multiplier affect pitch,issue,negative,negative,neutral,neutral,negative,negative
673280070,I'm closing this issue due to lack of interest. My commits so far in #472 are to the effect of removing the vestiges of support for ARPAbet. Someone sufficiently motivated will have no trouble figuring it out.,issue due lack interest far effect removing support someone sufficiently trouble,issue,negative,negative,neutral,neutral,negative,negative
673250229,"Just had an idea @mbdash . We can take the English 768/768 model in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/458#issuecomment-671675613 , and copy the weights for the LSTM layers into our new model. What needs to match is the input size ([`mel_n_channels=40`](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/encoder/params_data.py#L5)) and the number of hidden layers which we're changing to 768.

Then we will fix those weights and only train the nn.Linear at the end which takes the last hidden layer (size 768) and projects it down to `model_embedding_size=256`. Once the loss comes down to an acceptable level then we will test the performance of the resulting model and resume training of the full model (allowing the LSTM weights to change) if not satisfied.

There are only 196,684 parameters to train (=768*256 + 256 for the nn.Linear bias). It should train quickly.",idea take model copy new model need match input size number hidden fix train end last hidden layer size loss come acceptable level test performance resulting model resume training full model change satisfied train bias train quickly,issue,negative,positive,positive,positive,positive,positive
673227337,"> python demo_toolbox.py -d datasets

hoo...finally it's working...thanks a bunch",python finally working thanks bunch,issue,negative,positive,neutral,neutral,positive,positive
673216728,"Try putting the `LibriSpeech` folder inside a folder named `datasets` and run:

`python demo_toolbox.py -d datasets`",try folder inside folder run python,issue,negative,neutral,neutral,neutral,neutral,neutral
673197647,"> What does the warning message say when you specify full path to the folder just 1 level above LibriSpeech?
> Can you show me where you are placing your dataset and the python command used to launch the toolbox?

![Capture](https://user-images.githubusercontent.com/34035011/90084583-53f72d80-dd33-11ea-88c0-8aea44591d93.PNG)
",warning message say specify full path folder level show python command used launch toolbox capture,issue,negative,positive,positive,positive,positive,positive
673184586,Mozilla TTS tried it and encountered a bug with RNNs to be fixed in a future Pytorch release. https://github.com/mozilla/TTS/issues/486#issuecomment-670774865,tried bug fixed future release,issue,negative,positive,neutral,neutral,positive,positive
673178578,"@sberryman Pinging you again just in case you did not see https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/458#issuecomment-672039095

Specifically, if you can locate the 768/256 (hidden/model) **mixed** encoder from Nov-Dec 2019 that would be very much appreciated. All that is previously shared in #126 for mixed is 256/768.",case see specifically locate mixed would much previously mixed,issue,negative,neutral,neutral,neutral,neutral,neutral
673170027,"@NerdIt I apologize for the installation difficulties, we're continuing to work on it. I've always thought of it as a research code, and it is quite user-friendly from that perspective. Thanks for giving the toolbox another chance.

Make sure you're using Python 3.6 or 3.7 (check it with `python --version`). Don't hesitate to reach out when you encounter the inevitable error message.",apologize installation work always thought research code quite perspective thanks giving toolbox another chance make sure python check python version hesitate reach encounter inevitable error message,issue,positive,positive,positive,positive,positive,positive
673149341,"@blue-fish oh. thx for the suggestions. I'll try both the fixes and see if it works. My computer skills are not particularly developed for the console, but i understand software pretty well.",oh try see work computer particularly console understand pretty well,issue,positive,positive,positive,positive,positive,positive
673130183,"I think I will preprocess again all the dataset which the logs failed....
And then try again 1 DS at the time like you proposed",think try time like,issue,negative,neutral,neutral,neutral,neutral,neutral
673129617,"I have mesed around with LibriSpeeech, VoxCeleb 1&2, CommonVoice and VCTK in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/458

I am a bit tired so I am not sure if VoxCeleb was wav, ensure it is all converted to wav.
Also, once I succeed in training, a new encoder, I will share everything i can.",around bit tired sure ensure converted also succeed training new share everything,issue,positive,positive,neutral,neutral,positive,positive
673126217,"Could it be related to this? https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/76#issuecomment-529013562

Also you might want to follow #458 since we're also running into issues with preprocessing and training. If you modify code to fix a problem would you please contribute it back as a pull request?",could related also might want follow since also running training modify code fix problem would please contribute back pull request,issue,negative,neutral,neutral,neutral,neutral,neutral
673098296,"We already have a pretrained tensorflow 1.x synthesizer that is based on Rayhane-mamah's Tacotron2. I want to convert those weights to pytorch. My current pt model is Tacotron1 so it is incompatible. I'll find a torch-based Tacotron2 code, add the speaker embedding part and make it line up with the tf-based Tacotron2. Then hope for the best when I run your conversion script.

Edit: I only have 4 GB, everyone else trains at batch=36 so I convert my own numbers to make them comparable. At this point in the training schedule I'm doing batch=6 with max_length=500. Not limited by memory. This is my first synth from scratch and I am experimenting to see what works well.",already synthesizer based want convert current model incompatible find code add speaker part make line hope best run conversion script edit everyone else convert make comparable point training schedule limited memory first scratch see work well,issue,positive,positive,positive,positive,positive,positive
673083800,"Tomorrow i will make the pipeline to train the model too (with my architecture tf2.0 code) and my siamese encoder and will say you if i have better results 

To have better results for you you could try a partial transfer-learning from pretrained model like the NVIDIA implementation in pytorch (i said partial transfer because all layers will not have same shape because of the additionnal embedding vector) but as i tested it for other models, a partial transfer is good too (the code for tensorflow partial transfer is in my converter code in my github, you can easily adapt it to pytorch i think)

If you prefer use a tf implementation, i can share you my implementation in tf2.0 and you can transfert weights from the pretrained NVIDIA model 

Ps : you use 32 batch-size :o what is your GPU memory and the max_length of the mel spect ? With my implementation i can’t do batch 32... (i will invastigate because i think the tensorflow CuDNNLSTM is very memory efficient)

Edit : i just stop training my siamese network (3sec fixed length audio) at step 15 and still improving ! 95% binary accuracy and 0.02 BCE loss i’ts so good !",tomorrow make pipeline train model architecture code say better better could try partial model like implementation said partial transfer shape vector tested partial transfer good code partial transfer converter code easily adapt think prefer use implementation share implementation model use memory mel implementation batch think memory efficient edit stop training network sec fixed length audio step still improving binary accuracy loss good,issue,positive,positive,positive,positive,positive,positive
673078067,"For anyone who's following, I am still disappointed in the model that has been training (currently at 78k steps, equivalent batch size 36. It still has these issues:
1. Voice quality is not as good
2. Gaps and voice drops off when synthesizing longer utterances (a semi-frequent occurrence)
3. Does not handle punctuation well

If it is a matter of additional training (which we do not know for sure), it will take over a week of nonstop training to match the LibriTTS_200k model in #449. As much as I want to see #472 merged it can wait until we have a good pretrained model to ship with it.

To keep myself occupied while it is training I am going to test out a pytorch-based Tacotron2 and see if I can convert the weights from the existing synth over.",anyone following still disappointed model training currently equivalent batch size still voice quality good voice longer occurrence handle punctuation well matter additional training know sure take week nonstop training match model much want see wait good model ship keep training going test see convert,issue,positive,positive,positive,positive,positive,positive
673069902,"@rlutsyshyn I suggest you write a script that does this:
1. Make a list of every metadata.csv
2. For each metadata.csv:
    * Parse the metadata using something like this code snippet: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/449#issuecomment-671071445
    * You will have 3 variables: filename, text, and normalized text
    * Save the normalized text into filename.txt (so you will have 1 file per utterance)

After you do this, you can move files around to make it look like https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-666099538 and the command I provided there should work.",suggest write script make list every parse something like code snippet text text save text file per utterance move around make look like command provided work,issue,positive,neutral,neutral,neutral,neutral,neutral
673052060,"Can you help me with creating dataset for training? I use data from https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/ for Ukrainian.
Data looks like:
```
uk_UK
    |---by_book
          |----female
          |----male
              |---speaker_name
                        |---wavs
                        |---metadata.csv (which consists <filename.wav> | Text in that sound

            ......

```
Mb I can do it automatically or something like that?       ",help training use data data like female male text sound automatically something like,issue,positive,positive,positive,positive,positive,positive
673044592,"> I just have a question, my pretrained Tacotron is pretrained for 22050Hz and my Waveglow vocoder too but my encoder uses 16kHz audio... do you think it can be a problem to use embedding from 16kHz to train the synthesizer on 22050Hz or no ?
Therically i think no because this is just a speaker embedding so an abstract representation of the speaker but... not sure
>
> Another think, my encoder is 64-length embedding, is this ok as input or is it too small ?

I agree a 16,000 Hz encoder should work with a 22,050 Hz synth/vocoder. As you say, it is just an abstract representation of the voice and there is no information being passed that depends on sample rate.

From [1806.04558](https://arxiv.org/pdf/1806.04558.pdf) (the SV2TTS paper), it looks like the output similarity may be penalized slightly but they still achieve a MOS of 3+ with an embedding size of 64. So I think 64 is sufficient unless you have a large number of speakers in your **synthesizer** training dataset (1,000+). I am aware that the speaker counts in the table are based on the encoder training set... but if you do not have enough speakers for the synth, I hypothesize it cannot effectively utilize the additional dimensions in the encoder output.

<img width=""660"" alt=""1806 04558_screenshot"" src=""https://user-images.githubusercontent.com/67130644/89597463-1c5a2400-d80f-11ea-82a4-c313ed38a677.png"">",question audio think problem use train synthesizer think speaker abstract representation speaker sure another think input small agree work say abstract representation voice information sample rate paper like output similarity may slightly still achieve size think sufficient unless large number synthesizer training aware speaker table based training set enough hypothesize effectively utilize additional output,issue,positive,positive,positive,positive,positive,positive
673033702,"So the results for my tests on siamese and the actual encoder : 
3-layer RNN encoder (256-embedding) : 
- Low processing time (melspectrogram)
- Bad implementation of GE2E loss (memory efficient) (my implementation but no tf2 open-source available)
- GE2E loss of 0.6 (if my results are ok but not sure...)
- Can perform arbitrary length sample

Siamese encoder (64-embedding) : 

- No audio processing (except resampling if needed)
- Very fast to train (200ms for batch size 32 on my single GPU with audios pre-loaded)
- Have meaningful metrics to evaluate it (binary_accuracy, true_positives and true_negatives)
- BCE loss 0.09 with 94% accuracy (and can improve it more i think !)
- Theorically works with arbitrary length input but i don’t know why, it doesn’t converge (so for my best model i use 3 seconds of raw 16kHz audio)
- GE2E loss of 0.8 (for my old model of 89% accuracy)

In two cases, the embedding plot seems good (but slightly better for the siamese i find)

Plan : 
- Adapt my Tacotron-2 arch to accept speaker embedding as input
- Create speaker embedding for all speakers in my 2 Fr datasets (Common Voice and SIWIS) (with my Siamese encoder)
- Create input pipeline for Tacotron-2 (including the embedding)
- Transfert weights of my Fr pretrained Tacotron-2 and train it for a few days (with the speaker embedding)
- Create a complete pipeline to generate audio with arbitrary voice (based on an input audio (3sec))

I just have a question, my pretrained Tacotron is pretrained for 22050Hz and my Waveglow vocoder too but my encoder uses 16kHz audio... do you think it can be a problem to use embedding from 16kHz to train the synthesizer on 22050Hz or no ? 
Therically i think no because this is just a speaker embedding so an abstract representation of the speaker but... not sure 

Another think, my encoder is 64-length embedding, is this ok as input or is it too small ?",actual low time bad implementation gee loss memory efficient implementation available gee loss sure perform arbitrary length sample audio except fast train batch size single meaningful metric evaluate loss accuracy improve think theorically work arbitrary length input know converge best model use raw audio gee loss old model accuracy two plot good slightly better find plan adapt arch accept speaker input create speaker common voice create input pipeline train day speaker create complete pipeline generate audio arbitrary voice based input audio sec question audio think problem use train synthesizer think speaker abstract representation speaker sure another think input small,issue,positive,positive,positive,positive,positive,positive
672994989,"Good, i haven’t tested the toolbox yet in fact but good to know, i think i will test it to see which impact it can have on the voice ",good tested toolbox yet fact good know think test see impact voice,issue,positive,positive,positive,positive,positive,positive
672958470,"Can you try making a separate test folder with a small subset of speakers and see if the training script will work on that? I recall having to experiment a bit to get it to work. (I have never trained an encoder, but I have tested the scripts to make sure training will work on CPU)",try making separate test folder small subset see training script work recall experiment bit get work never trained tested make sure training work,issue,negative,positive,positive,positive,positive,positive
672956488,"#### https://gist.github.com/blue-fish/7eccac290cdd8b9205b8c5f488e39bb2
This will replace your utterance embeddings with speaker embeddings.
```
python make_speaker_embeds.py path/to/datasets_root/SV2TTS/synthesizer/embeds
```",replace utterance speaker python,issue,negative,neutral,neutral,neutral,neutral,neutral
672946453,"> So... what if the embedded input is a music or something else than a real human voice ??!

You can try this with the pretrained models of this toolbox. Either load a non-voice sample, or record something with your microphone. Then try to synthesize some text. You still get intelligible speech as output because the synthesizer is only trained to make speech.",input music something else real human voice try toolbox either load sample record something microphone try synthesize text still get intelligible speech output synthesizer trained make speech,issue,negative,neutral,neutral,neutral,neutral,neutral
672945740,"Hi,
I tried to start the training

Here is the error I got, I will look at the code when I have some time.
![image](https://user-images.githubusercontent.com/32403586/90034948-41c3b380-dc8f-11ea-9181-42553234bbcb.png)

Update: ok it doesnt seem connected to the error above, but I already can see that the log files from preprocessing are used and I got some invalid/empty ones
",hi tried start training error got look code time image update doesnt seem connected error already see log used got,issue,negative,neutral,neutral,neutral,neutral,neutral
672924691,"Oh @blue-fish another funny thing you can try with embedding vector as the synthesizer !
As a fun application of the siamese, i made scripts to detect different speakers in an audio file and cluster them (to detect where they speach)
I launched it on a radio interview with 2 people and the model returns me 3 speakers... I listened to samples of the 3 detected and, in fact, the first detected speaker was the introduction music and the 2 other were the true speakers perfectly separated !

So... what if the embedded input is a music or something else than a real human voice ??!",oh another funny thing try vector synthesizer fun application made detect different audio file cluster detect radio interview people model fact first speaker introduction music true perfectly input music something else real human voice,issue,positive,positive,positive,positive,positive,positive
672912428,"Oh yes indeed, resampling is needed but no mel spectrogram computation i would say (this repo uses à 40-features mel-spectrogram as input so processing is resampling and melspectrogram computation)
Another thing is that i didn’t try variable sample length in the siamese (the 2 samples are fixed length) but i will launch a training with variable length when my actual training finishes 
",oh yes indeed mel spectrogram computation would say input computation another thing try variable sample length fixed length launch training variable length actual training,issue,negative,positive,neutral,neutral,positive,positive
672910623,"> the main advantage of the siamese approach is that no processing is needed (raw audio is given)

How do you handle audio files with different sample rates? I think encoder preprocessing for this repo just resamples the source audio to the desired rate. In other words I don't see the relative advantage to the GE2E approach here? My ignorance may be showing so please explain further.",main advantage approach raw audio given handle audio different sample think source audio desired rate see relative advantage gee approach ignorance may showing please explain,issue,positive,negative,neutral,neutral,negative,negative
672909216,"Another thing to note is that the siamese network is train to say if 2 samples of voice are from the same speaker or not and to do that it uses the euclidian distance between the embedded audios and this is a simple linear layer with sigmoid that gives the score 
(0 if 2 samples are same, if the target is to minimize distance or 1 if same if target is to maximize similarity, i suppose at the end it’s exactly the same for the encoder part)
But then the accuracy is computed as the mean between true-positives and true-negatives (actually i have 92% with my training model) 

The problem with the encoder of this repo (i find) is that we can’t get this think of metrics to evaluate the model so i can’t really compare them... 
The only thing i find is to see the GE2E loss of the encoder of the siamese network and as i tested it, my encoder has 0.8 loss and RNN encoder has also around 0.7 loss so... i think the 2 approachs can be interchanged but the main advantage of the siamese approach is that no processing is needed (raw audio is given) and we have meaningful metrics (accuracy, true-positive and true-negative) and it can be train with a simple binary_crossentropy loss

A second advantage is for the applications of the model itself, the encoder just embeds mels so if we want to say if 2 samples comes from same speaker, we must implement the distance threshold etc, with the siamese, the output layer says it for us and it can then have many fun applications !",another thing note network train say voice speaker distance simple linear layer sigmoid score target minimize distance target maximize similarity suppose end exactly part accuracy mean actually training model problem find get think metric evaluate model really compare thing find see gee loss network tested loss also around loss think main advantage approach raw audio given meaningful metric accuracy train simple loss second advantage model want say come speaker must implement distance threshold output layer u many fun,issue,negative,positive,positive,positive,positive,positive
672900478,"My first reaction to the concept is that the approach with siamese networks is not good enough if it only achieves 90% accuracy. But after thinking some more, a weakness of the current speaker encoder is that we train it to treat each voice as distinct even though perceptually, we consider certain pairs of voices more similar than others. So maybe less accuracy is actually a good thing if it relates unseen voices in a meaningful way to the voices that the encoder is trained on.

Also, for the encoder, a higher speaker embedding size may work better for speaker ID because it has more features to discriminate between voices. But it may perform poorly for voice cloning if the encoder relies on features that humans cannot perceive. Restrict the embedding size may force the encoder to use the more tangible features. It may also help to label highly similar voices (as perceived by human) as the same voice for training.",first reaction concept approach good enough accuracy thinking weakness current speaker train treat voice distinct even though perceptually consider certain similar maybe le accuracy actually good thing unseen meaningful way trained also higher speaker size may work better speaker id discriminate may perform poorly voice perceive restrict size may force use tangible may also help label highly similar human voice training,issue,positive,positive,positive,positive,positive,positive
672895489,"What does the warning message say when you specify full path to the folder just 1 level above LibriSpeech?
Can you show me where you are placing your dataset and the python command used to launch the toolbox?",warning message say specify full path folder level show python command used launch toolbox,issue,negative,positive,positive,positive,positive,positive
672893512,"An alignment file is used to split long utterances into smaller ones. It is unnecessary for datasets like LibriTTS where you can discard samples that are too long and still have a lot of data remaining. See the violin plot below

If you are making a custom dataset, just try to make your samples 2 to 7 seconds in length for training and don't bother with generating alignments. You can manually split long utterances yourself. If you have a very large number of files to work with and must automate it, use something like the [Montreal Forced Aligner](https://montreal-forced-aligner.readthedocs.io/en/latest/).

For finetuning your data just make your dataset look like: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-666099538

From https://arxiv.org/pdf/1904.02882v1.pdf
<img width=""386"" alt=""libritts_figure1"" src=""https://user-images.githubusercontent.com/67130644/89210812-a25b3e00-d575-11ea-9542-826193ebb9ae.png"">


",alignment file used split long smaller unnecessary like discard long still lot data see violin plot making custom try make length training bother generating manually split long large number work must use something like forced aligner data make look like,issue,negative,negative,neutral,neutral,negative,negative
672879529,"Yep, see this, but how can I create own one for future fine tuning on my data? ",yep see create one future fine tuning data,issue,positive,positive,positive,positive,positive,positive
672877693,"> Did you download the dataset from https://openslr.org/12 and extract it to your Real-Time-Voice-Cloning folder?
> 
> In the python command, try specifying full path to the folder just 1 level above LibriSpeech. It automatically looks for LibriSpeech/train-clean-100, etc. relative to the specified path.
yes I did
",extract folder python command try full path folder level automatically relative path yes,issue,negative,positive,positive,positive,positive,positive
672866170,"Did you download the dataset from https://openslr.org/12 and extract it to your Real-Time-Voice-Cloning folder?

In the python command, try specifying full path to the folder just 1 level above LibriSpeech. It automatically looks for LibriSpeech/train-clean-100, etc. relative to the specified path.",extract folder python command try full path folder level automatically relative path,issue,negative,positive,positive,positive,positive,positive
672863656,"check the warning message
 
i typed in python demo_toolbox.py -d LibriSpeech\train-clean-100",check warning message python,issue,negative,neutral,neutral,neutral,neutral,neutral
672863161,"> Which file are you trying to run, and how are you running it?
> 
> Also full traceback and error message would be very helpful for troubleshooting the problem.

![Capture](https://user-images.githubusercontent.com/34035011/90019350-e663e680-dccb-11ea-8d0f-14fec8fc21be.PNG)
",file trying run running also full error message would helpful problem capture,issue,negative,positive,positive,positive,positive,positive
672859194,"Which file are you trying to run, and how are you running it?

Also full traceback and error message would be very helpful for troubleshooting the problem.",file trying run running also full error message would helpful problem,issue,negative,positive,positive,positive,positive,positive
672858517,"Do you have the LibriSpeech alignments? A link is on this page: https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training

If it can't find the alignment text files then it thinks there's nothing to process for LibriSpeech.",link page ca find alignment text nothing process,issue,negative,neutral,neutral,neutral,neutral,neutral
672845964,"First of all, my GE2E loss implementation is really bad because it uses far too much memory... 
It’s because tensorflow graph accepts only tf operations and neither numpy function neither indexing so i used matrix tricks with mask to use the write centroid but i then have matrix with shape (N, M, N, embedding_dim) 3 times which is really expensive especially for 256-embedding... if you find a better open-source implementation, let me know please

But i compared my siamese encoder model and the 3-layer RNN encoder and it’es really fun because i have a 2-times better loss (0.8 for me and 1.5 for the RNN) (i calculated the loss as the mean of categorical-crossentropy of the similarity matrix)",first gee loss implementation really bad far much memory graph neither function neither indexing used matrix mask use write centroid matrix shape time really expensive especially find better implementation let know please model e really fun better loss calculated loss mean similarity matrix,issue,negative,positive,neutral,neutral,positive,positive
672742577,"If you want i can share my Tacotron-2 tf2.0 implementation
I think they are really similar but the big difference is that he uses Zoneout-LSTM and me normal LSTM (the difference is essentially a dropout, if i well understand the layer) so no matter in term of weights
I will invastiguate the variables in the checkpoint to see if i can use them in my model or if there is really a difference in the architecture...

Edit : i begin to train my siamese network and discover that when i l2-normalize my embedding before calculating distance, the loss is 0.6933 at around the 500th step and doesn’t change till the 100th step where i stopped training (no interesting)
Without normalization, the loss drops from 7 to 1 (from step 1 to 1000) so much interesting but i don’t know if it can serve as input of the tacotron model (because of the non-normalization)... 
Re-Edit : i try with normalization but with 64 embedding size and have good results, loss is actually at 0.33 after only 3k steps (1 epoch is 2k step) on a mixt of librispeech (en)  and common-voice (fr) datasets)

I will try to convert the encoder (3-layer RNN) in tf and convert weiths to see if i can obtain interesting results with that architecture and my siamese architecture and also compare loss and embeddings plot between my model and the 3-layer RNN encoder

Edit 2 : i converted the RNN encoder to tf2.0, it was really easy just a little trick to use my converter weights script was to rename the lstm weights (because pytorch use lstm.weight_xx_yy) to name weights of stacked lstm but my script get layer name by removing last part (all after the last ‘.’) so i had 12 weights for « lstm » then i renamed lstm_0 the 4th weights, lstm_1 (4 to 8) and lstm_2 (8 to 12) and all works fine !
I made test and i have a difference between the 2 models of 5e-6 (max) and 5e-7 (mean) for input np.ones((10, 1000, 40)) so really good !",want share implementation think really similar big difference normal difference essentially dropout well understand layer matter term see use model really difference architecture edit begin train network discover calculating distance loss around th step change till th step stopped training interesting without normalization loss step much interesting know serve input model try normalization size good loss actually epoch step en try convert convert see obtain interesting architecture architecture also compare loss plot model edit converted really easy little trick use converter script rename use name script get layer name removing last part last th work fine made test difference mean input really good,issue,positive,positive,positive,positive,positive,positive
672739688,"@Ananas120 Thank you! So it looks like after constructing the pytorch model with layers in the same order and size as the tf model, the weights should transfer over.

I'll need to study the Rayhane Tacotron-2 (tensorflow) and Fatchord Tacotron-1 (pytorch) and see how far off they are in terms of model structure.",ananas thank like model order size model transfer need study see far model structure,issue,positive,positive,neutral,neutral,positive,positive
672716725,"@blue-fish  it’s done, i push my convertor code on my github 
It’s not perfect and is mostly based on the layer weights order but in most case it is good (i transfered tacotron-2 weights from NVIDIA to my tf model with this code but i had to create layers in the same order as the pytorch model) ",done push convertor code perfect mostly based layer order case good model code create order model,issue,positive,positive,positive,positive,positive,positive
672692937,"Here are the results of my siamese encoder (with embedding size = 64 and 2 seconds of raw 16kHz audio as input)
The embedding is made with 10 audios of 20 random speaker of CommonVoice (fr) dataset with the UMAP projection (the projection code of this repo)
I also plot the training metrics (loss / metrics) over batch and steps 
![training](https://user-images.githubusercontent.com/45139111/89987501-74b17b80-dc7e-11ea-91b0-42d2cfef64fb.png)

Like you can see, i trained it only for 5 epochs (10k steps) that takes me less than 1 hour on my single GPU and the result is quite good !

![embedding_plot](https://user-images.githubusercontent.com/45139111/89987490-6f543100-dc7e-11ea-8b5f-0b0b6a50e7bb.png)

Another point to note is that, in the original model, i don't normalize my embedding (i make it only for the plot) so perhaps it can affect the plot results ?
I will retrain a model with normalization at the end of the embedding to see if it can improve results
The architecture i used is exactly the same as in the voicemap article except that i added step of 4 for the first convolution because i put 16khz audio as input and not 4khz (so step of 4 reduces 16khz to the equivalent of 4 khz in term of samples)

Note : all values in the plot are for training set (because of a bug in my code, it doesn’t keep track of validation metrics) but they are really similar",size raw audio input made random speaker projection projection code also plot training metric loss metric batch training like see trained le hour single result quite good another point note original model normalize make plot perhaps affect plot retrain model normalization end see improve architecture used exactly article except added step first convolution put audio input step equivalent term note plot training set bug code keep track validation metric really similar,issue,positive,positive,neutral,neutral,positive,positive
672644774,"@blue-fish thank you for the informations, i will post my convertions code today on my github (i will say you when they are posted)

In fact, my first target is to integrate this repo inside my own project (i copy many existing models and make experience with them to learn how they work and make fun applications of them)

[Moderator note: Moved question about speaker encoder to #484]",thank post code today say posted fact first target integrate inside project copy many make experience learn work make fun moderator note question speaker,issue,positive,positive,positive,positive,positive,positive
672436760,"> I always just get a 'torch module' doesn't exist even though I've clearly installed it several times.

@NerdIt I think the problem is that when you run `python` it launches python 2.x which doesn't work with torch or the toolbox. Retry the commands with `python3` in place of ""python"".

This is what it looks like when you try to import torch in python 2.x:
```
>>> import torch
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: No module named torch
>>> 
```",always get module exist even though clearly several time think problem run python python work torch toolbox retry python place python like try import torch python import torch recent call last file line module module torch,issue,negative,neutral,neutral,neutral,neutral,neutral
672352315,"alright, i just launched the conversion from flac to wav and am only keeping mic1 files.

Update: 
beginning pretraining of CommonVoice, 
will do vctk afterwards.
(doing it separately just in case...)",alright conversion keeping update beginning pretraining afterwards separately case,issue,negative,neutral,neutral,neutral,neutral,neutral
672294962,"@Ananas120 You might also be interested in my tf2.0 fork, on branch [370_tf2_compat](https://github.com/blue-fish/Real-Time-Voice-Cloning/tree/370_tf2_compat)

I ran the tf1.x --> tf2.0 conversion scripts and fixed most of the obvious problems. I spent quite a few hours on the conversion but couldn't get past an error message (look at history of this link: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/370#issuecomment-648798434). If you can figure it out, it could make your job easier to get a tf2.0 version of the synthesizer.

The hard part is not making a tf2.0 tacotron (there are plenty of those out there), but rather the integration of tacotron with the rest of the repo. There are quite a few things to consider if you care about preserving the functionality and UI.",ananas might also interested fork branch ran conversion fixed obvious spent quite conversion could get past error message look history link figure could make job easier get version synthesizer hard part making plenty rather integration rest quite consider care functionality,issue,positive,negative,neutral,neutral,negative,negative
672288384,"> For the tacotron-2 part, i will see how to do because i would like to re-train it in french but i have only 1 GPU and Corentin trains it in 1 week on 4 GPU with more memory than mine so...

Hi @Ananas120 ,
I used to think synthesizer training was impractical on my basic GPU until I tried it. It is slow but as long as you do not require a perfect model it is quite usable after 2 or 3 days of nonstop training. I use short utterances mainly because the prosody is more consistent, otherwise the model makes long gaps in the middle of sentences when synthesizing. Smaller memory footprint and faster training are nice side effects. For smaller datasets you may have no choice but to split utterances like you are doing.

> For the vocoder, i use the Waveglow model from NVIDIA which is really good (i find) because it converts spectrogram (in any language, etc) so if the quality of the spectrogram is good, the audio will be good too (without re-training)

Yes, a multi-speaker vocoder can also be thought of as a universal vocoder. It generalizes to new speakers and even languages without retraining. That is also the case with the pretrained WaveRNN from this repo. In my opinion, the spectrogram quality is not that good and would not benefit from a better vocoder. I might try out Waveglow if we train a 22,050 Hz synthesizer model (someone is working on it in #449).

You should read the new paper on the HooliGAN vocoder by Corentin's colleague fatchord: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/364#issuecomment-670367298 Faster vocoding + higher quality have me very intrigued. I don't think I can implement it properly though so I will wait for someone to write an open-source version.

> i made scripts to convert weights from pt to pytorch and pytorch to tf if it can help you

Please share those. I might convert the weights from the tensorflow synthesizer over to pytorch in case my model doesn't work out.

> If you have any implementation and checkpoint of the model in pytorch, it can be easier for me to convert it to tf2.0 (i don’t understand tf1.x and my implementation seems to be a bit different of the implementation of this repo)

You can find a working pytorch implementation on the [447_pytorch_synthesizer](https://github.com/blue-fish/Real-Time-Voice-Cloning/tree/447_pytorch_synthesizer) branch of my fork. The pretrained model is still in work.",part see would like week memory mine hi ananas used think synthesizer training impractical basic tried slow long require perfect model quite usable day nonstop training use short mainly prosody consistent otherwise model long middle smaller memory footprint faster training nice side effect smaller may choice split like use model really good find spectrogram language quality spectrogram good audio good without yes also thought universal new even without also case opinion spectrogram quality good would benefit better might try train synthesizer model someone working read new paper hooligan colleague faster higher quality think implement properly though wait someone write version made convert help please share might convert synthesizer case model work implementation model easier convert understand implementation bit different implementation find working implementation branch fork model still work,issue,positive,positive,positive,positive,positive,positive
672271152,"For ideas to test this thing you have many things to do ! but it can take lot of computational time and find datasets appropriated... 
For instance : 
- Add emotion using an embedded of the speaker with this particular emotion
- Try to improve the encoder to create « identity card » of a speaker (with the L2-norm of all his embeddings)
- ...",test thing many take lot computational time find instance add emotion speaker particular emotion try improve create identity card speaker,issue,positive,positive,positive,positive,positive,positive
672260769,"Hello @blue-fish , 
I just discover this repo and see your differents posts
I can’t help you to implement it in pytorch because i code in tensorflow 2.0 but if you want i already recode Tacotron-2 in tensorflow-2 based on the NVIDIA/DeepLearningExample repo and re-train it in french (with transfer learning, i converts weights from the NVIDIA pretrained model)
I also have hardware limitation with my memory GPU, a little trick i used was to train on whole spectrogram (of any length) but by part 
For example a spectrogram of size 500, i run into a for loop where i optimized the model every 50 steps and keep track of last rnn state to pass it for next 50frames training step (i know, it’s not a good idea but it solved my memory limitation)

Tomorrow i will try to re-implement the encoder part in tf2.0 and convert weights (i made scripts to convert weights from pt to pytorch and pytorch to tf if it can help you)

For the tacotron-2 part, i will see how to do because i would like to re-train it in french but i have only 1 GPU and Corentin trains it in 1 week on 4 GPU with more memory than mine so... 
If you have any implementation and checkpoint of the model in pytorch, it can be easier for me to convert it to tf2.0 (i don’t understand tf1.x and my implementation seems to be a bit different of the implementation of this repo)

For the vocoder, i use the Waveglow model from NVIDIA which is really good (i find) because it converts spectrogram (in any language, etc) so if the quality of the spectrogram is good, the audio will be good too (without re-training) (for example, audio —> mel —> waveglow gives an audio that sounds exacly like the original one)
The only problem is that it is trained on 22050Hz and not 16khz",hello discover see help implement code want already recode based transfer learning model also hardware limitation memory little trick used train whole spectrogram length part example spectrogram size run loop model every keep track last state pas next training step know good idea memory limitation tomorrow try part convert made convert help part see would like week memory mine implementation model easier convert understand implementation bit different implementation use model really good find spectrogram language quality spectrogram good audio good without example audio mel audio like original one problem trained,issue,positive,positive,positive,positive,positive,positive
672135224,"> 
> 
> Here are instructions for installing ffmpeg on windows: https://video.stackexchange.com/a/20496
> 
> It's really only needed if you want to use mp3 files as input (to create the speaker embedding).

Oh ok,
Thanks a lot for the help",really want use input create speaker oh thanks lot help,issue,positive,positive,positive,positive,positive,positive
672078312,@mbdash That is not a mistake. VCTK comes in .flac . It will need to be converted. I suggest exclusively using mic1 since there were a few speakers for whom mic2 failed.,mistake come need converted suggest exclusively since,issue,negative,neutral,neutral,neutral,neutral,neutral
672075549,"I am re-downloadeing VCTK, maybe i got the wrong version, since the dataset i have is all .flac and not wav",maybe got wrong version since,issue,negative,negative,negative,negative,negative,negative
672039095,"@sberryman I share the same hunch and will run the experiment if you are able to locate the file. I even think it might work with Corentin's synth and vocoder.
* The synth is the tricky part, it depends if the speaker embeds come out looking the same as the current encoder. But as I've said before I think the loss function will make your encoder output like the current one.
* A vocoder trained on multispeaker data will work well with any speech spectrograms, even those in a different language. See https://github.com/mozilla/TTS/issues/221 and this even continues to be worked on today: https://discourse.mozilla.org/t/training-a-universal-vocoder/65388

Apologies for sending you on the wild goose chase earlier, but we still don't have the mixed encoder with (768 hidden/256 model) size that you developed in Nov-Dec 2019. If you can find the files from that experiment it will save us several weeks of compute time.

In #126 you shared:
* Mixed, 697,500 steps (256 hidden, 768 model) - this is backwards from what we want https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/126#issuecomment-530832810
* Mixed, 1,005,000 steps (256 hidden, 768 model) - same as above https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/126#issuecomment-531350334
* Mixed, 2,215,200 steps - did not check size but I know that it can't be 768 hidden/256 model as you only started training it in November and this was released back in September 2019. https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/126#issuecomment-536097366",share hunch run experiment able locate file even think might work tricky part speaker come looking current said think loss function make output like current one trained data work well speech even different language see even worked today sending wild goose chase still mixed model size find experiment save u several compute time mixed hidden model backwards want mixed hidden model mixed check size know ca model training back,issue,positive,positive,neutral,neutral,positive,positive
672029909,"> We're one year after the initial publication of this project. I've been busy with both exams and work since, and it's only last week that I passed my last exam. During that year, I have received SO many messages from people asking for help in setting up the repo and I just had no time to allocate for any of that.
> I kinda wished that the popularity of this repo would have died down, but new people keep coming in at a fairly constant rate.
> I have no intentions to start developing on this repo again, but I hope I can answer some questions and possibly review some PRs. Use this issue to ask me questions and to bring light upon things that you believe need to be improved, and we'll see what can be done.

You wanted the popularity of this repo to go down because you couldn't handle the requests? That's kinda absurd, people's interest are good thing, more developers means less work on one person's shoulders. ;) ",one year initial publication project busy work since last week last exam year received many people help setting time allocate wished popularity would new people keep coming fairly constant rate start hope answer possibly review use issue ask bring light upon believe need see done popularity go could handle absurd people interest good thing le work one person,issue,positive,positive,positive,positive,positive,positive
672019373,"The synthesizer gaps (#53) will be mostly fixed by switching to pytorch. I think it is an artifact of the training data more than the implementation. Reducing `max_mel_frames` to 500 has made a big impact. Also now one can synthesize really long texts if the attention keeps up. (fatchord stops synthesizing whenever the synthesizer predicts empty frames, so the model may ""quit"" in the middle of a text)

Rate of speech (#347) may be an issue but it at least seems to be consistent and affects short text inputs too. (Edit: I also think this is from the training data which has similar speech rate.) We might add an option to the toolbox UI to postprocess the output with a speed multiplier that doesn't affect pitch.

At my level of compute power, punctuation causes more problems than it solves, but I haven't given up yet. My model is currently on par with the tensorflow model if I clean the input of all punctuation. Maybe slightly worse in terms of voice similarity and quality (it's quite similar to the samples previously shared), but better in terms of gaps and other weird behavior. It still has a few more days to go before completing its training schedule.",synthesizer mostly fixed switching think artifact training data implementation reducing made big impact also one synthesize really long attention whenever synthesizer empty model may quit middle text rate speech may issue least consistent short text edit also think training data similar speech rate might add option toolbox output speed multiplier affect pitch level compute power punctuation given yet model currently par model clean input punctuation maybe slightly worse voice similarity quality quite similar previously better weird behavior still day go training schedule,issue,negative,negative,neutral,neutral,negative,negative
672016043,"@blue-fish I don't think an encoder trained on multiple languages would impact the synthesizer or vocoder. But that is just a hunch, I haven't run an experiment to test that theory.
",think trained multiple would impact synthesizer hunch run experiment test theory,issue,negative,neutral,neutral,neutral,neutral,neutral
671982477,"@sberryman Sorry! I just noticed in the [screenshots](https://user-images.githubusercontent.com/324437/68428522-3623e900-0161-11ea-982f-91f27faa5c65.png) that the visdom environment says ""mixed"" so it looks like 768/256 is a mixed encoder. It was my wishful thinking to think it was English, but we will take it and finetune it from there if you are willing to dig it up.",sorry environment mixed like mixed wishful thinking think take willing dig,issue,negative,negative,neutral,neutral,negative,negative
671976798,@blue-fish I just went through all the files I saved from training and I can't find the English model with 768/256.,went saved training ca find model,issue,negative,neutral,neutral,neutral,neutral,neutral
671964027,"Here are instructions for installing ffmpeg on windows: https://video.stackexchange.com/a/20496

It's really only needed if you want to use mp3 files as input (to create the speaker embedding).",really want use input create speaker,issue,negative,positive,positive,positive,positive,positive
671954727,">Just extract them into C:\Users\Raphael\Documents\GitHub\Real-Time-Voice-Cloning....not in models folder

Thanks, that was my mistake.
Can you say me where I have to put the ffmpeg libary into?
",extract folder thanks mistake say put,issue,negative,positive,positive,positive,positive,positive
671951358,"> I downloaded the pretrained models from this link https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models
> Yes, I am running the `python demo_toolbox.py `command from this location `C:\Users\Raphael\Documents\GitHub\Real-Time-Voice-Cloning\`
> The path to the extracted model is `C:\Users\Raphael\Documents\GitHub\Real-Time-Voice-Cloning\models` in that I can find all of the model files:
> 
> ```
> encoder\saved_models\pretrained.pt
> synthesizer\saved_models\logs-pretrained
> vocoder\saved_models\pretrained\pretrained.pt
> ```

Just extract them into C:\Users\Raphael\Documents\GitHub\Real-Time-Voice-Cloning....not in models folder",link yes running python command location path extracted model find model extract folder,issue,negative,neutral,neutral,neutral,neutral,neutral
671938551,"I downloaded the pretrained models from this link [https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models)
Yes, I am running the `python demo_toolbox.py `command from this location `C:\Users\Raphael\Documents\GitHub\Real-Time-Voice-Cloning\`
The path to the extracted model is `C:\Users\Raphael\Documents\GitHub\Real-Time-Voice-Cloning\models` in that I can find all of the model files: 
```
encoder\saved_models\pretrained.pt
synthesizer\saved_models\logs-pretrained
vocoder\saved_models\pretrained\pretrained.pt
```",link yes running python command location path extracted model find model,issue,negative,neutral,neutral,neutral,neutral,neutral
671935140,"1. Did you download the pretrained models? The URL should be on the second line of the error message, but in case you can't see it, it is: https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models

2. Do the following files exist after extracting the archive in step 1?
(The file/folder paths are relative to the top-level folder of the repo)
```
encoder\saved_models\pretrained.pt
synthesizer\saved_models\logs-pretrained
vocoder\saved_models\pretrained\pretrained.pt
```

3. Are you running the `python demo_toolbox.py` command from this location?
`C:\Users\Raphael\Documents\GitHub\Real-Time-Voice-Cloning\`",second line error message case ca see following exist archive step relative folder running python command location,issue,negative,neutral,neutral,neutral,neutral,neutral
671746690,"> Having spent several hours to get the Swedish model (#257) to work, I think it is a good idea to save the hparams along with the models. Maybe even load them at run time from the model files.
> 
> Then we can mix and match in the toolbox, and it can check for compatibility, e.g. `speaker_embedding_size` in encoder and synth, `sample_rate` between the synth and vocoder. Then we can make helpful error messages to replace the python exceptions that occur when models are incompatible.
> 
> Since we have to re-release models when this gets merged to master, it is an opportunity to implement this new checkpoint format. In addition to ""model_state"" and ""optimizer_state"", we can also save a dictionary of ""model_parameters"" which would contain something like:
> 
> #### Encoder
> * `sample_rate`
> * `speaker_embedding_size`
> 
> #### Synthesizer
> * symbols
> * language?
> * `speaker_embedding_size`
> * `sample_rate`, `n_mels`, `n_fft`, etc.
> 
> #### Vocoder
> * `sample_rate`, `n_mels`, `n_fft`, etc.
> 
> I'll put this in a new issue and see if anyone wants this feature.

This is exactly what you shouldn't do, and it's good that you gave up on it. 

For example, say you start training the voice encoder with default hparams. Then you train the synthesizer, and you decide to change some unrelated hparam in the synthesizer. You're already running into trouble because your hparams won't match that of the voice encoder checkpoint. In another scenario, you have a pretrained synthesizer model you want to finetune on another dataset, but the dataset requires different hparams than the one the synthesizer was trained on. Clash again. 

To work around that issue you would have to say which hparams are relevant to which model/situation, and which hparams should be compared and which should not. It's far from trivial. You also have to think about how the user inputs hparams in the CLI, and how the overrides happen. Does a model apply hparam overrides when you load it in memory? What if you have two instances of a model with different hparams?

As I said, better to have a simplistic but limited solution than to try to solve this. Not that there isn't a good solution, only that it takes a major development effort.

Regarding your synthesizer, is it expected to generate exactly the same output as the current tf one? If yes, make sure of it. If no, you'll have to retrain a vocoder.",spent several get model work think good idea save along maybe even load run time model mix match toolbox check compatibility make helpful error replace python occur incompatible since master opportunity implement new format addition also save dictionary would contain something like synthesizer language put new issue see anyone feature exactly good gave example say start training voice default train synthesizer decide change unrelated synthesizer already running trouble wo match voice another scenario synthesizer model want another different one synthesizer trained clash work around issue would say relevant far trivial also think user happen model apply load memory two model different said better simplistic limited solution try solve good solution major development effort regarding synthesizer generate exactly output current one yes make sure retrain,issue,positive,positive,positive,positive,positive,positive
671696398,"@sberryman I am inquiring about the 768/256 English model that you started training here: https://github.com/resemble-ai/Resemblyzer/issues/13#issuecomment-551269980 Right before you closed the issue it reached 922,500 steps. The dropbox model you linked is trained to 2,143,500 steps, not the right one (it looks like 768/768).

It must be nice to be so productive that you don't fully remember the details of everything you've worked on! 😄 ",inquiring model training right closed issue model linked trained right one like must nice productive fully remember everything worked,issue,positive,positive,positive,positive,positive,positive
671680333,"> I always just get a 'torch module' doesn't exist even though I've clearly installed it several times.

It sounds like you may have a really old version of pip. Upgrade your pip as follows:
```
python -m pip install pip --upgrade
```

Then you can `pip install torch` and get the real thing.",always get module exist even though clearly several time like may really old version pip upgrade pip python pip install pip upgrade pip install torch get real thing,issue,positive,positive,positive,positive,positive,positive
671679333,"@NerdIt I feel your frustration. How do you self-rate your computer skills? I will offer to walk you through the setup if you want to give it one last try. We will quickly diagnose what is going on in Windows, and if that doesn't work then you can set up a virtual machine with Ubuntu and run it there.",feel frustration computer offer walk setup want give one last try quickly diagnose going work set virtual machine run,issue,negative,positive,positive,positive,positive,positive
671678093,"For anyone who is following this: I decided against including hparams along with the model checkpoints at this time. I just think the effort can be better spent elsewhere. Also don't see much benefit to consolidating the hparams in a single file, so will not be doing that either.

I am having trouble producing a pretrained model that can equal the current synthesizer. Although I generally don't have gaps, the cloned voice has some unpleasant distortion (like #314). Also, at my level of compute power I think removing all punctuation will help achieve a usable model faster. I'll be adding an option to ignore punctuation in training and/or inference.

The synthesized spectrograms from the fatchord tacotron are always identical. I might not be activating the dropout at inference time. Expect it should take a few days to resolve all of these.",anyone following decided along model time think effort better spent elsewhere also see much benefit single file either trouble model equal current synthesizer although generally voice unpleasant distortion like also level compute power think removing punctuation help achieve usable model faster option ignore punctuation training inference always identical might dropout inference time expect take day resolve,issue,positive,negative,neutral,neutral,negative,negative
671675613,"@blue-fish, I trained a model with 768 hidden and 768 output. I didn't drop the output back down to 256d (I had started training before Corentin replied.) Or maybe I did and completely forgot about it as I trained so many variations.

I know you read through my issue #126 as you commented on it. So remember, I only trained the encoder model. I didn't have any lucky training the synthesizer or vocoder. I honestly don't remember what languages the uploaded model was trained with but it did perform better than Corentin's pertained model for my task of speaker identification.

Model:
https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/126#issuecomment-531350334

You can also try this, I'm assuming it is the English only model 😁
https://www.dropbox.com/s/3xvjobg130x0tr9/english_run.pt?dl=0

",trained model hidden output drop output back training maybe completely forgot trained many know read issue remember trained model lucky training synthesizer honestly remember model trained perform better model task speaker identification model also try assuming model,issue,positive,positive,positive,positive,positive,positive
671674803,On second thought I am not going to add the feature at this time. I still think it is a good idea. Anyone interested is welcome to reopen the issue.,second thought going add feature time still think good idea anyone interested welcome reopen issue,issue,positive,positive,positive,positive,positive,positive
671674228,"Thanks for the response, I am on windows, so for now I will settle for quick audio edits in audacity (I saw you call webrtcvad a nightmare earlier, so for me it'd probably be a nightmare inception haha)  I could also reinstall on an Ubuntu VM, but I don't think it's worth it to me.

Thanks for the link, your proposed tool to facilitate creating a custom dataset sounds very helpful - I'll definitely be playing around with integrating datasets, and possibly making a single speaker TTS model tomorrow if I can manage it (learning from scratch).  
edit: just opened the toolbox for the first time and noticed the long inputs & pauses were already features lmao",thanks response settle quick audio audacity saw call nightmare probably nightmare inception could also reinstall think worth thanks link tool facilitate custom helpful definitely around possibly making single speaker model tomorrow manage learning scratch edit toolbox first time long already,issue,positive,positive,positive,positive,positive,positive
671672941,@sberryman I had no idea you already trained an English model with (768 hidden size / 256 output size) like we propose to do here. Are you able to share your model with us? We could resume the training where it left off.,idea already trained model hidden size output size like propose able share model u could resume training left,issue,positive,positive,positive,positive,positive,positive
671665508," My entire goal of working with this project was the embedder (edit: encoder). If you want to learn a lot more about the embeddings you can read my conversation with Corentin on this issue:
https://github.com/resemble-ai/Resemblyzer/issues/13",entire goal working project edit want learn lot read conversation issue,issue,negative,neutral,neutral,neutral,neutral,neutral
671662925,"@mbdash reducing the pool to 5 was a bare minimum in my opinion. You wan't the same speaker speaking multiple phrases ideally with different environments and microphones. Considering Common Voice is free, it isn't surprising 21,494 people haven't provided 5 or more samples.

I did way more than Common Voice and Librispeech. If you look through my WIP branch you'll see quite a few scripts to help process other datasets. Most of the other datasets were extremely easy to pre-process though. 

VCTK: https://github.com/sberryman/Real-Time-Voice-Cloning/blob/wip/encoder/preprocess.py#L198-L210
",reducing pool bare minimum opinion wa speaker speaking multiple ideally different considering common voice free surprising people provided way common voice look branch see quite help process extremely easy though,issue,positive,positive,positive,positive,positive,positive
671648066,"@woodrow73 
1. For mac and linux, installing webrtcvad is a no-brainer, it's easy and fixes the synthesizer gaps (check the ""enhance vocoder output"" box in the toolbox). For windows I would only recommend it if preprocessing datasets for training.

2. No, anything you do inside the toolbox UI does not alter the model weights. You will know if you are running a training script, it is impossible to do by accident. If you need better voice quality try the single-speaker finetuning process in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-663308789. It's slow on mac due to lack of GPU support but that's what I used for my experiments, so it's possible if you're patient.",woodrow mac easy synthesizer check enhance output box toolbox would recommend training anything inside toolbox alter model know running training script impossible accident need better voice quality try process slow mac due lack support used possible patient,issue,positive,negative,neutral,neutral,negative,negative
671596286,"Took me the morning, but I'm happy to say I got it running.  Really want to thank you blue-fish, Corentin, and others for the contributions to this project, and making this masterpiece accessible.  After a little tinkering with audio and text input, I already have audio output that blew my socks off.  I've noticed a few hacks to control the output, like putting a period after every word if I want more word spacing, and also how the word choice (perhaps mostly at the start) changes the tone of the output - reflecting the training data.

I'm thinking of making an application that automatically gives voices to the characters in the text based game called AI Dungeon, I'm hype for that.

I have a 2 quick questions if I may - 1. do you recommend installing webrtcvad, and 2.  Do additional datasets permanently change the model, or can you just remove the datasets to nullify them?  Also if there's any interesting or memey datasets you recommend outside of train-clean-100, hit me up

edit: also feel free to remove this later, since it's off topic",took morning happy say got running really want thank project making masterpiece accessible little audio text input already audio output control output like period every word want word spacing also word choice perhaps mostly start tone output reflecting training data thinking making application automatically text based game ai dungeon quick may recommend additional permanently change model remove nullify also interesting recommend outside hit edit also feel free remove later since topic,issue,positive,positive,positive,positive,positive,positive
671496809,"shawwn's fork is out of date now. Since then I got CPU support merged upstream in #366 and added a lot of other small enhancements that make it a lot more usable. I have a mac and Corentin's repo runs just fine on it.

It annoys me that the toolbox defaults to `.DS_Store` for synthesizer, which leads to that error. We should fix it to ignore hidden folders.",fork date since got support upstream added lot small make lot usable mac fine toolbox synthesizer error fix ignore hidden,issue,negative,positive,neutral,neutral,positive,positive
671482636,"@mbdash To be clear, I'm not specifying a bitrate but a sample rate. That sample rate needs to match what you have defined in `encoder/params_data.py`
https://github.com/sberryman/Real-Time-Voice-Cloning/blob/master/encoder/params_data.py#L9",clear sample rate sample rate need match defined,issue,negative,positive,positive,positive,positive,positive
671477273,"I can confirm I previously deleted the tsv during the process....
I can see them now that i am re-extracting the archive.
I will also re-process using your script since you specify a bitrate for wav conversion, which i didnt do when i did it last weekend.
I will keep you guys posted.",confirm previously process see archive also script since specify conversion didnt last weekend keep posted,issue,negative,negative,neutral,neutral,negative,negative
671462526,"I should also clarify that the script I linked to does NOT add the language to the speaker specific folders. I just grabs the first 20 chars of the speaker_id provided by CV.

Ensure you are using the modifications I made to `encoder/preprocess.py` for preprocessing common voice. 
https://github.com/sberryman/Real-Time-Voice-Cloning/blob/wip/encoder/preprocess.py#L224-L244",also clarify script linked add language speaker specific first provided ensure made common voice,issue,negative,negative,neutral,neutral,negative,negative
671460443,"@mbdash no need to apologize, none of this is really documented. Requires reading through tons of comments on issues, some of which are closed I'm sure.

It is neat to see such strong demand for this project.

I had an idea a while ago to create a platform for cloning facial images and speech and standardize the training process a bit by making it easy to swap out backbone architectures, etc. Then ideally people could join in the project in various capacities. Some might help label new data, add new datasets, contribute their GPU(s) to a training pool, etc. Then we could build a web based UI to interact and run inference on pre-trained models. It is clear to me that the UI aspect of this project made it very approachable to everyone. But fine tuning or changing out datasets is confusing. (I've been working on the visual side recently)",need apologize none really reading closed sure neat see strong demand project idea ago create platform facial speech standardize training process bit making easy swap backbone ideally people could join project various might help label new data add new contribute training pool could build web based interact run inference clear aspect project made approachable everyone fine tuning working visual side recently,issue,positive,positive,positive,positive,positive,positive
671455674,"@sberryman yes thank you for your quick response, I guess i might have ""misplaced"" a folder :-s

I will delete my extracted files, un tar again and be more careful.

thank you",yes thank quick response guess might folder delete extracted un tar careful thank,issue,positive,positive,positive,positive,positive,positive
671451115,@sberryman Thank you so much for the prompt and helpful replies.,thank much prompt helpful,issue,positive,positive,positive,positive,positive,positive
671449033,"@mbdash There is a validated.tsv included in every language download from Common Voice.

For example:

1. Download Greek dataset
1. Extract `el.tar.gz`
1. `base_dir=""./cv-corpus-5.1-2020-06-22/{lang}/""`
1. You will see several tsv files, `validated.tsv` being one of them.

The clips folder is where all the audio clips are stored and what I'm assuming you are running encoder pre-processing against.

My script will create a new folder called `speakers` in the `base_dir` and will then create a new sub folder for each speaker which will include the language (el) and the first 20 chars of the speaker_id provided by CV.

Once that script finishes you'll be able to run the encoder pre-processing against the `{base_dir}/speakers` directory.",included every language common voice example extract see several one clip folder audio clip assuming running script create new folder create new sub folder speaker include language el first provided script able run directory,issue,negative,positive,positive,positive,positive,positive
671433647,"The test failed for me, however listed the same issue.  The fix for me was to properly copy over the pre-trained models, by copying over the saved_models folders **without** the folders they originally were contained within.

The resulting file paths for the pre-trained models should be:

- enc_model_fpath: encoder\saved_models\pretrained.pt

- syn_model_dir: synthesizer\saved_models\logs-pretrained

- voc_model_fpath: vocoder\saved_models\pretrained\pretrained.pt",test however listed issue fix properly copy without originally within resulting file,issue,negative,positive,positive,positive,positive,positive
671430960,"@blue-fish and @mbdash 

You should cancel your current pre-processing. You need a unique folder per speaker in the pre-processed folder for the encoder. I wrote a little script to pre-process common voice dataset(s) for each language. It was run against a release of CV from at least a year ago. I doubt the format of `validated.tsv` has changed but just keep that in mind.

Script: https://github.com/sberryman/Real-Time-Voice-Cloning/blob/wip/scripts/cv_2_speakers.py

You'll need to adjust line 26 for the base directory of common voice. One of the arguments to the script is `--lang` which is just the subfolder for the language. Fairly useless if you plan to hardcode the path on line 26.

The other arguments are for min and max number of audio segments per speaker. Feel free to adjust that based on your needs, I found that minimum of 5 worked well for me.

So this loops over every speaker id in the `validated.tsv` file and groups the audio clips per speaker into a dictionary. Then it processes each speaker, grabs the first 20 chars of the speaker id and uses that for the path name in the pre-processed directory. Finally it uses ffmpeg to convert the mp3's to wav and downsamples to 16000hz. The sample rate is hardcoded so if you want to adjust that change it on line 93.

It takes a while but works great, I did the entire CV dataset for my encoder (all languages.)

**Edit: Also be very careful about lines 60-61. It will `rmtree` the output path!**
Edit 2: When this step has finished you can run the encoder pre-processing script against the `{base_path}/{lang}/speakers` directory.",cancel current need unique folder per speaker folder wrote little script common voice language run release least year ago doubt format keep mind script need adjust line base directory common voice one script language fairly useless plan path line min number audio per speaker feel free adjust based need found minimum worked well every speaker id file audio clip per speaker dictionary speaker first speaker id path name directory finally convert sample rate want adjust change line work great entire edit also careful output path edit step finished run script directory,issue,positive,negative,neutral,neutral,negative,negative
671415082,"> I do not know if the training quality will be affected by this since random files from different speakers are mix within those folders.

Thanks for bringing this up @mbdash . The whole point of the speaker encoder is to learn to distinguish voices from different speakers. If the folder name is used to uniquely ID the speaker then mixing will be disastrous. Is there any metadata in CommonVoice that can help sort things out before you preprocess? @sberryman can you share how you preprocessed CommonVoice for encoder training?

Edit: Would this issue still exist if you treat each CommonVoice subfolder as its own dataset?",know training quality affected since random different mix within thanks whole point speaker learn distinguish different folder name used uniquely id speaker disastrous help sort share training edit would issue still exist treat,issue,positive,negative,neutral,neutral,negative,negative
671409247,Since this is a known issue I am going to close this now. Please feel free to open a new issue if you experience any other problems during setup or use.,since known issue going close please feel free open new issue experience setup use,issue,positive,positive,positive,positive,positive,positive
671403282,"Having spent several hours to get the Swedish model (#257) to work, I think it is a good idea to save the hparams along with the models. Maybe even load them at run time from the model files.

Then we can mix and match in the toolbox, and it can check for compatibility, e.g. `speaker_embedding_size` in encoder and synth, `sample_rate` between the synth and vocoder. Then we can make helpful error messages to replace the python exceptions that occur when models are incompatible.

Since we have to re-release models when this gets merged to master, it is an opportunity to implement this new checkpoint format. In addition to ""model_state"" and ""optimizer_state"", we can also save a dictionary of ""model_parameters"" which would contain something like:

#### Encoder
* `sample_rate`
* `speaker_embedding_size`
#### Synthesizer
* symbols
* language?
* `speaker_embedding_size`
* `sample_rate`, `n_mels`, `n_fft`, etc.
#### Vocoder
* `sample_rate`, `n_mels`, `n_fft`, etc.

I'll put this in a new issue and see if anyone wants this feature.",spent several get model work think good idea save along maybe even load run time model mix match toolbox check compatibility make helpful error replace python occur incompatible since master opportunity implement new format addition also save dictionary would contain something like synthesizer language put new issue see anyone feature,issue,positive,positive,positive,positive,positive,positive
671385206,"Please note that by separating the CommonVoice files in subfolders, the preprocess in interpreting each folder as a different speaker.

I do not know if the training quality will be affected by this since random files from different speakers are mix within those folders.

Also note this error:
![image](https://user-images.githubusercontent.com/32403586/89793155-831f5c00-daf3-11ea-9680-d8fba8efc479.png)

The reprocessing seems to be going even with the warning.",please note separating folder different speaker know training quality affected since random different mix within also note error image going even warning,issue,negative,negative,negative,negative,negative,negative
671340271,We are not going to pursue tensorflow v2 now that the torch-based synthesizer is working (#472). Thanks to all who contributed their time here.,going pursue synthesizer working thanks time,issue,negative,positive,positive,positive,positive,positive
671337302,"Hi @shoegazerstella !
* hop_length = sample_rate (Hz = 1/seconds) * 0.0125 seconds
    * For 16,000 Hz, hop_length = 16000*0.0125 = 200
    * For 22,050 Hz, hop_length = 22050*0.0125 = 275.675 but it needs to be an integer, so we can round down to 275
* win_length  = sample_rate * 0.05 = hop_length * 4
    * For 16,000 Hz, win_length = 200*4 = 800
    * For 22,050 Hz, win_length = 275*4 = 1100

Something else I discovered since then, I made a mistake in how I was passing the data to the vocoder. Once I fixed it, I found that the original vocoder (16,000 Hz) works quite well. Since I am already training a model at 16,000 Hz, why don't you use 22,050 Hz for better quality? We don't have a 22,050 Hz vocoder model so it will be a nice contribution.

I have also had good results with changing [max_mel_frames](https://github.com/blue-fish/Real-Time-Voice-Cloning/blob/8491b1f3d4dfc7e942c0e38a145215f9560036d3/synthesizer/hparams.py#L50) to 500. This has the following benefits:
1. Shorter utterances are less likely to have long pauses
2. Trains faster
3. Requires less GPU memory, allowing larger batch sizes

What I am currently struggling with is punctuation. If my text has a comma, then my model introduces a 3-4 second pause. Additional training should fix it.",hi need integer round something else discovered since made mistake passing data fixed found original work quite well since already training model use better quality model nice contribution also good following shorter le likely long faster le memory batch size currently struggling punctuation text comma model second pause additional training fix,issue,positive,positive,positive,positive,positive,positive
671283054,"I've had a really good experience with anaconda.  I've installed this project a few times and it's been quite simple if I create and activate a new environment, then, on it, `conda install tensorflow=1.15`,  then `conda install pytorch torchvision -c pytorch` then on the repo `pip install -r requirements.txt`.  Conda makes sure versions are compatible, etc. 
",really good experience anaconda project time quite simple create activate new environment install install pip install sure compatible,issue,positive,positive,positive,positive,positive,positive
671209400,"Hi @blue-fish 
So I cloned [your fork](https://github.com/blue-fish/Real-Time-Voice-Cloning/tree/447_pytorch_synthesizer),

> Do you need me to push the updated hparams to my fork, or do you prefer to figure it out yourself? Note the preprocessing scripts in #472 still reference the old synth, so you will need to modify the old synth's hparams for preprocessing.

For preprocessing, I am modifying the hparams [here](https://github.com/blue-fish/Real-Time-Voice-Cloning/blob/447_pytorch_synthesizer/synthesizer/hparams.py#L4), is that correct?
I will change the sample rate to be 22,050. Do I need to also change hop and win_length accordingly? How can I figure out what values to assign?

Thanks!
",hi fork need push fork prefer figure note still reference old need modify old correct change sample rate need also change hop accordingly figure assign thanks,issue,negative,positive,positive,positive,positive,positive
671185990,"Would like someone to take this on. Preferably using #472 since this project will be pytorch-based going forward, it will ensure compatibility with future synthesizer models.",would like someone take preferably since project going forward ensure compatibility future synthesizer,issue,positive,neutral,neutral,neutral,neutral,neutral
671181883,Presumed to be resolved (reopen and provide more information if it is not),resolved reopen provide information,issue,negative,neutral,neutral,neutral,neutral,neutral
671179674,"@sid0791 Please see #30 regarding support for other languages, or #400 for a list of pretrained models (including languages other than English). You might also want to check out https://github.com/Tomiinek/Multilingual_Text_to_Speech , but I cannot help with it as I have never used it.

I am going to close this issue, you are welcome to open a new one if you have a more specific question.",please see regarding support list might also want check help never used going close issue welcome open new one specific question,issue,positive,positive,positive,positive,positive,positive
671163850,"Thank you for the clarification. For the ""constant"" case is there much value to collect them in the same file? It might be nice for ensuring compatibility between the synth and vocoder, but you already took care of that by [importing the relevant hparams](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/a32962bb7b4827660646ac6dabf62309aea08a91/vocoder/hparams.py#L4-L16) from the synthesizer.

I lack the expertise to implement anything more sophisticated, so I might leave things as they are if there is not much to be gained by using a single hparams file.",thank clarification constant case much value collect file might nice compatibility already took care relevant synthesizer lack implement anything sophisticated might leave much single file,issue,positive,positive,positive,positive,positive,positive
671153355,"If we keep the idea of them being constant or quasi constant through development, then it's fine.

If we start considering serializing hparams with model checkpoints, comparing hparams between models, adding backwards compatibility, ability to override etc... it can get very complex (whether merging in a single file or not).",keep idea constant quasi constant development fine start considering model backwards compatibility ability override get complex whether single file,issue,negative,positive,neutral,neutral,positive,positive
671140467,"Can you elaborate a little bit more on that? Are you saying that for certain parameters, we might want to use different values for different parts of the model (encoder, synthesizer, vocoder)?

Edit: My intention was to define them all in the same file without breaking anything. Did you have something else in mind for a ""unified hyperparameter set""?",elaborate little bit saying certain might want use different different model synthesizer edit intention define file without breaking anything something else mind unified set,issue,positive,positive,positive,positive,positive,positive
671136140,"Be careful with hparams, from my experience with big deep learning projects, no simple implementation of hparams will be a good all-round solution. I know the system I put in this repo is crappy, but beware that you are most likely opening a can of worms when trying to unify them.",careful experience big deep learning simple implementation good solution know system put beware likely opening trying unify,issue,positive,positive,neutral,neutral,positive,positive
671117725,"> Can I simply move all the Celeb 1&2 folders out of the /SV2TTS/encoder folder and move them back in phase 2?

Good idea @mbdash , I looked at the code and I think that will work.",simply move folder move back phase good idea code think work,issue,negative,positive,positive,positive,positive,positive
671115822,"@blue-fish I am still converting CommonVoice mp3 to wav. It has been doing it for hours.

(I began downloading VCTK)

Can I simply move all the Celeb 1&2 folders out of the /SV2TTS/encoder folder and move them back in phase 2?
Or should I restart the preprocess only selecting LibriSpeech + CommonVoice?

I did not look deep enough in the code to see if the preprocess does anything else then populating the /SV2TTS/encoder folder

Here is a sample of what the /SV2TTS/encoder folder looks like:
![image](https://user-images.githubusercontent.com/32403586/89744237-0354ab80-da79-11ea-8612-e72247695bc7.png)
",still converting simply move folder move back phase restart look deep enough code see anything else folder sample folder like image,issue,negative,neutral,neutral,neutral,neutral,neutral
671113623,"@mbdash One more request to make if training has not started yet. I just read https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/364#issuecomment-660069631 and would like to do the encoder training in 2 phases.

1. Start training on LibriSpeech + CommonVoice only, it should converge relatively fast, save off the model
2. Resume training on the model after adding VoxCeleb 1+2 to the training set

The question is, does the phase 2 model perform better than the model from phase 1? I forget how the SV2TTS folder is structured and if this can be easily implemented (maybe you could make a separate `datasets_root/SV2TTS/encoder` folder and use symbolic links to present only the selected datasets to the encoder). It would be a very interesting data point that would help those who want to do encoder training in the future.

Edit: Given that [VCTK](https://datashare.is.ed.ac.uk/handle/10283/3443) is a popular dataset it would be nice to include it either in phase 1 or 1.5 to ensure the resulting model performs well with it. But it is only 110 voices so just a drop in the bucket compared to the others, and not worth holding up the training for it.",one request make training yet read would like training phase start training converge relatively fast save model resume training model training set question phase model perform better model phase forget folder structured easily maybe could make separate folder use symbolic link present selected would interesting data point would help want training future edit given popular would nice include either phase ensure resulting model well drop bucket worth holding training,issue,positive,positive,positive,positive,positive,positive
671110894,">If there is any modifications you want to make prior to me starting the training, please let me know.

Just one mod to make, `model_hidden_size = 768` in [encoder/params_model.py](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/encoder/params_model.py#L3)",want make prior starting training please let know one make,issue,negative,neutral,neutral,neutral,neutral,neutral
671095566,"I am about to begin preprocess on CommonVoice.
If there is any modifications you want to make prior to me starting the training, please let me know.

**update:** 
mp3 return the same errors as m4a...
I guess i will have to convert to wav...

![image](https://user-images.githubusercontent.com/32403586/89738583-e8b60e80-da47-11ea-8356-65c5e6f0a58b.png)

**update 2h later:** i think i have converted 50%+ to wav",begin want make prior starting training please let know update return guess convert image update later think converted,issue,negative,neutral,neutral,neutral,neutral,neutral
671072619,"Restarted training with adjusted hparams to make the tacotron model more similar to this repo. Switched back to Corentin's version of [symbols.py](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/synthesizer/utils/symbols.py) including the use of the EOS symbol ""~"" even though fatchord's tacotron doesn't require it for a stop prediction. See here for more details: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/449#issuecomment-671071445

The pytorch fork is cleaned up and usable for anyone who wishes to test it. If anyone can contribute hardware to train a model that will help bring this to fruition faster.

Use the following command to clone it, or [download a zip file](https://github.com/blue-fish/Real-Time-Voice-Cloning/archive/447_pytorch_synthesizer.zip).
```
git clone -b 447_pytorch_synthesizer --depth 1 https://github.com/blue-fish/Real-Time-Voice-Cloning
```",training make model similar switched back version use symbol even though require stop prediction see fork usable anyone test anyone contribute hardware train model help bring fruition faster use following command clone zip file git clone depth,issue,negative,neutral,neutral,neutral,neutral,neutral
671071445,"I notice that at a low number of steps (say 25k), inference is very sensitive to trailing punctuation. For example `Hello world` (top plot) synthesizes with a lot of trailing emptiness, while `Hello world.` (bottom plot) cleanly terminates. The LibriTTS_200k model from @mbdash shows that it can be overcome with additional training, but I do not like this behavior.

<img width=""623"" alt=""helloworld"" src=""https://user-images.githubusercontent.com/67130644/89736476-8268c600-da1e-11ea-815e-6229b75c515b.png"">

Now experimenting with stripping trailing punctuation which should use the [end of sequence symbol](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/a32962bb7b4827660646ac6dabf62309aea08a91/synthesizer/utils/text.py#L38-L39) ""~"") as an indication of when to stop, instead of the punctuation. If it works well I will add an hparam to ignore punctuation at the end of a text.

Also, now restricting the training set to 500 mel frames or less (default 900) to avoid long silences in the middle of utterances (https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/364#issuecomment-653996443). Here is the code snippet I use to post-process `datasets_root/SV2TTS/synthesizer/train.txt` to implement both of these changes:

```
from pathlib import Path
import string

with Path(""train.txt"").open(""r"") as metadata_file:
    metadata = [line.split(""|"") for line in metadata_file]
    max_frames = 500
    x0 = [x[0] for x in metadata if int(x[4])<=max_frames] # audio filename
    x1 = [x[1] for x in metadata if int(x[4])<=max_frames] # mel filename
    x2 = [x[2] for x in metadata if int(x[4])<=max_frames] # embed filename
    x3 = [x[3] for x in metadata if int(x[4])<=max_frames] # timesteps
    x4 = [x[4] for x in metadata if int(x[4])<=max_frames] # mel frames
    x5 = [x[5] for x in metadata if int(x[4])<=max_frames] # text
        
with Path(""train_edit.txt"").open(""w"") as output_file:
    for i in range(len(x0)):
        text = x5[i].strip().strip(string.punctuation) # first strip() removes newline
        output_file.write(""|"".join([x0[i], x1[i], x2[i], x3[i], x4[i], text]) + ""\n"") 
```",notice low number say inference sensitive trailing punctuation example hello world top plot lot trailing emptiness hello bottom plot cleanly model overcome additional training like behavior stripping trailing punctuation use end sequence symbol indication stop instead punctuation work well add ignore punctuation end text also training set mel le default avoid long middle code snippet use implement import path import string path line audio mel embed mel text path range text first strip text,issue,negative,positive,positive,positive,positive,positive
671054398,"No worries, I need the time anyway to train a model. I'll start working on the unified hyperparameters file and re-add you as a reviewer when I think it is ready for merge.",need time anyway train model start working unified file reviewer think ready merge,issue,positive,positive,positive,positive,positive,positive
671023865,Sorry but I won't be available for 1 week. Take your time,sorry wo available week take time,issue,negative,negative,neutral,neutral,negative,negative
671018035,"@CorentinJ This is a huge PR and merging the hparams will make it even more difficult to review. Please do an initial review and we can either add it before the final review, or do a new PR once this has been merged.

We are still missing a pretrained model. The one I have works well for small utterances but does not align for longer ones, leading to large gaps in the output. I am training a new model to see if I can fix the issue.",huge make even difficult review please initial review either add final review new still missing model one work well small align longer leading large output training new model see fix issue,issue,negative,negative,neutral,neutral,negative,negative
670998716,"The toolbox expects python to be run from the Real-Time-Voice-Cloning directory. Otherwise the relative paths will not work.

If this happens to others we may want to determine the absolute path from the location of Real-Time-Voice-Cloning and use that as the default. Thank you for reporting the issue and your solution @proVoice228 .",toolbox python run directory otherwise relative work may want determine absolute path location use default thank issue solution,issue,positive,positive,neutral,neutral,positive,positive
670951891,I've fixed that by changing the relative path to the absolute path,fixed relative path absolute path,issue,negative,positive,positive,positive,positive,positive
670929263,"@Tombaysbot You can try telling the toolbox you don't have any GPUs so it won't try to load those modules. Set the `CUDA_VISIBLE_DEVICES` environment variable to """".

On my fork, I have a [branch](https://github.com/blue-fish/Real-Time-Voice-Cloning/tree/423_add_cpu_mode) that does this whenever you pass the `--cpu` argument to demo_cli.py or demo_toolbox.py. Use the following command to get it (or download a [zip file](https://github.com/blue-fish/Real-Time-Voice-Cloning/archive/423_add_cpu_mode.zip))

```
git clone -b 423_add_cpu_mode --depth 1 https://github.com/blue-fish/Real-Time-Voice-Cloning
```

With the code, then run one of the following commands:
```
python demo_cli.py --cpu
python demo_toolbox.py --cpu
```

If you find it helpful let me know and I'll put it on the list of things to get merged upstream.",try telling toolbox wo try load set environment variable fork branch whenever pas argument use following command get zip file git clone depth code run one following python python find helpful let know put list get upstream,issue,negative,neutral,neutral,neutral,neutral,neutral
670928462,"@proVoice228 Make sure the files can be found at the locations that the toolbox is expecting.
```
enc_model_fpath: encoder\saved_models\pretrained.pt
syn_model_dir: synthesizer\saved_models\logs-pretrained
voc_model_fpath: vocoder\saved_models\pretrained\pretrained.pt
```

Would appreciate any suggestions for how to make the error message more helpful since someone else had a similar issue in #463.",make sure found toolbox would appreciate make error message helpful since someone else similar issue,issue,positive,positive,positive,positive,positive,positive
670683610,"> Nice results @DereWah ! (I did not expect the ""thank you"" wav... you're welcome!) Thanks for sharing them and for providing independent confirmation that the process in #437 works to finetune a single-speaker model. (Same goes for @Ori-Pixel)
> 
> For future work we should develop a tool that makes it easier, since assembling the dataset is a very manual process.

Thank you for all the work you're putting into this!",nice expect thank welcome thanks providing independent confirmation process work model go future work develop tool easier since manual process thank work,issue,positive,positive,positive,positive,positive,positive
670682094,"Nice results @DereWah ! (I did not expect the ""thank you"" wav... you're welcome!) Thanks for sharing them and for providing independent confirmation that the process in #437 works to finetune a single-speaker model. (Same goes for @Ori-Pixel)

For future work we should develop a tool that makes it easier, since assembling the dataset is a very manual process.",nice expect thank welcome thanks providing independent confirmation process work model go future work develop tool easier since manual process,issue,positive,positive,positive,positive,positive,positive
670678240,"> @DereWah Thanks for updating the links! They work for me now. FYI, you can also put everything in a zip file and then upload to Github in your message window. I think the only disadvantage is that it may be harder (or impossible) to delete if you ever wanted to remove it.

Yeah, I'll stick to that for the future :)",thanks link work also put everything zip file message window think disadvantage may harder impossible delete ever remove yeah stick future,issue,negative,negative,negative,negative,negative,negative
670677874,"@DereWah Thanks for updating the links! They work for me now. FYI, you can also put everything in a zip file and then upload to Github in your message window. I think the only disadvantage is that it may be harder (or impossible) to delete if you ever wanted to remove it.",thanks link work also put everything zip file message window think disadvantage may harder impossible delete ever remove,issue,negative,negative,negative,negative,negative,negative
670677558,"> @DereWah All of your links give me a ""file not found"" error, could you check them?

I have updated the page with working links. Sorry for that ;) @blue-fish ",link give file found error could check page working link sorry,issue,negative,negative,negative,negative,negative,negative
670674138,"@DereWah All of your links give me a ""file not found"" error, could you check them?",link give file found error could check,issue,negative,neutral,neutral,neutral,neutral,neutral
670673528,"### First audio samples with pytorch synthesizer: [samples_tf278k_pt67k.zip](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/5043175/samples_tf278k_pt67k.zip)


Samples for pytorch synth, trained on LibriTTS for an equivalent of 67k steps with batch size of 36. (My [actual batches](https://github.com/blue-fish/Real-Time-Voice-Cloning/blob/bbe57fd90218dc4d7a47450dcda2c60b1ede4cf2/synthesizer_pt/hparams.py#L39-L43) were smaller due to limited GPU memory.) I also synthesize the same utterances with tensorflow (LibriSpeech_278k) for comparison. I use a random seed of 1, enhance vocoder output to trim silences (pytorch did not need it but tf did). The original vocoder (428k) is used in all cases.

This a 16kHz model using fatchord's default settings which are optimized for single speaker. The model is 140 MB (half the size of the current), most of that retaining the optimizer state so training can be stopped and restarted safely. (Thanks to @CorentinJ for finding this issue in https://github.com/fatchord/WaveRNN/issues/87#issuecomment-499456228 and @TheButlah for implementing it.)

For lack of a better term, the tensorflow versions have more ""depth"" to the voice. Otherwise I find the result to be similar. Neither model works particularly well on these speakers (VCTK p240 and p260).

The pytorch model fails to align for longer inputs, resulting in gaps and stuttering speech. We will see if additional training fixes it. Edit: Still the case with 92k equivalent steps. I have discarded this model and started training a new one.",first audio synthesizer trained equivalent batch size actual smaller due limited memory also synthesize comparison use random seed enhance output trim need original used model default single speaker model half size current retaining state training stopped safely thanks finding issue lack better term depth voice otherwise find result similar neither model work particularly well model align longer resulting stuttering speech see additional training edit still case equivalent model training new one,issue,positive,positive,neutral,neutral,positive,positive
670624211,"If tensorflow is entirely removed from this repo, I will change that message for sure.

I still get a lot of feedback from people who spent hours trying to set things up.",entirely removed change message sure still get lot feedback people spent trying set,issue,negative,positive,positive,positive,positive,positive
670613760,@CorentinJ I noticed the updates to the README telling potential users to find a different repo. Has anything changed regarding your intentions for this repo? Do you still want the pytorch synthesizer?,telling potential find different anything regarding still want synthesizer,issue,negative,neutral,neutral,neutral,neutral,neutral
670433426,"The tacotron loss function in the WaveRNN repo does not match the source paper [1703.10135](https://arxiv.org/abs/1703.10135) (see section 3.4 and figure 4). There is a postnet which is supposed to predict linear spectrograms from the mels, but fatchord keeps it in mel scale so it doesn't do much. See https://github.com/fatchord/WaveRNN/issues/123

I tried to work around this by converting the mels back to linear but it is a lossy transformation. A better way of doing it is to preprocess the training wavs into linear spectrograms for the loss calculation. Something to experiment with at a later date, it already works quite well without this feature.",loss function match source paper see section figure supposed predict linear mel scale much see tried work around converting back linear transformation better way training linear loss calculation something experiment later date already work quite well without feature,issue,negative,positive,positive,positive,positive,positive
670426858,"Update: after [fixing a bug that I introduced](https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/472/commits/7420d9c060ae9bfeda4734566205fd85db896554), the torch-based synth works well with the pretrained vocoder.

The output mels from tacotron are scaled to max_abs_value as a result of preprocessing, so they will have a range of [-4, 4] by default. Speech is still intelligible with the erroneous clipping but very degraded. The max_abs_value is used to normalize the mels in the functions where they are consumed.",update fixing bug work well output scaled result range default speech still intelligible erroneous clipping degraded used normalize,issue,negative,negative,negative,negative,negative,negative
670410625,I'm going to close this due to inactivity. It's a nice dataset but not in our planned list of things to do. You could submit an issue to https://github.com/pytorch/audio requesting them to add support in [torchaudio.datasets](https://pytorch.org/audio/datasets.html). That would make it much easier to work with.,going close due inactivity nice list could submit issue add support would make much easier work,issue,positive,positive,positive,positive,positive,positive
670393457,"As @deepgandhideep mentioned, you can finetune a single-speaker model and I have provided directions here. The training can be completed in less than a day without a GPU. https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-663308789
",model provided training le day without,issue,negative,neutral,neutral,neutral,neutral,neutral
670387630,"Questions have been answered, and this issue is inactive. @mbdash Thanks again for inspiring #437 and for your ongoing contributions towards better models. Feel free to open another issue anytime if you have anything to discuss.",issue inactive thanks inspiring ongoing towards better feel free open another issue anything discus,issue,positive,positive,positive,positive,positive,positive
670385373,Closing due to lack of interest. This will be left as an exercise for the reader.,due lack interest left exercise reader,issue,negative,negative,neutral,neutral,negative,negative
670384241,Closing due to inactivity. (Also duplicate of #411),due inactivity also duplicate,issue,negative,negative,negative,negative,negative,negative
670383834,"It turns out `demo_cli.py` already runs the generated_wav through the preprocess which will trim silences if `webrtcvad` is available.

I have changed my mind and do not think it is worth the time to add this feature. It is actually instructive to have the wav output match the displayed spectrogram in the toolbox.",turn already trim available mind think worth time add feature actually instructive output match displayed spectrogram toolbox,issue,negative,positive,positive,positive,positive,positive
670381164,"Actually I will go ahead and close this one now, and reopen it if it is needed.",actually go ahead close one reopen,issue,negative,neutral,neutral,neutral,neutral,neutral
670380704,"With #472 we should not bother with this as I expect we will remove the tensorflow-based synthesizer from the repo. When that is merged, this issue will be closed along with #370.",bother expect remove synthesizer issue closed along,issue,negative,negative,neutral,neutral,negative,negative
670379905,Closed issue due to inactivity.,closed issue due inactivity,issue,negative,negative,negative,negative,negative,negative
670379477,@kkprabhu Closing as duplicate of #411. You are welcome to reopen this issue if you have audio samples and a reproducible test case for the number 2 item (skipped words).,duplicate welcome reopen issue audio reproducible test case number item,issue,positive,positive,positive,positive,positive,positive
670378226,"@hmmhellohi You need to navigate to the directory that contains the Real-Time-Voice-Cloning code, then run the `python demo_toolbox.py` command. However, I think it is very unlikely that it was set up correctly based on the information you have provided.

I wrote this in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/428#issuecomment-660242818 and it is also applicable here.

> You are not the target audience for this project, it is a research code and not a finished product. It happens to be very user-friendly as far as research codes go, but may cause frustration if you do not have any relevant computer experience. I commend you for trying, but you may want to wait for the compiled version (#267) which is in work.

You can also consider the colab notebook (in this repository) and Sagemaker fork #398 as alternatives if you are experiencing difficulties setting up the toolbox.

Although I am closing this issue, you are welcome to open a new one if you have a specific question or problem concerning this toolbox.",need navigate directory code run python command however think unlikely set correctly based information provided wrote also applicable target audience project research code finished product far research go may cause frustration relevant computer experience commend trying may want wait version work also consider notebook repository fork setting toolbox although issue welcome open new one specific question problem concerning toolbox,issue,negative,positive,positive,positive,positive,positive
670373171,"> If Text input is in German but output is needed in English.

Perform machine translation on your German text to convert it to English. Then use the toolbox as normal with the English text as input. We already have [pretrained models](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models), there is no need to train your own based on what you have told me so far.

I apologize but I need to close this issue because your query is outside the scope of what we can provide support for here. Issues are mainly for bug reports and feature requests. Help for toolbox installation and use is only provided if we have time for it. We cannot consult or assist with personal projects.

Mozilla TTS has an online community where you can ask your question. https://discourse.mozilla.org/c/tts/285",text input german output perform machine translation german text convert use toolbox normal text input already need train based told far apologize need close issue query outside scope provide support mainly bug feature help toolbox installation use provided time consult assist personal community ask question,issue,positive,positive,neutral,neutral,positive,positive
670342796,"HI 

Thanks for the quick reply. When i dig into the code , i found it. i am very new to Speech Algorithm.
But i have an another query regarding mixed data:
If Text input is in German but output is needed in English. Could you please help, which data needs to be trained in three sets: Encoder, Synthesizer and Vocoder.


",hi thanks quick reply dig code found new speech algorithm another query regarding mixed data text input german output could please help data need trained three synthesizer,issue,positive,positive,positive,positive,positive,positive
670299376,"@Tayal-S Still training the encoder (#467)?

You will want to change this line to `librispeech_datasets[""dev""][""clean""]`:
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/785e5513b45daf6a83b86b0b459f1717033859cb/encoder/preprocess.py#L122

The available configurations are defined here:
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/785e5513b45daf6a83b86b0b459f1717033859cb/encoder/config.py#L1-L14

The software is not too user-friendly so you will need to edit the code yourself to make it do new things. When the preprocessing is working please close out the issues you have opened.",still training want change line dev clean available defined need edit code make new working please close,issue,positive,positive,positive,positive,positive,positive
670265330,"Following up on my earlier comments, this table is from [1806.04558](https://arxiv.org/pdf/1806.04558.pdf) (the SV2TTS paper):

<img width=""660"" alt=""1806 04558_screenshot"" src=""https://user-images.githubusercontent.com/67130644/89597463-1c5a2400-d80f-11ea-82a4-c313ed38a677.png"">

> Finally, we note that the proposed model, which uses a speaker encoder trained separately on a corpus of 18K speakers, significantly outperforms all baselines

@mbdash It is still worth one attempt to add the 60k speakers from CommonVoice to the encoder and increase the hidden layers to see if we can achieve open-source zero-shot voice cloning that is as good as the results they published. While you're preparing that dataset I will also read the GE2E paper to see if anything else should be changed for this experiment.

Edit: If you think VoxCeleb is too noisy of a dataset we can also try LibriSpeech + CommonVoice, or just CommonVoice alone.",following table paper finally note model speaker trained separately corpus significantly still worth one attempt add increase hidden see achieve voice good also read gee paper see anything else experiment edit think noisy also try alone,issue,positive,positive,positive,positive,positive,positive
670248422,"I have begun downloading the Mozilla CommonVoice dataset and will add it to the encoder pretraining.

I am adding the preprocess fn to my version of **encoder/preprocess.py**
(you can note that I hardcoded a default fallback value for lang = lang or 'en'
```
def preprocess_commonvoice(datasets_root: Path, out_dir: Path, lang=None, skip_existing=False):
    lang = lang or 'en'    
    # simple dataset path
    dataset_name = ""CommonVoice/{0}/speakers"".format(lang)

    # Initialize the preprocessing
    dataset_root, logger = _init_preprocess_dataset(dataset_name, datasets_root, out_dir)
    if not dataset_root:
        return

    # Preprocess all speakers
    speaker_dirs = sorted(list(dataset_root.glob(""*"")))

    # speaker_dirs = speaker_dirs[0:4000] (complete)
    # speaker_dirs = speaker_dirs[4000:5000] (complete)
    # speaker_dirs = speaker_dirs[5000:7000] (complete)
    # speaker_dirs = speaker_dirs[7000:8000] (complete)
    # speaker_dirs = speaker_dirs[8000:9000] (in-progress)
    # speaker_dirs = speaker_dirs[9000:] (in-progress)

    _preprocess_speaker_dirs(speaker_dirs, dataset_name, datasets_root, out_dir, ""wav"",
                             skip_existing, logger)
```

I also updated my **encoder_preprocess.py** accordingly.

I will keep you guys updated.
(and attempt to create a push for my changes when done)",begun add pretraining version note default fallback value path path simple path initialize logger return sorted list complete complete complete complete logger also accordingly keep attempt create push done,issue,positive,positive,neutral,neutral,positive,positive
670248226,"### Some thoughts on the encoder model

Maybe a better encoder is not needed after all, depending on the objective. Although the SV2TTS paper demonstrated the possibility of high-quality zero-shot cloning, I think what most people are after is a high-quality single-speaker TTS. If that is the objective, we have demonstrated in #437 that a decent single-speaker model can be finetuned from the pretrained models at significantly less effort than traditional TTS models. The required dataset goes from 10+ hours to about 10 minutes, a reduction of nearly 2 orders of magnitude.

For this purpose, **the speaker encoder acts as a starting point for the finetuning task** and the quality of encoding mainly determines how much finetuning is needed. The best case is that no additional training is needed, i.e. high-quality zero shot voice cloning per the SV2TTS paper. The worst case is bounded by the 10+ hours needed to train a single-speaker TTS.

With a better encoder and synthesizer, the required dataset for finetuning can really only go down by 1 order of magnitude: just 1 minute of audio. An reduction of 2 orders of magnitude (dataset of 10 seconds) is equivalent to zero-shot in terms of performance.

While the idea of making a voice with just 1 minute of training data is more appealing than the current 10 minutes, is it an order of magnitude improvement from the perspective of the end user? Or in other words, how much effort is appropriate for the encoder given the potential improvement to be had? Arguably, the encoder is already good enough and our limited resources are better spent on the synthesizer which has a lot of known issues.",model maybe better depending objective although paper possibility think people objective decent model significantly le effort traditional go reduction nearly magnitude purpose speaker starting point task quality mainly much best case additional training zero shot voice per paper worst case bounded train better synthesizer really go order magnitude minute audio reduction magnitude equivalent performance idea making voice minute training data appealing current order magnitude improvement perspective end user much effort appropriate given potential improvement already good enough limited better spent synthesizer lot known,issue,positive,positive,positive,positive,positive,positive
670227682,"@mbdash Although it is preferable to change just one variable with our training experiment, we know that the encoder gets better with more voices so I would like to suggest including the Mozilla CommonVoice dataset, which has over 60k unique English speakers: https://voice.mozilla.org/en

Let's try to incorporate this one if you have the time and patience to preprocess it. @sberryman has written a snippet of code for just that purpose: https://github.com/sberryman/Real-Time-Voice-Cloning/blob/d6ba3e1ec0f950636e9cac3656c0be5c331821cc/encoder/preprocess.py#L224-L244",although preferable change one variable training experiment know better would like suggest unique let try incorporate one time patience written snippet code purpose,issue,positive,positive,positive,positive,positive,positive
670079332,"@blue-fish I agree:
`I would still like to make an effort to have all communication here that may be of interest in the future. The slack channel is intended as a temporary thing for the next few weeks while we get new models trained up, and will not be archived.`",agree would still like make effort communication may interest future slack channel intended temporary thing next get new trained,issue,positive,positive,neutral,neutral,positive,positive
670057409,"@mbdash We can try it out. Invited you to a repo that has the invite link. If anyone else wants it, reply here. (Edit: for serious collaboration purposes only. Not for tech support questions about the toolbox.)

I would still like to make an effort to have all communication here that may be of interest in the future. The slack channel is intended as a temporary thing for the next few weeks while we get new models trained up, and will not be archived.",try invite link anyone else reply edit serious collaboration tech support toolbox would still like make effort communication may interest future slack channel intended temporary thing next get new trained,issue,positive,negative,neutral,neutral,negative,negative
670029769,"> > Also, when training the vocoder, it seems to autonatically stop after around epoch 350.
> 
> @DereWah I ran into this issue too for small datasets. Adjust this line:
> https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/785e5513b45daf6a83b86b0b459f1717033859cb/vocoder/train.py#L78
> 
> Would like to hear some of your generated wavs if you don't mind sharing.

Thank you! That solved the error for me. As soon as I train to a  consistent number of steps (like 10k/15k) I'll share the results I have got once I finish the training of the vocoder.

For now I have trained the pretrained Synthesizer for more 1.6k steps with a total of 0.25 hours of recordings splitted in 177 utterances of 5 seconds each.",also training stop around epoch ran issue small adjust line would like hear mind thank error soon train consistent number like share got finish training trained synthesizer total,issue,positive,neutral,neutral,neutral,neutral,neutral
670007847,"> Also, when training the vocoder, it seems to autonatically stop after around epoch 350.

@DereWah I ran into this issue too for small datasets. Adjust this line:
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/785e5513b45daf6a83b86b0b459f1717033859cb/vocoder/train.py#L78

Would like to hear some of your generated wavs if you don't mind sharing.",also training stop around epoch ran issue small adjust line would like hear mind,issue,negative,negative,negative,negative,negative,negative
669999260,"If the PR for SV2TTS on Mozilla's TTS had been submitted 2 weeks earlier, I would have abandoned my effort on the pytorch synthesizer. But since I'm already so far along on #472 (and having learned much along the way), I will try and bring it to completion. It should improve the longevity of this repo and make it a lot more maintainable in the long run.",would abandoned effort synthesizer since already far along learned much along way try bring completion improve longevity make lot maintainable long run,issue,negative,positive,neutral,neutral,positive,positive
669996054,"Please see #472 for the pull request. There is still much work to do. I am training a model at 16000 Hz with fatchord's default settings, and have found that it does not work well with our vocoder, so I expect that needs to be retrained too. Will release some results when they are more respectable but have already proved to myself that voice cloning will work.",please see pull request still much work training model default found work well expect need release respectable already proved voice work,issue,positive,positive,positive,positive,positive,positive
669993194,"@shoegazerstella Thank you so much! It expect it will take 4-7 days to get a pretrained model for each config. Maybe half that if we're just testing hparams and not training to perfection. As a reference point, @mbdash trained LibriTTS_200k in just over 2 days on a 2080ti. Please download the torch-based synthesizer from #472. This will be our new code base which will eventually support global style tokens (#230).

Since putting out the request for help, I discovered that we will need a new vocoder so we should take this opportunity to increase the sample rate to 22,050 or 24,000 Hz. This will require preprocessing to be restarted, but we will get better audio quality in the end.

Do you need me to push the updated hparams to my fork, or do you prefer to figure it out yourself? Note the preprocessing scripts in #472 still reference the old synth, so you will need to modify the old synth's hparams for preprocessing.",thank much expect take day get model maybe half testing training perfection reference point trained day ti please synthesizer new code base eventually support global style since request help discovered need new take opportunity increase sample rate require get better audio quality end need push fork prefer figure note still reference old need modify old,issue,positive,positive,neutral,neutral,positive,positive
669948289,"Just popping in to say that if you have experienced backwards compatibility with matlab, it must have been the result of some miracle fairy. Try to get a class of 20 students to install matlab on their own and to run the same code from 2008, tell me how that works in 2020.",say experienced backwards compatibility must result miracle fairy try get class install run code tell work,issue,positive,positive,positive,positive,positive,positive
669821079,"Hi @blue-fish 
We should be able to contribute too for retraining the model. The max we can use is v100 GPUs. I'll make some trials and see how many we can provide. How long do you think it would take? if not to fully complete, at least to achieve something you can after continue and finish?
I am now downloading LibriTTS and will proceed with its preprocessing following the steps you suggested in the comment above, will let you know before starting the train so we can discuss if some hparam needs to be changed.",hi able contribute model use make see many provide long think would take fully complete least achieve something continue finish proceed following comment let know starting train discus need,issue,negative,positive,positive,positive,positive,positive
669777740,"I have got it to work, and after  1100 steps of synthesizer training and 100 steps on the vocoder it gave pretty decent results. I guess that's because the voice I am training is similar to one the pretrained has already been trained on.

Also, when training the vocoder, it seems to autonatically stop after around epoch 350.",got work synthesizer training gave pretty decent guess voice training similar one already trained also training stop around epoch,issue,negative,positive,positive,positive,positive,positive
669680385,"an even lower tech solution I use-- insert ""scat"" words/syllables at the beginning and end of the sentence and somehow it fixes the gaps. For instance, the sentence ""I have something important to tell you"" gaps terribly on its own, but ""skee diddly bop I have something important to tell you action jackson"" renders perfectly. Then I just trim the ""scat"" off in Audacity. Perhaps that can provide a hint what is wrong in the code.",even lower tech solution use insert scat beginning end sentence somehow instance sentence something important tell terribly skee bop something important tell action perfectly trim scat audacity perhaps provide hint wrong code,issue,negative,positive,neutral,neutral,positive,positive
669619406,"@blue-fish  m4u to wav conversion in progress.
(I was out of commission for a few days)
24k files done while writing this.
",mu conversion progress commission day done writing,issue,negative,neutral,neutral,neutral,neutral,neutral
669404254,"> > @adfost I'm sure you've taken a look at the [wiki](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#datasets). You could find voices similar to what you need in there, or you could transform some other sets to fit the trainer (such as google's audioset).
> > The inability to pronounce the /r/ [r] in /program/ may be due to the embedding space. Try using the embedding from a clip that has that [r]. You could potentially [find it in the spectogram](https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20170720073647023-0563:9781107279865:05157fig8_3.png?pub-status=live), but it will vary based on accent.
> > @DereWah what steps does your trainer start on? It should start on ~278000 if you're training on the correct set. You may be training from 0 again since the command you just gave:
> > `!python synthesizer_train.py first-run synthesizer/saved_models/logs-first-run/datasets_root/SV2TTS/synthesizer --summary_interval 125 --checkpoint_interval 100`
> > Is not the same as the one from above where you were finetuning on the pretrained set:
> > `!python synthesizer_train.py pretrained synthesizer/saved_models/logs-first-run/datasets_root/SV2TTS/synthesizer --summary_interval 125 --checkpoint_interval 100`
> 
> @Ori-Pixel
>@blue-fish
> 
> The checkpoints are starting from 278000. Before starting the training I had copied taco_pretrained from logs-pretrained into logs-first-run
> 
> Every 125 steps it saves a summary, is there a way to check through that if everything's okay?

[This](https://file.io/q1N8t0dTUnZF) is the generated wav file from the synthesizer training after 1k steps.",sure taken look could find similar need could transform fit trainer inability pronounce may due space try clip could potentially find vary based accent trainer start start training correct set may training since command gave python one set python starting starting training copied every summary way check everything file synthesizer training,issue,negative,positive,positive,positive,positive,positive
669394652,"> One of its major weakness is its inability to pronounce the word ""program"" correctly, is there a good dataset including this word for instance?

@adfost Training is the hard way to fix this kind of problem. You can work around it by modifying all instances of ""program"" in your input strings to something that yields a correct pronunciation. (""pro-gram""?) For the transformations you do often, define a text cleaner in [synthesizer/utils/cleaners.py](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/synthesizer/utils/cleaners.py) which automatically does the replacement for you.",one major weakness inability pronounce word program correctly good word instance training hard way fix kind problem work around program input something correct pronunciation often define text cleaner automatically replacement,issue,negative,positive,positive,positive,positive,positive
669379313,"> @adfost I'm sure you've taken a look at the [wiki](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#datasets). You could find voices similar to what you need in there, or you could transform some other sets to fit the trainer (such as google's audioset).
> 
> The inability to pronounce the /r/ [r] in /program/ may be due to the embedding space. Try using the embedding from a clip that has that [r]. You could potentially [find it in the spectogram](https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20170720073647023-0563:9781107279865:05157fig8_3.png?pub-status=live), but it will vary based on accent.
> 
> @DereWah what steps does your trainer start on? It should start on ~278000 if you're training on the correct set. You may be training from 0 again since the command you just gave:
> 
> `!python synthesizer_train.py first-run synthesizer/saved_models/logs-first-run/datasets_root/SV2TTS/synthesizer --summary_interval 125 --checkpoint_interval 100`
> 
> Is not the same as the one from above where you were finetuning on the pretrained set:
> 
> `!python synthesizer_train.py pretrained synthesizer/saved_models/logs-first-run/datasets_root/SV2TTS/synthesizer --summary_interval 125 --checkpoint_interval 100`

@Ori-pixel

The checkpoints are starting from 278000. Before starting the training I had copied taco_pretrained from logs-pretrained into logs-first-run

Every 125 steps it saves a summary, is there a way to check through that if everything's okay?",sure taken look could find similar need could transform fit trainer inability pronounce may due space try clip could potentially find vary based accent trainer start start training correct set may training since command gave python one set python starting starting training copied every summary way check everything,issue,negative,positive,positive,positive,positive,positive
669374332,"@adfost I'm sure you've taken a look at the [wiki](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#datasets). You could find voices similar to what you need in there, or you could transform some other sets to fit the trainer (such as google's audioset). 

The inability to pronounce the /r/ [r] in /program/ may be due to the embedding space. Try using the embedding from a clip that has that [r]. You could potentially [find it in the spectogram](https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20170720073647023-0563:9781107279865:05157fig8_3.png?pub-status=live), but it will vary based on accent.
 

@DereWah what steps does your trainer start on? It should start on ~278000 if you're training on the correct set. You may be training from 0 again since the command you just gave:

`!python synthesizer_train.py first-run synthesizer/saved_models/logs-first-run/datasets_root/SV2TTS/synthesizer --summary_interval 125 --checkpoint_interval 100`

Is not the same as the one from above where you were finetuning on the pretrained set:

`!python synthesizer_train.py pretrained synthesizer/saved_models/logs-first-run/datasets_root/SV2TTS/synthesizer --summary_interval 125 --checkpoint_interval 100`",sure taken look could find similar need could transform fit trainer inability pronounce may due space try clip could potentially find vary based accent trainer start start training correct set may training since command gave python one set python,issue,negative,positive,positive,positive,positive,positive
669348378,"> > You said that it is easy to run it on a GPU, is that done automatically?
> 
> Yes, if `torch.cuda.is_available()` evaluates True then the vocoder runs on GPU.

Okay, thank you 👍 

Also while training the synthesizer these are the wavs I am getting every hundred steps: [200 steps](https://file.io/HawfJNPNWHIk) [500 steps](https://file.io/bMIwvU3TYtHV) These seems like distorted as in a previous issue I had (Outputs were doubled and distorted on the toolbox, but not on the CLI) . 

To train the synthesizer I am running `!python synthesizer_train.py first-run synthesizer/saved_models/logs-first-run/datasets_root/SV2TTS/synthesizer --summary_interval 125 --checkpoint_interval 100`

Thank you for all the patience and the help @blue-fish 

",said easy run done automatically yes true thank also training synthesizer getting every hundred like distorted previous issue doubled distorted toolbox train synthesizer running python thank patience help,issue,positive,positive,positive,positive,positive,positive
669346870,"@blue-fish Do you know of any other good datasets I can train this on? One of its major weakness is its inability to pronounce the word ""program"" correctly, is there a good dataset including this word for instance?",know good train one major weakness inability pronounce word program correctly good word instance,issue,negative,positive,positive,positive,positive,positive
669329417,"> You said that it is easy to run it on a GPU, is that done automatically?

Yes, if `torch.cuda.is_available()` evaluates True then the vocoder runs on GPU.",said easy run done automatically yes true,issue,positive,positive,positive,positive,positive,positive
669318957,"> It's 45 sec/step when I run it on CPU so that's not completely unexpected. A fast computer can get it in less than 10 sec/step (I've seen as low as 6 sec/step on CPU and know it can go even faster).

Yeah, I got it on a colab and it's going at 4.215 sec/step . The next step should be preprocessing the vocoder and training it. You said that it is easy to run it on a GPU, is that done automatically? When I was generating the epochs I noticed my colab was set the wrong way and there was no GPU. Running the command will put the process automatically on the GPU?
Thank you @blue-fish ",run completely unexpected fast computer get le seen low know go even faster yeah got going next step training said easy run done automatically generating set wrong way running command put process automatically thank,issue,negative,positive,neutral,neutral,positive,positive
669205135,It's 45 sec/step when I run it on CPU so that's not completely unexpected. A fast computer can get it in less than 10 sec/step (I've seen as low as 6 sec/step on CPU and know it can go even faster).,run completely unexpected fast computer get le seen low know go even faster,issue,negative,positive,positive,positive,positive,positive
669179760,"@blue-fish Thank you for the reply.

> By the way that's really slow. Activating GPU for pytorch is much easier and you should be able to get in the neighborhood of 1 step/sec. You need this to evaluate true:

>import torch
>torch.cuda.is_available()
>Go find a troubleshooting guide. You can also go through the pytorch source code to where that's set, and work your way back >to see why it's evaluating to False.

I am currently running on a colab, so I should be able to run it on a GPU. When I tried that I was getting some errors about the process location, so I just set it manually to CPU. I thought that it wasn't possible for it to run on the GPU:

> @DereWah I have been unsuccessful in my own attempts to get Tensorflow GPU support, so I can't help you there. GPU support for pytorch is much easier. We'll have it for the synthesizer following #447 (if it ever gets done).

I am now redoing everything from scrap because I noticed I did some errors. Now training the synthesizer seems really really slower than before:

`!python synthesizer_train.py pretrained synthesizer/saved_models/logs-first-run/datasets_root/SV2TTS/synthesizer --summary_interval 125 --checkpoint_interval 100`

```
 Step  278001 [82.243 sec/step, loss=1.38716, avg_loss=1.38716]
Step  278002 [77.581 sec/step, loss=1.25744, avg_loss=1.32230]
Step  278003 [75.563 sec/step, loss=1.28918, avg_loss=1.31126]
Step  278004 [75.285 sec/step, loss=1.22804, avg_loss=1.29045]
Step  278005 [75.143 sec/step, loss=1.14774, avg_loss=1.26191]
Step  278006 [74.507 sec/step, loss=0.99636, avg_loss=1.21765]
Step  278007 [74.083 sec/step, loss=0.84842, avg_loss=1.16490]
Step  278008 [73.845 sec/step, loss=0.80442, avg_loss=1.11984]
Step  278009 [73.502 sec/step, loss=0.80982, avg_loss=1.08540]
Step  278010 [73.284 sec/step, loss=0.77843, avg_loss=1.05470]
Step  278011 [73.211 sec/step, loss=0.74973, avg_loss=1.02698]
Step  278012 [72.951 sec/step, loss=0.75596, avg_loss=1.00439]
Step  278013 [72.785 sec/step, loss=0.74752, avg_loss=0.98463]
```

I'm running on a CPU @ 2.2 GHz

",thank reply way really slow much easier able get neighborhood need evaluate true import torch go find guide also go source code set work way back see false currently running able run tried getting process location set manually thought possible run unsuccessful get support ca help support much easier synthesizer following ever done everything scrap training synthesizer really really python step step step step step step step step step step step step step running,issue,positive,positive,positive,positive,positive,positive
669175139,"@adfost The problems with inputs too long also occurs with other Tacotron-based text-to-speech implementations. It would be nice to fix but I don't know where to start. For now we just work around it by manually conditioning our input text.

@DereWah Yes, the pretrained vocoder has 428k steps so it will resume from there. I also have a 1159k that I think is slightly better (see the ""002"" model in #400). The loss bounces all over the place, you have to wait a very large number of steps (25k or more) before the loss is consistently lower than where you started.

For the args, the documentation is built into the code:
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/054f16ecc186d8d4fa280a890a67418e6b9667a8/vocoder_train.py#L32-L37

Based on your command, the saved models are in `vocoder/saved_models/pretrained/pretrained_429k.pt` (once it gets enough steps to trigger ""--backup_every""). Make sure you always set ""--save_every"" to 0 or keep a backup of pretrained.pt in case you want to use the toolbox with other speakers. My experiments in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-663308789 show that finetuning a vocoder for single-speaker severely degrades it for other speakers.

You can edit the display to get more decimal points if it's rounding down to 0.0 step/sec:
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/054f16ecc186d8d4fa280a890a67418e6b9667a8/vocoder/train.py#L120

By the way that's really slow. Activating GPU for pytorch is much easier and you should be able to get in the neighborhood of 1 step/sec. You need this to evaluate true:

```
import torch
torch.cuda.is_available()
```

Go find a troubleshooting guide. You can also go through the pytorch source code to where that's set, and work your way back to see why it's evaluating to False.",long also would nice fix know start work around manually input text yes resume also think slightly better see model loss place wait large number loss consistently lower documentation built code based command saved enough trigger make sure always set keep backup case want use toolbox show severely edit display get decimal rounding way really slow much easier able get neighborhood need evaluate true import torch go find guide also go source code set work way back see false,issue,positive,positive,positive,positive,positive,positive
669126115,"@blue-fish Hi, I have started training the vocoder. I wanted to ask about a few concerns I had while going through the process.
While training the synthesizer, it seems to resume a checkpoint of 428k steps. Is that what it should do?

[These](https://pastebin.com/B10Gsa2K) are the steps generated during the synthesizer 


And also, while training the vocoder I did this:

> Because the training dataset is very small, generating wavs for every epoch is impractical and will slow down training. Set voc_gen_at_checkpoint = 0 in vocoder/hparams.py to disable.

But the Loss at every epoch doesn't seems to ""increase"" than slightly decrease than ""increase again"":

```
{| Epoch: 1 (2/2) | Loss: 3.7864 | 0.0 steps/s | Step: 428k | }
{| Epoch: 2 (2/2) | Loss: 3.9207 | 0.0 steps/s | Step: 428k | }
{| Epoch: 3 (2/2) | Loss: 3.7410 | 0.0 steps/s | Step: 428k | }
{| Epoch: 4 (2/2) | Loss: 3.9409 | 0.0 steps/s | Step: 428k | }
{| Epoch: 5 (2/2) | Loss: 3.9248 | 0.0 steps/s | Step: 428k | }
```

Checking [this](https://camo.githubusercontent.com/9dd719e31d9ae24e167dabd09c481cb06bd4aa8d/68747470733a2f2f692e696d6775722e636f6d2f724231786b30622e706e67) image on the training wiki it seems to match the Loss graph of the encoder. Is that the graph it should follow?

Last thing, for the training of the vocoder I am using this command:
`!python vocoder_train.py pretrained synthesizer/saved_models/logs-first-run/datasets_root --save_every 0 --backup_every 1000`

The args for --save_every and --backup_every refer to what? The steps?
If so per every epoch how many steps is it using? I can't really tell since it is saying 0.0 steps/s | Step: 428k 

Where can I find the saves to then use in the toolbox?

Thank you for all the help 👍 ",hi training ask going process training synthesizer resume synthesizer also training training small generating every epoch impractical slow training set disable loss every epoch increase slightly decrease increase epoch loss step epoch loss step epoch loss step epoch loss step epoch loss step image training match loss graph graph follow last thing training command python refer per every epoch many ca really tell since saying step find use toolbox thank help,issue,negative,negative,neutral,neutral,negative,negative
669082176,"> @biplab1 please create a virtual environment using ""virtualenv venv_name -p python3"" and not using ""python3 -m venv venv_name""
> then you will note get this error.

couldn't make it work using this method. Still getting the error mentioned above.",please create virtual environment python python note get error could make work method still getting error,issue,negative,neutral,neutral,neutral,neutral,neutral
668984905,Would anyone else like to contribute a GPU to help develop a better synthesizer model? Reply here and get started by preprocessing LibriTTS: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/449#issuecomment-663785345,would anyone else like contribute help develop better synthesizer model reply get,issue,positive,positive,positive,positive,positive,positive
668921049,"@biplab1  please create a virtual environment using ""virtualenv venv_name -p python3"" and not using ""python3 -m venv venv_name"" 
then you will note get this error.",please create virtual environment python python note get error,issue,negative,neutral,neutral,neutral,neutral,neutral
668889944,"@blue-fish another problem: if I put in too long of an input, I get it sometimes completely mispronouncing the last word or couple words",another problem put long input get sometimes completely last word couple,issue,negative,negative,neutral,neutral,negative,negative
668829171,"You probably don't have `webrtcvad` on the PC unless you went out of your way to install it. We took it out of requirements.txt because it was causing grief (#375). But it will clean the audio before making a speaker embed with it.

For quality of the saved wavs using toolbox vs CLI they both use `soundfile.write()` with resampling enabled. There could be some platform differences in libsndfile.

If you have some examples of the ""distorted and doubled"" wavs please open a new issue so we can investigate.",probably unless went way install took causing grief clean audio making speaker embed quality saved toolbox use could platform distorted doubled please open new issue investigate,issue,negative,negative,neutral,neutral,negative,negative
668822711,"> @DereWah I have been unsuccessful in my own attempts to get Tensorflow GPU support, so I can't help you there. GPU support for pytorch is much easier. We'll have it for the synthesizer following #447 (if it ever gets done).
> 
Understood. I'll just go with the CPU. Also while talking with a friend about the project we found out we got totally different outputs while trying it. Him, while using only the CLI on a colab was gettin .wav files without much noise or distortion.

Instead while I tried using the toolbox on my pc was getting distorted and ""doubled"" results. Both of use were using the same voices for embedding (not from LibriSpeech).

Maybe the CLI gives better results? Or maybe I'm having issues with the pc, idk.

(My files remained like that even while using the replay function.)",unsuccessful get support ca help support much easier synthesizer following ever done understood go also talking friend project found got totally different trying without much noise distortion instead tried toolbox getting distorted doubled use maybe better maybe like even replay function,issue,positive,positive,positive,positive,positive,positive
668816944,"@DereWah I have been unsuccessful in my own attempts to get Tensorflow GPU support, so I can't help you there. GPU support for pytorch is much easier. We'll have it for the synthesizer following #447 (if it ever gets done).

@adfost The ""enhance vocoder output"" feature is enabled when `webrtcvad` is installed. For demo_cli.py, it is [always active](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/054f16ecc186d8d4fa280a890a67418e6b9667a8/demo_cli.py#L178-L179). If using demo_toolbox.py, click the checkbox on the right side of the toolbox UI: https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/432#issuecomment-660940390",unsuccessful get support ca help support much easier synthesizer following ever done enhance output feature always active click right side toolbox,issue,positive,positive,neutral,neutral,positive,positive
668808858,"> @adfost The gaps in spectrograms are a known issue (#53). I would also like to fix this but it's not easy: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/364#issuecomment-653996443 . To workaround the issue you can use the ""enhance vocoder output"" option to trim the silences if you have `webrtcvad` installed.
> 
> @DereWah I've finetuned a voice using as little as 5 minutes of data. I would consider this to be the bare minimum, and 10-20 minutes to be generally adequate. Of course it also depends on how varied the dataset is, and how closely your target speaker matches one in the training set. The purpose of this issue is to discuss best practices for finetuning, please share what works well for you.

Thank you

Also do you know how to acivate the GPU instead of the CPU? I have the correct cuda versions and files. Thank you.",known issue would also like fix easy issue use enhance output option trim voice little data would consider bare minimum generally adequate course also varied closely target speaker one training set purpose issue discus best please share work well thank also know instead correct thank,issue,positive,positive,positive,positive,positive,positive
668806088,"### Invalid syntax `def print_args(args: argparse.Namespace, parser=None)`

#### Summary
On many systems (Linux in particular), `python` defaults to Python 2.x which is not compatible with the toolbox. You will need to replace `python` with `python3` in all commands to use the correct version.

#### More information
The error message looks like this.
```
$: python demo_cli.py
Traceback (most recent call last):
  File ""demo_cli.py"", line 2, in <module>
    from utils.argutils import print_args
  File ""Real-Time-Voice-Cloning-master/utils/argutils.py"", line 22
    def print_args(args: argparse.Namespace, parser=None):
                       ^
SyntaxError: invalid syntax
```",invalid syntax summary many particular python python compatible toolbox need replace python python use correct version information error message like python recent call last file line module import file line invalid syntax,issue,negative,positive,positive,positive,positive,positive
668801958,"@adfost The gaps in spectrograms are a known issue (#53). I would also like to fix this but it's not easy: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/364#issuecomment-653996443 . To workaround the issue you can use the ""enhance vocoder output"" option to trim the silences if you have `webrtcvad` installed.

@DereWah I've finetuned a voice using as little as 5 minutes of data. I would consider this to be the bare minimum, and 10-20 minutes to be generally adequate. Of course it also depends on how varied the dataset is, and how closely your target speaker matches one in the training set. The purpose of this issue is to discuss best practices for finetuning, please share what works well for you.",known issue would also like fix easy issue use enhance output option trim voice little data would consider bare minimum generally adequate course also varied closely target speaker one training set purpose issue discus best please share work well,issue,positive,positive,positive,positive,positive,positive
668795602,"I meant only 13 and a half minutes of datasets, not of overall time.",meant half overall time,issue,negative,negative,neutral,neutral,negative,negative
668780756,"@blue-fish If I say something like ""Thank you for using our product"", it says ""thank you"" followed by a very long pause, and then the rest of the sentence.",say something like thank product thank long pause rest sentence,issue,positive,negative,neutral,neutral,negative,negative
668750045,"@blue-fish I will try to convert them when I have some time. 
I'll keep you posted.",try convert time keep posted,issue,negative,neutral,neutral,neutral,neutral,neutral
668743548,"@Ori-Pixel Thanks for helping others get this to work.

@adfost In general the toolbox performs best for inputs of 10-20 words. What problems are you noticing? They might be inherited from the synthesizer architecture and pretrained models.",thanks helping get work general toolbox best might synthesizer architecture,issue,positive,positive,positive,positive,positive,positive
668740108,"@blue-fish Thank you for the help, I got it to work. However, the model I trained seems to work much better with longer input than shorter phrases. I think that the problem is that lack of shorter examples in the training set I used. Any suggestions for better training sets?",thank help got work however model trained work much better longer input shorter think problem lack shorter training set used better training,issue,positive,positive,positive,positive,positive,positive
668736174,"@DereWah The model, from the above CLI instruction `--checkpoint_interval 100` only saves checkpoints ever 100 steps. So cancelling training after a fixed amount of time, if it hasn't reached a checkpoint and saved, won't change anything. You need to just check back with the terminal every once in a while and see if it's saved. The recommended amount from blue-fish is at least 200 steps (2 checkpoint saves).",model instruction ever training fixed amount time saved wo change anything need check back terminal every see saved amount least,issue,positive,negative,neutral,neutral,negative,negative
668732366,"I think I have already setup the CUDA 10.0, and when I run it tensorflow says success message about opening cudart64_100.dll .
I have also followed this tutorial :https://poorlydocumented.com/2019/11/installing-corentinjs-real-time-voice-cloning-project-on-windows-10-from-scratch/ and I have cudnn64_7.dll . I just don't know how to activate the GPU. Also, from training with only 13 and a half minutes did you get mediocre or good results?",think already setup run success message opening also tutorial know activate also training half get mediocre good,issue,positive,positive,neutral,neutral,positive,positive
668726321,"> I noticed tho that my GPU (1060Ti ) is at 0%, while the CPU (i7+ 8th gen) is doing the work.

The tensorflow 1.15 binaries provided by pip are only compatible with CUDA 10.0. GPU support for the synthesizer (the only part that relies on tensorflow) requires you get a proper nvidia driver version and cuda libraries. You will lose much more time setting that up, than you stand to gain in speedup for those 400 steps. (If it is 10 sec/step with the i7 and 1 sec/step with GPU, the speedup will only save an hour over 400 steps.)",tho ti th gen work provided pip compatible support synthesizer part get proper driver version lose much time setting stand gain save hour,issue,positive,positive,neutral,neutral,positive,positive
668721622,"Let me know if this doesn't solve your issue: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/431#issuecomment-665165023

And if it does resolve it, please close this one.",let know solve issue resolve please close one,issue,positive,neutral,neutral,neutral,neutral,neutral
668721152,"This looks like the same issue as #161 and #262. Please make sure you specify the path to your <datasets_root> folder as described in the [wiki documentation](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#preprocessing-and-training).

If it still doesn't work, please share the command that you are using to run encoder_train.py.",like issue please make sure specify path folder documentation still work please share command run,issue,positive,positive,positive,positive,positive,positive
668718487,"@DereWah 
>It is now saying No model to load at synthesizer/saved_models/logs-first_run\taco_pretrained

Make sure your logs-first_run folder is a copy of the logs-pretrained or you won't get a good output. In that folder are the checkpoints it's loading to then finetune on top of.

edit: you also don't have tensorflow gpu setup/running if your CPU is the one processing. You should be getting a dll warning of some sort indicating that you aren't using the GPU. Gpu is faster, but it isn't unreasonable to train to 400 steps with CPU.",saying model load make sure folder copy wo get good output folder loading top edit also one getting warning sort faster unreasonable train,issue,positive,positive,positive,positive,positive,positive
668705602,"Fixed, by using and editing the command in the readme:
Now the pretrained synthesizer model can be trained on the reduced dataset. No changes to hparams are needed.
```
python synthesizer_train.py first_run synthesizer/saved_models/logs-first_run/datasets_root/SV2TTS/synthesizer --summary_interval 125 --checkpoint_interval 100
```
It is now saying No model to load at synthesizer/saved_models/logs-first_run\taco_pretrained

Generated 0 test batches of size 36 in 0.000 sec
Generated 64 train batches of size 36 in 137.217 sec
etc.

I guess it's training.

I noticed tho that my GPU (1060Ti ) is at 0%, while the CPU (i7+ 8th gen) is doing the work. Based on those specs, on which part should I run the training (to optimize time/consume)? 

",fixed command synthesizer model trained reduced python saying model load test size sec train size sec guess training tho ti th gen work based spec part run training optimize,issue,negative,negative,neutral,neutral,negative,negative
668699506,"I am getting this error:

Traceback (most recent call last):
  File ""synthesizer_train.py"", line 55, in <module>
    tacotron_train(args, log_dir, hparams)
  File ""D:\Python\Real-Time-Voice-Cloning\synthesizer\train.py"", line 392, in tacotron_train
    return train(log_dir, args, hparams)
  File ""D:\Python\Real-Time-Voice-Cloning\synthesizer\train.py"", line 144, in train
    feeder = Feeder(coord, metadat_fpath, hparams)
  File ""D:\Python\Real-Time-Voice-Cloning\synthesizer\feeder.py"", line 28, in __init__
    with open(metadata_filename, encoding=""utf-8"") as f:
FileNotFoundError: [Errno 2] No such file or directory: 'dataset_root/SV2TTS/synthesizer\\\train.txt'

For some reasons there is \\\ before the file name. The slashes are 2.

while running 
python synthesizer_train.py first_run dataset_root/SV2TTS/synthesizer --checkpoint_interval 100
",getting error recent call last file line module file line return train file line train feeder feeder file line open file directory file name running python,issue,negative,neutral,neutral,neutral,neutral,neutral
668695750,"1. The pretrained models can generalize to voices and speech (text) unseen during training. This is known as zero-shot voice cloning.
    * The finetuning in #437 is completely optional and will get better results by training on voice samples of the target speaker.
2. Any length of audio can be used (I've done 20 minutes before) but the characteristics of the speaker will be averaged over the entire timeframe, so it may not produce better results than a 5-10 second sample.",generalize speech text unseen training known voice completely optional get better training voice target speaker length audio used done speaker entire may produce better second sample,issue,positive,positive,positive,positive,positive,positive
668690460,"@EnergeticSpaceCore Can you provide an update? I hope this is resolved, but if you're still trying to set up, it would be helpful to know the current problem point.",provide update hope resolved still trying set would helpful know current problem point,issue,positive,neutral,neutral,neutral,neutral,neutral
668684413,"@DereWah mine only have the line of text of the spoken line: 
utterance-000.txt contains `Mireska is here.`
utterance-001.txt contains `Are you ready to have some fun, ya?`
etc.",mine line text spoken line ready fun ya,issue,positive,positive,positive,positive,positive,positive
668680276,"And also, how should the utterance.txt file be formatted? Thank you for the help and the patience",also file thank help patience,issue,positive,neutral,neutral,neutral,neutral,neutral
668679667,"Try replacing `python` with `python3` in your command.
On many systems ""python"" defaults to python2 which is incompatible.",try python python command many python python incompatible,issue,negative,positive,positive,positive,positive,positive
668669694,"This is the command I am using:
python synthesizer_preprocess_audio.py  synthesizer/saved_models/logs-singlespeaker/datasets_root -n 1 --no_alignment

Using no_alignment isn't changing anything. Also I am trying to run this on my GPU, I have cudart64_100.dll and I have installed requirements_gpu.txt

There must be a problem with generating the metadata, I think it will be fixed by making the folder in the same structure as in comment https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-666099538",command python anything also trying run must problem generating think fixed making folder structure comment,issue,negative,positive,neutral,neutral,positive,positive
668668332,"@DereWah check this [comment](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-666641548)

I think the no-alignments option will fix this error since I had that as well in the comment above. Also add your console/terminal command",check comment think option fix error since well comment also add command,issue,negative,neutral,neutral,neutral,neutral,neutral
668665722,"@DereWah As you reasoned, the transcripts are necessary to train the synthesizer (which you can think of as a black box that converts text to mels). Make your folder in the same structure as https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-666099538 and it should work. In the future, please also include the python command that was run to facilitate troubleshooting. Good luck!",reasoned necessary train synthesizer think black box text make folder structure work future please also include python command run facilitate good luck,issue,positive,positive,positive,positive,positive,positive
668662281,"I am getting this error while preprocessing the audio for my own dataset (not from LibriSpeech, it has no alignment) 
Traceback (most recent call last):
  File ""synthesizer_preprocess_audio.py"", line 59, in <module>
    preprocess_dataset(**vars(args))
  File ""D:\Python\Real-Time-Voice-Cloning\synthesizer\preprocess.py"", line 49, in preprocess_dataset
    print(""Max input length (text chars): %d"" % max(len(m[5]) for m in metadata))
ValueError: max() arg is an empty sequence",getting error audio alignment recent call last file line module file line print input length text empty sequence,issue,negative,negative,neutral,neutral,negative,negative
668591562,"I have a set of 5 seconds long flac files from a single speaker. Is it possible to train it without any transcript?

And also, I noticed that in this procedure the training of the encoder, audio and embedding is completely skipped (python encoder_preprocess.py <datasets_root>) without leaving any data to restrict synthesizer training. [I'm new to deep learning and this is what I have ""deciphered"" from this guide. Maybe I've gotten something wrong idk]

I'm trying to train on a voice (not from LibriSpeech)

Thank you

I guess transcript are needed, I wrote a script that does it automatically.  I set it so the transcript is in the  same format of 211-122425.trans.txt",set long single speaker possible train without transcript also procedure training audio completely python without leaving data restrict synthesizer training new deep learning guide maybe gotten something wrong trying train voice thank guess transcript wrote script automatically set transcript format,issue,negative,negative,neutral,neutral,negative,negative
668501774,"Mozilla TTS has a PR implementing SV2TTS: https://github.com/mozilla/TTS/pull/472

Have not tried it yet, but if/when it outperforms this one, it would be a good project to port the toolbox UI and maybe the preprocessing scripts over to that implementation. (I will not be the one to do it, but putting it out there as an idea.)",tried yet one would good project port toolbox maybe implementation one idea,issue,negative,positive,positive,positive,positive,positive
668488561,"Thanks for sharing the samples @shoegazerstella ! The increased noise on LibriTTS_200k is quite obvious. In addition to more training I think it could also benefit from a new vocoder.

LibriTTS_200k is trained from scratch. We have several problems with LibriSpeech_278k, the most annoying of which is the long gaps that appear in the middle of spectrograms (#53). The training from scratch is part of an effort to fix these issues: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/364#issuecomment-653996443 .

I think the next step is to lower [`max_mel_frames`](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/054f16ecc186d8d4fa280a890a67418e6b9667a8/synthesizer/hparams.py#L95-L103) and find some way to clean up LibriTTS (probably calculate the ratio of wav to transcript lengths and removing outliers).

Switching to a pytorch-based synthesizer in #447 may also help since the Rayhane-mamah tacotron that we currently use has some known bugs that would go away by switching to fatchord's implementation in [WaveRNN](https://github.com/fatchord/WaveRNN).",thanks noise quite obvious addition training think could also benefit new trained scratch several annoying long appear middle training scratch part effort fix think next step lower find way clean probably calculate ratio transcript removing switching synthesizer may also help since currently use known would go away switching implementation,issue,positive,negative,neutral,neutral,negative,negative
668455141,"Hi @blue-fish , shure I can share some examples here:
* [Example 1: (LibriSpeech_278)](https://vocaroo.com/jMqzmL7LbHW)
* [Example 2: (LibriTTS_200k)](https://vocaroo.com/9gGRuY2Wh1O)

* [Example 3: (LibriSpeech_278)](https://voca.ro/90ZsePILzWb)
* [Example 4: (LibriTTS_200k)](https://voca.ro/6Z9lkjWS3xa)

Thank you for the explanation on the preprocessing steps!
I have one question, was the model trained from scratch for LibriTTS or you started from a pre-trained model done on LibriSpeech_278? Do you think this approach could make sense for increasing its performances?",hi shure share example example example example thank explanation one question model trained scratch model done think approach could make sense increasing,issue,positive,neutral,neutral,neutral,neutral,neutral
668315362,@mbdash Searching on the error message I came across the suggestion to convert the m4v files to wav which should fix the problem for voxceleb2: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/76#issuecomment-529013562,searching error message came across suggestion convert fix problem,issue,negative,neutral,neutral,neutral,neutral,neutral
668291505,"I fixed the previous error (see bottom of comment) **but I got another crash in VoxCeleb2:**

![image](https://user-images.githubusercontent.com/32403586/89236804-89777a80-d5bf-11ea-9573-2240618b0956.png)


Here is my current pysoundfile version:
![image](https://user-images.githubusercontent.com/32403586/89237320-e6bffb80-d5c0-11ea-854f-39e3edbec0e5.png)

Here is the last files processed:
```
drwxr-xr-x 1    99 users    8676 Aug  3 21:08 VoxCeleb1_wav_id11249
drwxr-xr-x 1    99 users    3594 Aug  3 20:48 VoxCeleb1_wav_id11250
drwxr-xr-x 1    99 users    2586 Aug  3 20:56 VoxCeleb1_wav_id11251
drwxr-xr-x 1    99 users      24 Aug  3 21:34 VoxCeleb2_dev_aac_id00517
drwxr-xr-x 1    99 users     948 Aug  3 21:34 VoxCeleb2_dev_aac_id00906
drwxr-xr-x 1    99 users     948 Aug  3 21:34 VoxCeleb2_dev_aac_id00924
drwxr-xr-x 1    99 users     864 Aug  3 21:34 VoxCeleb2_dev_aac_id01184
drwxr-xr-x 1    99 users     192 Aug  3 21:34 VoxCeleb2_dev_aac_id02074
drwxr-xr-x 1    99 users     570 Aug  3 21:34 VoxCeleb2_dev_aac_id02477
drwxr-xr-x 1    99 users    1074 Aug  3 21:34 VoxCeleb2_dev_aac_id03184
drwxr-xr-x 1    99 users     948 Aug  3 21:34 VoxCeleb2_dev_aac_id03701
drwxr-xr-x 1    99 users    1074 Aug  3 21:34 VoxCeleb2_dev_aac_id04961
drwxr-xr-x 1    99 users     948 Aug  3 21:34 VoxCeleb2_dev_aac_id06261
drwxr-xr-x 1    99 users     318 Aug  3 21:34 VoxCeleb2_dev_aac_id07417
drwxr-xr-x 1    99 users     108 Aug  3 21:34 VoxCeleb2_dev_aac_id07531
```

**For the previous crash,**
I took a guess at the issue, my guess is that in encoder/preprocess.py
the file handle is kept open for too long (1h30min+) and the file handle get's f'ed up.
So I made some mods locally to only open for write the log file during the init, then I open for append for each write / finalizing.


```
class DatasetLog:
    def __init__(self, root, name):
        self.fpath = Path(root, ""Log_%s.txt"" % name.replace(""/"", ""_""))
        self.sample_data = dict()
        start_time = str(datetime.now().strftime(""%A %d %B %Y at %H:%M""))
        with open(self.fpath, ""w"") as f:
            self.write_line(""Creating dataset %s on %s"" % (name, start_time), file_handle=f)
            self.write_line(""-----"", file_handle=f)
            self._log_params(file_handle=f)
        
    def _log_params(self, file_handle):
        from encoder import params_data
        self.write_line(""Parameter values:"", file_handle=file_handle)
        for param_name in (p for p in dir(params_data) if not p.startswith(""__"")):
            value = getattr(params_data, param_name)
            self.write_line(""\t%s: %s"" % (param_name, value), file_handle=file_handle)
        self.write_line(""-----"", file_handle=file_handle)
    
    def write_line(self, line, file_handle=None):
        if file_handle:
            file_handle.write(""%s\n"" % line)
        else:
            with open(self.fpath, ""a"") as f:
                f.write(""%s\n"" % line)
        
    def add_sample(self, **kwargs):
        for param_name, value in kwargs.items():
            if not param_name in self.sample_data:
                self.sample_data[param_name] = []
            self.sample_data[param_name].append(value)
            
    def finalize(self):
        with open(self.fpath, ""a"") as f:
            self.write_line(""Statistics:"", file_handle=f)
            for param_name, values in self.sample_data.items():
                self.write_line(""\t%s:"" % param_name, file_handle=f)
                self.write_line(""\t\tmin %.3f, max %.3f"" % (np.min(values), np.max(values)), file_handle=f)
                self.write_line(""\t\tmean %.3f, median %.3f"" % (np.mean(values), np.median(values)), file_handle=f)
            self.write_line(""-----"", file_handle=f)
            end_time = str(datetime.now().strftime(""%A %d %B %Y at %H:%M""))
            self.write_line(""Finished on %s"" % end_time, file_handle=f)
```



",fixed previous error see bottom comment got another crash image current version image last previous crash took guess issue guess file handle kept open long file handle get made locally open write log file open append write class self root name path root open name self import parameter value value self line line else open line self value value finalize self open statistic median finished,issue,negative,negative,neutral,neutral,negative,negative
668286837,"The issue with matplotlib is a known issue: #455 

Does the audio issue occur when you click the ""replay"" button? I notice if the computer has high CPU usage trying to plot the umap projection it will affect the playback. But I don't get that issue when using ""replay"".",issue known issue audio issue occur click replay button notice computer high usage trying plot projection affect playback get issue replay,issue,negative,positive,positive,positive,positive,positive
668169283,"Nope. 
It might have been a hickup due to using and nfs share. 

I noticed this file rights:
-rw-r--r-- 1 99 users  Log_LibriSpeech_train-other-500.txt

Rights inheritance might have caused some issues since my user would fall under group:users.
Changing the dataset root folder owner recursively instead of relying on group membership should fix the issue.

I'll keep you posted on updates.",nope might due share file inheritance might since user would fall group root folder owner instead group membership fix issue keep posted,issue,negative,negative,negative,negative,negative,negative
668166913,"@adfost Which set of instructions are you following? LibriSpeech (https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issue-663639627) or VCTKp240 (https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-663308789)?

Most likely, when you run `synthesizer_train.py` it cannot find the pretrained model so it starts training a new synthesizer model from scratch. Please make sure you copied the entire contents of `synthesizer/saved_models/logs-pretrained` to another ""logs-XXXX"" folder in the same location, and specify the name (XXXX) to synthesizer_train.py as the first argument.",set following likely run find model training new synthesizer model scratch please make sure copied entire content another folder location specify name first argument,issue,negative,positive,positive,positive,positive,positive
668156809,By any chance did you have the text file (`<datasets_root>/LibriSpeech/_sources.txt`) open in a viewer?,chance text file open viewer,issue,negative,neutral,neutral,neutral,neutral,neutral
668151871,"@shoegazerstella Thank you for reporting the issue, would you please share some audio samples with us that demonstrate what you are talking about?

Just to speculate, the audio preprocessing could be adding noise or other artifacts into the sound files, it is worth doing a before and after comparison. LibriTTS is 24 KHz instead of the 16 KHz in LibriSpeech (used to train the original models), and since it's not an integer multiple this means our training data also needs to be interpolated as it is resampled. The librosa resampling process can be found in: [librosa/core/audio.py](https://github.com/librosa/librosa/blob/4b827ce9b2ff5221792dc069b601bef18d9eacb4/librosa/core/audio.py#L460-L592) (the actual resampling is done by scipy or resampy)

However I think that is unlikely. Could also be due to fewer training steps (200k vs 278k). Also LibriSpeech utterances are longer on average than LibriTTS so for a given number of steps I would expect a more refined model from LibriSpeech.

From https://arxiv.org/pdf/1904.02882v1.pdf
<img width=""386"" alt=""libritts_figure1"" src=""https://user-images.githubusercontent.com/67130644/89210812-a25b3e00-d575-11ea-9542-826193ebb9ae.png"">
",thank issue would please share audio u demonstrate talking speculate audio could noise sound worth comparison instead used train original since integer multiple training data also need process found actual done however think unlikely could also due training also longer average given number would expect refined model,issue,positive,positive,neutral,neutral,positive,positive
668145006,"update
I got a crash. I have to figure out what happens. 

The dataset resides on a nfs share on my unraid host. many TB avail, so it is not a lack of space for the dataset.
I will force change chown -R user and chmod -R 766 on the whole dataset and try again.

![image](https://user-images.githubusercontent.com/32403586/89206107-c3785a80-d586-11ea-957e-546dd8405f6a.png)
",update got crash figure share host many avail lack space force change user whole try image,issue,negative,positive,positive,positive,positive,positive
668113701,"@blue-fish I did exactly what you said, after over 10000 steps with the synthesizer, I try to open the toolbox. I type the text to convert into the box, and I get some unrelated text in an almost incomprehensible ramble. 

",exactly said synthesizer try open toolbox type text convert box get unrelated text almost incomprehensible ramble,issue,negative,positive,positive,positive,positive,positive
668102358,"It's optional, but starting a visdom server allows you to visualize the training results by navigating to `http://localhost:8097`

The umap projections will let us know whether the encoder has learned to distinguish between the voices in the training set. This in turn helps us decide when to stop training.",optional starting server visualize training let u know whether learned distinguish training set turn u decide stop training,issue,negative,neutral,neutral,neutral,neutral,neutral
668078949,"Hi @mbdash and thank you for sharing this new synth model!
I have tried it and it seems the voice is identical wrt the one generated by the old synth model. To me also the other was fairly similar to the input voice. However, I find it to be noisier compared to the old one. Do you think you can achieve better performances by also training the encoder? Is the noise due to some imperfections in the embedding computation phase?",hi thank new model tried voice identical one old model also fairly similar input voice however find old one think achieve better also training noise due computation phase,issue,positive,positive,positive,positive,positive,positive
667758549,update: still preparing the data. I might start the training tomorrow.,update still data might start training tomorrow,issue,negative,neutral,neutral,neutral,neutral,neutral
667236835,"@3dfernando I sincerely thank you for taking the time to elaborate on your setup experience and for sharing your thoughts. We see a lot of frustration vented in the comments section of Corentin's youtube video so I do think that this is a representative toolbox experience (unfortunately). Porting the code to Matlab would be good for those who have it, but I don't think anyone here has the right experience and time to make that happen.

If I may take the Matlab suggestion off the table, is there anything else we could or should do to improve the experience for those who want to try the toolbox? Trying to be constructive here. Maybe a compiled version would help (#267) or a preconfigured Linux virtual machine image.",sincerely thank taking time elaborate setup experience see lot frustration section video think representative toolbox experience unfortunately code would good think anyone right experience time make happen may take suggestion table anything else could improve experience want try toolbox trying constructive maybe version would help virtual machine image,issue,positive,positive,positive,positive,positive,positive
667227905,"You know you are not paying for this right?
This is open source code maintained by volunteers.
Also, use Ubuntu, because just like you said about windows 95, windows is windows.

BUT, I get what you mean, I tried to install this repo on windows and I gave up.
And I also get the frustration, I sometimes can write stuff not very diplomatic when pissed.

So here is the Unbuntu 20 install instruction, given you have installed your NVidia drivers if using GPU.

(you will need miniconda3 installed)

**Unzip then Install the conda env I am providing**
[rtvc_py373.zip](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/5008100/rtvc_py373.zip)

`conda env create -f rtvc_py373.yml`

**Activate the virtual env prior to using the toolbox or cli**
`conda activate rtvc_py373`


",know paying right open source code also use like said get mean tried install gave also get frustration sometimes write stuff diplomatic install instruction given need install providing create activate virtual prior toolbox activate,issue,negative,negative,neutral,neutral,negative,negative
667214170,"Blue-fish, since you hinted at me not reading the documentation (which is rather outrageous at the state I'm at right now), I'll share my user experience as an occasional python user:

I get to know about the paper. Then I think - you know what; I want to give this a try. Then you download this project and start reading the documentation. It tells you to install a bunch of shit that the author used to make it and that I really don't care. But okay. You install them and start working at the console. You're supposed to, as the readme says, run `python demo_cli.py` and ""if all tests pass, you're good to go"". Well, you'll not be ""good to go"" any time soon.

3 hours later, and all I get is a bunch of package messages of compatibility issues, one after another, after being willing to put my time into solving each one. When you install requirements.txt, it keeps on failing. You then have to delete entries from the requirements.txt so you can move on. But of course, those you have to find a way to install manually somehow. After hours of researching, you find you have the wrong python version and you have to start over. You know what? This is bullshit. I quit.

This reminds me of Windows 95 when nothing worked and you had to spend serious amounts of time making it work by moving the right file to the right folder. It is frustrating because the barrier of entry is too high for a casual user, and this code is not even old!!

This is why I said, ""why don't you use a solid foundation like Matlab"". I have code from 2008 that still works. It is a paid platform, but at least they do a good job of cleaning up after themselves. Open source platforms generally don't care about backwards compatibility because no one is getting paid to make it work. And don't tell me ""you're getting this for free"". This is academic work and grant money from the public is used to fund this project. This project is not free. It is, indeed, supposed to be widely accessible - which is clearly not the case.

This is why I think your effort to port the code to python 3.8 is useless. 6 months from now it won't work anymore because something else changed. The foolish builder builds his house on a sand foundation!",since reading documentation rather outrageous state right share user experience occasional python user get know paper think know want give try project start reading documentation install bunch author used make really care install start working console supposed run python pas good go well good go time soon later get bunch package compatibility one another willing put time one install failing delete move course find way install manually somehow find wrong python version start know quit nothing worked spend serious time making work moving right file right folder barrier entry high casual user code even old said use solid foundation like code still work platform least good job cleaning open source generally care backwards compatibility one getting make work tell getting free academic work grant money public used fund project project free indeed supposed widely accessible clearly case think effort port code python useless wo work something else foolish builder house sand foundation,issue,positive,positive,neutral,neutral,positive,positive
667197410,"@mbdash I will be unavailable this weekend. Hopefully the commands will just work. You can reach out to the community for help, in particular @sberryman who has gone through this in #126 .

There are two things that I hope to learn from training this new encoder model.
1. Does the voice cloning improve with the new model? (i.e. does increasing hidden layers from 256 to 768 make a difference when it is still projected down to 256 at the end)
2. Will the new encoder model be compatible with the existing synthesizer?

My hypothesis is that for 1, we will not see a difference unless we also retrain the synth with many more voices. And for 2, that it should be compatible since the dimensions, input data and loss function are not changing. I may very well be wrong on that since I have not studied the encoder in detail.",unavailable weekend hopefully work reach community help particular gone two hope learn training new model voice improve new model increasing hidden make difference still end new model compatible synthesizer hypothesis see difference unless also retrain many compatible since input data loss function may well wrong since studied detail,issue,positive,positive,neutral,neutral,positive,positive
667188328,"Before submitting the pull request please test it and make sure you can run demo_cli.py or demo_toolbox.py without errors, when using the latest librosa and numba.

Also make sure that `pip install -r requirements.txt` works. For this one it is preferred to start from scratch in a new virtualenv, but for testing purposes you can also uninstall librosa and numba from a working env to test the command.

The hard part is not the requirements.txt change, but taking the time to test it.",pull request please test make sure run without latest also make sure pip install work one preferred start scratch new testing also working test command hard part change taking time test,issue,positive,positive,positive,positive,positive,positive
667186057,"Thank you for sharing your experience with the toolbox setup. It is clear we still have much improvement to make in this regard.

Although the [setup instructions](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/README.md#setup) clearly state Python 3.6 or 3.7 is required, not everyone reads the documentation and we should add a version check in demo_cli.py and demo_toolbox.py that prints a helpful error message. Same for the tensorflow version since many people try to run it with TF 2.x.

This is not hard. Here's the basic idea:
```
import sys
import tensorflow as tf

if sys.version_info[1] >= 8:
    print(""Python 3.8 and higher are not supported at this time. Please use 3.6 or 3.7"")
    exit(-1)

if tf.VERSION not in [""1.15.0"", ""1.15.1"", ""1.15.2""]:
    print(""Tensorflow 1.15 is required to use the toolbox. Your version is:"", tf.VERSION)
    exit(-1)
```

I would appreciate it if someone in the community would submit a pull request for this. This is what I would suggest:
* Move `utils/model_utils.py` to `utils/config_utils.py`
    * Fix the imports in demo_cli.py and demo_toolbox.py
* Add a new function `check_dependencies()` in `utils/config_utils.py` with the above code (clean up as needed)
* Use the function just before `check_model_paths()` in demo_cli.py and demo_toolbox.py

I need to reserve my focus for #447 (Python 3.8 compatibility, removing Tensorflow) so I am not going to work on the little stuff until that is done. From my work on this repo in the past month, you know that a lot of small code improvements add up over time so I hope someone reading this is inspired to become a contributor and perhaps also take on some of the ""good first issue"" items. :)",thank experience toolbox setup clear still much improvement make regard although setup clearly state python everyone documentation add version check helpful error message version since many people try run hard basic idea import import print python higher time please use exit print use toolbox version exit would appreciate someone community would submit pull request would suggest move fix add new function code clean use function need reserve focus python compatibility removing going work little stuff done work past month know lot small code add time hope someone reading inspired become contributor perhaps also take good first issue,issue,positive,positive,positive,positive,positive,positive
667078874,"Use Ubuntu 20 with a conda env.
I can share my conda env if you want. 
(I already shared it in another thread)",use share want already another thread,issue,negative,neutral,neutral,neutral,neutral,neutral
666916584,"wanted to add a workflow aside: synthesizing (at least on my rig) is cheaper.  so I tweak the line break and ""synthesize only"" a few times -- even a few times without changing the line breaks -- until the gaps on the spectrogram look better.  only then do I vocode. ",add aside least rig tweak line break synthesize time even time without line spectrogram look better,issue,positive,positive,neutral,neutral,positive,positive
666851411,"@blue-fish I did train on CPU(autocorrect!!) (I always have issues with gpu setup. Luckily im building a new pc when the 30xx cards drop with the new zen2 amd cpus). After trying to train from 200-400 it would seem that it takes ~25s per step after 20 steps, so around 2 hours for 200 steps on i5 4690k.

The next steps for me would be encoder/vocoder training but I don't want to invest the compute power since Im working on another NLP problem for my actual research (sentiment analysis) I'll let it run overnight again and this time see how far it gets :)

edit: as @blue-fish said, it seems training it to 400 steps made a large difference. Here's an example of the same voice as above, but with 400 steps of training the p261 set on my own collected voice samples:

original voice: https://raw.githubusercontent.com/Ori-Pixel/files/master/Vo_dark_willow_sylph_attack_14.mp3
200 steps: https://raw.githubusercontent.com/Ori-Pixel/files/master/biggest_oversight.flac
400 steps: https://raw.githubusercontent.com/Ori-Pixel/files/master/dark%20willow%20400.flac",train always setup luckily building new drop new trying train would seem per step around next would training want invest compute power since working another problem actual research sentiment analysis let run overnight time see far edit said training made large difference example voice training set collected voice original voice,issue,negative,positive,positive,positive,positive,positive
666809271,"@Ori-Pixel Nice! It's remarkable how much that voice comes through after 200 steps of finetuning. In my own experiments going up to 400 steps yields a noticeable improvement in the voice quality. More than 400 doesn't seem to help, though it doesn't hurt either.

Edit: You trained on CPU right? How long did it take?",nice remarkable much voice come going noticeable improvement voice quality seem help though hurt either edit trained right long take,issue,positive,positive,positive,positive,positive,positive
666777922,"@blue-fish Okay, so I got it to train, and I can also train my own dataset for the synthesizer. Really thankful for the help. Here's a result from 200 steps of training if you're interested:

https://raw.githubusercontent.com/Ori-Pixel/files/master/crooked_creek_dw.flac

https://raw.githubusercontent.com/Ori-Pixel/files/master/biggest_oversight.flac

",got train also train synthesizer really thankful help result training interested,issue,positive,positive,positive,positive,positive,positive
666641548,"@Ori-Pixel You also need to add the `--no_alignments` option to use a non-LibriSpeech dataset that doesn't have an alignments file. I've also fixed the command in the instructions above. Sorry for leaving that out earlier.

```
python synthesizer_preprocess_audio.py datasets_root --datasets_name LibriTTS --subfolders train-clean-100 --no_alignments
```

Edit: If preprocessing completes without finding a wav file, we should remind the user to pass the `--no_alignments` flag. Or possibly default it to True if the datasets_name is not LibriSpeech.",also need add option use file also fixed command sorry leaving python edit without finding file remind user pas flag possibly default true,issue,negative,negative,neutral,neutral,negative,negative
666581671,"> Check that you are using the new synthesizer model

Ah, I didn't have that drop down selected. My results are then this, with the same settings:

https://raw.githubusercontent.com/Ori-Pixel/files/master/take%20a%20look%20at%20these%20pages%20for%20crooked%20creek%20drive%20fine%20tuned.flac

I'm also taking your comment above and trying to train my own dataset, but at first I got a dataset roots folder doesnt exist error, so I made the folder and added my files, but when I go to train, I get:

    Arguments:
    datasets_root:   datasets_root
    out_dir:         datasets_root\SV2TTS\synthesizer
    n_processes:     None
    skip_existing:   False
    hparams:
    no_alignments:   False
    datasets_name:   LibriTTS
    subfolders:      train-clean-100
    Using data from:
    datasets_root\LibriTTS\train-clean-100
    LibriTTS:   0%|                                                                            | 0/1 [00:00<?, ?speakers/s]2

*gpu warnings here*

```
LibriTTS: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.20s/speakers]
The dataset consists of 0 utterances, 0 mel frames, 0 audio timesteps (0.00 hours).
Traceback (most recent call last):
  File ""synthesizer_preprocess_audio.py"", line 59, in <module>
    preprocess_dataset(**vars(args))
  File ""H:\ttss\Real-Time-Voice-Cloning-master\synthesizer\preprocess.py"", line 49, in preprocess_dataset
    print(""Max input length (text chars): %d"" % max(len(m[5]) for m in metadata))
ValueError: max() arg is an empty sequence
```
![image](https://user-images.githubusercontent.com/54956527/88959495-b5b19500-d267-11ea-84d1-8a541b015400.png)

Utterances have just the text that was spoken in them, so utterance-000.txt contains `Let's have some fun, shall we...`

edit: I assume I will need to go through the training docs and start by training the encoder?",check new synthesizer model ah drop selected also taking comment trying train first got folder doesnt exist error made folder added go train get none false false data mel audio recent call last file line module file line print input length text empty sequence image text spoken let fun shall edit assume need go training start training,issue,negative,negative,neutral,neutral,negative,negative
666565535,"@Ori-Pixel 

Here is the dataset in the same format as p240 (embeds overwritten with the one corresponding to p261_001.flac): **https://www.dropbox.com/s/o6fz2r6w56djwkf/dataset_p261.zip?dl=0**

* The source p261 dataset so you can listen to the audios: https://www.dropbox.com/s/ynf823o5619j2q5/p261.zip?dl=0
* The processed audio for vocoder training (put this in `SV2TTS/synthesizer/audio`): https://www.dropbox.com/s/q3bihpem7os54yi/p261_audio.zip?dl=0
* The original embeds for the full set (you should not have any use for these except to perform synthesizer training experiments): https://www.dropbox.com/s/y012fvf0zyk50xg/p261_embeds.zip?dl=0

> resulting audio: https://raw.githubusercontent.com/Ori-Pixel/files/master/welcome_to_toolbox_fine_tuned.flac

Your results sound American to me. Check that you are using the new synthesizer model, then try this text: `Take a look at these pages for crooked creek drive.`
And compare to my results for 200 steps: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-663050677",format one corresponding source listen audio training put original full set use except perform synthesizer training resulting audio sound check new synthesizer model try text take look crooked creek compare,issue,negative,positive,positive,positive,positive,positive
666517695,"@blue-fish p261 is relatively close. if I could get that slice, that would be very helpful (my internet at my current house is sadly 1MB/s)

I trained as per instructions above, sadly I didnt get to see the console output as my power went out after about an hour or so, but I did get this in the training logs, so I think this is as far as it trained.

`[2020-07-30 01:36:31.676]  Step  278202 [28.894 sec/step, loss=0.64379, avg_loss=0.64339]`

Also, just to make sure I did the test, this is the cmd I used:

`python demo_toolbox.py -d H:\ttss\Real-Time-Voice-Cloning-master\dataset_p240`

Where random seed = 1, enhanced vocoder output is checked, embedding was from p240_1.flac.

resulting audio: https://raw.githubusercontent.com/Ori-Pixel/files/master/welcome_to_toolbox_fine_tuned.flac",relatively close could get slice would helpful current house sadly trained per sadly didnt get see console output power went hour get training think far trained step also make sure test used python random seed enhanced output checked resulting audio,issue,negative,negative,negative,negative,negative,negative
666446751,"Visdom is not hard to install but if you are installing packages manually it can be skipped to save time. It's your call as to whether to remove that line from requirements.txt.

The only toolbox program that uses it is`encoder_train.py` and 99% of users will never touch it.",hard install manually save time call whether remove line toolbox program never touch,issue,negative,negative,negative,negative,negative,negative
666428089,"Using `pip` to install each package succeeded, but the resultant setup threw the same error as before, with the `%1`. Pursuing the Ubuntu angle now.

Also, I'm curious what you mean by saying that I don't ""need"" `visdom`. It is contained in `requirements.txt` and will throw an error if not acquired properly, so I assume you mean that the setup would mostly function if I removed that line from the text file?",pip install package resultant setup threw error angle also curious mean saying need throw error acquired properly assume mean setup would mostly function removed line text file,issue,negative,negative,neutral,neutral,negative,negative
666390190,"1. See #53. It gets a little better with a LibriTTS-trained model: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/449#issuecomment-665645153 .
    * As a workaround, the ""enhance vocoder output"" option in the toolbox will also use voice activation detection to trim out these gaps if you have the `webrtcvad` package installed.
2. Have not seen this before, can you try to find an input sequence + random seed that does this? And provide the source audio file for the embed.
3. Issue was also reported in #347 (don't have an explanation yet)

Also see #411 for discussion about some things that should be changed or improved.",see little better model enhance output option toolbox also use voice activation detection trim package seen try find input sequence random seed provide source audio file embed issue also explanation yet also see discussion,issue,negative,negative,neutral,neutral,negative,negative
666297871,"I am taking a break from this. It's one thing to get it to work, but refactoring the existing code is a tedious chore, not to mention keeping the code quality on par with the rest of the repo.

For anyone who wants to attempt this, I suggest using the vocoder as a template since that's already fatchord-style.",taking break one thing get work code tedious chore mention keeping code quality par rest anyone attempt suggest template since already,issue,negative,negative,negative,negative,negative,negative
666283581,"Also see https://github.com/NVIDIA/mellotron ([samples](https://nv-adlr.github.io/Mellotron)), it can make ""singing text"".

It works like this: first, you train mellotron on someone else's voice to develop a text-to-speech model. When you synthesize new speech from that model, condition the output using your singing voice to make it sing.",also see make singing text work like first train someone else voice develop model synthesize new speech model condition output singing voice make sing,issue,negative,positive,positive,positive,positive,positive
666236118,"Hey! Havent followed this. Yeah the results were not that good but I managed to get it to say a few words that were difficult to distinguish. Tried it on > 100 people and only one could guess all the cloned voices. If I did it again I would test something else, like mellotron or something that followed / transformer based. In general I think you need a lot more data and cleaner than what I was using. It mixes dialects to the left and right so another voice embedding modell trained on Swedish would probably be necessary.",hey havent yeah good get say difficult distinguish tried people one could guess would test something else like something transformer based general think need lot data cleaner left right another voice trained would probably necessary,issue,negative,positive,neutral,neutral,positive,positive
666159447,"Ah I see. I'll give it a look tomorrow along with the results and let you know then, thanks again for being so active!",ah see give look tomorrow along let know thanks active,issue,positive,positive,neutral,neutral,positive,positive
666157814,">Is there a list of their speakers somewhere? 

The zip file I uploaded includes `speaker-log.txt` (which is included in the full VCTK dataset) which has a list of speaker metadata such as:
```
ID  AGE  GENDER  ACCENTS  REGION COMMENTS 
p225  23  F    English    Southern  England
p226  22  M    English    Surrey
p227  38  M    English    Cumbria
p228  22  F    English    Southern  England
```",list somewhere zip file included full list speaker id age gender region southern surrey southern,issue,negative,positive,positive,positive,positive,positive
666153932,"@blue-fish 
> I have preprocessed VCTK, if you can make your decision based on a single recording, request up to 3 speakers and I'll put them on dropbox for you. https://www.dropbox.com/s/6ve00tjjaab4aqj/VCTK_samples.zip?dl=0

Is there a list of their speakers somewhere? I only was able to find the 10GB file with not even a magnet link or anything denoting samples or file structure. I mean realistically anything Irish, Scottish, or Gaelic would work. I may also look into downloading it direct to drive (if possible) and even possibly training there (if possible -- as far as I'm aware you can mount the drive and run bash.)

> Oh, and just to be clear, you cannot train the voice and accent independently at this time. The accent is associated with the voice via the speaker embedding. After #447, we will work on #230 to add the Mozilla TTS implementation of GSTs. That should allow us to generalize accents to new voices.

Yeah, I just meant using a vctk pretrained that wasn't horribly inconsistent with my single speaker's accent and then fine tuning with my custom labeled lines on top. 

I also have a couple idle GPUs in my machine but I always run into venv issues with gpu training so I'll just use colab if I really need a GPU. Too bar downloading from a link to ",make decision based single recording request put list somewhere able find file even magnet link anything file structure mean realistically anything would work may also look direct drive possible even possibly training possible far aware mount drive run bash oh clear train voice accent independently time accent associated voice via speaker work add implementation allow u generalize new yeah meant horribly inconsistent single speaker accent fine tuning custom top also couple idle machine always run training use really need bar link,issue,positive,positive,neutral,neutral,positive,positive
666133824,"Oh, and just to be clear, you cannot train the voice and accent independently at this time. The accent is associated with the voice via the speaker embedding. After #447, we will work on #230 to add the Mozilla TTS implementation of GSTs. That should allow us to generalize accents to new voices.",oh clear train voice accent independently time accent associated voice via speaker work add implementation allow u generalize new,issue,positive,positive,neutral,neutral,positive,positive
666129979,"@Ori-Pixel If you have a GPU you can quickly run a few experiments to see how far you can trim the dataset before the audio quality breaks down. Simply delete lines from train.txt and they won't be used.

One of my experiments involved re-recording some of the VCTK p240 utterances with a different voice. 5 minutes of mediocre data (80 utterances) still resulted in a half-decent model. If the labeling is extremely tedious you can try training a model on part of it while continuing to label.

I have preprocessed VCTK, if you can make your decision based on a single recording, request up to 3 speakers and I'll put them on dropbox for you. https://www.dropbox.com/s/6ve00tjjaab4aqj/VCTK_samples.zip?dl=0",quickly run see far trim audio quality simply delete wo used one involved different voice mediocre data still model extremely tedious try training model part label make decision based single recording request put,issue,negative,negative,neutral,neutral,negative,negative
666114460,"@blue-fish thanks. yeah, I can see that it's saving to a new directory, I'll run it again with the correct params and post results.

Also, thanks for the preprocessing tips you gave to @Adam-Mortimer . I was not looking forward to custom labeling, but it doesn't seem that bad if I only have ~200 lines/~34 minutes. I'm trying to make a fake (semi-Gaelic) accent video game character say some lines, so I'll probably scrape the audio files from the wiki site, then slap them into a folder structure like above with a simple script and then run this single speaker fine tuning again. And for the accent, I think I can just find a semi-close one in the VCTK dataset (although a 10Gb download will take me a few days sadly).",thanks yeah see saving new directory run correct post also thanks gave looking forward custom seem bad trying make fake accent video game character say probably scrape audio site slap folder structure like simple script run single speaker fine tuning accent think find one although take day sadly,issue,negative,negative,negative,negative,negative,negative
666099616,"@Ori-Pixel There was a problem with my command and I fixed it. If you are following everything to the letter it should be:
```
python synthesizer_train.py vctkp240 dataset_p240/SV2TTS/synthesizer --checkpoint_interval 100
```

Where the first arg `vctkp240` describes the path to the model you are training (in this case, it tells python to look for the model in `synthesizer/saved_models/logs-vctkp240`), and the second arg is the path to the location containing train.txt, and the mels and embeds folders. Please share your results and feel free to ask for help if you get stuck.",problem command fixed following everything letter python first path model training case python look model second path location please share feel free ask help get stuck,issue,positive,positive,positive,positive,positive,positive
666099538,"> I'm still stymied by the inability to create custom datasets from scratch. Are you still working on this ""custom dataset"" tool that you mention here?

Hi @Adam-Mortimer. The custom dataset tool is still planned, but currently on hold as I've just started working #447 (switching out the synthesizer for fatchord's tacotron). #447 will be bigger than all of my existing pull requests combined if it ever gets finished. In other words, it's going to take quite some time.

I started writing the custom dataset tool for a voice cloning experiment. I didn't get very far with the tool before I added LibriTTS support in #441 which made it much easier to create a dataset by putting your data in this kind of directory structure:
```
datasets_root
    * LibriTTS
        * train-clean-100
            * speaker-001
                * book-001
                    * utterance-001.wav
                    * utterance-001.txt
                    * utterance-002.wav
                    * utterance-002.txt
                    * utterance-003.wav
                    * utterance-003.txt
```
Where each `utterance-###.wav` is a short utterance (2-10 sec) and the `utterance-###.txt` contains the corresponding transcript. Then you can process this dataset using:
```
python synthesizer_preprocess_audio.py datasets_root --datasets_name LibriTTS --subfolders train-clean-100 --no_alignments
```
When this completes, your dataset is in the SV2TTS format and subsequent preprocessing commands (`synthesizer_preprocess_embeds.py`, `vocoder_preprocess.py`) will work as described on the [training wiki page](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training).

I would still like to write the custom dataset tool but I think #447 is a more pressing matter since the toolbox is incompatible with Python 3.8 due to our reliance on Tensorflow 1.x.",still inability create custom scratch still working custom tool mention hi custom tool still currently hold working switching synthesizer bigger pull combined ever finished going take quite time writing custom tool voice experiment get far tool added support made much easier create data kind directory structure short utterance sec corresponding transcript process python format subsequent work training page would still like write custom tool think pressing matter since toolbox incompatible python due reliance,issue,positive,positive,neutral,neutral,positive,positive
666092206,"@blue-fish any reason why im getting the following error: ""synthesizer_train.py: error: the following arguments are required: synthesizer_root""? I'm trying to run:

synthesizer_train.py H:\ttss\Real-Time-Voice-Cloning-master\dataset_p240\SV2TTS\synthesizer --checkpoint_interval 100

the second argument is the folder that contains embeds, mels, and train.txt

nevermind I fixed it while writing this. The argument isn't --synthesizer_root as all of the other arguments, but actually just synthesizer_root. Also, the above testing instructions are thus wrong (or at least not working for me). The command should be:

`python synthesizer_train.py synthesizer_root dataset_p240/SV2TTS/synthesizer --checkpoint_interval 100`

(it at least bumped me to a dll error - still working through that one)


",reason getting following error error following trying run second argument folder fixed writing argument actually also testing thus wrong least working command python least error still working one,issue,negative,negative,negative,negative,negative,negative
665942373,"> ""I am going to write a tool that will allow users to manually select or record files to add to a custom dataset, and facilitate transcription (maybe using [DeepSpeech](https://github.com/mozilla/DeepSpeech)). This tool will be hosted in a separate repository.""

Thank you for all your hard work on this repo - even as an almost complete newcomer to deep learning, I've been able to decipher some things, but I'm still stymied by the inability to create custom datasets from scratch. Are you still working on this ""custom dataset"" tool that you mention here? ",going write tool allow manually select record add custom facilitate transcription maybe tool separate repository thank hard work even almost complete newcomer deep learning able decipher still inability create custom scratch still working custom tool mention,issue,negative,positive,neutral,neutral,positive,positive
665864235,"You don't need `visdom` to use the toolbox. It is only ever used for encoder training which few users will ever perform.

The toolbox is a breeze to set up on a Linux system (with the exception of GPU support which is optional). You might find it easier to run it on a virtual machine with Ubuntu 18.04 if you have that option.",need use toolbox ever used training ever perform toolbox breeze set system exception support optional might find easier run virtual machine option,issue,positive,neutral,neutral,neutral,neutral,neutral
665826890,"@blue-fish Unfortunately, the full set of required packages are not available on the standard conda channels. By adding the channels `conda-forge` and `dsdale24` the missing packages can be found, but they do not all have the same name (having ""python"" or ""py"" prepended).

Missing packages:
`visdom`
`soundfile`
`umap-learn`
`multiprocess`
`librosa[version='>=0.5.1']`
`pyqt5`
`sounddevice`

Modifying the `requirements.txt` file to contain the updated names appears to break the whole thing by introducing package conflicts, which was expected, but I figured was worth a shot.

I'm also investigating the possibility that forcing conda to only download 32-bit packages (but still using pip as originally) could resolve the issue, but no progress on that yet.",unfortunately full set available standard missing found name python missing file contain break whole thing package figured worth shot also investigating possibility forcing still pip originally could resolve issue progress yet,issue,negative,positive,positive,positive,positive,positive
665780980,"Corentin's youtube demo has some of the best results I've seen: https://www.youtube.com/watch?v=-O_hYhToKoA 

I have considered trying to replicate these results, but haven't gotten to it yet.",best seen considered trying replicate gotten yet,issue,positive,positive,positive,positive,positive,positive
665774967,If anyone else is silently following along I would appreciate any comments on the LibriTTS_200k model (https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/449#issuecomment-665645153) so we can use that feedback to make the next one better.,anyone else silently following along would appreciate model use feedback make next one better,issue,positive,positive,positive,positive,positive,positive
665758832,"Great to hear,
I am still downloading the voxceleb files. 
Once done, i will train the encoder and we can try again training the synth from scratch.",great hear still done train try training scratch,issue,positive,positive,positive,positive,positive,positive
665758697,"Thank you for suggestions.

Reading all issues now I better understand that it will be hard to achieve good quality with the current pre-trained model.

Do you have an example of the best samples achieved with the current repo on pre-trained only model?
",thank reading better understand hard achieve good quality current model example best current model,issue,positive,positive,positive,positive,positive,positive
665747119,@ash1407 If you're not working on this actively then I'll close the issue for now. Reopen it when you're ready to give it a try.,ash working actively close issue reopen ready give try,issue,positive,positive,neutral,neutral,positive,positive
665745032,"Presumed to be resolved. Reopen this issue if you are still experiencing this specific problem, or open a new issue if you have a different problem.",resolved reopen issue still specific problem open new issue different problem,issue,negative,positive,neutral,neutral,positive,positive
665743627,"Thanks @mbdash ! I got it to work but needed to put it in a folder structure like this:
```
logs-LibriTTS_200k
    * taco_pretrained
        * checkpoint
        * tacotron_model.ckpt-200000.data-00000-of-00001
        * tacotron_model.ckpt-200000.index
        * tacotron_model.ckpt-200000.meta
```

The `checkpoint` is not included but it is easy enough to make it. It is a text file with a single line:
```
model_checkpoint_path: ""tacotron_model.ckpt-200000""
```

So far I am finding cloned voices sound nearly identical to Corentin's LibriSpeech_278k model, with better performance for very short text inputs (1-5 words). It is still liable to have gaps, but they are not multiple seconds like we have with LibriSpeech_278k. The synthesizer can fail spectacularly, but this is a rare exception and not the norm. Some punctuation has an effect (periods and commas), but I don't notice anything with question marks. I think question marks would be better handled using a global style token like we are discussing in #230.

Overall an improvement over the existing model, though a slight one. This is all we could expect.",thanks got work put folder structure like included easy enough make text file single line far finding sound nearly identical model better performance short text still liable multiple like synthesizer fail spectacularly rare exception norm punctuation effect notice anything question think question would better handled global style token like overall improvement model though slight one could expect,issue,positive,positive,neutral,neutral,positive,positive
665645153,"Synth Trained on LibriTTS 200k steps with old /original encoder.

https://drive.google.com/drive/folders/1ah6QNyB8jIcFuKusPOVdx0pPIZxeZeul?usp=sharing

Let me know if the link works. or not and if any files are missing.",trained old let know link work missing,issue,negative,negative,neutral,neutral,negative,negative
665461834,"@KuangDD Would you please upload the pretrained models for zhrtvc to another location? I would like to try it out but I am unable to download them from your Baidu links. It would be much appreciated if you can take the time to do this!

If anyone else has the zhrtvc pretrained models and can share them with me, that would work too.",would please another location would like try unable link would much take time anyone else share would work,issue,positive,negative,negative,negative,negative,negative
665441585,"Here are some [**samples.zip**](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/4992767/samples.zip), where I take VCTK p240 and p260 and clone an English utterance with Corentin's models, and a Swedish utterance with these models. In my opinion the Swedish performs better for this example, though they both leave something to be desired.

* Text: ""Lite är känt om dess föda eller häckning och lätet som tillskrivs zapatarallen kan tillhöra en annan art."" ([source](https://sv.wikipedia.org/wiki/Zapatarall))
* English translation: ""Little is known about its food or nesting and the sound attributed to the zapatara may belong to another species.""",take clone utterance utterance opinion better example though leave something desired text lite om des och kan en art source translation little known food sound may belong another specie,issue,positive,positive,positive,positive,positive,positive
665428894,"This is resolved @ViktorAlm ! Just needed to add the extra characters in the Swedish alphabet to `synthesizer/utils/symbols.py`. If anyone wants to use these models it might be helpful to use the [`400_pretrained_swe_301`](https://github.com/blue-fish/Real-Time-Voice-Cloning/tree/400_pretrained_swe_301) branch of my fork. I will accept pull requests if anyone wants to contribute improvements.

~~I am finding that all cloned voices sound male.~~ The sound quality can be good for some speakers and prosody overall feels natural though fast. It might make a decent starting point for a Swedish TTS.

https://github.com/blue-fish/Real-Time-Voice-Cloning/compare/054f16e...blue-fish:400_pretrained_swe_301

Edit: Fixed the sound issue, I had a bad encoder model in my folder. Once fixed it worked much better, see samples in following post.

Thank you for sharing your models @ViktorAlm , I do think that others will find it useful for text to speech.",resolved add extra alphabet anyone use might helpful use branch fork accept pull anyone contribute finding sound sound quality good prosody overall natural though fast might make decent starting point edit fixed sound issue bad model folder fixed worked much better see following post thank think find useful text speech,issue,positive,positive,positive,positive,positive,positive
665360284,Checking in to see if you've been able to make any progress on the exe.,see able make progress,issue,negative,positive,positive,positive,positive,positive
665345076,@mbdash Are you able to share the 200k [checkpoint files](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/449#issuecomment-664711953) or vocoded samples at the very least? I'd like to see how well the 200k model performs!,able share least like see well model,issue,negative,positive,neutral,neutral,positive,positive
665343576,"The training on the synth   reached 200k, I stopped it to give a break to the server.
![image](https://user-images.githubusercontent.com/32403586/88735816-7a4b8500-d106-11ea-8602-17312f2a5a9e.png)

I am still downloading the datasets for the encoder, i will get started on it tomorrow.
",training stopped give break server image still get tomorrow,issue,negative,neutral,neutral,neutral,neutral,neutral
665283280,"It may be caused by mixing `conda install` and `pip install` , you should do one or the other.

If you want to use conda, try the following after cleaning up your environment:
```
conda install --file requirements.txt
```",may install pip install one want use try following cleaning environment install file,issue,negative,neutral,neutral,neutral,neutral,neutral
665277780,"I hope that you don't mind that I try to build the FAQ while solving your problem (which also occurred in #163). Provide any info that is useful to the resolution and I will add it.

https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/431#issuecomment-665277071

> ### OSError: [WinError 193] %1 is not a valid Win32 application
> #### Summary
> 
> This error message occurs when you have multiple python environments and they are conflicting with each other.
> 
> For example, this is the traceback for #163:
> 
> ```
> Traceback (most recent call last):
>   File ""D:\Real-Time-Voice-Cloning-master\demo_cli.py"", line 2, in <module>
>     from utils.argutils import print_args
>   File ""D:\Real-Time-Voice-Cloning-master\utils\argutils.py"", line 2, in <module>
>     import numpy as np
>   File ""C:\Users\username\AppData\Roaming\Python\Python37\site-packages\numpy\__init__.py"", line 140, in <module>
>     from . import _distributor_init
>   File ""C:\Users\username\AppData\Roaming\Python\Python37\site-packages\numpy\_distributor_init.py"", line 26, in <module>
>     WinDLL(os.path.abspath(filename))
>   File ""C:\Users\username\AppData\Local\Programs\Python\Python37\lib\ctypes\__init__.py"", line 364, in __init__
>     self._handle = _dlopen(self._name, mode)
> OSError: [WinError 193] %1 is not a valid Win32 application
> >>> 
> ```
> 
> From the traceback you see these python paths causing a conflict with each other:
> `C:\Users\username\AppData\Roaming\Python\Python37\`
> `C:\Users\username\AppData\Local\Programs\Python\Python37\`
> #### Solution
> 
> Look at your traceback and identify the conflicting python environments. Then check your `PATH` and remove one of the environments. Of course the environment you keep still needs to have all of the requirements. A similar issue is reported here: [pytorch/pytorch#27693](https://github.com/pytorch/pytorch/issues/27693)",hope mind try build problem also provide useful resolution add valid win application summary error message multiple python conflicting example recent call last file line module import file line module import file line module import file line module file line mode valid win application see python causing conflict solution look identify conflicting python check path remove one course environment keep still need similar issue,issue,positive,positive,positive,positive,positive,positive
665277071,"### OSError: [WinError 193] %1 is not a valid Win32 application

#### Summary

This error message occurs when you have multiple python environments and they are conflicting with each other.

For example, this is the traceback for #163:
```
Traceback (most recent call last):
  File ""D:\Real-Time-Voice-Cloning-master\demo_cli.py"", line 2, in <module>
    from utils.argutils import print_args
  File ""D:\Real-Time-Voice-Cloning-master\utils\argutils.py"", line 2, in <module>
    import numpy as np
  File ""C:\Users\username\AppData\Roaming\Python\Python37\site-packages\numpy\__init__.py"", line 140, in <module>
    from . import _distributor_init
  File ""C:\Users\username\AppData\Roaming\Python\Python37\site-packages\numpy\_distributor_init.py"", line 26, in <module>
    WinDLL(os.path.abspath(filename))
  File ""C:\Users\username\AppData\Local\Programs\Python\Python37\lib\ctypes\__init__.py"", line 364, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: [WinError 193] %1 is not a valid Win32 application
>>> 
```

From the traceback you see these python paths causing a conflict with each other:
`C:\Users\username\AppData\Roaming\Python\Python37\`
`C:\Users\username\AppData\Local\Programs\Python\Python37\`

#### Solution

Look at your traceback and identify the conflicting python environments. Then check your `PATH` and remove one of the environments. Of course the environment you keep still needs to have all of the requirements. A similar issue is reported here: https://github.com/pytorch/pytorch/issues/27693",valid win application summary error message multiple python conflicting example recent call last file line module import file line module import file line module import file line module file line mode valid win application see python causing conflict solution look identify conflicting python check path remove one course environment keep still need similar issue,issue,negative,positive,positive,positive,positive,positive
665183019,"For the next synth model, I will update the code to include a few user-defined custom embedding parameters that are concatenated with the speaker embedding. These would all default to zero, but could be used to represent things like language or accent to faciltate fine-tuning and perhaps speed up training if the classification is known.

Currently, we cannot finetune an accent on the models in a way that generalizes to new speakers for voice cloning (see https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-664704917). My hypothesis is that the accent is attributed to the speaker embedding (of the dataset used for finetuning), so it never generalizes. This would give us a tool to help get around that limitation.

Edit: This is essentially implementing Global Style Tokens: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/230 . Will use Mozilla's repo as a guide to follow.",next model update code include custom speaker would default zero could used represent like language accent perhaps speed training classification known currently accent way new voice see hypothesis accent speaker used never would give u tool help get around limitation edit essentially global style use guide follow,issue,negative,positive,neutral,neutral,positive,positive
665166082,@ACG-MM Would you please provide an update? Is this resolved?,would please provide update resolved,issue,negative,neutral,neutral,neutral,neutral,neutral
665165482,"https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/431#issuecomment-665165023
> ### Pip error: `Could not find a version that satisfies the requirement tensorflow==1.15`
> 
> You are likely using Python 3.8+ which is incompatible with Tensorflow 1.15. To resolve this, you will need to switch to Python 3.6 or 3.7.
> 
> The supported Python versions are listed in README.md. Please follow the [setup instructions](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/README.md#setup) carefully to avoid subsequent problems.
> 
> We plan to support Python 3.8+ at a later date by updating the synthesizer code to a PyTorch-only implementation (#447) or by using a Tensorflow 2.x-compatible tacotron (#370).",pip error could find version requirement likely python incompatible resolve need switch python python listed please follow setup carefully avoid subsequent plan support python later date synthesizer code implementation,issue,negative,negative,neutral,neutral,negative,negative
665165023,"### Pip error: `Could not find a version that satisfies the requirement tensorflow==1.15`

You are likely using Python 3.8+ which is incompatible with Tensorflow 1.15. To resolve this, you will need to switch to Python 3.6 or 3.7.

The supported Python versions are listed in README.md. Please follow the [setup instructions](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/README.md#setup) carefully to avoid subsequent problems.

Python 3.8+ will be supported when the synthesizer code is upgraded to a PyTorch-only implementation. This is currently in the final stages of development and testing (#472).",pip error could find version requirement likely python incompatible resolve need switch python python listed please follow setup carefully avoid subsequent python synthesizer code implementation currently final development testing,issue,negative,negative,neutral,neutral,negative,negative
665141574,"This bug is confirmed, we will be tracking and fixing it in #455. Thank you for submitting the report @FulvioZozix .",bug confirmed fixing thank report,issue,negative,positive,positive,positive,positive,positive
665139132,"That is exactly what happened, the API changed in matplotlib 3.3.0:
https://github.com/matplotlib/matplotlib/blob/a8831d57207db5e9bf681e30810ce0ea146f4a31/doc/api/prev_api_changes/api_changes_3.3.0/removals.rst

> `colorbar.ColorbarBase.get_clim` (use `ScalarMappable.get_clim` instead)
> `colorbar.ColorbarBase.set_clim` (use `ScalarMappable.set_clim` instead)

@barubbabba123 Would you be so kind as to make an update and submit a pull request? Also update requirements.txt to require matplotlib>=3.3 to ensure compatibility.",exactly use instead use instead would kind make update submit pull request also update require ensure compatibility,issue,positive,positive,positive,positive,positive,positive
665136912,"Here is the general plan:

#### Check for compatibility of fatchord synthesizer with our vocoder
1. In the original WaveRNN code, modify the spectrograms to have a range of [-1, 1] instead of [0, 1]
    * See https://github.com/fatchord/WaveRNN/issues/92#issuecomment-500373398
2. Save off a mel during generation with a WaveRNN pretrained model
3. Run it through pretrained vocoder in Real-Time-Voice-Cloning repo to check for compatibility
4. Implement any needed changes into the WaveRNN synthesizer code that we are importing.

#### Refactor code
1. I don't have any training data in fatchord's preferred format, so we will make it accept the format in <datasets_root>/SV2TTS/synthesizer.
2. This is a large enough undertaking that everything else can be updated at the same time.

#### Train a new synthesizer model
1. I will need help reviewing and modifying the hparams from fatchord's tacotron
    * Here are the [current hparams](https://github.com/blue-fish/Real-Time-Voice-Cloning/blob/447_pytorch_synthesizer/synthesizer/hparams.py), modified for 16,000 Hz. I may move this file up a level to have a unified hyperparameter set, at least for synthesizer and vocoder.
2. See how well LibriTTS-based model in #449 works for cloning
    * If at least as good as the current model, then train on LibriTTS.
    * If not, then will train on LibriSpeech to remain consistent with the [training process](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training)
3. Test the pytorch-based model and adjust as needed
    * If voice cloning performance is roughly equal or better, then push the pytorch synthesizer to master and publish the new pretrained synthesizer model
    * If not as good, then push it to a branch of the main Real-Time-Voice-Cloning repo for further development if a few more iterations do not close the performance gap

",general plan check compatibility synthesizer original code modify range instead see save mel generation model run check compatibility implement synthesizer code code training data preferred format make accept format large enough undertaking everything else time train new synthesizer model need help current may move file level unified set least synthesizer see well model work least good current model train train remain consistent training process test model adjust voice performance roughly equal better push synthesizer master publish new synthesizer model good push branch main development close performance gap,issue,positive,positive,positive,positive,positive,positive
665120962,"@FulvioZozix is also experiencing this in #448 but I could not get enough data to confirm.

Can you run `pip freeze` in your terminal? I wonder if matplotlib was recently updated to remove `set_clim`",also could get enough data confirm run pip freeze terminal wonder recently remove,issue,negative,neutral,neutral,neutral,neutral,neutral
665120780,@blue-fish exciting! I can't wait to hear how the training goes!,exciting ca wait hear training go,issue,negative,positive,positive,positive,positive,positive
665119204,"Thanks for pointing that out @sberryman ! Pushed a couple of commits that should fix it. It is now done at the very end of the encoder to avoid duplicating the code for both training and synthesis modes. I ran WaveRNN with the bundled pretrained model to confirm that the dimensions are still correct.

https://github.com/blue-fish/Real-Time-Voice-Cloning/compare/2f59298...blue-fish:4e8d89b",thanks pointing couple fix done end avoid code training synthesis ran model confirm still correct,issue,negative,positive,positive,positive,positive,positive
665032717,"It looks like you added the speaker embeddings to the Encoder. I would have guessed you concat after the encoder. If you look at Corentin's code you'll see he concat's the speaker embeddings to the output of the encoder. 

https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/054f16ecc186d8d4fa280a890a67418e6b9667a8/synthesizer/models/tacotron.py#L158",like added speaker would look code see speaker output,issue,negative,neutral,neutral,neutral,neutral,neutral
664858190,Peer review? Here's the [**updated code**](https://github.com/blue-fish/Real-Time-Voice-Cloning/blob/d6e657832d8ba46c99eaeb80664ccf7ea3e5581b/synthesizer/models/tacotron.py#L25-L50) in the fatchord version of tacotron.py. That was the easy part... now to get training and synthesize to work.,peer review code version easy part get training synthesize work,issue,negative,positive,positive,positive,positive,positive
664761656,"Thank you for the prompt response, that gives me enough to work with for now. It looks like Corentin left plenty of clues in the code as to what changed for SV2TTS, like the following which describes what you mention. I find your explanation helpful. I'll let you know if I can't figure this part out.

https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/054f16ecc186d8d4fa280a890a67418e6b9667a8/synthesizer/models/tacotron.py#L151-L160

Edit: Reviewed all differences between Corentin's and Rayhane-mamah's repo and concluded this is the only significant one. Now to implement it in pytorch.",thank prompt response enough work like left plenty code like following mention find explanation helpful let know ca figure part edit significant one implement,issue,positive,positive,neutral,neutral,positive,positive
664755165,"I never pursued it when I didn't get a response. My assumption is that you have to concat the speaker embeddings to every time step. So basically you take the speaker embedding, expand a dimension `unsqueeze(dim=0)` and then duplicate it based on number of timesteps you are feeding forward for tacotron. So you concat that duplicated embedding to the spectrogram and that becomes the input to the network. I'm writing this as my brain has already shut down for the day but hopefully that is enough to get you started?

If not let me know and I can try and help some more. ",never get response assumption speaker every time step basically take speaker expand dimension duplicate based number feeding forward spectrogram becomes input network writing brain already shut day hopefully enough get let know try help,issue,positive,neutral,neutral,neutral,neutral,neutral
664739182,"@sberryman Did you ever figure out an answer to this question?: https://github.com/fatchord/WaveRNN/issues/139

If you can share anything you learned working with fatchord's tacotron it would save me a lot of time.",ever figure answer question share anything learned working would save lot time,issue,positive,neutral,neutral,neutral,neutral,neutral
664723618,"Just to double-check, you have the LibriSpeech (not LibriTTS) version of train-other-500? I know it's not going to make a big difference but I'd prefer the LibriSpeech version so we can precisely replicate Corentin's setup with only one change (hidden model size).

Let's also plan on training the synth to 278k before switching to encoder training. In the meantime I am trying to work out the pytorch synthesizer (#447).",version know going make big difference prefer version precisely replicate setup one change hidden model size let also plan training switching training trying work synthesizer,issue,negative,positive,neutral,neutral,positive,positive
664711953,"The files in `synthesizer/saved_models/logs-new_model_name/taco_pretrained`

What we need is:
```
tacotron_model.ckpt-######.data-00000-of-00001
tacotron_model.ckpt-######.meta
tacotron_model.ckpt-######.index
checkpoint
```

Every time it reaches a new checkpoint interval it overwrites the oldest checkpoint. It's good to keep a few intermediate checkpoints in case something gets messed up along the way.",need every time new interval good keep intermediate case something along way,issue,negative,positive,positive,positive,positive,positive
664711489,"I already got train-other-500.
VoxCeleb 1 and 2 are password protected... so i gotta wait for an email with a password.
I think we should eventually get a Slack to be more efficient with comms.
I will wait for instructions and will let you know when i have all the files.
until then, i will let the Synth train and reach 125k,
Cheers!",already got password got ta wait password think eventually get slack efficient wait let know let train reach,issue,negative,neutral,neutral,neutral,neutral,neutral
664710815,"In light of #53 we should try and use fatchord's tacotron (the one bundled with [WaveRNN](https://github.com/fatchord/WaveRNN)). Main advantages being code simplicity*, no gaps in spectrograms, and known vocoder compatibility. I'll give it a shot and try to integrate it with this repo.

*Cannot be understated. From an aesthetic point of view this is the nicest Tacotron I've ever seen. Will be a nice addition if we can get it to work.",light try use one main code simplicity known compatibility give shot try integrate aesthetic point view ever seen nice addition get work,issue,negative,positive,positive,positive,positive,positive
664709031,"which files do you want me to backup so i don't mess this up?
I don't want to loose any of that work. (117K now)
I'll zip it and share.",want backup mess want loose work zip share,issue,negative,negative,negative,negative,negative,negative
664704917,"> Also I did another experiment and trained the synthesizer for about 5,000 additional steps on the entire VCTK dataset (trying to help out on #388). The accent still does not transfer for zero-shot cloning. I suspect the synthesizer needs to be trained from scratch if that is the goal.

Changing my mind on training from scratch, I think we just need to add an extra input parameter to the synthesizer which indicates the accent or more accurately the dataset that it is trained on. A simple implementation might be a single bit representing LibriSpeech or VCTK. Next, finetune the existing models on VCTK with the added parameter. Then for inference specify the dataset that you want the result to sound like. I'm at a loss how to implement this with the current set of models, but I think this repo will have clues: https://github.com/Tomiinek/Multilingual_Text_to_Speech

I'm all done with accent experiments for now but I hope this is helpful to anyone who wants to continue this work.",also another experiment trained synthesizer additional entire trying help accent still transfer suspect synthesizer need trained scratch goal mind training scratch think need add extra input parameter synthesizer accent accurately trained simple implementation might single bit next added parameter inference specify want result sound like loss implement current set think done accent hope helpful anyone continue work,issue,positive,positive,neutral,neutral,positive,positive
664694985,"I hope this is now resolved, but if not, feel free to reopen the issue when you have more information that will allow us to continue troubleshooting.",hope resolved feel free reopen issue information allow u continue,issue,positive,positive,positive,positive,positive,positive
664668653,"> Let's imagine that I have some samples text-voice of cloned voice

You should fine-tune the original pretrained models on your voice samples. I have a process and example here: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-663308789",let imagine voice original voice process example,issue,negative,positive,positive,positive,positive,positive
664666890,"> You said:
> `there are many other better open-source implementations of neural TTS out there, and new ones keep coming every day.`
> Which one are better? can you name them?

> Prior to becoming my colleague, fatchord wrote not only WaveRNN but also a Tacotron 1 implementation (which, by the way, is not proved inferior to Tacotron 2): https://github.com/fatchord/WaveRNN
> 
> NVIDIA has a Tacotron 2 implementation: https://github.com/NVIDIA/tacotron2
> 
> Mozilla as well, with more frequent updates & features: https://github.com/mozilla/TTS
> 
> I would also check paperswithcode.com and ignore my repo and the ones above if you're looking for something else; perhaps something more recent, as neural TTS is still very much growing. https://paperswithcode.com/task/text-to-speech-synthesis

Also when I said:
>  and new ones keep coming every day

I actually meant that new _papers_ keep coming every day, not open source implementations (sadly)",said many better neural new keep coming every one better name prior becoming colleague wrote also implementation way proved inferior implementation well frequent would also check ignore looking something else perhaps something recent neural still much growing also said new keep coming every day actually meant new keep coming every day open source sadly,issue,negative,positive,positive,positive,positive,positive
664663321,"If I were going to do this, I'd preprocess train.txt and convert the texts to ARPAbet using one of the methods suggested here: https://stackoverflow.com/questions/11911028/python-arpabet-phonetic-transcription

Then concat the new file with the original train.txt and finetune the model on the combined file. However, it might take a very large number of iterations before it learns ARPAbet and generalizes it to new situations. Probably just as fast to train the synthesizer from scratch on ARPAbet only and add a text preprocessor that runs at inference time.

@shawwn Do you still want this feature? We are just starting to train a new synthesizer in #449, no guarantees we will have the GPU time to accommodate the request but it is something that jumps out as being feasible.",going convert one new file original model combined file however might take large number new probably fast train synthesizer scratch add text inference time still want feature starting train new synthesizer time accommodate request something feasible,issue,negative,positive,positive,positive,positive,positive
664661919,"You said:
`there are many other better open-source implementations of neural TTS out there, and new ones keep coming every day.`
Which one are better? can you name them?",said many better neural new keep coming every one better name,issue,positive,positive,positive,positive,positive,positive
664657160,"Please start by downloading the following datasets. These datasets are huge!
- **[LibriSpeech](http://www.openslr.org/12/):** train-other-500 (extract as `LibriSpeech/train-other-500`)
- **[VoxCeleb1](http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html):** Dev A - D as well as the metadata file (extract as `VoxCeleb1/wav` and `VoxCeleb1/vox1_meta.csv`)
- **[VoxCeleb2](http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html):** Dev A - H (extract as `VoxCeleb2/dev`)

Later I'll open a new issue for the encoder training and post instructions there.",please start following huge extract dev well file extract dev extract later open new issue training post,issue,positive,positive,positive,positive,positive,positive
664646524,"Can you make a backup of the 100k model checkpoint (or one that is in this range)? Just in case we want to come back to it later.

Is the average loss still coming down? Perhaps it converges much faster with LibriTTS. When I did the single-speaker finetuning on LibriSpeech p211 the synthesizer loss started at 0.70, and you are already in the 0.60-0.65 range.",make backup model one range case want come back later average loss still coming perhaps much faster synthesizer loss already range,issue,negative,positive,neutral,neutral,positive,positive
664628429,"Alright, then, let's switch training on the Encoder then.
We just passed the 103k on Synth.

Provide me with the instructions and i'll do it",alright let switch training provide,issue,negative,neutral,neutral,neutral,neutral,neutral
664620373,"Thank you for contributing your time and hardware @mbdash .

If you've had a chance to look at the figure in https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/30#issuecomment-508381648 you'll notice that:

1. The encoder makes training embeds for the synthesizer,
2. The encoder and synthesizer are used to make training mels for the vocoder.

If an upstream element is changed, the downstream elements need to be retrained in most cases. Therefore if we are changing the encoder we should also retrain or at least finetune the synthesizer. If the synthesizer changes, then similarly update the vocoder.

So we should make our best effort to train the encoder and do any follow-on work with the synth and vocoder. If it turns out that the outputs are similar enough, we should be able to jump back and forth and finetune as you are proposing. Though it may still be good to proceed serially if there is any desire to make the training process repeatable for those who want to improve on the models in the future.",thank time hardware chance look figure notice training synthesizer synthesizer used make training upstream element downstream need therefore also retrain least synthesizer synthesizer similarly update make best effort train work turn similar enough able jump back forth though may still good proceed serially desire make training process repeatable want improve future,issue,positive,positive,positive,positive,positive,positive
664541903,"I will gladly put my GPU to work whenever I can.

We could go in milestones, 
IE: 
reach 250k on the Synth model,  (92k as we speak)
then bring the Encoder model on par, 
then switch back to the Synth and bring it to 500k 
back and forth to 750k, 1M 1.25M etc

Eventually I will need it for other things,
but until then, I can put it to good use.


",gladly put work whenever could go ie reach model speak bring model par switch back bring back forth eventually need put good use,issue,positive,positive,positive,positive,positive,positive
664489622,"@mbdash If you don't have plans for your GPU after the LibriTTS model finishes, would you be willing to help train a new encoder for better voice cloning quality?

You would use the same process as [wiki/Training](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training), but change the params for a hidden layer size of 768 instead of the current 256. There is a lot of info on this in #126 but the model in that issue was trained with an output size of 768 which makes it incompatible with everything else we have. According to [wiki/Pretrained-models](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models) the current encoder trained to 1.56M steps in 20 days on a 1080ti.",model would willing help train new better voice quality would use process change hidden layer size instead current lot model issue trained output size incompatible everything else according current trained day ti,issue,positive,positive,positive,positive,positive,positive
664434662,"I don't think the numbers are very accurate

![image](https://user-images.githubusercontent.com/32403586/88554788-eb9b1300-cff4-11ea-8ac8-f171f24cb4ab.png)

I try counting Mississippis but they pop / print way faster and sometimes in fast sequences
![image](https://user-images.githubusercontent.com/32403586/88555058-47fe3280-cff5-11ea-88cb-3cff756b7a4c.png)
",think accurate image try counting pop print way faster sometimes fast image,issue,negative,positive,positive,positive,positive,positive
664430427,How long does it take to run each step now? Clearly it is progressing faster than 1.3-1.4 sec/step that is in the screenshot from yesterday.,long take run step clearly faster yesterday,issue,negative,positive,neutral,neutral,positive,positive
664429238,"### GPU support for the toolbox

Configuring GPU support for the toolbox is difficult. Fortunately, the toolbox can run on the CPU. Download the [`423_add_cpu_mode`](https://github.com/blue-fish/Real-Time-Voice-Cloning/archive/423_add_cpu_mode.zip) branch on my fork. Then you can run `python demo_toolbox.py --cpu` and it will override the `CUDA_VISIBLE_DEVICES` environment variable to force the toolbox to run in CPU-only mode.

If you must have GPU support, questions about CUDA installation should be submitted to a different support channel. Try asking your question in the CUDA setup and installation section of the NVIDIA developer forums: [https://forums.developer.nvidia.com/c/accelerated-computing/cuda/cuda-setup-and-installation](https://forums.developer.nvidia.com/c/accelerated-computing/cuda/cuda-setup-and-installation)",support toolbox support toolbox difficult fortunately toolbox run branch fork run python override environment variable force toolbox run mode must support installation different support channel try question setup installation section developer,issue,positive,negative,neutral,neutral,negative,negative
664429028,Yes I keep listening to them paying attention to details and I can clearly ear the tts using the punctuation.,yes keep listening paying attention clearly ear punctuation,issue,positive,positive,positive,positive,positive,positive
664427410,"Hi, I am sorry to hear that you are having difficulties with the toolbox. Unfortunately the best I can offer is this form response from https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/431#issuecomment-663577257

> ### All questions about Colab Notebook
> 
> The Colab Notebook is a community-developed resource to enable users to run the toolbox without having a GPU or going through a complicated setup. Recently, these issues have been resolved with CPU support being added (#366) and the installation process streamlined (#375). **We recommend that you use a normal Python environment.**
> 
> Users who still prefer using Colab Notebook should understand that no official support will be provided, though you are welcome to ask questions to get help from the community. If you believe you've found a bug with the underlying toolbox code, please try to replicate the issue in a normal Python environment. We are not Colab Notebook users and are unable to troubleshoot Colab Notebook errors.",hi sorry hear toolbox unfortunately best offer form response notebook notebook resource enable run toolbox without going complicated setup recently resolved support added installation process streamlined recommend use normal python environment still prefer notebook understand official support provided though welcome ask get help community believe found bug underlying toolbox code please try replicate issue normal python environment notebook unable notebook,issue,positive,positive,neutral,neutral,positive,positive
664421197,"@mbdash From that batch I find the 50k sample remarkable. Your LibriTTS-based model is much closer to the ground truth, capturing the effect of the 3 commas and question mark on prosody.

For this one clip I say your model performs better than LibriSpeech_278k but it will be interesting to see how well the model generalizes to new voices (embeddings) unseen during training.

`As they sat thus something brushed against peter as light as a kiss, and stayed there, as if saying timidly, ""Can I be of any use?""`

[step-50000_comparison.zip](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/4982446/step-50000_comparison.zip)",batch find sample remarkable model much closer ground truth effect question mark prosody one clip say model better interesting see well model new unseen training sat thus something brushed peter light kiss stayed saying timidly use,issue,positive,positive,positive,positive,positive,positive
664401497,If you must have GPU support then try asking your question in the CUDA setup and installation section of the NVIDIA developer forums: [https://forums.developer.nvidia.com/c/accelerated-computing/cuda/cuda-setup-and-installation](https://forums.developer.nvidia.com/c/accelerated-computing/cuda/cuda-setup-and-installation),must support try question setup installation section developer,issue,negative,neutral,neutral,neutral,neutral,neutral
664400731,"Download the [`423_add_cpu_mode`](https://github.com/blue-fish/Real-Time-Voice-Cloning/archive/423_add_cpu_mode.zip) branch on my fork. Then you can run `python demo_toolbox.py --cpu` and it will override the `CUDA_VISIBLE_DEVICES` environment variable to force the toolbox to run in CPU-only mode.

If this resolves your problem I will go ahead and submit a pull request for it.",branch fork run python override environment variable force toolbox run mode problem go ahead submit pull request,issue,negative,neutral,neutral,neutral,neutral,neutral
664063699,"@gfhsrtras I'm not sure on the status on the exe, if you don't want to wait for it another option is to run it in an Amazon Sagemaker notebook, see #398 .",sure status want wait another option run notebook see,issue,negative,positive,positive,positive,positive,positive
664047867,"I used the original pretrained models (hereafter, LibriSpeech_278k) to synthesize the same utterance as the 20k example, also inverting it with Griffin-Lim. The clarity is about the same but there is less harshness with LibriSpeech_278k (not sure what the correct technical term for that is). 

""When he spoke of the execution he wanted to pass over the horrible details, but Natasha insisted that he should not omit anything.""

You can definitely hear more of a pause after ""details"" in the 20k wav so the new model is learning how to deal with punctuation!

#### [4592_22178_000024_000001.zip](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/4978798/4592_22178_000024_000001.zip)",used original hereafter synthesize utterance example also clarity le harshness sure correct technical term spoke execution pas horrible omit anything definitely hear pause new model learning deal punctuation,issue,positive,positive,neutral,neutral,positive,positive
664038511,"### No module named 'tensorflow.contrib'

#### Summary

The toolbox requires Tensorflow 1.15 and this error message occurs when Tensorflow 2.x is installed.

#### Solution

Install Tensorflow 1.15.

If you get a pip error `Could not find a version that satisfies the requirement tensorflow==1.15` then you are likely using Python 3.8+ which is incompatible with TF 1.15. To resolve that, you will need to switch to Python 3.6 or 3.7.",module summary toolbox error message solution install get pip error could find version requirement likely python incompatible resolve need switch python,issue,negative,neutral,neutral,neutral,neutral,neutral
664036465,@davysouthernboy Please open a new issue if you have problems setting up the toolbox on a new operating system.,please open new issue setting toolbox new operating system,issue,negative,positive,neutral,neutral,positive,positive
664036376,"Closing this issue as we've found out the root cause (duplicate of #401). @ACG-MM , if you have further issues with toolbox setup please open a new issue.",issue found root cause duplicate toolbox setup please open new issue,issue,negative,positive,neutral,neutral,positive,positive
664033998,"Overall, the synthesizer training seems to be progressing nicely! I'll be interested to see as many plots and wavs as you care to share, but otherwise it's a lot of waiting now.

It would be nice if you can share in-work checkpoints, say starting at 100k and every 50k steps after that. Or generate some samples using the toolbox. I've never trained from the start and it would be interesting to see the progression.",overall synthesizer training nicely interested see many care share otherwise lot waiting would nice share say starting every generate toolbox never trained start would interesting see progression,issue,positive,positive,positive,positive,positive,positive
664032080,"if this can help:
I have Ubuntu 20.

Here is my conda env file:
[rtvc_py373.zip](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/4978276/rtvc_py373.zip)

**Setup a Conda Virtual Env.**
```
conda env create -f rtvc_py373.yml

# then before starting the demo:
conda activate rtvc_py373

python demo.cli.py
```",help file setup virtual create starting activate python,issue,positive,neutral,neutral,neutral,neutral,neutral
664031383,"@sagar-spkt You might find this interesting. @mbdash is training the synthesizer on LibriTTS with default hparams in #449. We did not create alignments, instead discarding long utterances per @CorentinJ 's suggestion https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/413#issuecomment-657382603 .",might find interesting training synthesizer default create instead long per suggestion,issue,positive,positive,positive,positive,positive,positive
664030450,"What is your operating system? You might need to upgrade. This person reported an issue with Ubuntu 14, resolved by upgrading to Ubuntu 16: https://github.com/tensorforce/tensorforce/issues/303#issuecomment-359344743",operating system might need upgrade person issue resolved,issue,negative,neutral,neutral,neutral,neutral,neutral
664029164,"> Where are located the wavs you want me to share?

Check out the training logs area: `synthesizer/saved_models/logs-new_model_name/wavs`

The files in the `plots` folder are also interesting and show how well the new synthesizer model is working.
",want share check training area folder also interesting show well new synthesizer model working,issue,positive,positive,positive,positive,positive,positive
664028858,"step 10k reached @ 15h30
so we can estimate ~10k steps / 3h

Where are located the wavs you want me to share?
When I try to `ls datasets/SV2TTS/synthesizer/audio` my terminal hang.

![image](https://user-images.githubusercontent.com/32403586/88487405-cbb31300-cf52-11ea-91af-5bbd24611a3a.png)
",step estimate want share try terminal image,issue,negative,neutral,neutral,neutral,neutral,neutral
664026622,"@ViktorAlm This is the difference between your model structure and the default. The input embedding batch size changed from 66 to 72. Do you know where that could be coming from? I diffed the synthesizer code of taco2swe and this repo, and can't find where that is coming from. Using the taco2swe version of the hparams doesn't seem to fix it either.

```
- ('Tacotron_model/Tacotron_model/inference/inputs_embedding/Adam', [66, 512]),
- ('Tacotron_model/Tacotron_model/inference/inputs_embedding/Adam_1', [66, 512]),
+ ('Tacotron_model/Tacotron_model/inference/inputs_embedding/Adam', [72, 512]),
+ ('Tacotron_model/Tacotron_model/inference/inputs_embedding/Adam_1', [72, 512]),
- ('Tacotron_model/inference/inputs_embedding', [66, 512]),
+ ('Tacotron_model/inference/inputs_embedding', [72, 512]),
```",difference model structure default input batch size know could coming synthesizer code ca find coming version seem fix either,issue,negative,neutral,neutral,neutral,neutral,neutral
664024791,"If that's a typical batch generation time now, 2.3 sec for 64 batches is just 0.036 sec per step or 1 hour over 100,000 steps. Not worth it to transfer the data over to the SSD in my opinion.",typical batch generation time sec sec per step hour worth transfer data opinion,issue,negative,negative,neutral,neutral,negative,negative
664023877,"latest @ 14h25:
![image](https://user-images.githubusercontent.com/32403586/88486458-9ce56e80-cf4b-11ea-92bb-f5ba3f51992b.png)


My setup is not optimal. It is currently residing on the HDD side on my array, I just added a new SSD but is is not been used atm.
When I stop the training, I will  move the data on a share living on the SSD or even a passthrough NVME.",latest image setup optimal currently side array added new used stop training move data share living even,issue,negative,positive,positive,positive,positive,positive
664018038,Any updates @FulvioZozix ? If I had to guess you might need to upgrade your matplotlib version.,guess might need upgrade version,issue,negative,neutral,neutral,neutral,neutral,neutral
664016409,"> `Generated 64 train batches of size 36 in 21.814 sec`

This seems to be a bottleneck, is the data on an external drive? I'm averaging about 14 sec for batch generation on a slow CPU but the data lives on a SSD.",train size sec bottleneck data external drive sec batch generation slow data,issue,negative,negative,negative,negative,negative,negative
664015371,"I don't understand Korean so the demo didn't make much of an impact. And I am also new to TTS and ML in general so I can't claim to understand the paper either. The general concept is promising though. I wonder if others have attempted something similar.

From [the paper](https://arxiv.org/abs/1906.05507):
> The experiments were performed on our internal Korean dataset, containing seven emotions (the neutral and six basic emotions) uttered by a male and female speaker. Every style category has 3000 sentences, recorded in 16kHz sampling rate.

Training a multispeaker TTS requires a lot of input data, which could also be used to train an ""emotion encoder"" to automatically assign (P,A,D) values based on clues from text and the recorded speech. (Section 2.2 says that the actual model uses 32 dimensions for emotion so the emotion encoder could output in 32-D.)  Then use that in synthesizer training. I think it should generalize well because how the emotion manifests itself in an utterance should be independent of the voice of the person speaking it. Furthermore you could also use the info to correct or normalize the utterance embeddings generated by the speaker encoder.",understand make much impact also new general ca claim understand paper either general concept promising though wonder something similar paper internal seven neutral six basic male female speaker every style category sampling rate training lot input data could also used train emotion automatically assign based text speech section actual model emotion emotion could output use synthesizer training think generalize well emotion utterance independent voice person speaking furthermore could also use correct normalize utterance speaker,issue,positive,positive,neutral,neutral,positive,positive
664013715,Wow that is fast. At that rate it will take just over 4 days to reach the 278k steps in the current model. And it will train even faster as the model gets better. Please share some griffin-lim wavs when they become intelligible.,wow fast rate take day reach current model train even faster model better please share become intelligible,issue,positive,positive,positive,positive,positive,positive
664008878,"haha Great answer.
But again. 
I did not ask for you to implement it, but simply your opinion.
And I understand what you meant. It is not that simple and would requires lots of work.

thx again.
",great answer ask implement simply opinion understand meant simple would lot work,issue,positive,positive,positive,positive,positive,positive
664005830,"@mbdash wrote this in #449 but I am moving it here just to keep the issues organized:
> Also, as a side note, 
the test you did using a dataset of 1 voice had great results! After training on LibriTTS the result should be even more amazing.
I was just lying in bed yesterday night and thinking about the current RTVC potential and was wondering what you would think about this:
AzamRabiee/Emotional-TTS
see this: https://youtu.be/bh2HP0n2ik8
Have you seen that one before?

In general it takes a lot of effort to make a practical implementation of whatever is demonstrated in research papers. This project is one example, and Corentin made a masters thesis out of it, which is on the order of 1,000 hours of work. So my reaction to most new research tends to be ""cool, but I'll wait for someone else to build it."" Just because you can do it doesn't mean you should. Life is too short.",wrote moving keep organized also side note test voice great training result even amazing lying bed yesterday night thinking current potential wondering would think see seen one general lot effort make practical implementation whatever research project one example made thesis order work reaction new research cool wait someone else build mean life short,issue,positive,positive,positive,positive,positive,positive
664003772,"Ok, great, 
if you tell me it is as designed I will continue. it is currently at 50% Embedding.

I opened the image but I need slightly more coffee to really look at it ;-)

thx for the quick response.",great tell designed continue currently image need slightly coffee really look quick response,issue,positive,positive,positive,positive,positive,positive
664003701,"Please use python 3.7. We have an issue open to make the toolbox compatible with TF 2.x: #370 , but it is not ready yet.",please use python issue open make toolbox compatible ready yet,issue,positive,positive,neutral,neutral,positive,positive
664001573,"@mbdash Look at the middle part of the image here and hopefully it will make more sense why the pretrained **encoder** model is needed to generate embeddings for synthesizer training: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/30#issuecomment-508381648 Please speak up if it still doesn't make sense.

Think of the synthesizer as a black box with 2 inputs: an embedding, and text to synthesize. Different speakers sound different even when speaking the same text. The synthesizer uses the embedding to impart that voice information in the  mel spectrogram that it produces as output. The synthesizer gets the embedding from the encoder, which in turn can be thought of a black box that turns a speaker's wav data into an embedding.

So you need to run the encoder model to get the embedding, and you get the error message because it can't find the model.",look middle part image hopefully make sense model generate synthesizer training please speak still make sense think synthesizer black box text synthesize different sound different even speaking text synthesizer impart voice information mel spectrogram output synthesizer turn thought black box turn speaker data need run model get get error message ca find model,issue,negative,positive,neutral,neutral,positive,positive
664000579,"Hi have an error because synthesizer_preprocess_embeds.py wants a pretrained model?

I fail to understand why we need to provide pre-trained data when trying to train from scratch, but i will stick in the latest pretrained model until told otherwise.

```
(rtvc_py373) username@vm:~/github/Real-Time-Voice-Cloning$ python synthesizer_preprocess_embeds.py /mnt/nfs/a_share/rtvc_LibriTTS/datasets/SV2TTS/synthesizer/
Arguments:
    synthesizer_root:      /mnt/nfs/a_share/rtvc_LibriTTS/datasets/SV2TTS/synthesizer
    encoder_model_fpath:   encoder/saved_models/pretrained.pt
    n_processes:           4

Embedding:   0%|                                                                                                                                                  | 0/111521 [00:02<?, ?utterances/s]
multiprocessing.pool.RemoteTraceback:
""""""
Traceback (most recent call last):
  File ""/opt/miniconda3/envs/rtvc_py373/lib/python3.7/multiprocessing/pool.py"", line 121, in worker
    result = (True, func(*args, **kwds))
  File ""/home/username/github/Real-Time-Voice-Cloning/synthesizer/preprocess.py"", line 228, in embed_utterance
    encoder.load_model(encoder_model_fpath)
  File ""/home/username/github/Real-Time-Voice-Cloning/encoder/inference.py"", line 33, in load_model
    checkpoint = torch.load(weights_fpath, _device)
  File ""/opt/miniconda3/envs/rtvc_py373/lib/python3.7/site-packages/torch/serialization.py"", line 384, in load
    f = f.open('rb')
  File ""/opt/miniconda3/envs/rtvc_py373/lib/python3.7/pathlib.py"", line 1186, in open
    opener=self._opener)
  File ""/opt/miniconda3/envs/rtvc_py373/lib/python3.7/pathlib.py"", line 1039, in _opener
    return self._accessor.open(self, flags, mode)
FileNotFoundError: [Errno 2] No such file or directory: 'encoder/saved_models/pretrained.pt'
""""""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""synthesizer_preprocess_embeds.py"", line 25, in <module>
    create_embeddings(**vars(args))
  File ""/home/username/github/Real-Time-Voice-Cloning/synthesizer/preprocess.py"", line 254, in create_embeddings
    list(tqdm(job, ""Embedding"", len(fpaths), unit=""utterances""))
  File ""/opt/miniconda3/envs/rtvc_py373/lib/python3.7/site-packages/tqdm/std.py"", line 1130, in __iter__
    for obj in iterable:
  File ""/opt/miniconda3/envs/rtvc_py373/lib/python3.7/multiprocessing/pool.py"", line 748, in next
    raise value
FileNotFoundError: [Errno 2] No such file or directory: 'encoder/saved_models/pretrained.pt'
```",hi error model fail understand need provide data trying train scratch stick latest model told otherwise python recent call last file line worker result true file line file line file line load file line open file line return self mode file directory exception direct cause following exception recent call last file line module file line list job file line iterable file line next raise value file directory,issue,negative,positive,neutral,neutral,positive,positive
663998996,"> > it always shows Could not find a version that satisfies the requirement tensorflow==1.15
> 
> What version of python do you have? The toolbox requires python 3.6 or 3.7.
> 
> Tensorflow==1.15 is not available for python 3.8 or newer.



> > it always shows Could not find a version that satisfies the requirement tensorflow==1.15
> 
> What version of python do you have? The toolbox requires python 3.6 or 3.7.
> 
> Tensorflow==1.15 is not available for python 3.8 or newer.

i do use python 3.8 and i just get it now. may this project support TF 2x?  am i have to use python 3.7",always could find version requirement version python toolbox python available python always could find version requirement version python toolbox python available python use python get may project support use python,issue,negative,positive,positive,positive,positive,positive
663993142,">  it always shows Could not find a version that satisfies the requirement tensorflow==1.15

What version of python do you have? The toolbox requires python 3.6 or 3.7.

Tensorflow==1.15 is not available for python 3.8 or newer.",always could find version requirement version python toolbox python available python,issue,negative,positive,positive,positive,positive,positive
663992734,the cudart64_101.dll in C:\Program Files\NVIDIA Corporation\NvStreamSrv. and i tried install tensorflow-gpu still error above. and when i try pip install -r requirements.txt  it always shows Could not find a version that satisfies the requirement tensorflow==1.15,tried install still error try pip install always could find version requirement,issue,negative,neutral,neutral,neutral,neutral,neutral
663952935,"So I've got some good news and bad news.

* Bad news first: In https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-663951476 I mention trying to add an accent using the VCTK dataset and it does not generalize to all speakers. You need to train a synthesizer from scratch to impart an accent with zero-shot cloning.

* Good news: If you only require a single speaker you can finetune a model in a matter of hours on CPU. (You also need to prepare the dataset, with recordings and text file transcripts, and preprocess them.) Here are my latest results and an example to follow: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-663308789",got good news bad news bad news first mention trying add accent generalize need train synthesizer scratch impart accent good news require single speaker model matter also need prepare text file latest example follow,issue,negative,positive,neutral,neutral,positive,positive
663951476,"Some general observations to share:
1. Finetuning improves both quality and similarity with the target voice, and transfers accent.
2. Decent single-speaker models require as little as 5 min of audio and 400 steps of synthesizer training.
3. Finetuning the vocoder is not as impactful as finetuning the synthesizer. In fact given the quality limitations of the underlying models (see #411) I would not bother with additional vocoder training.

Also I did another experiment and trained the synthesizer for about 5,000 additional steps on the entire VCTK dataset (trying to help out on #388). The accent still does not transfer for zero-shot cloning. I suspect the synthesizer needs to be trained from scratch if that is the goal.

P.S. @mbdash I updated the VCTKp240 post with a single-speaker dataset if you would like to try that out. https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/437#issuecomment-663308789",general share quality similarity target voice accent decent require little min audio synthesizer training synthesizer fact given quality underlying see would bother additional training also another experiment trained synthesizer additional entire trying help accent still transfer suspect synthesizer need trained scratch goal post would like try,issue,negative,positive,neutral,neutral,positive,positive
663940739,"Thank you for sharing your code with us @plummet555 .

Please ask any follow-up questions as needed @mbdash and close the issue when you are satisfied.",thank code u plummet please ask close issue satisfied,issue,positive,positive,positive,positive,positive,positive
663940605,"Some other pytorch-based tacotrons are [Mozilla's TTS](https://github.com/mozilla/TTS) and [this repo from Tomiinek](https://github.com/Tomiinek/Multilingual_Text_to_Speech) which does voice cloning with multilingual capability.

Does anyone know if the method proposed here is viable? Supposedly you can convert a tensorflow model to pytorch, and refactor the code in the process. https://medium.com/huggingface/from-tensorflow-to-pytorch-265f40ef2a28",voice multilingual capability anyone know method viable supposedly convert model code process,issue,negative,neutral,neutral,neutral,neutral,neutral
663939539,"Thanks for the update and correction.

Let's run training with the default hparams. We're already switching from LibriSpeech to LibriTTS and it's best to only change one parameter at a time.",thanks update correction let run training default already switching best change one parameter time,issue,positive,positive,positive,positive,positive,positive
663926980,"Update 2020-07-25 22h20 EST: 

step 5 Generate mel spectrograms for training
Currently at 25% all  cpus available to VM are at full load.

For posterity, note typo in command in step 5, missing ""s"" in the flag ""--datasets_name""
python synthesizer_preprocess_audio.py ~/rtvc_LibriTTS/datasets --no_alignments **--datasets**_name LibriTTS

",update step generate mel training currently available full load posterity note typo command step missing flag python,issue,negative,positive,positive,positive,positive,positive
663795110,"Ok, 
I will sync LibriTTS overnight, try to set this up over the weekend and get the GPU working on it.",sync overnight try set weekend get working,issue,negative,neutral,neutral,neutral,neutral,neutral
663791658,"This is the code that prints ""4 more points"":
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/054f16ecc186d8d4fa280a890a67418e6b9667a8/toolbox/ui.py#L99-L111

Try commenting these lines out and see if it makes a difference:
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/054f16ecc186d8d4fa280a890a67418e6b9667a8/toolbox/__init__.py#L118-L119

I do not know what else could be clearing `self.utterances` in the toolbox.

Can you do a `pip freeze` and share the output? You might have some incompatible package versions.",code try see difference know else could clearing toolbox pip freeze share output might incompatible package,issue,negative,neutral,neutral,neutral,neutral,neutral
663787626,"From what I understand, LibriTTS offers several advantages over LibriSpeech:
1. The transcripts contain punctuation so the model will respond to it instead of ignoring it as it does currently.
2. Audio has been split into smaller segments making alignments unnecessary
3. Higher sampling rate of 24 kHz instead of 16 kHz

We should consider updating the hparams so we can ultimately generate 24 kHz audio from this:
    * Edit: There are more fundamental problems than bitrate affecting quality, so keeping it at 16,000 is preferable as it speeds training and retains compatibility with the current vocoder
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/054f16ecc186d8d4fa280a890a67418e6b9667a8/synthesizer/hparams.py#L113-L117

@CorentinJ also suggests reducing the max allowable utterance duration (these hparams are used in synthesizer/preprocess.py):
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/054f16ecc186d8d4fa280a890a67418e6b9667a8/synthesizer/hparams.py#L95-L103

I don't have any solutions for the other suggestions mentioned (switching attention paradigm, removing speakers with bad prosody): https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/364#issuecomment-653996443",understand several contain punctuation model respond instead currently audio split smaller making unnecessary higher sampling rate instead consider ultimately generate audio edit fundamental affecting quality keeping preferable training compatibility current also reducing allowable utterance duration used switching attention paradigm removing bad prosody,issue,negative,negative,negative,negative,negative,negative
663785345,"@mbdash I just noticed this. This would be a really nice contribution if you are up for it!

On the [pretrained models page](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models) it says the synthesizer was trained in a week on 4 GPUs (1080ti). If you are not willing to tie up your GPU for a full month, it will still be helpful if you can get to a partially-trained model that has intelligible speech so others can continue training and finetuning.

### Training instructions for synthesizer

1. Pull the latest copy of the repo to get LibriTTS support in #441.
2. Download LibriTTS ""train-clean-100"" and ""train-clean-360"" from here: https://openslr.org/60/
    * While it is downloading, enable tensorflow GPU support if not already done
3. Make a datasets folder, it can be on an external drive if you don't have enough storage (this will consume 150-200 GB)
4. Extract LibriTTS downloads to this path: datasets/LibriTTS
5. Generate mel spectrograms for training: `python synthesizer_preprocess_audio.py path/to/datasets_folder --no_alignments --datasets_name LibriTTS`
6. Generate embeddings for training: `python synthesizer_preprocess_embeds.py path/to/datasets_folder/SV2TTS/synthesizer`
7. Start training from scratch: `python synthesizer_train.py new_model_name path/to/datasets_folder/SV2TTS/synthesizer`
    * You will start seeing wavs when it reaches each checkpoint interval (default: 2,000 steps)

You can quit and resume training at any time, though you will lose all progress since the last checkpoint. It will be interesting to see how well it does with default hparams.",would really nice contribution page synthesizer trained week ti willing tie full month still helpful get model intelligible speech continue training training synthesizer pull latest copy get support enable support already done make folder external drive enough storage consume extract path generate mel training python generate training python start training scratch python start seeing interval default quit resume training time though lose progress since last interesting see well default,issue,positive,positive,positive,positive,positive,positive
663775429,"This occurs when the synthesizer is unable to generalize to your voice. It is either caused by the encoder or the synthesizer not having been trained on a similar voice, but I don't know which. More likely the synthesizer I think.

In any event I will close this as a duplicate of #41. Thank you for posting the sample and reporting the issue.",synthesizer unable generalize voice either synthesizer trained similar voice know likely synthesizer think event close duplicate thank posting sample issue,issue,negative,negative,negative,negative,negative,negative
663763891,"What happens when you load a file? Does the embedding and mel spectrogram display?

Can you post a screenshot?",load file mel spectrogram display post,issue,negative,neutral,neutral,neutral,neutral,neutral
663725600,"Yes, that fixes the issue.
Also changing the try, except block in `synthesizer/preprocess.py` to this:

```
try:
     alignments_fpath = next(book_dir.glob(""*.alignment.txt""))
     with alignments_fpath.open(""r"") as alignments_file:
          alignments = [line.rstrip().split("" "") for line in alignments_file]
      except :
           # A few alignment files will be missing
           continue
```

keeps the preprocessing running for the non-problematic files.",yes issue also try except block try next line except alignment missing continue running,issue,negative,negative,neutral,neutral,negative,negative
663682420,"Looks like the project has moved forward (which is great!) but I think it will be a while before I get a chance to rebase and try it out. So, if it helps. for now I'll just copy here the code I wrote to add the word 'skip' to the start of each line, then to find the silence following it so it can be trimmed back out from the output. It's a copy of demo_cli.py (which I called sv2tts_cli.py).

You can run it as e.g.:
python3 sv2tts_cli.py input_sample.wav input.txt exported.mp3 --cpu

where input.txt contains one or more lines of text. --cpu is optional.

Hope this helps


```
import warnings
warnings.filterwarnings('ignore',category=FutureWarning)
warnings.filterwarnings('ignore',category=DeprecationWarning)
warnings.filterwarnings('ignore',message=""The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead."")
#import tensorflow.python.util.deprecation as deprecation
#deprecation._PRINT_DEPRECATION_WARNINGS = False

import traceback
from encoder.params_model import model_embedding_size as speaker_embedding_size
from utils.argutils import print_args
from synthesizer.inference import Synthesizer
from encoder import inference as encoder
from vocoder import inference as vocoder
from pathlib import Path
import numpy as np
import librosa
import argparse
import torch
import sys
from pydub import effects
from scipy.io.wavfile import read
from pydub import AudioSegment
from pydub.silence import detect_silence
from pydub.playback import play
import ffmpeg

##Expects numpy array of float
def find_silence(samples, rate):
    tenms = int(rate / 100)
    ms = int(rate / 1000)

    threshold  = 0.04 #lower means more is considered to be noise, higher means more is considered to be silence
    run_threshold = 5
    silence_count = 0
    mids=[]
    last_block = int(samples.size / tenms) * tenms

    for outer in range(0,last_block,tenms):
        average = 0

        for inner in range(outer,outer+tenms-1):
            average += abs(samples[inner])

        #print (""%d,%f"" %(outer/ms, average/tenms))
        if ((average/tenms >= threshold) or (outer == last_block - tenms)):
            if (silence_count >= run_threshold):
                start_sample = outer - (silence_count * tenms)
                end_sample = outer -1
                mid_sample = (start_sample + end_sample) /2
                #print (""silence found %d, %d, mid %d"" %(start_sample / ms, end_sample / ms, mid_sample/ms))
                mids.append(mid_sample/ms)
            silence_count = 0

        else:
            silence_count = silence_count+ 1

    return mids

if __name__ == '__main__':
    ## Info & args
    parser = argparse.ArgumentParser(
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument(""reference"", type=Path,
                        help=""Path to a reference file"")
    parser.add_argument(""input"", type=Path,
                        help=""Path to an input file"")
    parser.add_argument(""output"", type=Path,
                        help=""Path to an output file"")
    parser.add_argument(""-e"", ""--enc_model_fpath"", type=Path,
                        default=""encoder/saved_models/pretrained.pt"",
                        help=""Path to a saved encoder"")
    parser.add_argument(""-s"", ""--syn_model_dir"", type=Path,
                        default=""synthesizer/saved_models/logs-pretrained/"",
                        help=""Directory containing the synthesizer model"")
    parser.add_argument(""-v"", ""--voc_model_fpath"", type=Path,
                        default=""vocoder/saved_models/pretrained/pretrained.pt"",
                        help=""Path to a saved vocoder"")
    parser.add_argument(""--low_mem"", action=""store_true"", help=\
        ""If True, the memory used by the synthesizer will be freed after each use. Adds large ""
        ""overhead but allows to save some GPU memory for lower-end GPUs."")
    parser.add_argument(""--no_sound"", action=""store_true"", help=\
        ""If True, audio won't be played."")
    parser.add_argument(
        '--cpu', help='Use CPU.', action='store_true')
    args = parser.parse_args()
    print_args(args, parser)
    if not args.no_sound:
        import sounddevice as sd


    ## Print some environment information (for debugging purposes)
    print(""Running a test of your configuration...\n"")
    if args.cpu:
        encoder.load_model(args.enc_model_fpath)
    elif torch.cuda.is_available():
        device_id = torch.cuda.current_device()
        gpu_properties = torch.cuda.get_device_properties(device_id)
        print(""Found %d GPUs available. Using GPU %d (%s) of compute capability %d.%d with ""
            ""%.1fGb total memory.\n"" %
            (torch.cuda.device_count(),
            device_id,
            gpu_properties.name,
            gpu_properties.major,
            gpu_properties.minor,
            gpu_properties.total_memory / 1e9))
    else:
        print(""Your PyTorch installation is not configured. If you have a GPU ready ""
              ""for deep learning, ensure that the drivers are properly installed, and that your ""
              ""CUDA version matches your PyTorch installation."", file=sys.stderr)
        quit(-1)

    ## Load the models one by one.
    print(""Preparing the encoder, the synthesizer and the vocoder..."")
    encoder.load_model(args.enc_model_fpath)
    synthesizer = Synthesizer(args.syn_model_dir.joinpath(""taco_pretrained""), low_mem=args.low_mem)
    vocoder.load_model(args.voc_model_fpath)


    ## Run a test
    #print(""Testing your configuration with small inputs."")
    # Forward an audio waveform of zeroes that lasts 1 second. Notice how we can get the encoder's
    # sampling rate, which may differ.
    # If you're unfamiliar with digital audio, know that it is encoded as an array of floats
    # (or sometimes integers, but mostly floats in this projects) ranging from -1 to 1.
    # The sampling rate is the number of values (samples) recorded per second, it is set to
    # 16000 for the encoder. Creating an array of length <sampling_rate> will always correspond
    # to an audio of 1 second.
    #print(""\tTesting the encoder..."")
    #encoder.embed_utterance(np.zeros(encoder.sampling_rate))

    # Create a dummy embedding. You would normally use the embedding that encoder.embed_utterance
    # returns, but here we're going to make one ourselves just for the sake of showing that it's
    # possible.
    #embed = np.random.rand(speaker_embedding_size)
    # Embeddings are L2-normalized (this isn't important here, but if you want to make your own
    # embeddings it will be).
    #embed /= np.linalg.norm(embed)
    # The synthesizer can handle multiple inputs with batching. Let's create another embedding to
    # illustrate that
    #embeds = [embed, np.zeros(speaker_embedding_size)]
    #texts = [""test 1"", ""test 2""]
    #print(""\tTesting the synthesizer... (loading the model will output a lot of text)"")
    #mels = synthesizer.synthesize_spectrograms(texts, embeds)

    # The vocoder synthesizes one waveform at a time, but it's more efficient for long ones. We
    # can concatenate the mel spectrograms to a single one.
    #mel = np.concatenate(mels, axis=1)
    # The vocoder can take a callback function to display the generation. More on that later. For
    # now we'll simply hide it like this:
    #no_action = lambda *args: None
    #print(""\tTesting the vocoder..."")
    # For the sake of making this test short, we'll pass a short target length. The target length
    # is the length of the wav segments that are processed in parallel. E.g. for audio sampled
    # at 16000 Hertz, a target length of 8000 means that the target audio will be cut in chunks of
    # 0.5 seconds which will all be generated together. The parameters here are absurdly short, and
    # that has a detrimental effect on the quality of the audio. The default parameters are
    # recommended in general.
    #vocoder.infer_waveform(mel, target=200, overlap=50, progress_callback=no_action)

    #print(""All test passed! You can now synthesize speech.\n\n"")


    ## Interactive speech generation

    try:
        # Get the reference audio filepath
        message = ""Reference voice: enter an audio filepath of a voice to be cloned (mp3, "" \
                  ""wav, m4a, flac, ...):\n""
        in_fpath = args.reference

        ## Computing the embedding
        # First, we load the wav using the function that the speaker encoder provides. This is
        # important: there is preprocessing that must be applied.

        # The following two methods are equivalent:
        # - Directly load from the filepath:
        preprocessed_wav = encoder.preprocess_wav(in_fpath)
        # - If the wav is already loaded:
        original_wav, sampling_rate = librosa.load(in_fpath)
        preprocessed_wav = encoder.preprocess_wav(original_wav, sampling_rate)
        print(""Loaded file succesfully"")

        # Then we derive the embedding. There are many functions and parameters that the
        # speaker encoder interfaces. These are mostly for in-depth research. You will typically
        # only use this function (with its default parameters):
        embed = encoder.embed_utterance(preprocessed_wav)
        print(""Created the embedding"")


        ## Generating the spectrogram
        f = open(args.input,""r"")
        text = f.read()
        f.close()
        print (text)

        # The synthesizer works in batch, so you need to put your data in a list or numpy array
        input_texts = text.splitlines()
        audio_np_full = None

        for line in input_texts:

            text = ""skip "" + line
            texts=text
            texts=[texts]

            embeds = [embed] * len(texts)
            # If you know what the attention layer alignments are, you can retrieve them here by
            # passing return_alignments=True
            specs = synthesizer.synthesize_spectrograms(texts, embeds)
            print(""Created the mel spectrogram"")

            spec=specs[0]

            ## Generating the waveform
            print(""Synthesizing the waveform:"")
            # Synthesizing the waveform is fairly straightforward. Remember that the longer the
            # spectrogram, the more time-efficient the vocoder.

            ## Post-generation
            # There's a bug with sounddevice that makes the audio cut one second earlier, so we
            # pad it.
            generated_wav = np.pad(vocoder.infer_waveform(spec, overlap=800, target=8000, normalize=True, batched=False), (0, synthesizer.sample_rate), mode=""constant"")
            #at this point we have floats in the range -1.0 to 1.0
            #sd.play(generated_wav, synthesizer.sample_rate)
            #sd.wait()
            #librosa.output.write_wav(""pretrim.wav"", generated_wav, synthesizer.sample_rate)

            mids = find_silence(generated_wav, synthesizer.sample_rate)
            print (""Silences:"")
            print (mids)

            skip_ms=400
            for mid in mids:
                if ((mid > 350) and (mid < 650)):
                    skip_ms = mid
                    break

            skip_pos = int(skip_ms * synthesizer.sample_rate / 1000)
            print (""Skip mid %d, pos %d"" %(skip_ms, skip_pos))
            if (audio_np_full is not None):
                audio_np_full = np.append(audio_np_full, generated_wav[skip_pos:])
            else:
                audio_np_full = generated_wav[skip_pos:]

        flt = librosa.util.buf_to_float(audio_np_full, n_bytes=2)
        librosa.output.write_wav(""output.wav"", audio_np_full.astype(np.float32), synthesizer.sample_rate)
        #sd.play(audio_np_full, synthesizer.sample_rate)
        #sd.wait()
        #if not args.no_sound:
    #        play(audio_segment_full)
        #audio_segment = AudioSegment.from_file(args.output, format=""wav"")
        #audio_segment.export(""output.mp3"", format=""mp3"")

        print (args.output)
        ffmpeg.input(""output.wav"").filter(""loudnorm"",I=-14, TP=-3, LRA=11).output(str(args.output)).overwrite_output().run()

        # ffmpeg -i test.mp3 -af loudnorm=I=-14:TP=-3:LRA=11:print_format=json -f null -

        print (""Done"")
        exit(0)

    except Exception as e:
        print(""Caught exception: %s"" % repr(e))
        print (e)
        traceback.print_exc()
        exit(1)


```",like project forward great think get chance rebase try copy code wrote add word start line find silence following back output copy run python one text optional hope import name please use instead import deprecation false import import import import synthesizer import inference import inference import path import import import import torch import import effect import read import import import play import array float rate rate rate threshold lower considered noise higher considered silence outer range average inner range outer average inner print threshold outer outer outer print silence found mid else return mids parser reference path reference file input path input file output path output file path saved directory synthesizer model path saved true memory used synthesizer freed use large overhead save memory true audio wo parser import print environment information print running test configuration print found available compute capability total else print installation ready deep learning ensure properly version installation quit load one one print synthesizer synthesizer synthesizer run test print testing configuration small forward audio second notice get sampling rate may differ unfamiliar digital audio know array sometimes mostly ranging sampling rate number per second set array length always correspond audio second print create dummy would normally use going make one sake showing possible embed important want make embed embed synthesizer handle multiple let create another illustrate embed test test print synthesizer loading model output lot text one time efficient long concatenate mel single one mel take function display generation later simply hide like lambda none print sake making test short pas short target length target length length parallel audio hertz target length target audio cut together absurdly short detrimental effect quality audio default general mel print test synthesize interactive speech generation try get reference audio message reference voice enter audio voice first load function speaker important must applied following two equivalent directly load already loaded print loaded file derive many speaker mostly research typically use function default embed print generating spectrogram open text print text synthesizer work batch need put data list array none line text skip line embed know attention layer retrieve passing spec print mel spectrogram generating print fairly straightforward remember longer spectrogram bug audio cut one second pad spec constant point range mids print print mids mid mids mid mid mid break print skip mid none else play print null print done exit except exception print caught exception print exit,issue,positive,positive,neutral,neutral,positive,positive
663577257,"### All questions about Colab Notebook

The Colab Notebook is a community-developed resource to enable users to run the toolbox without having a GPU or going through a complicated setup. Recently, these issues have been resolved with CPU support being added (#366) and the installation process streamlined (#375). **We recommend that you use a normal Python environment.**

Users who still prefer using Colab Notebook should understand that no official support will be provided, though you are welcome to ask questions to get help from the community. If you believe you've found a bug with the underlying toolbox code, please try to replicate the issue in a normal Python environment. We are not Colab Notebook users and are unable to troubleshoot Colab Notebook errors.",notebook notebook resource enable run toolbox without going complicated setup recently resolved support added installation process streamlined recommend use normal python environment still prefer notebook understand official support provided though welcome ask get help community believe found bug underlying toolbox code please try replicate issue normal python environment notebook unable notebook,issue,positive,positive,neutral,neutral,positive,positive
663368079,"@FulvioZozix If this did not solve your issue, please reopen it. Thanks @mbdash for answering the part about datasets.",solve issue please reopen thanks part,issue,positive,positive,positive,positive,positive,positive
663367744,"You need to load an audio file and/or synthesize at least 4 times until the projections will be shown.

Click the ""browse"" button to load a file. An embedding will be calculated. Once you have this you can synthesize your own speech.",need load audio file synthesize least time shown click browse button load file calculated synthesize speech,issue,negative,negative,negative,negative,negative,negative
663320527,"I can't promise anything since I have never used GitHub for anything else then cloning.
But I could try over the weekend if no one else did it by then.",ca promise anything since never used anything else could try weekend one else,issue,negative,neutral,neutral,neutral,neutral,neutral
663317242,"Thank you, 
I will look at it tomorrow morning I am only staying up for a few more minutes, I am a bit too tired to think straight right now..

Tonight I am trying to keep it simple and see if I can Jam a regular ""hand modeled"" 3D head mesh into VOCA (Voice Operated Character Animation) (another GitHub project)

Update: nope it exploded.",thank look tomorrow morning bit tired think straight right tonight trying keep simple see jam regular hand head mesh voice character animation another project update nope exploded,issue,positive,positive,neutral,neutral,positive,positive
663314828,"> Now I just need to dumb down all you wrote to be able to reproduce it.

@mbdash In the first post I included a dropbox link that has fairly detailed instructions for the single-speaker LibriSpeech example. You can try that and ask if you have any trouble reproducing the results. If you want VCTKp240 I can make a zip file for you tomorrow.

This was much easier and faster than expected. I am sharing the results to generate interest, so we can collaborate on how much training is needed, best values of hparams, etc.",need dumb wrote able reproduce first post included link fairly detailed example try ask trouble want make zip file tomorrow much easier faster generate interest collaborate much training best,issue,positive,positive,positive,positive,positive,positive
663314046,"Wow that is amazing... I only asked your opinion and you actually did it!

The difference is incredible.

Now I just need to dumb down all you wrote to be able to reproduce it.

Also try your_input_text.replace('hi', 'eye') it is a little cheat that I find gives better results currently.
At least in the multi speaker model.",wow amazing opinion actually difference incredible need dumb wrote able reproduce also try little cheat find better currently least speaker model,issue,positive,positive,positive,positive,positive,positive
663311738,"use arguments 
-d 
or 
--datasets_root

ie:
_windows:_
python demo_toolboc.py --datasets_root d:/RTVC/datasets/train-clean-360

_ubuntu:_
python demo_toolboc.py --datasets_root /home/myuser/RTVC/datasets/train-clean-360",use ie python python,issue,negative,neutral,neutral,neutral,neutral,neutral
663308789,"### Single-speaker finetuning using VCTK dataset: [samples_vctkp240.zip](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/4969802/samples_vctkp240.zip)

Here are some samples from the latest experiment. VCTK p240 is used to add 4.4k steps to the synthesizer, and 1.0k to the vocoder. Synthesized audios have filename `speaker_utterance_SYN_VOC.wav` and use all combinations of pretrained (""pre"") and finetuned (""fin"") models for the synthesizer and vocoder, respectively.

Synthesized utterances using speaker p240's hardcoded embedding (derived from p240_001_mic1.flac) show the success in finetuning to match the voice, including the accent. Samples made from speaker p260's embedding demonstrate how much quality is lost when finetuning a single-speaker model.

In these samples, the synthesizer has far more impact on quality, though this result could be due to insufficient finetuning of the vocoder. Though the finetuned vocoder has only a slight advantage over the original for p240, it severely degrades voice cloning quality for p260.

Also compare to the samples for p240 and p260 in the Google SV2TTS paper: https://google.github.io/tacotron/publications/speaker_adaptation/

### Replicating this experiment

Here is a **[preprocessed p240 dataset](https://www.dropbox.com/s/qskoopjcdjdwuvw/dataset_p240.zip?dl=0)** if you would like to repeat this experiment. The embeds for utterances 002-380 are overwritten with the one for 001, as the hardcoding makes for a more consistent result. Use the audio file `p240_001.flac` to generate embeddings for inference. The audios are not included to keep the file size down, so if you care to do vocoder training you will need to get and preprocess VCTK.

Directions:
1. Copy the folder `synthesizer/saved_models/logs-pretrained` to `logs-vctkp240` in the same location. This will make a copy of your pretrained model to be finetuned.
2. Unzip the dataset files to `datasets_p240` in your Real-Time-Voice-Cloning folder (or somewhere else if you desire)
3. Train the model: `python synthesizer_train.py vctkp240 dataset_p240/SV2TTS/synthesizer --checkpoint_interval 100`
4. Let it run for 200 to 400 iterations, then stop the program.
    * This should complete in a reasonable amount of time even on CPU.
    * You can safely stop and resume training at any time though you will lose all progress since the last checkpoint
5. Test the finetuned model in the toolbox using `dataset_p240/p240_001.flac` to generate the embedding",latest experiment used add synthesizer use fin synthesizer respectively speaker derived show success match voice accent made speaker demonstrate much quality lost model synthesizer far impact quality though result could due insufficient though slight advantage original severely voice quality also compare paper experiment would like repeat experiment one consistent result use audio file generate inference included keep file size care training need get copy folder location make copy model folder somewhere else desire train model python let run stop program complete reasonable amount time even safely stop resume training time though lose progress since last test model toolbox generate,issue,positive,positive,positive,positive,positive,positive
663230900,"Hello, @blue-fish  please how can I add another voice on your colab (I'd like to upload or link it to a 5 - 10 seconds wav sample)? thanks!

https://colab.research.google.com/drive/1akxtrLZHKuMiQup00tzO2olCaN-y3KiD


",hello please add another voice like link sample thanks,issue,positive,positive,positive,positive,positive,positive
663222964,"@ash1407 My fine-tuning in #437 is done using CPU only, and the models are converging quickly enough. Do not get a GPU unless you find it to be much too slow.",ash done converging quickly enough get unless find much slow,issue,negative,negative,negative,negative,negative,negative
663158963,"> 
> 
> @ash1407 Are you still trying? When you get to step 4 (synthesizer preprocessing on new dataset), pull the latest master. The #441 changes should make this step a lot easier.
> 
> If using AccentDB, will you finetune a single accent or just throw them all into the mix? It would be interesting to find out if this is enough voices to generalize well for cloning. Also see my latest reply in #437 , it is a promising result to see the synthesizer acquire the accent after a small number of steps (with the caveat that I finetuned with data from a single speaker).
> 
> <img alt=""screenshot"" width=""735"" src=""https://user-images.githubusercontent.com/67130644/88316414-aad98c00-cccc-11ea-8fb1-a289e7cdeb89.png"">

I was not having Nvidia GPU , any idea which Gpu i should purchase for Machine learning (i have budget of 4oooRS INR)",ash still trying get step synthesizer new pull latest master make step lot easier single accent throw mix would interesting find enough generalize well also see latest reply promising result see synthesizer acquire accent small number caveat data single speaker idea purchase machine learning budget,issue,positive,positive,positive,positive,positive,positive
663126216,"@ash1407 Are you still trying? When you get to step 4 (synthesizer preprocessing on new dataset), pull the latest master. The #441 changes should make this step a lot easier.

If using AccentDB, will you finetune a single accent or just throw them all into the mix? It would be interesting to find out if this is enough voices to generalize well for cloning. Also see my latest reply in #437 , it is a promising result to see the synthesizer acquire the accent after a small number of steps (with the caveat that I finetuned with data from a single speaker).

<img width=""735"" alt=""screenshot"" src=""https://user-images.githubusercontent.com/67130644/88316414-aad98c00-cccc-11ea-8fb1-a289e7cdeb89.png"">
",ash still trying get step synthesizer new pull latest master make step lot easier single accent throw mix would interesting find enough generalize well also see latest reply promising result see synthesizer acquire accent small number caveat data single speaker,issue,positive,positive,positive,positive,positive,positive
663076421,"@blue-fish, Would it be useful if I was to offer a GPU (2080 ti) for contributing on training a new model based on LibriTTS ?
I have yet to train any models and would gladly exchange GPU time for an opportunity to learn.
I wonder how long it would take on a single 2080 ti.",would useful offer ti training new model based yet train would gladly exchange time opportunity learn wonder long would take single ti,issue,positive,positive,positive,positive,positive,positive
663071998,"If you get this error message on mac when preprocessing:
`objc[67570]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.`

I resolved it by adding the following line to `~/.bash_profile` , then exit completely from the terminal and open a new one.
```
export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES
```

For more information: [https://stackoverflow.com/a/52230415](https://stackoverflow.com/a/52230415)",get error message mac initialize may progress another thread fork safely call ignore fork child process instead set resolved following line exit completely terminal open new one export information,issue,negative,positive,positive,positive,positive,positive
663050677,"Pretrained synthesizer + 200 steps of training on VCTK p240 samples (0.34 hours of speech). Still using the original vocoder model. This is just a few minutes of CPU time for fine-tuning. It is remarkable that the synthesizer is already imparting the accent on the result. This is good news for anyone who is fine-tuning an accent: it should not take too long, even for multispeaker.

I did notice a lot more gaps and sound artifacts than usual with the finetuned model (this result is cherry-picked). Is it because I did not hardcode all the samples to a single utterance embedding?

[samples_vctkp240_200steps.zip](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/4966743/samples_vctkp240_200steps.zip)
",synthesizer training speech still original model time remarkable synthesizer already accent result good news anyone accent take long even notice lot sound usual model result single utterance,issue,positive,positive,positive,positive,positive,positive
662847627,"https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/eaf5ec4467795344e7d9601515b017fd8c46e44b/synthesizer/preprocess.py#L60

Try making this modification:
```
with alignments_fpath.open(""r"", encoding=""ascii"") as alignments_file:
```",try making modification ascii,issue,negative,neutral,neutral,neutral,neutral,neutral
662815072,Can you try it with the `392_single_threaded_preprocess` branch of my fork and post the traceback? It will help to know which alignment file it is breaking on.,try branch fork post help know alignment file breaking,issue,negative,neutral,neutral,neutral,neutral,neutral
662196442,This might help resolve the umap issue: https://github.com/lmcinnes/umap/issues/24#issuecomment-346438635,might help resolve issue,issue,positive,neutral,neutral,neutral,neutral,neutral
662055731,Hi @Mazlak . Can you copy and paste the full traceback from your terminal?,hi copy paste full terminal,issue,negative,positive,positive,positive,positive,positive
661966640,"@plummet555, That would be great! 

This is my 1st time interacting with a GitHub community and you guys are awesome. I originally really just wanted to say thanks to @blue-fish and wasn't expecting this kind of exchange.

I will try to learn how to use GitHub properly to follow your example and also share back any changes I will make to the libraries I currently am experimenting with.",plummet would great st time community awesome originally really say thanks kind exchange try learn use properly follow example also share back make currently,issue,positive,positive,positive,positive,positive,positive
661951910,I'd be happy to share the change I made to add and then cut out a leading word. Just need to find some time to tidy up the repo.,happy share change made add cut leading word need find time tidy,issue,positive,positive,positive,positive,positive,positive
661299551,Thanks you so much! That's exactly what I am looking for :),thanks much exactly looking,issue,negative,positive,positive,positive,positive,positive
661297791,"
Re: Q1: 
I had a similar idea, but not as good as @plummet555, 
(pad the input and cut it out afterwards)
But cutting after the 1st silence looks like a better direction to take.

Re: Q2: 
Yes, an open-source version of the resemble.ai voice cloner was what I had in mind.

I will look into both your suggestions.
 thanks again.",similar idea good plummet pad input cut afterwards cutting st silence like better direction take yes version voice mind look thanks,issue,positive,positive,positive,positive,positive,positive
661296679,Also see here for more info on what it would take to properly improve the models to fix your Q1: **https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/364#issuecomment-653996443** . The problem has been reported previously in #53 and #227 .,also see would take properly improve fix problem previously,issue,negative,negative,neutral,neutral,negative,negative
661294839,"@annaskarzynska You seem to be describing voice style transfer, here are some examples of repositories that do this. It will take a lot of work to train a model to do what you are proposing.

* https://github.com/auspicious3000/autovc ([audio samples](https://auspicious3000.github.io/autovc-demo/))
* https://github.com/auspicious3000/SpeechSplit ([audio samples](https://auspicious3000.github.io/SpeechSplit-Demo/))
* https://github.com/mazzzystar/randomCNN-voice-transfer ([sample1](https://soundcloud.com/mazzzystar/sets/stairway2nightcall), [sample2](https://soundcloud.com/mazzzystar/sets/speech-conversion-sample))
* https://github.com/andabi/deep-voice-conversion ([audio samples](https://soundcloud.com/andabi/sets/voice-style-transfer-to-kate-winslet-with-deep-neural-networks))",seem voice style transfer take lot work train model audio audio sample sample audio,issue,negative,neutral,neutral,neutral,neutral,neutral
661288377,"@mbdash

### Q1
**Do you see a way in the future to reduce / tweak the minimum output audio length below the minimum 5 sec?**

It can be worked around by padding the input with extra words, and then post-processing to remove the padding. For example @plummet555 found this workaround (https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/360#issue-637180849):

> 4. I found that often there would be a harsh pop or other artifact at the start of the audio. I did a lot of experimenting with that. In the end, I added the word 'clip' to the start of every input sentence, then removed it from the output with silence detection (find the first gap in the output audio)

### Q2
> Would using a dataset purely generated by a single actor, result in a better audio output when reproducing solely that actor's voice?

Yes, the toolbox should perform much better on speakers that it is trained on.

> Do you have any guess of how big of a dataset would be required to reproduce the voice of a single voice actor?
1 to 1.
> Essentially removing the capacity to reproduce any other voices properly when using that specific model,
for the purpose of achieving better cloning accuracy for a single voice.

If I were attempting this, I would extract a single embedding for the desired speaker and then fine-tune the synthesizer and vocoder models using that hardcoded embedding, following this training process: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/429#issuecomment-660514897 (substituting your single-speaker dataset for the accent datasets).

The amount of data would depend on how well the existing models work on the target speaker.

It would be an interesting project to attempt an open-source version of the resemble.ai voice cloner, where we define a  set of utterances to be recorded and fine-tune a single-speaker model using the above process. I would guess 5-10 minutes of data should be sufficient for most voices.
",see way future reduce tweak minimum output audio length minimum sec worked around padding input extra remove padding example plummet found found often would harsh pop artifact start audio lot end added word start every input sentence removed output silence detection find first gap output audio would purely single actor result better audio output solely actor voice yes toolbox perform much better trained guess big would reproduce voice single voice actor essentially removing capacity reproduce properly specific model purpose better accuracy single voice would extract single desired speaker synthesizer following training process substituting accent amount data would depend well work target speaker would interesting project attempt version voice define set model process would guess data sufficient,issue,positive,positive,neutral,neutral,positive,positive
661277644,@mbdash Opened #433 to discuss your questions. Let's continue the conversation there.,discus let continue conversation,issue,negative,neutral,neutral,neutral,neutral,neutral
661274063,"I would have not dared asking for anything, but since you mentioned it...

If I may ask for your opinion on 2 questions I have been thinking about:
_(and I hope these are not stupid questions)_


**Q1**

**Do you see a way in the future to reduce / tweak the minimum output audio length below the minimum 5 sec?**

For example,
Something that would allow input text lengths as low as single words such as:

- Hi
- Hi _your-name-here_
- How are you
- I'm fine thank you
- yes
- no
- thank you

My understanding is that the minimum audio output length is around 5 sec.
I have experimented with 90, 70, 60, 50 and 40 characters of input text.
The minimum workable input seem to be 60-70 chars to fill that 5 sec of audio, 
below that, the audio output is just weird / creepy. 
The sweet spot seems to be a minimum of 80-90 characters to fill nicely the minimum 5 sec audio output.


**Q2** 
_this one is a weird one and might go against the design itself..._

**Would using a dataset purely generated by a single actor, result in a better audio output when reproducing solely that actor's voice?**

and if so,

Do you have any guess of how big of a dataset would be required to reproduce the voice of a single voice actor?
1 to 1.
Essentially removing the capacity to reproduce any other voices properly when using that specific model, 
for the purpose of achieving better cloning accuracy for a single voice.


ie:
a single voice actor reads 12h of transcript (or more)
then we can generate higher quality TTS for that single actor.



thank you for any feedback.",would anything since may ask opinion thinking hope stupid see way future reduce tweak minimum output audio length minimum sec example something would allow input text low single hi hi fine thank yes thank understanding minimum audio output length around sec experimented input text minimum workable input seem fill sec audio audio output weird creepy sweet spot minimum fill nicely minimum sec audio output one weird one might go design would purely single actor result better audio output solely actor voice guess big would reproduce voice single voice actor essentially removing capacity reproduce properly specific model purpose better accuracy single voice ie single voice actor transcript generate higher quality single actor thank feedback,issue,positive,negative,neutral,neutral,negative,negative
661271134,"Just pushed a fix for a small bug found during testing. Tacotron was incorrectly retaining the seed after the ""random seed"" checkbox transitioned from a checked to unchecked state. No further changes are expected.",fix small bug found testing incorrectly retaining seed random seed checked unchecked state,issue,negative,negative,negative,negative,negative,negative
661215925,"Thank you for the kind words @mbdash , it is nice knowing that others also find these improvements worthwhile. Feel free to provide feedback to help guide development, though as usual we find ourselves long on ideas and short on developers.",thank kind nice knowing also find feel free provide feedback help guide development though usual find long short,issue,positive,positive,positive,positive,positive,positive
661172261,Check out [AutoVC](https://github.com/auspicious3000/autovc) ([audio samples](https://auspicious3000.github.io/autovc-demo/)) as an example of voice style transfer. This seems to be what you are looking for.,check audio example voice style transfer looking,issue,negative,neutral,neutral,neutral,neutral,neutral
661171285,Check out [AutoVC](https://github.com/auspicious3000/autovc) ([audio samples](https://auspicious3000.github.io/autovc-demo/)) as another example of voice style transfer.,check audio another example voice style transfer,issue,negative,neutral,neutral,neutral,neutral,neutral
661074675,"+1 for repeatability feature. Proposed changes looks neat.

@blue-fish I just want to say thank you for your work, you are adding features and fixing bugs that are really helpful and appreciated.
thank you to @CorentinJ for also allowing blue to make / integrate all the updates.
(also thanks to the original author of the feature, if i am not mistaken I saw someone else made the initial code change suggestion and blue is pimping it out)",repeatability feature neat want say thank work fixing really helpful thank also blue make integrate also thanks original author feature mistaken saw someone else made initial code change suggestion blue pimping,issue,positive,positive,positive,positive,positive,positive
660940390,"This PR resolves #384, and introduces a workaround for the problem identified in #53.
User interface after the proposed changes. ""Random seed"" and ""Enhance vocoder output"" are new.

![screenshot](https://user-images.githubusercontent.com/67130644/87927135-96309600-ca37-11ea-8032-9a47d991fd83.png)

",problem user interface random seed enhance output new,issue,negative,negative,negative,negative,negative,negative
660934927,"I have performed additional experimentation to identify the minimum change needed for repeatability. Ready for review.

Some thoughts:
1. Tacotron's randomness is a feature that is useful for fixing the large gaps that it sometimes creates. I find it useful to control the synthesizer output by adjusting the seed.
2. It would be nice to get repeatable output without reloading the synthesizer and vocoder models on every use, but it works.",additional experimentation identify minimum change repeatability ready review randomness feature useful fixing large sometimes find useful control synthesizer output seed would nice get repeatable output without synthesizer every use work,issue,positive,positive,positive,positive,positive,positive
660883503,"> Ensuring that the dropout in the prenet is set to inference mode at inference time would work too, it's the only source of randomness in tacotron

Thanks for the info, I'll implement it and test it out. Much better than the brute force approach.

Edit: Will this suggestion make the synthesizer output unaffected by the state of the random number generator?

Because this tacotron is liable to produce gaps in the output (#53), I think it is preferable to keep the randomness, and allow for controlling it by setting the seed. When using the toolbox I repeatedly click ""synthesize only"" until a spectrogram with no large gaps appears. Vocoding is reserved for good spectrograms, especially since it is very slow with CPU inference.",dropout set inference mode inference time would work source randomness thanks implement test much better brute force approach edit suggestion make synthesizer output unaffected state random number generator liable produce output think preferable keep randomness allow setting seed toolbox repeatedly click synthesize spectrogram large reserved good especially since slow inference,issue,positive,positive,neutral,neutral,positive,positive
660881503,"Ensuring that the dropout in the prenet is set to inference mode at inference time would work too, it's the only source of randomness in tacotron",dropout set inference mode inference time would work source randomness,issue,negative,neutral,neutral,neutral,neutral,neutral
660726619,"Making some more improvements, will mark as ready when complete",making mark ready complete,issue,negative,positive,positive,positive,positive,positive
660683358,"I found a very low-tech fix for this, which is to always run ""trim_long_silences"" on the vocoder output. The function uses webrtcvad and is found in encoder/audio.py. Will submit a PR when I get a chance.",found fix always run output function found submit get chance,issue,negative,neutral,neutral,neutral,neutral,neutral
660674935,"With the changes on the [384_repeatable_voice_cloning](https://github.com/blue-fish/Real-Time-Voice-Cloning/tree/384_repeatable_voice_cloning) branch of my fork, I get fully repeatable voice cloning output using demo_toolbox.py .  Tested on a computer without a GPU.

### Test case
* Reference audio: https://google.github.io/tacotron/publications/speaker_adaptation/demos/groundtruth/p240_00000.wav
* Text: `Take a look at these pages for crooked creek drive.`
* Export vocoder output as .wav file

You should find that the exported .wav files are identical for subsequent synthesize and vocode attempts within a session, and even across new toolbox sessions.

### Required changes
1. Reload synthesizer and vocoder models every time they are used
2. Set `torch.manual_seed()` every time the vocoder is used
3. Set tensorflow seed every time synthesizer is used
    * The other RNG seeds are not relevant.
    * Also see https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/432#issuecomment-660881503

### Other changes
I reverted these changes and results are still repeatable on my platform. Listing here in case it helps troubleshooting in the future.

4. Force tensorflow to use a single-threaded session
5. Use kernel initializer for `tf.nn.rnn_cell.GRUCell()` and `tf.layers.dense()`
    * This might affect repeatability of training, but without these, inference is still repeatable.

### References
(2) https://pytorch.org/docs/stable/notes/randomness.html
(3, 4) https://stackoverflow.com/a/52897216
(5) https://stackoverflow.com/a/51558159",branch fork get fully repeatable voice output tested computer without test case reference audio text take look crooked creek export output file find identical subsequent synthesize within session even across new toolbox session reload synthesizer every time used set every time used set seed every time synthesizer used relevant also see still repeatable platform listing case future force use session use kernel might affect repeatability training without inference still repeatable,issue,negative,positive,positive,positive,positive,positive
660648189,"Here is the solution for NoBackendError: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/431#issuecomment-660647785

If you followed the instructions in README.md, ffmpeg should already be installed and all you need to do is edit the environment variables to add them to path.",solution already need edit environment add path,issue,negative,neutral,neutral,neutral,neutral,neutral
660647785,"### NoBackendError

#### Summary
Typically fixed by installing [ffmpeg](https://ffmpeg.org/download.html#get-packages). Windows users follow these instructions: https://video.stackexchange.com/a/20496

If you still experience NoBackendError after installing ffmpeg, try the below instructions and get support at [librosa](https://github.com/librosa/librosa/issues) if needed.

#### More information
This error message occurs when opening a mp3 file. Audioread (a dependency of librosa) needs additional software to open mp3 files. The following is taken from https://github.com/librosa/librosa#audioread and may be helpful:

> To fuel `audioread` with more audio-decoding power (e.g., for reading MP3 files),
you may need to install either *ffmpeg* or *GStreamer*.
>
> *Note that on some platforms, `audioread` needs at least one of the programs to work properly.*
>
> If you are using Anaconda, install *ffmpeg* by calling
> ```
> conda install -c conda-forge ffmpeg
> ```
>
> If you are not using Anaconda, here are some common commands for different operating systems:
>
> * Linux (apt-get): `apt-get install ffmpeg` or `apt-get install gstreamer1.0-plugins-base gstreamer1.0-plugins-ugly`
> * Linux (yum): `yum install ffmpeg` or `yum install gstreamer1.0-plugins-base gstreamer1.0-plugins-ugly`
> * Mac: `brew install ffmpeg` or `brew install gstreamer`
> * Windows: download binaries from this [website]( https://gstreamer.freedesktop.org/) 
>
> For GStreamer, you also need to install the Python bindings with 
> ```
> pip install pygobject
> ```",summary typically fixed follow still experience try get support information error message opening file dependency need additional open following taken may helpful fuel power reading may need install either note need least one work properly anaconda install calling install anaconda common different operating install install install install mac brew install brew install also need install python pip install,issue,negative,negative,neutral,neutral,negative,negative
660541704,"> This is not an easy undertaking so before you start, make sure you satisfy the prerequisites. You must be able to answer ""yes"" to all questions below:
> 
> * Does your computer have a NVIDIA GPU?
> * Do you have coding experience?
> * Are you willing to devote at least 20 hours to the task?
> 
> I have not gone through the process myself, but I'll try to outline it since we don't have a good explanation. What you need to do is to fine-tune the pretrained synthesizer and vocoder models on a suitable dataset.
> 
> 1. Find a suitable dataset. Freely available resources include [AccentDB](https://accentdb.org/) (Indian accent) and [VCTK](https://datashare.is.ed.ac.uk/handle/10283/3443) (other English accents). For best results on your own voice, record your own dataset though this will take many hours.
> 2. Follow the steps in [README.md](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/README.md) to enable GPU support.
> 3. Go to the [training wiki page](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training) and follow the steps for the synthesizer and vocoder training on the LibriSpeech dataset.
>    
>    * Review the preprocessing code and understand what it is doing.
>    * Understand the format of the files in the <datasets_root>/SV2TTS folder
> 4. Preprocess your dataset from step 1 to generate training data for the synthesizer.
>    
>    * At a minimum, this requires editing the preprocessing scripts.
>    * You will likely need to write your own code to process the data into a suitable format for the toolbox.
>    * **We do not have a tutorial for this. You are on your own here!**
> 5. Continue training the pretrained synthesizer model on your dataset until it has converged.
> 6. Using your new synthesizer model, preprocess your dataset to generate training data for the vocoder.
> 7. Continue training the pretrained vocoder model on your dataset until the output is satisfactory.
> 
> With luck, your trained models will now generalize to your voice and impart the desired accent. **There are no guarantees this will work.**
> 
> If you succeed, please share your models and I will add them to the list in #400.

I will give a try. Thanks for the guidance friend. ",easy undertaking start make sure satisfy must able answer yes computer experience willing devote least task gone process try outline since good explanation need synthesizer suitable find suitable freely available include accent best voice record though take many follow enable support go training page follow synthesizer training review code understand understand format folder step generate training data synthesizer minimum likely need write code process data suitable format toolbox tutorial continue training synthesizer model new synthesizer model generate training data continue training model output satisfactory luck trained generalize voice impart desired accent work succeed please share add list give try thanks guidance friend,issue,positive,positive,positive,positive,positive,positive
660530673,Closing this issue due to inactivity. Please reopen the issue if you have more to discuss.,issue due inactivity please reopen issue discus,issue,negative,negative,negative,negative,negative,negative
660529513,"> First, thx for your reply. Do you know GST(global style token)?
> When i read [gst](https://arxiv.org/abs/1803.09017) paper, i found it contains not only the token but also the tone of the speaker. In other word, can we separate prosody from the ref audio as much as possible, or can we synthesis speaker-A's tone with speaker-B's prosody? If this is possible, it will be more interesting!

@niu0717 Let's continue the discussion in #230 . You might find the links interesting if you haven't read them already.",first reply know global style token read paper found token also tone speaker word separate prosody ref audio much possible synthesis tone prosody possible interesting let continue discussion might find link interesting read already,issue,positive,positive,positive,positive,positive,positive
660528100,The tacotron2 used here is based on Rayhane-mamah's implementation which does not support GST.,used based implementation support,issue,negative,neutral,neutral,neutral,neutral,neutral
660526531,Closing this issue due to inactivity. Please ask your question in #364 if this is still an active concern.,issue due inactivity please ask question still active concern,issue,positive,negative,negative,negative,negative,negative
660525325,Closing this issue due to inactivity. Anyone willing to take this on is welcome to reopen the issue.,issue due inactivity anyone willing take welcome reopen issue,issue,negative,positive,positive,positive,positive,positive
660524541,Closing this issue due to inactivity. @phiresky please open a new issue if this is still an active concern.,issue due inactivity please open new issue still active concern,issue,positive,negative,neutral,neutral,negative,negative
660524342,Closing this due to inactivity. As a reference point I achieved 0.45 steps/sec on a GTX1650 after correcting for batch size and sampling rate differences.,due inactivity reference point correcting batch size sampling rate,issue,negative,negative,negative,negative,negative,negative
660523937,"Closing this issue due to inactivity. @dipjyoti92 if you would like an answer to your additional questions, try asking in #364 .",issue due inactivity would like answer additional try,issue,negative,negative,negative,negative,negative,negative
660523318,@shawwn Did you get ARPAbet support working with the toolbox?,get support working toolbox,issue,negative,neutral,neutral,neutral,neutral,neutral
660523079,Closing this issue due to inactivity. Please file a new issue if this is still an active concern.,issue due inactivity please file new issue still active concern,issue,positive,negative,neutral,neutral,negative,negative
660522174,"@strich I don't think it would be an easy task to set it up, but one possibility is to use something like the [Montreal Forced Aligner](https://montreal-forced-aligner.readthedocs.io/en/latest/introduction.html) using your source utterance and transcript as inputs. This would provide alignments for the speech allowing the problematic speech segment to be removed.

Then feed the source utterance into the RVTC toolbox to generate an embedding and replacement speech segment. Splice the replacement in to create the new utterance.

Another possibility is to use a commercial product like the one linked here: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/182#issuecomment-548422235

I am going to close this issue due to inactivity, but reopen if you are still interested in doing this.",strich think would easy task set one possibility use something like forced aligner source utterance transcript would provide speech problematic speech segment removed feed source utterance toolbox generate replacement speech segment splice replacement create new utterance another possibility use commercial product like one linked going close issue due inactivity reopen still interested,issue,positive,positive,neutral,neutral,positive,positive
660520101,Closing this issue due to inactivity. I apologize you did not get an answer to your question @zoldaten . Please reopen the issue if it is still an active concern.,issue due inactivity apologize get answer question please reopen issue still active concern,issue,positive,negative,negative,negative,negative,negative
660519967,"@ViktorAlm Can you share your code with me? If needed, in a private repository so I can figure out exactly what changes are needed for your Swedish pretrained models to work? ",share code private repository figure exactly work,issue,negative,positive,positive,positive,positive,positive
660519213,This is beyond the scope for what we can provide support. You will need to figure out how to integrate the toolbox with your application. The demo_cli.py script is designed to help with that. Good luck with your project!,beyond scope provide support need figure integrate toolbox application script designed help good luck project,issue,positive,positive,positive,positive,positive,positive
660518133,"Closing this issue due to inactivity. Please reopen if such a service has been created, or if someone is interested in taking this on.

I really doubt this is feasible as a purely client-side web application since it mostly uses pytorch. Not sure if [torch-js](https://github.com/torch-js/torch-js) can help there.",issue due inactivity please reopen service someone interested taking really doubt feasible purely web application since mostly sure help,issue,positive,positive,positive,positive,positive,positive
660517313,Closing this issue due to inactivity. @SijinJohn please reopen if you have a specific question about the code.,issue due inactivity please reopen specific question code,issue,negative,negative,neutral,neutral,negative,negative
660516518,Closed due to inactivity. @brcisna please reopen the issue if you have more to discuss.,closed due inactivity please reopen issue discus,issue,negative,negative,negative,negative,negative,negative
660515416,"See here for information on how to fine-tune the model to a particular accent: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/429#issuecomment-660514897

If you do not wish to perform training, periodically check #400 and see if anyone has uploaded a model with the desired accent.",see information model particular accent wish perform training periodically check see anyone model desired accent,issue,positive,positive,positive,positive,positive,positive
660514897,"This is not an easy undertaking so before you start, make sure you satisfy the prerequisites. You must be able to answer ""yes"" to all questions below:
* Does your computer have a NVIDIA GPU?
* Do you have coding experience?
* Are you willing to devote at least 20 hours to the task?

I have not gone through the process myself, but I'll try to outline it since we don't have a good explanation. What you need to do is to fine-tune the pretrained synthesizer and vocoder models on a suitable dataset.

1. Find a suitable dataset. Freely available resources include [AccentDB](https://accentdb.org/) (Indian accent) and [VCTK](https://datashare.is.ed.ac.uk/handle/10283/3443) (other English accents). For best results on your own voice, record your own dataset though this will take many hours.
2. Follow the steps in [README.md](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/README.md) to enable GPU support.
3. Go to the [training wiki page](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training) and follow the steps for the synthesizer and vocoder training on the LibriSpeech dataset.
    * Review the preprocessing code and understand what it is doing.
    * Understand the format of the files in the <datasets_root>/SV2TTS folder
4. Preprocess your dataset from step 1 to generate training data for the synthesizer.
    * At a minimum, this requires editing the preprocessing scripts.
    * You will likely need to write your own code to process the data into a suitable format for the toolbox.
    * **We do not have a tutorial for this. You are on your own here!**
5. Continue training the pretrained synthesizer model on your dataset until it has converged.
6. Using your new synthesizer model, preprocess your dataset to generate training data for the vocoder.
7. Continue training the pretrained vocoder model on your dataset until the output is satisfactory.

With luck, your trained models will now generalize to your voice and impart the desired accent. **There are no guarantees this will work.**

If you succeed, please share your models and I will add them to the list in #400.",easy undertaking start make sure satisfy must able answer yes computer experience willing devote least task gone process try outline since good explanation need synthesizer suitable find suitable freely available include accent best voice record though take many follow enable support go training page follow synthesizer training review code understand understand format folder step generate training data synthesizer minimum likely need write code process data suitable format toolbox tutorial continue training synthesizer model new synthesizer model generate training data continue training model output satisfactory luck trained generalize voice impart desired accent work succeed please share add list,issue,positive,positive,positive,positive,positive,positive
660304339,"@DRob81 I would like to continue the tensorflow2 effort, can you please commit your changes or submit a pull request to my fork?",would like continue effort please commit submit pull request fork,issue,positive,neutral,neutral,neutral,neutral,neutral
660280882,"a) It's a matter of what you want. If you want to reach VCTK quality, then LibriTTS samples vastly outnumbering VCTK samples is going to cancel that out due to sampling being uniform. In a classical multispeaker model with a speaker table (i.e. an embedding layer), it would still make sense to have a 10 to 1 ratio if your goal was only to encode a voice for these speakers in the speaker table;

b) I can't elaborate too much, no. Just know that some of the data is of poor quality, and some is great. A bit of data exploration should give you an idea.",matter want want reach quality vastly going cancel due sampling uniform classical model speaker table layer would still make sense ratio goal encode voice speaker table ca elaborate much know data poor quality great bit data exploration give idea,issue,negative,positive,positive,positive,positive,positive
660242818,"There is not enough info to determine a root cause of the DLL error. For some possibilities see https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156 . This is a tensorflow installation issue and you should request assistance at [https://github.com/tensorflow/tensorflow/issues](https://github.com/tensorflow/tensorflow/issues) . Mention that you require version 1.15 specifically.

> I cannot stress enough that I have no idea what I'm doing.

You are not the target audience for this project, it is a research code and not a finished product. It happens to be very user-friendly as far as research codes go, but may cause frustration if you do not have any relevant computer experience. I commend you for trying, but you may want to wait for the compiled version (#267) which is in work.",enough determine root cause error see installation issue request assistance mention require version specifically stress enough idea target audience project research code finished product far research go may cause frustration relevant computer experience commend trying may want wait version work,issue,negative,positive,positive,positive,positive,positive
660230282,"Thank you so much for your answer. Two follow-up questions:
a) Why would dataset balance be an issue? Assume i have 10 times more samples from LibriTTS than from VCTK - if the input format, sampling rate and preprocessing is the same, why should this imbalance matter? (provided the clips are of somewhat same quality w.r.t to noise). Same for for SP.
b) You mentioned manually curating LibriTTS. Could you elaborate what you did in a bit more detail? Are there any papers, tools, etc. you can point me to? Did you listen to all audiofiles? (I cannot imagine this)

Again, than you so much for your answers. At my university (Munich, Germany), nobody is doing speech synthesis - i'm a bit on my own here.",thank much answer two would balance issue assume time input format sampling rate imbalance matter provided clip somewhat quality noise manually could elaborate bit detail point listen imagine much university nobody speech synthesis bit,issue,negative,positive,positive,positive,positive,positive
660075042,"a) Yes I would. For having manually curated LibriTTS myself, I can definitely say that a lot of speakers are very noisy. Do a little data exploration to convince yourself of that: pick 100 random samples and listen to all of them. There are still many issues with this improved version of LibriSpeech: inconsistent volume, background noise, poor mic quality, mic bumps, ... Regarding denoising alone, here's a sample from LibriTTS and its denoised version:
https://puu.sh/G839T.wav
https://puu.sh/G839Y.wav

b) Yes you can, some gotchas:
- Ensure that your preprocessed data is sampled to the same sample rate
- Ensure you normalize volume
- Beware of balance: compare the size of LibriTTS vs that of VCTK and compare the number of speakers. You might need to prune away some data from LibriTTS

c) I don't know if it's worth the effort. The voice encoder is a nice example of ""throw more resources at it and it'll keep improving"", if you merge your datasets (although again, balance might be an issue given the size of voxceleb) and train for long enough it should perform well anyway.
",yes would manually definitely say lot noisy little data exploration convince pick random listen still many version inconsistent volume background noise poor quality regarding alone sample version yes ensure data sample rate ensure normalize volume beware balance compare size compare number might need prune away data know worth effort voice nice example throw keep improving merge although balance might issue given size train long enough perform well anyway,issue,positive,positive,neutral,neutral,positive,positive
660069631,"Dear @CorentinJ , thank you for your amazing work and you continued support here. I have a few questions:
a) Would you still apply denoising to LibriTTS? I find that the samples are high quality, and the data itself has already been cleaned.
b) Can i train on both LibriTTS and VCTK? If so, what should i look out for?
c) When training speaker encoder (SE), i find that there is a difference in the difficulty of the datasets: VCTK, LibriTTS, Mozilla Commonvoice are 'easy' for the SE, and it achieves low loss and low EER quickly. However, VoxCeleb{1,2} are much harder.
-> Should i train on each data set separately, and once the model has 'trained out' on the easier datasets, skip them in favor of more iterations on voxceleb?


",dear thank amazing work continued support would still apply find high quality data already train look training speaker se find difference difficulty se low loss low eer quickly however much harder train data set separately model easier skip favor,issue,positive,positive,positive,positive,positive,positive
659830260,"Thanks for response, It's little bit complex for single user prediction using above link. I have tried in this way: 
Please guide :) Thanks in advance

```
from resemblyzer import VoiceEncoder, preprocess_wav
from pathlib import Path
import numpy as np

fpath = Path(""lokesh.wav"")
wav = preprocess_wav(fpath)
list=[]
list.append(wav)
encoder = VoiceEncoder()
#embed = encoder.embed_utterance(wav)
abc=encoder.embed_speaker(list)
print(abc)
print(abc.argmax(0))
y_prob=np.mean(abc,axis=0).flatten()
print(y_prob)
```

till now trying to understand your code.",thanks response little bit complex single user prediction link tried way please guide thanks advance import import path import path embed list print print print till trying understand code,issue,positive,negative,neutral,neutral,negative,negative
659522460,"@HumanG33k I have already performed automatic conversion using that process on the `370_tf2_compat` branch of my fork. There are still a bunch of errors that need to be worked through. I have published fixes for some of these, and getting stuck on some others where @DRob81 is also helping. The current errors are runtime so we may be getting close.

If you can run `demo_cli.py` without errors, please commit those changes to your fork and we can continue developing from there. If not, let's concentrate the effort on my tensorflow2 fork. I am accepting pull requests.",already automatic conversion process branch fork still bunch need worked getting stuck also helping current may getting close run without please commit fork continue let concentrate effort fork pull,issue,negative,neutral,neutral,neutral,neutral,neutral
659508085,"hi,
keep in mind,
- I too tired to read all things now but tensorflow 1.15 just not work with python 3.8 (at this day) throught pip. 
- I just don’t have cuda and never will i prefer the [opencl](https://en.wikipedia.org/wiki/OpenCL)

I think it can be good for the record to keep one brach tensorflow 1.X and go for a new master based on tensorflow 2. 
I just check and there is an ""automatic code updater"" provide by tsf. I just execute it. I provide the report. In short what y do :
- do the standard failing install of the repo.
- check my tensorflow version with 
```python
import tensorflow as tf

print(tf.__version__)
```
2.2.0 in my case (debian testing)
Use the following script in the parent directory of the project directory
```bash
#!/bin/bash
tf_upgrade_v2 \
  --intree Real-Time-Voice-Cloning/ \
  --outtree Real-Time-Voice-Cloning_v2/ \
  --reportfile report.txt
```
and will have a bunch of ouput and a 
[report.txt](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/4928730/report.txt)

I just not look at the output or content in report.txt

But for check go in v2 directory, edit the requirement.txt and set  ```tensorflow==2.2.0 ```

retry the pip install to be sure : 
```bash
pip install -r requirements.txt
```
Everythings look satisfied.
No have the time to check more. Feel free to ping me and try to continue that if i have time this week end.

Oh and i just follow tensorflow documentation a least some part 
https://www.tensorflow.org/guide/migrate
https://www.tensorflow.org/guide/upgrade
",hi keep mind tired read work python day pip never prefer think good record keep one brach go new master based check automatic code provide execute provide report short standard failing install check version python import print case testing use following script parent directory project directory bash bunch look output content check go directory edit set retry pip install sure bash pip install look satisfied time check feel free ping try continue time week end oh follow documentation least part,issue,negative,positive,positive,positive,positive,positive
659472067,@ash1407 Good to know that this is resolved. Thanks for posting the solution that worked for you.,ash good know resolved thanks posting solution worked,issue,positive,positive,positive,positive,positive,positive
659449419,"> 
> 
> You need to update your pip. After that it will be able to find the latest torch.
> 
> ```
> pip install pip --upgrade
> ```

Thanks friend it also helped me .1)upgraded it  2)than i used this  pip3 install https://download.pytorch.org/whl/cpu/torch-1.0.1-cp37-cp37m-win_amd64.whl  . than it worked ",need update pip able find latest torch pip install pip upgrade thanks friend also used pip install worked,issue,positive,positive,positive,positive,positive,positive
659356608,Would you please let me know which code to be removed? And should I add TextGrids in the code?,would please let know code removed add code,issue,negative,neutral,neutral,neutral,neutral,neutral
659350627,"You don't need alignments, it's only something I did for LibriSpeech but it's overengineering for little results. Remove the code that splits sentences based on alignments and you'll be fine.

As a matter of fact we should be removing alignments repo-wide.",need something little remove code based fine matter fact removing,issue,negative,positive,positive,positive,positive,positive
659254911,I have a custom dataset which contain a bunch of wav files with there text transcriptions. I require alignment.txt for those respective wav files. Is there a way where we can create those .alignment.txt for a custom dataset?,custom contain bunch text require respective way create custom,issue,positive,neutral,neutral,neutral,neutral,neutral
659039383,"You need to update your pip. After that it will be able to find the latest torch.

```
pip install pip --upgrade
```",need update pip able find latest torch pip install pip upgrade,issue,negative,positive,positive,positive,positive,positive
659027250,@ash1407 Thanks for confirming that the root cause is a Python version issue.,ash thanks confirming root cause python version issue,issue,negative,positive,positive,positive,positive,positive
659011824,"> 
> 
> Please make sure you are using a supported version of Python (README.md specifies this as 3.6 or 3.7) and try the Tensorflow 1.15 installation again. As noted in #401 , Python 3.8+ only supports Tensorflow 2.x which is not compatible with the toolbox.

Thanks friend problem got solved .from your advice.",please make sure version python try installation noted python compatible toolbox thanks friend problem got advice,issue,positive,positive,positive,positive,positive,positive
658927669,"Please make sure you are using a supported version of Python (README.md specifies this as 3.6 or 3.7) and try the Tensorflow 1.15 installation again. As noted in #401 , Python 3.8+ only supports Tensorflow 2.x which is not compatible with the toolbox.",please make sure version python try installation noted python compatible toolbox,issue,positive,positive,positive,positive,positive,positive
658902348,"
![image](https://user-images.githubusercontent.com/68329823/87576938-bb4b9e80-c6ef-11ea-95c0-2245ba4994c7.png)
i tried virtual env and in there i tried to install Tensorflow 1.15 but i failed ",image tried virtual tried install,issue,negative,neutral,neutral,neutral,neutral,neutral
658846148,"The current version of the code requires Tensorflow 1.15. If you are having trouble installing it, try setting up a Python virtual environment.",current version code trouble try setting python virtual environment,issue,negative,negative,neutral,neutral,negative,negative
657970140,"@Kreijstal that defeats the purpose, TTS already as a model built in, RTVC is for making the model.",purpose already model built making model,issue,negative,neutral,neutral,neutral,neutral,neutral
657382603,"> Thanks for sharing all that @sberryman . Is there any reason we cannot generate and publish a set of alignments for LibriTTS (just like the [LibriSpeech alignments](https://github.com/CorentinJ/librispeech-alignments#download-links))? This seems like a good idea, to keep the training setup as easy and repeatable as it currently is.
> 
> I'm also surprised that the [sample rate in synthesizer/hparams.py](https://github.com/sberryman/Real-Time-Voice-Cloning/blob/d6ba3e1ec0f950636e9cac3656c0be5c331821cc/synthesizer/hparams.py#L117) is not updated to match the 24k sampling rate of LibriTTS. Can you confirm if an hparams update is needed for synthesizer training on LibriTTS?

I found that using alignments to split long text samples is a waste of time. Simply discard samples that are too long.",thanks reason generate publish set like like good idea keep training setup easy repeatable currently also sample rate match sampling rate confirm update synthesizer training found split long text waste time simply discard long,issue,positive,positive,positive,positive,positive,positive
657382179,"Yeah and the LTT video is using models dating from january, our sound quality has way improved since",yeah video dating sound quality way since,issue,negative,positive,positive,positive,positive,positive
657352038,"Not sure if it is any different, not a hassle at all to share it though.

My gut says that any retraining or even fine-tuning of the encoder requires re-training of all downstream modules. I haven't tried to train another encoder model and compare the embeddings though, so at this point it is just a guess.

I would also really like to see improvement in this field but also keep in mind that the pace of research is incredible. Given the number of people who starred this repo on github it is obviously a very interesting topic to a lot of people. Maybe a quick survey would be beneficial to see what people are looking for? Quite a few people could be using Corentin's work in further research  and not interested in contributing code. Maybe most are looking to implement virtual voices in their own projects? If we knew the use case for most of the interested developers we could try and build a foundation for further research a development?

For clarity, my purpose of re-training was to focus on the encoder. I wanted to train embeddings on voices similar to facial recognition while taking advantage of GE2E loss. I have a personal dataset of over 7,000 hours of local and national broadcast news in the USA from about 300 stations. That video was recorded during a single week from 2019-05-27 through 2019-06-02. I had already run some of the most popular ML networks (YOLOv3, MaskRCNN, Face detection and embeddings) on all of the keyframes from the video. My next task was to identify the voices and determine when the face and voice embeddings overlapped. Then I could easily tell if the person was on-camera and speaking at the same time. As of right now (visual only) it is very easy to see the most common clusters of faces across a station and nationally.

## Local
![image](https://user-images.githubusercontent.com/324437/87270967-9f918f80-c486-11ea-8f60-b069e84c0c73.png)

## National
![image](https://user-images.githubusercontent.com/324437/87271010-c8b22000-c486-11ea-9668-76515091775d.png)


 It would be interesting to hear if you have a project in mind, I would assume everyone here does.

Edit:
Here is the link to the TextGrid files: https://www.dropbox.com/s/xov6qyc6e33tf7n/libritts.textgrid.zip?dl=0",sure different hassle share though gut even downstream tried train another model compare though point guess would also really like see improvement field also keep mind pace research incredible given number people starred obviously interesting topic lot people maybe quick survey would beneficial see people looking quite people could work research interested code maybe looking implement virtual knew use case interested could try build foundation research development clarity purpose focus train similar facial recognition taking advantage gee loss personal local national broadcast news video single week already run popular face detection video next task identify determine face voice could easily tell person speaking time right visual easy see common across station nationally local image national image would interesting hear project mind would assume everyone edit link,issue,positive,positive,positive,positive,positive,positive
657340682,"@sberryman Thank you for the offer, but would your raw textgrid differ from the set that's publicly available via LibriTTSLabel? I don't want to waste your time, so let's assume it's not necessary unless their files give me trouble. 

Your discussions in #126 were really helpful for understanding the training process. I get what you're saying about downstream models needing to be retrained when an upstream element is changed. From a naive point of view, if the new encoder also has an embedding size of 256, and the loss function makes it resemble the original encoder (taking both to be a black box), it seems like we could use the existing synthesizer and vocoder models as a starting point for training. My knowledge is seriously lacking and I need to put the computer down and pick up a textbook before going any further. But it does seem like an opportunity to save time and electricity.

My goal with this project is incremental improvement with a focus on usability. I would also like better quality output, but I'm not suitable to contribute anything except enthusiasm and low-level grunt work. LibriTTS support is one step in that direction. I don't expect to get there alone, just clear an obstacle or two so someone else can pick it up and take it the rest of the way.",thank offer would raw differ set publicly available via want waste time let assume necessary unless give trouble really helpful understanding training process get saying downstream needing upstream element naive point view new also size loss function resemble original taking black box like could use synthesizer starting point training knowledge seriously need put computer pick textbook going seem like opportunity save time electricity goal project incremental improvement focus usability would also like better quality output suitable contribute anything except enthusiasm grunt work support one step direction expect get alone clear obstacle two someone else pick take rest way,issue,positive,positive,neutral,neutral,positive,positive
657327192,"> At Resemble.AI we also have better results by using a new vocoder that my colleague @fatchord developed. I believe he's about to publish the paper he wrote about it.

Absolutely astounding what you're all doing at Resemble, as well. Saw the LTT videos done in cooperation with you lot as well; was very happy to see some publicity in front of the average tech nerd.",also better new colleague believe publish paper wrote absolutely astounding resemble well saw done lot well happy see publicity front average tech,issue,positive,positive,positive,positive,positive,positive
657323380,"@blue-fish I found the TextGrid files if you want them.

You can't adjust anything upstream without retraining downstream. So the encoder is at the top of the stream (upstream) followed by the synthesizer and finally the vocoder. If you make a change at the encoder level you need to retrain the synthesizer and vocoder. If you change the synthesizer you need to retrain the vocoder.

I'm also not an expert in any of this. I'm a programmer who happened to drop by and test a few things out along the way. All I can say is that I'm happy to share my experiences but nobody should rely on anything I say. I'm probably wrong.

What is your goal with this project?
1. Reproduce Corentin's results?
1. Make it more accessible?
1. Train on new languages or datasets?

I understand you have commit access to the repository but I don't understand your motivation or goal.
",found want ca adjust anything upstream without downstream top stream upstream synthesizer finally make change level need retrain synthesizer change synthesizer need retrain also expert programmer drop test along way say happy share nobody rely anything say probably wrong goal project reproduce make accessible train new understand commit access repository understand motivation goal,issue,positive,positive,positive,positive,positive,positive
657306932,"#182 is a possible solution for this issue. With GPU acceleration it is already faster than real-time which seems like a reasonable level of performance already.

I am going to close this issue due to inactivity. It can be reopened if someone wants to work on it.",possible solution issue acceleration already faster like reasonable level performance already going close issue due inactivity someone work,issue,positive,positive,neutral,neutral,positive,positive
657306111,Closing this as a duplicate of #53 (also see #411). Thank you @macriluke for the helpful information.,duplicate also see thank helpful information,issue,positive,neutral,neutral,neutral,neutral,neutral
657293874,"If I make that change all tests still pass for me, using both the original encoder and yours. I have no way to reproduce the error that you are seeing. Are there any other changes that `git status` or `git diff` reports?

To ensure that further troubleshooting is productive, I suggest following the steps here: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/383#issuecomment-657284710 , starting from a clean clone of the latest code, add your encoder model, and verify everything works before continuing.

It's very possible that training is breaking something but I want to first rule out any other issues with your setup that are interfering with operation of the toolbox.",make change still pas original way reproduce error seeing git status git ensure productive suggest following starting clean clone latest code add model verify everything work possible training breaking something want first rule setup interfering operation toolbox,issue,positive,positive,positive,positive,positive,positive
657290617,"Hi @blue-fish: actually I have no models produced for the Synthesizer yet - I just finished the pre-processing step for the synthesizer. The only place I have changed was: 
 /synthesizer/models/modules.py"", line 114 self._cell = tf.contrib.rnn.LSTMBlockCell(num_units, name=name)",hi actually produced synthesizer yet finished step synthesizer place line,issue,negative,neutral,neutral,neutral,neutral,neutral
657289115,Can you move the incomplete synthesizer model out of the saved model folder entirely so it doesn't see it? Which files in your repo are modified?,move incomplete synthesizer model saved model folder entirely see,issue,negative,neutral,neutral,neutral,neutral,neutral
657287386,"> My issue is once I have trained a model from scratch, the demo apps no longer work.

If #382 is still an active issue, I think what happened is that your synthesizer model is not valid and it is having a problem loading it. You can pass the ""--syn_model_dir"" argument to demo_cli.py to point to the good pretrained model from Corentin. Try that and see if you still get the message.",issue trained model scratch longer work still active issue think synthesizer model valid problem loading pas argument point good model try see still get message,issue,negative,positive,positive,positive,positive,positive
657287069,"This path has always worked for me My issue is once I have trained a model from scratch, the demo apps no longer work. And, I got this error. ValueError: Variable Tacotron_model/inference/inputs_embedding already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? ",path always worked issue trained model scratch longer work got error variable already mean set,issue,negative,negative,negative,negative,negative,negative
657284710,"@goodmangu Please do the following:
1. Clone a fresh copy of the repo (pick up all the recent changes)
2. Get a clean copy of the [pretrained models](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models) and place in the correct location
3. Run demo_cli.py and confirm no errors
4. Copy your encoder `my_run.pt` to `encoder/saved_models/pretrained.pt` (overwrite it)
5. Run demo_cli.py and see if that works for you.

After completing these 5 steps we can continue debugging the issue with your config.",please following clone fresh copy pick recent get clean copy place correct location run confirm copy overwrite run see work continue issue,issue,positive,positive,positive,positive,positive,positive
657253765,"We can reduce artifacts in the vocoder with additional training (https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/126#issuecomment-650549809) . However, it does not make a perceptible difference in the cloned voice. This result also suggests that to the extent the vocoder has an impact on the output quality, we are reaching the limits of what is possible with WaveRNN.",reduce additional training however make perceptible difference voice result also extent impact output quality reaching possible,issue,negative,neutral,neutral,neutral,neutral,neutral
657252453,"@sberryman Taking a closer look at [LibriTTSLabel](https://github.com/kan-bayashi/LibriTTSLabel), it has the raw textgrid output from MFA, so that will save a lot of time. I will use your script to generate the alignment files after downloading the datasets to get the transcript files.

> If someone is going to take on the full stack training they should start from scratch and adjust the hparams in the encoder to have 768 hidden units and an embedding size of 256. I had started training before Corentin replied back to keep the embedding size at 256.

If we keep the embedding size at 256, while increasing the hidden units from 256 to 768, would the resulting encoder be compatible with the existing synthesizer and vocoder models?
1. If not, what changes need to be made to the synthesizer and vocoder hparams?
2. If yes, can we continue training @CorentinJ 's pretrained synthesizer and vocoder models, or would those have to be trained from scratch?

Looking at [encoder/model.py](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/encoder/model.py) the answer seems to be yes in a strict sense. It depends on how well the new encoder's embedding matches the original encoder's when torch.nn.Linear reduces the size from 768 to 256. But I would think the loss function used in encoder training should cause it to converge in a similar direction.

> I'm currently playing around with [talking heads](https://arxiv.org/pdf/1905.08233.pdf) / [code](https://github.com/grey-eye/talking-heads). I've also come across [Lip2Wav](https://github.com/Rudrabha/Lip2Wav) which is a really interesting project that uses this repository as a base.

Those are very interesting, thanks for sharing the links.",taking closer look raw output save lot time use script generate alignment get transcript someone going take full stack training start scratch adjust hidden size training back keep size keep size increasing hidden would resulting compatible synthesizer need made synthesizer yes continue training synthesizer would trained scratch looking answer yes strict sense well new original size would think loss function used training cause converge similar direction currently around talking code also come across really interesting project repository base interesting thanks link,issue,positive,positive,neutral,neutral,positive,positive
657245988,"> Sometimes the results ends up with slight what i would call 'wind in the microphone' muffling effect at either start or finish of generated audio.

@brcisna Can you try this vocoder model and let me know whether you still get ""wind in the microphone"" effect?  https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/126#issuecomment-650531957

Because the synthesizer is not deterministic you will need a few attempts to conclude if there is a difference. If it still occurs with the new vocoder it is likely an artifact of the synthesizer.

> I personally use this in kind of an odd manner,in that I use it to clone voices to be used in historic videos as a narrator type scenario Do old auto racing history so my routine is.

If you plan on distributing your work please be mindful of the legal implications of using someone else's voice and make sure you have secured rights if necessary.",sometimes slight would call microphone effect either start finish audio try model let know whether still get wind microphone effect synthesizer deterministic need conclude difference still new likely artifact synthesizer personally use kind odd manner use clone used historic narrator type scenario old auto racing history routine plan work please mindful legal someone else voice make sure necessary,issue,positive,positive,positive,positive,positive,positive
657183332,"You're welcome @OuterEnd07 , thank you for following up to let us know that it fixed the problem. ",welcome thank following let u know fixed problem,issue,negative,positive,positive,positive,positive,positive
657182894,"Thank you so much, it worked like a charm.
",thank much worked like charm,issue,positive,positive,positive,positive,positive,positive
657180528,"Try this, or the solution below it: [https://stackoverflow.com/a/60015194](https://stackoverflow.com/a/60015194)

Some have also reported success with:
```
pip uninstall h5py
conda install h5py
```

The issue is certainly not with the code in the toolbox, so our ability to support will be limited.",try solution also success pip install issue certainly code toolbox ability support limited,issue,positive,positive,positive,positive,positive,positive
657163803,"This worked~ not sure why sounddevice has moments of weird dastardliness at times. My friend was also experiencing this issue, and an update to his nvidia hd driver actually fixed it for him. My guess is it reset sounddevice somehow as well. All in all, thanks for the help and let's keep on going!!",sure weird dastardliness time friend also issue update driver actually fixed guess reset somehow well thanks help let keep going,issue,positive,positive,neutral,neutral,positive,positive
657154844,"Doing a quick search on the error message, it looks like a problem with sounddevice (or portaudio)... if you start python and type `import sounddevice`, does that command give the same error message?

One possible resolution (found [here](https://stackoverflow.com/questions/62412684)):
1. Download a sounddevice .whl file here: https://www.lfd.uci.edu/~gohlke/pythonlibs/#sounddevice
    * Likely `sounddevice‑0.3.15‑cp37‑cp37m‑win_amd64.whl` based on your python and OS.
2. `pip uninstall sounddevice`
3. `pip install sounddevice-0.3.15-cp37-cp37m-win_amd64.whl`

If that does not solve your issue, check this out: https://github.com/spatialaudio/python-sounddevice/issues/7 ... I didn't read all of it but it seems like uninstalling and reinstalling sounddevice is the resolution. You could ask there if you continue to have issues with sounddevice.",quick search error message like problem start python type import command give error message one possible resolution found file likely based python o pip pip install solve issue check read like resolution could ask continue,issue,negative,positive,positive,positive,positive,positive
657146331,"1. How far do you get starting up the toolbox?
2. Can you post the error message and full traceback?",far get starting toolbox post error message full,issue,negative,positive,positive,positive,positive,positive
656968237,"We just updated the installation instructions, thanks for confirming that installing ffmpeg solved the issue. Have fun with the toolbox!",installation thanks confirming issue fun toolbox,issue,positive,positive,positive,positive,positive,positive
656961471,"This solved it for me! Thank you for an amazing response @blue-fish!! 
Forgot to install ffmpeg. Once I did so, and linked my system path variable to the location of ffmpeg, it worked perfectly!",thank amazing response forgot install linked system path variable location worked perfectly,issue,positive,positive,positive,positive,positive,positive
656958559,"Please make sure you have followed the installation instructions. You need to install ffmpeg which is a backend for audioread (which is used to load mp3 files).

Install ffmpeg and let us know if that solved the problem for you. A link is in README.md",please make sure installation need install used load install let u know problem link,issue,negative,positive,positive,positive,positive,positive
656955457,"Here is the error:
Traceback (most recent call last):
  File ""D:\downloads\Real-Time-Voice-Cloning\toolbox\__init__.py"", line 84, in <lambda>
    func = lambda: self.load_from_browser(self.ui.browse_file())
  File ""D:\downloads\Real-Time-Voice-Cloning\toolbox\__init__.py"", line 145, in load_from_browser
    wav = Synthesizer.load_preprocess_wav(fpath)
  File ""D:\downloads\Real-Time-Voice-Cloning\synthesizer\inference.py"", line 111, in load_preprocess_wav
    wav = librosa.load(str(fpath), hparams.sample_rate)[0]
  File ""C:\Users\paul1\AppData\Local\Programs\Python\Python37\lib\site-packages\librosa\core\audio.py"", line 162, in load
    y, sr_native = __audioread_load(path, offset, duration, dtype)
  File ""C:\Users\paul1\AppData\Local\Programs\Python\Python37\lib\site-packages\librosa\core\audio.py"", line 186, in __audioread_load
    with audioread.audio_open(path) as input_file:
  File ""C:\Users\paul1\AppData\Local\Programs\Python\Python37\lib\site-packages\audioread\__init__.py"", line 116, in audio_open
    raise NoBackendError()
audioread.exceptions.NoBackendError
Traceback (most recent call last):
  File ""C:\Users\paul1\AppData\Local\Programs\Python\Python37\lib\site-packages\librosa\core\audio.py"", line 129, in load
    with sf.SoundFile(path) as sf_desc:
",error recent call last file line lambda lambda file line file line file line load path offset duration file line path file line raise recent call last file line load path,issue,negative,neutral,neutral,neutral,neutral,neutral
656872108,"@pritxm You will need to use another dataset to continue training of the pretrained models, to fine-tune the vocoder output to have the desired accent. The dataset needs to have the accent which you wish to impart on the text-to-speech output.

We do not have any guides on how to do this and it is outside the scope for what we can provide support for. But if you succeed in doing this please follow up and let us know! Other people have requested this, in #318 and #388 .",need use another continue training output desired accent need accent wish impart output outside scope provide support succeed please follow let u know people,issue,positive,neutral,neutral,neutral,neutral,neutral
656807038,"I think I've deleted my copy of the alignments but I don't see why you wouldn't be able to include those to speed up training. I would suggest documenting and leaving scripts in the repo to generate alignments though. That will make it much easier for people to use new/other datasets for training. I wasted a lot of time trying to figure out how to install the dependencies, generate the TextGrid files and converting them into the proper format for training. 

I don't think I updated `synthesizer/hparams.py` on my branch. Like I said, code is a mess and I have 10+ derivatives of it scattered around.

If someone is going to take on the full stack training they should start from scratch and adjust the hparams in the encoder to have 768 hidden units and an embedding size of 256. I had started training before Corentin replied back to keep the embedding size at 256. They should also be prepared to wait, a LONG LONG time to train the 3 models. Or ideally have access to better GPU's than my 1080 Ti's. The more memory you have on the GPU for training the encoder the faster it will converge (if it ever does?). I know Corentin has tried batch sizes significantly higher than I was able to hit with two 1080 TI's which trained much more quickly. If you read though my issue here and on the Resemblizer project you'll see that Corentin suggested using a different vocoder. There are quite a few opensource vocoders out there that should be fairly easy to implement. You just need to concat the speaker embedding to make them multi-speaker. 

It would also be nice to know the impact of training with multiple languages. I didn't have time to test the effect of an unbalanced dataset (>70% of the audio was English) but I did notice the embeddings from my model did a slightly better job when testing against unseen English audio. My assumption is this was due to more training steps, not multiple languages. 

I'm currently playing around with [talking heads](https://arxiv.org/pdf/1905.08233.pdf) / [code](https://github.com/grey-eye/talking-heads). I've also come across [Lip2Wav](https://github.com/Rudrabha/Lip2Wav) which is a really interesting project that uses this repository as a base.",think copy see would able include speed training would suggest leaving generate though make much easier people use training wasted lot time trying figure install generate converting proper format training think branch like said code mess scattered around someone going take full stack training start scratch adjust hidden size training back keep size also prepared wait long long time train ideally access better ti memory training faster converge ever know tried batch size significantly higher able hit two ti trained much quickly read though issue project see different quite fairly easy implement need speaker make would also nice know impact training multiple time test effect unbalanced audio notice model slightly better job testing unseen audio assumption due training multiple currently around talking code also come across really interesting project repository base,issue,positive,positive,positive,positive,positive,positive
656799377,"Thanks for sharing all that @sberryman . Is there any reason we cannot generate and publish a set of alignments for LibriTTS (just like the [LibriSpeech alignments](https://github.com/CorentinJ/librispeech-alignments#download-links))? This seems like a good idea, to keep the training setup as easy and repeatable as it currently is.

I'm also surprised that the [sample rate in synthesizer/hparams.py](https://github.com/sberryman/Real-Time-Voice-Cloning/blob/d6ba3e1ec0f950636e9cac3656c0be5c331821cc/synthesizer/hparams.py#L117) is not updated to match the 24k sampling rate of LibriTTS. Can you confirm if an hparams update is needed for synthesizer training on LibriTTS?
",thanks reason generate publish set like like good idea keep training setup easy repeatable currently also sample rate match sampling rate confirm update synthesizer training,issue,positive,positive,positive,positive,positive,positive
656755828,"@genewitch I am going to close this as a duplicate of issues #53 (for short text inputs causing gaps) and #347 (long text inputs causing fast or garbled output). As you have discovered, the model is optimized for outputs about 5-6 seconds in length and degrades when the inputs are deviate too far from this.

I find it is easier to work around this in the toolbox where you can paste a long text and manually add line breaks to break it into 5-6 second chunks.",going close duplicate short text causing long text causing fast output discovered model length deviate far find easier work around toolbox paste long text manually add line break second,issue,negative,positive,neutral,neutral,positive,positive
656749936,"Several problems are mentioned in this issue:
* `RuntimeError: Error opening 'filename.mp3': File contains data in an unknown format.` (resolved by #414 to instruct users to install ffmpeg to avoid underlying `Audioread: NoBackendError`)
* Need to convert Path objects to str (resolved by #371)
* `Caught exception: PortAudioError('Error querying device -1',)` when audio playback is attempted in demo_cli.py (resolved by #417)

They have all been resolved in the latest code, so we can finally close this out. Thanks to all here who reported an issue or shared workarounds.",several issue error opening file data unknown resolved instruct install avoid underlying need convert path resolved caught exception querying device audio playback resolved resolved latest code finally close thanks issue,issue,negative,positive,positive,positive,positive,positive
656749806,"@blue-fish It is good to see someone continue working on this project!

I've made an enourmous amount of changes to the project but a few of the most important changes I've included in a forked branch which you can find at https://github.com/sberryman/Real-Time-Voice-Cloning/tree/wip

For the encoder you'll see preprocessing methods for libritts, voxceleb, vctk, common voice, timit, nasjonal and tedlium datasets.
https://github.com/sberryman/Real-Time-Voice-Cloning/blob/wip/encoder/preprocess.py

For the synthesizer I've only implemented preprocessing for libritts.
https://github.com/sberryman/Real-Time-Voice-Cloning/blob/wip/synthesizer/preprocess.py

Generating alignments for LibriTTS I used montreal forced aligner. I created a dockerfile specifically for alignment which you can find in my fork:
https://github.com/sberryman/Real-Time-Voice-Cloning/blob/wip/Dockerfile.align

Alignments:
```shell
bin/mfa_align \
  /datasets/CommonVoice/en/speakers \
  /datasets/slr60/english.dict \
  /opt/Montreal-Forced-Aligner/dist/montreal-forced-aligner/pretrained_models/english.zip \
  /output/montreal-aligned/cv-en/
```
That is an example of generating alignments for CommonVoice but changing the path to slr60/LibriTTS should be very simple.
Once you generate the TextGrid files using MFA, you need to convert them to alignment files in the format required for training. You'll find that script in the repository as well:
https://github.com/sberryman/Real-Time-Voice-Cloning/blob/wip/scripts/textgrid_to_alignments.py

I spent months training and trying all sorts of variants on the encoder so my code and project is a complete mess. I'm happy to help answer any questions you have though.
",good see someone continue working project made amount project important included forked branch find see common voice synthesizer generating used forced aligner specifically alignment find fork shell example generating path simple generate need convert alignment format training find script repository well spent training trying code project complete mess happy help answer though,issue,positive,positive,positive,positive,positive,positive
656553798,At Resemble.AI we also have better results by using a new vocoder that my colleague @fatchord developed. I believe he's about to publish the paper he wrote about it.,also better new colleague believe publish paper wrote,issue,negative,positive,positive,positive,positive,positive
656551839,"Don't forget this too:

> * Use LibriTTS instead of LibriSpeech in order to have punctuation.
> * LibriTTS needs to be curated of speakers with bad prosody.
> * You can lower the upper bound I put on utterance duration, which I suspect has for effect of removing long utterances that are more likely to have more pauses (I formally evaluated models trained this way to generate less frequent long pauses). It also trains faster and does not have drawbacks (with a good attention paradigm, the model can generate longer sentences than seen in training).
> * The attention paradigm needs to be replaced, forward attention is poor.
> * If the attention paradigm holds prosody-specific parameters, it may be complemented with a speaker embedding mechanism

https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/364#issuecomment-653996443
",forget use instead order punctuation need bad prosody lower upper bound put utterance duration suspect effect removing long likely formally trained way generate le frequent long also faster good attention paradigm model generate longer seen training attention paradigm need forward attention poor attention paradigm may speaker mechanism,issue,negative,negative,neutral,neutral,negative,negative
656504359,"Hi @bllurr , I am closing this issue because it has not been updated in a while. I hope you were able to get the toolbox to work. It would be nice to know the current status. Feel free to reopen this issue if you have any questions or comments.",hi issue hope able get toolbox work would nice know current status feel free reopen issue,issue,positive,positive,positive,positive,positive,positive
656503320,"Soundfile does not support mp3 (which is a popular input type judging from issues #116, #198, #214, #289, #385, #395, #407) so I think it is best to continue supporting librosa.load() which requires ffmpeg.

The new recommendation is to update README.md to list ffmpeg as a requirement.",support popular input type think best continue supporting new recommendation update list requirement,issue,positive,positive,positive,positive,positive,positive
656352577,I'm going to deprecate this issue in favor of #413 . @sagar-spkt perhaps someone will give the answers you need over there.,going deprecate issue favor perhaps someone give need,issue,negative,neutral,neutral,neutral,neutral,neutral
656352027,"Naturally we will also want to train the vocoder on LibriTTS, but one step at a time.",naturally also want train one step time,issue,negative,positive,neutral,neutral,positive,positive
656299284,"
> 
> 
> Now that the toolbox has CPU support, we should consider a release in compiled form. Would someone from the community like to work on this?

I'll happily give this a go.",toolbox support consider release form would someone community like work happily give go,issue,positive,positive,positive,positive,positive,positive
656291181,"Now that the toolbox has CPU support, we should consider a release in compiled form. Would someone from the community like to work on this?",toolbox support consider release form would someone community like work,issue,positive,neutral,neutral,neutral,neutral,neutral
656289769,Also asking the community for help on this one. It should be a very easy fix and a good opportunity to learn how the toolbox code works.,also community help one easy fix good opportunity learn toolbox code work,issue,positive,positive,positive,positive,positive,positive
656288914,"Can someone from the community look into this? First, to see if soundfile can be a complete replacement for `librosa.load()` without the need to install a backend like ffmpeg. If that is the case, then submit a pull request replacing `librosa.load()` with the corresponding soundfile call in demo_cli.py and encoder/audio.py . Maybe some other places.",someone community look first see complete replacement without need install like case submit pull request corresponding call maybe,issue,negative,positive,positive,positive,positive,positive
656285494,"I'm unable to get past this part, before or after the last fix: https://github.com/blue-fish/Real-Time-Voice-Cloning/blob/621f62f150f5d0995ce61930479bec9e9043aebe/synthesizer/models/tacotron.py#L212-L218

It might be easier if you fork my repo using these instructions (https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/401#issuecomment-653929209), that would make it easier to share code updates and discuss issues like these.",unable get past part last fix might easier fork would make easier share code discus like,issue,positive,negative,negative,negative,negative,negative
656270153,@blue-fish i fixed that already. Can you tell me which line exactly? I think my line numbers differ from yours now.,fixed already tell line exactly think line differ,issue,negative,positive,positive,positive,positive,positive
656242755,If this solves your problem can you please go ahead and close this issue?,problem please go ahead close issue,issue,negative,neutral,neutral,neutral,neutral,neutral
656231227,"Thanks for sharing that @DRob81 . If I make that change then the next error message is `TypeError: can only concatenate list (not ""int"") to list` at line 218 of synthesizer/models/tacotron.py . It's hard to tell if it's getting further than before, since that is the same line where it errored out before. It is very hard to debug errors with the custom decoder.",thanks make change next error message concatenate list list line hard tell getting since line hard custom,issue,negative,negative,neutral,neutral,negative,negative
656223585,"Hi @sbkim052 , thanks for reporting this. You need to download a model from here: https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models

We should make the error message direct the user to the wiki page to download the models.",hi thanks need model make error message direct user page,issue,negative,positive,positive,positive,positive,positive
656066685,"@blue-fish i also debugged and ended up with batch_size. Glad i found that 
batch_size = tf.TensorShape(0).with_rank_at_least(1)[0]
is the solution here. Still more problems to fix",also ended glad found solution still fix,issue,positive,positive,positive,positive,positive,positive
656038506,"@DRob81 Although I solved it once before, it is eluding me this time. I thought I added with_rank() somewhere in synthesizer/models/helpers.py or custom_decoder.py, but it is not working. Thank you for your guess, I inspected `batch_size` in the debugger for tensorflow v1 and v2 and could not find any difference. I still think it is somewhere in the custom decoder, based on the error message.",although time thought added somewhere working thank guess could find difference still think somewhere custom based error message,issue,negative,neutral,neutral,neutral,neutral,neutral
656036626,"I would recommend reading #41 and the first few posts of #126 for some context. The main points as I understand them:

* SV2TTS authors trained speaker encoder to 50M steps, this one is just over 1M.
* SV2TTS authors used embedding size of 768, this one uses 256.
* SV2TTS authors used a larger, proprietary dataset for encoder training which gets better results.

> The speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with median duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not transcribed, but contains anonymized speaker identities. It is never used to train synthesis networks.

![image](https://user-images.githubusercontent.com/67130644/87026713-f77f7c00-c190-11ea-80c3-7ad481fef3ad.png)
",would recommend reading first context main understand trained speaker one used size one used proprietary training better speaker trained proprietary voice search corpus median duration united speaker never used train synthesis image,issue,positive,positive,positive,positive,positive,positive
655925682,"> This model requires only a short speech sample to clone a voice, while the style transfer requires a large dataset for the target voice. In other words, the advantage of our 3-stage SV2TTS process is that it generalizes well to voices unseen during training. Also, it can be used to synthesize new speech from any text, while style transfer requires a speech sample as input.

First, thx for your reply. Do you know GST(global style token)?
When i read [gst](https://arxiv.org/abs/1803.09017) paper, i found it contains not only the token but also the tone of the speaker. In other word, can we separate prosody from the ref audio as much as possible,   or can we synthesis speaker-A's tone with speaker-B's prosody? If this is possible, it will be more interesting!",model short speech sample clone voice style transfer large target voice advantage process well unseen training also used synthesize new speech text style transfer speech sample input first reply know global style token read paper found token also tone speaker word separate prosody ref audio much possible synthesis tone prosody possible interesting,issue,positive,positive,positive,positive,positive,positive
655865582,Closing this issue now that #402 is merged. Please pull the latest code for the toolbox enhancement that lets you replay and save wavs generated in the toolbox.,issue please pull latest code toolbox enhancement replay save toolbox,issue,positive,positive,positive,positive,positive,positive
655865149,"My apologies @matheusfillipe , I got ""collaborator"" and ""contributor"" mixed up and thought that you would have write access as a contributor. I'll merge it in now.",got collaborator contributor mixed thought would write access contributor merge,issue,negative,neutral,neutral,neutral,neutral,neutral
655860165,"> Hi @matheusfillipe , please go ahead and perform the squash and merge.

I don't have write access @blue-fish .",hi please go ahead perform squash merge write access,issue,negative,neutral,neutral,neutral,neutral,neutral
655859258,"Hi @matheusfillipe , please go ahead and perform the squash and merge.",hi please go ahead perform squash merge,issue,negative,neutral,neutral,neutral,neutral,neutral
655852778,"This model requires only a short speech sample to clone a voice, while the style transfer requires a large dataset for the target voice. In other words, the advantage of our 3-stage SV2TTS process is that it generalizes well to voices unseen during training. Also, it can be used to synthesize new speech from any text, while style transfer requires a speech sample as input.",model short speech sample clone voice style transfer large target voice advantage process well unseen training also used synthesize new speech text style transfer speech sample input,issue,negative,positive,positive,positive,positive,positive
655848386,"> Yes, there is another approach of using style transfer to impart characteristics of a reference audio to another audio. Here is an example of such a repo: https://github.com/mazzzystar/randomCNN-voice-transfer
> 
> It might involve fewer models but I am not convinced that it requires less training overall.

so what is the benefit of this complex model, or why not use another approach to do it ??",yes another approach style transfer impart reference audio another audio example might involve convinced le training overall benefit complex model use another approach,issue,positive,negative,negative,negative,negative,negative
655732955,"After reviewing code from @plummet555 , I realize that we should also set `random.seed()` for the python built-in RNG and `np.random.seed()` for the numpy RNG. I will try this later and see if repeatability improves.",code plummet realize also set python try later see repeatability,issue,negative,neutral,neutral,neutral,neutral,neutral
655732292,"Thank you @plummet555 ! I found the changes you were describing. I'll add a note to #384 .

If you didn't notice my comment about updated vocoder model above, you can try plugging that in (no change to hparams needed) and see if the audio quality gets better. I've noticed fewer artifacts but no difference in voice. https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/126#issuecomment-650531957",thank plummet found add note notice comment model try plugging change see audio quality better difference voice,issue,positive,positive,positive,positive,positive,positive
655689300,"> I figured out a solution to the above issue, to use tf.TensorShape().with_rank() to increase the rank as needed. Now working through a different set of errors.
> 
> Edit: Although it makes that one error go away, I do not know if it is the correct fix so I have not committed it.

Can you tell how you fixed 'Shape must be rank 1 but is rank 0' error?
I guess it is 'batch_size' in line 98 tacotron.py",figured solution issue use increase rank working different set edit although one error go away know correct fix tell fixed must rank rank error guess line,issue,negative,negative,negative,negative,negative,negative
655686237,"Hi @blue-fish - sorry I forgot to reply earlier.

I've just shared my repo with you. I didn't make it public as it is a bit messy but hopefully it will help you.

Look at the changes to tacotron2 in the 'silence detection and seeding' commit. ",hi sorry forgot reply make public bit messy hopefully help look detection commit,issue,positive,negative,negative,negative,negative,negative
655675983,Thanks for reporting this issue @unassumingbadger . The training scripts do not support the --low-mem flag. You can try reducing your batch size or training with CPU instead (pull the latest code).,thanks issue training support flag try reducing batch size training instead pull latest code,issue,positive,positive,positive,positive,positive,positive
655673847,"As mentioned in #404, I believe the embedding only depends on the last file loaded. In other words, the toolbox has no memory and does not learn as it is used. If you have multiple recordings you should experiment to find the one that produces the best results.

For best results on a single voice, you could fine-tune the synthesizer and vocoder model by training on your target voice. However that requires the preparation of a dataset. We are not able to walk you through the process of making a custom dataset, but I would suggest making something that looks like the LibriSpeech set as described here: https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training

If anyone has done this and wants to contribute a tutorial on making a custom dataset, please reopen this issue. We can push the content to a wiki page for better visibility.",believe last file loaded toolbox memory learn used multiple experiment find one best best single voice could synthesizer model training target voice however preparation able walk process making custom would suggest making something like set anyone done contribute tutorial making custom please reopen issue push content page better visibility,issue,positive,positive,positive,positive,positive,positive
655670258,"The wiki is supposed to do this but it is still a TODO: https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/How-it-all-works

You have a better chance at getting a response if you submit a specific question. The scope of what you are asking is too broad.",supposed still better chance getting response submit specific question scope broad,issue,positive,positive,positive,positive,positive,positive
655668687,"Closing this issue due to inactivity, feel free to reopen. I would appreciate an answer on how to set the tacotron2 hardcoded seeds for better repeatability.",issue due inactivity feel free reopen would appreciate answer set better repeatability,issue,positive,positive,positive,positive,positive,positive
655622275,"You are also to be commended on your determination to get it working. Thanks for uncovering several pain points that need to be improved:

* For ease of installation, possibly suggest users to install in a virtual environment with torch+cpu, and have an separate guide for GPU support?
* Need to disable ""Load"" button in toolbox GUI when launched without <datasets_root> path
* Should ask users to install ffmpeg as a prerequisite to avoid audioread ""no backend"" issue

I am going to close this issue now that it is resolved. Best of luck on your project!",also determination get working thanks several pain need ease installation possibly suggest install virtual environment separate guide support need disable load button toolbox without path ask install prerequisite avoid issue going close issue resolved best luck project,issue,positive,positive,positive,positive,positive,positive
655617120,"We have an issue for this in #370. Would like to either upgrade the synthesizer to tensorflow2, or switch to a pytorch-based synthesizer. (The synthesizer is the only part that uses TF.) It would be appreciated if you would contribute your help to add tensorflow2 support.

I am going to close this issue, if you have anything further to say on the issue please do so in #370.",issue would like either upgrade synthesizer switch synthesizer synthesizer part would would contribute help add support going close issue anything say issue please,issue,positive,neutral,neutral,neutral,neutral,neutral
655449517,Dude it worked! Thank you so mcuh for being so patient and helping me! Cheers man! Awesome guy! :D ,dude worked thank patient helping man awesome guy,issue,positive,positive,positive,positive,positive,positive
655438022,"I am pretty sure the problem is that audioread has no backend. If you look at your terminal it should have the traceback. You can solve that by installing ffmpeg on windows. It is the same issue as #386 or librosa/librosa#219.

If you don't want to install ffmpeg, you should be able to load .wav files without ffmpeg since no conversion is needed.",pretty sure problem look terminal solve issue want install able load without since conversion,issue,positive,positive,positive,positive,positive,positive
655436451,"Nothing happens when I select an mp3 or when I load it
I get a blank exception when I browse and select a file
When I try to load I get this 
Exception : expected str, bytes or os.PathLike object, not NoneType when I load one",nothing select load get blank exception browse select file try load get exception object load one,issue,negative,neutral,neutral,neutral,neutral,neutral
655434887,"We need to make this toolbox GUI friendlier. Click the ""Browse"" button and select your mp3 file.",need make toolbox click browse button select file,issue,negative,neutral,neutral,neutral,neutral,neutral
655431464,"> Could you tell me how do I deactivate the virtual environment?

Type `deactivate` in your shell.

> Exception : expected str, bytes or os.PathLike object, not NoneType

Click the ""Browse"" button to load a file. The ""Load"" button only works if you specified <datasets_root> when launching the toolbox. We should disable that button when <datasets_root> is not specified.",could tell deactivate virtual environment type deactivate shell exception object click browse button load file load button work toolbox disable button,issue,negative,neutral,neutral,neutral,neutral,neutral
655430543,"Actually I haven't tried it yet. Could you tell me how do I deactivate the virtual environment? 

> I take it that GPU still doesn't work?



Also I can't load any audio files on it. I can record but cannot load any of the mp3 files. 
I'm getting this  : 
Exception :  expected str, bytes or os.PathLike object, not NoneType

Would you happen to  know why is this happening?",actually tried yet could tell deactivate virtual environment take still work also ca load audio record load getting exception object would happen know happening,issue,negative,neutral,neutral,neutral,neutral,neutral
655424284,"I take it that GPU still doesn't work?

> Could you please tell me how to set my default environment as a virtual environment?

I don't know how to automatically launch a virtual environment (but I am sure guides exist). If you only ever use python to use this toolbox, you should try uninstalling the torch 1.2.0 from your default environment and install the working torch. The virtualenv is only needed if you run other programs with conflicting requirements which make a common environment impossible.

> And after setting up the virtual environment can I just type python demo_toolbox.py and play it?

Yes, after you have activated a virtual environment with the correct requirements, the toolbox should run.",take still work could please tell set default environment virtual environment know automatically launch virtual environment sure exist ever use python use toolbox try torch default environment install working torch run conflicting make common environment impossible setting virtual environment type python play yes virtual environment correct toolbox run,issue,positive,negative,negative,negative,negative,negative
655420375,"Could you please tell me how to set my default environment as a virtual environment?
And after setting up the virtual environment can I just type python demo_toolbox.py and play it?
",could please tell set default environment virtual environment setting virtual environment type python play,issue,positive,neutral,neutral,neutral,neutral,neutral
655419900,"> 3- feed (4) 10 second clips to the toolbox, synthezising each clip.

I believe the embedding only depends on the last file loaded. In other words, the toolbox has no memory and does not learn as it is used. So you can experiment to see which of the clips results in the best cloned voice. (Will be a lot easier with the toolbox once #402 is merged)

> Also i am not sure how to interpret the lower left box were your points are generated projections are all over from the same voice. Am pretty sure these points should be almost directly on top of one another.

You are correct, if the speaker encoder is good then all the points from a single speaker should form a distinct cluster away from other speakers. However, if it is only plotting data from a single speaker then I think the autoscaling will make those points appear farther apart than they are in reality.",feed second clip toolbox clip believe last file loaded toolbox memory learn used experiment see clip best voice lot easier toolbox also sure interpret lower left box voice pretty sure almost directly top one another correct speaker good single speaker form distinct cluster away however plotting data single speaker think make appear farther apart reality,issue,positive,positive,positive,positive,positive,positive
655404996,I wonder if the problematic DLL would have been installed when you installed torch 1.2.0 just now.,wonder problematic would torch,issue,negative,neutral,neutral,neutral,neutral,neutral
655402977,Actually I renamed it already when I was looking at the installation guide. I'll try deleting it.,actually already looking installation guide try,issue,negative,neutral,neutral,neutral,neutral,neutral
655399896,"Glad it is working for you now :D

The virtual environment is suggested, but you don't need it if your default python environment is just like the virtualenv. If you want to try GPU support again, you can `deactivate` your virtual environment and try following the quoted step in my comment here: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/407#issuecomment-655384859

Seems like renaming/deleting a DLL may be all that's needed. If you don't mind could you try it just to see if that would fix it?",glad working virtual environment need default python environment like want try support deactivate virtual environment try following step comment like may mind could try see would fix,issue,positive,positive,positive,positive,positive,positive
655398097,"DUDE! You're awesome this worked! Do I always have to create a virtual environment to use this? Or is there any other way I can play it directly? And thanks alot btw 👍 :)

Here's the log if it helps :

PS D:\Game\Real-Time-Voice-Cloning-master> python demo_toolbox.py
D:\Game\Real-Time-Voice-Cloning-master\encoder\audio.py:13: UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended.
  warn(""Unable to import 'webrtcvad'. This package enables noise removal and is recommended."")
2020-07-08 14:45:06.231221: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
Arguments:
    datasets_root:    None
    enc_models_dir:   encoder\saved_models
    syn_models_dir:   synthesizer\saved_models
    voc_models_dir:   vocoder\saved_models
    low_mem:          False

Warning: you did not pass a root directory for datasets as argument.
The recognized datasets are:
        LibriSpeech/dev-clean
        LibriSpeech/dev-other
        LibriSpeech/test-clean
        LibriSpeech/test-other
        LibriSpeech/train-clean-100
        LibriSpeech/train-clean-360
        LibriSpeech/train-other-500
        LibriTTS/dev-clean
        LibriTTS/dev-other
        LibriTTS/test-clean
        LibriTTS/test-other
        LibriTTS/train-clean-100
        LibriTTS/train-clean-360
        LibriTTS/train-other-500
        LJSpeech-1.1
        VoxCeleb1/wav
        VoxCeleb1/test_wav
        VoxCeleb2/dev/aac
        VoxCeleb2/test/aac
        VCTK-Corpus/wav48
Feel free to add your own. You can still use the toolbox by recording samples yourself.



",dude awesome worked always create virtual environment use way play directly thanks log python unable import package noise removal warn unable import package noise removal successfully dynamic library none false warning pas root directory argument feel free add still use toolbox recording,issue,positive,positive,neutral,neutral,positive,positive
655389835,"Try these instructions to run the toolbox on CPU. Nothing needs to be uninstalled before starting, we will use a virtual environment.

#### Make a virtual environment and activate it:
```
python -m venv env
env\Scripts\activate.bat
```

#### Install torch for CPU and the requirements
```
pip install torch==1.5.1+cpu torchvision==0.6.1+cpu -f https://download.pytorch.org/whl/torch_stable.html
pip install -r requirements.txt
```

#### Run the toolbox
```
python demo_toolbox.py
```",try run toolbox nothing need uninstalled starting use virtual environment make virtual environment activate python install torch pip install pip install run toolbox python,issue,negative,neutral,neutral,neutral,neutral,neutral
655384859,"Is that error message with torch 1.2.0 for CUDA 10.0?

Edit: [clip_grad_norm_](https://pytorch.org/docs/1.2.0/nn.html#torch.nn.utils.clip_grad_norm_) exists in torch 1.2.0 so it looks like an installation error with torch, or a path issue. Based on the poorlydocumented.com link you may have missed this step:
> After this is installed you have to remove the bundled cuDNN DLL. You can go directly to this directory using %userprofile%\AppData\Local\Programs\Python\Python37\Lib\site-packages\torch\lib in your File Explorer bar. Find cudnn64_7.dll and either rename it (e.g., cudnn64_7.dll.bak) or delete it.",error message torch edit torch like installation error torch path issue based link may step remove go directly directory file explorer bar find either rename delete,issue,negative,positive,neutral,neutral,positive,positive
655384498,"PS D:\Game\Real-Time-Voice-Cloning-master> python demo_toolbox.py
Traceback (most recent call last):
  File ""demo_toolbox.py"", line 2, in <module>
    from toolbox import Toolbox
  File ""D:\Game\Real-Time-Voice-Cloning-master\toolbox\__init__.py"", line 1, in <module>
    from toolbox.ui import UI
  File ""D:\Game\Real-Time-Voice-Cloning-master\toolbox\ui.py"", line 5, in <module>
    from encoder.inference import plot_embedding_as_heatmap
  File ""D:\Game\Real-Time-Voice-Cloning-master\encoder\inference.py"", line 2, in <module>
    from encoder.model import SpeakerEncoder
  File ""D:\Game\Real-Time-Voice-Cloning-master\encoder\model.py"", line 5, in <module>
    from torch.nn.utils import clip_grad_norm_
  File ""C:\Users\Pritam\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\__init__.py"", line 81, in <module>
    from torch._C import *
ImportError: DLL load failed: The specified module could not be found.


:(((
What this might be any idea?",python recent call last file line module toolbox import toolbox file line module import file line module import file line module import file line module import file line module import load module could found might idea,issue,negative,neutral,neutral,neutral,neutral,neutral
655369627,"This might be able to help you with the installation: https://stackoverflow.com/a/27909082

If it doesn't work I would recommend downloading the latest torch for CPU.",might able help installation work would recommend latest torch,issue,positive,positive,positive,positive,positive,positive
655356042,Thank you for the link. I'll download and try running it. While it's downloading could you please tell me how can I install it using pip install? I'm new to it.,thank link try running could please tell install pip install new,issue,positive,positive,positive,positive,positive,positive
655353585,"Interesting, I found this file and thought 1.2.0 would have CUDA 10.0 support:
https://download.pytorch.org/whl/cu100/torch-1.2.0-cp37-cp37m-win_amd64.whl

You should be able to download that file and then `pip install` it locally.",interesting found file thought would support able file pip install locally,issue,positive,positive,positive,positive,positive,positive
655352822,"Since the installation guide was written, we added CPU support so you're not required to match the torch and CUDA versions. It provides a nice speedup but if you experience a lot of frustration at the install I would just install any recent torch and try the toolbox. Then at a later date, fix your setup to get the GPU speedup if you really want to dive into training or generating a lot of cloned speech.",since installation guide written added support match torch nice experience lot frustration install would install recent torch try toolbox later date fix setup get really want dive training generating lot speech,issue,negative,positive,positive,positive,positive,positive
655352757," ERROR: Could not find a version that satisfies the requirement torch==1.4.0+cu100 (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2, 0.4.1, 1.0.0, 1.0.1, 1.1.0, 1.2.0, 1.2.0+cpu, 1.2.0+cu92, 1.3.0, 1.3.0+cpu, 1.3.0+cu92, 1.3.1, 1.3.1+cpu, 1.3.1+cu92, 1.4.0, 1.4.0+cpu, 1.4.0+cu92, 1.5.0, 1.5.0+cpu, 1.5.0+cu101, 1.5.0+cu92, 1.5.1, 1.5.1+cpu, 1.5.1+cu101, 1.5.1+cu92)
ERROR: No matching distribution found for torch==1.4.0+cu100




 ERROR: Could not find a version that satisfies the requirement torch==1.2.0+cu100 (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2, 0.4.1, 1.0.0, 1.0.1, 1.1.0, 1.2.0, 1.2.0+cpu, 1.2.0+cu92, 1.3.0, 1.3.0+cpu, 1.3.0+cu92, 1.3.1, 1.3.1+cpu, 1.3.1+cu92, 1.4.0, 1.4.0+cpu, 1.4.0+cu92, 1.5.0, 1.5.0+cpu, 1.5.0+cu101, 1.5.0+cu92, 1.5.1, 1.5.1+cpu, 1.5.1+cu101, 1.5.1+cu92)
ERROR: No matching distribution found for torch==1.2.0+cu100
Tried both. None worked :( ",error could find version requirement post post error matching distribution found error could find version requirement post post error matching distribution found tried none worked,issue,negative,neutral,neutral,neutral,neutral,neutral
655350938,"It means that CUDA 10.0 is not available for the latest version of torch, but it is for earlier versions (which should work with the toolbox).

Try this:
```
pip install  torch==1.4.0+cu100 -f https://download.pytorch.org/whl/torch_stable.html
```

There is a chance 1.4.0 is not available for Windows, in which case try torch==1.2.0+cu100 instead.",available latest version torch work toolbox try pip install chance available case try instead,issue,negative,positive,positive,positive,positive,positive
655345230,"Hey @blue-fish thank you for replying. I've uninstalled pytorch. I'm running CUDA 10.0 (as told in the Voice Clone installation guide https://poorlydocumented.com/2019/11/installing-corentinjs-real-time-voice-cloning-project-on-windows-10-from-scratch/ ) But I can't find pytorch for CUDA 10.0. Which one should i get? The only CUDA options I see are 9.2
10.1
10.2
None",hey thank uninstalled running told voice clone installation guide ca find one get see none,issue,negative,neutral,neutral,neutral,neutral,neutral
655336568,"@pritxm Looks very similar to this, could be a torch version issue. https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/260#issuecomment-655039901

Could you please 1) uninstall torch, 2) visit https://pytorch.org and install the appropriate version , and try again?",similar could torch version issue could please torch visit install appropriate version try,issue,negative,positive,positive,positive,positive,positive
655171330,"@goodmangu Is ""loss exploded"" still a problem for your synthesizer training? If it is resolved, would you please go ahead and close this issue?",loss exploded still problem synthesizer training resolved would please go ahead close issue,issue,negative,neutral,neutral,neutral,neutral,neutral
655113702,The original question has been addressed. I am going to close this issue since it is not an active area of development for this repo. If anyone manages to set this up please reopen this issue and share some results.,original question going close issue since active area development anyone set please reopen issue share,issue,positive,positive,positive,positive,positive,positive
655105460,Clear to perform squash and merge @matheusfillipe . Thank you for this very nice toolbox enhancement.,clear perform squash merge thank nice toolbox enhancement,issue,positive,positive,positive,positive,positive,positive
655103500,"This PR is no longer applicable since we eliminated the ""--cpu"" flag in #406 and auto-detect the user's configuration. Thank you for suggesting this change @maxi07 , which led to an even better enhancement.",longer applicable since flag user configuration thank suggesting change led even better enhancement,issue,positive,positive,positive,positive,positive,positive
655076398,"I am going to propose that we auto-detect the user's configuration and proceed appropriately. This is already the case for demo_toolbox.py. See #406 .

If #406 is merged then it will make this documentation update unnecessary.",going propose user configuration proceed appropriately already case see make documentation update unnecessary,issue,negative,positive,neutral,neutral,positive,positive
655064062,Nice! Thank you for documenting your troubleshooting steps @maxi07 . I see you opened #405 to fix the documentation issue. I'll comment on that aspect in the pull request.,nice thank see fix documentation issue comment aspect pull request,issue,positive,positive,positive,positive,positive,positive
655039901,"Hi @blue-fish , I followed your steps but without success, as pip was already using the latest version 20.1.1.. After that I deinstalled Python (which was previously installed through the Microsoft Store) and upgraded to the latest 3.7.8, in hope this would resolve the issue. This actually changed the error message.

Running the **testing script**
```
2020-07-07 19:57:48.802524: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found
2020-07-07 19:57:48.805869: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Traceback (most recent call last):
  File ""demo_cli.py"", line 3, in <module>
    from synthesizer.inference import Synthesizer
  File ""C:\Users\max\Real-Time-Voice-Cloning\synthesizer\inference.py"", line 1, in <module>
    from synthesizer.tacotron2 import Tacotron2
  File ""C:\Users\max\Real-Time-Voice-Cloning\synthesizer\tacotron2.py"", line 3, in <module>
    from synthesizer.models import create_model
  File ""C:\Users\max\Real-Time-Voice-Cloning\synthesizer\models\__init__.py"", line 1, in <module>
    from .tacotron import Tacotron
  File ""C:\Users\max\Real-Time-Voice-Cloning\synthesizer\models\tacotron.py"", line 5, in <module>
    from synthesizer.models.modules import *
  File ""C:\Users\max\Real-Time-Voice-Cloning\synthesizer\models\modules.py"", line 2, in <module>
    import torch
  File ""C:\Users\max\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\__init__.py"", line 81, in <module>
    from torch._C import *
ImportError: DLL load failed: The module was not found.
```

Running the actual **toolbox:**
```
Traceback (most recent call last):
  File ""demo_toolbox.py"", line 2, in <module>
    from toolbox import Toolbox
  File ""C:\Users\max\Real-Time-Voice-Cloning\toolbox\__init__.py"", line 1, in <module>
    from toolbox.ui import UI
  File ""C:\Users\max\Real-Time-Voice-Cloning\toolbox\ui.py"", line 5, in <module>
    from encoder.inference import plot_embedding_as_heatmap
  File ""C:\Users\max\Real-Time-Voice-Cloning\encoder\inference.py"", line 2, in <module>
    from encoder.model import SpeakerEncoder
  File ""C:\Users\max\Real-Time-Voice-Cloning\encoder\model.py"", line 5, in <module>
    from torch.nn.utils import clip_grad_norm_
  File ""C:\Users\max\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\__init__.py"", line 81, in <module>
    from torch._C import *
ImportError: DLL load failed: The module was not found.
```

## The solution
After some quick research I figured out, that I was using the wrong PyTorch version. I was following along your suggested setup guide from [here](https://poorlydocumented.com/2019/11/installing-corentinjs-real-time-voice-cloning-project-on-windows-10-from-scratch/), which had an install command for PyTorch. I headed over so the [PyTorch site](https://pytorch.org/get-started/locally/) and downloaded the latest version with ```pip install torch==1.5.1+cpu torchvision==0.6.1+cpu -f https://download.pytorch.org/whl/torch_stable.html``` (as I don't have a GPU ready).

And this fixed it! The test script now runs, but your documentation in the Readme.md is missing the ```--cpu``` tag when running the script.",hi without success pip already latest version python previously store latest hope would resolve issue actually error message running testing script could load dynamic library found ignore set machine recent call last file line module import synthesizer file line module import file line module import file line module import file line module import file line module import torch file line module import load module found running actual toolbox recent call last file line module toolbox import toolbox file line module import file line module import file line module import file line module import file line module import load module found solution quick research figured wrong version following along setup guide install command headed site latest version pip install ready fixed test script documentation missing tag running script,issue,negative,positive,neutral,neutral,positive,positive
654974316,"Thanks for giving it another try @maxi07 . Doing a quick search it seems you may need to upgrade your pip to get a later version of matplotlib that is compatible with your PyQt5. Try this and see if the toolbox will work:

```
pip uninstall matplotlib
python -m pip install --upgrade pip
pip install matplotlib
```",thanks giving another try quick search may need upgrade pip get later version compatible try see toolbox work pip python pip install upgrade pip pip install,issue,positive,positive,positive,positive,positive,positive
654894758,"Hi @blue-fish ,  the issue still resists with the current commit. 
Running the test with ```python demo_cli.py``` results into
```
2020-07-07 15:57:44.912120: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found
2020-07-07 15:57:44.918696: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Traceback (most recent call last):
  File ""demo_cli.py"", line 3, in <module>
    from synthesizer.inference import Synthesizer
  File ""C:\Users\max\Real-Time-Voice-Cloning\synthesizer\inference.py"", line 1, in <module>
    from synthesizer.tacotron2 import Tacotron2
  File ""C:\Users\max\Real-Time-Voice-Cloning\synthesizer\tacotron2.py"", line 3, in <module>
    from synthesizer.models import create_model
  File ""C:\Users\max\Real-Time-Voice-Cloning\synthesizer\models\__init__.py"", line 1, in <module>
    from .tacotron import Tacotron
  File ""C:\Users\max\Real-Time-Voice-Cloning\synthesizer\models\tacotron.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\max\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\__init__.py"", line 99, in <module>
    from tensorflow_core import *
  File ""C:\Users\max\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow_core\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""<frozen importlib._bootstrap>"", line 1019, in _handle_fromlist
  File ""C:\Users\max\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\max\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.7_3.7.2288.0_x64__qbz5n2kfra8p0\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\max\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow_core\python\__init__.py"", line 83, in <module>
    from tensorflow.python import keras
  File ""C:\Users\max\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow_core\python\keras\__init__.py"", line 26, in <module>
    from tensorflow.python.keras import activations
  File ""C:\Users\max\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow_core\python\keras\__init__.py"", line 26, in <module>
    from tensorflow.python.keras import activations
  File ""C:\Users\max\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow_core\python\keras\activations.py"", line 23, in <module>
    from tensorflow.python.keras.utils.generic_utils import deserialize_keras_object
  File ""C:\Users\max\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow_core\python\keras\utils\__init__.py"", line 38, in <module>
    from tensorflow.python.keras.utils.multi_gpu_utils import multi_gpu_model
  File ""C:\Users\max\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow_core\python\keras\utils\multi_gpu_utils.py"", line 22, in <module>
    from tensorflow.python.keras.engine.training import Model
  File ""C:\Users\max\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow_core\python\keras\engine\training.py"", line 47, in <module>
    from tensorflow.python.keras.engine import training_arrays
  File ""C:\Users\max\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow_core\python\keras\engine\training_arrays.py"", line 41, in <module>
    from scipy.sparse import issparse  # pylint: disable=g-import-not-at-top
  File ""C:\Users\max\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\scipy\__init__.py"", line 104, in <module>
    from . import _distributor_init
  File ""C:\Users\max\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\scipy\_distributor_init.py"", line 61, in <module>
    WinDLL(os.path.abspath(filename))
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.7_3.7.2288.0_x64__qbz5n2kfra8p0\lib\ctypes\__init__.py"", line 364, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: [WinError 126] The module was not found.
```

And running the toolbox itself returns
```
Traceback (most recent call last):
  File ""demo_toolbox.py"", line 2, in <module>
    from toolbox import Toolbox
  File ""C:\Users\max\Real-Time-Voice-Cloning\toolbox\__init__.py"", line 1, in <module>
    from toolbox.ui import UI
  File ""C:\Users\max\Real-Time-Voice-Cloning\toolbox\ui.py"", line 1, in <module>
    from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
  File ""C:\Users\max\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\matplotlib\backends\backend_qt5agg.py"", line 11, in <module>
    from .backend_qt5 import (
  File ""C:\Users\max\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\matplotlib\backends\backend_qt5.py"", line 15, in <module>
    import matplotlib.backends.qt_editor.figureoptions as figureoptions
  File ""C:\Users\max\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\matplotlib\backends\qt_editor\figureoptions.py"", line 12, in <module>
    from matplotlib.backends.qt_compat import QtGui
  File ""C:\Users\max\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\matplotlib\backends\qt_compat.py"", line 168, in <module>
    raise ImportError(""Failed to import any qt binding"")
ImportError: Failed to import any qt binding
```

I tried manually installing PyQt5, but the requirement is already satisfied. The requirements are installed via ```pip install -r requirements.txt```",hi issue still current commit running test python could load dynamic library found ignore set machine recent call last file line module import synthesizer file line module import file line module import file line module import file line module import file line module import file line module import file frozen line file line module file line module file line return name level package level file line module import file line module import file line module import file line module import file line module import file line module import model file line module import file line module import file line module import file line module file line mode module found running toolbox recent call last file line module toolbox import toolbox file line module import file line module import file line module import file line module import file line module import file line module raise import binding import binding tried manually requirement already satisfied via pip install,issue,positive,positive,neutral,neutral,positive,positive
654565806,"@Liujingxiu23 The info about the model training comes from this page: https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models

The encoder and synthesizer are the original models by @CorentinJ . All I did was take his original vocoder model and continued the training to see what would result. I didn't even change any parameters except to cut the batch size in half (100 to 50) so it would fit in my GPU's limited memory.

Edit: In case it is not clear, I used the training code in the repo without modification. I also used the same datasets (LibriSpeech train-clean-100 and -360) and processed them following these instructions: https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training

Also, since Chinese is your target language, you should see @KuangDD 's work here: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/30#issuecomment-629979383 if you haven't already.",model training come page synthesizer original take original model continued training see would result even change except cut batch size half would fit limited memory edit case clear used training code without modification also used following also since target language see work already,issue,positive,positive,positive,positive,positive,positive
654562709,"@blue-fish 
Thank  you for you reply!

You train encoder，synthsizer as well as the vocoder by yourself as follows?
Encoder: trained 1.56M steps (20 days with a single GPU) with a batch size of 64
Synthesizer: trained 256k steps (1 week with 4 GPUs) with a batch size of 144
Vocoder: trained 428k steps (4 days with a single GPU) with a batch size of 100

I trained the encoder and synthsizer using chinese corpus, but the result is not as good as yours.


For the encoder, have you remove the relu Activation Function in the last linear layer?
For the synthesizer, you use the same data (VCTK+LibriSpeech)as the paper?
",thank reply train well trained day single batch size synthesizer trained week batch size trained day single batch size trained corpus result good remove activation function last linear layer synthesizer use data paper,issue,positive,positive,positive,positive,positive,positive
654542636,"will do thanks

On Mon, Jul 6, 2020 at 8:04 PM blue-fish <notifications@github.com> wrote:

> @brcisna <https://github.com/brcisna> Based on #404
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/404> can I
> conclude that the toolbox is working for you now? If so can you please go
> ahead and close this issue?
>
> (Resolution: Use python 3.7)
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/401#issuecomment-654538744>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ALJUC6UQMIFCKMORLBZL6V3R2JYABANCNFSM4OQ6HYKQ>
> .
>
",thanks mon wrote based conclude toolbox working please go ahead close issue resolution use python reply directly view,issue,positive,positive,positive,positive,positive,positive
654539705,"@Oktai15 I think these are the settings you need to use @sberryman 's mixed encoder: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/126#issuecomment-573523609

I have not tried it though. Please let us know if it works for you.",think need use mixed tried though please let u know work,issue,negative,neutral,neutral,neutral,neutral,neutral
654538744,"@brcisna Based on #404 can I conclude that the toolbox is working for you now? If so can you please go ahead and close this issue?

(Resolution: Use python 3.7)",based conclude toolbox working please go ahead close issue resolution use python,issue,negative,neutral,neutral,neutral,neutral,neutral
654534934,"I think the extension issue is fixed now, on my system the default is wav for some reason :P",think extension issue fixed system default reason,issue,negative,positive,neutral,neutral,positive,positive
654527806,@matheusfillipe I tested the update and it works very nicely. The replay and history features are a very useful addition!,tested update work nicely replay history useful addition,issue,positive,positive,positive,positive,positive,positive
654509806,"@Oktai15 I thought I posted the links to the encoder for the mixed version. The tacotron and vocoder weights are useless that I trained. However the encoder is quite good.
https://www.dropbox.com/s/xl2wr13nza10850/encoder.zip?dl=0
",thought posted link mixed version useless trained however quite good,issue,negative,positive,neutral,neutral,positive,positive
654505071,"@Liujingxiu23 I don't have any suggestion regarding your data. As for the audio quality, you can improve it by finetuning both tacotron and the vocoder on a single speaker. To improve the quality of voice cloning in general, there's a lot more working, starting with the list I gave above.",suggestion regarding data audio quality improve single speaker improve quality voice general lot working starting list gave,issue,positive,negative,neutral,neutral,negative,negative
654374985,"> when i use this command on my original (was working) virtualenv
'python3.7 -m virtualenv MyEnv ' says "" virtualenv module not found'

Try installing virtualenv for your python3.7 and then try the above command again.
```
python3.7 -m pip install virtualenv
```",use command original working module try python try command python pip install,issue,negative,positive,positive,positive,positive,positive
654367721,">  If I just use whatever the latest files are from the master branch here, there's no way to guarantee they're compatible with my code. I'm open to other suggestions too.

I'd suggest that you fork this repo, and for the Sagemaker project to consume the fork. You would manage the fork to maintain compatibility between the two codebases. It also gives you flexibility to implement changes to the RTVC code that make your life easier with Sagemaker.",use whatever latest master branch way guarantee compatible code open suggest fork project consume fork would manage fork maintain compatibility two also flexibility implement code make life easier,issue,positive,positive,positive,positive,positive,positive
654366313,"blue-fish,

finally with much wrangling used deb. snapshots repositories to get python3.7 reinstalled on this system and appears to be back to were it should be,,,,but,, when i use this command on my original (was working) virtualenv  
'python3.7 -m virtualenv MyEnv ' says "" virtualenv module not found'
BUT ,,, my newly created virtualenv ( a day ago) with my default python3.8 the demo_cli.py fails with the error you listed above. I tried doing the one line change you suggested above but get syntax errors.

The UPSIDE is the toolbox.py opens fine and i can do recordings! Not sure if the synthesizer or other stuff works. Kinda burned out right now,,Would really like to get this back to working as i would like to use this again for some race history voices ( of now deceased people to narrate) of some videos,i do as a hobby.

Thank You

",finally much used deb get python system back use command original working module newly day ago default python error listed tried one line change get syntax upside fine sure synthesizer stuff work burned right really like get back working would like use race history deceased people narrate hobby thank,issue,positive,positive,positive,positive,positive,positive
654258648,"@blue-fish 

> I am interested in the training use case. Use the repo locally on CPU or low-end GPU for development, then transfer to Sagemaker when everything is working.

Sure. It seems there's a demand for it. And I don't think it wouldn't be too much effort for me to get that up and running.

> In your README.md you mention using ml.p2.xlarge which costs $1.26/hour for (4 vCPU + 61 GB mem) + (K80 GPU with 12 GB). On the pricing page there is also an option of using ml.g4dn.xlarge which is $0.74/hour for (4 vCPU + 16 GB mem) + (T4 GPU with 16 GB). Which would be preferred here?

Unfortunately, ml.g4dn.xlarge's won't work for batch transform jobs. Only certain instance types are supported. ml.p2.xlarge is still the cheapest GPU option.

> Now that it runs on CPU, inference could run on the same ml.t2.medium instance you recommend for installation ($0.0464/hour for 2 vCPU + 4 GB mem). Though I would still recommend getting a GPU instance if playing with the toolbox for more than a few minutes.

It would have to be ml.m5.large at $0.134/hour but yeah, still much cheaper than P2's.

> Also for maintainability it might be best to make the Sagemaker version a distinct repo instead of a direct fork, and incorporate files from this repo as a submodule.

I agree that makes more sense, especially now that you're actively maintaining it. Do you have any plans for versioning or distribution with this project? It would be nice if I could grab the files from PyPI and then import it as a library. If I just use whatever the latest files are from the master branch here, there's no way to guarantee they're compatible with my code. I'm open to other suggestions too.",interested training use case use locally development transfer everything working sure demand think would much effort get running mention mem page also option mem would preferred unfortunately wo work batch transform certain instance still option inference could run instance recommend installation mem though would still recommend getting instance toolbox would yeah still much also might best make version distinct instead direct fork incorporate agree sense especially actively distribution project would nice could grab import library use whatever latest master branch way guarantee compatible code open,issue,positive,positive,positive,positive,positive,positive
654140383,"@CorentinJ  
It's a pity that you decide no to update this project any more.
I have followed your work since latter half of 2019.
For the encoder part, I removed the Relu Activation function of the last linear layer and train with 18k speakers(Chinese+English) for about  2~3 month. I using ""Resemblyzer-master"" tool to analysize the embedding generated by the model as well as my own tool. I guess the encoder is  ready. 
For the systhsizer,  Can your help me and give me some advices?
1. My target lanuage is Chinese, I did not have enough TTS corpus to train the synthesizer, only asr corpus can be found. For example , the aishell, but the quality is not so good. Do you have any suggesion to preprocess the wavs ？
2. When giving a target wav, the end2end systhesized wavs  have some characteristic of the target timbre， but they are just similar in a low level.  Do you have any suggesion to improve the similarity？ How is your result? Could you share some of your best result?",pity decide update project work since latter half part removed activation function last linear layer train month tool model well tool guess ready help give target enough corpus train synthesizer corpus found example quality good giving target characteristic target similar low level improve result could share best result,issue,positive,positive,positive,positive,positive,positive
654134696,"@Liujingxiu23 They are end-to-end results where I replicate the audio samples of the SV2TTS paper: https://google.github.io/tacotron/publications/speaker_adaptation/

I use the reference audio from VCTK p240 and p260 to create the embedding and generate synthesized samples #0 and #1 using tacotron and the vocoder model.",replicate audio paper use reference audio create generate model,issue,negative,neutral,neutral,neutral,neutral,neutral
654131754,"@blue-fish  The wavs that you shared sounds good! Are the wavs just the result of vocoder, or an end2end results which using encoder to predict the embedding then using tacotron and vocoder model to synthesize?",good result predict model synthesize,issue,negative,positive,positive,positive,positive,positive
653996443,"This issue of large gaps is something that also occurred at Resemble.AI, and that I have worked on and fixed. It's a serious amount of work, I'll give you the big lines:
- Use LibriTTS instead of LibriSpeech in order to have punctuation.
- LibriTTS needs to be curated of speakers with bad prosody. 
- You can lower the upper bound I put on utterance duration, which I suspect has for effect of removing long utterances that are more likely to have more pauses (I formally evaluated models trained this way to generate less frequent long pauses). It also trains faster and does not have drawbacks (with a good attention paradigm, the model can generate sentences longer than seen in training). 
- The attention paradigm needs to be replaced, forward attention is poor.",issue large something also worked fixed serious amount work give big use instead order punctuation need bad prosody lower upper bound put utterance duration suspect effect removing long likely formally trained way generate le frequent long also faster good attention paradigm model generate longer seen training attention paradigm need forward attention poor,issue,negative,negative,neutral,neutral,negative,negative
653967332,That may be what ill have to do. i was really hoping to try and get this setup back to 3.7 Thanks for helping out too.,may ill really try get setup back thanks helping,issue,negative,negative,neutral,neutral,negative,negative
653966099,Can you run a virtual machine with Ubuntu 18.04 and install python3.7 on that?,run virtual machine install python,issue,negative,neutral,neutral,neutral,neutral,neutral
653954220,"I did spend quite a bit of time trying to get python3.7 installed back onto here. python3.7 is simply not in the debian bullseye repositories now,,,nada . tried setting up backports and all sorts of stuff,, i would guess there was a cutoff date and the maintainers are full on python3.8 for their new dsitro release. I thought i got python3.7 installed through source with the --altinstall switch but cant see it anywhere,,?i never found any decent howtos on installing for a local user in an virtualenv either?.If i can get 3.7 back on,it will work as it did before,, i never realized when i done the autoremove of python3.7 as suggested a couple weeks ago it would kill this virtualenv setup,,never thought about it at the time until i tried using the toolbox again.
Sorry im sorta lost on this stuff.
",spend quite bit time trying get python back onto python simply tried setting stuff would guess cutoff date full python new release thought got python source switch cant see anywhere never found decent local user either get back work never done python couple ago would kill setup thought time tried toolbox sorry lost stuff,issue,negative,positive,neutral,neutral,positive,positive
653953491,"I should have mentioned at the start that the most direct way for you to get the toolbox working is to reinstall python3.7 (or run it inside a virtual machine). While I would greatly appreciate your help figuring out tensorflow_v2 compatibility, getting the toolbox to run on python3.8 is likely much harder than getting python3.7 back. I'll understand if you want to try restoring python3.7 instead.",start direct way get toolbox working reinstall python run inside virtual machine would greatly appreciate help compatibility getting toolbox run python likely much harder getting python back understand want try python instead,issue,positive,positive,positive,positive,positive,positive
653953141,"OK I tried changing the line you suggested to this:   return tf.TensorShape([]).with_rank(1)
I get a  syntax error,,I am not a coder so this is probably not understood by me correctly,,,

Thanks. ",tried line return get syntax error coder probably understood correctly thanks,issue,negative,positive,positive,positive,positive,positive
653951833,"Thank You,,I wasnt sure how to enable --cpu even after googling,,,
 after getting saved models added here and there  i am ending up with the error you have mentioned above

Getting closer,,,This is running demo_cli.py

Thanks

",thank wasnt sure enable even getting saved added ending error getting closer running thanks,issue,positive,positive,positive,positive,positive,positive
653950279,"blue-fish,
This is error i get after getting all the requirements installed.. I think when i initally configured pytorch on their configuartion wizard i selected a CUDA version rather than NONE? have tried uninstalling and resintalling but still end up with Your PyTorch is unconfigured"":

Arguments:
    enc_model_fpath:   encoder/saved_models/pretrained.pt
    syn_model_dir:     synthesizer/saved_models/logs-pretrained
    voc_model_fpath:   vocoder/saved_models/pretrained/pretrained.pt
    low_mem:           False
    no_sound:          False
    cpu:               False

Running a test of your configuration...

Your PyTorch installation is not configured. If you have a GPU ready for deep learning, ensure that the drivers are properly installed, and that your CUDA version matches your PyTorch installation.

If you're trying to use a cpu, please use the option --cpu.

I do NOT have a CUDA capable GPU ,,Just want to use cpu if possible with this setup.

Goes without saying,,I am very green at this.


Thanks",error get getting think wizard selected version rather none tried still end false false false running test configuration installation ready deep learning ensure properly version installation trying use please use option capable want use possible setup go without saying green thanks,issue,positive,negative,neutral,neutral,negative,negative
653946602,"Let's start on demo_cli.py, I find it easier to troubleshoot. After updating the requirements do you get the same error message as me?
```
ValueError: Shape must be rank 1 but is rank 0 for '{{node Tacotron_model/inference/decoder/concat}} = ConcatV2[N=2, T=DT_INT32, Tidx=DT_INT32](Tacotron_model/inference/decoder/concat/values_0, Tacotron_model/inference/decoder/concat/values_1, Tacotron_model/inference/decoder/concat/axis)' with input shapes: [1], [], [].
```

I recall working around it by changing [this line](https://github.com/blue-fish/Real-Time-Voice-Cloning/blob/49d4759503163a5133c2ac815e4a25eea9570fa2/synthesizer/models/helpers.py#L24) to `return tf.TensorShape([]).with_rank(1)` but I'm not sure if it is proper. It might be masking a different error that needs to be fixed.",let start find easier get error message shape must rank rank node input recall working around line return sure proper might different error need fixed,issue,negative,negative,negative,negative,negative,negative
653945421,"OK,

I apologize. I was thinking your dir branch patched my existing directory,
So,,now,,  i cloned your dir to fresh directory,
Now i am ending up with the following error when running both demo_cli.py & demo_toolbox.py 

from tensor2tensor.utils.hparam import HParams
ModuleNotFoundError: No module named 'tensor2tensor'

EDIT: Opps I did not run the requirements.txt,,,, Let me do this and report back.

Thanks

",apologize thinking branch directory fresh directory ending following error running import module edit run let report back thanks,issue,negative,positive,positive,positive,positive,positive
653936544,Closing this as duplicate of #53. Let's work the issue there.,duplicate let work issue,issue,negative,neutral,neutral,neutral,neutral,neutral
653936453,Thank you for referencing the issue @macriluke. I am going to reopen this issue since I have some interest in fixing it. Another possibility is that it goes away in #370 when @dathudeptrai modifies the tensorflowTTS/tacotron2 code to work with this repo.,thank issue going reopen issue since interest fixing another possibility go away code work,issue,positive,neutral,neutral,neutral,neutral,neutral
653935428,"Cloning makes a copy of the files with version control enabled. I think the way you ran it placed another Real-Time-Voice-Cloning directory inside the existing Real-Time-Voice-Cloning dir. Then when you tried it out, it was still running the old code.

To help keep things straight, you should either remove the existing copy before cloning, or perform the clone in a different location entirely.",copy version control think way ran another directory inside tried still running old code help keep straight either remove copy perform clone different location entirely,issue,positive,positive,neutral,neutral,positive,positive
653933587,"Hi blue-fish,

Done the #1 routine here just to see what happens. I believe i am correct ,,cloning your branch simply dumps your patched files into my existing Real-Time-Voice-Cloning dir? I am still ending up with identical ' tensorflow.contrib not found'.  I did also deactivate and activate the virtualenv one time also after cloning your branch to my dir. Thought i may end up with a  slightly different error after cloning your branch? 

line 3, in <module>
    from tensorflow.contrib.seq2seq import Helper
ModuleNotFoundError: No module named 'tensorflow.contrib'

Adding:
tensorboard            2.2.2
tensorboard-plugin-wit 1.7.0
tensorflow-addons      0.10.0
tensorflow-cpu         2.2.0
tensorflow-estimator   2.2.0





Thanks 
",hi done routine see believe correct branch simply still ending identical found also deactivate activate one time also branch thought may end slightly different error branch line module import helper module thanks,issue,negative,positive,neutral,neutral,positive,positive
653929209,"No worries @brcisna ! Happy you are taking an interest to this. You need another copy of the Real-Time-Voice-Cloning toolbox, but it will work with your existing python3.8 + tensorflow2 virtualenv. Here are two ways to do it:

### 1. Quick (harder to share code changes)
This clones the branch from my fork but you will not be able to push anything back without doing a bit of setup.
```
git clone --depth 1 https://github.com/blue-fish/Real-Time-Voice-Cloning -b 370_tf2_compat
```

### 2. Fork method (very easy to share code changes)
If you are willing to spend just a little more time on the setup, I suggest you make a fork so we can easily collaborate on this:
1. In your web browser, go to my fork: https://github.com/blue-fish/Real-Time-Voice-Cloning
2. Click the fork button at top-right
3. In a terminal, clone your fork: `git clone https://github.com/brcisna/Real-Time-Voice-Cloning`
4. Go into the folder: `cd Real-Time-Voice-Cloning`
5. Switch to the branch: `git checkout 370_tf2_compat`

As you make fixes, push your changes back to your fork. Then I will be able to pull them into mine, and push them upstream when everything is working.

### Post-setup (after either method)
6. Copy a set of pretrained models to the usual locations
7. At this point you can activate your python3.8 virtualenv and run demo_cli.py or demo_toolbox.py. It will error because there are still some tensorflow2 compatibility issues to work through. But you will be a few hours ahead compared to starting from scratch!",happy taking interest need another copy toolbox work python two way quick harder share code branch fork able push anything back without bit setup git clone depth fork method easy share code willing spend little time setup suggest make fork easily collaborate web browser go fork click fork button terminal clone fork git clone go folder switch branch git make push back fork able pull mine push upstream everything working either method copy set usual point activate python run error still compatibility work ahead starting scratch,issue,positive,positive,positive,positive,positive,positive
653927291,"> @CorentinJ can you please take a quick look at #227 (synthesizer produces large gaps when processing very short texts) and give us a clue where that issue might be coming from, or where to start if we want to fix it?
> 
> Edit: @macriluke says it results from the training dataset. Is it really because the models are trained on medium to long utterances? [#291 (comment)](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/291#issuecomment-610519787)

I was going off of this bit of the thesis:

>  The prosody is however sometimes unnatural, with pauses at unexpected locations in the sentence, or the lack of pauses where they are expected. This is
particularly noticeable with the embedding of some speakers who talk slowly, showing
that the speaker encoder does capture some form of prosody. The lack of punctuation
in LibriSpeech is partially responsible for this, forcing the model to infer punctuation
from the text alone. This issue was highlighted by the authors as well, and can be
heard on some of their samples of LibriSpeech speakers. The limits we imposed on
the duration of utterances in the dataset (1.6s - 11.25s) are likely also problematic.
Sentences that are too short will be stretched out with long pauses, and for those that
are too long the voice will be rushed.

It looks like maybe I made the wrong assumption of the meaning of the word ""pauses"" here, as I see in #53 It's mentioned that this is an issue introduced through the code.

EDIT: I will say that while the wooshing and long pauses aren't this common on other pretrained tacotrons, I have heard them on mid-training evaluations of different synthesis models, so the real cause could potentially be both training and code here.",please take quick look synthesizer large short give u clue issue might coming start want fix edit training really trained medium long comment going bit thesis prosody however sometimes unnatural unexpected sentence lack particularly noticeable talk slowly showing speaker capture form prosody lack punctuation partially responsible forcing model infer punctuation text alone issue well duration likely also problematic short long long voice rushed like maybe made wrong assumption meaning word see issue code edit say long common different synthesis real cause could potentially training code,issue,negative,positive,neutral,neutral,positive,positive
653926653,"@ViktorAlm I tried this out but I am getting an error message when the synthesizer checkpoint is loaded. 

> tensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [66,512] rhs shape= [72,512]
	 [[{{node save/Assign_40}}]]

What I did is replace the synthesizer and vocoder with your models. I did not make any synthesizer code changes for Swedish yet. Can you let me know if you changed any files or hparams to make this work?

<details>
<summary>Error message including full traceback</summary>

```
Initialized Tacotron model. Dimensions (? = dynamic shape): 
  Train mode:               False
  Eval mode:                False
  GTA mode:                 False
  Synthesis mode:           True
  Input:                    (?, ?)
  device:                   0
  embedding:                (?, ?, 512)
  enc conv out:             (?, ?, 512)
  encoder out (cond):       (?, ?, 768)
  decoder out:              (?, ?, 80)
  residual out:             (?, ?, 512)
  projected residual out:   (?, ?, 80)
  mel out:                  (?, ?, 80)
  <stop_token> out:         (?, ?)
  Tacotron Parameters       28.439 Million.
Loading checkpoint: synthesizer/saved_models/logs-pretrained/taco_pretrained/tacotron_model.ckpt-200000
2020-07-05 14:59:25.497588: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fa249744430 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-05 14:59:25.497641: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
Traceback (most recent call last):
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1365, in _do_call
    return fn(*args)
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1350, in _run_fn
    target_list, run_metadata)
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1443, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [66,512] rhs shape= [72,512]
	 [[{{node save/Assign_40}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py"", line 1290, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 956, in run
    run_metadata_ptr)
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1359, in _do_run
    run_metadata)
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [66,512] rhs shape= [72,512]
	 [[node save/Assign_40 (defined at venv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]

Original stack trace for 'save/Assign_40':
  File ""demo_cli.py"", line 95, in <module>
    mels = synthesizer.synthesize_spectrograms(texts, embeds)
  File ""/Real-Time-Voice-Cloning/synthesizer/inference.py"", line 77, in synthesize_spectrograms
    self.load()
  File ""/Real-Time-Voice-Cloning/synthesizer/inference.py"", line 58, in load
    self._model = Tacotron2(self.checkpoint_fpath, hparams)
  File ""/Real-Time-Voice-Cloning/synthesizer/tacotron2.py"", line 62, in __init__
    saver = tf.compat.v1.train.Saver()
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py"", line 828, in __init__
    self.build()
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py"", line 840, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py"", line 878, in _build
    build_restore=build_restore)
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py"", line 508, in _build_internal
    restore_sequentially, reshape)
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py"", line 350, in _AddRestoreOps
    assign_ops.append(saveable.restore(saveable_tensors, shapes))
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/training/saving/saveable_object_util.py"", line 73, in restore
    self.op.get_shape().is_fully_defined())
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/ops/state_ops.py"", line 227, in assign
    validate_shape=validate_shape)
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_state_ops.py"", line 66, in assign
    use_locking=use_locking, name=name)
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py"", line 794, in _apply_op_helper
    op_def=op_def)
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 3357, in create_op
    attrs, op_def, compute_device)
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 3426, in _create_op_internal
    op_def=op_def)
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""demo_cli.py"", line 95, in <module>
    mels = synthesizer.synthesize_spectrograms(texts, embeds)
  File ""/Real-Time-Voice-Cloning/synthesizer/inference.py"", line 77, in synthesize_spectrograms
    self.load()
  File ""/Real-Time-Voice-Cloning/synthesizer/inference.py"", line 58, in load
    self._model = Tacotron2(self.checkpoint_fpath, hparams)
  File ""/Real-Time-Voice-Cloning/synthesizer/tacotron2.py"", line 63, in __init__
    saver.restore(self.session, checkpoint_path)
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py"", line 1326, in restore
    err, ""a mismatch between the current graph and the graph"")
tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Assign requires shapes of both tensors to match. lhs shape= [66,512] rhs shape= [72,512]
	 [[node save/Assign_40 (defined at venv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]

Original stack trace for 'save/Assign_40':
  File ""demo_cli.py"", line 95, in <module>
    mels = synthesizer.synthesize_spectrograms(texts, embeds)
  File ""/Real-Time-Voice-Cloning/synthesizer/inference.py"", line 77, in synthesize_spectrograms
    self.load()
  File ""/Real-Time-Voice-Cloning/synthesizer/inference.py"", line 58, in load
    self._model = Tacotron2(self.checkpoint_fpath, hparams)
  File ""/Real-Time-Voice-Cloning/synthesizer/tacotron2.py"", line 62, in __init__
    saver = tf.compat.v1.train.Saver()
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py"", line 828, in __init__
    self.build()
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py"", line 840, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py"", line 878, in _build
    build_restore=build_restore)
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py"", line 508, in _build_internal
    restore_sequentially, reshape)
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py"", line 350, in _AddRestoreOps
    assign_ops.append(saveable.restore(saveable_tensors, shapes))
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/training/saving/saveable_object_util.py"", line 73, in restore
    self.op.get_shape().is_fully_defined())
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/ops/state_ops.py"", line 227, in assign
    validate_shape=validate_shape)
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_state_ops.py"", line 66, in assign
    use_locking=use_locking, name=name)
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py"", line 794, in _apply_op_helper
    op_def=op_def)
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 3357, in create_op
    attrs, op_def, compute_device)
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 3426, in _create_op_internal
    op_def=op_def)
  File ""venv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()
```
</details>",tried getting error message synthesizer loaded assign match node replace synthesizer make synthesizer code yet let know make work summary error message full model dynamic shape train mode false mode false mode false synthesis mode true input device cond residual residual mel million loading service platform host guarantee used device host default version recent call last file line return file line file line assign match node handling exception another exception recent call last file line restore file line run file line file line file line raise type message assign match node defined original stack trace file line module file line file line load file line saver file line file line build file line file line reshape file line file line restore file line assign file line assign file line file line return file line file line file line handling exception another exception recent call last file line module file line file line load file line file line restore err mismatch current graph graph likely due mismatch current graph graph please ensure graph based original error assign match node defined original stack trace file line module file line file line load file line saver file line file line build file line file line reshape file line file line restore file line assign file line assign file line file line return file line file line file line,issue,positive,positive,neutral,neutral,positive,positive
653924587,"Hi blue-fish,

Thank You for the information. Am showing my stupidity. I see the #370 comment but don't know how to install the fork of your tensorflow_v2 to my existing virtualenv? I see several commits,but have never dealt with this type of files,,i guess.  Would like to hack around on this if i can figure out how to patch your files in? 
Just an fyi. What happens is in a fresh python3.8 virtualenv the demo_toolbox.py ends up with the common 'tensorflow.contrib not found' ,,which of course does not exist in tensorflow_v2

Thanks again ",hi thank information showing stupidity see comment know install fork see several never dealt type guess would like hack around figure patch fresh python common found course exist thanks,issue,positive,negative,neutral,neutral,negative,negative
653917972,"@brcisna As you note, the code will not be compatible with python 3.8 until we can use tensorflow 2.x. There is an issue for this: #370 .

If you want to hack at it, you are welcome to pull my tensorflow 2.x branch and try to figure out the error messages. Start here for more info. https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/370#issuecomment-648798434 

Edit: If you're able to make my tensorflow2 fork run on Python 3.8 (that is, get through demo_cli.py without an error, regardless of output quality), I will take on the task of converting the pretrained model and getting it merged upstream with the latest updates.",note code compatible python use issue want hack welcome pull branch try figure error start edit able make fork run python get without error regardless output quality take task converting model getting upstream latest,issue,negative,positive,positive,positive,positive,positive
653892487,"@CorentinJ can you please take a quick look at #227 (synthesizer produces large gaps when processing very short texts) and give us a clue where that issue might be coming from, or where to start if we want to fix it?

Edit: @macriluke says it results from the training dataset. Is it really because the models are trained on medium to long utterances? https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/291#issuecomment-610519787",please take quick look synthesizer large short give u clue issue might coming start want fix edit training really trained medium long,issue,negative,positive,positive,positive,positive,positive
653883430,"Hello @vankhoa21991 . We recently made a change to the codebase and we are unable to troubleshoot problems that occurred with the old code. May I ask that you please pull the latest code and try again? I expect it should just work, but feel free to open a new issue if it doesn't.",hello recently made change unable old code may ask please pull latest code try expect work feel free open new issue,issue,positive,positive,neutral,neutral,positive,positive
653883210,"I am going to close this issue out because it is stale. @Samuelsar777 if you are using a colab notebook due to not having a GPU, you will be glad to know that the voice cloning toolbox no longer requires a GPU. Please pull the latest code and give it a try!",going close issue stale notebook due glad know voice toolbox longer please pull latest code give try,issue,positive,positive,neutral,neutral,positive,positive
653883028,"@mmilk90 I wanted to let you know we have implemented CPU support in #366 so you do not need CUDA to run the voice cloning toolbox. So, exactly what you wanted! The setup process is a little bit easier now too. Please pull the latest and try again.",let know support need run voice toolbox exactly setup process little bit easier please pull latest try,issue,positive,positive,positive,positive,positive,positive
653882677,"Hi @oneduality ! Just letting you know that we updated the voice cloning toolbox recently with new features such as running without a GPU. I think it installs easier too. If you were not able to get it to work before, you are invited to pull the latest and try again.

I realize you likely invested a lot of time getting to your current point but the codebase has changed and we unfortunately do not have the time to troubleshoot errors that occurred on an old version. I hope you will understand.",hi know voice toolbox recently new running without think easier able get work pull latest try realize likely lot time getting current point unfortunately time old version hope understand,issue,negative,positive,positive,positive,positive,positive
653882296,"Hi @XxPro1445GamerxX . As @CorentinJ said, you need to write a synthesizer script that runs at the top level of the repo. Take the synthesizer code you need from demo_cli.py . If what you need is not found in that file, you can: 1) write some kind of interface to allow you to interact with it from the root level, or 2) rewrite the synthesizer to be standalone.

Unfortunately we do not have the time to provide support for doing this, but best of luck getting the synthesizer to do what you need.",hi said need write synthesizer script top level take synthesizer code need need found file write kind interface allow interact root level rewrite synthesizer unfortunately time provide support best luck getting synthesizer need,issue,positive,positive,positive,positive,positive,positive
653881631,"@SaylorTwift I have good news for you! We made webrtcvad an optional package in #375, the voice cloning toolbox will now run without it. Please pull the latest version of the repository and try again.",good news made optional package voice toolbox run without please pull latest version repository try,issue,negative,positive,positive,positive,positive,positive
653881424,Feel free the reopen this issue if you experience problems with the latest code.,feel free reopen issue experience latest code,issue,positive,positive,positive,positive,positive,positive
653881374,Hi there @Stellarize . Would you kindly pull the latest version of the repo and try again? We have updated the codebase quite a bit in the last few weeks and are unable to troubleshoot errors on an old version of the toolbox.,hi would kindly pull latest version try quite bit last unable old version toolbox,issue,negative,positive,positive,positive,positive,positive
653881015,I fixed the command on the wiki for running the encoder training script. Someone reported a similar issue in #262.,fixed command running training script someone similar issue,issue,negative,positive,neutral,neutral,positive,positive
653880608,"@rdrlima It looks like you have 2 environments and they're causing a conflict with each other:
`C:\Users\Rodrigo\AppData\Roaming\Python\Python37\`
`C:\Users\Rodrigo\AppData\Local\Programs\Python\Python37\`

Please check your `PATH` and remove one of the environments. Of course the environment you keep still needs to have all of the requirements.

Very similar issue described here: https://github.com/pytorch/pytorch/issues/27693",like causing conflict please check path remove one course environment keep still need similar issue,issue,negative,neutral,neutral,neutral,neutral,neutral
653879896,"Possible solutions:
1. Decrease the batch size so it fits in GPU memory.
2. Look at the modifications for low mem and implement something similar for training. (Huge performance hit)
3. Pull the latest code and train on CPU. (Also huge performance hit)",possible decrease batch size memory look low mem implement something similar training huge performance hit pull latest code train also huge performance hit,issue,negative,positive,positive,positive,positive,positive
653879531,You can run the main repo without CUDA support now. It is slower on CPU but still quite usable by my experience. Please pull the latest and try again.,run main without support still quite usable experience please pull latest try,issue,negative,positive,positive,positive,positive,positive
653879106,"@randyadams2000 The codebase has changed quite a bit in the last few weeks, it is not worthwhile for us to troubleshoot an issue reported 8 months ago. May I ask that you please pull the latest and see if your error is reproducible? Most likely it will just work. Good luck!",quite bit last u issue ago may ask please pull latest see error reproducible likely work good luck,issue,positive,positive,positive,positive,positive,positive
653878758,I don't mean to discourage anyone from using Kaggle Notebook with this repo. It should work. We just cannot provide setup support and troubleshooting for issues that may be Kaggle-specific.,mean discourage anyone notebook work provide setup support may,issue,negative,negative,negative,negative,negative,negative
653878498,"Hello @smmiSlam , thank you for reporting this issue. Unfortunately we cannot support Kaggle Notebook as a platform (we already get more questions than we can handle for Windows, macOS, Linux, and Google colab).

I would like to mention that we have recently enabled CPU support for the toolbox so you should be able to run on most computers now. The GPU is no longer required. Please pull the latest on your computer and try it out!",hello thank issue unfortunately support notebook platform already get handle would like mention recently support toolbox able run longer please pull latest computer try,issue,positive,positive,positive,positive,positive,positive
653875644,"@hawkmoon77 It sounds like what you want to do is a **style transfer** where you take a speaker embedding and apply it to an existing speech wav to change the voice. I touch on it briefly here: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/251#issuecomment-653851389

Let's look at the current workflow to understand the different parts:
C-1. file1.wav --> speaker\_encoder (inference) --> embedding
C-2. embedding + text --> synthesizer (inference) --> mel spectrogram (with the voice of the speaker in file1.wav)
C-3. mel spectrogram --> vocoder (inference) --> output.wav

Now let's look at your proposed workflow:
P-1. file1.wav --> speaker\_encoder (inference) --> embedding  (NO CHANGE)
P-2. embedding + file2.wav --> synthesizer (inference) --> mel spectrogram (of file2.wav modified to have the voice of the speaker in file1.wav)
P-3. mel spectrogram --> vocoder (inference) --> output.wav

As I understand it, it's not really possible within the framework of the repo because the synthesizer is designed to text as input and not a wav file. To train your proposed synthesizer model (P-2) you would need a large corpus with thousands of different voices saying the exact same set of phrases. And at that point it's not really a synthesizer but rather a voice transfer engine. (Also, at that point you might combine P-2 and P-3 into a single step and generate the output wavs directly.) Feasible and in some respects likely easier than accomplishing the same thing with text-to-speech, but the lack of an appropriately large and diverse dataset seems to be the obstacle here.

Disclaimer: I am new to this, so take all of the above with a grain of salt.",like want style transfer take speaker apply speech change voice touch briefly let look current understand different inference text synthesizer inference mel spectrogram voice speaker mel spectrogram inference let look inference change synthesizer inference mel spectrogram voice speaker mel spectrogram inference understand really possible within framework synthesizer designed text input file train synthesizer model would need large corpus different saying exact set point really synthesizer rather voice transfer engine also point might combine single step generate output directly feasible likely easier thing lack appropriately large diverse obstacle disclaimer new take grain salt,issue,negative,positive,neutral,neutral,positive,positive
653872830,"@CorentinJ Can you explain if there are any differences between the models used for your Youtube demo, and the pretrained models released on the wiki page? Would like to put any speculation to rest.

In #197 , it is noticed that you used a different vocoder called ""gen_s_mel_raw"" for the video but I don't think that is it.",explain used page would like put speculation rest used different video think,issue,negative,neutral,neutral,neutral,neutral,neutral
653872430,"> Whenever I use the toolbox to make a voice, it sounds not even close to being as good as the demo video was. 

Closing this as a duplicate of #162 , @Traincraft101 good observation on the pretrained vocoder as `gen_s_mel_raw` though I am doubtful that the entire performance difference could be attributed to the vocoder.",whenever use toolbox make voice even close good video duplicate good observation though doubtful entire performance difference could,issue,positive,positive,positive,positive,positive,positive
653871381,"### Data
Before launching synthesizer_preprocess_embeds.py (on LibriSpeech train-clean-100 and 360):
```
$ free -h
              total        used        free      shared  buff/cache   available
Mem:            62G        874M         60G         19M        827M         61G
Swap:          979M          0B        979M
```

Immediately after launching synthesizer_preprocess_embeds.py:
```
$ free -h
              total        used        free      shared  buff/cache   available
Mem:            62G        8.6G         52G         67M        1.6G         53G
Swap:          979M          0B        979M
```

At 35% completion (about 40 min on my computer):
```
$ free -h
              total        used        free      shared  buff/cache   available
Mem:            62G        8.8G         23G         67M         30G         52G
Swap:          979M          0B        979M
```

### Conclusion
It looks like the data is being cached as it is loaded/being processed, causing free memory to decrease. The cache can be freed for other purposes if needed, so it is not a serious issue. (See: https://www.linuxatemyram.com/ ) Anyone with data is welcome to reopen this issue.",data free total used free available mem swap immediately free total used free available mem swap completion min computer free total used free available mem swap conclusion like data causing free memory decrease cache freed serious issue see anyone data welcome reopen issue,issue,positive,positive,positive,positive,positive,positive
653869733,"Thank you for pioneering CPU support on this project @ByPort .

@pusalieth has implemented an elegant solution in #366 which provides CPU support as a fallback if necessary. Training works on CPU too! It is implemented in the main repo so I would like to suggest that everyone use that instead so we can continue developing it together.",thank support project elegant solution support fallback necessary training work main would like suggest everyone use instead continue together,issue,positive,positive,positive,positive,positive,positive
653868568,"To everyone in the thread, sincere thanks for taking the time to share what has worked for you and to help others get the toolbox working.

However, the advice in this issue is now outdated due to code changes made in the last few weeks so I will be closing it. I am hopeful that the changes will make it easier to install since we've dropped GPU as a hard requirement, and made the webrtcvad package optional (it is difficult to install on Windows).

If you want my suggestion please go here. It is not the quickest way but I believe it is more robust. https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/225#issuecomment-653850695

",everyone thread sincere thanks taking time share worked help get toolbox working however advice issue outdated due code made last hopeful make easier install since hard requirement made package optional difficult install want suggestion please go way believe robust,issue,positive,negative,neutral,neutral,negative,negative
653867964,Feel free to reopen the issue if you continue to not get any audio. The toolbox should be a little more helpful and provide a warning (in the terminal) if your output device does not support the sample rate (16 kHz).,feel free reopen issue continue get audio toolbox little helpful provide warning terminal output device support sample rate,issue,positive,positive,positive,positive,positive,positive
653867831,@ghost Try pulling the latest version of the code and see if the issue persists. I believe that it is likely related to #390 which we merged in the last day. So hopefully fixed now.,ghost try latest version code see issue believe likely related last day hopefully fixed,issue,negative,positive,positive,positive,positive,positive
653867611,"It is due to using a version of tensorflow that is too old, or too new (>=2.0).

You need to be using tensorflow 1.15 with the very latest version of the repo. Please pull the latest and repeat the installation steps. I am confident that you won't be getting the same error.",due version old new need latest version please pull latest repeat installation confident wo getting error,issue,negative,positive,positive,positive,positive,positive
653867286,"Hi there @applejack1063 . You should also ensure that the pretrained model is located at encoder/saved_models with respect to the location that demo_cli.py, or demo_toolbox.py is located.

There is not enough info to troubleshoot further, please provide the error message and full traceback, and reopen the issue.",hi applejack also ensure model respect location enough please provide error message full reopen issue,issue,positive,positive,positive,positive,positive,positive
653865202,"@hnjiakai Please reopen this issue if @royshil 's suggestion doesn't fix the problem for you. Though I would make a slight modification by doing an export:
```
export CUDA_VISIBLE_DEVICES=0
python3 demo_cli.py
```",please reopen issue suggestion fix problem though would make slight modification export export python,issue,negative,negative,negative,negative,negative,negative
653864897,Closing this as a duplicate of #227 . Thank you for reporting this issue @morangeman . I also find this to be a problem.,duplicate thank issue also find problem,issue,negative,neutral,neutral,neutral,neutral,neutral
653864338,"@philip93dk Please reopen the issue after doing the following, I would like more information on this error so we can troubleshoot it:
1. Pull the latest code from the repo
2. Provide the full traceback that accompanies the error message and reopen the issue.
    * If the error doesn't occur with the latest code then that would be good to know too!",please reopen issue following would like information error pull latest code provide full error message reopen issue error occur latest code would good know,issue,negative,positive,positive,positive,positive,positive
653863979,"Hi there @rkjoan . Thanks for your thoughts and feedback. I've responded to your questions below.

> I understand that commas and periods don't work, but in the demonstration video it was mentioned that line breaks are a way to get around this for now... however that's done in the toolbox application. How would it be done in code?

https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/49d4759503163a5133c2ac815e4a25eea9570fa2/toolbox/__init__.py#L171-L176

Basically splits up the text input to the synthesizer using `\n` as a delimiter and then concatenates the output into a single spectrogram for the vocoder.

> On top of this, how could I improve the voice in colab? In regards to training, it's mentioned that a decent session requires around 500gb or more... since I don't exactly have that in colab, is there another way to go about doing this?

You need better pretrained models. I'm not aware of any that perform better than Corentin's models. Another way is to use Amazon Sagemaker for training, though it is a paid solution. See #398 .

> I've tried the code with the input being longer than 10 seconds, but apparently if the input is more than 10 seconds or so the voice seems more jittery than it would be if it were capped at 10 seconds. 

This seems to be a similar issue to #347 . I don't see a need to keep this issue open so I will be closing it. If you want to continue the conversation feel free to reopen the issue.",hi thanks feedback understand work demonstration video line way get around however done toolbox application would done code basically text input synthesizer delimiter output single spectrogram top could improve voice training decent session around since exactly another way go need better aware perform better another way use training though solution see tried code input longer apparently input voice jittery would capped similar issue see need keep issue open want continue conversation feel free reopen issue,issue,positive,positive,positive,positive,positive,positive
653862393,"A GPU is no longer required to run the toolbox. Please try again after pulling the latest code and ensuring you have the packages in requirements.txt. The code has changed a bit recently and I don't expect we have much to gain by troubleshooting this issue, so I am closing it.",longer run toolbox please try latest code code bit recently expect much gain issue,issue,positive,positive,positive,positive,positive,positive
653861211,"If you compile it, you will need to add it to your path using `export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/path/to/the/compiled/portaudio/lib` before launching the toolbox.",compile need add path export toolbox,issue,negative,neutral,neutral,neutral,neutral,neutral
653860748,"Root cause appears to be using tensorflow 2.0 when the repo requires 1.x. The suggestion is to pull the latest version of the code, reinstall the requirements and try again. The current toolbox uses tensorflow 1.15.",root cause suggestion pull latest version code reinstall try current toolbox,issue,negative,positive,positive,positive,positive,positive
653860530,"Duplicate of #178, root cause is using tensorflow 2.0 when the repo requires 1.x. The suggestion is to pull the latest version of the code, reinstall the requirements and try again. The current toolbox uses tensorflow 1.15.",duplicate root cause suggestion pull latest version code reinstall try current toolbox,issue,negative,positive,positive,positive,positive,positive
653859038,"I think the colab notebook comes close. With the recent additions for CPU support I think a web version is becoming more feasible. Is your vision to clone the repo, point a web browser at some local file and have it work without installing anything at all? Or have someone operate a website where this is provided as a service?",think notebook come close recent support think web version becoming feasible vision clone point web browser local file work without anything someone operate provided service,issue,positive,positive,positive,positive,positive,positive
653858033,It's nice to see you all found a workaround. We formally resolved this problem in #371 . Closing this issue.,nice see found formally resolved problem issue,issue,negative,positive,positive,positive,positive,positive
653857748,"> 1. What will i require to create my own english dataset to train the voice cloning model, other than audio and transcript files?

You will need to prepare an alignment file, see this for the format: https://github.com/CorentinJ/librispeech-alignments#format-txt-alignments

Personally, I think it would be easiest to make your dataset look like LibriSpeech, that way you should be able to reuse the existing preprocessing and training code as written. But I haven't tried training with a custom dataset.",require create train voice model audio transcript need prepare alignment file see format personally think would easiest make look like way able reuse training code written tried training custom,issue,positive,positive,positive,positive,positive,positive
653857462,"@bmccallister Good idea on the collection of pretrained models, it inspired me to start #400. I have no control whether it will get pinned, but it is a start.

As for the issues you raised about the quality of output if there is too little/much reference text, I will prefer to track that in specific issues. #53 (long gaps in the middle of synthesized mel for a short text) and #347 (too fast when a lot of text). Good ideas and feedback here, but will close this as it duplicates more specific issues.",good idea collection inspired start control whether get pinned start raised quality output reference text prefer track specific long middle mel short text fast lot text good feedback close specific,issue,positive,positive,positive,positive,positive,positive
653856775,"@migueltopsp This same issue is seen in #178, where the root cause is using tensorflow 2.0 when the repo requires 1.x. The suggestion is to pull the latest version of the code, reinstall the requirements and try again. The current toolbox uses tensorflow 1.15.",issue seen root cause suggestion pull latest version code reinstall try current toolbox,issue,negative,positive,positive,positive,positive,positive
653855758,"Someone clicked ""reference in new issue"" on https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/30#issuecomment-508381648

There is nothing actionable here so this issue is being closed.",someone reference new issue nothing actionable issue closed,issue,negative,positive,neutral,neutral,positive,positive
653855505,"Can you please pull the latest code in the repo and try again? We currently require tensorflow 1.15 and the GPU is optional. Now that we have CPU support it is a lot easier to get the toolbox working.

I'm going to close this issue as it is stale. We haven't solved this particular problem but at least you won't have to make the tensorflow and CUDA versions line up.",please pull latest code try currently require optional support lot easier get toolbox working going close issue stale particular problem least wo make line,issue,positive,negative,neutral,neutral,negative,negative
653854015,"I'm going to close this issue since there is a fork that provides Chinese support. From here: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/30#issuecomment-629979383

@KuangDD wrote:
> 我弄了一个中文版本的Synthesizer，对中文支持良好，提供Demo模型，可以参考一下。
> https://github.com/KuangDD/zhrtvc
> 
> 另外，
> 语音预处理可用：aukit
> https://github.com/KuangDD/aukit
> 
> 中文的音素方案可用：phkit
> https://github.com/KuangDD/phkit
> 
> 可用这两个工具处理模型外的事情，zhrtvc用aukit和phkit分别承担audio和text模块的工作。",going close issue since fork support wrote,issue,negative,neutral,neutral,neutral,neutral,neutral
653853415,"@daddybird292 Congratulations on installing webrtcvad on Windows, that is quite an accomplishment.
@GiuseppeGiacoppo You will be pleased to know that webrtcvad is no longer strictly required to run the toolbox, please pull the latest code and try again. The GPU is also not required anymore so installation should be much easier.",quite accomplishment know longer strictly run toolbox please pull latest code try also installation much easier,issue,positive,positive,positive,positive,positive,positive
653852334,"Please try again, pulling the latest updates from the repo and using our colab notebook: https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/demo_toolbox_collab.ipynb . We cannot provide support for code outside the repo. You can also try installing the toolbox on your own computer, the GPU is no longer required. I wrote some directions here: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/225#issuecomment-653850695

Please open a new issue if you experience any problems with the current version of the toolbox.",please try latest notebook provide support code outside also try toolbox computer longer wrote please open new issue experience current version toolbox,issue,positive,positive,positive,positive,positive,positive
653851837,This issue is resolved now that #390 is merged. Please pull the latest version of the code and try again.,issue resolved please pull latest version code try,issue,negative,positive,positive,positive,positive,positive
653851389,"Yes, there is another approach of using style transfer to impart characteristics of a reference audio to another audio. Here is an example of such a repo: https://github.com/mazzzystar/randomCNN-voice-transfer

It might involve fewer models but I am not convinced that it requires less training overall.",yes another approach style transfer impart reference audio another audio example might involve convinced le training overall,issue,positive,neutral,neutral,neutral,neutral,neutral
653850695,"Here is a relatively fool-proof way to get started.
1. Install Ubuntu 18.04 and connect to the internet. Use a virtual machine if you can't devote a computer to it.
2. Open a terminal and install Python 3.7 and the virtualenv package.
```
sudo apt install python3.7 python3-venv python3.7-venv
```
3. Install git and clone this repo:
```
sudo apt install git
git clone https://github.com/CorentinJ/Real-Time-Voice-Cloning
```
4. Set up a virtual environment and activate it:
```
cd Real-Time-Voice-Cloning
python37 -m venv env
source env/bin/activate
```
5. Install requirements
```
pip install torch
pip install -r requirements.txt
```
6. Launch the toolbox
```
python demo_toolbox.py
```",relatively way get install connect use virtual machine ca devote computer open terminal install python package apt install python install git clone apt install git git clone set virtual environment activate python source install pip install torch pip install launch toolbox python,issue,negative,positive,positive,positive,positive,positive
653830346,"This list is no longer maintained. The synthesizer models below will only work with an older version of the repo code. Use [this commit](https://github.com/CorentinJ/Real-Time-Voice-Cloning/tree/5425557efe30863267f805851f918124191e0be0) and these [install instructions](https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/642#issuecomment-768900542).

### English
| # | Author | Type | Link | Hparams | Description |
| - | ------ | ---- | ----------- | ------ | ---------- |
| 001 | @CorentinJ | All models | [wiki/Pretrained-models](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models) | Default | Original models |
| 002 | @blue-fish | Vocoder | https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/126#issuecomment-650531957 | Default | Add 700k steps to the 001 vocoder, fully compatible with original models. Voice quality same but better at reducing sound artifacts? https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/126#issuecomment-650549809 |
| 003 | @sberryman | All models | https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/126#issuecomment-536097366 | [Modified](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/126#issuecomment-573523609) | Highly trained encoder. Sound quality is good but cloned voices do not resemble reference audio. https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/126#issuecomment-604999848 |
| 004 | @mbdash | Synthesizer | https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/449#issuecomment-665645153 | Default | Trained to 200k steps on LibriTTS |
| 005 | @mbdash | Encoder | https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/458#issuecomment-677662104 | [Modified](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/458#issuecomment-671110894) | Trained on LibriSpeech, VCTK, CommonVoice |
| 006 | @blue-fish | Synthesizer | #538 | Default | Fewer gaps in output compared to original synthesizer model |

### Multi-language
| # | Author | Type | Link | Hparams | Description |
| - | ------ | ---- | ----------- | ------ | ---------- |
| 101 | @sberryman | Encoder | https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/126#issuecomment-535936275 | [Modified](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/126#issuecomment-573523609) | Highly trained encoder on mixed language set. Suitable for training a synthesizer or vocoder in any language?   |

### Chinese
| # | Author | Type | Link | Hparams | Description |
| - | ------ | ---- | ----------- | ------ | ---------- |
| 201 | @KuangDD | All models | https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/30#issuecomment-689799718 | [Modified](https://github.com/KuangDD/zhrtvc/tree/932d6e334c54513b949fea2923e577daf292b44e) | Fork, see https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/30#issuecomment-629979383 |

### Swedish
| # | Author | Type | Link | Hparams | Description |
| - | ------ | ---- | ----------- | ------ | ---------- |
| 301 | @ViktorAlm | Synthesizer<br> Vocoder | #257 | Default | To be used with encoder model from 001. Minor [code changes](https://github.com/blue-fish/Real-Time-Voice-Cloning/compare/054f16e...blue-fish:400_pretrained_swe_301) are needed for this to work. | 

### German
| # | Author | Type | Link | Hparams | Description |
| - | ------ | ---- | ----------- | ------ | ---------- |
| 401 | @padmalcom | All models | https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/571#issuecomment-716054792 | Default | Trained on female voices in M-AILABS German dataset | ",list longer synthesizer work older version code use commit install author type link description default original default add fully compatible original voice quality better reducing sound highly trained sound quality good resemble reference audio synthesizer default trained trained synthesizer default output original synthesizer model author type link description highly trained mixed language set suitable training synthesizer language author type link description fork see author type link description synthesizer default used model minor code work german author type link description default trained female german,issue,positive,positive,positive,positive,positive,positive
653829054,"> So, is this framework can not trained only with one speaker's audios in the steps of Synthesizer and Vocoder?

@Honghe You are correct. It is necessary to use multiple speakers to train the synthesizer and vocoder. The training process teaches the synthesizer and vocoder how the embeddings from the speaker encoder (left side of diagram) relate to the unique characteristics of each speaker used to train the synthesizer and vocoder.

If a single speaker is used to train the synthesizer and vocoder then it is not able to learn this information. I hope this explanation is satisfactory. If you have follow-up questions please reopen this issue.",framework trained one speaker synthesizer correct necessary use multiple train synthesizer training process synthesizer speaker left side diagram relate unique speaker used train synthesizer single speaker used train synthesizer able learn information hope explanation satisfactory please reopen issue,issue,positive,positive,positive,positive,positive,positive
653828594,@ryan0930 If you still have the problem after trying the suggestion from @YukihimeX please reopen the issue. I am confident that it should be resolved.,still problem trying suggestion please reopen issue confident resolved,issue,positive,positive,positive,positive,positive,positive
653828478,"As mentioned in #325 tensorflow-gpu is not provided for macOS. Fortunately, there has been a recent change to provide CPU support so the voice cloning toolbox will run on mac. Please pull the latest and update your requirements. Setup should be considerably easier now.",provided fortunately recent change provide support voice toolbox run mac please pull latest update setup considerably easier,issue,positive,positive,positive,positive,positive,positive
653828419,"> Thanks for contributing @matheusfillipe . By any chance did you get an invitation to join as a contributor so you can squash and merge your changes into the repo? If not I'll do it.

No I actually didn't receive any invitation so far. I guess you will have to do it :P",thanks chance get invitation join contributor squash merge actually receive invitation far guess,issue,positive,positive,positive,positive,positive,positive
653828303,Thanks for contributing @matheusfillipe . By any chance did you get an invitation to join as a contributor so you can squash and merge your changes into the repo? If not I'll do it.,thanks chance get invitation join contributor squash merge,issue,positive,positive,positive,positive,positive,positive
653823148,"@RaghunandanVenkatesh It is not really possible. There are three steps to voice cloning:
1. Calculate the embedding from the voice sample
2. Synthesize the mel spectrogram from the embedding + text
3. Convert the mel spectrogram to a wave file using the vocoder

Even if you reuse the embedding from step 1, it turns out that steps 2 and 3 are computationally expensive and must be performed each time the text is changed.",really possible three voice calculate voice sample synthesize mel spectrogram text convert mel spectrogram wave file even reuse step turn expensive must time text,issue,negative,negative,negative,negative,negative,negative
653822973,"I think this is specific to the vocoder algorithm being used. In the toolbox, you can select Griffin-Lim as the vocoder and it will be much faster with a serious degradation in quality. I don't know how to make the vocoder faster if you are already getting GPU acceleration. A different algorithm perhaps? This one is already reasonably fast and has good quality for a vocoder.",think specific algorithm used toolbox select much faster serious degradation quality know make faster already getting acceleration different algorithm perhaps one already reasonably fast good quality,issue,negative,positive,positive,positive,positive,positive
653822764,"@sam10001 We have recently added CPU support, please pull the latest code from the repo and try it out! No NVIDIA GPU required.",sam recently added support please pull latest code try,issue,positive,positive,positive,positive,positive,positive
653822592,"Can you please pull the latest code from the repo, update to the current `requirements.txt` and try again? We have added CPU support and made changes which should make the setup easier. Reopen this issue if you continue to experience this issue with the latest code.",please pull latest code update current try added support made make setup easier reopen issue continue experience issue latest code,issue,positive,positive,positive,positive,positive,positive
653822372,Thank you @natravedrova for answering the question about the collab notebook. You can also run it on your CPU as we have recently made the GPU optional. Pull the latest code and try it out!,thank question notebook also run recently made optional pull latest code try,issue,negative,positive,positive,positive,positive,positive
653822237,"I am going to presume this was resolved with identification of the tensorflow issues, but if not reopen the issue and provide the error message and full traceback as requested.",going presume resolved identification reopen issue provide error message full,issue,negative,positive,positive,positive,positive,positive
653822184,"@jgraf451 I'm sorry you were unable to get it set up. We have recently improved the code to add CPU support and make a difficult-to-install dependency optional. You are invited to pull the latest and try again. Having installed both GPU and CPU versions, the latter is significantly easier in my experience since you don't need to get the dependencies to line up with your GPU drivers.

If you continue to experience a problem with setup please reopen the issue or open a new one.",sorry unable get set recently code add support make dependency optional pull latest try latter significantly easier experience since need get line continue experience problem setup please reopen issue open new one,issue,positive,positive,neutral,neutral,positive,positive
653821982,@Ixiplious I'm glad that it's working for you now. Thanks for sharing your solution.,glad working thanks solution,issue,positive,positive,positive,positive,positive,positive
653821937,"Can you please pull the latest code from the repo, update your requirements and try again? The code has changed a bit recently making it hard to troubleshoot. If you continue to have a problem please reopen this issue and paste the error message with full traceback like you did above.",please pull latest code update try code bit recently making hard continue problem please reopen issue paste error message full like,issue,negative,positive,positive,positive,positive,positive
653821823,This is not a support channel for resemble.ai . Though I will add that I made an attempt to find some customer support link and was not successful.,support channel though add made attempt find customer support link successful,issue,positive,positive,positive,positive,positive,positive
653821478,You need to install libsndfile to make it work. http://www.mega-nerd.com/libsndfile/#Download,need install make work,issue,negative,neutral,neutral,neutral,neutral,neutral
653821078,"@MahmoudAliEng That is in the encoder folder of the repository. Both scripts for training and inference are provided.

If you have a more specific question that is not answered by this please reopen the issue.",folder repository training inference provided specific question please reopen issue,issue,negative,neutral,neutral,neutral,neutral,neutral
653820986,You need to install tensorflow 1.15 . I would recommend pulling the latest code from the repo while you are at it. If this does not solve your problem please reopen the issue.,need install would recommend latest code solve problem please reopen issue,issue,negative,positive,positive,positive,positive,positive
653820896,"Ok, I am going to presume that you just want to try out the toolbox. If this is the case then pull the latest code with CPU support, update your requirements and you should be good to go without figuring out the CUDA driver/runtime issue.

If you need the GPU speedup then I am afraid that this kind of troubleshooting is outside the scope of the issues here. You would need to find a more appropriate support channel for that.",going presume want try toolbox case pull latest code support update good go without issue need afraid kind outside scope would need find appropriate support channel,issue,positive,positive,positive,positive,positive,positive
653820795,@XxBlackBirdxX If you are still having an issue please provide a more detailed description of your problem and we will try to help.,still issue please provide detailed description problem try help,issue,negative,positive,positive,positive,positive,positive
653820696,"Can you please try pulling the latest code from the repo and see it resolves your issue? I have no idea where this could be coming from, there is not enough info to troubleshoot.

If there is still a problem, please reopen the issue and provide the error message and full traceback from the terminal.",please try latest code see issue idea could coming enough still problem please reopen issue provide error message full terminal,issue,negative,positive,positive,positive,positive,positive
653820597,"Hi @Ebrahemots , at the time you installed the toolbox a GPU was required. CPU is now supported so you can try pulling the latest and it should be compatible. Just make sure you also update your requirements as we are now up to tensorflow==1.15. Hope this resolves your issue, if it does not feel free to reopen.",hi time toolbox try latest compatible make sure also update hope issue feel free reopen,issue,positive,positive,positive,positive,positive,positive
653820464,"You are getting the error message because your user does not have admin rights on the system, try creating a virtual environment or using the --user option with pip. See this for more info: https://stackoverflow.com/a/31512491",getting error message user system try virtual environment user option pip see,issue,negative,neutral,neutral,neutral,neutral,neutral
653820309,Thanks for reporting this issue. I don't have an issue installing from requirements.txt so I will assume that a more recent PyQt5 has fixed the bug that you discovered. If there is a reproducible problem with the current PyQt5 please reopen this with a full description of the error.,thanks issue issue assume recent fixed bug discovered reproducible problem current please reopen full description error,issue,negative,positive,positive,positive,positive,positive
653820180,webrtvcad is no longer required as of #375 so everyone who got stuck on this step is invited to pull the latest from the repo and try again.,longer everyone got stuck step pull latest try,issue,negative,positive,positive,positive,positive,positive
653820108,"I am not sure which set of instructions you are following, but a similar issue was also reported in #286 . Given the recent update to support CPU, I think you should have a much easier time getting the toolbox to work. Please pull the latest and try again.",sure set following similar issue also given recent update support think much easier time getting toolbox work please pull latest try,issue,positive,positive,positive,positive,positive,positive
653819919,"@berzi There is a helpful chart in the quick start at https://pytorch.org/ which directs you to download the appropriate version of torch to match your CUDA version.

The repo supports CPU now, so you don't have to go through the trouble to match. Please pull the latest and try again.",helpful chart quick start appropriate version torch match version go trouble match please pull latest try,issue,negative,positive,positive,positive,positive,positive
653819791,"You can choose any of them but it is likely the embedding that is calculated will vary slightly. This means your results will depend on which utterance is selected as your reference.

If you have additional questions please reopen this issue.",choose likely calculated vary slightly depend utterance selected reference additional please reopen issue,issue,negative,negative,neutral,neutral,negative,negative
653819695,"I am closing this issue as it has been stale. If you would like help, please pull the latest code from the repo and try again. Reopen this issue, and provide the traceback and error message if the problem occurs.",issue stale would like help please pull latest code try reopen issue provide error message problem,issue,negative,neutral,neutral,neutral,neutral,neutral
653819547,Please pull the latest code from the repo and see if you continue to have this issue. We recently made changes such as adding CPU support.,please pull latest code see continue issue recently made support,issue,positive,positive,positive,positive,positive,positive
653819411,You might need to install a backend such as ffmpeg for use with audioread. That performs the conversion to other formats. This comes up rather frequently so we might need to catch the exception and print a more helpful error message.,might need install use conversion come rather frequently might need catch exception print helpful error message,issue,negative,positive,neutral,neutral,positive,positive
653819254,I cannot find any code to this effect in the repo. More info would be needed to troubleshoot. Could you please try pulling the latest code and see if this problem still exists? Tensorflow 1.15 is required. Please reopen the issue if you have more info.,find code effect would could please try latest code see problem still please reopen issue,issue,negative,positive,positive,positive,positive,positive
653818919,"Yes, I can confirm that the variation in the Tacotron output is a feature. In issue #384 , I am trying to find a way to control the output to facilitate benchmarking. Thanks for submitting the bug report.",yes confirm variation output feature issue trying find way control output facilitate thanks bug report,issue,positive,positive,positive,positive,positive,positive
653818590,Torch installation is done separately for compatibility reasons surrounding GPU support. Depending on the version of NVIDIA drivers a different pytorch version is needed. It is not something that can be automated into requirements.txt.,torch installation done separately compatibility surrounding support depending version different version something,issue,negative,neutral,neutral,neutral,neutral,neutral
653818455,"Please use tensorflow==1.15 , that is the only version that will work currently.",please use version work currently,issue,negative,neutral,neutral,neutral,neutral,neutral
653818422,"We made many changes recently which make the GPU optional, as well as the webrtcvad package which is a nightmare to install on windows. Please pull the latest and try again.",made many recently make optional well package nightmare install please pull latest try,issue,positive,positive,positive,positive,positive,positive
653818218,If the above suggestions did not help you can try looking at librosa/librosa#219. Closing this issue as it does not strictly relate to the voice cloning toolbox.,help try looking issue strictly relate voice toolbox,issue,negative,neutral,neutral,neutral,neutral,neutral
653818124,"I've got great news for you, we recently added support for CPU inference (and even training)! It even works on macOS. Please pull the latest from the repo and try again.",got great news recently added support inference even training even work please pull latest try,issue,positive,positive,positive,positive,positive,positive
653818022,@Tomotori is correct. I am closing this issue as the tensorflow version issue has been solved by updating requirements.txt in #366 and more recently in #399.,correct issue version issue recently,issue,negative,neutral,neutral,neutral,neutral,neutral
653817902,"The toolbox supports CPU now, so please try pulling the latest and try again.",toolbox please try latest try,issue,negative,positive,positive,positive,positive,positive
653817647,Closing this item because these issues are not strictly related to this repository.,item strictly related repository,issue,negative,neutral,neutral,neutral,neutral,neutral
653817193,"We recently added CPU support, please pull the latest changes and try again. Feel free to reopen this issue if that does not solve your problem with using the toolbox.",recently added support please pull latest try feel free reopen issue solve problem toolbox,issue,positive,positive,positive,positive,positive,positive
653816795,"Closing issue as the ""no backend"" error is an issue with the user's configuration and not the repo code. If this did not fix your problem, please reopen the issue.",issue error issue user configuration code fix problem please reopen issue,issue,negative,neutral,neutral,neutral,neutral,neutral
653800190,"Starting with a default install that has all the pretrained models, I overwrite `encoder/saved_models/pretrained.pt` with your `my_run.pt`. Then I can run demo_cli.py with no problems. The speech is barely intelligible since the synthesizer wasn't trained on it, but at least there are no errors.",starting default install overwrite run speech barely intelligible since synthesizer trained least,issue,negative,negative,neutral,neutral,negative,negative
653799468,"If you get that error message, you may need to upgrade pip with `python -m pip install –upgrade pip`.

I'm going to close this as questions about installing dependencies are out of scope for this repo.",get error message may need upgrade pip python pip install pip going close scope,issue,negative,neutral,neutral,neutral,neutral,neutral
653797594,"If you want it to sound like an audiobook, use the pretrained models based on LibriSpeech.
If you want it to sound like a video game, you need to train on an appropriate dataset. 

When I say train, I mean going through the process on [this page](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training). A very brief search has found these resources:
https://languageatplay.de/vgcost-corpus-video-game-text-speech/
https://judithvanstegeren.com/blog/2019/video-game-corpora.html

Likely for copyright reasons, only the written text is available... you will need to figure out how to get the speech data and prepare the dataset which is a very tedious undertaking. The time required will be weeks, if not months, to accomplish what you are proposing.",want sound like use based want sound like video game need train appropriate say train mean going process page brief search found likely copyright written text available need figure get speech data prepare tedious undertaking time accomplish,issue,positive,positive,neutral,neutral,positive,positive
653795991,Not enough info to troubleshoot. Please provide more details about your problem and reopen the issue.,enough please provide problem reopen issue,issue,negative,neutral,neutral,neutral,neutral,neutral
653795530,"I am going to close this because CPU support can be used as a fallback, even for training. The inability to use older GPUs is a limitation of the dependencies (PyTorch, Tensorflow). If you need GPU acceleration, it will be required to upgrade your GPU.",going close support used fallback even training inability use older limitation need acceleration upgrade,issue,negative,positive,positive,positive,positive,positive
653795478,"@blue-fish: do you have a version that passes synthesizer training, and also can run the demo apps?",version synthesizer training also run,issue,negative,neutral,neutral,neutral,neutral,neutral
653793394,"I've confirmed that this occurs when the alignments are not present. If you have the alignments and still experience the problem, please reopen the issue.",confirmed present still experience problem please reopen issue,issue,negative,positive,positive,positive,positive,positive
653792460,"Take a look at librosa/librosa#219, you need to install a backend. The person who had this same issue in #386 resolved this by installing ffmpeg.",take look need install person issue resolved,issue,negative,neutral,neutral,neutral,neutral,neutral
653792203,"I am not sure what happened with your copy of the repo, but you can see that it is referenced as `unidecode` with proper capitalization.

https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/f6fabeaf3effdf2f023db98b37bdabd1ec3d81e5/synthesizer/utils/cleaners.py#L14

Make your 14 of synthesizer/utils/cleaners.py match this and it will resolve the issue.",sure copy see proper capitalization make match resolve issue,issue,positive,positive,positive,positive,positive,positive
653791696,"Did you modify any hyperparameters or other files in the repo? If you want, you could upload your pretrained encoder and I can see if I also get the error message.

Also can you list your installed packages and their versions (`pip list` or `conda list`)?",modify want could see also get error message also list pip list list,issue,negative,neutral,neutral,neutral,neutral,neutral
653791370,Pleased that you were able to get it working @marshonhuckleberry ! I am going to close this issue now as it is resolved.,able get working going close issue resolved,issue,negative,positive,positive,positive,positive,positive
653790952,It is likely you are missing the LibriSpeech alignments which tell the training script which datasets are available to it. Please read the directions on this page closely: https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training,likely missing tell training script available please read page closely,issue,negative,positive,neutral,neutral,positive,positive
653790531,"I believe this can be closed as @CorentinJ has outlined a vision for continued development, and is allowing the community to provide contributions to the repo. See #364",believe closed outlined vision continued development community provide see,issue,negative,negative,neutral,neutral,negative,negative
653789969,"Yes, I can run both demo apps before starting encoder training using all
pretrained models. After finish encoder training, I am running issues with
running the demo apps.


On Sat, Jul 4, 2020 at 11:02 AM blue-fish <notifications@github.com> wrote:

> Have you ever successfully run demo_cli.py or demo_toolbox.py with the
> pretrained models in the wiki? Your 2nd message looks to be such an
> attempt. Try starting with a clean copy of the code and verify that
> demo_cli.py works with the pretrained models before switching to your own.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/383#issuecomment-653776137>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ACZ6P67Y5VFL4K5Z2LHK25TRZ476XANCNFSM4OJYJDPA>
> .
>
",yes run starting training finish training running running sat wrote ever successfully run message attempt try starting clean copy code verify work switching thread reply directly view,issue,positive,positive,positive,positive,positive,positive
653788017,"I'm also concerned that people are just doing the bare minimum to install, seeing a lot of the ""webrtcvad not found"" warnings in recent error logs.

Can we add a `requirements_gpu.txt` which contains the optional tensorflow packages and webrtcvad support?

So the installation procedure is:
```
pip install -r requirements.txt
pip install -r requirements_gpu.txt
```

And if it fails on the second one they can either try to get it to work, or just enjoy the toolbox using CPU.",also concerned people bare minimum install seeing lot found recent error add optional support installation procedure pip install pip install second one either try get work enjoy toolbox,issue,negative,positive,positive,positive,positive,positive
653785214,"Tensorflow-gpu is [no longer supported on mac](https://stackoverflow.com/q/44744737) so `pip install -r requirements.txt` will fail on tensorflow-gpu==1.15

However, tensorflow-gpu is no longer a hard requirement since we added CPU support. I'll submit a pull request to resolve this issue.",longer mac pip install fail however longer hard requirement since added support submit pull request resolve issue,issue,negative,negative,negative,negative,negative,negative
653784658,"Also for maintainability it might be best to make the Sagemaker version a distinct repo instead of a direct fork, and incorporate files from this repo as a submodule.",also might best make version distinct instead direct fork incorporate,issue,positive,positive,positive,positive,positive,positive
653783859,"Now that it runs on CPU, inference could run on the same `ml.t2.medium` instance you recommend for installation ($0.0464/hour for 2 vCPU + 4 GB mem). Though I would still recommend getting a GPU instance if playing with the toolbox for more than a few minutes.",inference could run instance recommend installation mem though would still recommend getting instance toolbox,issue,positive,neutral,neutral,neutral,neutral,neutral
653780558,"I am interested in the training use case. Use the repo locally on CPU or low-end GPU for development, then transfer to Sagemaker when everything is working.

In your README.md you mention using `ml.p2.xlarge` which costs $1.26/hour for (4 vCPU + 61 GB mem) + (K80 GPU with 12 GB). On the pricing page there is also an option of using `ml.g4dn.xlarge` which is $0.74/hour for (4 vCPU + 16 GB mem) + (T4 GPU with 16 GB). Which would be preferred here? ",interested training use case use locally development transfer everything working mention mem page also option mem would preferred,issue,negative,positive,positive,positive,positive,positive
653778438,"> @rustygentile Do you know if training would work with Sagemaker? That could also be another use case.

Yes, that would absolutely be possible. I might take that up now if there's enough interest.",know training would work could also another use case yes would absolutely possible might take enough interest,issue,positive,neutral,neutral,neutral,neutral,neutral
653778214,You might also want to check out #398 as an potential alternative if you continue to have issues with the setup.,might also want check potential alternative continue setup,issue,negative,neutral,neutral,neutral,neutral,neutral
653778040,"@blue-fish , hey, thanks for the feedback! I've submitted issue [#398](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/398). I'll be sure to checkout the latest changes. I can probably get the cost of running this in the cloud down too now that there's CPU support.",hey thanks feedback issue sure latest probably get cost running cloud support,issue,positive,positive,positive,positive,positive,positive
653777919,@rustygentile Do you know if training would work with Sagemaker? That could also be another use case.,know training would work could also another use case,issue,negative,neutral,neutral,neutral,neutral,neutral
653777652,The [last issue](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/386#issuecomment-650557195) is a duplicate of #321 (no solution posted). If you were able to get past that step would you please share your solution?,last issue duplicate solution posted able get past step would please share solution,issue,positive,positive,neutral,neutral,positive,positive
653777241,"Seems like tensorflow version mismatch. Please pull the latest version of this repo and ensure you are running the version of tensorflow in requirements.txt (currently 1.15).

We changed that code in #366:
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/f6fabeaf3effdf2f023db98b37bdabd1ec3d81e5/synthesizer/models/modules.py#L92

Please reopen this issue if you continue to have problems after implementing the suggestions above.",like version mismatch please pull latest version ensure running version currently code please reopen issue continue,issue,positive,positive,positive,positive,positive,positive
653776137,Have you ever successfully run demo_cli.py or demo_toolbox.py with the pretrained models in the wiki? Your 2nd message looks to be such an attempt. Try starting with a clean copy of the code and verify that demo_cli.py works with the pretrained models before switching to your own.,ever successfully run message attempt try starting clean copy code verify work switching,issue,positive,positive,positive,positive,positive,positive
653775407,"Issue is stale and not a bug related to the code in this repo, closing at this time. @NumpyG feel free to reopen if this continues to be a problem.",issue stale bug related code time feel free reopen problem,issue,negative,negative,neutral,neutral,negative,negative
653775118,"Looks to be resolved, feel free to reopen if the issue persists.",resolved feel free reopen issue,issue,positive,positive,positive,positive,positive,positive
653774945,It also appears that the package requirements are not installed. Please reopen this issue if you continue to experience problems.,also package please reopen issue continue experience,issue,negative,neutral,neutral,neutral,neutral,neutral
653774015,#227 is also related (too few words and large gaps in output appear),also related large output appear,issue,negative,positive,positive,positive,positive,positive
653771399,"I should also add that the voice cloning code is not meant to consume pretrained models that have not been trained on this codebase. The Tacotron2 in this repo has a special modification to take the embedding from the speaker encoder, without it cloning a voice is not possible. The WaveRNN also needs to be trained using data from the modified Tacotron2.

Please take a look at [this](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/30#issuecomment-508381648) and hopefully it will make a little more sense.",also add voice code meant consume trained special modification take speaker without voice possible also need trained data please take look hopefully make little sense,issue,positive,positive,neutral,neutral,positive,positive
653769451,"It's not a straight copy of WaveRNN, you will need to make some modifications. For example, the problematic line of code in this repo is:

https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/ba1a78d869030f996c771fd2e9f244e57dfd2e8d/vocoder/models/fatchord_version.py#L4",straight copy need make example problematic line code,issue,negative,positive,positive,positive,positive,positive
653690349,"Take a look at librosa/librosa#219, you may need to install a backend.

Edit: The person who had this same issue in #386 resolved this by installing ffmpeg on windows.",take look may need install edit person issue resolved,issue,negative,neutral,neutral,neutral,neutral,neutral
653688871,"Sure, here you go:
```
Traceback (most recent call last):
  File ""C:\Users\HP Pavilion\AppData\Local\Programs\Python\Python37\lib\site-packages\librosa\core\audio.py"", line 129, in load
    with sf.SoundFile(path) as sf_desc:
  File ""C:\Users\HP Pavilion\AppData\Local\Programs\Python\Python37\lib\site-packages\soundfile.py"", line 629, in __init__
    self._file = self._open(file, mode_int, closefd)
  File ""C:\Users\HP Pavilion\AppData\Local\Programs\Python\Python37\lib\site-packages\soundfile.py"", line 1184, in _open
    ""Error opening {0!r}: "".format(self.name))
  File ""C:\Users\HP Pavilion\AppData\Local\Programs\Python\Python37\lib\site-packages\soundfile.py"", line 1357, in _error_check
    raise RuntimeError(prefix + _ffi.string(err_str).decode('utf-8', 'replace'))
RuntimeError: Error opening 'D:\\cdfiles\\voicesamplecd.mp3': File contains data in an unknown format.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\HP Pavilion\Downloads\Real-Time-Voice-Cloning-master\toolbox\__init__.py"", line 77, in <lambda>
    func = lambda: self.load_from_browser(self.ui.browse_file())
  File ""C:\Users\HP Pavilion\Downloads\Real-Time-Voice-Cloning-master\toolbox\__init__.py"", line 119, in load_from_browser
    wav = Synthesizer.load_preprocess_wav(fpath)
  File ""C:\Users\HP Pavilion\Downloads\Real-Time-Voice-Cloning-master\synthesizer\inference.py"", line 111, in load_preprocess_wav
    wav = librosa.load(str(fpath), hparams.sample_rate)[0]
  File ""C:\Users\HP Pavilion\AppData\Local\Programs\Python\Python37\lib\site-packages\librosa\core\audio.py"", line 162, in load
    y, sr_native = __audioread_load(path, offset, duration, dtype)
  File ""C:\Users\HP Pavilion\AppData\Local\Programs\Python\Python37\lib\site-packages\librosa\core\audio.py"", line 186, in __audioread_load
    with audioread.audio_open(path) as input_file:
  File ""C:\Users\HP Pavilion\AppData\Local\Programs\Python\Python37\lib\site-packages\audioread\__init__.py"", line 116, in audio_open
    raise NoBackendError()
audioread.exceptions.NoBackendError
```",sure go recent call last file line load path file line file file line error opening file line raise prefix error opening file data unknown format handling exception another exception recent call last file line lambda lambda file line file line file line load path offset duration file line path file line raise,issue,negative,positive,neutral,neutral,positive,positive
653685728,Can you copy and paste the error message and traceback from the terminal? We need more info to troubleshoot the issue.,copy paste error message terminal need issue,issue,negative,neutral,neutral,neutral,neutral,neutral
653646224,"I would like to request someone look at this as well. Other tacotron implementations do not have this issue.

There is a chance it will improve if we upgrade to tensorflowTTS/tacotron2 (see #370) ... but I don't know how much of this is embedded in the pretrained models.",would like request someone look well issue chance improve upgrade see know much,issue,positive,positive,positive,positive,positive,positive
653642525,"@CorentinJ This PR resolves an issue where for some users, portaudio doesn't pick the intended output device leading to a lot of confusion. Audio devices are now validated on toolbox startup, and warnings logged if a device is not supported. It provides the user with an option to switch between valid devices for greater visibility and control.

This is what the toolbox UI looks like after the update. Sharing this so you can see the placement of the new ""audio input"" and ""audio output"" elements.
<img width=""1435"" alt=""screenshot"" src=""https://user-images.githubusercontent.com/67130644/86491189-eeb92100-bd1e-11ea-9d82-22691b5ce24b.png"">
",issue pick intended output device leading lot confusion audio toolbox logged device user option switch valid greater visibility control toolbox like update see placement new audio input audio output,issue,negative,positive,positive,positive,positive,positive
653635813,I'm going to close this issue since #395 indicates the original issue has been resolved. We will work through any further issues you discover over there.,going close issue since original issue resolved work discover,issue,negative,positive,positive,positive,positive,positive
653612608,"Thank you for responding and comments @CorentinJ @blue-fish This is a really cool project.

The initial pull request only has the wandb instrumentation. After that I saw this repo is not supported anymore. I wasn't hoping that the slackbot and twitter bot to be merged into this. I can do a separate pull request with only wandb instrumentation if anyone is interested. 

I did the twitterbot/slackbot as a fun experiment. I used the trained model you provided for the bots, but the results are okay. Does the voice have to be in the training dataset? I wanted to generate people's mentions on twitter into the voice of Stephen Fry. And I used 20 minutes of his voice. I don't know, the cloned voice seemed really monotonous, and isn't capturing the gaps between the words, it seems like the gaps also has some information regarding a persons voice? I don't know. I didn't really check out other voice cloning projects. Do you have any suggestions on getting better results?

Thank you.",thank really cool project initial pull request instrumentation saw twitter bot separate pull request instrumentation anyone interested fun experiment used trained model provided voice training generate people twitter voice fry used voice know voice really monotonous like also information regarding voice know really check voice getting better thank,issue,positive,positive,positive,positive,positive,positive
653533340,"@CorentinJ Requesting that this PR be closed without merging. Please see my review for rationale.

Another possibility is to start a wandb branch, but I don't think we should take on a feature if it's not going to be maintained.",closed without please see review rationale another possibility start branch think take feature going,issue,negative,negative,neutral,neutral,negative,negative
653526900,"As mentioned in the review #366 already resolved the tensorflow version issue so this PR can be closed without merge. I do think the notebook structure is a little better than the current one by performing the user voice recording in a different cell than the synthesis, so please feel free to submit a pull request without the tensorflow change.",review already resolved version issue closed without merge think notebook structure little better current one user voice recording different cell synthesis please feel free submit pull request without change,issue,positive,positive,positive,positive,positive,positive
653518465,"@CorentinJ I find this to be a slight improvement over the existing wording in README.md , why don't you make a decision on whether to merge it so we can close out the pull request.",find slight improvement wording make decision whether merge close pull request,issue,negative,negative,negative,negative,negative,negative
653514089,"Hi, I checked out your fork and what you've accomplished is neat. It's nicely documented too. But with CPU support added (#366) and webrtcvad now optional (#375) the major pain points are resolved.

May I suggest that you open an issue plugging your Sagemaker fork and we can refer people to it if they have difficulties with the installation?",hi checked fork accomplished neat nicely support added optional major pain resolved may suggest open issue plugging fork refer people installation,issue,positive,positive,positive,positive,positive,positive
653508794,@dathudeptrai I am new to TTS and don't have the expertise to make the changes you are describing. Would you be willing to point out exactly what needs to be changed? Or submit a pull request (working or not) to get us started?,new make would willing point exactly need submit pull request working get u,issue,negative,positive,positive,positive,positive,positive
653434574,"@blue-fish the most important thing is that we need to re-use the pretrained model here so converting the weight to be able to load on my Tacotron2 implementation is the right way :)). My implementation is 90% the same as the tacotron2 implementation here, just need modify some layer and parameter to replicate the model then we can ez to load the pretrained weight here to inference.",important thing need model converting weight able load implementation right way implementation implementation need modify layer parameter replicate model load weight inference,issue,negative,positive,positive,positive,positive,positive
653426847,@dathudeptrai Yes! That's much better than me trying to convert the existing code to Tensorflow2.,yes much better trying convert code,issue,positive,positive,positive,positive,positive,positive
653425170,@CorentinJ I think it's not hard to convert pretrained tacotron2 here to my tensorflow2 implementation since my implementation based on the tacotron2 code used here. ,think hard convert implementation since implementation based code used,issue,negative,negative,negative,negative,negative,negative
653417332,"@dathudeptrai cool, do go ahead, but remember that you'll have to ensure that the data compatibility between wavernn and the synthesizer must be held, and that you will have to provide new pretrained weights for both these models.",cool go ahead remember ensure data compatibility synthesizer must provide new,issue,positive,positive,positive,positive,positive,positive
653412932,"Try clicking the ""Browse"" button in the toolbox to load your own file.

The ""Load"" button only works if you have a dataset loaded.",try browse button toolbox load file load button work loaded,issue,negative,neutral,neutral,neutral,neutral,neutral
653411111,"At the very least you will need to modify the synthesizer code, and train new synthesizer and vocoder models. See #30",least need modify synthesizer code train new synthesizer see,issue,negative,negative,neutral,neutral,negative,negative
653410389,"If you're training your own models then you should start with the instructions here: [https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training)

I recommend first trying the toolbox with the pretrained models as it is a significant time investment to train your own.",training start recommend first trying toolbox significant time investment train,issue,positive,positive,positive,positive,positive,positive
653341152,"@CorentinJ @cantrell Can you guys take a look our recent TTS framework here (https://github.com/TensorSpeech/TensorflowTTS). We supported Tacotron2, FastSpeech, FastSpeech2, Multiban-melgan on native Tensorflow implementation. We also have a plan to support other languages, tflite for mobile, tensorrt for Deploy server. Almost supported model are real-time now.

audio samples: https://tensorspeech.github.io/TensorflowTTS/
colab demo: https://colab.research.google.com/drive/1akxtrLZHKuMiQup00tzO2olCaN-y3KiD?usp=sharing

I can make pull request if you want :D.",take look recent framework native implementation also plan support mobile deploy server almost model audio make pull request want,issue,negative,neutral,neutral,neutral,neutral,neutral
653339620,"@blue-fish hi, can i contribute my Tensorflow2 tacotron2 to this repo ?, our framework also plan to support tflite for TTS model both real-time vocoder and Text2Mel model. Can you take a look our framework ?

github: https://github.com/TensorSpeech/TensorflowTTS
sample audio: https://tensorspeech.github.io/TensorflowTTS/
colab demo: https://colab.research.google.com/drive/1akxtrLZHKuMiQup00tzO2olCaN-y3KiD?usp=sharing

we also supported FastSpeech2 which quality comparable with tacotron2 and much faster.  Futhermore, we are working to support for other languages such as chinese, JP ...",hi contribute framework also plan support model model take look framework sample audio also quality comparable much faster working support,issue,positive,positive,positive,positive,positive,positive
653339014,I thought that was if you werent going to make your own? (im making my own btw) Or am I mixing that up with something else?,thought werent going make making something else,issue,negative,neutral,neutral,neutral,neutral,neutral
653196930,"Got it. Thanks, @CorentinJ. I'll take a closer look (and/or check out the Mozilla implementation).",got thanks take closer look check implementation,issue,negative,positive,positive,positive,positive,positive
653186765,"@cantrell You've got to understand the way voice cloning works in this repo. The Tacotron 2 architecture in my repo barely differs from the usual Tacotron 2. The only thing that's added is a way to condition it on a speaker's voice, which is a very minor addition. Hence why it should be simple to transfer that over to an existing Tacotron 2 implementation. The Mozilla repo has ongoing (or maybe finished?) work on that, so that's one alternative.

Do understand that it's not a matter of _training_ the model on only 5 seconds of audio, it's an entirely different procedure which does not involve any training.",got understand way voice work architecture barely usual thing added way condition speaker voice minor addition hence simple transfer implementation ongoing maybe finished work one alternative understand matter model audio entirely different procedure involve training,issue,negative,negative,neutral,neutral,negative,negative
653159600,"This also solved the issue for me, maybe the Colab Book needs to be updated.",also issue maybe book need,issue,negative,neutral,neutral,neutral,neutral,neutral
653132343,"Hi, @CorentinJ. This is a fantastic project which I've had a lot of fun playing around with.

The biggest challenge with using other projects seems to be data sets. All the other projects I've found are most easily trained on the LJSpeech data set whereas this one can generate unique results with a small sample of audio. Are you aware of any other projects that can be used to clone speech with small audio samples? Thanks!",hi fantastic project lot fun around biggest challenge data found easily trained data set whereas one generate unique small sample audio aware used clone speech small audio thanks,issue,positive,positive,positive,positive,positive,positive
653118159,"Yeah, I face a similar problem. I think if you could find datasets of speakers in the UK accent. You can finetune the speaker encoding model. I think that can make the results better.",yeah face similar problem think could find accent speaker model think make better,issue,negative,positive,positive,positive,positive,positive
652948411,"Hey @LordBaaa I did a pip list and i still cant import any qt binding. 

I ran pip install PyQt5 and it says requirement already satisfied 


 File ""C:\Users\Blah\AppData\Local\Programs\Python\Python37\lib\site-packages\matplotlib\backends\qt_compat.py"", line 168, in <module>
    raise ImportError(""Failed to import any qt binding"")
ImportError: Failed to import any qt binding",hey pip list still cant import binding ran pip install requirement already satisfied file line module raise import binding import binding,issue,negative,positive,positive,positive,positive,positive
652820193,You need a model. Get one here: [https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models),need model get one,issue,negative,neutral,neutral,neutral,neutral,neutral
652800387,"> I am also not sure why, but something about the changes gives me an error when I use the microphone to record audio. If using > your master branch (without the change) then the recording function works.
> ```
> Recording 5 seconds of audio
> Could not record anything. Is your recording device enabled?
> Your device must be connected before you start the toolbox.
> ```
Hmmm, I have not been able to reproduce this on linux with pulseaudio. Have you tried changing the audio device? I suspect soundevice is going to be tricky to set up properly in multiple platforms. On my device it shows the pipewire deivce and pulse, while on mac seems like it lists ""built in output"" but it should not be specific to output or input as far as I know. We will probably need to switch from sounddevice library to something better for handling multiple input devices more properly.  

> But I suggest that we go with this for now, and think of some other features we want to incorporate and revamp the UI all at once. For example, a button to replay or save .wav of synthesized audio (#352).

That's actually a nice feature! I will work on that one next.

",also sure something error use microphone record audio master branch without change recording function work recording audio could record anything recording device device must connected start toolbox able reproduce tried audio device suspect going tricky set properly multiple device pulse mac like built output specific output input far know probably need switch library something better handling multiple input properly suggest go think want incorporate revamp example button replay save audio actually nice feature work one next,issue,positive,positive,positive,positive,positive,positive
652183656,"This is how the toolbox looks on my screen. It seems there should be a better spot to place the audio device dropdown menu. But I suggest that we go with this for now, and think of some other features we want to incorporate and revamp the UI all at once. For example, a button to replay or save .wav of synthesized audio (#352).
<img width=""1435"" alt=""screenshot"" src=""https://user-images.githubusercontent.com/67130644/86203002-a7614380-bb18-11ea-9caa-0d14282e82c3.png"">
",toolbox screen better spot place audio device menu suggest go think want incorporate revamp example button replay save audio,issue,positive,positive,positive,positive,positive,positive
652009580,"> I think having a load and click free GUI app is the appeal of your software.

Yeah, I can't say I expected it to have that big of an impact on the popularity of this repo when I wrote it. Too bad it only _looks_ easy, but still is out of reach for most people with little experience in programming.

Prior to becoming my colleague, fatchord wrote not only WaveRNN but also a Tacotron 1 implementation (which, by the way, is not proved inferior to Tacotron 2): https://github.com/fatchord/WaveRNN

NVIDIA has a Tacotron 2 implementation: https://github.com/NVIDIA/tacotron2

Mozilla as well, with more frequent updates & features: https://github.com/mozilla/TTS

I would also check paperswithcode.com and ignore my repo and the ones above if you're looking for something else; perhaps something more recent, as neural TTS is still very much growing. https://paperswithcode.com/task/text-to-speech-synthesis ",think load click free appeal yeah ca say big impact popularity wrote bad easy still reach people little experience prior becoming colleague wrote also implementation way proved inferior implementation well frequent would also check ignore looking something else perhaps something recent neural still much growing,issue,positive,positive,neutral,neutral,positive,positive
651977779,"> I kinda wished that the popularity of this repo would have died down, but new people keep coming in at a fairly constant rate.
>
> Use this issue to ask me questions and to bring light upon things that you believe need to be improved, and we'll see what can be done.
>
> there are many other better open-source implementations of neural TTS out there, and new ones keep coming every day.

It would be awesome if you could point out some alternatives, maybe people would start using them instead. I'm not knowledgeable at all in this field so I don't know how to find anything on my own and how to compare which repos are good and which ones work best for what. 

I think having a load and click free GUI app is the appeal of your software.
",wished popularity would new people keep coming fairly constant rate use issue ask bring light upon believe need see done many better neural new keep coming every day would awesome could point maybe people would start instead knowledgeable field know find anything compare good work best think load click free appeal,issue,positive,positive,positive,positive,positive,positive
651908985,"The issue was resolved by making preprocessing single-threaded. I've pushed fixes to the `392_single_threaded_preprocess` branch of my fork. See the differences here: [https://github.com/CorentinJ/Real-Time-Voice-Cloning/compare/ba1a78d...blue-fish:392_single_threaded_preprocess](https://github.com/CorentinJ/Real-Time-Voice-Cloning/compare/ba1a78d...blue-fish:392_single_threaded_preprocess)

I do not plan to submit a PR for this. The baseline code works on other systems.",issue resolved making branch fork see plan submit code work,issue,negative,neutral,neutral,neutral,neutral,neutral
651697833,I am facing the same issue. Anyone resolved it?,facing issue anyone resolved,issue,negative,neutral,neutral,neutral,neutral,neutral
651633805,"I updated the wiki documentation to fix the original issue. Now the user is directed to run the following train command:
```
python encoder_train.py <datasets_root>/SV2TTS/encoder
```

This will work if the default path is used during the preprocess stage. Closing this issue.",documentation fix original issue user directed run following train command python work default path used stage issue,issue,negative,positive,positive,positive,positive,positive
651632598,"Please try again, mac should work now with --cpu option for demo_cli.py",please try mac work option,issue,negative,neutral,neutral,neutral,neutral,neutral
651533719,"Thanks for sharing @plummet555 ! I have a question and a suggestion:

> 3. Set hardcoded seeds in Tacotron2, otherwise the result are very inconsistent

Can you please share the changes? This would help me for #384 where I am trying to make the output more consistent.

> 4. I found that often there would be a harsh pop or other artifact at the start of the audio.

You can try this vocoder model with additional training: [https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/126#issuecomment-650531957](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/126#issuecomment-650531957) The speech quality is nearly identical but I find it cuts down on these types of artifacts. If you try it out I'd like to know if it worked for you.",thanks plummet question suggestion set otherwise result inconsistent please share would help trying make output consistent found often would harsh pop artifact start audio try model additional training speech quality nearly identical find try like know worked,issue,positive,positive,neutral,neutral,positive,positive
651411400,Hello @sberryman! Could you provide pretrained weights from https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/126#issuecomment-532400349 for Mixed version?,hello could provide mixed version,issue,negative,neutral,neutral,neutral,neutral,neutral
651395943,"I'm not sure training with CPU needs to be a supported mode, but maybe someone out there wants it?",sure training need mode maybe someone,issue,negative,positive,positive,positive,positive,positive
651395581,"Thanks for reporting this @JokerYan . I did not test training with CPU before merging #366 .

Off-hand I think we just need to delete lines 13 and 14, the synchronize isn't needed for CPU.",thanks test training think need delete synchronize,issue,negative,positive,positive,positive,positive,positive
651391558,Closing this issue as it appears to be resolved. @deltabravozulu Please reopen this issue if you continue to have additional questions.,issue resolved please reopen issue continue additional,issue,negative,neutral,neutral,neutral,neutral,neutral
651389461,You need a model. A pretrained model is available for download here: [https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models),need model model available,issue,negative,positive,positive,positive,positive,positive
650580081,"I'm about out of ideas...
1. Try uninstalling `tensorflow-gpu`
2. What GPU do you have?
3. Can you list out your installed packages and their versions? If you are using pip or conda, `pip list` or `conda list` will return this information.",try list pip pip list list return information,issue,negative,neutral,neutral,neutral,neutral,neutral
650557195,"@blue-fish I tried that but this happens:
```
Traceback (most recent call last):
  File ""demo_cli.py"", line 95, in <module>
    mels = synthesizer.synthesize_spectrograms(texts, embeds)
  File ""C:\Users\fcbge\Downloads\Real-Time-Voice-Cloning-master\Real-Time-Voice-Cloning-master\synthesizer\inference.py"", line 77, in synthesize_spectrograms
    self.load()
  File ""C:\Users\fcbge\Downloads\Real-Time-Voice-Cloning-master\Real-Time-Voice-Cloning-master\synthesizer\inference.py"", line 58, in load
    self._model = Tacotron2(self.checkpoint_fpath, hparams)
  File ""C:\Users\fcbge\Downloads\Real-Time-Voice-Cloning-master\Real-Time-Voice-Cloning-master\synthesizer\tacotron2.py"", line 59, in __init__
    self.session = tf.compat.v1.Session(config=config)
  File ""C:\Users\fcbge\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\client\session.py"", line 1585, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File ""C:\Users\fcbge\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\client\session.py"", line 699, in __init__
    self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version
```",tried recent call last file line module file line file line load file line file line super session self target graph file line status driver version insufficient version,issue,positive,positive,positive,positive,positive,positive
650555178,"Thanks for the feedback @LordBaaa . I generated that sample five times on the 428k model trying to get that pop to go away, before I became convinced that it was a feature of the model.",thanks feedback sample five time model trying get pop go away convinced feature model,issue,positive,positive,positive,positive,positive,positive
650554444,"I tried option no 2 but it shows the same error, I want to try option 1  but how do you use the CPU option? @blue-fish ",tried option error want try option use option,issue,negative,neutral,neutral,neutral,neutral,neutral
650551378,"Which version of tensorflow are you using? If it is 1.15 may I suggest that you use demo_cli.py until the toolbox is updated to offer CPU support.

To use tensorflow on GPU requires a specific CUDA driver version to be installed. tensorflow==1.15 requires CUDA 10.0. [https://www.tensorflow.org/install/source_windows#gpu](https://www.tensorflow.org/install/source_windows#gpu)",version may suggest use toolbox offer support use specific driver version,issue,negative,neutral,neutral,neutral,neutral,neutral
650549814,"@blue-fish  now this happens after i installed ffmpeg:
```
Traceback (most recent call last):
  File ""C:\Users\fcbge\Downloads\Real-Time-Voice-Cloning-master\Real-Time-Voice-Cloning-master\toolbox\__init__.py"", line 87, in <lambda>
    func = lambda: self.synthesize() or self.vocode()
  File ""C:\Users\fcbge\Downloads\Real-Time-Voice-Cloning-master\Real-Time-Voice-Cloning-master\toolbox\__init__.py"", line 173, in synthesize
    specs = self.synthesizer.synthesize_spectrograms(texts, embeds)
  File ""C:\Users\fcbge\Downloads\Real-Time-Voice-Cloning-master\Real-Time-Voice-Cloning-master\synthesizer\inference.py"", line 77, in synthesize_spectrograms
    self.load()
  File ""C:\Users\fcbge\Downloads\Real-Time-Voice-Cloning-master\Real-Time-Voice-Cloning-master\synthesizer\inference.py"", line 58, in load
    self._model = Tacotron2(self.checkpoint_fpath, hparams)
  File ""C:\Users\fcbge\Downloads\Real-Time-Voice-Cloning-master\Real-Time-Voice-Cloning-master\synthesizer\tacotron2.py"", line 59, in __init__
    self.session = tf.compat.v1.Session(config=config)
  File ""C:\Users\fcbge\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\client\session.py"", line 1585, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File ""C:\Users\fcbge\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\client\session.py"", line 699, in __init__
    self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version
```",recent call last file line lambda lambda file line synthesize spec file line file line load file line file line super session self target graph file line status driver version insufficient version,issue,positive,positive,positive,positive,positive,positive
650549809,"@blue-fish awesome thanks! It’s subtle but yes I can here a difference. Listening to both of the 428 vs 1159 I feel like I hear a slight amount of “background noise”. Like when some leaves there mic on continuous transmission and there is a little bit of like ambient noise. I hear it particularly on the male voice. It seems the improvement makes it “cleaner”. When the male voice stops in 428 there is an audio pop/drop. His noisiness I think is most notable on his last few words. In 1159 pop is gone, it is more continuous and the background noise is less or not there. Again like I say very subtle but it is better.",awesome thanks subtle yes difference listening feel like hear slight amount background noise like leaf continuous transmission little bit like ambient noise hear particularly male voice improvement cleaner male voice audio noisiness think notable last pop gone continuous background noise le like say subtle better,issue,positive,positive,positive,positive,positive,positive
650532751,"Take a look at [https://github.com/librosa/librosa/issues/219](https://github.com/librosa/librosa/issues/219), you may need to install a backend.",take look may need install,issue,negative,neutral,neutral,neutral,neutral,neutral
650532607,Have you tried demo_cli.py? I have never used the toolbox. Likely some part of it needs to be updated for compatibility with the recent changes.,tried never used toolbox likely part need compatibility recent,issue,negative,neutral,neutral,neutral,neutral,neutral
650531957,"Here are some samples @LordBaaa , can you hear the difference? I also provide a download link for the in-work model. No changes to hparams are needed to use it.

Samples: [wavs.zip](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/4840247/wavs.zip)
Model: [https://www.dropbox.com/s/2skjbec4d67q3zo/vocoder_1159k.pt?dl=0](https://www.dropbox.com/s/2skjbec4d67q3zo/vocoder_1159k.pt?dl=0)",hear difference also provide link model use model,issue,negative,neutral,neutral,neutral,neutral,neutral
650516980,"20.04 is very much bleeding edge for this project considering how many moving parts are involved.

That path bug was squashed in #371 . Still, I appreciate you taking the time to report it and share the fix. I understand you will not want to mess with your working 20.04 setup, but if you install on 18.04 consider pulling the latest.",much bleeding edge project considering many moving involved path bug still appreciate taking time report share fix understand want mess working setup install consider latest,issue,negative,positive,positive,positive,positive,positive
650514511,"Sweet. Thanks blue-fish. I'll give this a try tomorrow. I didn't think about 20.04 breaking things specifically. I'll probably just use a VM and/or toss a live-usb or SSD in and see if 18.04 is easier.  

I ultimately succeeded in my initial setup, but only for a few seconds before I got posix path errors (e.g. TypeError: invalid file: PosixPath('/home/$USER/Github/rtvc/test.mp3'). If anyone else lands on this particular bug/issue ever, https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/235#issuecomment-626354257 fixed it, though the line may vary. ",sweet thanks give try tomorrow think breaking specifically probably use toss see easier ultimately initial setup got path invalid file anyone else particular ever fixed though line may vary,issue,positive,positive,positive,positive,positive,positive
650499229,"If you just want to try it out, pull the latest updates from this repo. We added CPU support in the last week. Run demo_cli.py with the --cpu option.

You get the message because `torch.cuda.is_available()` evaluates to False. Try troubleshooting that. The pytorch you need correlates with your driver version, check it at the command line with `nvidia-smi`. I spent a few hours trying to get this to work with 20.04, then gave up and went back to 18.04 where the setup is much more straightforward.

Tensorflow-CUDA compatibility is another consideration. The latest version of this repo requires tensorflow==1.15, where the precompiled binaries only support CUDA 10.0. Recent Nvidia drivers provide CUDA >= 10.1 which would mean building tensorflow from source.",want try pull latest added support last week run option get message false try need driver version check command line spent trying get work gave went back setup much straightforward compatibility another consideration latest version support recent provide would mean building source,issue,positive,positive,neutral,neutral,positive,positive
650469763,"And this is the error in the demo_cli.py - hangs in the testing synthesizer step while constructing Tacotron:

ValueError: Variable Tacotron_model/inference/inputs_embedding already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:

Detailed info:

(voice) goodman@goodman-dl-box:~/development/Real-Time-Voice-Cloning-master$ python demo_cli.py
Arguments:
    enc_model_fpath:   encoder/saved_models/pretrained.pt
    syn_model_dir:     synthesizer/saved_models/logs-pretrained
    voc_model_fpath:   vocoder/saved_models/pretrained/pretrained.pt
    low_mem:           False
    no_sound:          False
    cpu:               False

Running a test of your configuration...

Found 2 GPUs available. Using GPU 0 (GeForce GTX 1080 Ti) of compute capability 6.1 with 11.7Gb total memory.

Preparing the encoder, the synthesizer and the vocoder...
Loaded encoder ""pretrained.pt"" trained to step 1564501
Found synthesizer ""pretrained"" trained to step 278000
Building Wave-RNN
Trainable Parameters: 4.481M
Loading model weights at vocoder/saved_models/pretrained/pretrained.pt
Testing your configuration with small inputs.
	Testing the encoder...
	Testing the synthesizer... (loading the model will output a lot of text)
Constructing model: Tacotron
WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /home/goodman/development/Real-Time-Voice-Cloning-master/synthesizer/models/modules.py:424: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv1D` instead.
WARNING:tensorflow:From /home/goodman/voice/lib/python3.6/site-packages/tensorflow_core/python/layers/convolutional.py:218: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From /home/goodman/development/Real-Time-Voice-Cloning-master/synthesizer/models/modules.py:425: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).
WARNING:tensorflow:From /home/goodman/development/Real-Time-Voice-Cloning-master/synthesizer/models/modules.py:428: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dropout instead.
WARNING:tensorflow:From /home/goodman/development/Real-Time-Voice-Cloning-master/synthesizer/models/modules.py:238: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
WARNING:tensorflow:From /home/goodman/voice/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
WARNING:tensorflow:From /home/goodman/voice/lib/python3.6/site-packages/tensorflow_core/contrib/rnn/python/ops/lstm_ops.py:360: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.add_weight` method instead.
WARNING:tensorflow:From /home/goodman/voice/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/goodman/development/Real-Time-Voice-Cloning-master/synthesizer/models/modules.py:308: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.
WARNING:tensorflow:From /home/goodman/development/Real-Time-Voice-Cloning-master/synthesizer/models/modules.py:272: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
initialisation done /gpu:0
Traceback (most recent call last):
  File ""demo_cli.py"", line 95, in <module>
    mels = synthesizer.synthesize_spectrograms(texts, embeds)
  File ""/home/goodman/development/Real-Time-Voice-Cloning-master/synthesizer/inference.py"", line 77, in synthesize_spectrograms
    self.load()
  File ""/home/goodman/development/Real-Time-Voice-Cloning-master/synthesizer/inference.py"", line 58, in load
    self._model = Tacotron2(self.checkpoint_fpath, hparams)
  File ""/home/goodman/development/Real-Time-Voice-Cloning-master/synthesizer/tacotron2.py"", line 28, in __init__
    split_infos=split_infos)
  File ""/home/goodman/development/Real-Time-Voice-Cloning-master/synthesizer/models/tacotron.py"", line 136, in initialize
    ""inputs_embedding"", [len(symbols), hp.embedding_dim], dtype=tf.float32)
  File ""/home/goodman/voice/lib/python3.6/site-packages/tensorflow_core/python/ops/variable_scope.py"", line 1500, in get_variable
    aggregation=aggregation)
  File ""/home/goodman/voice/lib/python3.6/site-packages/tensorflow_core/python/ops/variable_scope.py"", line 1243, in get_variable
    aggregation=aggregation)
  File ""/home/goodman/voice/lib/python3.6/site-packages/tensorflow_core/python/ops/variable_scope.py"", line 567, in get_variable
    aggregation=aggregation)
  File ""/home/goodman/voice/lib/python3.6/site-packages/tensorflow_core/python/ops/variable_scope.py"", line 519, in _true_getter
    aggregation=aggregation)
  File ""/home/goodman/voice/lib/python3.6/site-packages/tensorflow_core/python/ops/variable_scope.py"", line 868, in _get_single_variable
    (err_msg, """".join(traceback.format_list(tb))))
ValueError: Variable Tacotron_model/inference/inputs_embedding already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:

  File ""/home/goodman/voice/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()
  File ""/home/goodman/voice/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 3426, in _create_op_internal
    op_def=op_def)
  File ""/home/goodman/voice/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 3357, in create_op
    attrs, op_def, compute_device)
  File ""/home/goodman/voice/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/home/goodman/voice/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py"", line 794, in _apply_op_helper
    op_def=op_def)
",error testing synthesizer step variable already mean set originally defined detailed voice goodman python false false false running test configuration found available ti compute capability total memory synthesizer loaded trained step found synthesizer trained step building trainable loading model testing configuration small testing testing synthesizer loading model output lot text model warning module included information please see related depend functionality listed please file issue warning removed future version use instead warning removed future version please use method instead warning removed future version use instead particular used consult documentation warning dropout removed future version use instead warning removed future version please use cell equivalent warning removed future version please use cell equivalent warning removed future version please use method instead warning removed future version use broadcast rule warning removed future version class equivalent warning dense removed future version use instead done recent call last file line module file line file line load file line file line initialize file line file line file line file line file line variable already mean set originally defined file line file line file line file line return file line,issue,negative,negative,neutral,neutral,negative,negative
650460274,"> `Checkpoint path: synthesizer/saved_models/logs-goodman_run/taco_pretrained/tacotron_model.ckpt`

Are you trying to continue training of the pretrained synthesizer using a dataset that is processed using embeddings from your custom encoder? I wonder if that has anything to do with it.

You can also read related issues at Rayhane-mamah's Tacotron2 repo: [https://github.com/Rayhane-mamah/Tacotron-2/issues?q=loss+exploded](https://github.com/Rayhane-mamah/Tacotron-2/issues?q=loss+exploded) From what I can tell it might be intermittent and you might just need to keep trying.",path trying continue training synthesizer custom wonder anything also read related tell might intermittent might need keep trying,issue,negative,neutral,neutral,neutral,neutral,neutral
650422303,"

These are the last two I got:
Encoding model training 
..........
Step 219910   Loss: 0.0274   EER: 0.0046   Step time:  mean:   658ms  std:   632ms

Average execution time over 10 steps:
  Blocking, waiting for batch (threaded) (10/10):  mean:  125ms   std:   13ms
  Data to cuda (10/10):                            mean:    3ms   std:    0ms
  Forward pass (10/10):                            mean:   38ms   std:    0ms
  Loss (10/10):                                    mean:   22ms   std:    1ms
  Backward pass (10/10):                           mean:  134ms   std:   22ms
  Parameter update (10/10):                        mean:  124ms   std:    3ms
  Extras (visualizations, saving) (10/10):         mean:   42ms   std:  126ms

..........
Step 219920   Loss: 0.0294   EER: 0.0050   Step time:  mean:   639ms  std:   472ms

Average execution time over 10 steps:
  Blocking, waiting for batch (threaded) (10/10):  mean:  284ms   std:  475ms
  Data to cuda (10/10):                            mean:    3ms   std:    0ms
  Forward pass (10/10):                            mean:   38ms   std:    0ms
  Loss (10/10):                                    mean:   22ms   std:    2ms
  Backward pass (10/10):                           mean:  126ms   std:   20ms
  Parameter update (10/10):                        mean:  123ms   std:    5ms
  Extras (visualizations, saving) (10/10):         mean:   36ms   std:  108ms
![speaker_encoder_training_result](https://user-images.githubusercontent.com/11790331/85904513-ec167300-b7d6-11ea-8533-8596f25f0c5d.png)
",last two got model training step loss eer step time mean average execution time blocking waiting batch threaded mean data mean forward pas mean loss mean backward pas mean parameter update mean saving mean step loss eer step time mean average execution time blocking waiting batch threaded mean data mean forward pas mean loss mean backward pas mean parameter update mean saving mean,issue,negative,negative,negative,negative,negative,negative
650420153,Thanks @CorentinJ . Do you have UMAP plot for the best pre-trained model that you have shared and loss at that point?,thanks plot best model loss point,issue,positive,positive,positive,positive,positive,positive
650418395,"Thanks @CorentinJ : This is the UMAP Projection plot persisted and it appears the clusters are well separated in space?
![my_run_umap_219900](https://user-images.githubusercontent.com/11790331/85903767-2848d400-b7d5-11ea-9fb7-76c7d3c659ef.png)
",thanks projection plot well space,issue,positive,positive,positive,positive,positive,positive
650349714,"All right, thanks for the information.  I used your repo, can I clone someone's voice to my liking without their permission?  I reassure you, I just want to know a little more about the laws, it's a friend's voice, I have no bad intentions. @CorentinJ ",right thanks information used clone someone voice liking without permission reassure want know little friend voice bad,issue,negative,negative,negative,negative,negative,negative
650317484,"The license is an [MIT license](https://choosealicense.com/licenses/mit/), meaning that you're free to do anything with it. 

The bit about legal work is purely related to Resemble.AI, and ""we"" thus stands for ""my team and I at Resemble"". Our work at Resemble.AI is independent from this project.

In the youtube video description I had invited people to clone their voice through Resemble.AI (under the free plan) rather than to use my repo, but I had omitted that at Resemble.AI we cannot allow anyone to clone anyone else's voice without proving that they own the rights to that voice. Hence the bit about legal work.",license license meaning free anything bit legal work purely related thus team resemble work independent project video description people clone voice free plan rather use allow anyone clone anyone else voice without proving voice hence bit legal work,issue,positive,positive,positive,positive,positive,positive
650224531,To use another language you will need to do some coding and train your own models. Please see #30 .,use another language need train please see,issue,negative,neutral,neutral,neutral,neutral,neutral
650184088,"> Yes, it will run on Linux.

thx for answering but my last question is; is it only in english or can we use another languages too?",yes run last question use another,issue,negative,neutral,neutral,neutral,neutral,neutral
650057833,"Check the plots for the speaker encoder. Are the clusters well separated in the space? If so, your model is good enough",check speaker well space model good enough,issue,positive,positive,positive,positive,positive,positive
649959013,"I have not yet trained a speaker encoder, so I don't have any experience to share. Still new to this. Currently trying to find out if the pretrained vocoder improves with additional training.",yet trained speaker experience share still new currently trying find additional training,issue,negative,positive,neutral,neutral,positive,positive
649954661,"Hey @blue-fish do you plan to share your models and if so could I get them. Even if they are not finished training I’d be curious to hear the difference. Thanks.
P.S I am unaware of a benchmark procedure.",hey plan share could get even finished training curious hear difference thanks unaware procedure,issue,positive,positive,neutral,neutral,positive,positive
649950120,"cool. Thanks @blue-fish! What is the best loss you have got so far, and at what steps?",cool thanks best loss got far,issue,positive,positive,positive,positive,positive,positive
649948739,"If you are using the defaults, per [encoder_train.py](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/070a3c187f87136ebe92aa72766f8343772d414e/encoder_train.py#L28) it saves to <run_id>.pt every 500 steps (and a backup to a separate checkpoint file every 7,500 steps). As long as you don't pass the --force_restart flag, it will reload <run_id>.pt and continue training where it left off.",per every backup separate file every long pas flag reload continue training left,issue,negative,negative,neutral,neutral,negative,negative
649944143,You can resume training of the model to determine the loss.,resume training model determine loss,issue,negative,neutral,neutral,neutral,neutral,neutral
649941429,"@CorentinJ : in the Jupyter notebook, it is mentioned that encoder ""pretrained.pt"" was trained to step 1,564,501. Could you advise on that the loss was at that stage?
",notebook trained step could advise loss stage,issue,negative,neutral,neutral,neutral,neutral,neutral
649917433,"I figured out a solution to the above issue, to use tf.TensorShape().with_rank() to increase the rank as needed. Now working through a different set of errors.

Edit: Although it makes that one error go away, I do not know if it is the correct fix so I have not committed it.",figured solution issue use increase rank working different set edit although one error go away know correct fix,issue,negative,negative,negative,negative,negative,negative
649875885,"If you want to run the encoder with a single audio file, look at line 145 of [demo_cli.py](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/demo_cli.py#L145)",want run single audio file look line,issue,negative,negative,neutral,neutral,negative,negative
649817194,"I've added another 600k steps to the pretrained vocoder. Loss started at 3.682 and is currently at 3.647. Though I hear an improvement in the samples produced during training, voice cloning results are unchanged. Is there a procedure to benchmark performance?",added another loss currently though hear improvement produced training voice unchanged procedure performance,issue,negative,neutral,neutral,neutral,neutral,neutral
649725153,"Take a look at #126 if you haven't already, there is some discussion about training the speaker encoder.",take look already discussion training speaker,issue,negative,neutral,neutral,neutral,neutral,neutral
649613952,"Following a recent change, CUDA is no longer a hard requirement and it is possible to run the voice cloning demo without GPU support. Use the --cpu option in demo_cli.py

Questions about CUDA installation should be submitted to a different support channel. Try asking your question in the CUDA setup and installation section of the NVIDIA developer forums: [https://forums.developer.nvidia.com/c/accelerated-computing/cuda/cuda-setup-and-installation](https://forums.developer.nvidia.com/c/accelerated-computing/cuda/cuda-setup-and-installation)",following recent change longer hard requirement possible run voice without support use option installation different support channel try question setup installation section developer,issue,negative,negative,neutral,neutral,negative,negative
649603867,"This will run on mac now that CPU support is enabled with #366 . However, CPU is considerably slower.",run mac support however considerably,issue,negative,positive,neutral,neutral,positive,positive
649411579,"I added a check in each of the preprocess scripts to verify that webrtcvad is installed before proceeding. The check can be disabled but the user is encouraged to install it. Can you take a look at this and tell me whether you want to keep this feature, or modify it?

We can also update the training wiki page to remind the user to install webrtcvad since it is no longer in requirements.txt",added check verify proceeding check disabled user install take look tell whether want keep feature modify also update training page remind user install since longer,issue,negative,negative,negative,negative,negative,negative
649384228,Can you share how did you do it? I want to try training a portuguese model,share want try training model,issue,negative,neutral,neutral,neutral,neutral,neutral
649327864,"How about we close this PR, it does not seem like a serious request. The branch has helped 1 or 2 people in #36 , but no one has mentioned it in a while. Those unable to use `--low_mem` can take advantage of CPU support now. ",close seem like serious request branch people one unable use take advantage support,issue,positive,negative,negative,negative,negative,negative
649301367,Good idea. I added a print statement if the import fails. Let me know if you prefer an actual warning instead.,good idea added print statement import let know prefer actual warning instead,issue,negative,positive,positive,positive,positive,positive
649264215,"> asdfasd

Yes

@blue-fish 
I still don't know if anybody has benefitted from this PR, not sure it works as intended",yes still know anybody sure work intended,issue,positive,positive,positive,positive,positive,positive
649262184,"Can do, but add a warning when the import fails because denoising should be enabled by default",add warning import default,issue,negative,neutral,neutral,neutral,neutral,neutral
648798434,"I am attempting to convert the existing synthesizer code to tensorflow v2, making some progress but could use some help with this error message. demo_cli.py gets as far as starting the synthesizer test.

Affected code is in `custom_decoder.py` and `helpers.py` in synthesizer/models. I have a branch here: [https://github.com/blue-fish/Real-Time-Voice-Cloning/commits/370_tf2_compat](https://github.com/blue-fish/Real-Time-Voice-Cloning/commits/370_tf2_compat)

[Edit: This issue is resolved. See the history for the original error message]",convert synthesizer code making progress could use help error message far starting synthesizer test affected code branch edit issue resolved see history original error message,issue,negative,positive,positive,positive,positive,positive
648769557,"It's fine I get notifications for all PRs

If I get tired of reviewing them I'll let you know",fine get get tired let know,issue,negative,positive,neutral,neutral,positive,positive
648711107,"@CorentinJ This is a very tiny change, I hesitate to contact you to personally review it. Do you mind this?

Mozilla TTS did the same thing: [https://github.com/mozilla/TTS/commit/9f83b6797f26be631d7717bcb8be53e59846f596](https://github.com/mozilla/TTS/commit/9f83b6797f26be631d7717bcb8be53e59846f596)",tiny change hesitate contact personally review mind thing,issue,negative,neutral,neutral,neutral,neutral,neutral
648615846,"Here is a link to the affected line of code along with my comments:
[https://github.com/CorentinJ/Real-Time-Voice-Cloning/commit/1b8d2e794b32039aa7ecc6367dabb64a3e5e6467#r40127321](https://github.com/CorentinJ/Real-Time-Voice-Cloning/commit/1b8d2e794b32039aa7ecc6367dabb64a3e5e6467#r40127321)",link affected line code along,issue,negative,neutral,neutral,neutral,neutral,neutral
648323229,"It's fine, no need to get too fancy",fine need get fancy,issue,negative,positive,positive,positive,positive,positive
648298328,"Yes, that line of code. This is what it looks like using `#` and `-` to generate the progbar. Not too bad though I prefer how it looks currently.

```
Synthesizing the waveform:
{| #############--- 36000/48000 | Batch Size: 5 | Gen Rate: 0.9kHz | }
```

I already merged the workaround #372. Do you want to back it out and implement something like this instead? We could also use tqdm with ascii=True. However, everyone would get the ASCII progbar unless we can figure out a way to detect incompatible systems.",yes line code like generate bad though prefer currently batch size gen rate already want back implement something like instead could also use however everyone would get ascii unless figure way detect incompatible,issue,positive,negative,negative,negative,negative,negative
647961839,"> 
> 
> My only issue is saving the recordings I've created to my pc.
> Does anyone know a good way to save audio voice clone recordings once you are done?
> Please share your thoughts.

It's been several months since I last visited this project, so my knowledge might be incomplete.
If you look into synthesizer/audio.py, there are two save methods available, for simplicity, I used save_wav. 

To use it, I added its call inside toolbox/__init__.py at the end of the vocode method.

        # Plot it
        self.ui.draw_embed(embed, name, ""generated"")
        self.ui.draw_umap_projections(self.utterances)

        # Save it
        from os import getcwd
        from pathlib import Path
        from time import strftime
        from vocoder.audio import save_wav
        wav_name = f'output_{strftime(""%m%d%Y_%H%M%S"")}.wav'
        wav_file = str(Path(getcwd(), wav_name))
        save_wav(x=wav, path=wav_file)

So the idea is that after the vocoding is done, the output audio file is saved in the current working directory. I hope this helps!",issue saving anyone know good way save audio voice clone done please share several since last project knowledge might incomplete look two save available simplicity used use added call inside end method plot embed name save o import import path time import import path idea done output audio file saved current working directory hope,issue,positive,positive,positive,positive,positive,positive
647948180,"[This](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/vocoder/display.py#L11)? They could simply be replaced then, or better, we'd use tqdm everywhere in the repo",could simply better use everywhere,issue,negative,positive,positive,positive,positive,positive
647946640,"> Are we maintaining that one too?
Not really, unsure how to proceed with that code",one really unsure proceed code,issue,negative,positive,positive,positive,positive,positive
647905824,"I found the code snippet to remove non-ascii chars here: [https://stackoverflow.com/q/20078816](https://stackoverflow.com/q/20078816)

Please feel free to suggest a better implementation.",found code snippet remove please feel free suggest better implementation,issue,positive,positive,positive,positive,positive,positive
647903066,"As mentioned in #89 the progbar in vocoder/display.py is incompatible with the encoding specified for python stdout. It has length 16 and occupies position 4-19 of the vocoder message stream, consistent with the 1st error message.",incompatible python length position message stream consistent st error message,issue,negative,positive,positive,positive,positive,positive
647901982,"The progbar in vocoder/display.py is incompatible with the encoding specified for python stdout. It has length 16 and occupies position 4-19 of the vocoder message stream, consistent with the 1st error message. Someone also reported a similar issue in #50 .",incompatible python length position message stream consistent st error message someone also similar issue,issue,negative,positive,positive,positive,positive,positive
647788179,"There is also another instance of librosa.load in the collab that could be fixed. Are we maintaining that one too?

Also, I verified that librosa.output.write_wav does not have this issue.",also another instance could fixed one also issue,issue,negative,positive,neutral,neutral,positive,positive
647640708,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`.

If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",wo notify release get touch new version available rather skip next major minor version let know ignore major version ignore minor version change mind resolve,issue,negative,positive,neutral,neutral,positive,positive
647393543,It does not give me that option. Would you like me to squash and merge on my own fork and submit a new PR?,give option would like squash merge fork submit new,issue,negative,positive,positive,positive,positive,positive
647384900,"Very nice, do you have the permission to squash & merge after I approved it?",nice permission squash merge,issue,negative,positive,positive,positive,positive,positive
647376502,Thank you for reviewing #331. In response I have submitted #366 which addresses your comments and carefully removes all unnecessary changes from the PR. When you have time please review and merge that one instead.,thank response carefully unnecessary time please review merge one instead,issue,positive,negative,negative,negative,negative,negative
647090067,"Hi @blue-fish 
I think the vocoder is actually the strongest part. The synthesiser is what makes or breaks the model.
If you look at the mfccs, you will notice that they are quite weird sometimes. For example they contain large pauses.
If you want to improve the model, train a new Synthesizer and possibly a new encoder.
I would suggest using mozillas TTS as a baseline, the code here is outdated. Also, use LibriTTS.",hi think actually part model look notice quite weird sometimes example contain large want improve model train new synthesizer possibly new would suggest code outdated also use,issue,negative,negative,neutral,neutral,negative,negative
647046279,"> Encoder: trained 1.56M steps (20 days with a single GPU) with a batch size of 64
Synthesizer: trained 256k steps (1 week with 4 GPUs) with a batch size of 144
Vocoder: trained 428k steps (4 days with a single GPU) with a batch size of 100

I am trying to squeeze just a little more quality out of Corentin's pretrained models by continuing to train the vocoder while leaving the other models unchanged. This also seems like a reasonable place to start as I still have much to learn. Has anyone else tried this?

My GPU only has 4gb so I reduced the batch size from 100 to 50 to make it fit. I am otherwise using default parameters and the same training set as in the wiki. Loss is slowly but steadily decreasing, from 3.682 to 3.677 after 10 epochs. I'll continue the training and see if results are noticeably better.",trained day single batch size synthesizer trained week batch size trained day single batch size trying squeeze little quality train leaving unchanged also like reasonable place start still much learn anyone else tried reduced batch size make fit otherwise default training set loss slowly steadily decreasing continue training see noticeably better,issue,positive,positive,positive,positive,positive,positive
646962129,I'll give a review to #331 tomorrow and probably will make some changes as well.,give review tomorrow probably make well,issue,negative,neutral,neutral,neutral,neutral,neutral
646883124,"I'd like #331 merged to enable CPU support by default. It also simplifies the install process for those with a goal of running demo_cli.py for evaluation purposes.

Some kind of API or improved CLI would be a worthwhile and easy enhancement for the community to pursue. Good usability will help keep this repo as the focal point for development of open-source SV2TTS. This is really neat stuff, many thanks for sharing your code and pre-trained models under a permissive license.",like enable support default also install process goal running evaluation kind would easy enhancement community pursue good usability help keep focal point development really neat stuff many thanks code permissive license,issue,positive,positive,positive,positive,positive,positive
646620600,"Second thing: webrtcvad. That package is hell to install on windows. There are alternatives for noise removal out there. There's also the possibility of not using it at all, but for both LibriSpeech and LibriTTS I would recommend it.",second thing package hell install noise removal also possibility would recommend,issue,negative,neutral,neutral,neutral,neutral,neutral
646619539,"First things first, the biggest issue for me with this project is the hecking tensorflow code. Tensorflow sucks, and it sucks just as much to install it let alone install an older version.

I believe it would lower the entry barrier for new users if the version of that package were to be upgraded. I've seen a PR for that but that's only for the collab version it seems. A PR for the entire repo would be appreciated. 

Ideally, we'd replace all of the synthesizer code with pytorch code (there are several open source pytorch synthesizers out there), but that's a lot of work.

If anybody is willing to pick up on either of these things, let me know.",first first biggest issue project code much install let alone install older version believe would lower entry barrier new version package seen version entire would ideally replace synthesizer code code several open source lot work anybody willing pick either let know,issue,negative,positive,positive,positive,positive,positive
645887018,"windows 10 64
python 3.6.8
visual studio community 2017
cuda 10.0
cudnn 10.0 (unpack to C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0)

1. unzip pretrained.zip to real-time-voice-cloning
2. install torch==1.0.1
3. install requirements.txt 
tensorflow-gpu==1.14.0
umap-learn==0.4.4
visdom==0.1.8.9
webrtcvad==2.0.10
librosa==0.7.2
matplotlib==3.2.2
numpy==1.18.5
scipy==1.4.1
tqdm==4.46.1
sounddevice==0.3.15
Unidecode==1.1.1
inflect==4.1.0
PyQt5==5.15.0
multiprocess==0.70.10
numba==0.48
4. run demo_toolbox.py


",python visual studio community unpack install install run,issue,negative,neutral,neutral,neutral,neutral,neutral
645238721,"https://poorlydocumented.com/2019/11/installing-corentinjs-real-time-voice-cloning-project-on-windows-10-from-scratch/

step by step guide, witout this it took apprx 10 hr for me to make up and running

and other resource 

http://www.xilodyne.com/SBS_RTVC_Demo_Setup",step step guide took make running resource,issue,negative,neutral,neutral,neutral,neutral,neutral
644443628,"win10 system
I solved this problem by installing ffmpeg and setting the environment variables
in Module audioread ffdec.py 
line 32 COMMANDS = ('ffmpeg', 'avconv')
",win system problem setting environment module line,issue,negative,positive,positive,positive,positive,positive
643855719,"i'm having the same problem !
couldn't find a way to save the generated voice.",problem could find way save voice,issue,negative,neutral,neutral,neutral,neutral,neutral
643753099,"Skill store links for those who are interested in the output:

US store: https://www.amazon.com/dp/B08B59XJLY
UK store: https://www.amazon.co.uk/dp/B08B3ZQ6SP

I've posted about it in a few Reddit forums and referenced this project.",skill store link interested output u store store posted project,issue,negative,positive,positive,positive,positive,positive
643618901,"Ok thanks a bunch, will run on my windows PC with a 1050Ti then!",thanks bunch run ti,issue,negative,positive,positive,positive,positive,positive
643617266,it wont work with a mac. you need Cuda cores. which requires a Nvidia GPU. macs dont have nvidia GPUS unless you do bootcamp with windows and run a egpu ,wont work mac need dont unless run,issue,negative,neutral,neutral,neutral,neutral,neutral
643271654,"also, this has work for me
```
%tensorflow_version 1.x
import tensorflow
print(tensorflow.__version__)

%tensorflow_version 1.x
!pip install umap-learn
!pip install visdom
!pip install webrtcvad
!pip install librosa>=0.5.1
!pip install matplotlib>=2.0.2
!pip install numpy>=1.14.0
!pip install scipy>=1.0.0
!pip install tqdm
!pip install sounddevice
!pip install Unidecode
!pip install inflect
!pip install PyQt5
!pip install multiprocess
!pip install numba
```",also work import print pip install pip install pip install pip install pip install pip install pip install pip install pip install pip install pip install inflect pip install pip install pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
642980446,Same issue. It seems its optimized to produce 5-6 seconds of output and will adjust accordingly. Maybe this is a setting that can be tweaked?,issue produce output adjust accordingly maybe setting,issue,negative,neutral,neutral,neutral,neutral,neutral
642811812,.. by the way @CorentinJ I'd be happy to attribute your project or resemble.ai as you prefer in the skill description. It's non-commercial (i.e. free),way happy attribute project prefer skill description free,issue,positive,positive,positive,positive,positive,positive
642222574,"If anyone wants to use this. Then, make sure to paste `%tensorflow_version 1.x` at any line of code",anyone use make sure paste line code,issue,negative,positive,positive,positive,positive,positive
641853493,"Yeah I should have worded it better. Sorry about that, I will correct it.",yeah worded better sorry correct,issue,positive,neutral,neutral,neutral,neutral,neutral
641695839,"First uninstalled tensorflow versions like tensorflow and tensorflow-gpu. 
Then install tensorflow like
 `pip3 install tensorflow==1.14.0`
Its work for me.",first uninstalled like install like pip install work,issue,positive,positive,positive,positive,positive,positive
641606612,"> 
> 
> Lads if you want to ask Resemble.AI to make the project open source, you go ahead and do it. If you expect a student who just finished university to work full-time on his own and for free on an open source project, you've probably never put a foot in the real world. The closest to what you're asking is [Mozilla's repo](https://github.com/mozilla/TTS).
> 
> > well TTS and voice cloning aren't exactly the same, I don't need a TTS service, I need to replicate a voice, that's all
> 
> Yeah they are the same, and you'll get all the features in this repo from Mozzila's repo. Last I checked, erogol had a lot of features from different papers implemented, including sv2tts. In fact he's even copied some code from my repo.

Thanks for refering to Mozilla TTS. However, I should emphasize that I did not ""copy"" anything from here.  And yet I' I'd be happy to cite your code as you implemented it before.",want ask make project open source go ahead expect student finished university work free open source project probably never put foot real world well voice exactly need service need replicate voice yeah get last checked lot different fact even copied code thanks however emphasize copy anything yet happy cite code,issue,positive,positive,positive,positive,positive,positive
640990817,"Are you using a new computer? There is a path length limit and it's low by default. 

see here: https://stackoverflow.com/questions/54778630/could-not-install-packages-due-to-an-environmenterror-errno-2-no-such-file-or

how to change that can be found here: https://superuser.com/questions/1119883/windows-10-enable-ntfs-long-paths-policy-option-missing",new computer path length limit low default see change found,issue,negative,positive,neutral,neutral,positive,positive
640219337,I installed 1.14 from the github release https://github.com/tensorflow/tensorflow/releases/tag/v1.14.0 and I still get this error. Any ideas?,release still get error,issue,negative,neutral,neutral,neutral,neutral,neutral
640152198,"First install torch, then install the requirements, The more explained installation is actually on this same repo
there will be some problems and you'll need to install visual studio build tools, cuda, & other stuff
As a recommendation, when you get errors, please read the traceback. It says you exactly what to do
Good luck",first install torch install installation actually need install visual studio build stuff recommendation get please read exactly good luck,issue,positive,positive,positive,positive,positive,positive
640147633,"Okay, this one was an easy one. Turns out it was trying to use my HDMI output :P which was the device 0 for sounddevice library. I just needed to set the variable as: `sd.default.device = 2` which was my pulse device. 
This could be improved adding a function like:

```
    def get_valid_devices(self, sr):
        supported_devices=[]
        for device in sd.query_devices():
            try:
                sd.check_output_settings(device=device['name'], samplerate=sr)
            except Exception as e:
                print(""Sample Rate: "", sr, ""Device: "", device['name'], e)
            else:
                supported_devices.append(device['name'])
        return supported_devices

```

And then retrieve the device like `sd.default.device = get_valid_devices(sample_rate)[-1]` Or ideally have a comboBox on the interface for this.

The thing is now it works, but the quality is far from the demo video. I will mess around with it more to see.

",one easy one turn trying use output device library set variable pulse device could function like self device try except exception print sample rate device device else device return retrieve device like ideally interface thing work quality far video mess around see,issue,positive,positive,positive,positive,positive,positive
640142282,"I manage to get passed that last error simply by using the else line of the if statement at line 113 of modules.py: 

  ```
  #    if torch.cuda.is_available():
   #        self._cell = tf.contrib.cudnn_rnn.CudnnLSTM(num_units=num_units, name=name, num_layers 5)
   #    else:
      self._cell = tf.contrib.rnn.LSTMBlockCell(num_units=num_units, name=name)
```

Even on tensorflow 1.14  the method CudnnLSTM takes num_layers, which is blank on the pull request. The problem is that It is not going to use the gpu this way right? ... 

This is the full output of the program now:
<details> 

```

$ python demo_toolbox.py -d /media/matheus/Elements/AI/
/home/matheus/programs/Real-time-Voice-fork/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.
Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.
  from numba.decorators import jit as optional_jit
/home/matheus/programs/Real-time-Voice-fork/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.
Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.
  from numba.decorators import jit as optional_jit
Arguments:
    datasets_root:    /media/matheus/Elements/AI
    enc_models_dir:   encoder/saved_models
    syn_models_dir:   synthesizer/saved_models
    voc_models_dir:   vocoder/saved_models
    low_mem:          False

Debug: /home/matheus/projects/TinaVoiceClone/samples/audio_2020-06-05_19-40-01.ogg.wav
Loaded encoder ""pretrained.pt"" trained to step 1564501
Debug: /home/matheus/projects/TinaVoiceClone/samples/audio_2020-06-05_19-40-07.ogg.wav
Debug: /home/matheus/projects/TinaVoiceClone/samples/audio_2020-06-05_19-40-13.ogg.wav
Debug: /home/matheus/projects/TinaVoiceClone/samples/audio_2020-06-05_19-40-47.ogg.wav
Found synthesizer ""pretrained"" trained to step 278000
Constructing model: Tacotron
WARNING:tensorflow:From /home/matheus/programs/Real-time-Voice-fork/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /home/matheus/programs/Real-time-Voice-fork/synthesizer/models/modules.py:423: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv1D` instead.
WARNING:tensorflow:From /home/matheus/programs/Real-time-Voice-fork/synthesizer/models/modules.py:424: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).
WARNING:tensorflow:From /home/matheus/programs/Real-time-Voice-fork/synthesizer/models/modules.py:427: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dropout instead.
WARNING:tensorflow:From /home/matheus/programs/Real-time-Voice-fork/synthesizer/models/modules.py:237: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
WARNING:tensorflow:From /home/matheus/programs/Real-time-Voice-fork/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
WARNING:tensorflow:From /home/matheus/programs/Real-time-Voice-fork/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/matheus/programs/Real-time-Voice-fork/synthesizer/models/modules.py:307: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.
WARNING:tensorflow:From /home/matheus/programs/Real-time-Voice-fork/synthesizer/models/modules.py:271: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:Entity <bound method ZoneoutLSTMCell.__call__ of <synthesizer.models.modules.ZoneoutLSTMCell object at 0x7f79cc0ba890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ZoneoutLSTMCell.__call__ of <synthesizer.models.modules.ZoneoutLSTMCell object at 0x7f79cc0ba890>>: ValueError: Failed to parse source code of <bound method ZoneoutLSTMCell.__call__ of <synthesizer.models.modules.ZoneoutLSTMCell object at 0x7f79cc0ba890>>, which Python reported as:
    def __call__(self, inputs, state, scope=None):
        """"""Runs vanilla LSTM Cell and applies zoneout.
        """"""
        # Apply vanilla LSTM
        output, new_state = self._cell(inputs, state, scope)

        if self.state_is_tuple:
            (prev_c, prev_h) = state
            (new_c, new_h) = new_state
        else:
            num_proj = self._cell._num_units if self._cell._num_proj is None else \
                                self._cell._num_proj
            prev_c = tf.slice(state, [0, 0], [-1, self._cell._num_units])
            prev_h = tf.slice(state, [0, self._cell._num_units], [-1, num_proj])
            new_c = tf.slice(new_state, [0, 0], [-1, self._cell._num_units])
            new_h = tf.slice(new_state, [0, self._cell._num_units], [-1, num_proj])

        # Apply zoneout
        if self.is_training:
            # nn.dropout takes keep_prob (probability to keep activations) not drop_prob (
                        # probability to mask activations)!
            c = (1 - self._zoneout_cell) * tf.nn.dropout(new_c - prev_c, (1 - self._zoneout_cell)) + prev_c
            h = (1 - self._zoneout_outputs) * tf.nn.dropout(new_h - prev_h, (1 - self._zoneout_outputs)) + prev_h
        else:
            c = (1 - self._zoneout_cell) * new_c + self._zoneout_cell * prev_c
            h = (1 - self._zoneout_outputs) * new_h + self._zoneout_outputs * prev_h

        new_state = tf.compat.v1.nn.rnn_cell.LSTMStateTuple(c, h) if self.state_is_tuple else tf.concat(1, [c,
                                                                                                  h])

        return output, new_state

This may be caused by multiline strings or comments not indented at the same level as the code.
WARNING:tensorflow:Entity <bound method ZoneoutLSTMCell.__call__ of <synthesizer.models.modules.ZoneoutLSTMCell object at 0x7f798c0710d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ZoneoutLSTMCell.__call__ of <synthesizer.models.modules.ZoneoutLSTMCell object at 0x7f798c0710d0>>: ValueError: Failed to parse source code of <bound method ZoneoutLSTMCell.__call__ of <synthesizer.models.modules.ZoneoutLSTMCell object at 0x7f798c0710d0>>, which Python reported as:
    def __call__(self, inputs, state, scope=None):
        """"""Runs vanilla LSTM Cell and applies zoneout.
        """"""
        # Apply vanilla LSTM
        output, new_state = self._cell(inputs, state, scope)

        if self.state_is_tuple:
            (prev_c, prev_h) = state
            (new_c, new_h) = new_state
        else:
            num_proj = self._cell._num_units if self._cell._num_proj is None else \
                                self._cell._num_proj
            prev_c = tf.slice(state, [0, 0], [-1, self._cell._num_units])
            prev_h = tf.slice(state, [0, self._cell._num_units], [-1, num_proj])
            new_c = tf.slice(new_state, [0, 0], [-1, self._cell._num_units])
            new_h = tf.slice(new_state, [0, self._cell._num_units], [-1, num_proj])

        # Apply zoneout
        if self.is_training:
            # nn.dropout takes keep_prob (probability to keep activations) not drop_prob (
                        # probability to mask activations)!
            c = (1 - self._zoneout_cell) * tf.nn.dropout(new_c - prev_c, (1 - self._zoneout_cell)) + prev_c
            h = (1 - self._zoneout_outputs) * tf.nn.dropout(new_h - prev_h, (1 - self._zoneout_outputs)) + prev_h
        else:
            c = (1 - self._zoneout_cell) * new_c + self._zoneout_cell * prev_c
            h = (1 - self._zoneout_outputs) * new_h + self._zoneout_outputs * prev_h

        new_state = tf.compat.v1.nn.rnn_cell.LSTMStateTuple(c, h) if self.state_is_tuple else tf.concat(1, [c,
                                                                                                  h])

        return output, new_state

This may be caused by multiline strings or comments not indented at the same level as the code.
initialisation done /gpu:0
Initialized Tacotron model. Dimensions (? = dynamic shape):
  Train mode:               False
  Eval mode:                False
  GTA mode:                 False
  Synthesis mode:           True
  Input:                    (?, ?)
  device:                   0
  embedding:                (?, ?, 512)
  enc conv out:             (?, ?, 512)
  encoder out (cond):       (?, ?, 768)
  decoder out:              (?, ?, 80)
  residual out:             (?, ?, 512)
  projected residual out:   (?, ?, 80)
  mel out:                  (?, ?, 80)
  <stop_token> out:         (?, ?)
  Tacotron Parameters       28.439 Million.
Loading checkpoint: synthesizer/saved_models/logs-pretrained/taco_pretrained/tacotron_model.ckpt-278000
2020-06-06 22:18:35.983701: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-06-06 22:18:35.988341: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2799925000 Hz
2020-06-06 22:18:35.988811: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55cfcc7261f0 executing computations on platform Host. Devices:
2020-06-06 22:18:35.988825: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2020-06-06 22:18:35.989070: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2020-06-06 22:18:35.989190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-06-06 22:18:35.989597: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2020-06-06 22:18:35.989635: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-06-06 22:18:35.989837: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory
2020-06-06 22:18:35.989954: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory
2020-06-06 22:18:35.990038: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory
2020-06-06 22:18:35.993748: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory
2020-06-06 22:18:35.993959: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory
2020-06-06 22:18:36.124646: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-06-06 22:18:36.124691: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...
2020-06-06 22:18:36.124904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-06-06 22:18:36.124954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0
2020-06-06 22:18:36.124987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N
2020-06-06 22:18:36.128068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-06-06 22:18:36.128683: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55cfcd793cb0 executing computations on platform CUDA. Devices:
2020-06-06 22:18:36.128718: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1050 Ti, Compute Capability 6.1
2020-06-06 22:18:36.304880: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
WARNING:tensorflow:From /home/matheus/programs/Real-time-Voice-fork/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Building Wave-RNN
Trainable Parameters: 4.481M
Loading model weights at vocoder/saved_models/pretrained/pretrained.pt
DEBUG: 16000
Expression 'paInvalidSampleRate' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 2043
Expression 'PaAlsaStreamComponent_InitialConfigure( &self->playback, outParams, self->primeBuffers, hwParamsPlayback, &realSr )' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 2716
Expression 'PaAlsaStream_Configure( stream, inputParameters, outputParameters, sampleRate, framesPerBuffer, &inputLatency, &outputLatency, &hostBufferSizeMode )' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 2837
Traceback (most recent call last):
  File ""/home/matheus/programs/Real-time-Voice-fork/toolbox/__init__.py"", line 89, in <lambda>
    func = lambda: self.synthesize() or self.vocode()
  File ""/home/matheus/programs/Real-time-Voice-fork/toolbox/__init__.py"", line 218, in vocode
    self.ui.play(wav, Synthesizer.sample_rate)
  File ""/home/matheus/programs/Real-time-Voice-fork/toolbox/ui.py"", line 144, in play
    sd.play(wav, sample_rate)
  File ""/home/matheus/programs/Real-time-Voice-fork/lib/python3.7/site-packages/sounddevice.py"", line 182, in play
    **kwargs)
  File ""/home/matheus/programs/Real-time-Voice-fork/lib/python3.7/site-packages/sounddevice.py"", line 2498, in start_stream
    **kwargs)
  File ""/home/matheus/programs/Real-time-Voice-fork/lib/python3.7/site-packages/sounddevice.py"", line 1455, in __init__
    **_remove_self(locals()))
  File ""/home/matheus/programs/Real-time-Voice-fork/lib/python3.7/site-packages/sounddevice.py"", line 861, in __init__
    'Error opening {0}'.format(self.__class__.__name__))
  File ""/home/matheus/programs/Real-time-Voice-fork/lib/python3.7/site-packages/sounddevice.py"", line 2653, in _check
    raise PortAudioError(errormsg, err)
sounddevice.PortAudioError: Error opening OutputStream: Invalid sample rate [PaErrorCode -9997]


```

</details>
</details>

Which comes back to the audio playback error... so I think I am very close. ",manage get last error simply else line statement line else even method blank pull request problem going use way right full output program python import module location import please update use pin version alias present version import import module location import please update use pin version alias present version import false loaded trained step found synthesizer trained step model warning calling removed future version call instance argument instead passing constructor warning removed future version use instead warning removed future version use instead particular used consult documentation warning dropout removed future version use instead warning removed future version please use cell equivalent warning removed future version please use cell equivalent warning removed future version use broadcast rule warning removed future version class equivalent warning dense removed future version use instead warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object parse source code bound method object python self state vanilla cell apply vanilla output state scope state else none else state state apply probability keep probability mask else else return output may indented level code warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object parse source code bound method object python self state vanilla cell apply vanilla output state scope state else none else state state apply probability keep probability mask else else return output may indented level code done model dynamic shape train mode false mode false mode false synthesis mode true input device cond residual residual mel million loading binary use frequency service platform host device undefined undefined successfully dynamic library successful node read negative value must least one node node zero found device name ti major minor successfully dynamic library could library open object file file directory could library open object file file directory could library open object file file directory could library open object file file directory could library open object file file directory successfully dynamic library skipping device interconnect strength edge matrix successful node read negative value must least one node node zero service platform device ti compute capability warning cluster set want either set use enable confirm active pas proper flag via set warning removed future version use standard file check prefix building trainable loading model expression line expression playback line expression stream line recent call last file line lambda lambda file line file line play file line play file line file line file line opening file line raise err error opening invalid sample rate come back audio playback error think close,issue,positive,positive,neutral,neutral,positive,positive
639991427,"By doing a print on the sample rate at line 141, that comes from the synthesizer i believe, i have 16000. I hard coded it to pass 44100 to portaudio and the error didn't appear but i didn't hear nothing as well. 

About the other issue with line 114 of ""synthesizer/models/modules.py"", Adding the named parameter to and adding num_layers to something (kinda random since I don't know nothing about AI :P) Makes it pass this line:

` self._cell = tf.contrib.cudnn_rnn.CudnnLSTM(num_units=num_units, name=name, num_layers=10) `

And get stuck at:
```
    return func(*args, **kwargs)
  File ""/home/matheus/programs/Real-time-Voice-fork/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn.py"", line 449, in bidirectional_dynamic_rnn
    rnn_cell_impl.assert_like_rnncell(""cell_fw"", cell_fw)
  File ""/home/matheus/programs/Real-time-Voice-fork/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py"", line 102, in assert_like_rnncell
    cell_name, cell, "", "".join(errors)))
TypeError: The argument 'cell_fw' (<synthesizer.models.modules.ZoneoutLSTMCell object at 0x7f71e83d3a90>) is not an RNNCell: 'output_size' property is missing, 'state_size' property is missing.
```
I really have the impression I can be running the wrong version of tensorflow but pip says it is 1.15. Maybe is a linux only issue idk?.

In case you are wondering I am running arch linux, I have just updated the nvidia drivers, i have a gtx 1050 ti with 4.2 Gb vram, and I created a virtualenv using conda. 
",print sample rate line come synthesizer believe hard pas error appear hear nothing well issue line parameter something random since know nothing ai pas line get stuck return file line file line cell argument object property missing property missing really impression running wrong version pip maybe issue case wondering running arch ti,issue,negative,negative,negative,negative,negative,negative
639986004,"Also having this problem when trying to synthesize:
```

Traceback (most recent call last):
  File ""/home/matheus/programs/Real-time-Voice-fork/toolbox/__init__.py"", line 89, in <lambda>
    func = lambda: self.synthesize() or self.vocode()
  File ""/home/matheus/programs/Real-time-Voice-fork/toolbox/__init__.py"", line 179, in synthesize
    specs = self.synthesizer.synthesize_spectrograms(texts, embeds)
  File ""/home/matheus/programs/Real-time-Voice-fork/synthesizer/inference.py"", line 77, in synthesize_spectrograms
    self.load()
  File ""/home/matheus/programs/Real-time-Voice-fork/synthesizer/inference.py"", line 58, in load
    self._model = Tacotron2(self.checkpoint_fpath, hparams)
  File ""/home/matheus/programs/Real-time-Voice-fork/synthesizer/tacotron2.py"", line 28, in __init__
    split_infos=split_infos)
  File ""/home/matheus/programs/Real-time-Voice-fork/synthesizer/models/tacotron.py"", line 146, in initialize
    zoneout=hp.tacotron_zoneout_rate, scope=""encoder_LSTM""))
  File ""/home/matheus/programs/Real-time-Voice-fork/synthesizer/models/modules.py"", line 221, in __init__
    name=""encoder_fw_LSTM"")
  File ""/home/matheus/programs/Real-time-Voice-fork/synthesizer/models/modules.py"", line 114, in __init__
    self._cell = tf.contrib.cudnn_rnn.CudnnLSTM(num_units, name=name)
TypeError: __init__() missing 1 required positional argument: 'num_units'

```",also problem trying synthesize recent call last file line lambda lambda file line synthesize spec file line file line load file line file line initialize file line file line missing positional argument,issue,negative,negative,neutral,neutral,negative,negative
639985160,"Is not detecting my audio output devices, this happens if I load a sample and hit play. And there's nothing on the speakers comboBox.

```
Expression 'paInvalidSampleRate' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 2043
Expression 'PaAlsaStreamComponent_InitialConfigure( &self->playback, outParams, self->primeBuffers, hwParamsPlayback, &realSr )' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 2716
Expression 'PaAlsaStream_Configure( stream, inputParameters, outputParameters, sampleRate, framesPerBuffer, &inputLatency, &outputLatency, &hostBufferSizeMode )' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 2837
Traceback (most recent call last):
  File ""/home/matheus/programs/Real-time-Voice-fork/toolbox/__init__.py"", line 83, in <lambda>
    func = lambda: self.ui.play(self.ui.selected_utterance.wav, Synthesizer.sample_rate)
  File ""/home/matheus/programs/Real-time-Voice-fork/toolbox/ui.py"", line 142, in play
    sd.play(wav, sample_rate)
  File ""/home/matheus/programs/Real-time-Voice-fork/lib/python3.7/site-packages/sounddevice.py"", line 182, in play
    **kwargs)
  File ""/home/matheus/programs/Real-time-Voice-fork/lib/python3.7/site-packages/sounddevice.py"", line 2498, in start_stream
    **kwargs)
  File ""/home/matheus/programs/Real-time-Voice-fork/lib/python3.7/site-packages/sounddevice.py"", line 1455, in __init__
    **_remove_self(locals()))
  File ""/home/matheus/programs/Real-time-Voice-fork/lib/python3.7/site-packages/sounddevice.py"", line 861, in __init__
    'Error opening {0}'.format(self.__class__.__name__))
  File ""/home/matheus/programs/Real-time-Voice-fork/lib/python3.7/site-packages/sounddevice.py"", line 2653, in _check
    raise PortAudioError(errormsg, err)
sounddevice.PortAudioError: Error opening OutputStream: Invalid sample rate [PaErrorCode -9997]
```",audio output load sample hit play nothing expression line expression playback line expression stream line recent call last file line lambda lambda file line play file line play file line file line file line opening file line raise err error opening invalid sample rate,issue,positive,neutral,neutral,neutral,neutral,neutral
637860870,"Maybe try converting it to FLAC or WAV? That worked for me
",maybe try converting worked,issue,negative,neutral,neutral,neutral,neutral,neutral
637851086,"@t-qureshi getting the following:

```
pip install tensorflow==1.14.0
ERROR: Could not find a version that satisfies the requirement tensorflow==1.14.0 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0)
ERROR: No matching distribution found for tensorflow==1.14.0

```",getting following pip install error could find version requirement error matching distribution found,issue,negative,neutral,neutral,neutral,neutral,neutral
637335039,"After installing it, SPyder says Zmq ImportError: DLL load failed: The specified module could not be found.",load module could found,issue,negative,neutral,neutral,neutral,neutral,neutral
637028120,"Hello, I've tried the workaround that was proposed and it only works on a single short sentence (due to its two clusters).

I couldn't use it for my project because I have a random number of random sentences as input (I can't predict the number of clusters I'll need for each line).

Nevertheless, I managed to find another simple workaround that removes every pause. 
I'm pretty sure that it can still be improved, but it still works really well in my case.

In `/toolbox/__init__.py` after `spec = np.concatenate(specs, axis=1)` simply add this :
```
spec_temp = np.transpose(spec)
arr = np.empty((0,80)) 
threshold = -3.9 # -4.0 works too, but it's the absolute limit
for i in spec_temp:
  if  np.average(i)>= threshold:
    arr = np.append(arr, np.array([i]), axis=0)
spec = np.transpose(arr)
```

----

When I run the toolbox and ask to synthesize :
```
this is cool
My name is Emile
```
I get this (I added an intermediary mel spectrogram for you to see) : 

![mel_spectrograms](https://i.imgur.com/6qoZKb5.png)

No pause. But I can still get some strange sounds from time to time.
 ",hello tried work single short sentence due two could use project random number random input ca predict number need line nevertheless find another simple every pause pretty sure still still work really well case spec spec simply add spec threshold work absolute limit threshold spec run toolbox ask synthesize cool name get added intermediary mel spectrogram see pause still get strange time time,issue,positive,positive,neutral,neutral,positive,positive
636392026,"There's an optimum point for length of input text - too short and you get hissy noise all over, too long and the speech rate goes way too high.  You just need to experiment with the length of input text strings.",optimum point length input text short get noise long speech rate go way high need experiment length input text,issue,negative,positive,positive,positive,positive,positive
635819103,"the tensor flow versions arent supporting in colab
",tensor flow arent supporting,issue,negative,positive,positive,positive,positive,positive
635806851,"for all those attempting to make it on a CPU http://www.xilodyne.com/SBS_RTVC_Demo_Setup 
this thread is amazing easily done in about 30 min but the results are quite flappy not satisfying i don't know why??
",make thread amazing easily done min quite satisfying know,issue,positive,positive,positive,positive,positive,positive
632404267,"> Thanks for the info. How do you save your recording once you are happy with the audio and text?
> I thought it was the ""Take genearated"" button but that doesn't do anything for me.

I also don't know how to save the generated audio. I use software that can record internally.You can learn about [VOICEMEETER BANANA](https://www.vb-audio.com/Voicemeeter/banana.htm).",thanks save recording happy audio text thought take button anything also know save audio use record learn banana,issue,positive,positive,positive,positive,positive,positive
632373815,"Thanks for the info.  How do you save your recording once you are happy with the audio and text?
I thought it was the ""Take genearated"" button but that doesn't do anything for me.",thanks save recording happy audio text thought take button anything,issue,positive,positive,positive,positive,positive,positive
631867212,"@Iamgoofball 
A little. Many of the calls and variables have been updated, but I believe it was this thread where I stated that nearly the entire program has to be recoded for 2.0 functionality. Compatibility is almost there, but functionality is a whole different ball game. That's an incredible amount of work that won't be done overnight. More like, I'll might slight improvements and take it in bite size chunks over time.",little many believe thread stated nearly entire program functionality compatibility almost functionality whole different ball game incredible amount work wo done overnight like might slight take bite size time,issue,negative,positive,positive,positive,positive,positive
629670938,"The error is coming from the file synthesizer\models\modules.py on line 115 an if statement is checking if torch is available and assigning self.cell  to either a CudnnLSTM or a LSTMBlockCell. The problem is that CudnnLSTM requires both the parameters num_layers and num_units, but only num_units iss being passed in. so the num_units being passed in is seen as num_layers giving the error. I am unsure of what value should be passed in for the layer. However a temp fix is to remove the if else statement and the line 
self._cell = tf.contrib.cudnn_rnn.CudnnLSTM(num_units, name=name) and only leave the line 
self._cell = tf.contrib.rnn.LSTMBlockCell(num_units=num_units, name=name) as the LSTMBlockCell does not require the num_layer parameter. A better fix however would be to pass in 
self._cell = tf.contrib.cudnn_rnn.CudnnLSTM(num_layers, num_units, name=name) 
on line 115 if that can be found.
            
       ",error coming file line statement torch available either problem seen giving error unsure value layer however temp fix remove else statement line leave line require parameter better fix however would pas line found,issue,negative,positive,positive,positive,positive,positive
628147349,"For those on Windows 10, using Python 3.7 + CUDA 10.0 + their corresponding versions of cuDNN and PyTorch have worked for me. I followed [this installation guide](https://poorlydocumented.com/2019/11/installing-corentinjs-real-time-voice-cloning-project-on-windows-10-from-scratch/) step by step.

I tried that TensorFlow 1.14 would not work with the newest CUDA 10.2 - a DLL error occurs when importing the library.",python corresponding worked installation guide step step tried would work error library,issue,negative,neutral,neutral,neutral,neutral,neutral
627642731,"> > > > @favarete I'm sorry but it always doesn't do anything and prints out ""Expection: "" under the big button under synthesize only and vocode only buttons.
> > > > ![image](https://user-images.githubusercontent.com/65095770/81487261-cb647f00-9263-11ea-975e-c9a925fa22a9.png) Please help me.
> > > 
> > > 
> > > ![image](https://user-images.githubusercontent.com/60817289/81488177-008ac480-9298-11ea-8bbe-56f4f7bf8c48.png)
> > > You check if the framed part of my photo is the same as yours.After selecting the sound file by clicking the browse button, does the sound spectrum appear in the lower right corner.
> > 
> > 
> > It didn't at first, but I realized I needed 5 seconds of audio or more. For some reason mp3 files refused to get imported but exporting them into .WAV using audacity fixed the issue. I'm so dumb..
> 
> Yes, it only supports wav . lol

then i wonder why it said that i could import any audio file lol",sorry always anything big button synthesize button image please help image check framed part photo sound file browse button sound spectrum appear lower right corner first audio reason get audacity fixed issue dumb yes wonder said could import audio file,issue,negative,positive,neutral,neutral,positive,positive
627606686,"I have this problem too!  Were you able to figure out why?  My conda info is:

```     active environment : forktts
    active env location : /data/nburns/anaconda3/envs/forktts
            shell level : 2
       user config file : /home/nburns/.condarc
 populated config files : /home/nburns/.condarc
          conda version : 4.8.3
    conda-build version : 3.15.1
         python version : 3.7.2.final.0
       virtual packages : __cuda=10.1
                          __glibc=2.17
       base environment : /data/nburns/anaconda3  (writable)
           channel URLs : https://repo.anaconda.com/pkgs/main/linux-64
                          https://repo.anaconda.com/pkgs/main/noarch
                          https://repo.anaconda.com/pkgs/r/linux-64
                          https://repo.anaconda.com/pkgs/r/noarch
          package cache : /data/nburns/anaconda3/pkgs
                          /home/nburns/.conda/pkgs
       envs directories : /data/nburns/anaconda3/envs
                          /home/nburns/.conda/envs
               platform : linux-64
             user-agent : conda/4.8.3 requests/2.19.1 CPython/3.7.2 Linux/3.10.0-862.14.4.el7.x86_64 centos/7.5.1804 glibc/2.17
                UID:GID : 31602:5000
             netrc file : None
           offline mode : False
```",problem able figure active environment active location shell level user file version version python version final virtual base environment writable channel package cache platform gid file none mode false,issue,negative,negative,negative,negative,negative,negative
627089720,@iwater Look forward to your work can be completed as soon as possible ,look forward work soon possible,issue,negative,neutral,neutral,neutral,neutral,neutral
627085829,"@zhilangtaosha that need more time, @CorentinJ pretrained that use 1 week with 4 GPUs, I only have one",need time use week one,issue,negative,neutral,neutral,neutral,neutral,neutral
626846031,"@zhilangtaosha 其他的模型文件需要依次训练，我之前训练了400人的3个模型，最新的其他模型正在依次训练中，训练好，我会继续放上去

The other model files need to be trained in sequence, I trained 3 models of 400 people earlier, the latest others are being trained in sequence, trained well, I'll keep putting them up",model need trained sequence trained people latest trained sequence trained well keep,issue,negative,positive,positive,positive,positive,positive
626354257,"Actually I just fixed it for myself.

I changed line 119 in toolbox/__init__.py  to:
`wav = Synthesizer.load_preprocess_wav(str(fpath))`
now it works without any problems.",actually fixed line work without,issue,negative,positive,neutral,neutral,positive,positive
626350651,"This bug is easy to debug. I was testing the algorithm so I hard-coded the url of the input files with the raw string modifier in python. E.g., r""PATH2MYFILE""",bug easy testing algorithm input raw string modifier python,issue,negative,positive,positive,positive,positive,positive
626348607,Bump. This issue is affecting me aswell!,bump issue affecting aswell,issue,negative,neutral,neutral,neutral,neutral,neutral
626323489,"> > > @favarete I'm sorry but it always doesn't do anything and prints out ""Expection: "" under the big button under synthesize only and vocode only buttons.
> > > ![image](https://user-images.githubusercontent.com/65095770/81487261-cb647f00-9263-11ea-975e-c9a925fa22a9.png) Please help me.
> > 
> > 
> > ![image](https://user-images.githubusercontent.com/60817289/81488177-008ac480-9298-11ea-8bbe-56f4f7bf8c48.png)
> > You check if the framed part of my photo is the same as yours.After selecting the sound file by clicking the browse button, does the sound spectrum appear in the lower right corner.
> 
> It didn't at first, but I realized I needed 5 seconds of audio or more. For some reason mp3 files refused to get imported but exporting them into .WAV using audacity fixed the issue. I'm so dumb..

Yes, it only supports wav .  lol",sorry always anything big button synthesize button image please help image check framed part photo sound file browse button sound spectrum appear lower right corner first audio reason get audacity fixed issue dumb yes,issue,negative,positive,neutral,neutral,positive,positive
626321555,"> > @favarete I'm sorry but it always doesn't do anything and prints out ""Expection: "" under the big button under synthesize only and vocode only buttons.
> > ![image](https://user-images.githubusercontent.com/65095770/81487261-cb647f00-9263-11ea-975e-c9a925fa22a9.png) Please help me.
> 
> ![image](https://user-images.githubusercontent.com/60817289/81488177-008ac480-9298-11ea-8bbe-56f4f7bf8c48.png)
> 
> You check if the framed part of my photo is the same as yours.After selecting the sound file by clicking the browse button, does the sound spectrum appear in the lower right corner.

It didn't at first, but I realized I needed 5 seconds of audio or more. For some reason mp3 files refused to get imported but exporting them into .WAV using audacity fixed the issue. I'm so dumb..",sorry always anything big button synthesize button image please help image check framed part photo sound file browse button sound spectrum appear lower right corner first audio reason get audacity fixed issue dumb,issue,negative,positive,neutral,neutral,positive,positive
626254229,"


> @favarete I'm sorry but it always doesn't do anything and prints out ""Expection: "" under the big button under synthesize only and vocode only buttons.
> ![image](https://user-images.githubusercontent.com/65095770/81487261-cb647f00-9263-11ea-975e-c9a925fa22a9.png) Please help me.

![image](https://user-images.githubusercontent.com/60817289/81488177-008ac480-9298-11ea-8bbe-56f4f7bf8c48.png)


You check if the framed part of my photo is the same as yours.After selecting the sound file by clicking the browse button, does the sound spectrum appear in the lower right corner.",sorry always anything big button synthesize button image please help image check framed part photo sound file browse button sound spectrum appear lower right corner,issue,negative,positive,positive,positive,positive,positive
626247829,"@favarete I'm sorry but it always doesn't do anything and prints out ""Expection: "" under the big button under synthesize only and vocode only buttons.
![image](https://user-images.githubusercontent.com/65095770/81487261-cb647f00-9263-11ea-975e-c9a925fa22a9.png) Please help me.
",sorry always anything big button synthesize button image please help,issue,positive,negative,negative,negative,negative,negative
626205718,"I fork this repo and add encoder preprocess scripts for Chinese datasets (aishell1,magicdata,aidatatang,thchs30,mozilla,primewords,stcmds) total 3865 speakers and release pretrained encoder model with Chinese datasets
https://github.com/iwater/Real-Time-Voice-Cloning-Chinese

![newplot](https://user-images.githubusercontent.com/108937/81479983-3a88a600-9259-11ea-80fe-c291fce47673.png)
![newplot1](https://user-images.githubusercontent.com/108937/81479984-3eb4c380-9259-11ea-9b23-b152feec420c.png)
<img width=""559"" alt=""截屏2020-05-10 上午12 59 37"" src=""https://user-images.githubusercontent.com/108937/81480030-93583e80-9259-11ea-9cb8-1d2cd7bd0026.png"">
",fork add total release model,issue,negative,neutral,neutral,neutral,neutral,neutral
625972723,"Memory doesn't ever go above 2.2GiB, and the CPU goes up for a few seconds, and then drops back down after synthesizing is finished. It's a vocoder problem, because the spectrogram is generated, and the crash occurs after that. here's the log in a text file since it's a tad too big to put in a github comment.
[errormessage.txt](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/4601113/errormessage.txt)

",memory ever go go back finished problem spectrogram crash log text file since tad big put comment,issue,negative,neutral,neutral,neutral,neutral,neutral
625942487,"@Knucklesfan Yeah, that should be more than enough. You might have to send me your error. Open task manager while you run it and see what it does to your RAM and CPU.",yeah enough might send error open task manager run see ram,issue,negative,neutral,neutral,neutral,neutral,neutral
625603049,@Knucklesfan What's your system specs? It might be too low of RAM.,system spec might low ram,issue,negative,neutral,neutral,neutral,neutral,neutral
625531626,"it works!! I gave it a shot just yesterday and was pleased to hear the AI speaking back to me. It still has a few bugs, though. If you try to load in a paragraph longer than 5-6 lines, it crashes upon attempting to synthesize and vocode, with the core being dumped. I can provide logs in a little bit, but it should be easy enough to reproduce just by opening it up and clicking synthesize and vocode with the default text paragraph.",work gave shot yesterday hear ai speaking back still though try load paragraph longer upon synthesize core provide little bit easy enough reproduce opening synthesize default text paragraph,issue,negative,positive,neutral,neutral,positive,positive
624775513,"@bmills20 I also have this error. I ran these commands:
conda install -c conda-forge python-sounddevice
conda install -c conda-forge/label/gcc7 python-sounddevice
conda install -c conda-forge/label/cf201901 python-sounddevice
conda install -c conda-forge/label/cf202003 python-sounddevice
but i still get the error.",also error ran install install install install still get error,issue,negative,neutral,neutral,neutral,neutral,neutral
624299869,"> 
> 
> Had the same error, was solved through removing numpy and installing the correct numpy version, making sure the Torch version you are using is compatible with it (the error is thrown b/c Python doesn't know how to import torch without NumPy).
> 
> For me, it was numpy-1.18.4+mkl-cp37-cp37m-win_amd64.whl (cp37 refers to python 3.7, windows 64bit).

So you're saying that version 1.18.4 is the incompatible version, or vice-a-versa?",error removing correct version making sure torch version compatible error thrown python know import torch without python bit saying version incompatible version,issue,negative,positive,positive,positive,positive,positive
624003460,"@pusalieth I should've rephrased that the Open Source comments were in relation to AI.  

essentially what @CorentinJ said:

> If you expect a student who just finished university to work full-time on his own and for free on an open source project, you've probably never put a foot in the real world",open source relation ai essentially said expect student finished university work free open source project probably never put foot real world,issue,positive,positive,positive,positive,positive,positive
623772957,"@oneduality make sure you installed the correct sounddevice version. After trial and error of the wrong version, I a wheel that works on my system. ""sounddevice-0.3.15-cp37-cp37m-win_amd64.whl"" (cp37 refers to python 3.7, amd64 is 64 bit windows)

I understand this is an old thread but just wanted to help if you hadn't solved it.",make sure correct version trial error wrong version wheel work system python bit understand old thread help,issue,negative,positive,neutral,neutral,positive,positive
623771729,"Had the same error, was solved through removing numpy and installing the correct numpy version, making sure the Torch version you are using is compatible with it (the error is thrown b/c Python doesn't know how to import torch without NumPy). 

For me, it was numpy-1.18.4+mkl-cp37-cp37m-win_amd64.whl (cp37 refers to python 3.7, windows 64bit).",error removing correct version making sure torch version compatible error thrown python know import torch without python bit,issue,negative,positive,positive,positive,positive,positive
623352404,"Lads if you want to ask Resemble.AI to make the project open source, you go ahead and do it. If you expect a student who just finished university to work full-time on his own and for free on an open source project, you've probably never put a foot in the real world. The closest to what you're asking is [Mozilla's repo](https://github.com/mozilla/TTS).

> well TTS and voice cloning aren't exactly the same, I don't need a TTS service, I need to replicate a voice, that's all

Yeah they are the same, and you'll get all the features in this repo from Mozzila's repo. Last I checked, erogol had a lot of features from different papers implemented, including sv2tts.
",want ask make project open source go ahead expect student finished university work free open source project probably never put foot real world well voice exactly need service need replicate voice yeah get last checked lot different,issue,positive,positive,positive,positive,positive,positive
623203799,"@jardayn 
> adrifcastr Got a link to the bot?
> 
> Also Open Source API's hurt the income of people. Not surprised that the best stuff is commercial.
> 
> 

I don't want to get into the tit for tat, which is where I think this thread seems to be going. If people want to release open source, it's their choice. Or they can monetize it, and if it's valuable enough to people who can't do it themselves, they'll buy it and that's literally the definition of commerce. Doesn't matter either way to me, but I do prefer open source, and here's why.

Linux, Apache, SQL, and php were/are the backbone of the internet, and all 4 are open source. All major corporation servers are open source. SSL is open source. Google runs 99% open source, including their products, like Android, YouTube, etc. Facebook almost completely runs on open source. Literally the richest software companies in the world run on open source. So, wealth generation and source type are not inextricably linked. There's a few exceptions, but the majority of wealthy corporations run the majority of their software using open source. 

Open source benefits the maximal amount of people with the least of amount of money. That's why I would choose open source over closed 80% of the time. The only trouble is certain pieces of software, it's extremely hard to monetize, so they use walled gardens instead. When it comes to AI, I'm in full support of OpenAI objectives. This is the prime time to make everything open source, and sell the models, or usage. That's just my opinion.",got link bot also open source hurt income people best stuff commercial want get tit tat think thread going people want release open source choice monetize valuable enough people ca buy literally definition commerce matter either way prefer open source apache backbone open source major corporation open source open source open source like android almost completely open source literally world run open source wealth generation source type inextricably linked majority wealthy run majority open source open source maximal amount people least amount money would choose open source closed time trouble certain extremely hard monetize use walled instead come ai full support prime time make everything open source sell usage opinion,issue,positive,positive,neutral,neutral,positive,positive
623201705,"well TTS and voice cloning aren't exactly the same, I don't need a TTS
service, I need to replicate a voice, that's all

On Mon., May 4, 2020, 00:35 jardayn, <notifications@github.com> wrote:

> But for what Jemine is working, there are commercial applications, so...
> yeah.
>
> I mean, if you want Voice Gen, there's Mozilla TTS
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/332#issuecomment-623193837>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AFI3T7QVDNQ5SHL6ODDAALDRPXWUBANCNFSM4MT2NAXA>
> .
>
",well voice exactly need service need replicate voice may wrote working commercial yeah mean want voice gen reply directly view,issue,positive,positive,neutral,neutral,positive,positive
623193837,"But for what Jemine is working, there are commercial applications, so... yeah.

I mean, if you want Voice Gen, there's Mozilla TTS",working commercial yeah mean want voice gen,issue,negative,negative,negative,negative,negative,negative
623193401,"> @adrifcastr Got a link to the bot?
> 
> Also Open Source API's hurt the income of people. Not surprised that the best stuff is commercial.
> 
> 

[here](https://github.com/gideonbot) you go, and well, I'm also not making money of my API, I just _provide_ it.",got link bot also open source hurt income people best stuff commercial go well also making money,issue,negative,positive,positive,positive,positive,positive
623192994,"@adrifcastr Got a link to the bot?

Also Open Source API's hurt the income of people. Not surprised that the best stuff is commercial.

",got link bot also open source hurt income people best stuff commercial,issue,negative,positive,positive,positive,positive,positive
623182120,"Well it's kinda sad to see this go behind a paywall, I myself am trying to
use this for my open source Discord Bot, so it's really sad to see this
paywalled behind resemble.ai, a free open source API never hurt anyone

On Sun., May 3, 2020, 20:35 Corentin Jemine, <notifications@github.com>
wrote:

> I finished my thesis around June last year and got offers as soon as I
> made the repo public. I started working immediately after that. I did
> expect to maintain the project a little more than that, but it was without
> accounting for the fact that I would work in a very similar vein, and thus
> having to keep the advancements for myself.
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/332#issuecomment-623159087>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AFI3T7WWKIFOMYC3RMKBRNDRPW2QDANCNFSM4MT2NAXA>
> .
>
",well sad see go behind trying use open source discord bot really sad see behind free open source never hurt anyone may wrote finished thesis around june last year got soon made public working immediately expect maintain project little without accounting fact would work similar vein thus keep reply directly view,issue,negative,negative,negative,negative,negative,negative
623159087,"I finished my thesis around June last year and got offers as soon as I made the repo public. I started working immediately after that. I did expect to maintain the project a little more than that, but it was without accounting for the fact that I would work in a very similar vein, and thus having to keep the advancements for myself.",finished thesis around june last year got soon made public working immediately expect maintain project little without accounting fact would work similar vein thus keep,issue,negative,negative,neutral,neutral,negative,negative
623089634,"@pusalieth also why Conda? Lots of people are using normal python venvs.


@CorentinJ thanks for the info. Yeah, there are loads of way of improving this.",also lot people normal python thanks yeah way improving,issue,positive,positive,positive,positive,positive,positive
623068627,"@CorentinJ 
Thanks for your input. People may not have noticed on your readme that you didn't plan on maintaining it pass Sep 2019. I've only gotten mixed up into ML for like 2 weeks now. Eventually I'll probably go for a Master's in it, who knows. I've got such little experience in it though, your advice is way out of my scope. I'm just coding to use the foundation you built, and make it work to it's maximal capacity.

Just out of curiosity, It's looks like you worked on it for a year or so past your thesis. Did you get hired, or co-found?",thanks input people may plan pas gotten mixed like eventually probably go master got little experience though advice way scope use foundation built make work maximal capacity curiosity like worked year past thesis get hired,issue,positive,negative,neutral,neutral,negative,negative
623067645,@Knucklesfan Will do. My only goals left for the software is to get tensorflow 2.0 working. Then I can implement a version for using AMD compute. That'll essentially cover all the bases I think at that point. Nothing really more than could be done. Anything left is just simple maintenance.,left get working implement version compute essentially cover base think point nothing really could done anything left simple maintenance,issue,negative,negative,negative,negative,negative,negative
623067225,"@jardayn 
Don't know for sure, but I would guess because the file was in your root dir. Could be many other things though. Either way, it's works now for sure.

I can tell you I'd only want to install pytorch from conda. There's so many dependencies, and conda already has the work done.",know sure would guess file root could many though either way work sure tell want install many already work done,issue,positive,positive,positive,positive,positive,positive
623032149,"I'm sure this makes no difference to you, but I want to make a note that this project was my thesis and nothing more. Making it open-source was one of the goals, but beyond a working prototype there were no real plans of maintaining it long-term. 

While I cannot share all the differences and improvements from our implementation at Resemble.AI and this one, I can definitely shed light on what is worth rewriting for this project:
- I highly suggest you entirely get rid of tensorflow. Yes, this is a big piece of work. Among open source implementations of Tacotron, I recommend either Fatchord's or Mozilla's.
- If the step above doesn't cover that, get rid of forward attention. Plenty new attention mechanism specific to TTS came out in the last years, which all perform better than forward attention and are usually much simpler.
- Likewise, you might be able to find a better vocoder. I haven't made a search for open-source available ones recently, but you might find some new ones that are faster or/and more robust to artifacts.
- I would consider using a unified hyperparameters paradigm among the 3 models.",sure difference want make note project thesis nothing making one beyond working prototype real share implementation one definitely shed light worth project highly suggest entirely get rid yes big piece work among open source recommend either step cover get rid forward attention plenty new attention mechanism specific came last perform better forward attention usually much simpler likewise might able find better made search available recently might find new faster robust would consider unified paradigm among,issue,positive,positive,positive,positive,positive,positive
623007606,I've got the same problem and can confirm that @lilydjwg's solution solved it.,got problem confirm solution,issue,negative,neutral,neutral,neutral,neutral,neutral
622962747,"CPU-only inference is currently not supported ? i flow this ,then show me  not support",inference currently flow show support,issue,negative,neutral,neutral,neutral,neutral,neutral
622955870,"Sounds good. If there's anything you need tested, I'm completely ready to help out.",good anything need tested completely ready help,issue,positive,positive,positive,positive,positive,positive
622934006,"@pusalieth
python demo_cli.py

Versions (tell me if i missed anything)

Ubuntu 18.04
Python 3.6
Latest Nvidia CUDA
Nvidia 440 drivers
All the versions from requirements.txt (it's missing torch)
Torch - latest one IntelliJ installed. (latest I guess)

mp3's were in the root directory
",python tell anything python latest missing torch torch latest one latest guess root directory,issue,negative,positive,positive,positive,positive,positive
622670975,"Alright guys, I think there's really only two things left now. Train new models on a CPU, and replace tf.contrib with tensorflow v2 compatible functions. The second will require a pretty extensive rewrite, so don't expect anything soon. Probably a few weeks at least.

I'll start running the training models on my CPU in the next couple days, and see how long it would take to train models on my CPU.",alright think really two left train new replace compatible second require pretty extensive rewrite expect anything soon probably least start running training next couple day see long would take train,issue,positive,positive,neutral,neutral,positive,positive
622650222,"@Knucklesfan Is see the warning now. It's due to librosa calling on numba, when numba is deprecating the import. So, I updated the call on librosa, and submitted a pull request. That'll update librosa for when numba 0.50.0 rolls out.

I also updated this program to use soundfile to write the resultant wav file to disk, since librosa deprecated the write to file function. So, we're now ready for librosa 0.8.0. Whew... LOL

Now I just need to work on tensorflow. I've already implemented a few changes, including while reading the docs to more effective functions for performance.",see warning due calling import call pull request update also program use write resultant file disk since write file function ready whew need work already reading effective performance,issue,negative,positive,positive,positive,positive,positive
622546186,"@Knucklesfan Good deal. Good to hear. Yes, the warnings are due to the fact the package uses tensforflow's apiv1, so it'll warning to say change those function calls to update to apiv2. Eventually I'll update them so we can migrate to the latest version.

The only true ""error"" that I've seen thus far, is something like this.
`Loading checkpoint: synthesizer\saved_models\logs-pretrained\taco_pretrained\tacotron_model.ckpt-278000
2020-05-01 13:08:29.021006: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-05-01 13:08:29.033412: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found`

That's what the windows error looks like when synthesizing. Linux will say something about failure to load checkpoint. This is due to the fact the checkpoint was produced using cuda, and not a cpu. So there's no model method to recall that checkpoint. I'll have to train a new model, solely on a cpu, and then upload that for usage.

As far as it goes though, the program should all run just fine now on cpu only.",good deal good hear yes due fact package warning say change function update eventually update migrate latest version true error seen thus far something like loading binary use could load dynamic library found error like say something failure load due fact produced model method recall train new model solely usage far go though program run fine,issue,positive,positive,positive,positive,positive,positive
622543862,@jardayn What command did you run? What OS are you using? And what is the path of the file you input?,command run o path file input,issue,negative,neutral,neutral,neutral,neutral,neutral
622453877,"Everything is working great! I do now get a NumbaDeprecationWarning, but it's just a warning so I don't think its anything to note. Great work!!",everything working great get warning think anything note great work,issue,positive,positive,positive,positive,positive,positive
622327500,@pusalieth I was using the original version with none of your changes present.,original version none present,issue,negative,positive,positive,positive,positive,positive
622212123,"Alright guys. That should be it. Windows good. GUI good on linux. Just need to train a CPU model now.

I'll probably start hosting some of the files on my own server so the download for models and datasets can be automated as well.",alright good good need train model probably start hosting server well,issue,positive,positive,positive,positive,positive,positive
622189710,@Knucklesfan I'm going to start working on your issue and get the gui running.,going start working issue get running,issue,negative,neutral,neutral,neutral,neutral,neutral
622189280,"@adrifcastr Alright, I think the issues are fixed with the windows version. I made a .bat file for setup. FYI, pytorch had to be my first install on conda, then with the conda prompt, use pip install for the rest. You may need a new env, or reinstall if you get weird errors. I can confirm though the cpu processing does work on windows.",alright think fixed version made file setup first install prompt use pip install rest may need new reinstall get weird confirm though work,issue,negative,negative,neutral,neutral,negative,negative
622118357,@adrifcastr Give me a bit. I'm working on windows right now. It's easier than I anticipated. I'll push to the repo soon.,give bit working right easier push soon,issue,negative,positive,positive,positive,positive,positive
622108898,"yeah I can use WSL, I'll just need to resetup python and the dependencies first",yeah use need python first,issue,negative,positive,positive,positive,positive,positive
622078454,"I'm getting a virtual machine up and running, so I can work on the gui portion. We'll see where that goes.

The dll issue must be a windows thing. I'm not sure how it loads tensorflow in windows, never used it. Right now, I'm going to debug the qt gui. Next I'll see how hard it is to use windows. In the mean time, if you use wsl like I am, it'll stream line things quite a bit.",getting virtual machine running work portion see go issue must thing sure never used right going next see hard use mean time use like stream line quite bit,issue,positive,positive,neutral,neutral,positive,positive
622075955,"@jardayn 
You might be using a version where I already fixed the issue with path. The demo_cli.py is the script I used/tested with.",might version already fixed issue path script,issue,negative,positive,neutral,neutral,positive,positive
622064065,"> In your pip usage, make sure you use 3.7, the libraries only work up to python 3.7.x. you'll see that as the very first line in requirements.txt.
> 
> For tensorflow 1.15.x, you can use pip, or conda. Sometimes anaconda is much easier to control the versions by using specific channels.

Aight so install worked, but running it errors out at tensorflow,

- Win 10
- Python 3.7.X
- all requirements installed
- latest pytorch installed
- CUDA 10.2
- cudnn installed

Stacktrace: https://hasteb.in/matojeme.coffeescript",pip usage make sure use work python see first line use pip sometimes anaconda much easier control specific install worked running win python latest,issue,positive,positive,positive,positive,positive,positive
622047694,"@pusalieth  I don't have a full understanding of all this stuff yet but I'm willing to help where I can. If you take this and work on it, we can just work out of your fork. There are already good solutions for video that are maintained, we need something in the audio world that is also maintained.",full understanding stuff yet willing help take work work fork already good video need something audio world also,issue,positive,positive,positive,positive,positive,positive
622038270,"Same problem. I install python 3.7.7 (64bit) and it start download.
But there are a lot of other problems, i spent 2 days to done it, but it didn't work.
I install MS VS Community 2017, Cuda 10, cudnn7, and other solutions from stackoverflow and it don't start working(((",problem install python bit start lot spent day done work install community start working,issue,negative,negative,neutral,neutral,negative,negative
621885130,"Okay, the error still occurs with PosixPath, same error format and everything. I checked to see if maybe git didn't update the repo but yeah still on the latest commit and the posixpath still fails.",error still error format everything checked see maybe git update yeah still latest commit still,issue,negative,positive,positive,positive,positive,positive
621875206,You are probably running a 32-bit version of Python. That happened to me and once I switched to 64-bit I was good to go.,probably running version python switched good go,issue,negative,positive,positive,positive,positive,positive
621732591,"@pusalieth 
`python demo_cli.py`

Versions (tell me if i missed anything)

Ubuntu 18.04
Python 3.6
Latest Nvidia CUDA 
Nvidia 440 drivers
All the versions from requirements.txt (it's missing torch)
Torch - latest one IntelliJ installed.


",python tell anything python latest missing torch torch latest one,issue,negative,positive,positive,positive,positive,positive
621715876,"@jardayn 
Which commands are you using to run the program? If you can get versions, that would be icing on the cake.",run program get would icing cake,issue,negative,neutral,neutral,neutral,neutral,neutral
621713137,"@pusalieth gonna leave this here: 
https://github.com/pusalieth/Real-Time-Voice-Cloning/commit/b333e733f516a7e13bccf7d1623ab35439fc9aa5

mp3's work just fine on Linux, without that change.
",gon na leave work fine without change,issue,negative,positive,positive,positive,positive,positive
621704985,"Idc about the main repo, you're the one who's putting his time into it, so
your fork is the one I'll be using

On Thu., Apr. 30, 2020, 02:17 pusalieth, <notifications@github.com> wrote:

> I'm going to be throwing some pretty heavy time into this, so up to you
> guys. Hopefully in the end it'll all get merged upstream.
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/332#issuecomment-621537450>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AFI3T7S7IY6ZGW4WP7GEDUDRPC7SRANCNFSM4MT2NAXA>
> .
>
",main one time fork one wrote going throwing pretty heavy time hopefully end get upstream reply directly view,issue,positive,positive,neutral,neutral,positive,positive
621537450,"I'm going to be throwing some pretty heavy time into this, so up to you guys. Hopefully in the end it'll all get merged upstream.",going throwing pretty heavy time hopefully end get upstream,issue,positive,positive,neutral,neutral,positive,positive
621498646,"In your pip usage, make sure you use 3.7, the libraries only work up to python 3.7.x. you'll see that as the very first line in requirements.txt.

For tensorflow 1.15.x, you can use pip, or conda. Sometimes anaconda is much easier to control the versions by using specific channels.",pip usage make sure use work python see first line use pip sometimes anaconda much easier control specific,issue,positive,positive,positive,positive,positive,positive
621491167,"> Let me know your results. I'm running all this form wsl in Windows 10 with Ubuntu 20.04 too, just no gui.
install fails immediately at tensorflow for me
`C:\Users\Adrian\code\Real-Time-Voice-Cloning>pip3.8 install -r requirements.txt
ERROR: Could not find a version that satisfies the requirement tensorflow==1.15 (from -r requirements.txt (line 2)) (fro
m versions: none)
ERROR: No matching distribution found for tensorflow==1.15 (from -r requirements.txt (line 2))`",let know running form install immediately pip install error could find version requirement line fro none error matching distribution found line,issue,negative,neutral,neutral,neutral,neutral,neutral
621484722,I believe this can be closed as @pusalieth seems to be working on their fork,believe closed working fork,issue,negative,negative,neutral,neutral,negative,negative
621402005,"Let me know your results. I'm running all this within wsl on Windows 10 with Ubuntu 20.04 too, just no gui.",let know running within,issue,negative,neutral,neutral,neutral,neutral,neutral
621293451,"There are lots of reasons why you wouldn't want to use the commercial service that made this open source project abandoned. Personally, I don't want to upload any voice data to someone else's servers. I'm hoping that someone who understands this stuff better than me picks this up to maintain and then we as a community can contribute and keep it going.",lot would want use commercial service made open source project abandoned personally want voice data someone else someone stuff better maintain community contribute keep going,issue,negative,positive,positive,positive,positive,positive
621290175,"Awesome, giving this a shot right now. I'm on an ubuntu 20.04 machine, and im using conda to actually run the python install. I'll be happy to test anything.",awesome giving shot right machine actually run python install happy test anything,issue,positive,positive,positive,positive,positive,positive
621050707,"Alright, I think I might have fixed your issue. I'm still working on getting Qt to work with wsl, and a x server... So, you might have to be my gui tester for now.",alright think might fixed issue still working getting work server might tester,issue,negative,positive,neutral,neutral,positive,positive
621032549,"I'll work on pathlib tonight and see if I can hunt down the issue... All work for no return, Pathlib!  ;)",work tonight see hunt issue work return,issue,negative,neutral,neutral,neutral,neutral,neutral
621032050,"That last part is your issue.
`TypeError: Invalid file: PosixPath('LibriSpeech/train-clean-100/1034/121119/1034-121119-0026.flac')`
Don't really know what's going on with pathlib, but it's royally screwing up. It was working fine for me at first, then literally out of no where it stopped working. I'm sure it's some kinda of versioning error.

The issue is here,
`""/home/knucklesfan/Documents/faketime/Real-Time-Voice-Cloning-master/toolbox/__init__.py"", line 59`
An fpath object is being passed to the function. How are you running it? I just used the terminal. But whatever is passing the file object to that function, the fpath is f'ed. I personally hate pathlib... adds unnecessary complexity. If you can figure out a way to pass a string directly to the function, it'll work.

Just fyi too, the cpu support is there, however the trained model is for cuda. So, you'll need to train a new checkpoint. There's where I'm currently at.",last part issue invalid file really know going royally screwing working fine first literally stopped working sure error issue line object function running used terminal whatever passing file object function personally hate unnecessary complexity figure way pas string directly function work support however trained model need train new currently,issue,negative,positive,neutral,neutral,positive,positive
620876407,"Hi, gave this branch a shot since I'm on a mostly AMD machine and liked the prospect of CPU support. On an attempt of loading audio from librispeech, i get this error
`Traceback (most recent call last):
  File ""/home/knucklesfan/Documents/faketime/Real-Time-Voice-Cloning-master/toolbox/__init__.py"", line 59, in <lambda>
    self.ui.browser_load_button.clicked.connect(lambda: self.load_from_browser())
  File ""/home/knucklesfan/Documents/faketime/Real-Time-Voice-Cloning-master/toolbox/__init__.py"", line 119, in load_from_browser
    wav = Synthesizer.load_preprocess_wav(fpath)
  File ""/home/knucklesfan/Documents/faketime/Real-Time-Voice-Cloning-master/synthesizer/inference.py"", line 111, in load_preprocess_wav
    wav = librosa.load(fpath, hparams.sample_rate)[0]
  File ""/home/knucklesfan/miniconda3/envs/tensorflow_improved/lib/python3.7/site-packages/librosa/core/audio.py"", line 129, in load
    with sf.SoundFile(path) as sf_desc:
  File ""/home/knucklesfan/miniconda3/envs/tensorflow_improved/lib/python3.7/site-packages/soundfile.py"", line 740, in __init__
    self._file = self._open(file, mode_int, closefd)
  File ""/home/knucklesfan/miniconda3/envs/tensorflow_improved/lib/python3.7/site-packages/soundfile.py"", line 1263, in _open
    raise TypeError(""Invalid file: {0!r}"".format(self.name))
TypeError: Invalid file: PosixPath('LibriSpeech/train-clean-100/1034/121119/1034-121119-0026.flac')
` I can't load in any audio from a directory, but the recording feature works as intended. Using python 3.7, and all dependencies from requirements.txt",hi gave branch shot since mostly machine prospect support attempt loading audio get error recent call last file line lambda lambda file line file line file line load path file line file file line raise invalid file invalid file ca load audio directory recording feature work intended python,issue,negative,positive,positive,positive,positive,positive
619560975,"Hello, for me works fine to cluster spectrogram and that delete all unnescessary parts
`import sklearn`
`signal = np.transpose(specs[0])`

`kmeans = sklearn.cluster.KMeans(n_clusters=2, random_state=0).fit(signal)`
`mask = kmeans.labels_.astype('bool')`
`results = np.transpose(signal[mask,...)`
",hello work fine cluster spectrogram delete import signal spec signal mask signal mask,issue,negative,positive,positive,positive,positive,positive
619446644,"I faced the same problem. There were two issues I had to resolve:
1. Looks like librosa does not like a Path object being sent to the load function. You will need to convert it to a string with full path, like this: str(path). You can change this in synthesizer/inference.py's load_preprocess_wav function.
2. Secondly, make sure your WAV file is 16000 Hz, 16 bit PCM. You can use audacity on windows to convert audio format.

Hope this helps.

Cheers
Raghu",faced problem two resolve like like path object sent load function need convert string full path like path change function secondly make sure file bit use audacity convert audio format hope,issue,positive,positive,positive,positive,positive,positive
618702838,So you closed your issue. Were you successful in getting it to run on a Mac? ,closed issue successful getting run mac,issue,positive,positive,positive,positive,positive,positive
618701661,"Same. I did all the installs, but when I run `python demo_cli.py`
I get:
File ""demo_cli.py"", line 45
    ""not supported."", file=sys.stderr)",run python get file line,issue,negative,neutral,neutral,neutral,neutral,neutral
618527230,I don't think that tensorflow 1.14 is available from pypl anymore. You will need to download a release of 1.14 directly from the tensorflow github repo.,think available need release directly,issue,negative,positive,positive,positive,positive,positive
618190848,My problem was solved by fully extracting the dataset files. They were .gz extracted to .tar then extracted to a folder which is recognized when I enter python demo_toolbox.py -d D:\Datasets,problem fully extracted extracted folder enter python,issue,negative,neutral,neutral,neutral,neutral,neutral
617543789,"I am also getting this error

Running Windows 10 and Python 3.8",also getting error running python,issue,negative,neutral,neutral,neutral,neutral,neutral
617083825,"I think we can use also tensorflow 2 using the import at the code:
```
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
```
Found at [https://www.tensorflow.org/guide/migrate](https://www.tensorflow.org/guide/migrate)

Using **mac** and **tensorflow-gpu** version **1.14.0** is impossible since it does not exists on the pypi directory https://github.com/tensorflow/tensorflow/issues/8251#issuecomment-617080152",think use also import code import found mac version impossible since directory,issue,negative,negative,negative,negative,negative,negative
617012042,"you should use **python demo_toolbox.py -d D:\Datasets\**
rather than **python demo_toolbox.py -d D:\Datasets\LibriSpeech\train-clean-100**",use python rather python,issue,negative,neutral,neutral,neutral,neutral,neutral
617004072,"Currently, Google Colab defaults to using Tensorflow 2 so we need to first change to Tensorflow 1 (see here for more info : https://colab.research.google.com/notebooks/tensorflow_version.ipynb) . But Google Colab is only working with specific versions of Tensorflow and the version that is imported in the file requirement.txt is too old. So a quick fix is to import everything manually except tensorflow instead of importing through requirement.txt.
",currently need first change see working specific version file old quick fix import everything manually except instead,issue,negative,positive,positive,positive,positive,positive
616677498,"@shawwn 
I get this error
""CUDA driver library cannot be found.
If you are sure that a CUDA driver is installed,
try setting environment variable NUMBA_CUDA_DRIVER
with the file path of the CUDA driver shared library.""
And I cannot see the generated one, any clue?

",get error driver library found sure driver try setting environment variable file path driver library see one clue,issue,negative,positive,positive,positive,positive,positive
616172521,"The pip version shouldn't matter but to upgrade pip run `pip install --upgrade pip`
For some of the stuff I had to use the .whl files for torch which you can get from here: https://download.pytorch.org/whl/cu100/torch_stable.html",pip version matter upgrade pip run pip install upgrade pip stuff use torch get,issue,negative,neutral,neutral,neutral,neutral,neutral
616082800,"> Here are the magic numbers:
> CUDA 10.0, CudNN 7.6.5, Python 3.7.7 (Other python versions may work), Torch 1.2.0, tensorflow 1.14.0.

@meme-lord  what should the pip version be for these magic versions cause when i try to install torch they say install the correct pip version",magic python python may work torch pip version magic cause try install torch say install correct pip version,issue,negative,positive,positive,positive,positive,positive
614924067,"It's not a microphone problem. Try to use something else than sounddevice to play the output audio and it will work. Instead of sd.stop() and sd.play(generated_wav, synthesizer.sample_rate) in demo_cli.py use scipy.io.wavfile",microphone problem try use something else play output audio work instead use,issue,negative,neutral,neutral,neutral,neutral,neutral
614748808,"> 
> 
> It works fine on windows.
> Package management and environment setup with python can be very tricky if you are not experienced with it.
> 
> The generalized workflow for setting up machine learning models with python is thus.
> 
>     1. Git clone the repository
> 
>     2. set up a [virtual environment](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/)
> 
>     3. in your virtual environment, pip install -r requirements.txt
> 
>     4. handle dependencies that were not listed in the requirements.txt there are many reasons for this, I won't go over them now.
> 
>     5. download pretrained models/ datasets
> 
>     6. run the eval function for the model.
> 
> 
> I recommend using an IDE like pycharm to consolidate management of virtual environments, version control, ect.

thanks a lot, now i know what learn exactly :D ",work fine package management environment setup python tricky experienced generalized setting machine learning python thus git clone repository set virtual environment virtual environment pip install handle listed many wo go run function model recommend ide like consolidate management virtual version control thanks lot know learn exactly,issue,positive,positive,positive,positive,positive,positive
614713408,"It works fine on windows.
Package management and environment setup with python can be very tricky if you are not experienced with it.

The generalized workflow for setting up machine learning models with python is thus.

1. Git clone the repository
2. set up a [virtual environment](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/)
3. in your virtual environment, pip install -r requirements.txt
4. handle dependencies that were not listed in the requirements.txt there are many reasons for this, I won't go over them now.
5. download pretrained models/ datasets
6. run the eval function for the model.

I recommend using an IDE like pycharm to consolidate management of virtual environments, version control, ect.",work fine package management environment setup python tricky experienced generalized setting machine learning python thus git clone repository set virtual environment virtual environment pip install handle listed many wo go run function model recommend ide like consolidate management virtual version control,issue,positive,positive,positive,positive,positive,positive
614219855,"I suppose it's just for linux, thanks anyway, it was very enlightening",suppose thanks anyway enlightening,issue,positive,positive,positive,positive,positive,positive
614185344,"Plenty of youtube videos available.
[This was the first that came up for me](https://youtu.be/12rdn9jazwE)",plenty available first came,issue,negative,positive,positive,positive,positive,positive
614012004,"I run  demo_toolbox.py in anaconda, nothing happened too. As follows:

(base) C:\Users\Administrator\Real-Time-Voice-Cloning-master\Real-Time-Voice-Cloning-master>python demo_toolbox.py

(base) C:\Users\Administrator\Real-Time-Voice-Cloning-master\Real-Time-Voice-Cloning-master>",run anaconda nothing base python base,issue,negative,negative,negative,negative,negative,negative
613755769,"once you get such issues, run command

pip install --upgrade tensorflow
pip install --upgrade tensorflow-estimator
pip install --upgrade tensorboard

then 
pip install -r requirements.txt

repeat if needed. That's how I was able to proceed",get run command pip install upgrade pip install upgrade pip install upgrade pip install repeat able proceed,issue,negative,positive,positive,positive,positive,positive
613532680,"> Why not just use the better model found on https://www.resemble.ai/?

Because they don't allow you to use someone other's voice. (Actually there is an option, but you need to pay for that)",use better model found allow use someone voice actually option need pay,issue,positive,positive,positive,positive,positive,positive
613503979,"> What is your question? Did you create this clip using a model you trained yourself or using the pretrained models?

thanks for the reply. I uploaded the recording as an attachment. I used a pretrained models.",question create clip model trained thanks reply recording attachment used,issue,positive,positive,positive,positive,positive,positive
613486738,What is your question? Did you create this clip using a model you trained yourself or using the pretrained models?,question create clip model trained,issue,negative,neutral,neutral,neutral,neutral,neutral
613486021,Your traceback states that it's trying to use cudnn 7.2.1. You most likely have two versions installed or cudnn 7.4.1 did not install correctly.,trying use likely two install correctly,issue,negative,neutral,neutral,neutral,neutral,neutral
613484542,Why not just use the better model found on [https://www.resemble.ai/](https://www.resemble.ai/)?,use better model found,issue,negative,positive,positive,positive,positive,positive
613483338,"You just need to install unidecode:
`pip install unidecode`
or if using anaconda
`conda install unidecode`",need install pip install anaconda install,issue,negative,neutral,neutral,neutral,neutral,neutral
613465434,"CUDA compatibility is defined by the PyTorch/TensorFlow developers, not the developers of this repo. Presently, the most recent version of CUDA supported by PyTorch and TensorFlow is CUDA 10.1.",compatibility defined presently recent version,issue,negative,neutral,neutral,neutral,neutral,neutral
613463867,"Unfortunately, your graphics card is too old for using most deep learning libraries. Based on your error message, you'll need a card that has a cuda capability of at least 3.5. You can look at [this chart](https://developer.nvidia.com/cuda-gpus) to help you find a graphics card that has such a capability. However, you will also need a card with enough memory to host the models and data. Off the top of my head, I'm not sure what the minimum memory requirements are for this model, but I'd venture to say that 12GB is a decent benchmark (Nvidia GTX 1080Ti should probably work).  

If you can't get a card with enough memory, you would need to reduce the memory requirements of the model by converting it to half precision.",unfortunately graphic card old deep learning based error message need card capability least look chart help find graphic card capability however also need card enough memory host data top head sure minimum memory model venture say decent ti probably work ca get card enough memory would need reduce memory model converting half precision,issue,negative,positive,neutral,neutral,positive,positive
612909769,"I will help you to install it

contact: Gmail: vishwanathdalawai321@gmail.com
LinkedIn: https://www.linkedin.com/in/vishwanath-dalawai/

Thanks,
Vishwanath D

On Fri, Apr 10, 2020 at 1:37 PM funnybone72 <notifications@github.com>
wrote:

> Hey, thanks for the note. I'm only looking to use this software once, to
> clone a voice and generate a 3 second sentence from it. That's it.
>
> I have a lot of audio files available for the input voice to be cloned.
>
> I have the tool installed on my machine but it's very glitchy and I don't
> know how to use it and some commands are greyed out.
>
> One other option is that I provide audio input files and someone generates
> the resulting sentence taht I'm looking for and send it over.
>
> I'll pay whoever can help me with this! Thanks.
>
> Any luck getting your request fulfilled?
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/308#issuecomment-611928643>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AGUF7DCFZW7GY75HZZCM4K3RL3HTVANCNFSM4L4MP7SQ>
> .
>
",help install contact thanks wrote hey thanks note looking use clone voice generate second sentence lot audio available input voice tool machine know use one option provide audio input someone resulting sentence looking send pay whoever help thanks luck getting request thread reply directly view,issue,positive,positive,positive,positive,positive,positive
612885679,"> @boltomli Take a look at this dataset (1505 hours, 6408 speakers, recorded on smartphones):
> https://www.datatang.com/webfront/opensource.html
> [Samples.zip](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/3413304/Samples.zip)
> Not sure if the quality is good enough for encoder training.

你好，您给的链接好像失效了",take look sure quality good enough training,issue,positive,positive,positive,positive,positive,positive
612833048,You could train any accents you want if you have enough data. Maybe you should try this one [https://www.resemble.ai/](https://www.resemble.ai/),could train want enough data maybe try one,issue,negative,neutral,neutral,neutral,neutral,neutral
612749714,@Tomotori Thanks for that fix. Do you know why that change solves those errors?,thanks fix know change,issue,negative,positive,positive,positive,positive,positive
612633238,"I am getting an error as given below when we are giving the command. 

C:\Users\anand\Real-Time-Voice-Cloning>python demo_toolbox.py -d ~/Desktop/LibriSpeech/train-clean-100/
Traceback (most recent call last):
  File ""demo_toolbox.py"", line 2, in <module>
    from toolbox import Toolbox
  File ""C:\Users\anand\Real-Time-Voice-Cloning\toolbox\__init__.py"", line 1, in <module>
    from toolbox.ui import UI
  File ""C:\Users\anand\Real-Time-Voice-Cloning\toolbox\ui.py"", line 1, in <module>
    from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
  File ""C:\Users\anand\AppData\Local\Programs\Python\Python38-32\lib\site-packages\matplotlib\backends\backend_qt5agg.py"", line 11, in <module>
    from .backend_qt5 import (
  File ""C:\Users\anand\AppData\Local\Programs\Python\Python38-32\lib\site-packages\matplotlib\backends\backend_qt5.py"", line 15, in <module>
    import matplotlib.backends.qt_editor.figureoptions as figureoptions
  File ""C:\Users\anand\AppData\Local\Programs\Python\Python38-32\lib\site-packages\matplotlib\backends\qt_editor\figureoptions.py"", line 12, in <module>
    from matplotlib.backends.qt_compat import QtGui
  File ""C:\Users\anand\AppData\Local\Programs\Python\Python38-32\lib\site-packages\matplotlib\backends\qt_compat.py"", line 168, in <module>
    raise ImportError(""Failed to import any qt binding"")
ImportError: Failed to import any qt binding

Please help

Regards
Gopal",getting error given giving command python recent call last file line module toolbox import toolbox file line module import file line module import file line module import file line module import file line module import file line module raise import binding import binding please help,issue,positive,neutral,neutral,neutral,neutral,neutral
612417957,"> ```
> I am trying to train the synthesize model on Chinese dataset, the value of loss seems reach the minmum after ten thousands steps training (the minmum values means same as 700000 training steps on Librispeech dataset ).
> ```
> 
> The quality of dataset are very good: almost no background noise. But the audio very short.:average audio duriaton is 4 seconds.
> I want to know whether i can stop the training on this condition?

你好，请问你训练之后测试的效果如何？",trying train synthesize model value loss reach ten training training quality good almost background noise audio short average audio want know whether stop training condition,issue,negative,positive,positive,positive,positive,positive
611928643,"> Hey, thanks for the note. I'm only looking to use this software once, to clone a voice and generate a 3 second sentence from it. That's it.
> 
> I have a lot of audio files available for the input voice to be cloned.
> 
> I have the tool installed on my machine but it's very glitchy and I don't know how to use it and some commands are greyed out.
> 
> One other option is that I provide audio input files and someone generates the resulting sentence taht I'm looking for and send it over.
> 
> I'll pay whoever can help me with this! Thanks.

Any luck getting your request fulfilled?",hey thanks note looking use clone voice generate second sentence lot audio available input voice tool machine know use one option provide audio input someone resulting sentence looking send pay whoever help thanks luck getting request,issue,positive,positive,positive,positive,positive,positive
611874362,"> @minmugil @mrgloom I'm having the same problem, did y'all ever figure this out?

no i gave up",problem ever figure gave,issue,negative,neutral,neutral,neutral,neutral,neutral
611630503,"I've been working on trying to install this for a day and a half.  I've explicitly followed the instructions but can't get it to work!  When I try to install pip install -r requirements.txt I get a lot of errors but the ones in red are these:  

ERROR: tensorflow 2.1.0 has requirement tensorboard<2.2.0,>=2.1.0, but you'll have tensorboard 1.14.0 which is incompatible.
ERROR: tensorflow 2.1.0 has requirement tensorflow-estimator<2.2.0,>=2.1.0rc0, but you'll have tensorflow-estimator 1.14.0 which is incompatible.

Does anyone know of an instruction sheet that details every step needed to get this to work?
",working trying install day half explicitly ca get work try install pip install get lot red error requirement incompatible error requirement incompatible anyone know instruction sheet every step get work,issue,negative,negative,neutral,neutral,negative,negative
611184819,Tensorflow isn't supported on Python 3.8 so you'll have to downgrade to 3.7 (using Anaconda or just uninstall and reinstall 3.7),python downgrade anaconda reinstall,issue,negative,neutral,neutral,neutral,neutral,neutral
610643902,"I was having the same problem and upgrading PyTorch from 1.1.0 to 1.2.0 worked for me.
The working combination I ended up with was: CUDA 10.0, CudNN 7.6.5, Python 3.7.7 (Other python versions may work), Torch 1.2.0, Tensorflow 1.14.0 .
Information like this should probably be in the README or maybe detailed info in INSTALL.md ?
Took days to work this out and lots of large downloads of different versions.",problem worked working combination ended python python may work torch information like probably maybe detailed took day work lot large different,issue,negative,positive,positive,positive,positive,positive
610640721,"Here are the magic numbers:
CUDA 10.0, CudNN 7.6.5, Python 3.7.7 (Other python versions may work), Torch 1.2.0, tensorflow 1.14.0.",magic python python may work torch,issue,negative,positive,positive,positive,positive,positive
610519787,"Long story short, you'll have to train up your own synthesizer model using a dataset of fully transcribed audio (with punctuation) from many speakers, with many hours of clips of many different lengths. You'll need to train this on a machine with well-endowed GPU specs.

The work-around is to find the sweet spot for sentence lengths the model is comfortable with generating and use line breaks. too short or too long, you'll get the long pauses and strange windy noises.",long story short train synthesizer model fully audio punctuation many many clip many different need train machine spec find sweet spot sentence model comfortable generating use line short long get long strange windy,issue,positive,positive,positive,positive,positive,positive
610445888,"> Zakajd I have a similar request. How can I contact you I don’t use Twitter?

You can find me in Telegram or FB using GitHub name (@zakajd) ",similar request contact use twitter find telegram name,issue,negative,neutral,neutral,neutral,neutral,neutral
610334407,"> > One other option is that I provide audio input files and someone generates the resulting sentence taht I'm looking for and send it over.
> 
> I've been playing with this repo for a while. Can help if you still interested. Text me on twitter @dja_zak



> > One other option is that I provide audio input files and someone generates the resulting sentence taht I'm looking for and send it over.
> 
> I've been playing with this repo for a while. Can help if you still interested. Text me on twitter @dja_zak

Zakajd I have a similar request. How can I contact you I don’t use Twitter?",one option provide audio input someone resulting sentence looking send help still interested text twitter one option provide audio input someone resulting sentence looking send help still interested text twitter similar request contact use twitter,issue,positive,positive,positive,positive,positive,positive
610062398,Could Mozilla's Common Voice be used for this? https://voice.mozilla.org,could common voice used,issue,negative,negative,negative,negative,negative,negative
610000803,"Copied entire path of Python install and pasted that into the ""Path"" variable.",copied entire path python install pasted path variable,issue,negative,neutral,neutral,neutral,neutral,neutral
609577477,"I met the same question, could you show your detail or solution?",met question could show detail solution,issue,negative,neutral,neutral,neutral,neutral,neutral
609478515,"> One other option is that I provide audio input files and someone generates the resulting sentence taht I'm looking for and send it over.

I've been playing with this repo for a while. Can help if you still interested. Text me on twitter @dja_zak
",one option provide audio input someone resulting sentence looking send help still interested text twitter,issue,positive,positive,positive,positive,positive,positive
609418407,Easiest would be to use TF 1.14.0 and Pytorch 1.2.0 which both work with cudnn=10.0. I would recommend you use a conda environment.,easiest would use work would recommend use environment,issue,positive,neutral,neutral,neutral,neutral,neutral
609418090,Please use TF 1.14.0 as stated by the author.,please use stated author,issue,negative,neutral,neutral,neutral,neutral,neutral
608632433,"Hey, thanks for the note. I'm only looking to use this software once, to clone a voice and generate a 3 second sentence from it. That's it. 

I have a lot of audio files available for the input voice to be cloned. 

I have the tool installed on my machine but it's very glitchy and I don't know how to use it and some commands are greyed out.

One other option is that I provide audio input files and someone generates the resulting sentence taht I'm looking for and send it over. 

I'll pay whoever can help me with this! Thanks.

",hey thanks note looking use clone voice generate second sentence lot audio available input voice tool machine know use one option provide audio input someone resulting sentence looking send pay whoever help thanks,issue,positive,positive,positive,positive,positive,positive
608628088," Hi Joe,

I have this running and have for a few months. I even have an API that allows you to specify an input wav and auto generate audio by it.  It has some glitches (only sounds good on sentences 3-6 seconds long). (I've deprioritized polishing this for various reasons)

What are you looking to do with this?  I don't have much interest in installing this on someone's machine - but this is part of a developer friendly AI platform we're developing so maybe there's something i can do that helps?


    On Friday, April 3, 2020, 12:32:43 PM PDT, JoeRac <notifications@github.com> wrote:  
 
 


I'm looking to pay someone to install this release on my machine and show me the way this works at a basic level, and run a text-to-speech test. Let me know if interested

—
You are receiving this because you are subscribed to this thread.
Reply to this email directly, view it on GitHub, or unsubscribe.
  ",hi joe running even specify input auto generate audio good long various looking much interest someone machine part developer friendly ai platform maybe something wrote looking pay someone install release machine show way work basic level run test let know interested thread reply directly view,issue,positive,positive,positive,positive,positive,positive
608253990,"I met the same question when I run demo_cli.py, have you solve it yet ",met question run solve yet,issue,negative,neutral,neutral,neutral,neutral,neutral
607560099,"> I tried using fatchord's WaveRNN model for the vocoder but I get a size mismatch error. do i need to modify something first before using a different pretrained model?
> 
> ```
> RuntimeError: Error(s) in loading state_dict for WaveRNN:
> 	size mismatch for upsample.up_layers.5.weight: copying a param with shape torch.Size([1, 1, 1, 23]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 17]).
> 	size mismatch for fc3.weight: copying a param with shape torch.Size([30, 512]) from checkpoint, the shape in current model is torch.Size([512, 512]).
> 	size mismatch for fc3.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([512]).
> ```
From the author's paper: 
""The vocoder model we use is an open source PyTorch implementation 15 that is
based on WaveRNN but presents quite a few different design choices made by github
user fatchord. We’ll refer to this architecture as the “alternative WaveRNN”.""

The current vocoder being used is already WaveRNN",tried model get size mismatch error need modify something first different model error loading size mismatch weight param shape shape current model size mismatch param shape shape current model size mismatch param shape shape current model author paper model use open source implementation based quite different design made user refer architecture alternative current used already,issue,negative,positive,neutral,neutral,positive,positive
607110408,"Try replacing `!pip install -r -q requirements.txt `
with
```
%tensorflow_version 1.x
!pip install umap-learn
!pip install visdom
!pip install webrtcvad
!pip install librosa>=0.5.1
!pip install matplotlib>=2.0.2
!pip install numpy>=1.14.0
!pip install scipy>=1.0.0
!pip install tqdm
!pip install sounddevice
!pip install Unidecode
!pip install inflect
!pip install PyQt5
!pip install multiprocess
!pip install numba
```",try pip install pip install pip install pip install pip install pip install pip install pip install pip install pip install pip install pip install inflect pip install pip install pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
606572744,@nicok1172 Which python version are you using? I used 3.7 and it worked fine,python version used worked fine,issue,negative,positive,positive,positive,positive,positive
606417400,"A:\Program files (x86)\Real Time Voice Cloning\Real-Time-Voice-Cloning-master>pip install -r requirements.txt
ERROR: Could not find a version that satisfies the requirement tensorflow-gpu==1.14.0 (from -r requirements.txt (line 1)) (from versions: 2.2.0rc1, 2.2.0rc2)
ERROR: No matching distribution found for tensorflow-gpu==1.14.0 (from -r requirements.txt (line 1))

Can't install the required version of tensorflow-gpu",time voice pip install error could find version requirement line error matching distribution found line ca install version,issue,negative,neutral,neutral,neutral,neutral,neutral
605701014,"The same for me. I have tried everything, but sill unable to make it work. ",tried everything sill unable make work,issue,negative,negative,negative,negative,negative,negative
604999848,"> @shawwn I've uploaded the models to my dropbox. The vocoder is still training and will be for another 24-48 hours. Please share whatever you end up making with them!
> 
> #### Encoder
> https://www.dropbox.com/s/xl2wr13nza10850/encoder.zip?dl=0
> 
> #### Synthesizer (Tacotron)
> https://www.dropbox.com/s/t7qk0aecpps7842/tacotron.zip?dl=0
> 
> #### Vocoder
> https://www.dropbox.com/s/bgzeaid0nuh7val/vocoder.zip?dl=0

Dear All,
i've downloaded the models from @sberryman and adapted the hyper parameters accordingly.
I created a few examples with them. I observe the following:
1) the sound quality is pretty good (clearly understandable, no bleeps or blops etc.)
2) the voice does not resemble the reference embedding. it's like a 'generic' voice.

I wonder why that is. Did anybody else experience this?
Thanks!",still training another please share whatever end making synthesizer dear hyper accordingly observe following sound quality pretty good clearly understandable voice resemble reference like voice wonder anybody else experience thanks,issue,positive,positive,positive,positive,positive,positive
603611413,"> I tried using fatchord's WaveRNN model for the vocoder but I get a size mismatch error. do i need to modify something first before using a different pretrained model?
> 
> ```
> RuntimeError: Error(s) in loading state_dict for WaveRNN:
> 	size mismatch for upsample.up_layers.5.weight: copying a param with shape torch.Size([1, 1, 1, 23]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 17]).
> 	size mismatch for fc3.weight: copying a param with shape torch.Size([30, 512]) from checkpoint, the shape in current model is torch.Size([512, 512]).
> 	size mismatch for fc3.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([512]).
> ```

I think it's because of the different value of params ""hop_size""， maybe  you can check it.",tried model get size mismatch error need modify something first different model error loading size mismatch weight param shape shape current model size mismatch param shape shape current model size mismatch param shape shape current model think different value maybe check,issue,negative,positive,neutral,neutral,positive,positive
603165108,"Do you have more log output?

This seems to be a microphone issue:
```
Expression 'paInvalidSampleRate' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 2048
Expression 'PaAlsaStreamComponent_InitialConfigure( &self->capture, inParams, self->primeBuffers, hwParamsCapture, &realSr )' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 2719
Expression 'PaAlsaStream_Configure( stream, inputParameters, outputParameters, sampleRate, framesPerBuffer, &inputLatency, &outputLatency, &hostBufferSizeMode )' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 2843
Error opening InputStream: Invalid sample rate [PaErrorCode -9997]
```

while this seems to be an output issue:
```
Expression 'paInvalidSampleRate' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 2048
Expression 'PaAlsaStreamComponent_InitialConfigure( &self->playback, outParams, self->primeBuffers, hwParamsPlayback, &realSr )' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 2722
Expression 'PaAlsaStream_Configure( stream, inputParameters, outputParameters, sampleRate, framesPerBuffer, &inputLatency, &outputLatency, &hostBufferSizeMode )' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 2843
Traceback (most recent call last):
  File ""/home/robo/code/audio/Real-Time-Voice-Cloning/toolbox/__init__.py"", line 81, in <lambda>
    func = lambda: self.ui.play(self.ui.selected_utterance.wav, Synthesizer.sample_rate)
  File ""/home/robo/code/audio/Real-Time-Voice-Cloning/toolbox/ui.py"", line 142, in play
    sd.play(wav, sample_rate)
  File ""/home/robo/code/audio/venv/lib/python3.7/site-packages/sounddevice.py"", line 182, in play
    **kwargs)
  File ""/home/robo/code/audio/venv/lib/python3.7/site-packages/sounddevice.py"", line 2498, in start_stream
    **kwargs)
  File ""/home/robo/code/audio/venv/lib/python3.7/site-packages/sounddevice.py"", line 1455, in __init__
    **_remove_self(locals()))
  File ""/home/robo/code/audio/venv/lib/python3.7/site-packages/sounddevice.py"", line 861, in __init__
    'Error opening {0}'.format(self.__class__.__name__))
  File ""/home/robo/code/audio/venv/lib/python3.7/site-packages/sounddevice.py"", line 2653, in _check
    raise PortAudioError(errormsg, err)
sounddevice.PortAudioError: Error opening OutputStream: Invalid sample rate [PaErrorCode -9997]
```

(and I have both of them... yay...)

Seems to be a problem with devices on PulseAudio not using a default sample rate that sounddevice expects. I'll try either changing sample rates / resolution in pulseaudio or diving into sounddevice's code to see if it's possible to correct the samplerate detection. (41 vs 48 khz?)

Ubuntu/Pop_OS, pulseaudio 12.2-2, Default Sample Specification: `s24le 2ch 48000Hz`  
Microphone: s16le 1ch 48000Hz  
Output: float32le 2ch 48000Hz / s32le 2ch 48000Hz",log output microphone issue expression line expression capture line expression stream line error opening invalid sample rate output issue expression line expression playback line expression stream line recent call last file line lambda lambda file line play file line play file line file line file line opening file line raise err error opening invalid sample rate problem default sample rate try either sample resolution diving code see possible correct detection default sample specification microphone output,issue,negative,neutral,neutral,neutral,neutral,neutral
603122845,"Hi, I also encountered this problem, my machine memory is 256G, but also MemoryError, did you solve this problem?",hi also problem machine memory also solve problem,issue,negative,neutral,neutral,neutral,neutral,neutral
602754481,"My mistake! The GPL would only apply to the distribution. If you compile the project, the resulting package would contain code from this repository and PyQt. So if you wanted to distribute that, you’d need to honor the terms of both PyQt’s GPL and your MIT license.

No changes are needed. Sorry for the confusion.",mistake would apply distribution compile project resulting package would contain code repository distribute need honor license sorry confusion,issue,negative,negative,negative,negative,negative,negative
602741654,"I tried using fatchord's WaveRNN model for the vocoder but I get a size mismatch error. do i need to modify something first before using a different pretrained model?

```
RuntimeError: Error(s) in loading state_dict for WaveRNN:
	size mismatch for upsample.up_layers.5.weight: copying a param with shape torch.Size([1, 1, 1, 23]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 17]).
	size mismatch for fc3.weight: copying a param with shape torch.Size([30, 512]) from checkpoint, the shape in current model is torch.Size([512, 512]).
	size mismatch for fc3.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([512]).
```",tried model get size mismatch error need modify something first different model error loading size mismatch weight param shape shape current model size mismatch param shape shape current model size mismatch param shape shape current model,issue,negative,positive,neutral,neutral,positive,positive
602650490,"Gosh... It’s not really my place to enforce this. I’m just reporting it.

In my opinion though, removing PyQt now would be taking a shortcut. You’ve already reaped lots of benefit and exposure from this project. And some of that should be shared with the creators of PyQt. I’d suggest reaching out to them: info@riverbankcomputing.com",gosh really place enforce opinion though removing would taking already lot benefit exposure project suggest reaching,issue,positive,positive,positive,positive,positive,positive
600933717,"> New lines add a short break to the audio flow, so you can break the line where you need pauses, the more line breaks, the bigger is the pause.

But if I use short line then all the audio will be broken - strange noises and pauses includes in it.",new add short break audio flow break line need line bigger pause use short line audio broken strange,issue,negative,negative,neutral,neutral,negative,negative
599161626,"I got this working on mac. 
Use this fork instead https://github.com/shawwn/Real-Time-Voice-Cloning
In requirements.txt change the first line to `tensorflow>=1.1.0,<=1.14.0`
Run
`pip install -r requirements.txt`
`pip install torch torchvision`
You'll need to download and install the pretrained models before running `python3 ./demo_cli.py` or it'll complain that it can't find them but you'll be good to go.",got working mac use fork instead change first line run pip install pip install torch need install running python complain ca find good go,issue,negative,positive,positive,positive,positive,positive
599108585,"New lines add a short break to the audio flow, so you can break the line where you need pauses, the more line breaks, the bigger is the pause.",new add short break audio flow break line need line bigger pause,issue,negative,positive,neutral,neutral,positive,positive
599107525,"You can click on `Browse`, next to `Utterrance` on the GUI, and then select your audio file.",click browse next select audio file,issue,negative,neutral,neutral,neutral,neutral,neutral
599106041,"Torch installation is a little more tricky (some GPU drivers, for example), I think this is the main reason for following the official docs and not to include this one in the requirements.txt",torch installation little tricky example think main reason following official include one,issue,negative,negative,neutral,neutral,negative,negative
599067229,"@CorentinJ  Hello! I 've read your code and paper for some days.A problem confuses me is that I don't understand the mean of ""create RandomCycler "",what does the class do in dataloader? I appretiate it if you can explain it for me,Sorry I 'm new studier.",hello read code paper problem understand mean create class explain sorry new studier,issue,negative,negative,negative,negative,negative,negative
598060929,"I'm also confused with the parameter mu_law=True in vocoder/hparams.py, since the trained label files(wav) which are generated in step2(synthesizer) are not encoded using mu_law. So I think this parameter may need to set False.  Do you mean this?",also confused parameter since trained label step synthesizer think parameter may need set false mean,issue,negative,negative,negative,negative,negative,negative
595670644,I noticed that solo-speaker trained WaveRNN vocoder outputs voice of much higher quality than one trained on the whole multi-speaker dataset. You must have speaker ID labels to train solo-speaker vocoder.,trained voice much higher quality one trained whole must speaker id train,issue,negative,positive,positive,positive,positive,positive
594873967,ok. everything working. thanks for nice soft:),everything working thanks nice soft,issue,positive,positive,positive,positive,positive,positive
591916258,"my os is window10 but totally same symptom with him

Traceback (most recent call last):
  File ""demo_toolbox.py"", line 32, in <module>
    Toolbox(**vars(args))
  File ""C:\Users\alsan\Real-Time-Voice-Cloning\toolbox\__init__.py"", line 48, in __init__
    self.ui = UI()
  File ""C:\Users\alsan\Real-Time-Voice-Cloning\toolbox\ui.py"", line 375, in __init__
    self.projections_layout.addWidget(FigureCanvas(fig))
TypeError: addWidget(self, QWidget, stretch: int = 0, alignment: Union[Qt.Alignment, Qt.AlignmentFlag] = Qt.Alignment()): argument 1 has unexpected type 'FigureCanvasQTAgg'
",o window totally symptom recent call last file line module toolbox file line file line fig self stretch alignment union argument unexpected type,issue,negative,positive,neutral,neutral,positive,positive
589299766,"Solve it by running: `CUDA_VISIBLE_DEVICES=0 python3 demo_cli.py`
(hint here: https://discuss.pytorch.org/t/how-to-specify-gpu-usage/945/2)",solve running python hint,issue,negative,neutral,neutral,neutral,neutral,neutral
588974593,"Not sure if that is the problem. I did change the PyTorch file at all and got it working. 
For the system environment paths check the guide I was referring to who explains how to do it: https://www.joe0.com/2019/10/19/how-resolve-tensorflow-2-0-error-could-not-load-dynamic-library-cudart64_100-dll-dlerror-cudart64_100-dll-not-found/",sure problem change file got working system environment check guide,issue,negative,positive,positive,positive,positive,positive
588653428,"> 
> 
> I think it is related to this step:
> 
> NVIDIA cuDNN for CUDA 10.0
> To set this up, you’ll need to extract the ZIP (e.g., cudnn-10.0-windows10-x64-v7.6.5.32.zip) which has three directories in its cuda\ directory (bin, include, and lib). Copy those three directories directly into your CUDA 10.0 install folder. If you used the default path on your C:\ drive, copy them into C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0
> 
> Then update system environment paths.
> 
> I followed this guide for installing tensorflow correctly: https://www.joe0.com/2019/10/19/how-resolve-tensorflow-2-0-error-could-not-load-dynamic-library-cudart64_100-dll-dlerror-cudart64_100-dll-not-found/
> 
> I started with installing CUDA 10.2 but it didn't work so installed CUDA 10 which worked.

sorry im not a programmer what does update system environment paths mean? I followed the steps, I installed CUDA 10 from the very beginning and I copied the files to from the cudnn-10.0-windows10-x64-v7.6.5.32.zip to C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0. 


in the website (instructions that I followed) I was told to rename the cudnn64_7.dll or delete it after installing PyTorch which I forgot and end up renaming it after installing CUDA 10 could that be a problem ?",think related step set need extract zip zip three directory bin include copy three directly install folder used default path drive copy update system environment guide correctly work worked sorry programmer update system environment mean beginning copied zip told rename delete forgot end could problem,issue,negative,negative,negative,negative,negative,negative
588564245,"I think it is related to this step:

NVIDIA cuDNN for CUDA 10.0
To set this up, you’ll need to extract the ZIP (e.g., cudnn-10.0-windows10-x64-v7.6.5.32.zip) which has three directories in its cuda\ directory (bin, include, and lib). Copy those three directories directly into your CUDA 10.0 install folder. If you used the default path on your C:\ drive, copy them into C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0

Then update system environment paths.

I followed this guide for installing tensorflow correctly: https://www.joe0.com/2019/10/19/how-resolve-tensorflow-2-0-error-could-not-load-dynamic-library-cudart64_100-dll-dlerror-cudart64_100-dll-not-found/

I started with installing CUDA 10.2 but it didn't work so installed CUDA 10 which worked.",think related step set need extract zip zip three directory bin include copy three directly install folder used default path drive copy update system environment guide correctly work worked,issue,negative,positive,neutral,neutral,positive,positive
588077222,"I was thinking to create the dataset by my own. 
Is there any tool like subtitle makers for writing or copying subtitle with the exact timeframe with the audio?",thinking create tool like subtitle writing subtitle exact audio,issue,positive,positive,positive,positive,positive,positive
587517855,"issue solved. If you have faced same issue please go to the link https://github.com/CorentinJ/librispeech-alignments 
and download the alignment dataset then merge the datset with your original data set.",issue faced issue please go link alignment merge original data set,issue,positive,positive,positive,positive,positive,positive
586819443,"> Now demo_cli.py gives the following message:
> 
> > Your PyTorch installation is not configured to use CUDA. If you have a GPU ready for deep learning, ensure that the drivers are properly installed, and that your CUDA version matches your PyTorch installation. CPU-only inference is currently not supported.
> 
> My GPU supports CUDA but according to an internet search it seems the drivers should be enough to run pytorch. How do I make the CUDA and pytorch versions ""match""?

please refer that to the relevant issue.",following message installation use ready deep learning ensure properly version installation inference currently according search enough run make match please refer relevant issue,issue,positive,positive,neutral,neutral,positive,positive
586720949,"I got same error, 
my solution:
Use Anacoda to create a python3.7 env. then its work
",got error solution use create python work,issue,negative,neutral,neutral,neutral,neutral,neutral
586621577,"> I got rid of all CUDA references in code and was able to run it on CPU
> Try [my repo](https://github.com/ByPort/Real-Time-Voice-Cloning) or take a look at [this commit](https://github.com/ByPort/Real-Time-Voice-Cloning/commit/31bbd88b0b1823a562f1bd419a2881b672bb639a)
> @genewitch @deepseek

Tried Running This Code, Got An Issue Now , nvcuda.dll not found.

Traceback (most recent call last):
  File ""RTVC/demo_cli.py"", line 3, in <module>
    from synthesizer.inference import Synthesizer
  File ""C:\Python\36\RTVC\synthesizer\inference.py"", line 1, in <module>
    from synthesizer.tacotron2 import Tacotron2
  File ""C:\Python\36\RTVC\synthesizer\tacotron2.py"", line 3, in <module>
    from synthesizer.models import create_model
  File ""C:\Python\36\RTVC\synthesizer\models\__init__.py"", line 1, in <module>
    from .tacotron import Tacotron
  File ""C:\Python\36\RTVC\synthesizer\models\tacotron.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Python\36\lib\site-packages\tensorflow\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Python\36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Python\36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 30, in <module>
    self_check.preload_check()
  File ""C:\Python\36\lib\site-packages\tensorflow\python\platform\self_check.py"", line 70, in preload_check
    % build_info.nvcuda_dll_name)

ImportError: ""Could not find 'nvcuda.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Typically it is installed in 'C:\Windows\System32'. If it is not present, ensure that you have a CUDA-capable GPU with the correct driver installed.""",got rid code able run try take look commit tried running code got issue found recent call last file line module import synthesizer file line module import file line module import file line module import file line module import file line module import file line module import file line module file line could find directory path environment variable typically present ensure correct driver,issue,positive,positive,positive,positive,positive,positive
586606108,"Now demo_cli.py gives the following message:
> Your PyTorch installation is not configured to use CUDA. If you have a GPU ready for deep learning, ensure that the drivers are properly installed, and that your CUDA version matches your PyTorch installation. CPU-only inference is currently not supported.

My GPU supports CUDA but according to an internet search it seems the drivers should be enough to run pytorch. How do I make the CUDA and pytorch versions ""match""?",following message installation use ready deep learning ensure properly version installation inference currently according search enough run make match,issue,positive,positive,neutral,neutral,positive,positive
586583152,"@tapankarnik I tried 1.14 but I think tensorflow-gpu==1.14 is no longer available. Lowest version available is 1.15 which does not comply with requirements. 
![image](https://user-images.githubusercontent.com/57598108/74587452-437c2980-5014-11ea-9439-7894ba8c223d.png)
",tried think longer available version available comply image,issue,negative,positive,positive,positive,positive,positive
586572886,"> PortAudio library not found

```
sudo apt-get update
sudo apt-get install libportaudio2
sudo apt-get install libasound-dev
sudo apt-get install python-pyaudio
sudo apt-get install python3-pyaudio
```
Try these, I don't remember which one but one of these worked for me. 
@berzi ",library found update install install install install try remember one one worked,issue,negative,neutral,neutral,neutral,neutral,neutral
586570887,"Thank you, yes, that worked.
I'm trying to run demo_cly.py but I keep getting exceptions, and I've installed all packages I could as they kept being reported as not found, but I'm stuck here:

> Traceback (most recent call last):
>   File ""demo_cli.py"", line 36, in <module>
>     import sounddevice as sd
>   File ""/home/brzrkr/.local/lib/python3.7/site-packages/sounddevice.py"", line 71, in <module>
>     raise OSError('PortAudio library not found')
> OSError: PortAudio library not found
> Error in sys.excepthook:
> Traceback (most recent call last):
>   File ""/usr/lib/python3/dist-packages/apport_python_hook.py"", line 63, in apport_excepthook
>     from apport.fileutils import likely_packaged, get_recent_crashes
>   File ""/usr/lib/python3/dist-packages/apport/__init__.py"", line 5, in <module>
>     from apport.report import Report
>   File ""/usr/lib/python3/dist-packages/apport/report.py"", line 30, in <module>
>     import apport.fileutils
>   File ""/usr/lib/python3/dist-packages/apport/fileutils.py"", line 23, in <module>
>     from apport.packaging_impl import impl as packaging
>   File ""/usr/lib/python3/dist-packages/apport/packaging_impl.py"", line 24, in <module>
>     import apt
>   File ""/usr/lib/python3/dist-packages/apt/__init__.py"", line 23, in <module>
>     import apt_pkg
> **ModuleNotFoundError: No module named 'apt_pkg'**
> 
> Original exception was:
> Traceback (most recent call last):
>   File ""demo_cli.py"", line 36, in <module>
>     import sounddevice as sd
>   File ""/home/brzrkr/.local/lib/python3.7/site-packages/sounddevice.py"", line 71, in <module>
>     raise OSError('PortAudio library not found')
> **OSError: PortAudio library not found**

In particular, I tried to find apt_pkg but couldn't find it with apt nor with pip. Are you able to help? @Sadam1195 ",thank yes worked trying run keep getting could kept found stuck recent call last file line module import file line module raise library found library found error recent call last file line import file line module import report file line module import file line module import file line module import apt file line module import module original exception recent call last file line module import file line module raise library found library found particular tried find could find apt pip able help,issue,positive,positive,positive,positive,positive,positive
586563398,"> @Sadam1195 neither of those links work for me (it just sends me to a github page says no result match the search), and there's no main.py file anywhere on the repo so I don't know where I should put the files even if I could get them.

Hey @berzi do not click the link, instead copy the text of link and paste in new tab and you'll be directed rightly. Hope it helps.",neither link work page result match search file anywhere know put even could get hey click link instead copy text link paste new tab directed rightly hope,issue,negative,positive,positive,positive,positive,positive
586556178,"remove me from receiving that much mail cant find were to click to remove
my soul !!!

On Thu, Feb 13, 2020 at 5:43 PM jgraf451 <notifications@github.com> wrote:

> Got pip to read the directory by using an absolute path but now getting an
> errno 13 message indicating access denied.
>
> Any ideas.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/277?email_source=notifications&email_token=AM5WSME6JFTWTMQ7YWWTY3TRCXEJBA5CNFSM4KU4OKS2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4INND36A>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AM5WSMFSFR2XA6VDVHPMURTRCXEJBANCNFSM4KU4OKSQ>
> .
>


-- 
*Best regards*,

*Bryan Beauvais*
“Simplicity is the ultimate sophistication.""  -Leonardo da Vinci
",remove much mail cant find click remove soul wrote got pip read directory absolute path getting message access thread reply directly view best simplicity ultimate sophistication da,issue,positive,positive,positive,positive,positive,positive
586336795,"@Sadam1195 neither of those links work for me (it just sends me to a github page says no result match the search), and there's no main.py file anywhere on the repo so I don't know where I should put the files even if I could get them.",neither link work page result match search file anywhere know put even could get,issue,negative,neutral,neutral,neutral,neutral,neutral
586261536,"> Currently this project lives on my C:

I enjoy metaphors like that.",currently project enjoy like,issue,positive,positive,positive,positive,positive,positive
585521433,please reply.. tell how you solved it.. please,please reply tell please,issue,positive,neutral,neutral,neutral,neutral,neutral
584300942,"@josegomez unfortunately I'm still getting that error :( 

I went with ""conda install -c conda-forge python-sounddevice""

It installed without any errors, but running the demo_cli or demo_toolbox script both fail with the same error above.",unfortunately still getting error went install without running script fail error,issue,negative,negative,negative,negative,negative,negative
584058754,"> I do however got Microsoft Visual C++ Build Tools installed

Install the full Visual Studio and enable C/C++ in the install options.",however got visual build install full visual studio enable install,issue,negative,positive,positive,positive,positive,positive
583850018,"You sure you are running python3 right? People forget more often than you think.
If thats okay, a simple pip install tensorflow-gpu==1.14 should be enough.

But tbh, I always recommend using anaconda for installing tensorflow because it automatically handles download CuDNN on its own.",sure running python right people forget often think thats simple pip install enough always recommend anaconda automatically,issue,negative,positive,positive,positive,positive,positive
583804235,I do however got Microsoft Visual C++ Build Tools installed,however got visual build,issue,negative,neutral,neutral,neutral,neutral,neutral
583804180,"When running:

` pip install -r requirements.txt`
I get:

```
Building wheels for collected packages: webrtcvad
  Building wheel for webrtcvad (setup.py) ... error
  ERROR: Command errored out with exit status 1:
   command: 'C:\Users\Lucas\Anaconda3\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\Lucas\\AppData\\Local\\Temp\\pip-install-3r98ntqu\\webrtcvad\\setup.py'""'""'; __file__='""'""'C:\\Users\\Lucas\\AppData\\Local\\Temp\\pip-install-3r98ntqu\\webrtcvad\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d 'C:\Users\Lucas\AppData\Local\Temp\pip-wheel-8dg3vf10' --python-tag cp37
       cwd: C:\Users\Lucas\AppData\Local\Temp\pip-install-3r98ntqu\webrtcvad\
  Complete output (9 lines):
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build\lib.win-amd64-3.7
  copying webrtcvad.py -> build\lib.win-amd64-3.7
  running build_ext
  building '_webrtcvad' extension
  error: Microsoft Visual C++ 14.0 is required. Get it with ""Microsoft Visual C++ Build Tools"": https://visualstudio.microsoft.com/downloads/
  ----------------------------------------
  ERROR: Failed building wheel for webrtcvad
  Running setup.py clean for webrtcvad
Failed to build webrtcvad
Installing collected packages: webrtcvad, audioread, resampy, soundfile, librosa, sounddevice, Unidecode, inflect, dill, multiprocess
  Running setup.py install for webrtcvad ... error
    ERROR: Command errored out with exit status 1:
     command: 'C:\Users\Lucas\Anaconda3\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\Lucas\\AppData\\Local\\Temp\\pip-install-3r98ntqu\\webrtcvad\\setup.py'""'""'; __file__='""'""'C:\\Users\\Lucas\\AppData\\Local\\Temp\\pip-install-3r98ntqu\\webrtcvad\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'C:\Users\Lucas\AppData\Local\Temp\pip-record-ezwhs4cx\install-record.txt' --single-version-externally-managed --compile
         cwd: C:\Users\Lucas\AppData\Local\Temp\pip-install-3r98ntqu\webrtcvad\
    Complete output (9 lines):
    running install
    running build
    running build_py
    creating build
    creating build\lib.win-amd64-3.7
    copying webrtcvad.py -> build\lib.win-amd64-3.7
    running build_ext
    building '_webrtcvad' extension
    error: Microsoft Visual C++ 14.0 is required. Get it with ""Microsoft Visual C++ Build Tools"": https://visualstudio.microsoft.com/downloads/
    ----------------------------------------
ERROR: Command errored out with exit status 1: 'C:\Users\Lucas\Anaconda3\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\Lucas\\AppData\\Local\\Temp\\pip-install-3r98ntqu\\webrtcvad\\setup.py'""'""'; __file__='""'""'C:\\Users\\Lucas\\AppData\\Local\\Temp\\pip-install-3r98ntqu\\webrtcvad\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'C:\Users\Lucas\AppData\Local\Temp\pip-record-ezwhs4cx\install-record.txt' --single-version-externally-managed --compile Check the logs for full command output.
```",running pip install get building collected building wheel error error command exit status command open compile code complete output running running build running build running building extension error visual get visual build error building wheel running clean build collected inflect dill running install error error command exit status command open compile code install record compile complete output running install running build running build running building extension error visual get visual build error command exit status open compile code install record compile check full command output,issue,negative,positive,neutral,neutral,positive,positive
582932131,"Hello,
I don't understand,. how I training the model?
What I need to the program speak my voice or famous People?
My language is Portuguese
Thx",hello understand training model need program speak voice famous people language,issue,negative,positive,positive,positive,positive,positive
580198049,"@sberryman hi,
you trained encoder module for speaker verification task. Have you benchmarked your model with any dataset? if you have, could you share your benchmark results and dataset used for benchmarking? I have benchmarked pre-trained model on the voxceleb1 dataset and results are not looking good. I am getting EER of 8%. ",hi trained module speaker verification task model could share used model looking good getting eer,issue,positive,positive,positive,positive,positive,positive
580019740,"Fixed. I re-downloaded cuDNN, and replaced the files in the path listed in the link above.",fixed path listed link,issue,negative,positive,neutral,neutral,positive,positive
578983354,"Thanks for trying to walk me thru this but in the end it was just too 
complicated for my limited skills.  Looking at the issues log and the 
comments section of You Tube for this program guess I am not alone.  Was 
wondering how hard it would be to create a repository with a working 
version of this.  Something that would be downloaded and easily 
launched.  That would certainly make it more available to people with 
fewer computing skills.  Baidu's Deep Voice is also out there.  One 
might be easier to do than the other.  Thanks again for your help.

Jim



n 2020-01-21 10:44, LordBaaa wrote:
> Well as with all problems I’d say give me the full traceback/log
> 
> --
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub [1], or unsubscribe
> [2].
> 
> Links:
> ------
> [1]
> https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/266?email_source=notifications&amp;email_token=AN55LJG37SZZKFC26AI6VWLQ647CLA5CNFSM4KI25IE2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEJQZ27I#issuecomment-576822653
> [2]
> https://github.com/notifications/unsubscribe-auth/AN55LJAL6QLZLHLROY2LG53Q647CLANCNFSM4KI25IEQ
",thanks trying walk end complicated limited looking log section tube program guess alone wondering hard would create repository working version something would easily would certainly make available people deep voice also one might easier thanks help wrote well say give full reply directly view link,issue,positive,positive,neutral,neutral,positive,positive
578842449,"I have/had the same problem
from what I can tell there are two potential issues to check

1. Are you running python 64bit?
2. Do you actually have a gpu in the machine your are triyng to install on",problem tell two potential check running python bit actually machine install,issue,negative,neutral,neutral,neutral,neutral,neutral
578611708,"I have the same issue with Windows Python and Pip installed via Choco
![image](https://user-images.githubusercontent.com/3740700/73154601-b9553b00-40cf-11ea-8bed-ea75b627ea8d.png)
",issue python pip via image,issue,negative,neutral,neutral,neutral,neutral,neutral
578575165,"Here is the conda environment file https://gist.github.com/syllogismos/0b3097c7714ce6119db500a3fcdcc8dd

you can install this environment using this command `conda env create -f environment.yml`

It has additional slack and twitter pip packages installed as I tried building slack and twitter mention bots that reply with an audio of what you typed automatically. Will post the code here.",environment file install environment command create additional slack twitter pip tried building slack twitter mention reply audio automatically post code,issue,negative,neutral,neutral,neutral,neutral,neutral
578574172,"Hey, I trained p2.xlarge EC2 instance, used amazon deep learning AMI as base machine, I can give you the conda environment file if you needed.

Also I trained on part of the training dataset. And even with part of the training dataset, I needed a 1TB harddrive to save all the generated models, data of encoder, synthesizer and vocoder models.",hey trained instance used deep learning ami base machine give environment file also trained part training even part training save data synthesizer,issue,negative,negative,negative,negative,negative,negative
577999091,"Undfoetunately this project requires at least a cursory knowledge of programming languages and if not ml, the ability to set up extensive libraries to run with a gpu.

I’m sorry but perhaps we can help get it running?",project least cursory knowledge ability set extensive run sorry perhaps help get running,issue,positive,negative,negative,negative,negative,negative
577097622,Hi! Have you solved this issue? I'm facing the exact the same problem. ,hi issue facing exact problem,issue,negative,positive,positive,positive,positive,positive
577032266,"- For **Ubuntu** users
[https://drive.google.com/file/d/1FYICqyEh1d45TUksibduBK30h1AjGUh8/view?usp=sharing](url)
use this link (pre-built/pre-compiled).
Unzip this folder and place all the files in it to your directory where main.py file

- But if you want to compile it for yourself click on link below for .wheel file
[https://drive.google.com/file/d/1Sdg8iRwmjIrju-c7ikOZrkxRyPzAP0oH/view?usp=sharing](url)
and install is with `pip install webrtcvad-2.0.10-cp37-cp37m-linux_x86_64.whl`

@blacksocket 
For users if needed in future",use link folder place directory file want compile click link file install pip install future,issue,negative,neutral,neutral,neutral,neutral,neutral
577023967,"> The same issue here on linux ubuntu 18.4:
> 
> ### Collecting webrtcvad
> ## Using cached https://files.pythonhosted.org/packages/89/34/e2de2d97f3288512b9ea56f92e7452f8207eb5a0096500badf9dfd48f5e6/webrtcvad-2.0.10.tar.gz
> Building wheels for collected packages: webrtcvad
> Building wheel for webrtcvad (setup.py) ... error
> ERROR: Command errored out with exit status 1:
> command: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-j_8ezvc3/webrtcvad/setup.py'""'""'; **file**='""'""'/tmp/pip-install-j_8ezvc3/webrtcvad/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(**file**);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, **file**, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-ydzv_bvg --python-tag cp37
> cwd: /tmp/pip-install-j_8ezvc3/webrtcvad/
> Complete output (20 lines):
> running bdist_wheel
> running build
> running build_py
> creating build
> creating build/lib.linux-x86_64-3.7
> copying webrtcvad.py -> build/lib.linux-x86_64-3.7
> running build_ext
> building '_webrtcvad' extension
> creating build/temp.linux-x86_64-3.7
> creating build/temp.linux-x86_64-3.7/cbits
> creating build/temp.linux-x86_64-3.7/cbits/webrtc
> creating build/temp.linux-x86_64-3.7/cbits/webrtc/common_audio
> creating build/temp.linux-x86_64-3.7/cbits/webrtc/common_audio/signal_processing
> creating build/temp.linux-x86_64-3.7/cbits/webrtc/common_audio/vad
> x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-9u8fmL/python3.7-3.7.5=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DWEBRTC_POSIX -Icbits -I/usr/include/python3.7m -c cbits/pywebrtcvad.c -o build/temp.linux-x86_64-3.7/cbits/pywebrtcvad.o
> cbits/pywebrtcvad.c:1:10: fatal error: Python.h: No such file or directory
> #include <Python.h>
> ^~~~~~~~~~
> compilation terminated.
> error: command 'x86_64-linux-gnu-gcc' failed with exit status 1
> ERROR: Failed building wheel for webrtcvad
> Running setup.py clean for webrtcvad
> Failed to build webrtcvad
> Installing collected packages: webrtcvad
> Running setup.py install for webrtcvad ... error
> ERROR: Command errored out with exit status 1:
> command: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-j_8ezvc3/webrtcvad/setup.py'""'""'; **file**='""'""'/tmp/pip-install-j_8ezvc3/webrtcvad/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(**file**);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, **file**, '""'""'exec'""'""'))' install --record /tmp/pip-record-p6nl70v7/install-record.txt --single-version-externally-managed --compile
> cwd: /tmp/pip-install-j_8ezvc3/webrtcvad/
> Complete output (20 lines):
> running install
> running build
> running build_py
> creating build
> creating build/lib.linux-x86_64-3.7
> copying webrtcvad.py -> build/lib.linux-x86_64-3.7
> running build_ext
> building '_webrtcvad' extension
> creating build/temp.linux-x86_64-3.7
> creating build/temp.linux-x86_64-3.7/cbits
> creating build/temp.linux-x86_64-3.7/cbits/webrtc
> creating build/temp.linux-x86_64-3.7/cbits/webrtc/common_audio
> creating build/temp.linux-x86_64-3.7/cbits/webrtc/common_audio/signal_processing
> creating build/temp.linux-x86_64-3.7/cbits/webrtc/common_audio/vad
> x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-9u8fmL/python3.7-3.7.5=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DWEBRTC_POSIX -Icbits -I/usr/include/python3.7m -c cbits/pywebrtcvad.c -o build/temp.linux-x86_64-3.7/cbits/pywebrtcvad.o
> cbits/pywebrtcvad.c:1:10: fatal error: Python.h: No such file or directory
> #include <Python.h>
> ^~~~~~~~~~
> compilation terminated.
> error: command 'x86_64-linux-gnu-gcc' failed with exit status 1
> ----------------------------------------
> ERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-j_8ezvc3/webrtcvad/setup.py'""'""'; **file**='""'""'/tmp/pip-install-j_8ezvc3/webrtcvad/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(**file**);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, **file**, '""'""'exec'""'""'))' install --record /tmp/pip-record-p6nl70v7/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.

- For Ubuntu you should check this out.
[https://github.com/wiseman/py-webrtcvad/issues/40#issuecomment-561576279](url)
@blacksocket",issue building collected building wheel error error command exit status command file open file compile code file complete output running running build running build running building extension fatal error file directory include compilation error command exit status error building wheel running clean build collected running install error error command exit status command file open file compile code file install record compile complete output running install running build running build running building extension fatal error file directory include compilation error command exit status error command exit status file open file compile code file install record compile check full command output check,issue,negative,positive,positive,positive,positive,positive
577022668,"[https://drive.google.com/file/d/1ze3PHQtfpzCietKjyRT9Nd1t5jXKqtfS/view?usp=sharing](url)

- The provided link is for windows 10, python 3. 
- Unzip this folder and place all the files in it to your directory where main.py file is.

@ZenithBerserker @rdrlima ",provided link python folder place directory file,issue,negative,neutral,neutral,neutral,neutral,neutral
576930234,@iroh7 Thanks for posting the conclusion. That will definitely be helpful to know when I start working with vocoder training.,thanks posting conclusion definitely helpful know start working training,issue,positive,positive,neutral,neutral,positive,positive
576860021,"Thanks for your help @LordBaaa !
Ok i solved the issue, utterances were too short (0.2 in my case). The min size of utterance you can use is 0.25 which corespond to *0.25x16000 samples/seconde* equal to *4000*. The shape of *3200* was our smallest audio samples (*0.2x16000*). 
I close the post.",thanks help issue short case min size utterance use equal shape audio close post,issue,positive,positive,neutral,neutral,positive,positive
576822653,Well as with all problems I’d say give me the full traceback/log,well say give full,issue,negative,positive,positive,positive,positive,positive
576817635,"Still can't get passed the invalid syntax error msg.  Know I'm typing it 
correctly at python prompt.  Any ideas?

On 2020-01-20 16:16, LordBaaa wrote:
> @jgraf451 [1] Well try the command in console. Again should just be
> pip install tensorflow-gpu==1.14.0 with no spaces between ==.
> Literally exactly like I typed it. If you just pip install
> tensorflow-gpu== it should show you a list of available versions.
> 
> --
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub [2], or unsubscribe
> [3].
> 
> Links:
> ------
> [1] https://github.com/jgraf451
> [2]
> https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/266?email_source=notifications&amp;email_token=AN55LJD4BQW7LMXPZVB2JFLQ6Y5ETA5CNFSM4KI25IE2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEJODFTY#issuecomment-576467663
> [3]
> https://github.com/notifications/unsubscribe-auth/AN55LJHOC3LBR6XXZWJ5SGTQ6Y5ETANCNFSM4KI25IEQ
",still ca get invalid syntax error know correctly python prompt wrote well try command console pip install literally exactly like pip install show list available reply directly view link,issue,negative,positive,positive,positive,positive,positive
576467663,@jgraf451 Well try the command in console. Again should just be `pip install tensorflow-gpu==1.14.0` with no spaces between ==. Literally exactly like I typed it. If you just `pip install tensorflow-gpu==` it should show you a list of available versions. Also yeah command looks fine. Requirements.txt is basically just a list of individual pip commands so you should be able to just run them individually    if need be ,well try command console pip install literally exactly like pip install show list available also yeah command fine basically list individual pip able run individually need,issue,positive,positive,positive,positive,positive,positive
576463599,"Hi,

So I went in and changed the tensorflow info in the requirements.txt 
file.  However, when I do ""pip install -r requirements.txt""
I get a invalid syntax error msg.  Is this command incorrect?

Thanks,

Jim.



On 2020-01-20 14:40, LordBaaa wrote:
> Yeah I had a bit of a problem with that in the past. the greater then
> or equal to (<=) stuff messed up for me a bit. Also problem that
> happened to me is it will install tensorflow 2 with tensorflow-gpu to
> match, but tensorflow 2 is incompatible. All you should need to do is
> change tensorflow-gpu>=1.10.0,<=1.14.0 to tensorflow-gpu==1.14.0. Or
> just use this requirements.txt [1]
> 
> --
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub [2], or unsubscribe
> [3].
> 
> Links:
> ------
> [1]
> https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/4088240/requirements.txt
> [2]
> https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/266?email_source=notifications&amp;email_token=AN55LJHARBQXZ2O6WUSQ4LLQ6YR67A5CNFSM4KI25IE2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEJN7EYI#issuecomment-576451169
> [3]
> https://github.com/notifications/unsubscribe-auth/AN55LJFKBRYSB4FMLHDNWB3Q6YR67ANCNFSM4KI25IEQ
",hi went file however pip install get invalid syntax error command incorrect thanks wrote yeah bit problem past greater equal stuff bit also problem install match incompatible need change use reply directly view link,issue,negative,positive,positive,positive,positive,positive
576452589,"@jgraf451 Oh also for me I got an error like `Warning: Passing (type, 1) or '1type' as a synonym of type is deprecated;` with just the requirements.txt when I tried to import tensorflow as a test. So what has been working for was doing `pip install tensorflow==1.15.0`. This successfully grabbed my `cudart64_100.dll` and yeah been working for me so far. But I'd say try it without this first.",oh also got error like warning passing type synonym type tried import test working pip install successfully yeah working far say try without first,issue,negative,positive,positive,positive,positive,positive
576451169,"@jgraf451 Yeah I had a bit of a problem with that in the past. The greater then or equal to (<=) stuff messed up for me a bit. Also problem that happened to me is it will install tensorflow 2 with tensorflow-gpu to match, but tensorflow 2 is incompatible If you were to just install without version number. All you should need to do is change `tensorflow-gpu>=1.10.0,<=1.14.0` to `tensorflow-gpu==1.14.0`. Or just use this [requirements.txt](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/4088240/requirements.txt)
",yeah bit problem past greater equal stuff bit also problem install match incompatible install without version number need change use,issue,negative,positive,neutral,neutral,positive,positive
576438618,"Thanks!  That got me one step closer.  But now I'm getting the following 
error: $ pip install -r requirements.txt
ERROR: Could not find a version that satisfies the requirement 
tensorflow-gpu<=1.14.0,>=1.10.0 (from -r requirements.txt (line 1)) 
(from versions: none)
ERROR: No matching distribution found for 
tensorflow-gpu<=1.14.0,>=1.10.0 (from -r requirements.txt (line 1))

Any ideas?


On 2020-01-19 13:37, LordBaaa wrote:
> @jgraf451 [1] did you make sure that where you are running the command
> in the same directory as the requirements.txt file? What I mean is
> like if you are in command prompt and you run python. python is gonna
> be looking for files where ever you are at that time. So like
> C:\users\your_username. So if you do the pip install make sure you
> specify the full path to the file or simply change your directory to
> be in the same folder as the files location.
> 
> --
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub [2], or unsubscribe
> [3].
> 
> Links:
> ------
> [1] https://github.com/jgraf451
> [2]
> https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/266?email_source=notifications&amp;email_token=AN55LJFB2YBTRTWSCYNYRODQ6TBY5A5CNFSM4KI25IE2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEJK5HTI#issuecomment-576050125
> [3]
> https://github.com/notifications/unsubscribe-auth/AN55LJAYFSUZJENSBOWLVJDQ6TBY5ANCNFSM4KI25IEQ
",thanks got one step closer getting following error pip install error could find version requirement line none error matching distribution found line wrote make sure running command directory file mean like command prompt run python python gon na looking ever time like pip install make sure specify full path file simply change directory folder location reply directly view link,issue,positive,positive,positive,positive,positive,positive
576433508,"Hmm well I’ll have to give it a try and see if I get a similar error. As I said I haven’t tried out that stage yet. I have tried synthesizer and encoder stuff, mainly was focusing on getting everything working the best I could before doing some real training. In the meantime you could start a new model to train just to make sure a fresh model works and you don’t have a problem somewhere else.",well give try see get similar error said tried stage yet tried synthesizer stuff mainly getting everything working best could real training could start new model train make sure fresh model work problem somewhere else,issue,positive,positive,positive,positive,positive,positive
576384072,"@LordBaaa Thank you for your quick reply !
A few clarifications, my post wasn't very detailed...
Yes, I try to do transfer learning from the pretrained english model to french. I've not changed any parameters in the vocoder/hparams.py, voc_batch_size is still at 100.
The audio conversion in preprocessing for the synthesizer training wasn't good. So I changed it and the training of synthesizer is much better (red vs blue curve) :
![image](https://user-images.githubusercontent.com/58814678/72748454-94d0fe80-3bb7-11ea-85c7-6833193b505d.png)
Then I run the vocoder training and I got almost the same thing :
![image](https://user-images.githubusercontent.com/58814678/72748919-dca45580-3bb8-11ea-8759-da3ab0c6c76a.png)
The only parameter I've changed is utterance_min_duration set to 0.2s in synthesizer/hparams.py.
Vocoder preprocessing compute mels_gta from the mels and audio synthesizer folders.",thank quick reply post detailed yes try transfer learning model still audio conversion synthesizer training good training synthesizer much better red blue curve image run training got almost thing image parameter set compute audio synthesizer,issue,positive,positive,positive,positive,positive,positive
576050125,@jgraf451 did you make sure that where you are running the command in the same directory as the requirements.txt file? What I mean is like if you are in command prompt and you run python. python is gonna be looking for files where ever you are at that time. So like C:\users\your_username. So if you do the pip install make sure you specify the full path to the file or simply change your directory to be in the same folder as the files location.,make sure running command directory file mean like command prompt run python python gon na looking ever time like pip install make sure specify full path file simply change directory folder location,issue,positive,positive,positive,positive,positive,positive
576018335,"@iroh7 I haven’t started vocoder training yet so I don’t know all the ins and outs but well first off it is having a problem combining shapes. The second one has more elements then the rest. Are you using a pretrained model and continuing to train it I have had problems with that before cause you gotta match the settings exactly. I notice it has 428k steps which incidentally is the same number of steps as the pretrained model. The pertained model runs on batch sizes of 100 and it looks like you are using batch sizes of 1. The default batch size in vocoder/hparams.py is 100, did you change it?",training yet know well first problem combining second one rest model train cause got ta match exactly notice incidentally number model model batch size like batch size default batch size change,issue,negative,positive,positive,positive,positive,positive
576018278,@jgraf451 if the module is missing just do a pip install unidecode ,module missing pip install,issue,negative,negative,negative,negative,negative,negative
576017432,I’d say make sure you have tensorflow 1.1.14 or 15 installed not 2.0.0 or higher. I think I am using .15. The newer ones cause problems cause the code is different  ,say make sure higher think cause cause code different,issue,negative,positive,positive,positive,positive,positive
575937823,"> Full trace back please.

How can i get it? (what flags i need to add to `python3 demo_cli.py`?)",full trace back please get need add python,issue,negative,positive,positive,positive,positive,positive
575936753,Full trace back please.,full trace back please,issue,negative,positive,positive,positive,positive,positive
575933170,"I did some research. It fails because there is no file `create_model` in `./synthesizer/models`. Can you explain me, what i am doing wrong?",research file explain wrong,issue,negative,negative,negative,negative,negative,negative
574728956,"Sberryman - thank you again for all the help and responses in this thread.  Really nice of you to take your time.

I've read through a good portion of https://puu.sh/DHgBg.pdf to try to understand how all this works.

It does appear that the encoder creates the embedding, the synthesizer uses this to build the spectrogram and the vocoder outputs the waveform.

It occurs to me these processes are sequential and linked.  Would it be possible to start with your heavily trained encoder, and then hook up to arbitrary datasets for the syntheszer and vocoder?

IE:  Can i start the process with your pretrained encoder and then move on to synth and vocoder after?

My goal is to produce multispeaker (single speaker is honestly ok) english with no accent at all.  It seems like that should be relatively simple, but i continue running into issues combining pretraining models (size / scale mismatches) etc.

I've also looked at the nvidia mellotron, but when i started working to get the project to work - i had some python mismatches which made me afraid i might never get the corentin project to run again if i messed with it :) ",thank help thread really nice take time read good portion try understand work appear synthesizer build spectrogram sequential linked would possible start heavily trained hook arbitrary ie start process move goal produce single speaker honestly accent like relatively simple continue running combining pretraining size scale also working get project work python made afraid might never get project run,issue,positive,positive,positive,positive,positive,positive
574671095,"@gdineshk6174 Hi Dinesh!

I'm not an expert and I failed to generate a good synthesizer and vocoder model so anything I say, please don't take it as fact. You should be able to use the pretrained encoder and fine-tune it on your Indian accent dataset (most likely won't require much fine tuning, may not require any.)  Once the encoder is producing tight, easily distinguishable clusters for each speaker you can move on to the synthesizer. The most important thing from what I've read on the synthesizer/vocoder is to have clean audio. Meaning you don't want background noise in the audio. You'll also want quite a bit of training data, this is usually the hardest part.

I never thought about skipping the encoder and synthesizer and jumping straight to the vocoder using the pre-trained models. You can try it and see how it performs, would be interesting if it works and produces high quality speech. Hopefully you have plenty of GPUs available and lots of time, training and running experiments takes quite a bit of time.",hi expert generate good synthesizer model anything say please take fact able use accent likely wo require much fine tuning may require tight easily distinguishable speaker move synthesizer important thing read clean audio meaning want background noise audio also want quite bit training data usually part never thought skipping synthesizer straight try see would interesting work high quality speech hopefully plenty available lot time training running quite bit time,issue,positive,positive,positive,positive,positive,positive
574625569,"@sberryman hello , my name is Dinesh,  i plan to generate english audio but in indian accent so i started training the model from scratch starting with encoder. the encoder is doing good but im stuck with synthesizer as i dont have time-aligned transcript of audio files. so i thought i could download pretrained synthesizer and pretrained vocoder and generate audio. it did generate audio from sample voice but it still has american accent. on reading CorentinJ's thesis more carefully i came to know that wavenet is responsible for naturalness in generated voice. so now i'm planning to train only the vocoder on mel- spectrograms generated from downloaded pretrained synthesizer. do you think this works? and if it does , how should i proceed. 
i would really appreciate it if you could give any insight on how to tackle this problem.",hello name plan generate audio accent training model scratch starting good stuck synthesizer dont transcript audio thought could synthesizer generate audio generate audio sample voice still accent reading thesis carefully came know responsible naturalness voice train synthesizer think work proceed would really appreciate could give insight tackle problem,issue,positive,positive,positive,positive,positive,positive
574209099,"@ChellyTots 

I had the same issue and the PyQt5 didnt solve it either.

I dont know what in the end solved the problem but I deinstalled python and had to install Anaconda first. 

Folllow this tutorial step by step he should cover all the problems.
[https://www.youtube.com/watch?v=pKoOw5a74XU]",issue didnt solve either dont know end problem python install anaconda first tutorial step step cover,issue,negative,positive,positive,positive,positive,positive
574057952,"yes, the traing result original mel spectrogram parameters are better",yes result original mel spectrogram better,issue,positive,positive,positive,positive,positive,positive
574044112,"> > @LordBaaa, @bmccallister this is what I changed when trying all three trained models [Shaun shared](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/126#issuecomment-536097366) in the thread. Resulting quality was poor indeed. Please try to change models' settings as follows:
> > **encoder/params_model.py**
> > ```python
> > model_hidden_size = 256
> > model_embedding_size = 768
> > ```
> > 
> > 
> > **synthesizer/hparams.py**
> > ```python
> > speaker_embedding_size=768
> > ```
> > 
> > 
> > **vocoder/hparams.py**
> > Just add these lines at the end of the file
> > ```python
> > n_fft=2048
> > hop_size=300
> > win_size=1200
> > sample_rate=24000
> > speaker_embedding_size=768
> > voc_upsample_factors=(5, 5, 12)
> > ```
> 
> Thank you for this info! So you said that after these pajama quality was bad? Worse than the pt models provided by corentin?

It was worse than the default pt models. All voices sounded very similar, there was no difference between male and female voices. Though there is a chance that I did something wrong.

> Did you have any success finding a model that worked better than corentins?

Unfortunately not.

",trying three trained thread resulting quality poor indeed please try change python python add end file python thank said pajama quality bad worse provided worse default similar difference male female though chance something wrong success finding model worked better unfortunately,issue,negative,negative,negative,negative,negative,negative
574040258,"do you have tried changed the num_mels parameter, it is also important for mel spectrogram quality. when you tried the griffin-lim as the vocoder, less num_mels always generated poor quality audios.",tried parameter also important mel spectrogram quality tried le always poor quality,issue,negative,neutral,neutral,neutral,neutral,neutral
573881988,"@bmccallister I was extremely happy with the encoder model I've trained. Although if I were to retrain a new model from scratch I would use 256 as the embedding dimension and leave 768 hidden units. I would have also replaced the ReLU activation with Tanh as Corentin mentioned in this thread or the one on Resemblizer. 

They are linked. If you make any changes to the encoder you need to re-train everything downstream.

Since my focus was never to recreate a voice I never spent much time on the synthesizer or vocoder. If I were to attempt multispeaker synthesis right now, I would be using mellotron from nvidia as my base.
https://github.com/NVIDIA/mellotron
",extremely happy model trained although retrain new model scratch would use dimension leave hidden would also activation tanh thread one linked make need everything downstream since focus never recreate voice never spent much time synthesizer attempt synthesis right would base,issue,positive,positive,neutral,neutral,positive,positive
573728050,"> I don’t think I ever released my synthesizer or vocoder models which were trained on my encoder. They were so poor that I trashed them. Maybe I did and forgot but I wouldn’t recommend using them if I did.

Thank you for your response!

So should we not bother using the other samples you provided up thread?( it looks like you deleted the comment you had with the links but natravedova wuoted you so you can find your links posted up thread.

Are the encoder synth and vocoder not linked and the models not needed to be done sequentially?

May I ask sberryman what your highest level of success has been and if you have tips for repeating it or perhaps we could begin a repo to host pretrained models we have all worked on?",think ever synthesizer trained poor maybe forgot recommend thank response bother provided thread like comment link find link posted thread linked done sequentially may ask highest level success perhaps could begin host worked,issue,positive,negative,neutral,neutral,negative,negative
573726371,"> @LordBaaa, @bmccallister this is what I changed when trying all three trained models [Shaun shared](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/126#issuecomment-536097366) in the thread. Resulting quality was poor indeed. Please try to change models' settings as follows:
> 
> **encoder/params_model.py**
> 
> ```python
> model_hidden_size = 256
> model_embedding_size = 768
> ```
> 
> **synthesizer/hparams.py**
> 
> ```python
> speaker_embedding_size=768
> ```
> 
> **vocoder/hparams.py**
> Just add these lines at the end of the file
> 
> ```python
> n_fft=2048
> hop_size=300
> win_size=1200
> sample_rate=24000
> speaker_embedding_size=768
> voc_upsample_factors=(5, 5, 12)
> ```


Thank you for this info! So you said that after these pajama quality was bad? Worse than the pt models provided by corentin?

Did you have any success finding a model that worked better than corentins?

I’m sure we will figure this out!! :)
",trying three trained thread resulting quality poor indeed please try change python python add end file python thank said pajama quality bad worse provided success finding model worked better sure figure,issue,negative,negative,neutral,neutral,negative,negative
573524437,"> noob here without a GPU. How would someone use this with Google Colab

There is an example in the repo: https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/demo_toolbox_collab.ipynb",without would someone use example,issue,negative,neutral,neutral,neutral,neutral,neutral
573523609,"@LordBaaa, @bmccallister this is what I changed when trying all three trained models [Shaun shared](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/126#issuecomment-536097366) in the thread. Resulting quality was poor indeed. Please try to change models' settings as follows:

**encoder/params_model.py**  
```python
model_hidden_size = 256
model_embedding_size = 768
```

**synthesizer/hparams.py**
```python
speaker_embedding_size=768
```
**vocoder/hparams.py**  
Just add these lines at the end of the file
```python
n_fft=2048
hop_size=300
win_size=1200
sample_rate=24000
speaker_embedding_size=768
voc_upsample_factors=(5, 5, 12)
```",trying three trained thread resulting quality poor indeed please try change python python add end file python,issue,negative,negative,negative,negative,negative,negative
573503015,I don’t think I ever released my synthesizer or vocoder models which were trained on my encoder. They were so poor that I trashed them. Maybe I did and forgot but I wouldn’t recommend using them if I did. ,think ever synthesizer trained poor maybe forgot recommend,issue,negative,negative,negative,negative,negative,negative
573482631,"> @bmccallister Yeah I don’t know at this point. If you see my issue it’s very similar but with different torch.Size number problem. This is basically what I have been facing

It appears to me that the size of the model that was used in the synthesizer does not match against what is expected in the vocoder.

I had to make assumptions with the vocoder as there was no checklist file in the tacotron pretrained zip file so I copied one I had to make it get past that part of the script.

Have you or anyone else been able to build any other models and publish them anywhere?

I’ll also say I did a search in every parameter I could for the 25 value - to see if I could change it to 17 and nothing worked - this is why I think it must be an issue with the relationship to one of his other supplies pretrained models.",yeah know point see issue similar different number problem basically facing size model used synthesizer match make file zip file copied one make get past part script anyone else able build publish anywhere also say search every parameter could value see could change nothing worked think must issue relationship one,issue,negative,positive,neutral,neutral,positive,positive
573448004,@bmccallister Yeah I don’t know at this point. If you see my issue it’s very similar but with different torch.Size number problem. This is basically what I have been facing  ,yeah know point see issue similar different number problem basically facing,issue,negative,neutral,neutral,neutral,neutral,neutral
573428555,"> Thank you @CorentinJ, it works fine. Firstly I installed BuldTools , and then webrtcvad and after that I ran pip install -r requirements.txt

Hi, I done with the BuildTools and also webrtcvad and it works. But still I can't run pip install -r requirements.txt

can help?",thank work fine firstly ran pip install hi done also work still ca run pip install help,issue,positive,positive,positive,positive,positive,positive
573425700,"This is an amazing effort by you guys. Thank you for all the assistance.

I am trying to get the latest models from sberryman working but get the following output:

Arguments:
    enc_model_fpath:   encoder/saved_models/pretrained.pt
    syn_model_dir:     synthesizer/saved_models/logs-pretrained
    voc_model_fpath:   vocoder/saved_models/pretrained/pretrained.pt
    low_mem:           False
    no_sound:          False

Running a test of your configuration...

Found 1 GPUs available. Using GPU 0 (GeForce GTX 980M) of compute capability 5.2 with 4.2Gb total memory.

Preparing the encoder, the synthesizer and the vocoder...
Loaded encoder ""pretrained.pt"" trained to step 2152001
Found synthesizer ""pretrained"" trained to step 324000
Building Wave-RNN
Trainable Parameters: 4.481M
Loading model weights at vocoder/saved_models/pretrained/pretrained.pt
Traceback (most recent call last):
  File ""demo_cli.py"", line 63, in <module>
    vocoder.load_model(args.voc_model_fpath)
  File ""/home/lucidz/Documents/projects/sberryman/Real-Time-Voice-Cloning/vocoder/inference.py"", line 31, in load_model
    _model.load_state_dict(checkpoint['model_state'])
  File ""/home/lucidz/.local/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 839, in load_state_dict
    self.__class__.__name__, ""\n\t"".join(error_msgs)))
RuntimeError: Error(s) in loading state_dict for WaveRNN:
	size mismatch for upsample.up_layers.5.weight: copying a param with shape torch.Size([1, 1, 1, 25]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 17]).

Any thoughts on this? \",amazing effort thank assistance trying get latest working get following output false false running test configuration found available compute capability total memory synthesizer loaded trained step found synthesizer trained step building trainable loading model recent call last file line module file line file line error loading size mismatch weight param shape shape current model,issue,positive,positive,neutral,neutral,positive,positive
573369756,"Well I have still be gathering them I want to have a big data set before I start. I have downloaded all of librispeech as well as some of Common speech (https://voice.mozilla.org/en/about) and VCTK. However I have only trained on Librespeech. My objective right now is to get all the training programs just running for me. You should take a look at #126. I got encoder stuff going so now I am working on synthesizer training. If you look at the pretrained synth and vocoder are the least trained. @sberryman has so pretrained encoders in #126 but I have had a hard time getting them to work with the toolbox. As far as I know it goes long as you let it, the longer the better. The pretraiend encoder is like 1.5M steps. I am not sure for —skip existing on train. I know on preprocess it is suppose to skip audio files it has already done but yeah on train I don’t know. On the synth —skip_existibg seems to still take the processing power but just doesn’t write a new file. I am gonna like at the codes a bit more directly but like I say in my test it seemed to still take as long. If you can get the encoder from #126 working pls let me know.",well still gathering want big data set start well common speech however trained objective right get training running take look got stuff going working synthesizer training look least trained hard time getting work toolbox far know go long let longer better like sure train know suppose skip audio already done yeah train know still take power write new file gon na like bit directly like say test still take long get working let know,issue,positive,positive,neutral,neutral,positive,positive
573362915,"Unfortunately voxceleb2 was unable to complete preprocessing, due to the files formats being in mp4. May i ask what datasets you've used (assuming they're public!)

Also - I apologize for the silly question, but I have gotten the system to begin the encoder training.  Does this process complete? It continuously updates the screen and i'm up to about 140 steps, but I wasn't sure if it's a finite process, or I just stop it when it gets to an err/loss point I want to experiment with?

Final question: If I do --skip-existing, can i restart this process if it dies or I stop it?

Thank you so much again!",unfortunately unable complete due may ask used assuming public also apologize silly question gotten system begin training process complete continuously screen sure finite process stop point want experiment final question restart process stop thank much,issue,negative,negative,neutral,neutral,negative,negative
573322042,"@LordBaaa The skip existing code works, I used it many MANY times. Look at these two lines, they immediately proceed the `preprocess_wav` function which processes each source file.
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/encoder/preprocess.py#L93-L94

There is a ton of room for improving the experience but this is research code. And quite frankly, it is  one easiest to read and most well documented research projects I've come across.",skip code work used many many time look two immediately proceed function source file ton room improving experience research code quite frankly one easiest read well research come across,issue,positive,positive,positive,positive,positive,positive
573284828,"@sberryman Thanks, yeah --skip_existing is in synthesizer preprocess as well. Functionally though I think it does the work and then checks if it exists. I was looking at how some of the code worked (didnt get super deep so I may be wrong) but mainly I say this cause it will continue to use up CPU on like 1/900 even though I know it already exists. Plus it would still be nice to have the ability to pause it as opposed to having to wait or completely close the window",thanks yeah synthesizer well functionally though think work looking code worked didnt get super deep may wrong mainly say cause continue use like even though know already plus would still nice ability pause opposed wait completely close window,issue,positive,positive,positive,positive,positive,positive
573284587,@bmccallister Well I haven't tried to uses voxceleb2 yet but I would figure it would be up to the program to understand the data not your OS. Did actually you try running preprocess with Voxceleb2?,well tried yet would figure would program understand data o actually try running,issue,negative,neutral,neutral,neutral,neutral,neutral
573284428,@noahsw Not entirely sure about your paths there but if you were to just run `pipenv --python <version_goes_here>` and then `pipenv shell` and 'pip install `tensorflow-gpu==1.14.0` should work just fine,entirely sure run python shell install work fine,issue,positive,positive,positive,positive,positive,positive
573283997,"I will try it thank you!

I do have visdom running in another cli as well :)

I promise I’m generally good at following instructions!

Any suggestions in the fact that my Ubuntu doesn’t support the apple ios mp4 situation in voxceleb2? I did the math and it would take quite a while to convert the mp4 to mp3 using ffmpeg (I did some tests)

Thank you again!",try thank running another well promise generally good following fact support apple situation math would take quite convert thank,issue,positive,positive,positive,positive,positive,positive
573283576,@ChellyTots I usually forget to @ ppl so hopefully you'll see this.,usually forget hopefully see,issue,negative,negative,negative,negative,negative,negative
573282961,"@bmccallister Ahh, this is a path problem. Don't feel bad I had the same problem at first. I am also getting each step working for myself to. What you should need to do instead is specify the output of the `encoder_preprocess` results as its input like this. `python encoder_train.py my_run ""datasets\SV2TTS\encoder""`. Let me know if that works for ya. Also you'll want to start a visdom server. You can follow the instructions here on how to do that https://github.com/noagarcia/visdom-tutorial",path problem feel bad problem first also getting step working need instead specify output input like python let know work ya also want start server follow,issue,negative,negative,negative,negative,negative,negative
573280891,"sure! thank you so mich for reponding.  I have included all logs below. This is starting with the voxceleb and librispeech datasets in the ./datasets/ folder.
    
lucidz@voice:~/Documents/projects/Real-Time-Voice-Cloning$ git fetch origin
git reset --hard origin/master
remote: Enumerating objects: 8, done.
remote: Counting objects: 100% (8/8), done.
Unpacking objects: 100% (12/12), done.
remote: Total 12 (delta 8), reused 8 (delta 8), pack-reused 4
From https://github.com/CorentinJ/Real-Time-Voice-Cloning
   3e72a7d..c5c2261  master     -> origin/master
lucidz@voice:~/Documents/projects/Real-Time-Voice-Cloning$ git reset --hard origin/master
HEAD is now at c5c2261 Update README.md
lucidz@voice:~/Documents/projects/Real-Time-Voice-Cloning$ python encoder_preprocess.py ./datasets^
lucidz@voice:~/Documents/projects/Real-Time-Voice-Cloning$ ln -s ../datasets
lucidz@voice:~/Documents/projects/Real-Time-Voice-Cloning$ python encoder_preprocess.py ./datasets
Arguments:
    datasets_root:   datasets
    out_dir:         datasets/SV2TTS/encoder
    datasets:        ['librispeech_other', 'voxceleb1']
    skip_existing:   False

Preprocessing librispeech_other
LibriSpeech/train-other-500: Preprocessing data for 1166 speakers.
LibriSpeech/train-other-500: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1166/1166 [2:24:09<00:00,  7.42s/speakers]
Done preprocessing LibriSpeech/train-other-500.

Preprocessing voxceleb1
VoxCeleb1: using samples from 1123 (presumed anglophone) speakers out of 1251.
VoxCeleb1: found 1088 anglophone speakers on the disk, 35 missing (this is normal).
VoxCeleb1: Preprocessing data for 1088 speakers.
VoxCeleb1: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1088/1088 [1:44:53<00:00,  5.78s/speakers]
Done preprocessing VoxCeleb1.

lucidz@voice:~/Documents/projects/Real-Time-Voice-Cloning$ python encoder_train.py my_run ./datasets
Arguments:
    run_id:            my_run
    clean_data_root:   datasets
    models_dir:        encoder/saved_models
    vis_every:         10
    umap_every:        100
    save_every:        500
    backup_every:      7500
    force_restart:     False
    visdom_server:     http://localhost
    no_visdom:         False

No model ""my_run"" found, starting training from scratch.
Updating the visualizations every 10 steps.
Setting up a new session...
Traceback (most recent call last):
  File ""encoder_train.py"", line 46, in <module>
    train(**vars(args))
  File ""/home/lucidz/Documents/projects/Real-Time-Voice-Cloning/encoder/train.py"", line 68, in train
    for step, speaker_batch in enumerate(loader, init_step):
  File ""/home/lucidz/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 819, in __next__
    return self._process_data(data)
  File ""/home/lucidz/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 846, in _process_data
    data.reraise()
  File ""/home/lucidz/.local/lib/python3.6/site-packages/torch/_utils.py"", line 385, in reraise
    raise self.exc_type(msg)
FileNotFoundError: Caught FileNotFoundError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File ""/home/lucidz/.local/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py"", line 178, in _worker_loop
    data = fetcher.fetch(index)
  File ""/home/lucidz/.local/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py"", line 47, in fetch
    return self.collate_fn(data)
  File ""/home/lucidz/Documents/projects/Real-Time-Voice-Cloning/encoder/data_objects/speaker_verification_dataset.py"", line 55, in collate
    return SpeakerBatch(speakers, self.utterances_per_speaker, partials_n_frames)
  File ""/home/lucidz/Documents/projects/Real-Time-Voice-Cloning/encoder/data_objects/speaker_batch.py"", line 8, in __init__
    self.partials = {s: s.random_partial(utterances_per_speaker, n_frames) for s in speakers}
  File ""/home/lucidz/Documents/projects/Real-Time-Voice-Cloning/encoder/data_objects/speaker_batch.py"", line 8, in <dictcomp>
    self.partials = {s: s.random_partial(utterances_per_speaker, n_frames) for s in speakers}
  File ""/home/lucidz/Documents/projects/Real-Time-Voice-Cloning/encoder/data_objects/speaker.py"", line 34, in random_partial
    self._load_utterances()
  File ""/home/lucidz/Documents/projects/Real-Time-Voice-Cloning/encoder/data_objects/speaker.py"", line 14, in _load_utterances
    with self.root.joinpath(""_sources.txt"").open(""r"") as sources_file:
  File ""/usr/lib/python3.6/pathlib.py"", line 1183, in open
    opener=self._opener)
  File ""/usr/lib/python3.6/pathlib.py"", line 1037, in _opener
    return self._accessor.open(self, flags, mode)
  File ""/usr/lib/python3.6/pathlib.py"", line 387, in wrapped
    return strfunc(str(pathobj), *args)
FileNotFoundError: [Errno 2] No such file or directory: 'datasets/LibriSpeech/_sources.txt'


",sure thank included starting folder voice git fetch origin git reset hard remote done remote counting done done remote total delta delta master voice git reset hard head update voice python voice voice python false data done found disk missing normal data done voice python false false model found starting training scratch every setting new session recent call last file line module train file line train step enumerate loader file line return data file line file line reraise raise caught worker process original recent call last file line data index file line fetch return data file line collate return file line file line file line file line file line open file line return self mode file line wrapped return file directory,issue,negative,negative,neutral,neutral,negative,negative
573270195,Could I see the log leading up to the failure?,could see log leading failure,issue,negative,negative,negative,negative,negative,negative
573102792,"@LordBaaa I apologize, I did miss your last response. As far as testing the encoder, that should be all you would need to change. I've never used any of CorentinJ's GUI features so I'm not sure how my encoder model will work there.

I did train a synthesizer and vocoder model based on my encoder weights but it didn't work out very well. My focus has been on the encoder only, unfortunately I am not much help on the other two components.

I've also used the entire Common Voice dataset as part of training (every language available) which is a great resource. I think I've managed to compile various datasets to get my unique speaker count up to just over 39,000. 

I also had a lot of conversations with CorentinJ over on his Resemblyzer repository for Resemble AI. 
https://github.com/resemble-ai/Resemblyzer/issues/13

There is a feature to ""resume"" the pre-processing step. If you look in the `encoder_preprocess.py` file you'll see a command line argument called `--skip_existing` which will skip over files which have already been processed.
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/encoder_preprocess.py#L37-L39",apologize miss last response far testing would need change never used sure model work train synthesizer model based work well focus unfortunately much help two also used entire common voice part training every language available great resource think compile various get unique speaker count also lot repository resemble ai feature resume step look file see command line argument skip already,issue,negative,positive,positive,positive,positive,positive
573097667,@sberryman (I didn’t put @ at the front last time so I figured maybe you didn’t see my response) Also I have been working on training a synth model.  I notice the synth and vocoder are the least trained in the pretrained models. I have been gathering processing datasets with the pre-process. I want to add in some other languages from common voice to. Sometimes I need to stop the preprocessing but when I do it has to do it all from scratch next time. Is there a function I don’t know about to pause/save progress or do I need to see about adding one? Thanks ,put front last time figured maybe see response also working training model notice least trained gathering want add common voice sometimes need stop scratch next time function know progress need see one thanks,issue,positive,negative,neutral,neutral,negative,negative
572924775,"I know some my problem: one is that 130k is too short for wavernn training. There may be other issue, I am training new Mel spectrogram parameters:n_fft=1024, hop_size=256,win_size=1024, sample_rate=22050,",know problem one short training may issue training new mel spectrogram,issue,negative,positive,neutral,neutral,positive,positive
572912480," text and the encoder embedding are in-set processing in clone model, as the same as the tacotron model",text clone model model,issue,negative,neutral,neutral,neutral,neutral,neutral
572911024,Maybe  the text and the encoder embedding of this aynthetic audio  are in-set data?,maybe text audio data,issue,negative,neutral,neutral,neutral,neutral,neutral
572909415,"> ![step-140000-align](https://user-images.githubusercontent.com/8288975/70890840-f59d7200-2020-11ea-86fe-4deb9ba85e50.png)
> ![step-140000-mel-spectrogram](https://user-images.githubusercontent.com/8288975/70890841-f6360880-2020-11ea-93ed-f86f05e67627.png)
> this is my synthesizer result, is that ok ?
> [result.zip](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/3966718/result.zip)
> this is my vocoder result, and you can hear the result audio (130k_steps_5_gen_batched_target8000_overlap400) is not good, it seems noise, I don't know how to solve it?
> I trained mandarin

I donnot undersand y u got unbad synthetic voice with bad alignments.... Character embedding or one-hot u have used as input of taco?  
 if u want to know quality of synthesizer, maybe can make embedding by python script.  just a experiment for synthesizer.  ",step align step synthesizer result result hear result audio good noise know solve trained mandarin donnot got synthetic voice bad character used input want know quality synthesizer maybe make python script experiment synthesizer,issue,negative,positive,neutral,neutral,positive,positive
572430054,"> I got the same problem.I run it in linux instead of windows.

You did it without Nvidia graphics card?",got run instead without graphic card,issue,negative,neutral,neutral,neutral,neutral,neutral
572422554,Is it possible to run this project without Nvidia graphics card? ,possible run project without graphic card,issue,negative,neutral,neutral,neutral,neutral,neutral
572207937,"Well like I said I changed the `model_hidden_size` &
`model_embedding_size` in `encoder/params_model.py` To 768 Like you had mentioned in the past (I read through all the old comments). Should I change anything else, what am I missing?",well like said like past read old change anything else missing,issue,positive,negative,negative,negative,negative,negative
572058218,"@LordBaaa , did you adjust `/encoder/params_model.py`? It is complaining about a mismatch of dimensions from what the model expects and what is in the weights file.",adjust mismatch model file,issue,negative,neutral,neutral,neutral,neutral,neutral
571908783,"@sberryman  I downloaded your models but I get this error when trying to load them. I tried using your fork but still doesn't load. Any recommendations? Thanks

`model_hidden_size = 256`
`model_embedding_size = 256`
![image](https://user-images.githubusercontent.com/48495287/71955318-f48dd980-31b5-11ea-990f-20861b1c9ae4.png)

`model_hidden_size = 768`
`model_embedding_size = 768`
![image](https://user-images.githubusercontent.com/48495287/71955121-6ade0c00-31b5-11ea-9275-1749a489a7e8.png)

",get error trying load tried fork still load thanks image image,issue,negative,positive,positive,positive,positive,positive
571338635,"@ChellyTots Well for `demo_cli.py` I'd say try importing scipy in python console and see if it errors out. If it works keep going in the list, aka try `scipy.optimize` and finnally `from scipy.optimize import minpack2` until it does. 

For `demo_toolbox.py` I had the same problem, it took me a while but I realized that `qt_compat.py` was doing a check for a `QT_API` and it had none. To fix this I just ran `pip install PyQt5`. (pip3.7 in my case). It loaded up immediately after that. Hope this helps.",well say try python console see work keep going list aka try import problem took check none fix ran pip install pip case loaded immediately hope,issue,negative,neutral,neutral,neutral,neutral,neutral
571061560,"@freecui, george_cql@163.com is my private email address, can you send me your wechat or communicate by email, that we can be more easier to communicate and optimize the model!",private address send communicate easier communicate optimize model,issue,positive,neutral,neutral,neutral,neutral,neutral
570891181,"did you change any hyper parameters? i also trained by chinese, but the synthesis model loss is very high. if is possible, can you share three models. ",change hyper also trained synthesis model loss high possible share three,issue,negative,positive,neutral,neutral,positive,positive
570890609,I trained three parts of this modle by chinese datasets; I just use server hundreds speakers; ,trained three use server,issue,negative,neutral,neutral,neutral,neutral,neutral
570591885,"> 
> 
> I had a similar issue (on Windows 7), except for me the error was
> ` C:\Python37\include\pyconfig.h(59): fatal error C1083: Cannot open include file: 'io.h': No such file or directory`
> `error: command 'C:\\Program Files (x86)\\Microsoft VisualStudio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.23.28105\\bin\\HostX86\\x64\\cl.exe' failed with exit status 2`
> 
> To solve this, I had to install the Windows 10 SDK option from the Visual Studios build tools.
> I then ran the ""Developer Command Prompt"" under Visual Studios in 'All Programs' in the Start menu. In this command prompt I was able to pip install webrtcvad successfully.

How did you install it in developer cmd?",similar issue except error fatal error open include file file directory error command exit status solve install option visual build ran developer command prompt visual start menu command prompt able pip install successfully install developer,issue,negative,positive,positive,positive,positive,positive
570081911,"Assuming you had everything downloaded and extracted to a folder in your hard drive (make sure there's no spaces in your path)
So do something simple like creating a folder called ""Datasets"" in the D drive directory 
(So your path would be = D:\Datasets\LibriSpeech\train-clean-100)

Now when you run the toolbox python script. Type this into the terminal and if everything else is installed correctly, then you're in luck.
`python demo_toolbox.py -d D:\Datasets`",assuming everything extracted folder hard drive make sure path something simple like folder drive directory path would run toolbox python script type terminal everything else correctly luck python,issue,positive,positive,neutral,neutral,positive,positive
570061209,"> I didn't use so many speakers, and I think speaker numbers just affect the clone, not the synthesizer ; now my synthetic sound quality is bad, I think

i also have this problem. but i think your quality is good, do you have trained three parts of this model by chinese datasets? and how many hours and how many speakers do your datasets have and what is the name of the Chinese database? and do you have midified some hyperparams, (such as: the num_mels? n_fft? hop_size? win_size? sample_rate?). and how about your clone results? hope for your  replying, thanks!!",use many think speaker affect clone synthesizer synthetic sound quality bad think also problem think quality good trained three model many many name clone hope thanks,issue,negative,positive,positive,positive,positive,positive
569672431,"Thank you very much. I did run encode_preprocess.py, but still reported an error. Maybe there is a problem with my data. I will see the reason from the data. Is your _source.txt in voxceleb1?",thank much run still error maybe problem data see reason data,issue,negative,positive,positive,positive,positive,positive
569671077,"sorry ,i got the false ldd file,and i find the libqxcb.so ,returned:
![image](https://user-images.githubusercontent.com/42055915/71582752-822e5100-2b46-11ea-8808-4996620f010a.png)
",sorry got false file find returned image,issue,negative,negative,negative,negative,negative,negative
569668789,"You can have this problem because:
1) you don't run encoder_preprocess.py
2) if you use data is not from the librispeech or voxceleb, your data can have different format.
3)last: if preprocessing is finished successfully, but you have this problem, you are writing command wrong. It must look like this: 
encoder_train.py <name of your encoder model(it can be anything)> <directory which was made by preprocessing(find it)>",problem run use data data different format last finished successfully problem writing command wrong must look like name model anything directory made find,issue,negative,positive,neutral,neutral,positive,positive
569665759,"@CorentinJ I have the same problem, when I run encoder_train.py, it shows that the _source.txt file cannot be found.I want to know if this file was written by myself, or should it be in the database?And in your code, the file is added in encoder / data_objects / speaker.py. The database I downloaded does not contain such a file. If we need to write it ourselves, we hope you can give us the way to write it, which is helpful for the subsequent training of encoder.",problem run file want know file written code file added contain file need write hope give u way write helpful subsequent training,issue,positive,neutral,neutral,neutral,neutral,neutral
568267366,"I used the gui in windows and it worked with wav files. mp3 is supported but I got the same error. Not sure what to do. Maybe while having a library already working, make a folder name it whatever inside and place some wav and mp3 m4a files try the gui ?",used worked got error sure maybe library already working make folder name whatever inside place try,issue,negative,positive,positive,positive,positive,positive
568267051,If you have a cuda gpu and still get this error after you run and install all cuda related dlls and stuff with: conda install -c conda-forge tensorflow-gpu        I suggest you start in a new virtual env from scratch  this command might install a newer version of tensorflow gpu (the requirements.txt has version 1.14.0) see what version you have after you run all commands and if you don't have 1.14.0 unninstall the version you have and run the requirements.txt command again to intsall 1.14.0 ,still get error run install related stuff install suggest start new virtual scratch command might install version version see version run version run command,issue,negative,positive,neutral,neutral,positive,positive
568230814,"That was super cool, thank you SquallAlex

I was really hoping that would do it:

(DeepvoiceC2) C:\DeepvoiceC2\Real-Time-Voice-Cloning>python demo_cli.py
Traceback (most recent call last):
  File ""C:\Users\bev\Anaconda3\envs\DeepvoiceC2\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\bev\Anaconda3\envs\DeepvoiceC2\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\bev\Anaconda3\envs\DeepvoiceC2\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\bev\Anaconda3\envs\DeepvoiceC2\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\bev\Anaconda3\envs\DeepvoiceC2\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""demo_cli.py"", line 3, in <module>
    from synthesizer.inference import Synthesizer
  File ""C:\DeepvoiceC2\Real-Time-Voice-Cloning\synthesizer\inference.py"", line 1, in <module>
    from synthesizer.tacotron2 import Tacotron2
  File ""C:\DeepvoiceC2\Real-Time-Voice-Cloning\synthesizer\tacotron2.py"", line 3, in <module>
    from synthesizer.models import create_model
  File ""C:\DeepvoiceC2\Real-Time-Voice-Cloning\synthesizer\models\__init__.py"", line 1, in <module>
    from .tacotron import Tacotron
  File ""C:\DeepvoiceC2\Real-Time-Voice-Cloning\synthesizer\models\tacotron.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\bev\Anaconda3\envs\DeepvoiceC2\lib\site-packages\tensorflow\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\bev\Anaconda3\envs\DeepvoiceC2\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\bev\Anaconda3\envs\DeepvoiceC2\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\bev\Anaconda3\envs\DeepvoiceC2\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\bev\Anaconda3\envs\DeepvoiceC2\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\bev\Anaconda3\envs\DeepvoiceC2\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\bev\Anaconda3\envs\DeepvoiceC2\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\bev\Anaconda3\envs\DeepvoiceC2\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors


",super cool thank really would python recent call last file line module import file line module file line description file line return name file file line return spec load dynamic link library routine handling exception another exception recent call last file line module import synthesizer file line module import file line module import file line module import file line module import file line module import file line module import file line module raise recent call last file line module import file line module file line description file line return name file file line return spec load dynamic link library routine load native see,issue,positive,positive,neutral,neutral,positive,positive
568201938,"Update:-
I should've put the audio files in the same folder of the script but after that I found 
Synthesizing the waveform:
{| ████████████████ 104500/105600 | Batch Size: 11 | Gen Rate: 12.1kHz | }Caught exception: PortAudioError('Error querying device -1',)
can anyone face this before. 


I've run the script and when I try to upload a file .mp3 
I got Caught exception: RuntimeError(""Error opening '.': File contains data in an unknown format."",)
and when I try to upload .wav file I got 
Caught exception: RuntimeError(""Error opening '/home/ubuntu/hello-this-my1576942509.wav': System error."",)
Is there something I need to do  I put the voices in /home/ubuntu 
What should I do? and I am using only the terminal ",update put audio folder script found batch size gen rate caught exception querying device anyone face run script try file got caught exception error opening file data unknown format try file got caught exception error opening system error something need put terminal,issue,negative,negative,neutral,neutral,negative,negative
568109923,"'conda activate deepvoice'-activate created virtual environment

For Cuda-CDNN-etc...

Update! There is no tedious process like this required now as anaconda is directly providing a command to install tensorflow-gpu.
Just create a new environment using 
conda create -n [name] python=[version]
And then use 
conda install -c conda-forge tensorflow-gpu 
and it will assess which version (CUDA,CUDNN, etc.) you require and download and install it directly ;)

pip install -r requirements.txt

conda install pytorch torchvision cudatoolkit=10.1 -c pytorch

conda install -c conda-forge tensorflow-gpu 

Toolbox
You can then try the toolbox:

python demo_toolbox.py -d H:\Deepvoice_Datasets  <---- root of the databese folders no spapes between folders

python encoder_preprocess.py H:\Deepvoice_Datasets

or
python demo_toolbox.py

",activate virtual environment update tedious process like anaconda directly providing command install create new environment create name version use install ass version require install directly pip install install install toolbox try toolbox python root python python,issue,negative,negative,neutral,neutral,negative,negative
568076700,Are there any experts for hire which could remote into a system and make this program work?,hire could remote system make program work,issue,negative,negative,neutral,neutral,negative,negative
567653450,"> @sberryman I'd be interested in hearing more samples.
> 
> In my experience, target=16000 overlap=800 produces high quality pop-free audio. I used it to make Dr. Kleiner sing: https://www.reddit.com/r/HalfLife/comments/d2rzf0/deepfaked_dr_kleiner_sings_i_am_the_very_model_of/

how did you train this exactly? I am interested in doing something similar",interested hearing experience high quality audio used make sing train exactly interested something similar,issue,positive,positive,positive,positive,positive,positive
567614668,Quick question. So if I want to make this speech generation work well on a specific person do I need to train it on a bunch of their annotated speech. Or can I just use the pretrained weights with a bunch of their recordings. Or am I looking for a completely different network architecture??,quick question want make speech generation work well specific person need train bunch speech use bunch looking completely different network architecture,issue,negative,positive,positive,positive,positive,positive
567086474,"Ahhh drat.... that machine has 32gb of Virtual memory on it.
I wonder if that's the error you get when the version of CuDNN, or TF or Cuda is not matched up just right.",drat machine virtual memory wonder error get version right,issue,negative,positive,positive,positive,positive,positive
567062507,"Sorry, I dont describe my problem. I try to train this on Russian. But when I run encoder_train.py I get this error",sorry dont describe problem try train run get error,issue,negative,negative,negative,negative,negative,negative
566451397,"I didn't use so many speakers, and I think speaker numbers just affect the clone, not the synthesizer ; now my synthetic sound quality is bad, I think",use many think speaker affect clone synthesizer synthetic sound quality bad think,issue,negative,positive,neutral,neutral,positive,positive
566435197,"how many spks when training GE2E? If have good embedding, with 1000spks train tacotron will get good voice i think.",many training gee good train get good voice think,issue,positive,positive,positive,positive,positive,positive
566434368,Maybe need choose a overfit point with tocotron.,maybe need choose overfit point,issue,negative,neutral,neutral,neutral,neutral,neutral
566404920,"I meant virtual memory - pagefile.sys for windows, swap for linux.
 I use Windows 7, 8 Gb RAM, 12Gb pagefile.sys
When you start tensorflow it will use GPU automatically if needed. You can check that with GPU-Z utility. ",meant virtual memory swap use ram start use automatically check utility,issue,negative,neutral,neutral,neutral,neutral,neutral
566363852,"Your problem is related to the librosa bug in the latest version. You can refer to this [link](https://github.com/librosa/librosa/issues/1037)

I solve it by converting the path into String
Python 3.6+: `path = os.fspath(path)`
Python 3.4+: `path = str(path)`
",problem related bug latest version refer link solve converting path string python path path python path path,issue,negative,positive,positive,positive,positive,positive
566308400,"Thanks for the idea, really appreciate it. 
This swap file.... is that for Linux?
I'm looking all over PyCharm for a swap file setting. The only thing which comes to mind is Windows virtual memory. I have 32gb on this system, but if it's not using it, that would make sense.
I could make a Linux VM but I don't know if that would use the GPU or not.",thanks idea really appreciate swap file looking swap file setting thing come mind virtual memory system would make sense could make know would use,issue,positive,positive,positive,positive,positive,positive
566218870,"> ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.
Try to increase swap file. It works for me with 12 Gb of swap (to train a network). To start up examples above enough 1-3Gb. 
Cuda 10.2, Tensorflow-gpu==1.15.0, GeForce 1060 6Gb",load dynamic link library routine try increase swap file work swap train network start enough,issue,positive,neutral,neutral,neutral,neutral,neutral
565403742,@khu834 that's exactly what I did. Worked beautifully.,khu exactly worked beautifully,issue,positive,positive,positive,positive,positive,positive
565248635,"@khu834  Thank you for your advice +1: I would try to concatenate the audio later.  

",khu thank advice would try concatenate audio later,issue,negative,neutral,neutral,neutral,neutral,neutral
565101132,"@Liujingxiu23 I have not tried adjusting the mel spectrogram features. Personally, I have a feeling using features (spectrogram) as input to the model can be avoided... At least for my use case.

My motivations for the speaker encoder are not inline with replicating voices, I'm more interested in using the embedding for speaker diarization.

Right now I'm training a completely new model on English using 22,553 speakers. ",tried mel spectrogram personally feeling spectrogram input model least use case speaker interested speaker right training completely new model,issue,negative,positive,neutral,neutral,positive,positive
564957119,"@sberryman  Have you tried other features or other version of SV.

For the feature, the SV paper says they use ""40-dimension log-mel-filterbank
energies as the features for each frame"" . This may differ from the feature we use now. I cannot judge how much influence about this diff. 

For the SV, I am tring to run ""https://github.com/Janghyun1230/Speaker_Verification/"".  I tring different learning rate now. However, the learning is also very slow, it seems hopeless and not helpfull to me now.
",tried version feature paper use frame may differ feature use judge much influence run different learning rate however learning also slow hopeless,issue,negative,negative,neutral,neutral,negative,negative
564851930,"@Liujingxiu23 I've already included SLR68 in my training dataset as well as SLR82. You are correct on SLR82, the audio has a lot of background noise (music, sound effects, other people talking, etc.) From what I remember based on conversations with CorentinJ and the paper, background audio is not bad for the encoder training. In fact it helps the encoder as it learns to focus on the spoken audio.

I completely agree, I've probably spent well over 90 days on various experiments related to encoder training. Right now I'm focusing on adding random noise to the speakers to make a more robust encoder. Training has been very tricky on though.",already included training well correct audio lot background noise music sound effect people talking remember based paper background audio bad training fact focus spoken audio completely agree probably spent well day various related training right random noise make robust training tricky though,issue,positive,negative,neutral,neutral,negative,negative
564829537,"@sberryman  About 2600 speakers can download from http://openslr.org/resources.php , you can use key word ""Chinese"" to find them, for example, SLR38, SLR68. I am sorry I can not share other datasets and the model. And I did not use SLR82,  I can't remember the exact reason, maybe the wavs in this dataset have loud background music.

The training of the encoder model is so time consuming，I cant wait to train a synthesizer model (when the loss is about 0.01), but the result is not good. Maybe I should wait for more days.",use key word find example sorry share model use ca remember exact reason maybe loud background music training model time cant wait train synthesizer model loss result good maybe wait day,issue,negative,positive,positive,positive,positive,positive
564565797,"@Liujingxiu23 So glad to hear that the Resemblyzer thread has helped you! @CorentinJ has been incredibly helpful answering my questions.

There have been quite a few people asking for a Chinese embedding, if you are able to post a link to your trained model I'm sure it would be helpful to quite a few people.

I have been experimenting on a completely new model for the embedding and am making a lot of progress. I've been using quite a few languages on in same model (including Chinese from http://openslr.org/82/ but that is only 1,000 speakers) Right now I'm training using 37,606 speakers of which a little more than 50% are English. Is the 9,000 speaker Chinese dataset you are using available for download? I'm always trying to add more speakers from different languages.

I have been focused on speaker embedding not the full pipeline, I've only attempted to train the vocoder and synthesizer once and didn't have the best success.",glad hear thread incredibly helpful quite people able post link trained model sure would helpful quite people completely new model making lot progress quite model right training little speaker available always trying add different speaker full pipeline train synthesizer best success,issue,positive,positive,positive,positive,positive,positive
564367726,"@sberryman  Thank you very much for your reply. The similarity image may be somewrong, I finetune the model again with chinese corpus(9000 speakers) and tested on different steps. Though the similar values get better with step increase, the speed is really slow.

Utterance
Same - Median: 0.757(1.5675M)    0.768(1.8075M)   0.712(2.07M)   0.701(2.3625M)
Different - Median:  0.916(1.5675M)    0.910(1.8075M)   0.921(2.07M)   0.920(2.3625M)

Speaker
Same - Median: 0.823(1.5675M)    0.832(1.8075M)   0.784(2.07M)   0.770(2.3625M)
Different - Median:  0.979(1.5675M)    0.980(1.8075M)   0.976(2.07M)   0.976(2.3625M)

I guess training from scratch may be a better choice.

You have had so many disscusion at https://github.com/resemble-ai/Resemblyzer/issues/13， I am tring to train a new model like yours,  the loss and err decream much faster. Thank you so much!

By the way, since the SV2TTS paper use 18k speakers, you have more speakers then, I guess you many get a good encoder? Then have you got some good end-to-end results, I mean synthesis wavs of speaker unseen. ",thank much reply similarity image may model corpus tested different though similar get better step increase speed really slow utterance median different median speaker median different median guess training scratch may better choice many train new model like loss err decream much faster thank much way since paper use guess many get good got good mean synthesis speaker unseen,issue,positive,positive,positive,positive,positive,positive
564166391,You'll need a real time speech to text system then use the text output to drive the real time voice cloning.,need real time speech text system use text output drive real time voice,issue,negative,positive,positive,positive,positive,positive
564165466,"It seems that the speech rate is dependent on the length of text.
The longer the text, the faster the speech.

You can try breaking down the sentences into shorter chunks, then concatenate the audio back together.
",speech rate dependent length text longer text faster speech try breaking shorter concatenate audio back together,issue,negative,neutral,neutral,neutral,neutral,neutral
563667590,"Is this project only usable with a nvidia gpu? I'm getting this error as well, but I have an intel gpu so I can't use CUDA.

Edit: Nevermind, this fork works fine https://github.com/shawwn/Real-Time-Voice-Cloning",project usable getting error well ca use edit fork work fine,issue,negative,positive,positive,positive,positive,positive
563276710,"@Liujingxiu23 Apologize for the delayed response as I was out of town but the plots look okay to me. How many additional steps did you finetune? It looks like it could be trained longer. Also take a look at this issue as I made a ton of comments and posted a lot of plots.
https://github.com/resemble-ai/Resemblyzer/issues/13
",apologize response town look many additional like could trained longer also take look issue made ton posted lot,issue,negative,positive,positive,positive,positive,positive
561515655,"@geekboood,  you train chinese encoder model from scratch? Have you started train the tacotron and wavernn model? How is the result? 
I finetuned the pretrained model with chinese speech with lr=0.00001, but the result(the image of ""cross-similarity between utterance"") is not as good as yours.

By the way, have you changed the activation function?  In he paper,  ""stack of 3 LSTM
layers of 768 cells, each followed by a projection to 256 dimensions. The final embedding is created
by L2-normalizing the output of the top layer at the final frame"" . I didnot found ""activation function"". Did I miss anything? 

@CorentinJ  I see ""https://github.com/HarryVolek/PyTorch_Speaker_Verification/blob/master/speech_embedder_net.py"" and ""https://github.com/Janghyun1230/Speaker_Verification/blob/master/model.py"", they both didnot use any activation function after projection , right ?

And, why did you use ""torch.optim.Adam"" instead SGD",train model scratch train model result model speech result image utterance good way activation function paper stack projection final output top layer final frame found activation function miss anything see use activation function projection right use instead,issue,negative,positive,positive,positive,positive,positive
561513360,"@sberryman Can you share some images generated by the tools ""Resemblyzer"", like follows.
I downloaded the pretrained model offered by CorentinJ, and finetune with chinese corpus (5000 speakers) with lr-0.00001, the the embedding seems not very good even though the loss becomes to 0.005.  Can you share your result for reference?
",share like model corpus good even though loss becomes share result reference,issue,positive,positive,positive,positive,positive,positive
561496241,"Fixed this problem by building a new system, now there's other problems.

Boy it's tough making this software run. ",fixed problem building new system boy tough making run,issue,negative,negative,neutral,neutral,negative,negative
560538225,"What do you mean? So now its working for you? (If yes, then what version of python are you using that makes it work for you?)",mean working yes version python work,issue,negative,negative,negative,negative,negative,negative
560242340,"Should I convert voice to text surely to perform that?
Or are there any method or technology?",convert voice text surely perform method technology,issue,negative,positive,positive,positive,positive,positive
560050321,"Thank you @adannup for your help. As it turns out, there is no libportaudio2 in the arch repository nor the AUR. I also tried to compile it myself, but that did not work either. From this point on I have switched over to windows 10.",thank help turn arch repository also tried compile work either point switched,issue,positive,neutral,neutral,neutral,neutral,neutral
559941359,Was waiting for someone to try this on Joker! ,waiting someone try joker,issue,negative,neutral,neutral,neutral,neutral,neutral
559928738,"> @CorentinJ @yaguangtang @ tail95 @zbloss @ HumanG33k我正在根据3100人的中文数据对编码器模型进行微调。我想知道如何判断微调是否还可以。在图0中，蓝线基于2100人，黄线基于3100人，现在已对其进行培训。
> 图0：
> ![图片](https://user-images.githubusercontent.com/40649244/63139015-3f446480-c00f-11e9-9ec2-3eadebd6023f.png)
> 
> 图1 ：（微调920k，从1565k到1610k步，基于2100人）
> ![图片](https://user-images.githubusercontent.com/40649244/63139038-5aaf6f80-c00f-11e9-85ca-f54fdd81431e.png)
> 
> 图2 ：（从3565人开始，以1565k到1610k的步幅微调45k）
> ![图片](https://user-images.githubusercontent.com/40649244/63139112-a8c47300-c00f-11e9-9097-fb65452df9fd.png)
> 
> 总的来说，我也知道如何进行步进操作。因为，我只知道一个接一个地训练合成器模型和声码器模式来判断效果。但这将花费很长时间。我的EER或损失如何？期待您的回复！


Hey brother, how do you get the time of the word in the audio?
Can you give me a contact？
",tail hey brother get time word audio give,issue,negative,neutral,neutral,neutral,neutral,neutral
559878437,"Here's what I have.
Anaconda installed in C:\
A Python 3.7.0 environment
Cuda 10.0
Cudnn for Cuda 10.0
Tensorflow gpu 1.14.0

Here's the full list of my installations when I run `pip list`
absl-py              0.8.1
astor                0.8.0
audioread            2.1.8
certifi              2019.9.11
cffi                 1.13.1
chardet              3.0.4
cycler               0.10.0
decorator            4.4.1
dill                 0.3.1.1
gast                 0.3.2
google-pasta         0.1.8
grpcio               1.25.0
h5py                 2.10.0
idna                 2.8
importlib-metadata   0.23
inflect              3.0.2
joblib               0.14.0
jsonpatch            1.24
jsonpointer          2.0
Keras-Applications   1.0.8
Keras-Preprocessing  1.1.0
kiwisolver           1.1.0
librosa              0.7.1
llvmlite             0.30.0
Markdown             3.1.1
matplotlib           3.1.1
mkl-fft              1.0.15
mkl-random           1.1.0
mkl-service          2.3.0
more-itertools       7.2.0
multiprocess         0.70.9
numba                0.46.0
numpy                1.17.4
olefile              0.46
Pillow               6.2.1
pip                  19.3.1
protobuf             3.10.0
pycparser            2.19
pyparsing            2.4.5
PyQt5                5.13.2
PyQt5-sip            12.7.0
python-dateutil      2.8.1
pyzmq                18.1.0
requests             2.22.0
resampy              0.2.2
scikit-learn         0.21.3
scipy                1.3.2
setuptools           41.6.0.post20191030
six                  1.13.0
sounddevice          0.3.14
SoundFile            0.10.2
tensorboard          1.14.0
tensorflow-estimator 1.14.0
tensorflow-gpu       1.14.0
termcolor            1.1.0
torch                1.3.1
torchfile            0.1.0
torchvision          0.4.2
tornado              6.0.3
tqdm                 4.38.0
umap-learn           0.3.10
Unidecode            1.1.1
urllib3              1.25.7
visdom               0.1.8.9
webrtcvad            2.0.10
websocket-client     0.56.0
Werkzeug             0.16.0
wheel                0.33.6
wincertstore         0.2
wrapt                1.11.2
zipp                 0.6.0
",anaconda python environment full list run pip list astor cycler decorator dill gast inflect markdown pillow pip post six torch tornado wheel,issue,negative,positive,positive,positive,positive,positive
557975932,"Re: cudnn version mismatch. It appears that torch package carries its own copy of cudnn library which happens to be found before the cudnn library which comes from cudnn package and it happens to be not the version that tensorflow is looking for.  
I've managed to work around that by removing `site-packages\torch\lib\off\cudnn64_7.dll` so there's only one cudnn variant that can be found in `Library\bin\cudnn64_7.dll`. Torch didn't seem to complain and tensorflow is happy now, too.",version mismatch torch package copy library found library come package version looking work around removing one variant found torch seem complain happy,issue,negative,positive,positive,positive,positive,positive
557940001,"Artem-B - Thanks for your post.  I fixed line 145 as shown below, now on to the next error. 
Fix: 
```
        if isinstance(str(path), six.string_types):
            warnings.warn('PySoundFile failed. Trying audioread instead.')
```

And now it's giving the warning on the next line: 
 warnings.warn('PySoundFile failed. Trying audioread instead.')
Caught exception: TypeError(""argument of type 'WindowsPath' is not iterable"")


So I changed next line as well: 
`     y, sr_native = __audioread_load(str(path), offset, duration, dtype)`

NOTE: I made the mistake of selecting a 30 minute mp3 file, so it takes a while for it to convert to mono, then do the resampling.  But finally got it working after 6 or more hours of tinkering with the installs. 

So adding some prints to audio.py can at least show what's happening: 

```
    if mono:
        print (""to_mono"")
        y = to_mono(y)

    if sr is not None:
        print (""starting resample function"")
        y = resample(y, sr_native, sr, res_type=res_type)
        print (""resampling done"")

```

",thanks post fixed line shown next error fix path trying instead giving warning next line trying instead caught exception argument type iterable next line well path offset duration note made mistake minute file convert mono finally got working least show happening mono print none print starting resample function resample print done,issue,negative,positive,neutral,neutral,positive,positive
557932377,"Looks to me like the readme.md5 is misleading.  It tells you to download the latest pretrained models, but the next step say to run `python demo_cli.py` ""without"" downloading any data sets.  But demo_cli.py needs the pretrained model. 

### Pretrained models
Download the latest [here](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models).

### Preliminary
Before you download any dataset, you can begin by testing your configuration with:

`python demo_cli.py`

If all tests pass, you're good to go.

",like misleading latest next step say run python without data need model latest preliminary begin testing configuration python pas good go,issue,negative,positive,positive,positive,positive,positive
557832625,"The problem is that this fallback path in librosa does not work due to the fact that path is `pathlib.WindowsPath`:  https://github.com/librosa/librosa/blob/master/librosa/core/audio.py#L145

Converting path to string allows file opening with audioread and it will use ffmpeg to load the audio.",problem fallback path work due fact path converting path string file opening use load audio,issue,negative,negative,negative,negative,negative,negative
557529732,"> @shawwn I've uploaded the models to my dropbox. The vocoder is still training and will be for another 24-48 hours. Please share whatever you end up making with them!
> 
> #### Encoder
> https://www.dropbox.com/s/xl2wr13nza10850/encoder.zip?dl=0
> 
> #### Synthesizer (Tacotron)
> https://www.dropbox.com/s/t7qk0aecpps7842/tacotron.zip?dl=0
> 
> #### Vocoder
> https://www.dropbox.com/s/bgzeaid0nuh7val/vocoder.zip?dl=0

@sberryman thanks a lot for the models. Could you share respective parameter settings as well? I mean the following three files: `encoder\params_model.py`, `synthesizer\hparams.py`, `vocoder\hparams.py`. You mentioned some of the parameters in the thread, but it's not clear which of them have to be applied when using these models. ",still training another please share whatever end making synthesizer thanks lot could share respective parameter well mean following three thread clear applied,issue,positive,negative,neutral,neutral,negative,negative
557408548,"i've been trying to figure out where the hard limit of 7 seconds comes from, demo_cli tries to stretch or jam everything into 7 seconds, and if you overload it, it just says random syllables with echo.

There's a section in the spectrum program that allows you to set the sizes and stuff but if you change it some library complains that the variables have the wrong values. I'm like 70% sure that all of the limitations of demo_cli.py are in the spectrum parameters and the libraries it uses to create the spectrum",trying figure hard limit come stretch jam everything overload random echo section spectrum program set size stuff change library wrong like sure spectrum create spectrum,issue,negative,negative,negative,negative,negative,negative
557286494,"Downgrading tensorflow to 1.13.1 worked for me. 
I also followed the instructions here: https://www.tensorflow.org/install/gpu#windows_setup
Make sure you add the right environment variables to the PATH.
With 1.14 versions and lower of tf, you need to install the CPU and GPU packages seperately.",worked also make sure add right environment path lower need install,issue,negative,positive,positive,positive,positive,positive
556953201,"> 6408



> @boltomli Take a look at this dataset (1505 hours, 6408 speakers, recorded on smartphones):
> https://www.datatang.com/webfront/opensource.html
> [Samples.zip](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/3413304/Samples.zip)
> Not sure if the quality is good enough for encoder training.



> maybe you can try other free chinese datasets: AISHELL-1 178h http://www.openslr.org/33/ ST-CMDS 500h http://www.openslr.org/38/ MAGICDATA Mandarin Chinese Read Speech Corpus 755h http://www.openslr.org/68/ | | george_cql | | george_cql@163.com | 签名由网易邮箱大师定制 On 11/21/2019 14:44，Lu Zhihe<notifications@github.com> wrote： 6408 @boltomli Take a look at this dataset (1505 hours, 6408 speakers, recorded on smartphones): https://www.datatang.com/webfront/opensource.html Samples.zip Not sure if the quality is good enough for encoder training. 404 The same as: ?? — You are receiving this because you commented. Reply to this email directly, view it on GitHub, or unsubscribe.

Thx, I will try.",take look sure quality good enough training maybe try free mandarin read speech corpus take look sure quality good enough training reply directly view try,issue,positive,positive,positive,positive,positive,positive
556947201,"maybe you can try other free chinese datasets:
                        AISHELL-1 178h  http://www.openslr.org/33/
                        ST-CMDS  500h   http://www.openslr.org/38/
                        MAGICDATA  Mandarin Chinese Read Speech Corpus    755h    http://www.openslr.org/68/


| |
george_cql
|
|
george_cql@163.com
|
签名由网易邮箱大师定制
On 11/21/2019 14:44，Lu Zhihe<notifications@github.com> wrote：

6408

@boltomli Take a look at this dataset (1505 hours, 6408 speakers, recorded on smartphones):
https://www.datatang.com/webfront/opensource.html
Samples.zip
Not sure if the quality is good enough for encoder training.

404
The same as:

??

—
You are receiving this because you commented.
Reply to this email directly, view it on GitHub, or unsubscribe.",maybe try free mandarin read speech corpus take look sure quality good enough training reply directly view,issue,positive,positive,positive,positive,positive,positive
556872184,"@CorentinJ Thank you very much for you quick reply, I will see the computation in the scripts carefully. ",thank much quick reply see computation carefully,issue,negative,positive,positive,positive,positive,positive
556510789,"Even in audio.py it says:
> **param fpath_or_wav:** either a filepath to an audio file (_many extensions are supported, not just .wav_), either the waveform as a numpy array of floats.

And yet...
Well, with ffmpeg and pydub one might alter the script, so it converts any mp3 to wav on the fly. It's a dirty work-around and not time efficient, though...",even param either audio file either array yet well one might alter script fly dirty time efficient though,issue,negative,positive,positive,positive,positive,positive
556406380,"> What pip command are people using to get pytorch 1.0.1?

I haven't tested it, yet, but give this a try:
`pip install torch==1.0.1 torchvision==0.2.2 -f https://download.pytorch.org/whl/cu100/torch_stable.html`",pip command people get tested yet give try pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
556398633,"Here is what worked for me so far:

**CUDA Toolkit 10.0 (Sept 2018)**
https://developer.nvidia.com/cuda-10.0-download-archive

**cuDNN v7.6.5 (November 5th, 2019), for CUDA 10.0** (you must create an account there to get it)
https://developer.nvidia.com/cudnn

**PyTorch, old enough for CUDA 10.0**
`pip install torch==1.2.0 torchvision==0.4.0 -f https://download.pytorch.org/whl/torch_stable.html`

But with the demo_toolbox.py I also get this convolution error, when I start the synthesizer + vocoder.
The demo_cli.py works fine and generates correct output, though.
I'm not really interested in the GUI as I will just build some web-interface around the CLI program. So it's fine for me...",worked far sept th must create account get old enough pip install also get convolution error start synthesizer work fine correct output though really interested build around program fine,issue,positive,positive,positive,positive,positive,positive
556387470,"With Tensorflow 1.14 make sure you have **CUDA 10.0**, not CUDA 10.1
Make sure the CUDA-path is listed in your PATH environment variable!

The next error will instruct you to install cuDNN. Make sure you take a version specifically for CUDA 10.0

Next up is PyTorch. The website doesn't directly offer older versions. Install via pip:
`pip install torch==1.2.0 torchvision==0.4.0 -f https://download.pytorch.org/whl/torch_stable.html`",make sure make sure listed path environment variable next error instruct install make sure take version specifically next directly offer older install via pip pip install,issue,negative,positive,positive,positive,positive,positive
556020652,"They are the same, just formulated differently. Note that there is a mistake in the GE2E paper, they've put an extra minus sign somewhere. ",differently note mistake gee paper put extra minus sign somewhere,issue,negative,negative,neutral,neutral,negative,negative
555549077,"I know the solution is to convert the files to a .wav file, but it takes up way more space; is there a way to remove that requirement of a .wav file?",know solution convert file way space way remove requirement file,issue,negative,neutral,neutral,neutral,neutral,neutral
555338407,"> Hi, I am having similar issue but during the installation in Linux Ubuntu:
> 
> ![image](https://user-images.githubusercontent.com/13651344/69082285-15507300-0a40-11ea-851a-f78f04ef94cb.png)
> and
> ![image](https://user-images.githubusercontent.com/13651344/69082316-25685280-0a40-11ea-81b7-63a33f0ca985.png)

I am facing same issue..please suggest how to resolve it.",hi similar issue installation image image facing issue please suggest resolve,issue,positive,neutral,neutral,neutral,neutral,neutral
555192309,With Tensorflow 1.14 I couldn't get CUDA10 to work. I already hyad CUDA10  9.0 installed so I installed tensorflow 1.10 and I could then get the test to pass.,could get work already could get test pas,issue,negative,neutral,neutral,neutral,neutral,neutral
555165938,"Hi, I am having similar issue but during the installation in Linux Ubuntu:

![image](https://user-images.githubusercontent.com/13651344/69082285-15507300-0a40-11ea-851a-f78f04ef94cb.png)
and
![image](https://user-images.githubusercontent.com/13651344/69082316-25685280-0a40-11ea-81b7-63a33f0ca985.png)
",hi similar issue installation image image,issue,negative,neutral,neutral,neutral,neutral,neutral
555042928,@WenjianDing I have not attempted to train the vocoder and synthesizer again so I have not solved the problem with all the voices sounding the same/female. I'm more focused on the encoder for speaker diarization.,train synthesizer problem sounding speaker,issue,negative,neutral,neutral,neutral,neutral,neutral
554968851,"Facing the same issue will performing""python encoder_preprocess.py [dataroot]"" as the data in VoxCeleb2 is in .m4a files
Also tried running this command ""sndfile-play video.m4a"" Give the same error : File contains data in an unknown format.",facing issue python data also tried running command give error file data unknown format,issue,negative,negative,neutral,neutral,negative,negative
554915653,"



> @shawwn - Have you tried the models yet? I was just doing some testing and every voice I tried to clone sounded the same. Wondering if you experienced the same? (They all sounded robotic and female)
> 
> My assumption is the synthesizer and vocoder didn't train properly as I'm able to cluster voices using the encoder.

@sberryman Hi, did you solve the problem that your vocoder model clone the sounds all female, I have meet the same problem?",tried yet testing every voice tried clone wondering experienced female assumption synthesizer train properly able cluster hi solve problem model clone female meet problem,issue,negative,positive,positive,positive,positive,positive
554793898,What pip command are people using to get pytorch 1.0.1?,pip command people get,issue,negative,neutral,neutral,neutral,neutral,neutral
554748592,"> > I have the same problem. After about 3 hours of trial and error I get the program to launch but tensorflow can't get the ""convolution"" algorithm.
> 
> I managed to get it running. Sanity saving guide:
> 
> * install python 3.7.4 (strictly greater than 3.7.0 (otherwise conda will fail to initialize)
> * create a venv; install conda through pip (you can't get the latest version directly)
> * install dependencies as mentioned in the README using pip (never try with conda (it will remove some that it considers to be either circular or deprecated (it does this by default and can't seem to be changed)
> * install strictly pytorch 1.0.1 (DON'T TRY NEWER VERSIONS AS THEY THROW AMBIGUOUS MISSING DLL ERRORS)
> * make sure to go through this when all else fails:
>   https://pytorch.org/docs/stable/notes/windows.html#import-error
> 
> and this
> 
> https://devtalk.nvidia.com/default/topic/1038737/cuda-setup-and-installation/windows-10-cuda-installation-failure-solved/
> 
> * always install pytorch as a wheel package through pip (conda installs DO NOT INSTALL vc and vc redistributables from
> 
> https://pytorch.org/get-started/previous-versions/
> 
> It took me longer than three hours; it boils down to educated brute force.
> 
> On a side note, make sure you have 25 GBs of RAM to spare otherwise you'll have wasted time.

Thank you so much for your help. I finally got it working even tough there are still other minor issues.
The settings that works for me is: PyTorch 1.0.1 with cuda 10.0 , and python 3.75.
I previously tried cuda 9.0, and the newer version of PyTorch, they didn't work at all !
For me, I have to use PyTorch 1.01. That's the only version that works.",problem trial error get program launch ca get convolution algorithm get running sanity saving guide install python strictly greater otherwise fail initialize create install pip ca get latest version directly install pip never try remove either circular default ca seem install strictly try throw ambiguous missing make sure go else always install wheel package pip install took longer three educated brute force side note make sure ram spare otherwise wasted time thank much help finally got working even tough still minor work python previously tried version work use version work,issue,negative,positive,neutral,neutral,positive,positive
554646510,"The 'demo_cli.py' only uses the ''preprocess_wav"" function, which is imported from the encoder.inference script and is defined in the encoder.audio script. It seems preprocessing functions for other file formats are not included yet.

TLDR: convert your files to .wav format to resolve ",function script defined script file included yet convert format resolve,issue,negative,neutral,neutral,neutral,neutral,neutral
554645048,"> How did you manage to get pytorch to work? I keep running into:
> 
> https://pytorch.org/docs/stable/notes/windows.html#import-error

managed to solve this by installing cuda 10.1 (had cuda 10.0, and VS2017 before), hope this helps
",manage get work keep running solve hope,issue,positive,neutral,neutral,neutral,neutral,neutral
554634196,"Same as above. I don't use Linux a lot and half the commands don't work, and it's not clear from the readme what to actually type in to use this software. ",use lot half work clear actually type use,issue,negative,negative,neutral,neutral,negative,negative
554633993,"> 
> 
> > I have the same problem. After about 3 hours of trial and error I get the program to launch but tensorflow can't get the ""convolution"" algorithm.
> 
> I managed to get it running. Sanity saving guide:
> 
>     * install python 3.7.4 (strictly greater than 3.7.0 (otherwise conda will fail to initialize)
> 
>     * create a venv; install conda through pip (you can't get the latest version directly)
> 
>     * install dependencies as mentioned in the README using pip (never try with conda (it will remove some that it considers to be either circular or deprecated (it does this by default and can't seem to be changed)
> 
>     * install strictly pytorch 1.0.1 (DON'T TRY NEWER VERSIONS AS THEY THROW AMBIGUOUS MISSING DLL ERRORS)
> 
>     * make sure to go through this when all else fails:
>       https://pytorch.org/docs/stable/notes/windows.html#import-error
> 
> 
> and this
> 
> https://devtalk.nvidia.com/default/topic/1038737/cuda-setup-and-installation/windows-10-cuda-installation-failure-solved/
> 
>     * always install pytorch as a wheel package through pip (conda installs DO NOT INSTALL vc and vc redistributables from
> 
> 
> https://pytorch.org/get-started/previous-versions/
> 
> It took me longer than three hours; it boils down to educated brute force.
> 
> On a side note, make sure you have 25 GBs of RAM to spare otherwise you'll have wasted time.

Could you go a bit more in depth on the conda installation and venv creation?
I've never dealt with either before.",problem trial error get program launch ca get convolution algorithm get running sanity saving guide install python strictly greater otherwise fail initialize create install pip ca get latest version directly install pip never try remove either circular default ca seem install strictly try throw ambiguous missing make sure go else always install wheel package pip install took longer three educated brute force side note make sure ram spare otherwise wasted time could go bit depth installation creation never dealt either,issue,negative,positive,positive,positive,positive,positive
554633066,Could anyone please write down all the terminal commands needed to get this started?,could anyone please write terminal get,issue,negative,neutral,neutral,neutral,neutral,neutral
554584213,After further experimentation I have concluded that it is most likely due to the speech outputs of lines being less than 5 seconds that causes the curious anomalies. ,experimentation likely due speech le curious,issue,negative,negative,neutral,neutral,negative,negative
554583610,"> On a side note, make sure you have 25 GBs of RAM to spare otherwise you'll have wasted time.

I'm not sure why you said that, I've been voice cloning just fine with 8 GBs of RAM.
Plus I also used Conda to install the packages to a functional state, including version 1.2 of Pytorch.
The main issue I dealt with was finding which version of tensorflow would work with my CPU, which turned out to be 1.14.",side note make sure ram spare otherwise wasted time sure said voice fine ram plus also used install functional state version main issue dealt finding version would work turned,issue,negative,positive,positive,positive,positive,positive
554580805,"> > > I have the same problem. After about 3 hours of trial and error I get the program to launch but tensorflow can't get the ""convolution"" algorithm.
> > 
> > 
> > Try tensorflow==1.13.1, worked for me to fix this error. #31
> 
> Why not tensorflow-gpu? It will default to the latest version in requirements.txt (1.14.0 (make sure you have cuda 10.0))

I had tensorflow-gpu as well, but needed tensorflow for some reason. Maybe incorrect configuration, tensorflow-gpu==1.13.1 may have also fixed it.",problem trial error get program launch ca get convolution algorithm try worked fix error default latest version make sure well reason maybe incorrect configuration may also fixed,issue,negative,positive,positive,positive,positive,positive
554547159,"> > I have the same problem. After about 3 hours of trial and error I get the program to launch but tensorflow can't get the ""convolution"" algorithm.
> 
> Try tensorflow==1.13.1, worked for me to fix this error. #31

Why not tensorflow-gpu? It will default to the latest version in requirements.txt (1.14.0 (make sure you have cuda 10.0)) ",problem trial error get program launch ca get convolution algorithm try worked fix error default latest version make sure,issue,negative,positive,positive,positive,positive,positive
554545938,"> I have the same problem. After about 3 hours of trial and error I get the program to launch but tensorflow can't get the ""convolution"" algorithm.

Try tensorflow==1.13.1, worked for me to fix this error. https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/31",problem trial error get program launch ca get convolution algorithm try worked fix error,issue,negative,neutral,neutral,neutral,neutral,neutral
554545251,"> I have the same problem. After about 3 hours of trial and error I get the program to launch but tensorflow can't get the ""convolution"" algorithm.

I managed to get it running. Sanity saving guide:

- install python 3.7.4 (strictly greater than 3.7.0 (otherwise conda will fail to initialize)
- create a venv; install conda through pip (you can't get the latest version directly)
- install dependencies as mentioned in the README using pip (never try with conda (it will remove some that it considers to be either circular or deprecated (it does this by default and can't seem to be changed)
- install strictly pytorch 1.0.1 (DON'T TRY NEWER VERSIONS AS THEY THROW AMBIGUOUS MISSING DLL ERRORS)
- make sure to go through this when all else fails:
https://pytorch.org/docs/stable/notes/windows.html#import-error

and this

https://devtalk.nvidia.com/default/topic/1038737/cuda-setup-and-installation/windows-10-cuda-installation-failure-solved/

- always install pytorch as a wheel package through pip (conda installs DO NOT INSTALL vc and vc redistributables from

https://pytorch.org/get-started/previous-versions/

It took me longer than three hours; it boils down to educated brute force.

On a side note, make sure you have 25 GBs of RAM to spare otherwise you'll have wasted time. ",problem trial error get program launch ca get convolution algorithm get running sanity saving guide install python strictly greater otherwise fail initialize create install pip ca get latest version directly install pip never try remove either circular default ca seem install strictly try throw ambiguous missing make sure go else always install wheel package pip install took longer three educated brute force side note make sure ram spare otherwise wasted time,issue,negative,positive,positive,positive,positive,positive
554525877,"I have the same problem. After about 3 hours of trial and error I get the program to launch but tensorflow can't get the ""convolution"" algorithm.",problem trial error get program launch ca get convolution algorithm,issue,negative,neutral,neutral,neutral,neutral,neutral
554509157,"have same problem 
![image](https://user-images.githubusercontent.com/23361821/68972315-b4455700-0815-11ea-8cef-f2152b2c72cf.png)

Interactive generation loop
Reference voice: enter an audio filepath of a voice to be cloned (mp3, wav, m4a, flac, ...):
F:\Users\SuperPC\Desktop\Real-Time-Voice-Cloning-master\test.mp3
Traceback (most recent call last):
  File ""demo_cli.py"", line 130, in <module>
    preprocessed_wav = encoder.preprocess_wav(in_fpath)
  File ""F:\Users\SuperPC\Desktop\Real-Time-Voice-Cloning-master\encoder\audio.py"", line 28, in preprocess_wav
    wav, source_sr = librosa.load(fpath_or_wav, sr=None)
  File ""C:\Users\nikkl\Anaconda3\lib\site-packages\librosa\core\audio.py"", line 149, in load
    six.reraise(*sys.exc_info())
  File ""C:\Users\nikkl\Anaconda3\lib\site-packages\six.py"", line 693, in reraise
    raise value
  File ""C:\Users\nikkl\Anaconda3\lib\site-packages\librosa\core\audio.py"", line 129, in load
    with sf.SoundFile(path) as sf_desc:
  File ""C:\Users\nikkl\Anaconda3\lib\site-packages\soundfile.py"", line 627, in __init__
    self._file = self._open(file, mode_int, closefd)
  File ""C:\Users\nikkl\Anaconda3\lib\site-packages\soundfile.py"", line 1182, in _open
    ""Error opening {0!r}: "".format(self.name))
  File ""C:\Users\nikkl\Anaconda3\lib\site-packages\soundfile.py"", line 1355, in _error_check
    raise RuntimeError(prefix + _ffi.string(err_str).decode('utf-8', 'replace'))
RuntimeError: Error opening 'F:\\Users\\SuperPC\\Desktop\\Real-Time-Voice-Cloning-master\\test.mp3': File contains data in an unknown format.",problem image interactive generation loop reference voice enter audio voice recent call last file line module file line file line load file line reraise raise value file line load path file line file file line error opening file line raise prefix error opening file data unknown format,issue,negative,negative,neutral,neutral,negative,negative
554452889,"How did you manage to get pytorch to work? I keep running into:

https://pytorch.org/docs/stable/notes/windows.html#import-error

",manage get work keep running,issue,negative,neutral,neutral,neutral,neutral,neutral
554439361,"Also from the dummy example, I noticed that the loader would select a speaker multiple times (e.g. say I indicated that I wanted four speakers in a batch) and then ten utterances per speaker, I noticed that the loader would sometimes give me a batch like the following:
**S1** - 10 utterances
S2 - 10 utterances
**S1** - 10 utterances
S3 - 10 utterances

Do you think batch configurations like these might lead to embeddings that do not actually represent the emotion or dissimilar embeddings to be generated for the same emotion?",also dummy example loader would select speaker multiple time say four batch ten per speaker loader would sometimes give batch like following think batch like might lead actually represent emotion dissimilar emotion,issue,positive,neutral,neutral,neutral,neutral,neutral
554393060,"I had this same problem, and installing different versions of the python packages didn't help. What eventually ended up working was this:

`sudo apt-get install python3-pycude nvidia-driver-435 nvidia-utils-435`

`sudo reboot`

Before I had been using the nvidia-384 driver, and I'm guessing that's what didn't play nice. Regardless, it does seem to start properly now.",problem different python help eventually ended working install driver guessing play nice regardless seem start properly,issue,positive,positive,positive,positive,positive,positive
554272520,I would be very interested for a french version; is there any dataset including french that is compatible with RTVC?,would interested version compatible,issue,negative,positive,positive,positive,positive,positive
554228910,"Yes, I have 4 GPUs. Do you mean that I need to run the program on a single GPU machine, otherwise there may be various errors, resulting in the program cannot run?",yes mean need run program single machine otherwise may various resulting program run,issue,negative,negative,negative,negative,negative,negative
554191437,"> I'm not sure if this is causing your issue, but this is the first thing that popped up in my head.
> 
> It might be that for the empty instances (if there are others that are properly populated), the melspectrograms generated were too short. In the encoder preprocess file, there is a condition to check if the length of the generated melspectrogram was less than the value of 'partials_n_frames', (the value of which I believe was set in the params_data file. If the spectrogram is too short, it is discarded. So that could possibly be your issue. I would try looking at the lengths of the generated spectrograms and comparing with the value of 'partials_n_frames' to see if this can be ruled out or not.

It is exactly what you said, thank you!",sure causing issue first thing head might empty properly short file condition check length le value value believe set file spectrogram short could possibly issue would try looking value see exactly said thank,issue,positive,positive,positive,positive,positive,positive
554176483,"> did somebody managed to get this working in Google Colab? Would really like to try but I don't have a gpu at home

I found this https://colab.research.google.com/github/ak9250/Real-Time-Voice-Cloning/blob/master/Real_Time_Voice_Cloning.ipynb",somebody get working would really like try home found,issue,negative,positive,positive,positive,positive,positive
554127898,"After further reading over the instructions in the toolbox, I suspect that this might be caused by my text lines not being 5-12 seconds in length. I'll have to do some further experimentation to confirm.",reading toolbox suspect might text length experimentation confirm,issue,negative,neutral,neutral,neutral,neutral,neutral
554091420,"Your GLaDOS examples illustrate the importance of trying a number of samples from the same voice.  For mysterious reasons, some samples give much better results than others.  Really one just needs to keep trying, and usually you'll eventually hit a really decent sample that gives nice synthesis results.  Shorter samples often work better than long ones in my experience thus far. Gaps and sibilant white noise happen when the text sample you're trying to synthesize is too short - too long and it can end up as high-speed babble.

Vocoder effects like GLaDOS don't translate though - you'll still just end up with a normalish sounding voice.",illustrate importance trying number voice mysterious give much better really one need keep trying usually eventually hit really decent sample nice synthesis shorter often work better long experience thus far sibilant white noise happen text sample trying synthesize short long end babble effect like translate though still end sounding voice,issue,positive,positive,positive,positive,positive,positive
554054939,"I had a similar issue (on Windows 7), except for me the error was
`    C:\Python37\include\pyconfig.h(59): fatal error C1083: Cannot open include file: 'io.h': No such file or directory`
 `error: command 'C:\\Program Files (x86)\\Microsoft VisualStudio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.23.28105\\bin\\HostX86\\x64\\cl.exe' failed with exit status 2`

To solve this, I had to install the Windows 10 SDK option from the Visual Studios build tools. 
I then ran the ""Developer Command Prompt"" under Visual Studios in 'All Programs' in the Start menu. In this command prompt I was able to pip install webrtcvad successfully.",similar issue except error fatal error open include file file directory error command exit status solve install option visual build ran developer command prompt visual start menu command prompt able pip install successfully,issue,negative,positive,positive,positive,positive,positive
553944661,did somebody managed to get this working in Google Colab? Would really like to try but I don't have a gpu at home,somebody get working would really like try home,issue,negative,positive,positive,positive,positive,positive
553941034,"I think people need to stop relying on human limitations like the inability to perfectly duplicate someone else's voice, and on the unavailability of technology to do certain things. Or if they do rely on it, they at least need to accept that they're taking a risk by doing so. Sure, it's better if people don't use stuff like this with bad intentions (or use anything with bad intentions, for that matter) but at least there's a silver lining in that the more people do, the more society realizes the risk they were taking. And hopefully the more things like that happen, the more people will get used to these types of technological barriers dissolving, and will stop thinking and acting like they somehow have a responsibility to keep said barriers in place, or thinking and acting like they're responsible for how other people use their work.

I loved how the whole Deepfakes controversy went, and I have nothing but respect for the Fakeapp developer for letting his software into the wild, instead of continuing to delay the inevitable. Same for you. :+1: ",think people need stop human like inability perfectly duplicate someone else voice unavailability technology certain rely least need accept taking risk sure better people use stuff like bad use anything bad matter least silver lining people society risk taking hopefully like happen people get used technological dissolving stop thinking acting like somehow responsibility keep said place thinking acting like responsible people use work whole controversy went nothing respect developer wild instead delay inevitable,issue,positive,positive,neutral,neutral,positive,positive
553930646," ImportError: Could not find 'cudart64_100.dll'.
To solve this install NVIDIA cuDNN, For the other problem follow what futurityab said.



",could find solve install problem follow said,issue,negative,neutral,neutral,neutral,neutral,neutral
553906657,"Hi Josh, you seem to have two issues which are not strictly related to this repository.
The first indicates that you are missing an enviroment path needed by Tensorflow;
_ImportError: Could not find 'cudart64_100.dll'._

The second issue indicates you are missing qt; check with a pip list to make sure all the requirements are properly installed.
_Failed to import any qt binding_

Hope it helps,
BR Fred",hi josh seem two strictly related repository first missing path could find second issue missing check pip list make sure properly import hope,issue,negative,positive,neutral,neutral,positive,positive
553823534,"I'm not sure if this  is causing your issue, but this is the first thing that popped up in my head. 

It might be that for the empty instances (if there are others that are properly populated), the melspectrograms generated were too short. In the encoder preprocess file, there is a condition to check if the length of the generated melspectrogram was less than the value of 'partials_n_frames', (the value of which I believe was set in the params_data file. If the spectrogram is too short, it is discarded. So that could possibly be your issue. I would try looking at the lengths of the generated spectrograms and comparing with the value of 'partials_n_frames' to see if this can be ruled out or not.",sure causing issue first thing head might empty properly short file condition check length le value value believe set file spectrogram short could possibly issue would try looking value see,issue,positive,positive,positive,positive,positive,positive
553794823,"if i put 7 words in, the file is 7 seconds long, with a long delay between each. if i put 120 words in, the file is 28 seconds long, but is completely unintelligible after 4 seconds. 40 words is also 28 seconds long but it stays intelligible for about 6-8 seconds. both of the latter text inputs speak MUCH faster. Is it the hdparms.py file that i need to spend more time looking at?

i tried editing spec = spec[0] to do the concatenate per the section above. 
`spec =  np.concatenate(specs, axis=1)`

admittedly i have no idea what i am doing nor how to proceed. Thanks for anyone's time.
",put file long long delay put file long completely unintelligible also long stay intelligible latter text speak much faster file need spend time looking tried spec spec concatenate per section spec spec admittedly idea proceed thanks anyone time,issue,negative,positive,neutral,neutral,positive,positive
553746170,"@UESTCgan  have you tried to train the tacotron model using the embedding created by the encoder model? Is the result good?  I trained the encoder part using Magic data(Chinese, 1017 speakers,  57w utterances, 755 hours), the loss like yours as showed in the picture before. Then I tried to train the tacotron model, using the same Magic data, cause I don't have 300+ hours TTS dataset, but the result is really bad.",tried train model model result good trained part magic data loss like picture tried train model magic data cause result really bad,issue,negative,positive,positive,positive,positive,positive
553738638,"> I got rid of all CUDA references in code and was able to run it on CPU
> Try [my repo](https://github.com/ByPort/Real-Time-Voice-Cloning) or take a look at [this commit](https://github.com/ByPort/Real-Time-Voice-Cloning/commit/31bbd88b0b1823a562f1bd419a2881b672bb639a)
> @genewitch @deepseek

I tried your repo, but returned same error. Need some special action?",got rid code able run try take look commit tried returned error need special action,issue,negative,positive,positive,positive,positive,positive
553714795,you could have the ai synthesize the words that you need replacing and replace them manually,could ai synthesize need replace manually,issue,negative,neutral,neutral,neutral,neutral,neutral
553663674,"I got rid of all CUDA references in code and was able to run it on CPU
Try [my repo](https://github.com/ByPort/Real-Time-Voice-Cloning) or take a look at [this commit](https://github.com/ByPort/Real-Time-Voice-Cloning/commit/31bbd88b0b1823a562f1bd419a2881b672bb639a)
@genewitch @deepseek ",got rid code able run try take look commit,issue,negative,positive,positive,positive,positive,positive
553642641,"I have the same problem, can you tell me what you did to solve this please ?",problem tell solve please,issue,negative,neutral,neutral,neutral,neutral,neutral
553561753,"> 
> 
> I had to manually edit the code to get it working

Can you do a PR / fork?",manually edit code get working fork,issue,negative,neutral,neutral,neutral,neutral,neutral
553538086,"@contractorwolf  For my own purposes I'm putting in filler words to make the generated sample up to the required minimum length.  This generally prevents silent gaps or hissy sibilant semigaps.  These are then easy to cut out in the audio file. 

IIRC the actual training data was a female voice, so it would make sense that females are generated more faithfully.",filler make sample minimum length generally silent sibilant easy cut audio file actual training data female voice would make sense faithfully,issue,positive,positive,positive,positive,positive,positive
553439905,I had to manually edit the code to get it working,manually edit code get working,issue,negative,neutral,neutral,neutral,neutral,neutral
553414332,"Technically it would be easier, but the actor has imparted a lot of tone and intent in such VO that no synthesizer is close to capable of inferring and repeating at this point. Being able to replace individual wors or small combinations within an existing file would be the perfect balance of generated voice and artistic intention.",technically would easier actor lot tone intent synthesizer close capable point able replace individual small within file would perfect balance voice artistic intention,issue,positive,positive,positive,positive,positive,positive
553404638,It is easier to resynthesize the whole sentence.,easier resynthesize whole sentence,issue,negative,positive,positive,positive,positive,positive
553149512,"> Were you able to fix this?

Yea, I accidentally installed 3.8.",able fix yea accidentally,issue,negative,positive,positive,positive,positive,positive
551369194,"Another problem is the ReLU activation function. It seems that the embedding vector is always larger than or equal to 0, which is not common in the face recognition (since their loss func are similar) from my point of view. Although ReLU can mitigate the overfit problem, our embedding is represented in a limited subspace of the original vector space. I know this setting is from the paper but maybe we could give other activation function a try, such as Leaky ReLU. ",another problem activation function vector always equal common face recognition since loss similar point view although mitigate overfit problem limited subspace original vector space know setting paper maybe could give activation function try leaky,issue,negative,positive,neutral,neutral,positive,positive
551367900,"@vezzick Actually I use hidden size of 768 and mel bins of 80 to train the model. My num of speakers per batch is 64 and num of utterances is 10, which is a bit small. In the training time, IO seems a problem and my GPU is not fully utilized. Also I train the model on a RTX card and use FP16 to accelerate the training process. ",actually use hidden size mel train model per batch bit small training time io problem fully also train model card use accelerate training process,issue,negative,negative,negative,negative,negative,negative
551364033,"> @CorentinJ But the similarity between utterances are not good, which means this encoder cannot match two utterances from the same speaker. Won't that be a problem?

They're good enough, look at the medians. It's not a major issue that the two distributions overlap a bit.

> A bit of a tangent but what setup are you doing the training on and what hyperparameters? That could be affecting your training, having a bit of trouble getting the right setup myself.

You might find answers [here](https://github.com/resemble-ai/Resemblyzer/issues/13)",similarity good match two speaker wo problem good enough look major issue two overlap bit bit tangent setup training could affecting training bit trouble getting right setup might find,issue,negative,positive,positive,positive,positive,positive
551358421,"A bit of a tangent but what setup are you doing the training on and what hyperparameters? That could be affecting your training, having a bit of trouble getting the right setup myself.",bit tangent setup training could affecting training bit trouble getting right setup,issue,negative,positive,neutral,neutral,positive,positive
551345762,"@CorentinJ But the similarity between utterances are not good, which means this encoder cannot match two utterances from the same speaker. Won't that be a problem? ",similarity good match two speaker wo problem,issue,negative,positive,positive,positive,positive,positive
551343412,"Your scores are decent, don't worry. If you can include more data it would definitely improve.",decent worry include data would definitely improve,issue,negative,positive,neutral,neutral,positive,positive
551340633,"Hi guys, I try to train the encoder on a mandarin dataset and encounter a problem. Can you guys take a look at this? `https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/192`",hi try train mandarin encounter problem take look,issue,negative,neutral,neutral,neutral,neutral,neutral
549232332,"> 
> 
> You'll need a good dataset (at least ~300 hours, high quality and transcripts) in the language of your choice, do you have that?

So these 2 - language and voice - are different and I do not need only from one person ~>300h to synthezise the voice, correct?
Steps would be:
1) Train a certain language with (>300h audio + transcripts)
2) Create the voice with it

What GPU(s) are you guys using?",need good least high quality language choice language voice different need one person voice correct would train certain language audio create voice,issue,positive,positive,positive,positive,positive,positive
549200209,"> Hello, how did you fix this? I'm testing it from and hour but it stay stuck here with your same error.

Well, I just check the dataset. Some datas are not audios,just delet it. Then everything is fine.",hello fix testing hour stay stuck error well check everything fine,issue,negative,positive,positive,positive,positive,positive
549162777,"Hello, how did you fix this? I'm testing it from and hour but it stay stuck here with your same error.",hello fix testing hour stay stuck error,issue,negative,neutral,neutral,neutral,neutral,neutral
549158622,"I saved the file by adding something like this to the end of ""def vocode(self):"" in __init__.py in toolbox:

```
        fpath = ""demo_output_toolbox.wav""
        librosa.output.write_wav(fpath, wav.astype(np.float32), 
                                    Synthesizer.sample_rate)
        print(""\nSaved output as %s\n\n"" % fpath)
```

(and import librosa)",saved file something like end self toolbox print output import,issue,positive,neutral,neutral,neutral,neutral,neutral
549157534,"Although still might be useful for others to see what versions to use, I found the problem was that pytorch installed a version of cudnn separately from installed on my computer, so had to replace that cudnn dll with the proper version.

I got it working with:
tensorflow-gpu 1.14.0
cuda: 10.0
cudnn: 7.4.2",although still might useful see use found problem version separately computer replace proper version got working,issue,negative,positive,positive,positive,positive,positive
548803152,how did you get this to work?? i cant even install the software :( ,get work cant even install,issue,negative,neutral,neutral,neutral,neutral,neutral
548422235,"@CorentinJ you are under-estimating your-self.
If you deem MelGAN as vastly superior, you can consider implementing it.
I truly can see your repository reaching the levels of [descript](https://youtu.be/Maz0TVyTwiw?t=121)

reopened for good luck ;)",deem vastly superior consider truly see repository reaching descript good luck,issue,positive,positive,positive,positive,positive,positive
548021154,"- MelGAN is more recent and has shown to give promising results. 
- It's also faster to execute.
- The codebase is probably not a zombie mix between pytorch and tensorflow.
- It is actively maintained.",recent shown give promising also faster execute probably zombie mix actively,issue,positive,positive,neutral,neutral,positive,positive
547882741,"The same issue here on linux  ubuntu 18.4:

### Collecting webrtcvad
  Using cached https://files.pythonhosted.org/packages/89/34/e2de2d97f3288512b9ea56f92e7452f8207eb5a0096500badf9dfd48f5e6/webrtcvad-2.0.10.tar.gz
Building wheels for collected packages: webrtcvad
  Building wheel for webrtcvad (setup.py) ... error
  ERROR: Command errored out with exit status 1:
   command: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-j_8ezvc3/webrtcvad/setup.py'""'""'; __file__='""'""'/tmp/pip-install-j_8ezvc3/webrtcvad/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-ydzv_bvg --python-tag cp37
       cwd: /tmp/pip-install-j_8ezvc3/webrtcvad/
  Complete output (20 lines):
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build/lib.linux-x86_64-3.7
  copying webrtcvad.py -> build/lib.linux-x86_64-3.7
  running build_ext
  building '_webrtcvad' extension
  creating build/temp.linux-x86_64-3.7
  creating build/temp.linux-x86_64-3.7/cbits
  creating build/temp.linux-x86_64-3.7/cbits/webrtc
  creating build/temp.linux-x86_64-3.7/cbits/webrtc/common_audio
  creating build/temp.linux-x86_64-3.7/cbits/webrtc/common_audio/signal_processing
  creating build/temp.linux-x86_64-3.7/cbits/webrtc/common_audio/vad
  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-9u8fmL/python3.7-3.7.5=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DWEBRTC_POSIX -Icbits -I/usr/include/python3.7m -c cbits/pywebrtcvad.c -o build/temp.linux-x86_64-3.7/cbits/pywebrtcvad.o
  cbits/pywebrtcvad.c:1:10: fatal error: Python.h: No such file or directory
   #include <Python.h>
            ^~~~~~~~~~
  compilation terminated.
  error: command 'x86_64-linux-gnu-gcc' failed with exit status 1
  ----------------------------------------
  ERROR: Failed building wheel for webrtcvad
  Running setup.py clean for webrtcvad
Failed to build webrtcvad
Installing collected packages: webrtcvad
    Running setup.py install for webrtcvad ... error
    ERROR: Command errored out with exit status 1:
     command: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-j_8ezvc3/webrtcvad/setup.py'""'""'; __file__='""'""'/tmp/pip-install-j_8ezvc3/webrtcvad/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /tmp/pip-record-p6nl70v7/install-record.txt --single-version-externally-managed --compile
         cwd: /tmp/pip-install-j_8ezvc3/webrtcvad/
    Complete output (20 lines):
    running install
    running build
    running build_py
    creating build
    creating build/lib.linux-x86_64-3.7
    copying webrtcvad.py -> build/lib.linux-x86_64-3.7
    running build_ext
    building '_webrtcvad' extension
    creating build/temp.linux-x86_64-3.7
    creating build/temp.linux-x86_64-3.7/cbits
    creating build/temp.linux-x86_64-3.7/cbits/webrtc
    creating build/temp.linux-x86_64-3.7/cbits/webrtc/common_audio
    creating build/temp.linux-x86_64-3.7/cbits/webrtc/common_audio/signal_processing
    creating build/temp.linux-x86_64-3.7/cbits/webrtc/common_audio/vad
    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-9u8fmL/python3.7-3.7.5=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DWEBRTC_POSIX -Icbits -I/usr/include/python3.7m -c cbits/pywebrtcvad.c -o build/temp.linux-x86_64-3.7/cbits/pywebrtcvad.o
    cbits/pywebrtcvad.c:1:10: fatal error: Python.h: No such file or directory
     #include <Python.h>
              ^~~~~~~~~~
    compilation terminated.
    error: command 'x86_64-linux-gnu-gcc' failed with exit status 1
    ----------------------------------------
ERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-j_8ezvc3/webrtcvad/setup.py'""'""'; __file__='""'""'/tmp/pip-install-j_8ezvc3/webrtcvad/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /tmp/pip-record-p6nl70v7/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.
",issue building collected building wheel error error command exit status command open compile code complete output running running build running build running building extension fatal error file directory include compilation error command exit status error building wheel running clean build collected running install error error command exit status command open compile code install record compile complete output running install running build running build running building extension fatal error file directory include compilation error command exit status error command exit status open compile code install record compile check full command output,issue,negative,positive,positive,positive,positive,positive
547834694,"> torch-1.3.0+cu92-cp37-cp37m-win_amd64
> torchvision-0.4.1+cu92-cp37-cp37m-win_amd64

I think you install PyTorch for CUDA 9.2

Install PyTorch for CUDA 10
```
pip install https://download.pytorch.org/whl/cu100/torch-1.0.1-cp37-cp37m-win_amd64.whl
pip install torchvision
```
Also, it seems like you have installed an incompatible version of cuDNN. Let me know if you face any issues regarding that. ",think install install pip install pip install also like incompatible version let know face regarding,issue,negative,neutral,neutral,neutral,neutral,neutral
547360109,"Solved this by installing Visual Studio 2017, CUDA 10.0 and cuDNN. The demo_toolbox.py doesn't run in anaconda environment, so I installed all the dependency in the local python environment and everything works fine. ",visual studio run anaconda environment dependency local python environment everything work fine,issue,negative,positive,positive,positive,positive,positive
547358752,"As it says, you have an incompatible version of tensorflow-estimator. Remove tensorflow-estimator 1.14.0 and install tensorflow-estimator<2.1.0,>=2.0.0

Simply run `pip install ""tensorflow-estimator<2.1.0,>=2.0.0""`",incompatible version remove install simply run pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
546639625,"While running toolbox_cli.py, I get the response, No such file or directory",running get response file directory,issue,negative,neutral,neutral,neutral,neutral,neutral
546579616,"I keep getting this error


Reference voice: enter an audio filepath of a voice to be cloned (mp3, wav, m4a, flac, ...):
https://drive.google.com/open?id=11c9rw8-WEwtRdMMsdxTo0ClvoypuptYx
Caught exception: FileNotFoundError(2, 'No such file or directory')
Restarting
",keep getting error reference voice enter audio voice caught exception file directory,issue,negative,neutral,neutral,neutral,neutral,neutral
546569803,"Ive installed tensorflow several times, removing it each time
",several time removing time,issue,negative,neutral,neutral,neutral,neutral,neutral
546569726,"What am I doing wrong here?

Zachs-MacBook-Pro:Real-Time-Voice-Cloning zachhightower$ python3 demo_cli.py
Traceback (most recent call last):
  File ""demo_cli.py"", line 3, in <module>
    from synthesizer.inference import Synthesizer
  File ""/Users/zachhightower/Desktop/Real-Time-Voice-Cloning/synthesizer/inference.py"", line 1, in <module>
    from synthesizer.tacotron2 import Tacotron2
  File ""/Users/zachhightower/Desktop/Real-Time-Voice-Cloning/synthesizer/tacotron2.py"", line 3, in <module>
    from synthesizer.models import create_model
  File ""/Users/zachhightower/Desktop/Real-Time-Voice-Cloning/synthesizer/models/__init__.py"", line 1, in <module>
    from .tacotron import Tacotron
  File ""/Users/zachhightower/Desktop/Real-Time-Voice-Cloning/synthesizer/models/tacotron.py"", line 4, in <module>
    from synthesizer.models.helpers import TacoTrainingHelper, TacoTestHelper
  File ""/Users/zachhightower/Desktop/Real-Time-Voice-Cloning/synthesizer/models/helpers.py"", line 3, in <module>
    from tensorflow.contrib.seq2seq import Helper
ModuleNotFoundError: No module named 'tensorflow.contrib'
Zachs-MacBook-Pro:Real-Time-Voice-Cloning zachhightower$ 
",wrong python recent call last file line module import synthesizer file line module import file line module import file line module import file line module import file line module import helper module,issue,negative,negative,negative,negative,negative,negative
546393512,"I fixed the issue by running the script in a virtualenv. Unless I need something else, I think I'm good now. Thank you for all your help.",fixed issue running script unless need something else think good thank help,issue,positive,positive,positive,positive,positive,positive
546295706,@sberryman Thank you very much!!  It was my fault. I was training a modified version with a bug on the gradients. ,thank much fault training version bug,issue,negative,positive,positive,positive,positive,positive
546186402,"@CorentinJ , How much memory at least to run this project ? I want to know GPU memeroy  limit.
Is 8G GPU  ok ?",much memory least run project want know limit,issue,negative,negative,neutral,neutral,negative,negative
546172150,"Thank you so much for all your help, I'm now doing the command the way you're doing it, but I now have this:

Traceback (most recent call last):
  File ""encoder_train.py"", line 46, in <module>
    train(**vars(args))
  File ""D:\real-time-voice-cloning-master\encoder\train.py"", line 61, in train
    vis.log_dataset(dataset)
  File ""D:\real-time-voice-cloning-master\encoder\visualizations.py"", line 85, in log_dataset
    dataset_string += ""\n"" + dataset.get_logs()
  File ""D:\real-time-voice-cloning-master\encoder\data_objects\speaker_verification_dataset.py"", line 29, in get_logs
    with log_fpath.open(""r"") as log_file:
  File ""D:\anaconda\lib\pathlib.py"", line 1193, in open
    opener=self._opener)
  File ""D:\anaconda\lib\pathlib.py"", line 1046, in _opener
    return self._accessor.open(self, flags, mode)
PermissionError: [Errno 13] Permission denied: 'D:\\Voice_Datasets\\SV2TTS\\encoder\\project__sources.txt'
(base) PS D:\real-time-voice-cloning-master>

I realize this is a bit off from the main topic at this point, so if it needs to be closed I understand. I looked this up, and it might be a Windows problem or an Anaconda problem, which I use, but I don't know. Would you know anything about issues with this? Searching the issues here doesn't bring up anything.

Thanks a lot.",thank much help command way recent call last file line module train file line train file line file line file line open file line return self mode permission base realize bit main topic point need closed understand might problem anaconda problem use know would know anything searching bring anything thanks lot,issue,negative,negative,neutral,neutral,negative,negative
545401241,"Hi @CorentinJ Thank you for your replies. I have another two questions if you could kindly reply:

(i) Is the WaveRNN vocoder trained with GTA aligned mel-spec from tacotron2 or mel-specs generated from natural speech?

(ii) I see in the WaveRNN hyper-parameters, some of the default parameters are not used as it is used in fatchord/WaveRNN's repo such as 
voc_upsample_factors = (5, 5, 11) ==> voc_upsample_factors = (5, 5, 8) 
voc_mode = 'MOL'==> voc_mode = 'RAW'   etc,

Is there any particular reason? Did you finetune yourself depending on the performance of the output samples?",hi thank another two could kindly reply trained natural speech see default used used particular reason depending performance output,issue,positive,positive,positive,positive,positive,positive
545245222,"@CorentinJ , how many gpus were you using on this project?  How much memory does a GPU have?
How long it takes to train encoder and synthesizer and vocoder models?
 ",many project much memory long train synthesizer,issue,negative,positive,positive,positive,positive,positive
544660189,"The synthesizer is conditioned on all the speakers by using speaker embeddings, yes.",synthesizer conditioned speaker yes,issue,negative,neutral,neutral,neutral,neutral,neutral
544596251,"Hi @railsloes,

Based on the graphs above you are correct that the loss was around 0.8 by 10k steps. I used LibriTTS not the LibriSpeech dataset CorentinJ trained on. The difference being that the audio sample rate was 24 kHz in LibriTTS (along with a few other differences.)

I was **NOT** able to obtain alignment using the LibriTTS dataset while training the synthesizer.",hi based correct loss around used trained difference audio sample rate along able obtain alignment training synthesizer,issue,negative,positive,positive,positive,positive,positive
544529635,"Thanks for the quick answer. However, Synthesizer is conditioned on multiple speakers, Is that right?",thanks quick answer however synthesizer conditioned multiple right,issue,negative,positive,positive,positive,positive,positive
544525475,"The vocoder is not conditioned on any speaker identity, rather it is trained on all the data available from all the speakers. ",conditioned speaker identity rather trained data available,issue,negative,positive,positive,positive,positive,positive
544504802,"@sberryman I´m training a synthesizer on librispeech on the unmodified code from 0 and after 10k steps got loss around 10.
Yours seems to be around 0.8   Is it so? ",training synthesizer unmodified code got loss around around,issue,negative,neutral,neutral,neutral,neutral,neutral
544387554,I am trying to test on overlap in vocoder trainding. Hope it has any change.,trying test overlap hope change,issue,negative,neutral,neutral,neutral,neutral,neutral
544240872,"librosa version? 
if librosa <= 0.6.2 than update
the same with numpy
minimum numpy version 1.14.3",version update minimum version,issue,negative,neutral,neutral,neutral,neutral,neutral
544240396,"Do you also checked the folder where are the npy files are located?

I'm using the command ""python C:\work\Real-Time-Voice-CloningML\encoder_train.py myLang C:\work\zTraindata\encoder -m C:\work\zTraindata\saved_models""

In the folder ""C:\work\zTraindata\encoder"" the npy files are stored.

Check if one of this folder is empty.",also checked folder command python folder check one folder empty,issue,negative,negative,neutral,neutral,negative,negative
544205470,"I tried that, and no empty directories. I also then tried other methods of formatting my data source, but whenever I do that, I get a RandomCycler error like this one:

Traceback (most recent call last):
  File ""encoder_train.py"", line 46, in <module>
    train(**vars(args))
  File ""D:\real-time-voice-cloning-master\encoder\train.py"", line 68, in train
    for step, speaker_batch in enumerate(loader, init_step):
  File ""D:\anaconda\lib\site-packages\torch\utils\data\dataloader.py"", line 819, in __next__
    return self._process_data(data)
  File ""D:\anaconda\lib\site-packages\torch\utils\data\dataloader.py"", line 846, in _process_data
    data.reraise()
  File ""D:\anaconda\lib\site-packages\torch\_utils.py"", line 369, in reraise
    raise self.exc_type(msg)
Exception: Caught Exception in DataLoader worker process 0.
Original Traceback (most recent call last):
  File ""D:\anaconda\lib\site-packages\torch\utils\data\_utils\worker.py"", line 178, in _worker_loop
    data = fetcher.fetch(index)
  File ""D:\anaconda\lib\site-packages\torch\utils\data\_utils\fetch.py"", line 47, in fetch
    return self.collate_fn(data)
  File ""D:\real-time-voice-cloning-master\encoder\data_objects\speaker_verification_dataset.py"", line 55, in collate
    return SpeakerBatch(speakers, self.utterances_per_speaker, partials_n_frames)
  File ""D:\real-time-voice-cloning-master\encoder\data_objects\speaker_batch.py"", line 8, in __init__
    self.partials = {s: s.random_partial(utterances_per_speaker, n_frames) for s in speakers}
  File ""D:\real-time-voice-cloning-master\encoder\data_objects\speaker_batch.py"", line 8, in <dictcomp>
    self.partials = {s: s.random_partial(utterances_per_speaker, n_frames) for s in speakers}
  File ""D:\real-time-voice-cloning-master\encoder\data_objects\speaker.py"", line 34, in random_partial
    self._load_utterances()
  File ""D:\real-time-voice-cloning-master\encoder\data_objects\speaker.py"", line 18, in _load_utterances
    self.utterance_cycler = RandomCycler(self.utterances)
  File ""D:\real-time-voice-cloning-master\encoder\data_objects\random_cycler.py"", line 14, in __init__
    raise Exception(""Can't create RandomCycler from an empty collection"")
Exception: Can't create RandomCycler from an empty collection

I do know there is an issue with this error already, but I am not sure what the author meant when he found an 'illegal speaker folder', nor could I find anything that seemed to be illegal. Thanks again for your help.",tried empty also tried data source whenever get error like one recent call last file line module train file line train step enumerate loader file line return data file line file line reraise raise exception caught exception worker process original recent call last file line data index file line fetch return data file line collate return file line file line file line file line file line raise exception ca create empty collection exception ca create empty collection know issue error already sure author meant found speaker folder could find anything illegal thanks help,issue,negative,positive,neutral,neutral,positive,positive
543869359,"@Tiege95 sorry for the 2+ week delay, somehow I missed your message. Any chance you've written a script to download all the voice/speech files from Sounds Resource? I was looking through it today and definitely a lot of clean audio from game characters.

On a side note, I got the flu and decided to let the English model keep training while stuck in bed. That model is up to over 1.5 million steps now. (768/768 embedding/hidden size and 17,688 speakers) This model has been training for almost 28 days now.

Then for fun I decided to start training a 1024/1024 model with same 17,688 English speakers and the remaining 9,744 mix of other languages. With a single 1080 TI training the large embedding model it is taking quite a long time. Up to 379k steps over ~7 days of training. The graph isn't complete due to a 12+ hour power outage.

## English
![image](https://user-images.githubusercontent.com/324437/67117598-f38d7300-f197-11e9-91c4-92695e70944b.png)

## Mixed
![image](https://user-images.githubusercontent.com/324437/67117662-1d469a00-f198-11e9-88b7-749d33bf2841.png)

",sorry week delay somehow message chance written script resource looking today definitely lot clean audio game side note got flu decided let model keep training stuck bed model million size model training almost day fun decided start training model mix single ti training large model taking quite long time day training graph complete due hour power outage image mixed image,issue,negative,negative,neutral,neutral,negative,negative
543769781,"> Yes you would have to install PyTorch for this. You can download the appropriate version of PyTorch according to your CUDA version here: https://pytorch.org/get-started/previous-versions/
> 
> Note: CUDA 10.1 doesn't work with Tensorflow, you will have to install CUDA 10.0 for this to work

Thanks!",yes would install appropriate version according version note work install work thanks,issue,positive,positive,positive,positive,positive,positive
543600392,"> I wasn't satisfied with the example I gave (new link here https://drive.google.com/open?id=1qZAYTfYe0sUobaOVaYkHDz075FcNWJgy) so I spent the evening running tests with paired high and low quality samples. Since I try to follow the data, I now want to retract my response above. I do still think it's important for getting the best quality results, but it's not the defining factor. Frankly I'm still not sure what that factor is yet - I can't identify it by any spectral features or differences by ear, but the practical upshot is that some subsamples of a voice clone better than others. The key to getting a good voice synthesis seems to me to be testing a range of different samples from the same speaker and discovering the one which clones the best. Some samples which sound fine give garbage results, while others are much better.
> 
> When I mentioned training, I meant the voice you're trying to clone - the step of training the vocoder (via the alternative waveRNN model I believe, but I'm no ML specialist) from the sample you're using, which is the bit which interests me right now.
> 
> For reference, here's about a minute of audio of three different voices reading three different quotes - this is about as good as I'm getting at the moment: https://drive.google.com/open?id=1vqWj1XPJ2BcWTNKkAbwGji2344sNxLOd

Thanks for your reply. I think our main problem is how to clone the tone of reference speech(i mean the few seconds audio out of the training data) as much as possible.  I don't think the audio quality is the main factor leading to the bad performance for tone cloning. My first step is to check if our evaluation script mentioned above has problems or if the pretained model the author provided lead to  bad performance.",satisfied example gave new link spent evening running paired high low quality since try follow data want retract response still think important getting best quality factor frankly still sure factor yet ca identify spectral ear practical upshot voice clone better key getting good voice synthesis testing range different speaker one best sound fine give garbage much better training meant voice trying clone step training via alternative model believe specialist sample bit right reference minute audio three different reading three different good getting moment thanks reply think main problem clone tone reference speech mean audio training data much possible think audio quality main factor leading bad performance tone first step check evaluation script model author provided lead bad performance,issue,positive,positive,positive,positive,positive,positive
543577016,"Sorry if I misunderstood your question, do you mean how you can start synthesising speech? Launch demo_toolbox.py.
",sorry misunderstood question mean start speech launch,issue,negative,negative,negative,negative,negative,negative
543575184,"Yes you would have to install PyTorch for this. You can download the appropriate version of PyTorch according to your CUDA version here: https://pytorch.org/get-started/previous-versions/

Note: CUDA 10.1 doesn't work with Tensorflow, you will have to install CUDA 10.0 for this to work",yes would install appropriate version according version note work install work,issue,negative,positive,positive,positive,positive,positive
543492852,"look in encoder_preprocess.py

especially the option:
    parser.add_argument(""-o"", ""--out_dir"", type=Path, default=argparse.SUPPRESS, help=\
        ""Path to the output directory that will contain the mel spectrograms. If left out, ""
        ""defaults to <datasets_root>/SV2TTS/encoder/"")

If you didn't specify your output directory, create the folder SV2TTS/encoder manually or improve the source code ;)",look especially option path output directory contain mel left specify output directory create folder manually improve source code,issue,positive,neutral,neutral,neutral,neutral,neutral
543486837,"empty file/folder?

try the programm ""Remove empty directories"" from https://www.jonasjohn.de/red.htm and scan your data source where your wav-files located",empty try remove empty scan data source,issue,negative,negative,neutral,neutral,negative,negative
543484987,change to a batch size of 16 only means you'll need to train a longer time.,change batch size need train longer time,issue,negative,neutral,neutral,neutral,neutral,neutral
543167918,"I wasn't satisfied with the example I gave (new link here https://drive.google.com/open?id=1qZAYTfYe0sUobaOVaYkHDz075FcNWJgy) so I spent the evening running tests with paired high and low quality samples.  Since I try to follow the data, I now want to retract my response above. I do still think it's important for getting the best quality results, but it's not the defining factor.  Frankly I'm still not sure what that factor is yet - I can't identify it by any spectral features or differences by ear, but the practical  upshot is that some subsamples of a voice clone better than others.  The key to getting a good voice synthesis seems to me to be testing a range of different samples from the same speaker and discovering the one which clones the best.  Some samples which sound fine give garbage results, while others are much better.

When I mentioned training, I meant the voice you're trying to clone - the step of training the vocoder (via the alternative waveRNN model I believe, but I'm no ML specialist) from the sample you're using, which is the bit which interests me right now. 

For reference, here's about a minute of audio of three different voices reading three different quotes - this is about as good as I'm getting at the moment:  https://drive.google.com/open?id=1vqWj1XPJ2BcWTNKkAbwGji2344sNxLOd",satisfied example gave new link spent evening running paired high low quality since try follow data want retract response still think important getting best quality factor frankly still sure factor yet ca identify spectral ear practical upshot voice clone better key getting good voice synthesis testing range different speaker one best sound fine give garbage much better training meant voice trying clone step training via alternative model believe specialist sample bit right reference minute audio three different reading three different good getting moment,issue,positive,positive,positive,positive,positive,positive
543038204,"> Some of it seems to depend on the quality of the audio file you're using. If it's been compressed at any point for web distribution (youtube videos, mp3s etc) then you often get a choppy and messy sounding voice. Results are far better if you use the closest thing you have to an original and uncompressed wav file. YMMV but this has been my experience.
> 
> Added an example here with the same text, trained from identical 48KHz mp3 and then FLAC sources of the same dialog segment:
> 
> https://clyp.it/0msznbtd
> 
> You can hear considerably more wobble at the start of the first iteration. Restarted between takes. It's a bit subtle here but in the interest of science I'm using raw actual results. In practice I get consistently better results from high-quality sound sources. The characteristics of the voice and the speech used in the training samples matters a lot too.

Thanks for your notes. But i can't open this link. Could you provide any other links, such as google drive ? By the way, do you mean  you train the entire model in this github on your own high quality English dataset without any model modifications?",depend quality audio file compressed point web distribution often get choppy messy sounding voice far better use thing original uncompressed file experience added example text trained identical segment hear considerably wobble start first iteration bit subtle interest science raw actual practice get consistently better sound voice speech used training lot thanks ca open link could provide link drive way mean train entire model high quality without model,issue,positive,positive,neutral,neutral,positive,positive
542813690,Deprecating this response - since it's replicated below in hopes of keeping up s/n.,response since replicated keeping,issue,negative,neutral,neutral,neutral,neutral,neutral
542810180,"> EDIT: You might also have to manually find the link to PyTorch to match with your CUDA version since the homepage only listed 9.2 and 10.1. I'm using PyTorch 1.2.0 with CUDA 10.0 for your reference.
> https://pytorch.org/get-started/previous-versions/
I ended up installing Pytorch 1.2.0 with CUDA 10.0 and it solved my problem exactly.
Thank you kind sir
",edit might also manually find link match version since listed reference ended problem exactly thank kind sir,issue,negative,positive,positive,positive,positive,positive
542578707,"I think you installed CUDA 10.1, it is not supported by tensorflow yet. I had this same issue and solved it by installing CUDA 10.0 instead.

EDIT: You might also have to manually find the link to PyTorch to match with your CUDA version since the homepage only listed 9.2 and 10.1. I'm using PyTorch 1.2.0 with CUDA 10.0 for your reference.
https://pytorch.org/get-started/previous-versions/

",think yet issue instead edit might also manually find link match version since listed reference,issue,negative,neutral,neutral,neutral,neutral,neutral
541798824,mabe. it's a solustion that  split the textgrid and the corresponding audio into smaller slice that only have 2-3 seconds ,split corresponding audio smaller slice,issue,negative,neutral,neutral,neutral,neutral,neutral
541699331,"I read the pre-processing alignment of other languages you talked about before.。。。I want to train this  synthesizer Chinese model, I saw that it needs to be aligned and arranged in the format of the LibriSpeech folder. That's too much trouble. Can you tell me how to skip synthesizer_preprocess_audio.py, directly perform my synthesizer_preprocess_embeds.py operation on my .txt and .wav files, and then train?Thanks Very much.",read alignment want train synthesizer model saw need format folder much trouble tell skip directly perform operation train thanks much,issue,negative,positive,neutral,neutral,positive,positive
541482261,"Have you guys sovled this issue? I had this nan problem around the 1000th step.  Moreover, I found the loss is nan too",issue nan problem around th step moreover found loss nan,issue,negative,neutral,neutral,neutral,neutral,neutral
541419680,"@Stellarize how to check ? 
i have almost 200hours train data, when i get this error .",check almost train data get error,issue,negative,neutral,neutral,neutral,neutral,neutral
541419121,did u check if u don't get other process running in background?,check get process running background,issue,negative,neutral,neutral,neutral,neutral,neutral
541412568,"i have 2gpu, one is gtx1080, other is intel uhd 630, dose any set on h param to relieve the memory when not engouh ?",one dose set param relieve memory,issue,negative,neutral,neutral,neutral,neutral,neutral
541311416,"```
import argparse
import os
import re
import numpy as np
import soundfile as sf
from encoder import inference as encoder_infer
from synthesizer import inference as syn_infer
from encoder import audio as encoder_audio
from synthesizer import audio
from functools import partial
import pypinyin
from synthesizer.hparams import hparams


def run_eval_part1(args):
  speaker_enc_ckpt = args.speaker_encoder_checkpoint
  syn_ckpt = args.syn_checkpoint
  speaker_name = args.speaker_name
  eval_results_dir = os.path.join(args.eval_results_dir,
                                  speaker_name)
  if not os.path.exists(eval_results_dir):
    os.makedirs(eval_results_dir)
  speaker_audio_dirs = {
      ""speaker_name"": [""speaker_audio_1.wav"", ""speaker_audio_2.wav""],
      ""vctk_p225"": [""/home/zhangwenbo5/lihongfeng/corpus/vctk_dataset/wav16/p225/p225_001.wav"",
                    ""/home/zhangwenbo5/lihongfeng/corpus/vctk_dataset/wav16/p225/p225_002.wav"",
                    ""/home/zhangwenbo5/lihongfeng/corpus/vctk_dataset/wav16/p225/p225_003.wav"",
                    ""/home/zhangwenbo5/lihongfeng/corpus/vctk_dataset/wav16/p225/p225_004.wav"",
                    ""/home/zhangwenbo5/lihongfeng/corpus/vctk_dataset/wav16/p225/p225_005.wav"",
                    ],
      ""vctk_p226"": [""/home/zhangwenbo5/lihongfeng/corpus/vctk_dataset/wav16/p226/p226_001.wav"",
                    ""/home/zhangwenbo5/lihongfeng/corpus/vctk_dataset/wav16/p226/p226_002.wav"",
                    ""/home/zhangwenbo5/lihongfeng/corpus/vctk_dataset/wav16/p226/p226_003.wav"",
                    ""/home/zhangwenbo5/lihongfeng/corpus/vctk_dataset/wav16/p226/p226_004.wav"",
                    ""/home/zhangwenbo5/lihongfeng/corpus/vctk_dataset/wav16/p226/p226_005.wav"",
                    ],
      ""vctk_p227"": [""/home/zhangwenbo5/lihongfeng/corpus/vctk_dataset/wav16/p227/p227_001.wav"",
                    ""/home/zhangwenbo5/lihongfeng/corpus/vctk_dataset/wav16/p227/p227_002.wav"",
                    ""/home/zhangwenbo5/lihongfeng/corpus/vctk_dataset/wav16/p227/p227_003.wav"",
                    ""/home/zhangwenbo5/lihongfeng/corpus/vctk_dataset/wav16/p227/p227_004.wav"",
                    ""/home/zhangwenbo5/lihongfeng/corpus/vctk_dataset/wav16/p227/p227_005.wav"",
                    ],
      ""vctk_p228"": [""/home/zhangwenbo5/lihongfeng/corpus/vctk_dataset/wav16/p228/p228_001.wav"",
                    ""/home/zhangwenbo5/lihongfeng/corpus/vctk_dataset/wav16/p228/p228_002.wav"",
                    ""/home/zhangwenbo5/lihongfeng/corpus/vctk_dataset/wav16/p228/p228_003.wav"",
                    ""/home/zhangwenbo5/lihongfeng/corpus/vctk_dataset/wav16/p228/p228_004.wav"",
                    ""/home/zhangwenbo5/lihongfeng/corpus/vctk_dataset/wav16/p228/p228_005.wav"",
                    ],
      ""biaobei_speaker"": [""/home/zhangwenbo5/lihongfeng/corpus/BZNSYP/wavs/000001.wav"",
                          ""/home/zhangwenbo5/lihongfeng/corpus/BZNSYP/wavs/000002.wav"",
                          ""/home/zhangwenbo5/lihongfeng/corpus/BZNSYP/wavs/000003.wav"",
                          ""/home/zhangwenbo5/lihongfeng/corpus/BZNSYP/wavs/000004.wav"",
                          ""/home/zhangwenbo5/lihongfeng/corpus/BZNSYP/wavs/000005.wav"",
                          ""/home/zhangwenbo5/lihongfeng/corpus/BZNSYP/wavs/000006.wav"",
                          ""/home/zhangwenbo5/lihongfeng/corpus/BZNSYP/wavs/000007.wav"",
                          ],
      ""aishell_C0002"": [""/home/zhangwenbo5/lihongfeng/corpus/aishell2/data/wav/C0002/IC0002W0001.wav"",
                        ""/home/zhangwenbo5/lihongfeng/corpus/aishell2/data/wav/C0002/IC0002W0002.wav"",
                        ""/home/zhangwenbo5/lihongfeng/corpus/aishell2/data/wav/C0002/IC0002W0003.wav"",
                        ""/home/zhangwenbo5/lihongfeng/corpus/aishell2/data/wav/C0002/IC0002W0004.wav"", ],
      ""aishell_C0896"": [""/home/zhangwenbo5/lihongfeng/corpus/aishell2/data/wav/C0896/IC0896W0001.wav"",
                        ""/home/zhangwenbo5/lihongfeng/corpus/aishell2/data/wav/C0896/IC0896W0002.wav"",
                        ""/home/zhangwenbo5/lihongfeng/corpus/aishell2/data/wav/C0896/IC0896W0003.wav"",
                        ""/home/zhangwenbo5/lihongfeng/corpus/aishell2/data/wav/C0896/IC0896W0004.wav"", ],
  }[speaker_name]
  sentences = [
    ""THAT MATTER OF TROY AND ACHILLES WRATH ONE TWO THREE RATS"",
    ""ENDED THE QUEST OF THE HOLY GRAAL JERUSALEM A HANDFUL OF ASHES BLOWN BY THE WIND EXTINCT"",
    ""She can scoop these things into three red bags"",
    ""and we will go meet her Wednesday at the train station"",
    ""This was demonstrated in a laboratory experiment with rats.""
  ]

  sentences = [sen.upper() for sen in sentences]

  sentences.append(""This was demonstrated in a laboratory experiment with rats"")

  print('eval part1> model: %s.' % syn_ckpt)
  syner = syn_infer.Synthesizer(syn_ckpt)
  encoder_infer.load_model(speaker_enc_ckpt)

  ckpt_step = ""pretrained""

  speaker_audio_wav_list = [encoder_audio.preprocess_wav(wav_dir) for wav_dir in speaker_audio_dirs]
  speaker_audio_wav = np.concatenate(speaker_audio_wav_list)
  print(os.path.join(eval_results_dir, '%s-000_refer_speaker_audio.wav' % speaker_name))
  audio.save_wav(speaker_audio_wav, os.path.join(eval_results_dir, '%s-000_refer_speaker_audio.wav' % speaker_name),
                 hparams.sample_rate)
  speaker_embed = encoder_infer.embed_utterance(speaker_audio_wav)
  for i, text in enumerate(sentences):
    path = os.path.join(eval_results_dir,
                        ""%s-%s-eval-%03d.wav"" % (speaker_name, ckpt_step, i))
    print('[{:<10}]: {}'.format('processing', path))
    mel_spec = syner.synthesize_spectrograms([text], [speaker_embed])[
        0]  # batch synthesize
    print('[{:<10}]:'.format('text:'), text)
    # print(np.shape(mel_spec))
    wav = syner.griffin_lim(mel_spec)
    audio.save_wav(wav, path, hparams.sample_rate)


def main():
  os.environ['CUDA_VISIBLE_DEVICES']= '2'
  parser = argparse.ArgumentParser()
  parser.add_argument('syn_checkpoint',
                      # required=True,
                      help='Path to synthesizer model checkpoint.')
  parser.add_argument('speaker_name',
                      help='Path to target speaker audio.')
  parser.add_argument('--speaker_encoder_checkpoint', default='encoder/saved_models/pretrained.pt',
                      help='Path to speaker encoder nodel checkpoint.')
  parser.add_argument('--eval_results_dir', default='overall_eval_results',
                      help='Overall evaluation results will be saved here.')
  args = parser.parse_args()
  hparams.set_hparam(""tacotron_num_gpus"", 1) # set tacotron_num_gpus=1 to synthesizer single wav.
  run_eval_part1(args)


if __name__ == '__main__':
  os.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""2""
  main()



```",import import o import import import import inference synthesizer import inference import audio synthesizer import audio import partial import import matter troy wrath one two three ended quest holy handful ash blown wind extinct scoop three red go meet train station laboratory experiment sen laboratory experiment print part model print text enumerate path print path text batch synthesize print text print path main parser synthesizer model target speaker audio speaker evaluation saved set synthesizer single main,issue,positive,negative,neutral,neutral,negative,negative
538660020,"Try:
Go to the installation folder (In my case it is): C:\Program Files (x86)\Microsoft Visual C++ Build Tools
Open Visual C++ 2015 x86 x64 Cross Build Tools Command Prompt
Type: pip install package_name
",try go installation folder case visual build open visual cross build command prompt type pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
537863421,Just remove the import and replace the body of [that function](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/encoder/audio.py#L58-L98) with `return wav`,remove import replace body function return,issue,negative,neutral,neutral,neutral,neutral,neutral
537806610,"Thank you, @CorentinJ but that didn't work for me. You mean I can remove the webrtcvad code from your source? Which files would I have to edit for that?",thank work mean remove code source would edit,issue,negative,negative,negative,negative,negative,negative
537801829,"Yeah that package is a pain in the ass to install, which is the reason why I wanted to move to a different denoiser. I know this: https://forum.qt.io/topic/90839/lnk1158-cannot-run-rc-exe/4 solved it for me but I don't know if it will help you. You can definitely remove all the webrtcvad code if you really want to get there.",yeah package pain as install reason move different know know help definitely remove code really want get,issue,positive,positive,neutral,neutral,positive,positive
537801211,You won't manage to run it with your card,wo manage run card,issue,negative,neutral,neutral,neutral,neutral,neutral
537800332,"I doubt it, I guess the model is limited in that way. You would need retraining, maybe by selecting only British speakers. In the SV2TTS paper they trained on VCTK, which is a smaller British dataset IIRC that has a much better quality. You could try that.",doubt guess model limited way would need maybe paper trained smaller much better quality could try,issue,negative,positive,positive,positive,positive,positive
537799133,It's just there to pull voices to clone. You won't improve the quality with more samples.,pull clone wo improve quality,issue,negative,neutral,neutral,neutral,neutral,neutral
537322075,"The pretrained model is in the folder: D:\Real-Time-Voice-Cloning\encoder\saved_models\
?

And you open the cmd or powershell box with adminrights?",model folder open box,issue,negative,neutral,neutral,neutral,neutral,neutral
537257435,"@sberryman I'm currently unable to experiment with this program since I don't have a computer with the proper specs to run it, but I love reading up on this kind of stuff. I figured that The Sounds Resource, while it's not technically not a dataset made for machine learning applications, is a huge resource of voice recordings. The PC/Computer section alone has ~1000 games to download sounds from (Overwatch, Dragon Ball Xenoverse, Half-Life, etc.). Voice files usually just have the corresponding character's name or are listed under something like ""Cutscene Voices"".

",currently unable experiment program since computer proper spec run love reading kind stuff figured resource technically made machine learning huge resource voice section alone overwatch dragon ball voice usually corresponding character name listed something like,issue,positive,positive,positive,positive,positive,positive
537254975,@Tiege95 Thanks so much for sharing; I'll be checking these sources out this evening! Have you attempted to train a model? Would really like to hear others experience on what worked or didn't work.,thanks much evening train model would really like hear experience worked work,issue,positive,positive,positive,positive,positive,positive
537251204,"@sberryman Hey, I have an idea that might make it possible to add more voices for training. A possibly large, untapped ""dataset"" is voice files ripped from video games. [The Sounds Resource](https://www.sounds-resource.com/) is probably one of the largest repositories of video game sound effects. You can just specially look up files for character dialogue, most of which is clean audio recorded in a studio (this mainly applies to video games made within the last 20 years).

The only limitation is that these voice clips would only be useful for the encoder since they unfortunately have no alignments. The upside is that there's a large variety of speakers, accents, and even options for Japanese dialogue if the game is from Japan. Most games made in the English-speaking world probably fall under EFIGS (English, French, Italian, German, Spanish) if they have been localized in Europe, so there might be options for those languages as well.

AAA games have the largest amount of voice actors, so it may be of interest to look into games like Skyrim, Fallout 4, GTA V, etc. since there's a large amount of NPC character dialogue.

Also, here are some links that may be helpful for finding new datasets:
https://www.cmswire.com/digital-asset-management/9-voice-datasets-you-should-know-about/
https://towardsdatascience.com/a-data-lakes-worth-of-audio-datasets-b45b88cd4ad
https://lionbridge.ai/datasets/12-best-audio-datasets-for-machine-learning/
https://skymind.ai/wiki/open-datasets
https://voices18.github.io/

An interesting dataset that I found recently is [The Spoken Wikipedia Corpora](https://nats.gitlab.io/swc/).",hey idea might make possible add training possibly large untapped voice video resource probably one video game sound effect specially look character dialogue clean audio studio mainly video made within last limitation voice clip would useful since unfortunately upside large variety even dialogue game japan made world probably fall german might well amount voice may interest look like since large amount character dialogue also link may helpful finding new interesting found recently spoken corpus,issue,positive,positive,neutral,neutral,positive,positive
537022582,"Thanks.. I'm using an old NVIDIA FX580 card with minimal cores (pulled from an old system). Ubuntu sees the drivers, so no need to try and change them. I was working on this last evening but bagged it due to the late hour. I'll try again tonight. Any link to the correct CUDA driver? I'm not sure about v9 vs v10.",thanks old card minimal old system need try change working last evening bagged due late hour try tonight link correct driver sure,issue,positive,positive,neutral,neutral,positive,positive
536813725,"At the terminal start a python repl
`python`
then

```
import torch
torch.cuda.is_available()
```

It should return true. If it is false try the pytorch pages to trouble shoot how to get CUDA enabled on your VM.

[https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/ )",terminal start python python import torch return true false try trouble shoot get,issue,negative,negative,neutral,neutral,negative,negative
536321766,"Thanks for the feedback @ViktorAlm! I went back and double checked that I was using the correct encoder, synthesizer and vocoder for each path I'm training and they all sounded the same. It was only a quick test using `demo_cli.py`

What preprocessing did you do to help train the synthesizer?",thanks feedback went back double checked correct synthesizer path training quick test help train synthesizer,issue,positive,positive,positive,positive,positive,positive
536265465,"I've had similar problems with the voice when i've used the wrong encoder for the synthesizer. My test run where i only did a few steps on each model was able to produce different voices atleast in the direction of the encoded voice. I dont have physical or remote access to my machine atm so cant see exactly what you mean. 

I did some more data preprocesssing and my swedish model seems to work alot better on male voices now. Have not trained the vocoder yet on this run just tried the synthesizer on griffin-lim.",similar voice used wrong synthesizer test run model able produce different direction voice dont physical remote access machine cant see exactly mean data model work better male trained yet run tried synthesizer,issue,negative,positive,neutral,neutral,positive,positive
536209189,"@shawwn - Have you tried the models yet? I was just doing some testing and every voice I tried to clone sounded the same. Wondering if you experienced the same? (They all sounded robotic and female)

My assumption is the synthesizer and vocoder didn't train properly as I'm able to cluster voices using the encoder.",tried yet testing every voice tried clone wondering experienced female assumption synthesizer train properly able cluster,issue,negative,positive,positive,positive,positive,positive
536205108,When running `python D:\Real-Time-Voice-Cloning-master\demo_toolbox.py` it says `Exception: No encoder models found in encoder\saved_models` too.,running python exception found,issue,negative,neutral,neutral,neutral,neutral,neutral
536202967,"Maybe try `pip install syntheziser.tacotron2`
I had to install several modules myself too.",maybe try pip install install several,issue,negative,neutral,neutral,neutral,neutral,neutral
536193401,"I figured that part out... now I have to figure out CUDA compatible device error. Not sure how to solve this issue. I am running this with 8 GB RAM on a VirtualBox instance. If anyone has any links/advice on how to fix this final error with NVidia/Cuda, I'd be very appreciative. Thanks.",figured part figure compatible device error sure solve issue running ram instance anyone fix final error appreciative thanks,issue,positive,positive,positive,positive,positive,positive
536097366,"@shawwn I've uploaded the models to my dropbox. The vocoder is still training and will be for another 24-48 hours. Please share whatever you end up making with them!

#### Encoder
https://www.dropbox.com/s/xl2wr13nza10850/encoder.zip?dl=0

#### Synthesizer (Tacotron)
https://www.dropbox.com/s/t7qk0aecpps7842/tacotron.zip?dl=0

#### Vocoder
https://www.dropbox.com/s/bgzeaid0nuh7val/vocoder.zip?dl=0",still training another please share whatever end making synthesizer,issue,positive,neutral,neutral,neutral,neutral,neutral
536094006,"@sberryman Would you be willing to upload your current encoder, synth, and vocoder models? Even if it's not finished training yet, I'd like to experiment with them.

Bonus points if you upload the tensorboard logs too :)

The samples sound promising!",would willing current even finished training yet like experiment bonus sound promising,issue,positive,positive,positive,positive,positive,positive
535964442,"Sorry but I can't quite remember what the loss was like when I trained the models. You could try to continue the training with my model and see what gives. The raw of value of the loss itself doesn't hold much meaning until you manage to compare it to a baseline. 
",sorry ca quite remember loss like trained could try continue training model see raw value loss hold much meaning manage compare,issue,negative,negative,negative,negative,negative,negative
535936275,"@shawwn I've attached the mixed and English results. Personally the mixed sounds better but I'm not convinced this is a very good model as the loss is very high. Not sure if @CorentinJ has an opinion on the loss, maybe that is expected? I'm assuming the loss is quite high as the synthesizer never managed to align.

### Mixed
[mixed_330_331k.zip](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/3662873/mixed_330_331k.zip)

### English
[english_300_301k.zip](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/3662891/english_300_301k.zip)
",attached mixed personally mixed better convinced good model loss high sure opinion loss maybe assuming loss quite high synthesizer never align mixed,issue,positive,positive,positive,positive,positive,positive
535852835,"@sberryman I'd be interested in hearing more samples.

In my experience, target=16000 overlap=800 produces high quality pop-free audio. I used it to make Dr. Kleiner sing: https://www.reddit.com/r/HalfLife/comments/d2rzf0/deepfaked_dr_kleiner_sings_i_am_the_very_model_of/",interested hearing experience high quality audio used make sing,issue,negative,positive,positive,positive,positive,positive
535706835,I received the same error and got past it by installing pyqt5 using 'pip install pyqt5'.,received error got past install,issue,negative,negative,negative,negative,negative,negative
535579359,@frossi65 The Italian language is only used as part of the encoder training. I did NOT use Italian as part of the synthesizer or vocoder training. ,language used part training use part synthesizer training,issue,negative,neutral,neutral,neutral,neutral,neutral
535521364,"Loss is still high at 3.6495 on the english model and 3.6416 on mixed. However, the quality is improving quite a bit. There are a few examples of generated audio that sounds just as good if not better than the original.

Based on generated examples while training, both models (from my perspective) do a better job on male than female speakers. While some of the generated audio sounds excellent, there are quite a few that have artifacts (pops, high-pitched, static, etc).

If anyone wants to listen to more generated examples, I will be happy to share them.",loss still high model mixed however quality improving quite bit audio good better original based training perspective better job male female audio excellent quite static anyone listen happy share,issue,positive,positive,positive,positive,positive,positive
535020993,"Training update

## Synthesizer
I've stopped training synthesizers for both the English and mixed datasets.

* English: 364,000 steps
* Mixed: 324,000 steps

## Vocoder

Started training a vocoder for each of the synthesizer models using the default hyper parameters with the following overrides:

* n_fft=2048
* hop_size=300
* win_size=1200
* sample_rate=24000
* speaker_embedding_size=768
* voc_upsample_factors=(5, 5, 12)

#### Mixed
[mixed.zip](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/3652615/mixed.zip)

Stdout:
```shell
{| Epoch: 1 (1158/1158) | Loss: 4.6526 | 1.4 steps/s | Step: 1k | }
{| Epoch: 2 (1158/1158) | Loss: 4.1365 | 1.4 steps/s | Step: 2k | }
{| Epoch: 3 (1158/1158) | Loss: 4.0376 | 1.4 steps/s | Step: 3k | }
...
{| Epoch: 75 (1158/1158) | Loss: 3.6903 | 1.4 steps/s | Step: 86k | }
{| Epoch: 76 (1158/1158) | Loss: 3.6877 | 1.4 steps/s | Step: 88k | }
{| Epoch: 77 (1158/1158) | Loss: 3.6839 | 1.4 steps/s | Step: 89k | }
```

Included files:
* 1k_steps_1_target.wav
* 1k_steps_1_gen_batched_target8000_overlap400.wav
* 1k_steps_2_target.wav
* 1k_steps_2_gen_batched_target8000_overlap400.wav
* 89k_steps_3_target.wav
* 89k_steps_3_gen_batched_target8000_overlap400.wav
* 89k_steps_5_target.wav
* 89k_steps_5_gen_batched_target8000_overlap400.wav

#### English
[english.zip](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/3652649/english.zip)

Stdout:
```shell
{| Epoch: 1 (1808/1808) | Loss: 4.5359 | 1.4 steps/s | Step: 1k | }
{| Epoch: 2 (1808/1808) | Loss: 4.0721 | 1.4 steps/s | Step: 3k | }
{| Epoch: 3 (1808/1808) | Loss: 3.9830 | 1.4 steps/s | Step: 5k | }
...
{| Epoch: 30 (1808/1808) | Loss: 3.7225 | 1.4 steps/s | Step: 54k | }
{| Epoch: 31 (1808/1808) | Loss: 3.7228 | 1.4 steps/s | Step: 56k | }
{| Epoch: 32 (1808/1808) | Loss: 3.7173 | 1.4 steps/s | Step: 57k | }
```

Included files:
* 1k_steps_1_target.wav
* 1k_steps_1_gen_batched_target8000_overlap400.wav
* 1k_steps_2_target.wav
* 1k_steps_2_gen_batched_target8000_overlap400.wav
* 57k_steps_4_target.wav
* 57k_steps_4_gen_batched_target8000_overlap400.wav
* 57k_steps_5_target.wav
* 57k_steps_5_gen_batched_target8000_overlap400.wav

Overall I would say the vocoders are starting to sound okay and it appears they are working without the synthesizer aligning. According to the [Pretrained models](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models) you trained the vocoder for 428k steps. I'll let these two models train until a similar target number of steps.",training update synthesizer stopped training mixed mixed training synthesizer default hyper following mixed shell epoch loss step epoch loss step epoch loss step epoch loss step epoch loss step epoch loss step included shell epoch loss step epoch loss step epoch loss step epoch loss step epoch loss step epoch loss step included overall would say starting sound working without synthesizer according trained let two train similar target number,issue,negative,positive,neutral,neutral,positive,positive
534484647,"Synthesizer: Text → Spectrogram
Vocoder: Spectrogram → Waveform",synthesizer text spectrogram spectrogram,issue,negative,neutral,neutral,neutral,neutral,neutral
533884643,"Synthesizer training is ongoing but I'm running into the same issues @CorentinJ ran into with LibriTTS where it fails to align. Since I skipped splitting on silence and noise reduction code I guess I'm not too surprised. What I am wondering is what is the impact of failing to align? The spectrograms and the wav files generated while training are easily distinguishable.

Edit:
Since it is failing to align, is it worth training the vocoder or would you suggest I continue training the synthesizer for a few more days/week to see if it improves?

### Mixed language encoder
![step-192000-align](https://user-images.githubusercontent.com/324437/65389025-72f96500-dd06-11e9-81b7-3bf358da4fd3.png)
![step-192000-mel-spectrogram](https://user-images.githubusercontent.com/324437/65389026-72f96500-dd06-11e9-9e13-66cb0d7a8b4f.png)
![step-194000-align](https://user-images.githubusercontent.com/324437/65389027-72f96500-dd06-11e9-8145-e57b4f7de5e3.png)
![step-194000-mel-spectrogram](https://user-images.githubusercontent.com/324437/65389028-7391fb80-dd06-11e9-8003-8cecd245d26a.png)
![step-196000-align](https://user-images.githubusercontent.com/324437/65389029-7391fb80-dd06-11e9-89a6-164862e92946.png)
![step-196000-mel-spectrogram](https://user-images.githubusercontent.com/324437/65389030-7391fb80-dd06-11e9-9214-228c31a92673.png)


### English only encoder

![step-190000-align](https://user-images.githubusercontent.com/324437/65389038-8278ae00-dd06-11e9-981f-af33c7c7055d.png)
![step-190000-mel-spectrogram](https://user-images.githubusercontent.com/324437/65389039-83114480-dd06-11e9-8da3-ebfcf55bd5a1.png)
![step-192000-align](https://user-images.githubusercontent.com/324437/65389040-83114480-dd06-11e9-8350-3d844649d8d7.png)
![step-192000-mel-spectrogram](https://user-images.githubusercontent.com/324437/65389041-83114480-dd06-11e9-8346-96af2420cd49.png)
![step-194000-align](https://user-images.githubusercontent.com/324437/65389043-83114480-dd06-11e9-861d-551adc3f1f57.png)
![step-194000-mel-spectrogram](https://user-images.githubusercontent.com/324437/65389044-83114480-dd06-11e9-9c57-4167a6933594.png)",synthesizer training ongoing running ran align since splitting silence noise reduction code guess wondering impact failing align training easily distinguishable edit since failing align worth training would suggest continue training synthesizer see mixed language step align step step align step step align step step align step step align step step align step,issue,negative,positive,positive,positive,positive,positive
533557139,"> Got it. Thanks a lot~

Because of my aphasia

I'm interested in this cloned voice.

I've had this thing for a day, but it's too hard for me.

May I ask for your help?

The tone of Baidu translation hopes not to be misunderstood.",got thanks aphasia interested voice thing day hard may ask help tone translation misunderstood,issue,positive,positive,neutral,neutral,positive,positive
533557062,"> 得到它了。非常感谢〜

Because of my aphasia

I'm interested in this cloned voice.

I've had this thing for a day, but it's too hard for me.

May I ask for your help?

The tone of Baidu translation hopes not to be misunderstood.",aphasia interested voice thing day hard may ask help tone translation misunderstood,issue,negative,negative,neutral,neutral,negative,negative
533488126,"For the encoder yes, for the synthesizer I wouldn't recommend it. For the vocoder, probably.",yes synthesizer would recommend probably,issue,positive,neutral,neutral,neutral,neutral,neutral
533431435,Can I continue training in Chinese corpus using the pre-training model you provided? @CorentinJ ,continue training corpus model provided,issue,negative,neutral,neutral,neutral,neutral,neutral
533278784,"I'm not saying it's impossible, but a ground-truth trained WaveRNN will produce excellent audio from 80-channels mel spectrograms. I reckon the problem is rather on the end of synthesizer",saying impossible trained produce excellent audio mel reckon problem rather end synthesizer,issue,negative,positive,positive,positive,positive,positive
533126689,"Thanks @CorentinJ, somehow I've missed the pretrained Wiki page. FYI, I still plan on figuring out fatchord/WaveRNN but I wanted a baseline version using your codebase.

This is a fun exercise, thanks for your patience!",thanks somehow page still plan version fun exercise thanks patience,issue,positive,positive,positive,positive,positive,positive
533121679,"I don't know about tensorboard, I didn't use it back then. As for the number of steps, you can check the [pretrained models page](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models).",know use back number check page,issue,negative,neutral,neutral,neutral,neutral,neutral
533118643,"Training updates

## Encoder

I've stopped training both the mixed and English encoders, the mixed encoder reached just over 2.1 million steps with 27,432 speakers.

## Synthesizer

Since I'm using LibriTTS I had to make some changes to the code base. First I used Montreal forced aligner to come up with the alignments. Then I realized google already normalized the audio and removed the leading and trailing silence. So at this point I just skipped the alignment portion of preprocessing and use the original transcript (as opposed to the normalized which is also provided) with all punctuation and capitalization left in place. I know the English cleaner converts everything to lowercase though.

I started training last night across two GTX 1080 Ti's and GPU utilization bounces between 20% and 93%.

#### Overridden hparams:
* tacotron_num_gpus=2
* tacotron_batch_size=64
* sample_rate=24000
* win_size=1200
* hop_size=300
* n_fft=2048
* speaker_embedding_size=768
* rescale=False


### Training progress

#### TensorBoard
![192 168 7 171_6006_](https://user-images.githubusercontent.com/324437/65245041-ae92f580-daa0-11e9-8e40-8e09101098fe.png)
![image](https://user-images.githubusercontent.com/324437/65245770-1269ee00-daa2-11e9-95f8-0a45c642596f.png)
![image](https://user-images.githubusercontent.com/324437/65245787-1bf35600-daa2-11e9-90d3-9ac3bfba4b28.png)



#### Stdout
```shell
Step   27753 [1.664 sec/step, loss=0.68117, avg_loss=0.67622]
Step   27754 [1.690 sec/step, loss=0.64809, avg_loss=0.67585]
Step   27755 [1.687 sec/step, loss=0.68754, avg_loss=0.67603]
Step   27756 [1.686 sec/step, loss=0.67575, avg_loss=0.67593]
Step   27757 [1.675 sec/step, loss=0.65758, avg_loss=0.67573]
Step   27758 [1.684 sec/step, loss=0.66391, avg_loss=0.67550]
Step   27759 [1.687 sec/step, loss=0.66689, avg_loss=0.67528]
Step   27760 [1.710 sec/step, loss=0.66279, avg_loss=0.67525]
Step   27761 [1.681 sec/step, loss=0.69119, avg_loss=0.67565]
Step   27762 [1.679 sec/step, loss=0.67129, avg_loss=0.67552]
Step   27763 [1.677 sec/step, loss=0.69174, avg_loss=0.67563]
Step   27764 [1.693 sec/step, loss=0.65657, avg_loss=0.67544]
Step   27765 [1.692 sec/step, loss=0.66381, avg_loss=0.67518]
Step   27766 [1.672 sec/step, loss=0.70290, avg_loss=0.67546]
```

#### Plots
![step-22000-align](https://user-images.githubusercontent.com/324437/65245217-0af61500-daa1-11e9-889d-740961434c4f.png)
![step-22000-mel-spectrogram](https://user-images.githubusercontent.com/324437/65245219-0af61500-daa1-11e9-9b64-f8f54545b1e5.png)
![step-24000-align](https://user-images.githubusercontent.com/324437/65245221-0b8eab80-daa1-11e9-9e1a-ec50b0f5d94c.png)
![step-24000-mel-spectrogram](https://user-images.githubusercontent.com/324437/65245224-0b8eab80-daa1-11e9-8460-f06b5a3e442f.png)
![step-26000-align](https://user-images.githubusercontent.com/324437/65245225-0b8eab80-daa1-11e9-8167-5951a2980430.png)
![step-26000-mel-spectrogram](https://user-images.githubusercontent.com/324437/65245226-0b8eab80-daa1-11e9-85b1-c70cb9ef1db2.png)

#### WAVs
[wavs.zip](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/3631239/wavs.zip)

## Questions:
1. Is it normal for the `max_gradient_norm`, `stop_token_loss` and `regularization_loss` to be increasing? Basically, do the tensorboard plots look okay?
2. How many steps did you train the synthesizer?
3. How many steps did you train the vocoder?",training stopped training mixed mixed million synthesizer since make code base first used forced aligner come already audio removed leading trailing silence point alignment portion use original transcript opposed also provided punctuation capitalization left place know cleaner everything though training last night across two ti utilization training progress image image shell step step step step step step step step step step step step step step step align step step align step step align step normal increasing basically look many train synthesizer many train,issue,negative,positive,neutral,neutral,positive,positive
533036384,"> Training the encoder is interesting, but I'm not entirely convinced that the problem is the encoder. (Where ""problem"" is defined as ""the current model has a lot of trouble reproducing female voices accurately."")

It's not intuitive, I agree. However, this is clearly the conclusion the authors of the sv2tts paper reached. They argue that most of the ability to clone voices lies in the training of the encoder. They also clearly show that the framework has limitations (which we observe in this repo as well): 
> An additional limitation lies in the model’s inability to transfer accents. Given sufficient training data, this could be addressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally, we note that the model is also not able to completely isolate the speaker voice from the prosody of the reference audio, ...

If you give a listen to [their librispeech samples](https://google.github.io/tacotron/publications/speaker_adaptation/index.html), you will notice that as well.",training interesting entirely convinced problem problem defined current model lot trouble female accurately intuitive agree however clearly conclusion paper argue ability clone training also clearly show framework observe well additional limitation model inability transfer given sufficient training data could synthesizer independent speaker accent finally note model also able completely isolate speaker voice prosody reference audio give listen notice well,issue,negative,positive,positive,positive,positive,positive
532978595,"Training the encoder is interesting, but I'm not entirely convinced that the problem is the encoder. (Where ""problem"" is defined as ""the current model has a lot of trouble reproducing female voices accurately."")

Are we certain that for every possible human voice, there exists an embedding which allows tacotron2 to produce spectrograms indistinguishable from that voice?

If not, then it seems beneficial if tacotron2 were trained on the new diverse speech dataset in addition to the encoder.

For example, in my experiments it has seemed impossible to generate spectrograms with cartoon-style inflections: lots of expressive vocalizations, rapid pitch changes, and so on.

If that's how a speaker sounds normally, then it seems like it's impossible for the encoder to generate any latent vector that would cause tacotron2 to produce spectrograms that sound anything like the speaker.

Perhaps I am confused, but just to confirm: there are three separate things that need to be trained, right? The encoder, the synthesizer (text to spectrogram), and the vocoder (spectrogram to wav). This training process is focusing entirely on the encoder. How is the loss being calculated? If the loss is calculated in terms of ""tacotron2 is able to generate spectrograms that sound more like this speaker,"" then the training here will not have a huge impact on overall quality or diversity. The training would need to be done on the synth, *then* the encoder.

Do I have this backwards? Is it true that the encoder's final quality is bounded by the expressiveness of the synth? If that's correct, then the synth is what would benefit from the larger dataset.",training interesting entirely convinced problem problem defined current model lot trouble female accurately certain every possible human voice produce indistinguishable voice beneficial trained new diverse speech addition example impossible generate lot expressive rapid pitch speaker normally like impossible generate latent vector would cause produce sound anything like speaker perhaps confused confirm three separate need trained right synthesizer text spectrogram spectrogram training process entirely loss calculated loss calculated able generate sound like speaker training huge impact overall quality diversity training would need done backwards true final quality bounded expressiveness correct would benefit,issue,positive,positive,neutral,neutral,positive,positive
532946180,"An interesting feat would be to be able to train it to, for instance, go through youtube playlists with hundreds of videos in a specified language, could grab the audio stream using something like the youtube-dl project to a temp folder, use it for training and repeat with each video, will try to see if that is possible, amazing work!",interesting feat would able train instance go language could grab audio stream something like project temp folder use training repeat video try see possible amazing work,issue,positive,positive,positive,positive,positive,positive
532772385,"please, can anybody give me the right link and some instruction on how to use it?",please anybody give right link instruction use,issue,negative,positive,positive,positive,positive,positive
532400349,"Training is still progressing on the mixed and english models. This is just to update anyone if they are following this issue.

### Mixed
![image](https://user-images.githubusercontent.com/324437/65079205-7a5dee80-d953-11e9-83b6-5c5ecc144531.png)

### English
![image](https://user-images.githubusercontent.com/324437/65079232-877add80-d953-11e9-921e-abe695803f53.png)
",training still mixed update anyone following issue mixed image image,issue,negative,neutral,neutral,neutral,neutral,neutral
532107717,"audio format resulit in this error,   using ffmpeg to convet the audio format to ' .wav ' and try again",audio format error audio format try,issue,negative,neutral,neutral,neutral,neutral,neutral
532048640,For anyone wanting to run this without CUDA. It's possible by removing the Cuda codes. I have been successfully able to run it without CUDA. Will try to raise a PR if I get some time.,anyone wanting run without possible removing successfully able run without try raise get time,issue,negative,positive,positive,positive,positive,positive
531927195,Will it work on Recoded Phone Calls ? If worked I can provide million of hours recordings in Bangali Language.,work phone worked provide million language,issue,negative,neutral,neutral,neutral,neutral,neutral
531872205,"@Jessicamat777 multi-gpu training is **NOT** implemented. If you do implement it, can you please submit a pull-request to this repository so others can benefit?",training implement please submit repository benefit,issue,positive,neutral,neutral,neutral,neutral,neutral
531871561,"Thanks to reply me,

Can I use multiple GPUs to train encoder data, so as to connect and make it
one at the end ? If I can save time training like this .

Please let me know ?

On Mon, 16 Sep 2019, 21:08 Shaun Berryman, <notifications@github.com> wrote:

> @Jessicamat777 <https://github.com/Jessicamat777> the models I have
> uploaded to drop box are all for experimentation and I have NOT trained the
> synthesizer or vocoder on them yet. So they will be on little value unless
> you wanted to use them with CorentinJ's Resemblyzer.
>
> That being said, the models on dropbox are from the following datasets.
>
>    1. LibriTTS <https://ai.google/tools/datasets/libri-tts/>
>    (train-other-500)
>    2. VoxCeleb1 <http://www.robots.ox.ac.uk/~vgg/data/voxceleb/>
>    3. VoxCeleb2 <http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html>
>    4. OpenSLR <http://www.openslr.org/resources.php> (42-44, 61-66, 69-80)
>    5. VCTK <https://datashare.is.ed.ac.uk/handle/10283/2651>
>    6. Common Voice <https://voice.mozilla.org/en/datasets>
>
> A vast majority of the speakers are English. Based on a very tiny sampling
> against languages it has NOT been trained on, it doesn't appear the foreign
> speakers make much of a difference. That is most likely due to the
> unbalanced training set, I didn't make any effort to balance. Just wanted
> to see if it made a difference including foreign languages while training.
> Meaning the clusters for foreign languages are okay but nowhere near as
> well defined as English speakers.
>
> Look at this issue where I show how my model(s) perform against the one
> trained by CorentinJ on Swedish and Norwegian.
> resemble-ai/Resemblyzer#9
> <https://github.com/resemble-ai/Resemblyzer/issues/9>
>
> I haven't made an effort to train on Chinese but it shouldn't be difficult
> if you have enough data. CorentinJ has done a great job of documenting the
> training process and answering questions on what size dataset you would
> need to train from scratch.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/126?email_source=notifications&email_token=AM3TVRFPT46WSVI22P4M6UTQJ6SBBA5CNFSM4IUT3NSKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD6ZSEOQ#issuecomment-531833402>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AM3TVRCI2RBDG37GJFUC7LTQJ6SBBANCNFSM4IUT3NSA>
> .
>
",thanks reply use multiple train data connect make one end save time training like please let know mon wrote drop box experimentation trained synthesizer yet little value unless use said following common voice vast majority based tiny sampling trained appear foreign make much difference likely due unbalanced training set make effort balance see made difference foreign training meaning foreign nowhere near well defined look issue show model perform one trained made effort train difficult enough data done great job training process size would need train scratch reply directly view mute thread,issue,positive,negative,neutral,neutral,negative,negative
531849071,"Hey guys, does anyone have pretrained models for Chinese. I need it for my collage project . ",hey anyone need collage project,issue,negative,neutral,neutral,neutral,neutral,neutral
531833402,"@Jessicamat777 the models I have uploaded to drop box are all for experimentation and I have NOT trained the synthesizer or vocoder on them yet. So they will be of little value unless you wanted to use them with CorentinJ's Resemblyzer.

That being said, the models on dropbox are from the following datasets.

1. [LibriTTS](https://ai.google/tools/datasets/libri-tts/) (train-other-500)
1. [VoxCeleb1](http://www.robots.ox.ac.uk/~vgg/data/voxceleb/)
1. [VoxCeleb2](http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html)
1. [OpenSLR](http://www.openslr.org/resources.php) (42-44, 61-66, 69-80)
1. [VCTK](https://datashare.is.ed.ac.uk/handle/10283/2651)
1. [Common Voice](https://voice.mozilla.org/en/datasets)

A vast majority of the speakers are English. Based on a very tiny sampling against languages it has NOT been trained on, it doesn't appear the foreign speakers make much of a difference. That is most likely due to the unbalanced training set and extremely small number of speakers per additional language. I just wanted to see if it made a difference including foreign languages while training. Meaning the clusters for foreign languages are okay but nowhere near as well defined as English speakers. 

Look at this issue where I show how my model(s) perform against the one trained by CorentinJ on Swedish and Norwegian.
https://github.com/resemble-ai/Resemblyzer/issues/9

I haven't made an effort to train on Chinese but it shouldn't be difficult if you have enough data. CorentinJ has done a great job of documenting the training process and answering questions on what size dataset you would need to train from scratch.


",drop box experimentation trained synthesizer yet little value unless use said following common voice vast majority based tiny sampling trained appear foreign make much difference likely due unbalanced training set extremely small number per additional language see made difference foreign training meaning foreign nowhere near well defined look issue show model perform one trained made effort train difficult enough data done great job training process size would need train scratch,issue,negative,negative,neutral,neutral,negative,negative
531831999,I need Chinese pretrained models for project in grad school. Can you guide me on that ?,need project grad school guide,issue,negative,neutral,neutral,neutral,neutral,neutral
531826870,"Hi sberryman, can I know which language your trained model in dropbox.com supports on?",hi know language trained model,issue,negative,neutral,neutral,neutral,neutral,neutral
531699677,"That's not going to happen. It's a substantial amount of work for which I don't have the time nor the will. This project was never meant to be used as a standalone application. It is purely a work of research upon which I decided to add a UI for newcomers. 

I agree that if you're not familiar with all the environment, it might be difficult to get anything to work for the first time; but that's going to be true for about every deep learning repository out there. @vanpelt recently pushed a lot of changes to make it runnable in google collab, maybe you could try that instead: https://colab.research.google.com/drive/1ts-KRxOuuAHmb-AtjU1XTM_lIFOepAN2.",going happen substantial amount work time project never meant used application purely work research upon decided add agree familiar environment might difficult get anything work first time going true every deep learning repository recently lot make runnable maybe could try instead,issue,positive,positive,neutral,neutral,positive,positive
531492070,Had to uninstall torch and install  the right one for my CUDA version 10.1.,torch install right one version,issue,negative,positive,positive,positive,positive,positive
531491847,"Hi! I've also had a runtime error because of compute capability of my GPU GeForce 820M which is 2.1. It seems that the lowest compute capability accepted is 3.0. I don't know if it can help you but I'm looking up if there were a way to avoid this step.

RuntimeError: cuDNN error: CUDNN_STATUS_ARCH_MISMATCH",hi also error compute capability compute capability accepted know help looking way avoid step error,issue,negative,neutral,neutral,neutral,neutral,neutral
531481154,"Hi,

I have a 16 gb gpu and long sentences like around 4 lines work flawlessly.
I dont think it uses all the space though.

Thanks.

On Fri., Sep. 13, 2019, 21:57 projoy, <notifications@github.com> wrote:

> @CorentinJ <https://github.com/CorentinJ> By the way, what's the total
> GPU memory this code consume? I currently run on a 4gb GPU but CUDA out of
> memory?
>
> put short sentences and it will not to out of memory.
> I just have 1gb GPU, I use ""hello world"" to run, that's ok
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/11?email_source=notifications&email_token=ANC53LU6WA7WKGVW2F6ZMOLQJRAKLA5CNFSM4HZFN7D2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD6WRMPI#issuecomment-531437117>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ANC53LVO25CIZ3ELT5SZRZ3QJRAKLANCNFSM4HZFN7DQ>
> .
>
",hi long like around work flawlessly dont think space though thanks wrote way total memory code consume currently run memory put short memory use hello world run reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
531437117,"> @CorentinJ By the way, what's the total GPU memory this code consume? I currently run on a 4gb GPU but CUDA out of memory?

put short sentences and it will not to out of memory.
I just have 1gb GPU, I use ""hello world"" to run, that's ok

",way total memory code consume currently run memory put short memory use hello world run,issue,negative,neutral,neutral,neutral,neutral,neutral
531412237,"Hi Guys, 

I resolved my issue. The issue is solved when I used PyQt5 version 5.11.3
Not sure what the difference is and also we need tensorflow version 2.0.0

Thanks for sharing this amazing tool Corentin.
",hi resolved issue issue used version sure difference also need version thanks amazing tool,issue,positive,positive,positive,positive,positive,positive
531350334,"@slavaGanzin I have pushed my work in progress to my own fork. There are hard coded paths and changes related to grouping all the .npy files into a single .npz for each speaker. I also use docker and volume mappings so I left the basic Dockerfile in there. I don't plan on ever submitting a PR for that branch as I'm still experimenting quite heavily.  Basically, feel free to use any of the scripts as a starting point but don't count on them working out of the box.

https://github.com/sberryman/Real-Time-Voice-Cloning/tree/wip

### Other updates
1. Mixed model with 256 hidden and 768 embedding size has finally hit 1,000,000 steps. Based on feedback from @CorentinJ I'm going to let that continue training for a while longer.
![image](https://user-images.githubusercontent.com/324437/64886896-15855a00-d61c-11e9-9f6d-e34413247dcb.png)

Model trained to 1,005,000 is available on my dropbox account now. https://www.dropbox.com/s/69wv21ajt6l2pag/cv_run_bak_1005000.pt?dl=0

2. English only model is progressing VERY slowly!
![image](https://user-images.githubusercontent.com/324437/64886856-08686b00-d61c-11e9-986a-e6b4cadd50cd.png)

",work progress fork hard related grouping single speaker also use docker volume left basic plan ever branch still quite heavily basically feel free use starting point count working box mixed model hidden size finally hit based feedback going let continue training longer image model trained available account model slowly image,issue,positive,negative,neutral,neutral,negative,negative
531206196,"@sberryman Shaun, would be awesome if you'll create PR. If you don't feel it's polished enough, just mark it WIP. So it wouldn't be merged, but will be just an inspiration for others :)",would awesome create feel polished enough mark would inspiration,issue,positive,positive,positive,positive,positive,positive
530996485,"Nah it's common for issues to serve a broader purpose than just solving bugs. I don't decay the learning rate simply because it's not a necessity with Adam. The original authors did not use Adam and they did decay the learning rate by the way. Also, you will have to read GE2E to know more about the speaker encoder, because there isn't much info in SV2TTS about how they train or evaluate it.",common serve purpose decay learning rate simply necessity original use decay learning rate way also read gee know speaker much train evaluate,issue,negative,positive,neutral,neutral,positive,positive
530980447,"Thanks @CorentinJ 

Well aware Resemblyzer is your project, that is how I ended up finding it. Thanks for open sourcing that project as well. Looking forward to seeing what your next project is!

Thanks for the test script, I was thinking about how I was going to evaluate the models I'm training and would be great to compare these to your public model. Originally I was just going to plot a random 5-10 utterances for every single speaker to get an idea of the overall distribution.

Interesting on not adjusting the learning rate; I'm more accustomed to training image classification models where reducing/decaying the learning rate is almost a requirement. I will not adjust the learning rate any further then.

I was not aware the SV2TTS authors trained for 50M steps, obviously it is time for me to read their paper.

Also, this is turning into more of a discussion than an ""issue"". I'm happy to move it to another location or can continue using GitHub issues; completely up to you.

Thanks again!",thanks well aware project ended finding thanks open project well looking forward seeing next project thanks test script thinking going evaluate training would great compare public model originally going plot random every single speaker get idea overall distribution interesting learning rate accustomed training image classification learning rate almost requirement adjust learning rate aware trained obviously time read paper also turning discussion issue happy move another location continue completely thanks,issue,positive,positive,positive,positive,positive,positive
530961218,"Also I don't know about that:

> I've reduced the learning rate from 1e-4 to 1e-5 on the mixed dataset which seems to help. I'll probably drop it down to 1e-6 around step 800-850k.

1. I've left my lr to 1e-4 all along, I think you should be fine with that same value as well

2. Don't forget that I never managed to fully train my speaker encoder. I trained it for 1M steps but the authors of sv2tts trained it for 50M steps. You should aim for more if you can.",also know reduced learning rate mixed help probably drop around step left along think fine value well forget never fully train speaker trained trained aim,issue,positive,positive,positive,positive,positive,positive
530960239,"I also would like to leave my script for evaluating the EER over the test set. It's not clean and I'm not sure if it's correct either (given that you won't find anywhere the right procedure to [evaluate the EER over a dataset](https://stats.stackexchange.com/questions/395345/computing-and-estimating-the-eer-on-an-entire-dataset)). You should use this if you want to formally evaluate the performance of the speaker encoder.

If someone manages to make it better then I would gladly include it in the repo

```
from encoder.data_objects import SpeakerVerificationDataLoader, SpeakerVerificationDataset
from encoder.model import SpeakerEncoder
from pathlib import Path
import matplotlib.pyplot as plt
import numpy as np
import torch


# This is my script for computing the test EER.
dataset_root = r""E:\Datasets\SV2TTS\encoder_test""

if __name__ == '__main__':
    speakers_per_batch = 32
    steps = 100
    
    dataset = SpeakerVerificationDataset(Path(dataset_root))
    
    model = SpeakerEncoder(torch.device(""cuda""), torch.device(""cpu""))
    checkpoint = torch.load(""saved_models/pretrained.pt"")
    model.load_state_dict(checkpoint[""model_state""])
    model.eval()
    
    results = []
    for utterances_per_speaker in range(6, 8):
        loader = SpeakerVerificationDataLoader(
            dataset,
            speakers_per_batch=speakers_per_batch,
            utterances_per_speaker=utterances_per_speaker,
            num_workers=8,
        )
        with torch.no_grad():
            eers = []
            for step, speaker_batch in zip(range(1, steps + 1), loader):
                inputs = torch.from_numpy(speaker_batch.data).cuda()
                embeds = model(inputs)
                embeds_loss = embeds.view((speakers_per_batch, utterances_per_speaker, -1)).cpu()
                _, eer = model.loss(embeds_loss)
                
                eers.append(eer)
                print(""Step %d    EER: %.3f"" % (step, np.mean(eers)))
        results.append(np.mean(eers))
        
    plt.plot(range(2, 11), results)
    plt.xlabel(""Enrollment utterances"")
    plt.ylabel(""Equal Error Rate"")
    plt.show()
```",also would like leave script eer test set clean sure correct either given wo find anywhere right procedure evaluate eer use want formally evaluate performance speaker someone make better would gladly include import import import path import import import torch script test eer path model range loader step zip range loader model eer eer print step eer step range enrollment equal error rate,issue,positive,positive,positive,positive,positive,positive
530956447,"Just in case this wasn't clear, Resemblyzer is also my project and is merely an interface to the speaker encoder of this repo. You can replace the pretrained model in the package and put yours instead. I could also distribute models that you provide me for other languages.",case clear also project merely interface speaker replace model package put instead could also distribute provide,issue,negative,negative,negative,negative,negative,negative
530940784,"Nice!

I edited my old comment because i did not want to clutter your thread with my bad screenshots. I added my converter with links to the datasets. Its very hacky and if you want to add them i really should clean up the code some. I think a simple merge of the folders and then looping through to get the spls(files with info on location etc) and loading the files would be the best way instead of my weird way of scanning the folders. I was testing on just one of the extracted folders and the speech folders did not contain the wavs which was specified in the spl file. Then everything went weird from there.

https://github.com/ViktorAlm/Nasjonalbank-converter",nice old comment want clutter thread bad added converter link hacky want add really clean code think simple merge looping get location loading would best way instead weird way scanning testing one extracted speech contain file everything went weird,issue,positive,positive,neutral,neutral,positive,positive
530938815,"@ViktorAlm Thanks for sharing!

Is your Swedish and Norwegian dataset private? I'm up for including those speakers in the next training run where I use 768 for hidden/embedding size if you can share. There are only 20 Swedish voices in the 25,668 speakers I am training on and zero Norwegian. Common voice had 44 speakers for Swedish but I filtered those down to 20 as I had a floor of 12 unique utterances per speaker.

### Other updates
1. I started the English only 768/768 training which takes significantly longer per step (about 4x) so don't expect those results for a while. Progress looks good so far though and it is only on step 8,100!
![image](https://user-images.githubusercontent.com/324437/64808270-bfe17c80-d54b-11e9-81e7-0a2355b1911a.png)
![image](https://user-images.githubusercontent.com/324437/64808274-c3750380-d54b-11e9-8343-527bcd41dc25.png)
![image](https://user-images.githubusercontent.com/324437/64808360-f3bca200-d54b-11e9-98f1-950b5990cf40.png)
![image](https://user-images.githubusercontent.com/324437/64808372-fcad7380-d54b-11e9-990d-499703739713.png)
2. I've reduced the learning rate from 1e-4 to 1e-5 on the mixed dataset which seems to help. I'll probably drop it down to 1e-6 around step 800-850k.

If anyone else is aware of other datasets I can include please let me know!
",thanks private next training run use size share training zero common voice floor unique per speaker training significantly longer per step expect progress good far though step image image image image reduced learning rate mixed help probably drop around step anyone else aware include please let know,issue,positive,positive,positive,positive,positive,positive
530897995,"Thanks! I have not changed any params. I was on step 150k with my data to try and do a real run with all the models. I did one where I only did 100k steps on each model with about 900 swedish speakers with about 90gb data in total. It did not clone the voice but produced a good audio quality and atleast a male voice came out when I ran my own voice. I paused it and did a quick test with yours and the encoding result is way better than the small testrun I did.

Swedish and Norwegian are pretty similar. I didnt see any specific Swedish/Norwegian cluster gathering but I only did two tests and umap might remove any visible difference I guess.

Heres a converter if you wish to add norwegian, danish and swedish data to your mix:
https://github.com/ViktorAlm/Nasjonalbank-converter

I also added some results from your encoder in /Results.

When i've played around a bit more i might make a script that evaluates different languages better.
",thanks step data try real run one model data total clone voice produced good audio quality male voice came ran voice quick test result way better small pretty similar didnt see specific cluster gathering two might remove visible difference guess converter wish add data mix also added around bit might make script different better,issue,positive,positive,positive,positive,positive,positive
530832810,"## Current:
I'm at ~700k steps and still quite a few tight clusters, not sure if this is due to the fact that I trained for 350k steps on 9,000 speakers prior to adding 16,668 more speakers (which also introduced quite a few more languages) I'm going to continue training for another 200k steps which will be done this time tomorrow morning.

![image](https://user-images.githubusercontent.com/324437/64787769-150a9780-d526-11e9-9ff5-60f146d2fb76.png)
![image](https://user-images.githubusercontent.com/324437/64787787-1936b500-d526-11e9-9787-c557222914a8.png)
![image](https://user-images.githubusercontent.com/324437/64787654-e4c2f900-d525-11e9-88b4-fb17fbbe392a.png)
![image](https://user-images.githubusercontent.com/324437/64787635-de348180-d525-11e9-9c62-dcfb84a51dcf.png)

## To-Do:
1. I'm going to start a new training for English only (there are a few non-english speakers) with ~17,680 speakers using 768/768 (hidden/embedding) size.
2. Once the mixed set reaches ~900k steps I will stop it and start it over from scratch with 768/768 as it is currently training on 256/768 (256 hidden and 768 embedding size) as I wasn't aware I had to bump both to 768.

## Comments

### @TheButlah 
First, thanks for the massive PR that landed on Fatchords WaveRNN 4 days ago, really excited you added mutli-gpu training and mel's in numpy format! To your question on a PR, I can certainly submit PRs to this repo and WaveRNN. The code to utilize most of the datasets from OpenSLR and Common Voice are bit of a hack but if people want them I'm open to working on a PR for that as well. 

Thanks for the feedback on Taco1 and WaveRNN from Fatchords repo, that will be the route I will go. I will most likely run into issues adding multi-speaker but I will start an issue in that repo when I get there.

### @ViktorAlm 
Great to hear about someone else testing multiple languages! Have you changed any of the data or model parameters? Funny you mentioned using [Resemble's diarization](https://github.com/resemble-ai/Resemblyzer) as I've had a tab open to that code for a few days and planned on using it against 7,000 hours of local (English) news video I have. That is once I finished training a new model.

As far as sharing the models I'm training, I'm open to it. Here is the model trained to 697,500 steps (768 model embedding size and 256 hidden layers.)
https://www.dropbox.com/s/2b5g2rt4vypx9qq/cv_run_bak_697500.pt?dl=0

Would be interested to know how it performs against your Swedish data @ViktorAlm.
",current still quite tight sure due fact trained prior also quite going continue training another done time tomorrow morning image image image image going start new training size mixed set stop start scratch currently training hidden size aware bump first thanks massive landed day ago really excited added training mel format question certainly submit code utilize common voice bit hack people want open working well thanks feedback route go likely run start issue get great hear someone else testing multiple data model funny resemble tab open code day local news video finished training new model far training open model trained model size hidden would interested know data,issue,positive,positive,neutral,neutral,positive,positive
530719061,I'm also very interested in the results. I'm currently training the encoder on about 2k speakers in Swedish and about 4k mixed mainly English. I would really like to see examples from your encoder model on multiple languages to see if its worth crawling radio and tv shows with resemblyzers diarization to create a a fully Swedish dataset or if 6k with 1/3 being Swedish can compare to 25k mixed mainly english for Swedish voice cloning. My hunch is m0ar data,also interested currently training mixed mainly would really like see model multiple see worth crawling radio create fully compare mixed mainly voice hunch mar data,issue,positive,positive,positive,positive,positive,positive
530600329,"@sberryman will you be submitting a pull request? Id be very interested to see the results using more data for the speaker encoder - the GE2E paper demonstrated that having more data for the encoder is critical to getting the similarity of the cloned speaker close to the original.

Also in my own experience, the compatibility of Fatchords Taco1 with WaveRNN makes it a great candidate, and the codebase is easy to work with. I still believe that Taco2 would be an upgrade in terms of quality of the inflection of the speaker, but that the out of the box compatibility of Fatchords synthesizer with the vocoder makes it a natural choice.

Do note that Fatchords synthesizer does not support multiple speakers, so you would need to add that capability yourself (and a PR on Fatchords repo would be especially appreciated for adding that capability :) )",pull request id interested see data speaker gee paper data critical getting similarity speaker close original also experience compatibility great candidate easy work still believe would upgrade quality inflection speaker box compatibility synthesizer natural choice note synthesizer support multiple would need add capability would especially capability,issue,positive,positive,positive,positive,positive,positive
530590670,"Nice! Thank you, I'll try it. For now, I'm just adapting my GPU to CUDA... but my GeForce 820M seems not to be really good for this. The issue is that the computing capability is 2.1 whereas tensorflow requires 3.5 at least. Do you think that I can go on even if this requirement is not satisfied?
",nice thank try really good issue capability whereas least think go even requirement satisfied,issue,positive,positive,positive,positive,positive,positive
530445547,"You are correct, it was not working until I changed the two lines to move the tensor to the GPU not the parameter. That was all I had to change (I believe, if not I can dig through all my changes and help you isolate that fix.) Technically I changed loss_device to `loss_device = device` just so I didn't miss anything in `train.py`. Either way, only one GPU is exposed to my docker container used for training.

Also in the sync function, I had to remove the device parameter and simply use `torch.cuda.synchronize()`

Clusters are getting tighter but I plan on training until at least 700-900k steps. I'm also tempted to train an English only model to compare.

",correct working two move tensor parameter change believe dig help isolate fix technically device miss anything either way one exposed docker container used training also sync function remove device parameter simply use getting plan training least also train model compare,issue,negative,negative,neutral,neutral,negative,negative
530416529,"Ah, I had put a warning not to compute the loss on GPU because for some reason it wasn't working (either it was some intricacies with torch or I forgot to enable grad on some tensor) and would return None. If that works, then I should update the repo to make it the default and have only 1 device for the encoder.",ah put warning compute loss reason working either torch forgot enable grad tensor would return none work update make default device,issue,negative,neutral,neutral,neutral,neutral,neutral
530411471,"Hi Corentin,

My pyqt version is 5.13.0.

I tried to install the pyqt version 5.12.2 but still the program wont run.
Not sure what the problem is.

Hope you can help.

Thanks

On Wed., Sep. 11, 2019, 03:59 Corentin Jemine, <notifications@github.com>
wrote:

> What's your pyqt version? You can try to install 5.12.2, the same as mine.
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/11?email_source=notifications&email_token=ANC53LQNU3DFEMWRZFAOMATQJCQNFA5CNFSM4HZFN7D2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD6NT6AQ#issuecomment-530267906>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ANC53LWY7IKIROVIQAQNFXDQJCQNFANCNFSM4HZFN7DQ>
> .
>
",hi version tried install version still program wont run sure problem hope help thanks wrote version try install mine reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
530369997,"Thanks for the continuous feedback.

2. Unfortunately I didn't have the patience to wait for your response so it has been training with the model and data parameters shown below.

```shell
## Model parameters:
learning_rate_init: 0.0001
model_embedding_size: 768
model_hidden_size: 256
model_num_layers: 3
speakers_per_batch: 64
utterances_per_speaker: 10

## Data parameters:
audio_norm_target_dBFS: -30
inference_n_frames: 80
mel_n_channels: 40
mel_window_length: 25
mel_window_step: 10
partials_n_frames: 160
sampling_rate: 16000
vad_max_silence_length: 6
vad_moving_average_width: 8
vad_window_length: 30
```

3. I trained with ~9,000 speakers (mixed languages but mostly English) through step 352,600 and included the UMAP projections for that below. I then remembered the [Common Voice](https://voice.mozilla.org/en/about) project from Mozilla and downloaded the entire thing. Then I placed all the individual speakers into unique folders and pruned all the speakers that didn't have 10 or more utterances. I then resumed training with the combined datasets bringing the total speakers to 25,668.
![stack_run_umap_358500](https://user-images.githubusercontent.com/324437/64698955-a8759700-d458-11e9-80b5-69e23f9c2256.png)
![stack_run_umap_358600](https://user-images.githubusercontent.com/324437/64698956-a8759700-d458-11e9-9773-0753ca65cbd4.png)

4. Thanks but I'll hold off on changing sample rate for now, already adjusting a lot.
5. I didn't download them from YouTube, they are available for download from TED.com at https://www.ted.com/talks/quick-list?page=1 and the alignments match TEDLIUM-3. The transcripts available from TED are of higher quality than the ones in TEDLIUM-3 dataset but alignments don't match due to the TED splash screen/banner that plays in the beginning.
6. Sounds good, Fatchord's version it is! Perfect timing as another person using this repository (@TheButlah) has just made a lot of improvements and included multi-gpu training.

The combined npz files have been working great for me, it will load all the utterances per speaker and still uses your same sampling code to grab a random sample per speaker. The only thing I removed is loading from individual npy files.

I assume I changed the backwards pass to GPU, either way the GPU utilization is much higher and the profiler is showing significantly lower mean duration's for ""Backward pass"". I changed the `loss_device` to run on the GPU.

Then on https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/encoder/model.py#L27-L28
```python
self.similarity_weight = nn.Parameter(torch.tensor([10.]).to(loss_device))
self.similarity_bias = nn.Parameter(torch.tensor([-5.]).to(loss_device))
```
Simply moved the tensor not the parameter to the GPU and changed the GPU sync in `train.py` to:
```python
def sync(device: torch.device):
    # FIXME
    # return
    # For correct profiling (cuda operations are async)
    if device.type == ""cuda"":
        # torch.cuda.synchronize(device)
        torch.cuda.synchronize()
```

I'm now up to step 447,200 and included the loss and UMAP to show progress. I also changed the UMAP visualization to show 30 speakers by adding more colors to the color map.

![37b6c18ff7d7d4](https://user-images.githubusercontent.com/324437/64699179-14f09600-d459-11e9-867e-01f7302f2708.jpg)
![37b6c19208f994](https://user-images.githubusercontent.com/324437/64699180-14f09600-d459-11e9-99fa-d64dd52e7bd8.jpg)

![cv_run_umap_447200](https://user-images.githubusercontent.com/324437/64699110-f4c0d700-d458-11e9-8636-bd4be69f8d9a.png)
![cv_run_umap_447300](https://user-images.githubusercontent.com/324437/64699112-f4c0d700-d458-11e9-9d0c-b447db8b94aa.png)

### New color map
```python
colormap = np.array([
    [32, 25, 35],
    [255, 255, 255],
    [252, 255, 93],
    [125, 252, 0],
    [14, 196, 52],
    [34, 140, 104],
    [138, 216, 232],
    [35, 91, 84],
    [41, 189, 171],
    [57, 152, 245],
    [55, 41, 79],
    [39, 125, 167],
    [55, 80, 219],
    [242, 32, 32],
    [153, 25, 25],
    [255, 203, 165],
    [230, 143, 102],
    [197, 97, 51],
    [150, 52, 28],
    [99, 40, 25],
    [255, 196, 19],
    [244, 122, 34],
    [47, 42, 160],
    [183, 50, 204],
    [119, 43, 157],
    [240, 124, 171],
    [211, 11, 148],
    [237, 239, 243],
    [195, 165, 180],
    [148, 106, 162],
    [93, 76, 134],
    [0, 0, 0],
    [183, 183, 183],
], dtype=np.float) / 255
```",thanks continuous feedback unfortunately patience wait response training model data shown shell model data trained mixed mostly step included common voice project entire thing individual unique training combined total thanks hold sample rate already lot available match available ted higher quality match due ted splash beginning good version perfect timing another person repository made lot included training combined working great load per speaker still sampling code grab random sample per speaker thing removed loading individual assume backwards pas either way utilization much higher profiler showing significantly lower mean duration backward pas run python simply tensor parameter sync python sync device return correct device step included loss show progress also visualization show color color map new color map python,issue,positive,positive,positive,positive,positive,positive
530272979,"2. Yes, sorry, you should adjust the hidden layer size as well. The way it is done in the GE2E paper is that all recurrent layers have an output of 768, but are projected down to 256 dimensions before being fed to the next. If you want to implement that you'll have to change the network architecture; but if it trains fast enough with 768 as hidden size, then you're fine.
3. Oh it's definitely going to work fine on different languages. The question is whether you'll manage to achieve an EER as low as on a single language dataset, and by extension a voice transfer that is just as good.
4. Hmm, you can give that a shot. You should listen to the quality of downsampled/upsampled audios to see what gives (you can do that in a REPL prompt with sounddevice)
5. I disagree. A whole lot of the source videos were removed from youtube. I know because I tried to guess the source language from the source videos.
6. Great. I personally recommend fatchord's (I have played around and analyzed both repos already). If you feel like Tacotron 1 might be a downgrade from Tacotron 2, know that it isn't - Tacotron 1 is still used more often than Tacotron 2 in the litterature. Fatchord's samples are also great. Know that if you reimplement the synthesizer, you will probably have to change some things so that the data format on the vocoder side is good. We can talk about that again then.

There are quite a few ways to gain disk reading speedups for the encoder, but don't forget that you still need variety in the samples/batches. Another bottleneck is the GPU VRAM not being entirely used. Since the complexity of the forward/backward pass is cubic w.r.t the batch size, you would need to put multiple batches in parallel on the same GPU rather than putting a larger batch size. It's something worth looking into.

I had no idea you could specify to run the backward pass on the gpu, how did you do that?",yes sorry adjust hidden layer size well way done gee paper recurrent output fed next want implement change network architecture fast enough hidden size fine oh definitely going work fine different question whether manage achieve eer low single language extension voice transfer good give shot listen quality see prompt disagree whole lot source removed know tried guess source language source great personally recommend around already feel like might downgrade know still used often also great know synthesizer probably change data format side good talk quite way gain disk reading forget still need variety another bottleneck entirely used since complexity pas cubic batch size would need put multiple parallel rather batch size something worth looking idea could specify run backward pas,issue,positive,positive,positive,positive,positive,positive
530267906,"What's your pyqt version? You can try to install 5.12.2, the same as mine.",version try install mine,issue,negative,neutral,neutral,neutral,neutral,neutral
530267377,"You'd be better off going to rayhane's repo for help. I didn't bother looking into arpabet at all. You might be able to finetune the model in a few thousands of steps (I'm speculating), you'll know it's working when alignment will look good.",better going help bother looking might able model know working alignment look good,issue,positive,positive,positive,positive,positive,positive
530201199,"I have made the necessary changes to support CPU-only inference here: https://github.com/shawwn/Real-Time-Voice-Cloning/commit/02db1d3c2b21b65daa9aec1fe70c341f49ae9720

You can use it by cloning https://github.com/shawwn/Real-Time-Voice-Cloning

",made necessary support inference use,issue,negative,neutral,neutral,neutral,neutral,neutral
530178530,"Hi Guys,

I am trying to run demo_toolbox.py but getting an error:

""Cannot mix incompatible Qt library (version 0x50602) with this library (version 0x50d00)""

I am using a windows 10 laptop and not sure how to fix this error. Can someone please help me with this. 

Thanks.",hi trying run getting error mix incompatible library version library version sure fix error someone please help thanks,issue,positive,positive,positive,positive,positive,positive
529833998,"Pretty cool! 

I think this line
`# Download dataset`
Is actually for downloading the pretrained models.

Also if you manage to find a way to suppress the output of the synthesizer in the last part, that'd be great. But it's alright if you don't",pretty cool think line actually also manage find way suppress output synthesizer last part great alright,issue,positive,positive,positive,positive,positive,positive
529722884,@ak9250 I found that there were long pauses when i attempted to make the model say something that didnt match the 5-7 second expected output (like a 3 second sentence),ak found long make model say something didnt match second output like second sentence,issue,negative,negative,neutral,neutral,negative,negative
529527259,`demo_cli.py` now passes but I've run into another bug with `demo_toolbox.py` so I can't verify that one.,run another bug ca verify one,issue,negative,neutral,neutral,neutral,neutral,neutral
529489691,"I've implemented it in a rush on the `low_mem_fix` branch, can you try and see what gives?",rush branch try see,issue,negative,neutral,neutral,neutral,neutral,neutral
529380190,"I've found out why and how to fix it: multiprocess uses forked workers by default, which inherits some state CUDA isn't expected. Switching to spawned workers fixes it.

<details>
<summary>patch</summary>

```diff

diff --git a/synthesizer/inference.py b/synthesizer/inference.py
index 99fb778..b9cc9c0 100644
--- a/synthesizer/inference.py
+++ b/synthesizer/inference.py
@@ -2,12 +2,12 @@ from synthesizer.tacotron2 import Tacotron2
 from synthesizer.hparams import hparams
 from multiprocess.pool import Pool  # You're free to use either one
 #from multiprocessing import Pool   # 
+from multiprocess.context import SpawnContext
 from synthesizer import audio
 from pathlib import Path
 from typing import Union, List
 import tensorflow as tf
 import numpy as np
-import numba.cuda
 import librosa
 
 
@@ -80,13 +80,15 @@ class Synthesizer:
             # Low memory inference mode: load the model upon every request. The model has to be 
             # loaded in a separate process to be able to release GPU memory (a simple workaround 
             # to tensorflow's intricacies)
-            specs, alignments = Pool(1).starmap(Synthesizer._one_shot_synthesize_spectrograms, 
-                                                [(self.checkpoint_fpath, embeddings, texts)])[0]
+            specs, alignments = Pool(1, context=SpawnContext()
+                                    ).starmap(Synthesizer._one_shot_synthesize_spectrograms, 
+                                              [(self.checkpoint_fpath, embeddings, texts)])[0]
     
         return (specs, alignments) if return_alignments else specs
 
     @staticmethod
     def _one_shot_synthesize_spectrograms(checkpoint_fpath, embeddings, texts):
+        import numba.cuda
         # Load the model and forward the inputs
         tf.reset_default_graph()
         model = Tacotron2(checkpoint_fpath, hparams)
```
</details>",found fix forked default state switching summary patch git index import import import pool free use either one import pool import synthesizer import audio import path import union list import import import class synthesizer low memory inference mode load model upon every request model loaded separate process able release memory simple spec pool spec pool return spec else spec import load model forward model,issue,negative,positive,positive,positive,positive,positive
529242074,"Thanks for the quick reply!

I also noticed the blocking operation taking a long time, found it very strange as the mel spectrograms are stored on a Samsung 960 EVO 1TB NVMe drive and `SpeakerVerificationDataLoader` has `num_workers=16` CPU bounces around from about 50-80% utilization and disk is showing 4-18% busy. nvidia-smi is showing low utilization. Maybe I completely glossed over the code where you are reading from the wav audio files during training? That would explain it as wav's are sitting on a slow spinning disk.

1. Thanks, I'll work on adding in TEDLIUM-3 into the encoder training set.
2. I'll restart training with an embedding size of 768 by adjusting `model_embedding_size = 768` in https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/encoder/params_model.py#L4. Would you adjust the `model_hidden_size` or any other parameters?
3. I have noticed them continue to tighten up even with multiple (very diverse) languages and an embedding size of 256.
4. Good to know on different sampling rates. Do you think I would be better off up-sampling the 16kHz to 24kHz for the embedding and down-sampling the remaining to 24kHz? VoxCeleb(1/2) and VCTK are in 16 kHz while the remaining speakers are in 24 kHz or ~44kHz.
5. It is a great dataset with a wide range of accents, they only provide the data in 16kHz but it is easy to find the source videos and extract 44kHz audio that aligns perfectly.
6. Once I get to synthesizer training I'll replace your code with fatchord's or nvidia's.

Edit:
The other thing I thought about for speeding up IO would be stacking the numpy files for each speaker into a single file as sequential reading is much faster. I would only have to open 10 files per step vs 100. I have plenty of memory in my computer I'm using for training so maybe that wont be an optimization many others could benefit from?

Edit 2:
I've gone through all the numpy files for each speaker and saved them into a combined file using `np.savez` and adjusted the code in `encoder/data_objects/speaker.py` and `encoder/data_objects/utterance.py` I'm now getting a much more consistent and lower load time for the data. Obviously increasing the embedding size from 256 to 768 has almost tripled the backward pass duration. Funny enough my overall step time has remained about the same but the embedding size tripled. So I consider that a win!

```shell
Step   1030   Loss: 3.2002   EER: 0.2662   Step time:  mean:   871ms  std:    58ms

Average execution time over 10 steps:
  Blocking, waiting for batch (threaded) (10/10):  mean:  103ms   std:   26ms
  Data to cuda (10/10):                            mean:    3ms   std:    0ms
  Forward pass (10/10):                            mean:    7ms   std:    1ms
  Loss (10/10):                                    mean:   73ms   std:    3ms
  Backward pass (10/10):                           mean:  569ms   std:   67ms
  Parameter update (10/10):                        mean:  116ms   std:    3ms
  Extras (visualizations, saving) (10/10):         mean:    1ms   std:    4ms
```

Edit 3:
I wasn't happy with the backward pass duration so I made the backwards pass run on the GPU. This is what I'm looking at now...

```shell
Step    310   Loss: 3.6576   EER: 0.3275   Step time:  mean:   425ms  std:   233ms

Average execution time over 10 steps:
  Blocking, waiting for batch (threaded) (10/10):  mean:  104ms   std:  122ms
  Data to cuda (10/10):                            mean:    3ms   std:    0ms
  Forward pass (10/10):                            mean:   39ms   std:    1ms
  Loss (10/10):                                    mean:   23ms   std:    1ms
  Backward pass (10/10):                           mean:   80ms   std:    5ms
  Parameter update (10/10):                        mean:  121ms   std:    2ms
  Extras (visualizations, saving) (10/10):         mean:    1ms   std:    3ms

..........
Step    320   Loss: 3.6723   EER: 0.3339   Step time:  mean:   322ms  std:    98ms

Average execution time over 10 steps:
  Blocking, waiting for batch (threaded) (10/10):  mean:   60ms   std:   97ms
  Data to cuda (10/10):                            mean:    3ms   std:    0ms
  Forward pass (10/10):                            mean:   39ms   std:    0ms
  Loss (10/10):                                    mean:   22ms   std:    1ms
  Backward pass (10/10):                           mean:   77ms   std:    4ms
  Parameter update (10/10):                        mean:  121ms   std:    2ms
  Extras (visualizations, saving) (10/10):         mean:    2ms   std:    4ms

..........
Step    330   Loss: 3.6419   EER: 0.3309   Step time:  mean:   362ms  std:   140ms

Average execution time over 10 steps:
  Blocking, waiting for batch (threaded) (10/10):  mean:   97ms   std:  139ms
  Data to cuda (10/10):                            mean:    3ms   std:    0ms
  Forward pass (10/10):                            mean:   39ms   std:    1ms
  Loss (10/10):                                    mean:   24ms   std:    3ms
  Backward pass (10/10):                           mean:   78ms   std:    4ms
  Parameter update (10/10):                        mean:  121ms   std:    1ms
  Extras (visualizations, saving) (10/10):         mean:    1ms   std:    3ms
```",thanks quick reply also blocking operation taking long time found strange mel drive around utilization disk showing busy showing low utilization maybe completely glossed code reading audio training would explain sitting slow spinning disk thanks work training set restart training size would adjust continue tighten even multiple diverse size good know different sampling think would better great wide range provide data easy find source extract audio perfectly get synthesizer training replace code edit thing thought speeding io would speaker single file sequential reading much faster would open per step plenty memory computer training maybe wont optimization many could benefit edit gone speaker saved combined file code getting much consistent lower load time data obviously increasing size almost backward pas duration funny enough overall step time size consider win shell step loss eer step time mean average execution time blocking waiting batch threaded mean data mean forward pas mean loss mean backward pas mean parameter update mean saving mean edit happy backward pas duration made backwards pas run looking shell step loss eer step time mean average execution time blocking waiting batch threaded mean data mean forward pas mean loss mean backward pas mean parameter update mean saving mean step loss eer step time mean average execution time blocking waiting batch threaded mean data mean forward pas mean loss mean backward pas mean parameter update mean saving mean step loss eer step time mean average execution time blocking waiting batch threaded mean data mean forward pas mean loss mean backward pas mean parameter update mean saving mean,issue,positive,negative,neutral,neutral,negative,negative
529235670,"Great work and great questions! I'll pin this issue for others in need of help.

Firstly, one thing I notice from your profiler output is that you would benefit from a 2x speedup by putting your data on a faster disk (or maybe increasing the number of threads in the DataLoader if you set them too low)

1. Yes, adding more speakers is always good. Not including the entire LibriSpeech dataset was, I believe, a deliberate choice of the SV2TTS authors to highlight the *transfer* learning aspect of their framework i.e. that the speaker encoder trained on some data will perform well on entirely new data (and a different purpose too).
2. That's a difficult question. Ideally you would have English-only speakers with a wide range of accents. I can't say that I have a definitive answer, however if you were to include a wide variety of languages I would recommend moving the speaker embedding size from 256 to 768 (as is done in SV2TTS). You could also do that for English-only speakers, simply I have found 256 to work well so far. A formal evaluation would require to compute the EER, and that is still a grey area for me (see the end of section 3.3.3 of my thesis)
3. Yes, your training looks like mine. You will see the clusters get tighter over time and the loss will continue decreasing steadily. If you have time, you can train for longer than I did (as I did not converge to 100%)
4. You're technically perfectly fine with different sample rates. Simply, any 24kHz audio you load/generate can be resampled (using librosa's resample function) to 16kHz for the encoder. I haven't tested the repo with different sampling rates, but I think I tried to make it possible to have different ones. I know there's an issue with that in the toolbox (https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/toolbox/__init__.py#L215), we can try to fix it when you need it.
5. Ok, I didn't know about that dataset but it seems promising.
6. I would greatly appreciate if someone were to replace entirely the synthesizer with a pytorch one. Both fatchord's and nvidia's would be fine.",great work great pin issue need help firstly one thing notice profiler output would benefit data faster disk maybe increasing number set low yes always good entire believe deliberate choice highlight transfer learning aspect framework speaker trained data perform well entirely new data different purpose difficult question ideally would wide range ca say definitive answer however include wide variety would recommend moving speaker size done could also simply found work well far formal evaluation would require compute eer still grey area see end section thesis yes training like mine see get time loss continue decreasing steadily time train longer converge technically perfectly fine different sample simply audio resample function tested different sampling think tried make possible different know issue toolbox try fix need know promising would greatly appreciate someone replace entirely synthesizer one would fine,issue,positive,positive,positive,positive,positive,positive
529225512,Thank you for your reply! I understand the problem now. Will make issue resolved. ,thank reply understand problem make issue resolved,issue,negative,neutral,neutral,neutral,neutral,neutral
529211694,"@yevgetman I'm not associated with this project at all but you are correct in that the Macbook does NOT have an nvidia GPU which means no CUDA. You wouldn't want to train any of the models without a GPU but I would assume you could run inference (at reduced performance) on a CPU.

That assumption of running inference on the CPU vs GPU would require you to make quite a few changes to the code base and dependencies. This is coming from my very high-level scan of this project and dependencies. It would be much easier to rent an instance from Google/Microsoft/Amazon which has an Nvidia GPU.

See issue #54 ",associated project correct would want train without would assume could run inference reduced performance assumption running inference would require make quite code base coming scan project would much easier rent instance see issue,issue,negative,negative,negative,negative,negative,negative
529058112,"thank you for your answer,I have solve the problem.but I solve it in a different way,just only change librosa version to 0.5.0 (from 0.7.0) I think the mainly problem is thread pool size.",thank answer solve solve different way change version think mainly problem thread pool size,issue,negative,positive,neutral,neutral,positive,positive
529013562,"@yyggithub I ran into that as well and I found two ways around it.

1. You can change the thread pool size to 1 (from 8) https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/encoder/preprocess.py#L114 Honestly I'm not sure why this worked.
1. You can convert every m4v file to wav. This was the route I went, just wrote a simple batch script and used ffmpeg (probably faster ways) to do the conversion.",ran well found two way around change thread pool size honestly sure worked convert every file route went wrote simple batch script used probably faster way conversion,issue,positive,positive,positive,positive,positive,positive
528943019,As long as you can run torch and tensorflow on GPU you should be fine,long run torch fine,issue,negative,positive,positive,positive,positive,positive
528718791,"I met the same issue, you can try:
1. pip3 uninstall pyqt5 
2. pip3 install pyqt5==5.12.0
it works for me",met issue try pip pip install work,issue,negative,neutral,neutral,neutral,neutral,neutral
528353998,"Yeah I don't know, other people have the same issue but I can't reproduce it with mp3 files.",yeah know people issue ca reproduce,issue,negative,neutral,neutral,neutral,neutral,neutral
528330449,You're probably not running scripts from the root directory,probably running root directory,issue,negative,neutral,neutral,neutral,neutral,neutral
528180524,"This problem resolved when I converted the file from mp3 to wav format, not sure if figuring out how to support mp3 is worth it or not, but if it isn't feel free to close this!",problem resolved converted file format sure support worth feel free close,issue,positive,positive,positive,positive,positive,positive
528000593,"thank you very much for your help, it works! The toolbox was shutting off because I did not add the pretrained models.",thank much help work toolbox shutting add,issue,positive,positive,positive,positive,positive,positive
527927041,Sorry but this is a project strictly meant for research/development. I have no intentions of distributing it in any other form.,sorry project strictly meant form,issue,negative,negative,negative,negative,negative,negative
527809468,"Replace <datasets_root> with the path of your librespeech
Also remember: https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models",replace path also remember,issue,negative,neutral,neutral,neutral,neutral,neutral
527800189,"When I copy and paste that code, it says the the syntax of the command is incorrect..I downloaded Linraspeexh but I don’t understand how to assign it as the datasets_root",copy paste code syntax command incorrect understand assign,issue,negative,neutral,neutral,neutral,neutral,neutral
527401710,"Hi:
I am trying to run your code on a ubuntu server . But when I try python3 demo_toolbox.py  , it prints
Arguments:
    datasets_root:    None
    enc_models_dir:   encoder/saved_models
    syn_models_dir:   synthesizer/saved_models
    voc_models_dir:   vocoder/saved_models
    low_mem:          False

Aborted (core dumped)

and I can run  python demo_cli.py successfully;  and I have tested without a GUI, but the same error;
is this the result of not sending data?",hi trying run code server try python none false aborted core run python successfully tested without error result sending data,issue,positive,positive,positive,positive,positive,positive
526914956,"I tried a Trump and Kate winslet, the voices have are a bit similar but no quite there. Also, there is a pause silence between the audio output where the voice drops the words",tried trump bit similar quite also pause silence audio output voice,issue,negative,neutral,neutral,neutral,neutral,neutral
526799914,Fixed. I worked out the install was wrong. Still need to set up CUDA tho but least the program is starting,fixed worked install wrong still need set tho least program starting,issue,negative,negative,negative,negative,negative,negative
526701816,"I've actually found that with Trump, using hour long speeches seems to be better (but still not great) than 5-7 seconds of data.

 That makes intuitive sense but I thought that the pretrained models were optimized for 5-12 second recordings... ",actually found trump hour long better still great data intuitive sense thought second,issue,positive,positive,positive,positive,positive,positive
526513068,@arun-ghontale Wavenet Ljspeech2 sounds best out of the bunch. I think it's a bit unnatural due to whata CorentinJ mentioned before - the audiobooks used in training.,best bunch think bit unnatural due whata used training,issue,positive,positive,positive,positive,positive,positive
526498924,It sounds pretty good. I tried cloning Rachel McAdams voice: https://soundcloud.com/arun-ghontale-235763095/rachel-mcadams-rachel-output?in=arun-ghontale-235763095/sets/name-synthesis,pretty good tried voice,issue,positive,positive,positive,positive,positive,positive
526432263,"Results are kinda crappy so far when I've tried Trump on this. 

Are there better Pre-trained embeddings available? ",far tried trump better available,issue,negative,positive,positive,positive,positive,positive
526186232,Well yeah you can expect better performance on data used for training. The speaker encoder is trained on VoxCeleb1 & 2 + the LibriSpeech-other sets while the synthesizer/vocoder are trained on the Librispeech-clean sets.,well yeah expect better performance data used training speaker trained trained,issue,positive,positive,positive,positive,positive,positive
526171268,"When I say ""others"", I mean for the recordings I supply.

I am not a native English speaker but I also tried with samples from Donald Trump's speech and it also does not give the same good results.

Did you use the following datasets for training? Can that be the reason?

        LibriSpeech/dev-clean
        LibriSpeech/dev-other
        LibriSpeech/test-clean
        LibriSpeech/test-other
        LibriSpeech/train-clean-100
        LibriSpeech/train-clean-360
        LibriSpeech/train-other-500
        LibriTTS/dev-clean
        LibriTTS/dev-other
        LibriTTS/test-clean
        LibriTTS/test-other
        LibriTTS/train-clean-100
        LibriTTS/train-clean-360
        LibriTTS/train-other-500
        LJSpeech-1.1
        VoxCeleb1/wav
        VoxCeleb1/test_wav
        VoxCeleb2/dev/aac
        VoxCeleb2/test/aac
        VCTK-Corpus/wav48",say mean supply native speaker also tried trump speech also give good use following training reason,issue,negative,positive,positive,positive,positive,positive
526109374,Thank you. You need a GPU and torch installed with cuda. Otherwise you can try to look for help in #54.,thank need torch otherwise try look help,issue,positive,neutral,neutral,neutral,neutral,neutral
526070030,"VoxCeleb samples usually aren't too bad for the most part. Also, the synthesizer/vocoder are strictly trained on native English speakers, so if you have any kind of accent you can expect it to fail.",usually bad part also strictly trained native kind accent expect fail,issue,negative,negative,negative,negative,negative,negative
526040826,"

> I run this tool successfully on my local machine without GPU. You need to replace all the codes which include cuda.

How did you replace all the cuda codes? Can you please explain ? That would be great help.

Thanks and regards,
Ateet Kumar



",run tool successfully local machine without need replace include replace please explain would great help thanks,issue,positive,positive,positive,positive,positive,positive
525932207,"I've tried it, it generated 6 seconds voice where synthesized a 20 words sentence with like 3 minutes video of a youtuber. It's like almost the same voice, but had a little bit of interruption and low quality in my case

editt: I think it's much more better with female voices, idk why.",tried voice sentence like video like almost voice little bit interruption low quality case think much better female,issue,positive,positive,neutral,neutral,positive,positive
525232344,"I shrunk it to not occupy too much space, did you have something else in mind?",shrunk occupy much space something else mind,issue,negative,positive,positive,positive,positive,positive
525226600,For this I can't help you. You're better off googling that error.,ca help better error,issue,negative,positive,positive,positive,positive,positive
525221189,"You need to install torch separately, as is said in the readme.

https://pytorch.org/get-started/locally/",need install torch separately said,issue,negative,neutral,neutral,neutral,neutral,neutral
525220477,"I don't like the idea. I suspect my answer is not going to be popular, so feel free to try and change my mind.

First off, you won't do anything that is too good with this repo. It's a proof of concept and also the work of a master's student alone so far. The voice generated sounds ok but it don't see it fooling anyone as it is. Especially given the lack of style control, you're often going to end with a very monotonic voice. Asking people not to use this to do evil sounds like unworthy sensational PR. If I get solid contributions from other developers, then maybe I will reconsider this.

Secondly, if you have bad intentions but you change your mind because I politely asked you not to do it, then you need to re-evaluate your villain career. Yeah, faceswap has that fancy paragraph that almost brings a tear to my eye, but how has that been working out for them? I have no way of verifying that you're going to use this repo without bad intents, and I don't see myself closing the doors on valuable work simply because a few want to abuse it. Again, this is considering my first point. If this were the perfect voice cloning software, I'd reconsider it _à la OpenAI_.",like idea suspect answer going popular feel free try change mind first wo anything good proof concept also work master student alone far voice see fooling anyone especially given lack style control often going end monotonic voice people use evil like unworthy sensational get solid maybe reconsider secondly bad change mind politely need villain career yeah fancy paragraph almost tear eye working way going use without bad see valuable work simply want abuse considering first point perfect voice reconsider la,issue,negative,positive,positive,positive,positive,positive
525213587,"You're welcome.

Could it be that while copying this new image its dimensions have changed???

# 650px × 364px",welcome could new image,issue,negative,positive,positive,positive,positive,positive
524583295,Can confirm. replacing `C:\Users\<USER>\AppData\Local\Programs\Python\Python37\Lib\site-packages\torch\lib)` with the version provided by nvidia works.,confirm user version provided work,issue,negative,neutral,neutral,neutral,neutral,neutral
524582725,nice I'll try that myself and see if I get the same result.,nice try see get result,issue,negative,positive,positive,positive,positive,positive
524554241,"You are not alone. I get the exact same error.

- Windows 10 (x64)
- Python 3.7.3
- CUDA 10.0.130
- cuDNN 7.6.3
- PyTorch 1.2

Greetings ~

Edit: tensorflow-gpu 1.14.0 is compiled against 7.4.1. I am not sure why the loaded runtime library version is 7.2.1. Is it controlled by the application?

Edit2: I got it working! The cuDNN runtime library is provided by torch. I just replaced the cudnn64_7.dll (which is located in C:\Users\\\<USER>\AppData\Local\Programs\Python\Python37\Lib\site-packages\torch\lib) with the cuDNN 7.6.3 runtime library.",alone get exact error python edit sure loaded library version application edit got working library provided torch user library,issue,negative,positive,positive,positive,positive,positive
524487133,"I'm going to try 1 last thing, which is to downgrade CUDA Toolkit from 10.1 to 10.0 which I see suggested in another git issue. will report back.

Edit: downgrade didn't help.",going try last thing downgrade see another git issue report back edit downgrade help,issue,negative,neutral,neutral,neutral,neutral,neutral
524485062,"The main error seems to be this one, in case anyone else gets this crash and searches

    2019-08-23 18:43:37.956870: E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.2.1 but source was compiled with: 7.4.1.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.
    2019-08-23 18:43:37.978871: E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.2.1 but source was compiled with: 7.4.1.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.
    Traceback (most recent call last):
      File ""C:\Users\michael\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\client\session.py"", line 1356, in _do_call
        return fn(*args)
      File ""C:\Users\michael\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\client\session.py"", line 1341, in _run_fn
        options, feed_dict, fetch_list, target_list, run_metadata)
      File ""C:\Users\michael\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\client\session.py"", line 1429, in _call_tf_sessionrun
        run_metadata)
    tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.
      (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
             [[{{node Tacotron_model/inference/encoder_convolutions/conv_layer_1_encoder_convolutions/conv1d/conv1d}}]]
             [[Tacotron_model/inference/add/_269]]
      (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
             [[{{node Tacotron_model/inference/encoder_convolutions/conv_layer_1_encoder_convolutions/conv1d/conv1d}}]]
    0 successful operations.
    0 derived errors ignored.

",main error one case anyone else crash loaded library source library major minor version need match higher minor version case later version binary install upgrade library building make sure library loaded compatible version compile configuration loaded library source library major minor version need match higher minor version case later version binary install upgrade library building make sure library loaded compatible version compile configuration recent call last file line return file line file line root error found unknown get convolution algorithm probably initialize try looking see warning log message printed node unknown get convolution algorithm probably initialize try looking see warning log message printed node successful derived,issue,negative,positive,positive,positive,positive,positive
524242953,"It’s really weird, I’ve done it according to you, and abandoning multithreaded programs is working fine.
`voxceleb2:   0%|                                                       | 5/5994 [01:21<26:39:08, 16.02s/speakers]`
The speed is a bit slow, but it is already very good, thank you for your patience.",really weird done according multithreaded working fine speed bit slow already good thank patience,issue,positive,positive,neutral,neutral,positive,positive
524239563,"What you mean is to also remove this line “with ThreadPool(30) as pool:” and only use ：
`list(tqdm(map(preprocess_speaker, speaker_dirs), dataset_name, len(speaker_dirs), unit=""speakers""))`",mean also remove line pool use list map,issue,negative,negative,negative,negative,negative,negative
524235591,"You can run the preprocessing in a single thread to be able to debug issues better. Replace [these lines](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/encoder/preprocess.py#L113-L116) by:
`list(tqdm(map(preprocess_speaker, speaker_dirs), dataset_name, len(speaker_dirs), unit=""speakers""))`",run single thread able better replace list map,issue,negative,positive,positive,positive,positive,positive
524193926,"yes,it is.
parser.add_argument(""-d"", ""--datasets"", type=str,default=""voxceleb2"")
Does it may cause some bad case?",yes may cause bad case,issue,negative,negative,negative,negative,negative,negative
523998891,"Makes sense! I agree that I like fatchord's synthesizer more as its easier to work with, although I think it would perform better qualitatively if it were tacotron 2 instead of taco1. Maybe someone will do a fork for it at some point to upgrade it.",sense agree like synthesizer easier work although think would perform better qualitatively instead maybe someone fork point upgrade,issue,positive,positive,positive,positive,positive,positive
523772693,"Oh I'm well aware the issue is present in this repo only. It's something I must have introduced while modifying rayhane's tacotron. Considering I hate to work with that codebase, I have in mind to switch to fatchord's tacotron to try and fix this bug at the same time. But as I said, I really don't have the time to work on that now, as I have work and university projects that take priority. If someone wants to work on that in a separate branch, I can definitely look over that from time to time.

As for long sentences, it's just a matter of the attention mechanism implemented. By splitting sentences on punctuation, you're fine with most sentences anyway.",oh well aware issue present something must considering hate work mind switch try fix bug time said really time work work university take priority someone work separate branch definitely look time time long matter attention mechanism splitting punctuation fine anyway,issue,negative,positive,neutral,neutral,positive,positive
523688986,"[200K-logs-eval.zip](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/3527713/200K-logs-eval.zip) (Rayhane Taco2, Griffin-Lim)
[Archive.zip](https://github.com/CorentinJ/Real-Time-Voice-Cloning/files/3527719/Archive.zip)(Fatchord Taco1, Fatchord WaveRNN)
Both @fatchord and @Rayhane-mamah repos do not exhibit gaps in middle of spectrograms like this repo does.

They both exhibit failure in the case of especially long sentences, which is expected. Taco 2 appears to fare much better in this case.",exhibit middle like exhibit failure case especially long fare much better case,issue,negative,positive,neutral,neutral,positive,positive
523688196,"@CorentinJ actually on going back through my synthesized recordings from @rayhane-mamah's repo, I haven't been able to observe any of the gaps I observe in your repo. I think its actually unique to this repository",actually going back able observe observe think actually unique repository,issue,negative,positive,positive,positive,positive,positive
522517376,"They're not mandatory, simply they allow to split long sentences into shorter ones and to profile the unvoiced regions for noise.

From the [SV2TTS paper](https://arxiv.org/pdf/1806.04558.pdf), section 3:
> We resegmented the data into shorter utterances by force aligning the audio to the transcript using an ASR model and breaking segments on silence, reducing the median duration from 14 to 5 seconds",mandatory simply allow split long shorter profile unvoiced noise paper section data shorter force audio transcript model breaking silence reducing median duration,issue,negative,negative,neutral,neutral,negative,negative
521948872,"If your speakers are cleanly separated in the space (like they are in the pictures), you should be good to go! I'd be interested to compare with the same plots but before any training step was made, to see how the model does on Chinese data.",cleanly space like good go interested compare training step made see model data,issue,positive,positive,positive,positive,positive,positive
521862843,"@CorentinJ @yaguangtang @tail95 @zbloss @HumanG33k I am finetuning the encoder model by Chhinese data of 3100 persons. I want to know how to judge whether the train of finetune is OK.    In Figure0, The blue line is based on 2100 persons , the yellow line is based on 3100 persons which is trained now.
Figure0:
![image](https://user-images.githubusercontent.com/40649244/63139015-3f446480-c00f-11e9-9ec2-3eadebd6023f.png)




Figure1:(finetune 920k , from 1565k to 1610k steps, based on 2100 persons)
![image](https://user-images.githubusercontent.com/40649244/63139038-5aaf6f80-c00f-11e9-85ca-f54fdd81431e.png)


 Figure2:(finetune 45k from 1565k to 1610k steps, based on 3100 persons)
![image](https://user-images.githubusercontent.com/40649244/63139112-a8c47300-c00f-11e9-9097-fb65452df9fd.png)

I also what to know how mang steps is OK , in general.  Because, I only know to train the synthesizer model and vocoder mode oneby one to judge the effect. But it will cost very long time.  How about my EER or Loss ?  Look forward your reply!",tail model data want know judge whether train figure blue line based yellow line based trained figure image figure based image figure based image also know mang general know train synthesizer model mode one judge effect cost long time eer loss look forward reply,issue,negative,positive,neutral,neutral,positive,positive
521835869,"No it won't. What you're thinking of is finetuning, whereas this is voice cloning. Both achieve the same objective but with a different order of data/time.",wo thinking whereas voice achieve objective different order,issue,negative,neutral,neutral,neutral,neutral,neutral
521834235,They don't need to. The encoder will resample audio to 16kHz prior to embbedding it.,need resample audio prior,issue,negative,neutral,neutral,neutral,neutral,neutral
521820726,"> I had the same problem and the error for me was fixed by only using .wav files.

Fine, closing the Issue.",problem error fixed fine issue,issue,negative,positive,positive,positive,positive,positive
521718577,"For what It's worth, Ive been working extensively on @fatchord's repo adding improvements to it. I've trained models on it and no longer experience the gaps in the audio we have observed using Rayhane's repo. However, the synthesizer is still somewhat sensitive to sentence length, particularly long sentences. Sentences four words or more in length are fine, but once sentences start to get really long, you get the same stammering you can observe in @CorentinJ 's repo. So yes, switching to @fatchord's synthesizer would probably be a big improvement, but you would also have to add to it the capability to do multi-speaker training, as right now it only has single-speaker capability.

I can also confirm that its an issue with the attention mechanism, not the stop token or anything else. @fatchord's repo just stops generating when the spectrogram frame is below a certain audio threshold. No stop tokens involved. You can also look at the attention graph and clearly see that the failure cases are due to the attention getting stuck on a particular time step and never progressing.",worth working extensively trained longer experience audio however synthesizer still somewhat sensitive sentence length particularly long four length fine start get really long get stammering observe yes switching synthesizer would probably big improvement would also add capability training right capability also confirm issue attention mechanism stop token anything else generating spectrogram frame certain audio threshold stop involved also look attention graph clearly see failure due attention getting stuck particular time step never,issue,negative,positive,neutral,neutral,positive,positive
520234716,"The batch size has no effect on the quality, rather what you're seeing is related to the length of the sentence you're generating. You can change the overlap and target parameters of the vocoder though, which will influence the batched inference.",batch size effect quality rather seeing related length sentence generating change overlap target though influence inference,issue,negative,neutral,neutral,neutral,neutral,neutral
520187562,"> This is something that I have been slowly piecing together. I have been gathering audiobooks and their text versions that are in the public domain (Project Gutenberg & LibriVox Recordings). My goal as of now is to develop a solid package that can gather an audiofile and corresponding book, performing necessary cleaning and such.

> Currently this project lives on my C:, but if there's interest in collaboration I'd gladly throw it here on GitHub.

@zbloss I'm very interested. Would you be able to upload your entire dataset somewhere? Or if it's difficult to upload, is there some way I could acquire it from you directly?

Thanks!",something slowly piecing together gathering text public domain project goal develop solid package gather corresponding book necessary cleaning currently project interest collaboration gladly throw interested would able entire somewhere difficult way could acquire directly thanks,issue,positive,positive,neutral,neutral,positive,positive
520132862,"> I run this tool successfully on my local machine without GPU. You need to replace all the codes which include cuda.

If it works on CPU, why not making this optional?
I saw this in other projects like:

```python
# its pseudocode

if torch.cuda and not options.force_cpu:
  torch.device = torch.cuda
else:
  torch.device = torch
```

and then rewrite all torch.cuda to torch.device. I think it's look a bit like a hack, but it works and it's easy to refactor",run tool successfully local machine without need replace include work making optional saw like python else torch rewrite think look bit like hack work easy,issue,positive,positive,positive,positive,positive,positive
519418926,"I see. Many thanks to all of the participants of the chat.
Given all of the above, I guess that for business production-like setting (i.e. short sentences) WaveGlow and FloWaveNet are the most balanced options for now.",see many thanks chat given guess business setting short balanced,issue,negative,positive,positive,positive,positive,positive
519417518,"I agree with CorentinJ, too. MOS results/links to samples reported in
original papers, open source implementations, and derivative work on speech
synthesis and voice conversion using alternate models don't seem to reach
WaveNet/WaveRNN quality yet. Of course, WaveNet which is kind of the gold
standard in vocoder technology is computationally too expensive to use in
run-time. I think WaveRNN is the best alternate vocoder so far. It's easier
to implement and to train. LPCNet looks also very interesting especially if
you have more limited computational resources, no GPU, etc.

On Thu, Aug 8, 2019 at 11:02 AM Corentin Jemine <notifications@github.com>
wrote:

> WaveGlow is both slower and of worse quality than WaveRNN. Do keep in mind
> that we're talking about the available public implementations, not the
> papers. But even in the papers it's the case.
>
> —
> You are receiving this because you commented.
>
>
> Reply to this email directly, view it on GitHub
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/82?email_source=notifications&email_token=ABMAQJ46JRIUOADIMYKQYWLQDPHH3A5CNFSM4IKGC462YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD32Z3JA#issuecomment-519413156>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ABMAQJ5NGM7C3CZRXEUKVNDQDPHH3ANCNFSM4IKGC46Q>
> .
>
",agree original open source derivative work speech synthesis voice conversion alternate seem reach quality yet course kind gold standard technology expensive use think best alternate far easier implement train also interesting especially limited computational wrote worse quality keep mind talking available public even case reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
519417421,"> Right, I guess I was wrong about the speed of WaveGlow then. WaveRNN uses batched inference, so its speed is proportional to the length of the spectrogram to synthesize. I've gone up to 20x real-time for WaveRNN, but on short sentences it's going to be around 1x.
> 
> Anyway, the reason I picked WaveRNN over WaveGlow was due to the quality of the samples each open source implementation presented.

ah yes, I should test on longer sentences. I ran tests on a single sentence <100 characters.",right guess wrong speed inference speed proportional length spectrogram synthesize gone short going around anyway reason picked due quality open source implementation ah yes test longer ran single sentence,issue,negative,negative,neutral,neutral,negative,negative
519417283,"> WaveGlow is both slower and of worse quality than WaveRNN. Do keep in mind that we're talking about the available public implementations, not the papers. But even in the papers it's the case.

Nvidia's implementation of WaveGlow is giving me nearly 2000kHz on my v100. I tried this repo's vocoder out of box, and maybe I'm doing sth wrong but it's sub real-time (0.6~0.8 RTS).",worse quality keep mind talking available public even case implementation giving nearly tried box maybe wrong sub,issue,negative,negative,neutral,neutral,negative,negative
519417012,"Right, I guess I was wrong about the speed of WaveGlow then. WaveRNN uses batched inference, so its speed is proportional to the length of the spectrogram to synthesize. I've gone up to 20x real-time for WaveRNN, but on short sentences it's going to be around 1x.

Anyway, the reason I picked WaveRNN over WaveGlow was due to the quality of the samples each open source implementation presented.",right guess wrong speed inference speed proportional length spectrogram synthesize gone short going around anyway reason picked due quality open source implementation,issue,negative,negative,neutral,neutral,negative,negative
519414664,"This repo aims to reproduce SV2TTS, where they use a 3 layer LSTM as encoder. That's why I went with that architecture.

The training of the encoder is not quite optimized, as the batch size could be made larger through same-GPU parallelization. Nonetheless, the encoder operates at around 1000x real-time speed, and is the best model for speaker identification I've ever seen, so I see no reason to try and change it.",reproduce use layer went architecture training quite batch size could made parallelization nonetheless around speed best model speaker identification ever seen see reason try change,issue,positive,positive,positive,positive,positive,positive
519414544,"> WaveGlow is both slower and of worse quality than WaveRNN

I (and some other people) tested that the official public implementation of WaveGlow was 4-8x RTS on one 1080Ti (inference)
Have not tested WaveRNN yet, but heard reports of it being <1 RTS

What are your benchmarks?",worse quality people tested official public implementation one ti inference tested yet,issue,negative,negative,negative,negative,negative,negative
519413156,"WaveGlow is both slower and of worse quality than WaveRNN. Do keep in mind that we're talking about the available public implementations, not the papers. But even in the papers it's the case.",worse quality keep mind talking available public even case,issue,negative,neutral,neutral,neutral,neutral,neutral
519372140,"Well actually WaveGlow's quality doesn't seem too bad compared to WaveRNN.
Important thing to note is that MOS is subjective and relative measure of audio quality. In the original paper, WaveNet's MOS of training dataset is much higher than that of WaveGlow, which might indicate that for the same quality WaveGlow's MOS may be lower.

But also, WaveGlow may have had a worse synthesizer than google did. But as far as vocoder's performance goes (spec to wav), WaveGlow does not seem too bad, but much faster.",well actually quality seem bad important thing note subjective relative measure audio quality original paper training much higher might indicate quality may lower also may worse synthesizer far performance go spec seem bad much faster,issue,negative,negative,neutral,neutral,negative,negative
519366083,"MOS quality of both WaveGlow and FloWaveNet seems to be significantly worse
than WaveNet according to original papers. WaveRNN’s is much closer to
WaveNet.

On Thu, Aug 8, 2019 at 6:49 AM Alexander Veysov <notifications@github.com>
wrote:

> Hi!
>
> For a change it is always great to see a repo where a real human did
> something as opposed to an endless stream of corporate / academic research
> that cannot essentially be reproduced.
>
> We have published a huge STT dataset
> <https://github.com/snakers4/open_stt/> and are also planning to extend
> our TTS dataset <https://github.com/snakers4/open_stt/> with 30-40 voices
> (at least). Our datasets are in Russian. So if you would like to extend
> your language support - please stay tuned.
>
> Anyway - I wanted to ask - why did you choose WaveRNN? It seems that
> WaveGlow / FloWaveNet are the go-to option now? I tested WaveGlow - it
> trains mostly as promised and code is really easy to use.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/82?email_source=notifications&email_token=ABMAQJ3Y2LIV3YL6H4YFGS3QDOJURA5CNFSM4IKGC462YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4HEBJZ3A>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ABMAQJ7VMJ32MLLGVBVCKRDQDOJURANCNFSM4IKGC46Q>
> .
>
",quality significantly worse according original much closer wrote hi change always great see real human something opposed endless stream corporate academic research essentially huge also extend least would like extend language support please stay tuned anyway ask choose option tested mostly code really easy use thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
519042658,"Even I am facing the same issue. 

@byuns9334 can you please help in solving this issue.",even facing issue please help issue,issue,positive,neutral,neutral,neutral,neutral,neutral
519040273,"> maybe 20 minutes or less?

Wouldn't that be wonderful. You'll still need a good week or so. A few hours if you use the pretrained model. Although at this point what you're doing is no longer voice cloning, so you're not really in the right repo for that.",maybe le would wonderful still need good week use model although point longer voice really right,issue,positive,positive,positive,positive,positive,positive
519031351,Yeah that seems within the range of what it should be. Nothing trains really fast in this repo.,yeah within range nothing really fast,issue,negative,positive,positive,positive,positive,positive
518986623,"> You'd want hundreds of speakers at least. In fact, LibriSpeech-clean makes for 460 speakers and it's still not enough.

@CorentinJ Hello, ignoring speakers out of training dataset, if I only want to assure the quality and similarity of wav synthesized with speakers in the training dataset(librispeech-clean), how much time (at least) for one speaker do I need for training, maybe 20 minutes or less? ",want least fact still enough hello training want assure quality similarity training much time least one speaker need training maybe le,issue,negative,negative,neutral,neutral,negative,negative
518166717,I run this tool successfully on my local machine without GPU. You need to replace all the codes which include cuda.,run tool successfully local machine without need replace include,issue,negative,neutral,neutral,neutral,neutral,neutral
518055188,"@magneter  I have not trained the Chinese model,  I don't have enough data to train the speaker verification model, I am trying to collect suitable data now
",trained model enough data train speaker verification model trying collect suitable data,issue,negative,positive,positive,positive,positive,positive
517935671,@Liujingxiu23  Have you trained a Chinese model?And could you share your model about the Chinese clone results?,trained model could share model clone,issue,negative,neutral,neutral,neutral,neutral,neutral
517903203,"That's fairly slow, it's faster on my GTX1080. Head over to https://github.com/fatchord/WaveRNN/issues to see if someone has had the same problem.",fairly slow faster head see someone problem,issue,negative,negative,negative,negative,negative,negative
517903054,"Eh, just stop when the loss doesn't decrease anymore or when it looks/sounds good to you from the visualizations.",eh stop loss decrease good,issue,negative,positive,positive,positive,positive,positive
517903008,"The preprocessing script will split LibriSpeech on long silences using the alignment data. This way, the model can be trained on shorter utterances and converge faster. It also allows me to profile the noise from the silences. In the end, doing this doesn't much of an advantage over using an unaligned dataset, so if that your goal I wouldn't worry.",script split long alignment data way model trained shorter converge faster also profile noise end much advantage unaligned goal would worry,issue,negative,positive,neutral,neutral,positive,positive
517513570,@CorentinJ Thank you for your reply，may be I should find some Chinese data-sets for ASR to train the speaker verification model.,thank find train speaker verification model,issue,negative,neutral,neutral,neutral,neutral,neutral
517238170,"You can do that, but I would then add the synthesizer dataset in the speaker encoder dataset. In SV2TTS, they use disjoint datasets between the encoder and the synthesizer, but I think it's simply to demonstrate that the speaker encoder generalizes well (the paper is presented as a transfer learning paper over a voice cloning paper after all).

There's no guarantee the speaker encoder works well on different languages than it was trained on. Considering the difficulty of generating good Chinese speech, you might want to do your best at finding really good datasets rather than hack your way around everything.",would add synthesizer speaker use disjoint synthesizer think simply demonstrate speaker well paper transfer learning paper voice paper guarantee speaker work well different trained considering difficulty generating good speech might want best finding really good rather hack way around everything,issue,positive,positive,positive,positive,positive,positive
516765506,"How about training the encoder/speaker_verification using English multi-speaker data-sets, but training the synthesizer using Chinese database, suppose both the data are enough for each individual model separately.",training training synthesizer suppose data enough individual model separately,issue,negative,neutral,neutral,neutral,neutral,neutral
516668146,"Trying to run through colab, cloned project, installed requirements, uploaded pretrained models. I am confused what to do next as running init in toolbox isn't working. I trained models and using my own audio file as input in encoder. Yes I am new into coding!",trying run project confused next running toolbox working trained audio file input yes new,issue,negative,negative,neutral,neutral,negative,negative
515798056,"@ak9250 This is because in demo_cli.py, when synthesizing completes, the synthesized audio will be played. However the Colab VM doesn't have any audio device. You need to modify the code a bit.",ak audio however audio device need modify code bit,issue,negative,neutral,neutral,neutral,neutral,neutral
515790072,"@CorentinJ just tried it got this error
/content/Real-Time-Voice-Cloning/UserAudio/speaker_01/audio_01.wav
Loaded file succesfully
Created the embedding
""this is a test""
Created the mel spectrogram
Synthesizing the waveform:
{| ████████████████ 57000/57600 | Batch Size: 6 | Gen Rate: 4.6kHz | }Caught exception: PortAudioError('Error querying device -1',)
Restarting",tried got error loaded file test mel spectrogram batch size gen rate caught exception querying device,issue,negative,neutral,neutral,neutral,neutral,neutral
515779974,"@CorentinJ I can share my model if it performs well, but just want to check a few things with you. 
The text in LibreSpeech dataset are all in uppercase and almost doesn't contain any puncturations. I saw the cleaner does the lower case conversion, but how about puncturations? Does it impact the training? VCTK is probably better since it's with normal text format? 

Thanks,
Shuze ",share model well want check text almost contain saw cleaner lower case conversion impact training probably better since normal text format thanks,issue,positive,positive,positive,positive,positive,positive
515774294,"Correct. It's a mistake I inadvertently introduced god knows when and it's hard to tell what impact it's having. I put the fix in 071a6f9eca9393ee302d3bdc8c0c39bce5813cba, but I should be retraining the synthesizer and vocoder. Don't have the hardware nor budget unfortunately.",correct mistake inadvertently god hard tell impact put fix synthesizer hardware budget unfortunately,issue,negative,negative,negative,negative,negative,negative
515763632,"@CorentinJ any update on this, once it works I can send a PR if you would like to add the colab notebook to the repo",update work send would like add notebook,issue,negative,neutral,neutral,neutral,neutral,neutral
515661593,"You can figure our the source of voxceleb files by their filenames, it's a youtube watch id.",figure source watch id,issue,negative,neutral,neutral,neutral,neutral,neutral
515636607,mac + cuda does not go together. high sierra was last time nvidia released drivers for mac.,mac go together high sierra last time mac,issue,negative,positive,neutral,neutral,positive,positive
514964706,"Regarding wavernn inference speed:

Maybe it can help replacing GRU with SRU layers:

https://github.com/taolei87/sru
https://github.com/taolei87/sru/issues/96

https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/vocoder/models/fatchord_version.py#L109",regarding inference speed maybe help,issue,negative,neutral,neutral,neutral,neutral,neutral
514726502,"Maybe your pytorch version is not compatible with the model:
Try `torch==1.1.0`",maybe version compatible model try,issue,negative,neutral,neutral,neutral,neutral,neutral
514653922,"Yeah so as I stated in my thesis I have no good grounds for comparison and whatever the number I reported are, they don't mean much because the authors of SV2TTS give next to no info on how they evaluate the EER, which is a delicate procedure. I can give you my script, but I don't know if will help you find the answer you're looking for.

```
from encoder.data_objects import SpeakerVerificationDataLoader, SpeakerVerificationDataset
from encoder.model import SpeakerEncoder
from pathlib import Path
import matplotlib.pyplot as plt
import numpy as np
import torch


# This is my script for computing the test EER.
dataset_root = r""E:\Datasets\SV2TTS\encoder_test""

if __name__ == '__main__':
    speakers_per_batch = 32
    steps = 100
    
    dataset = SpeakerVerificationDataset(Path(dataset_root))
    
    model = SpeakerEncoder(torch.device(""cuda""), torch.device(""cpu""))
    checkpoint = torch.load(""saved_models/pretrained.pt"")
    model.load_state_dict(checkpoint[""model_state""])
    model.eval()
    
    results = []
    for utterances_per_speaker in range(6, 8):
        loader = SpeakerVerificationDataLoader(
            dataset,
            speakers_per_batch=speakers_per_batch,
            utterances_per_speaker=utterances_per_speaker,
            num_workers=8,
        )
        with torch.no_grad():
            eers = []
            for step, speaker_batch in zip(range(1, steps + 1), loader):
                inputs = torch.from_numpy(speaker_batch.data).cuda()
                embeds = model(inputs)
                embeds_loss = embeds.view((speakers_per_batch, utterances_per_speaker, -1)).cpu()
                _, eer = model.loss(embeds_loss)
                
                eers.append(eer)
                print(""Step %d    EER: %.3f"" % (step, np.mean(eers)))
        results.append(np.mean(eers))
        
    plt.plot(range(2, 11), results)
    plt.xlabel(""Enrollment utterances"")
    plt.ylabel(""Equal Error Rate"")
    plt.show()
```",yeah stated thesis good ground comparison whatever number mean much give next evaluate eer delicate procedure give script know help find answer looking import import import path import import import torch script test eer path model range loader step zip range loader model eer eer print step eer step range enrollment equal error rate,issue,positive,positive,neutral,neutral,positive,positive
514549063,"@CorentinJ Sorry, I missed details in this part in your thesis before. However, after doing tests following this part, using your pretrained model I still can't reproduce your result that EER is 4.53%.Please help me check my process for testing. 
First, I preprocess the libirspeech-test-clean、librispeech-test-other、VoxCeleb1、2 test datasets and get 231 speakers totally, then following the demo_cli.py to get embeddings for each speaker using 6 utterances each, and then joint them to get an embedding array shapes (231,6,256). At last, I use the function ""loss"" in encoder/model.py to compute EER, and I get 1.37% which has a big difference between you. Which of my steps is wrong?",sorry part thesis however following part model still ca reproduce result eer help check process testing first test get totally following get speaker joint get array last use function loss compute eer get big difference wrong,issue,negative,negative,negative,negative,negative,negative
514525504,"> You'd want hundreds of speakers at least. In fact, LibriSpeech-clean makes for 460 speakers and it's still not enough.

Can not be hack to by create new speakers with ai like it is done for picture ?",want least fact still enough hack create new ai like done picture,issue,positive,negative,neutral,neutral,negative,negative
514452226,"> > I suggest you to look at the detail of 'preprocess.py' which in 'synthesizer' directory. The purpos of '.alignment.txt' is to split the wav file into pieces.I think you need to rewrite 'preprocess.py' for LibriSpeech, because it doesn't need to split wav file and it couldn't.It's what the way I process vctk dataset, hope it help, thanks.
> 
> This is wrong. You simply need to download the [alignments](https://github.com/CorentinJ/librispeech-alignments#download-links) as is explained [here](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training).

You are right, LibriSpeech has alignment.txt and I have download it before，I'm sorry I forget it. ",suggest look detail directory split file think need rewrite need split file way process hope help thanks wrong simply need right sorry forget,issue,negative,negative,negative,negative,negative,negative
514427488,I'd be great if one of you guys could hop into a debugger and see where it happens.,great one could hop see,issue,positive,positive,positive,positive,positive,positive
514427111,"I won't add torch to requirements.txt because the authors prefer you install it from the website. It will fail through pip, for example. I've added the version in the readme: 81ee160d46b11770d11b950895810c7793988873",wo add torch prefer install fail pip example added version,issue,negative,negative,negative,negative,negative,negative
514426466,I'll gladly upload if you give a mirror you want it uploaded to,gladly give mirror want,issue,negative,positive,positive,positive,positive,positive
514424191,"> I suggest you to look at the detail of 'preprocess.py' which in 'synthesizer' directory. The purpos of '.alignment.txt' is to split the wav file into pieces.I think you need to rewrite 'preprocess.py' for LibriSpeech, because it doesn't need to split wav file and it couldn't.It's what the way I process vctk dataset, hope it help, thanks.

This is wrong. You simply need to download the [alignments](https://github.com/CorentinJ/librispeech-alignments#download-links) as is explained [here](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training).",suggest look detail directory split file think need rewrite need split file way process hope help thanks wrong simply need,issue,positive,negative,neutral,neutral,negative,negative
514423503,[Starting from the last paragraph of page 19](https://matheo.uliege.be/bitstream/2268.2/6801/5/s123578Jemine2019.pdf#page=20). The test set is the join set of the LibriSpeech test set and the VoxCeleb test sets.,starting last paragraph page test set join set test set test,issue,negative,neutral,neutral,neutral,neutral,neutral
514423034,"> What I've seen the most significant differences are 16kHz sampling rate and a slightly smaller window size for the batched synthesis.

This is correct. I haven't changed anything else other than the data loader.",seen significant sampling rate slightly smaller window size synthesis correct anything else data loader,issue,negative,positive,positive,positive,positive,positive
514398698,I am having the same issue. Did you ever figure out the problem?,issue ever figure problem,issue,negative,neutral,neutral,neutral,neutral,neutral
514307693,"I wonder what is lower limit of sample_rate for reasonable speech quality? Can it be tested just by resampling by something like ffmpeg?

1. https://github.com/fatchord/WaveRNN
sample_rate = 22050
https://github.com/fatchord/WaveRNN/blob/master/hparams.py#L20

2. https://github.com/CorentinJ/Real-Time-Voice-Cloning
sample_rate=16000
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/vocoder/hparams.py#L6
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/synthesizer/hparams.py#L117

3. https://github.com/erogol/WaveRNN
mold model:
""sample_rate"": 22050
https://drive.google.com/drive/folders/1wpPn3a0KQc6EYtKL0qOi4NqEmhML71Ve
10bit model:
""sample_rate"": 22050
https://drive.google.com/drive/folders/1VnTJfg2zmvochFNyX7oyUv9TFq6JsnVp
universal vocoder:
""sample_rate"": 16000
https://drive.google.com/drive/u/0/folders/15JhAbc91dT-RRZwakh_v4tBVOEuoOikg

",wonder lower limit reasonable speech quality tested something like mold model bit model universal,issue,negative,positive,neutral,neutral,positive,positive
514300012,What I've seen the most significant differences are 16kHz sampling rate and a slightly smaller window size for the batched synthesis. ,seen significant sampling rate slightly smaller window size synthesis,issue,negative,positive,positive,positive,positive,positive
514083248,"I suggest you to look at the detail of 'preprocess.py' which in 'synthesizer' directory. The purpos of '.alignment.txt' is to split the wav file into pieces.I think you need to rewrite 'preprocess.py' for LibriSpeech, because it doesn't need to split wav file and it couldn't.It's what the way I process vctk dataset, hope it help, thanks.",suggest look detail directory split file think need rewrite need split file way process hope help thanks,issue,positive,positive,positive,positive,positive,positive
514041558,"Check his thesis. I believe he wrote which datasets he used.

On Mon, Jul 22, 2019 at 11:09 PM xw1324832579 <notifications@github.com>
wrote:

> Thank you for replying me in detail. And I have another question ,how do
> you get the EER 4.5% in your tests in speaker encoder, which test
> dataset(which speakers and which utterances) do you use? I haven't find
> this in your thesis, and I guess you use the same dataset as the author,
> right? However, after reading the paper, I find the author use 21 speakers
> from VCTK and Librispeech-clean using 100 test utterances each speaker, but
> totally get 21,000 or 23,100 trials for test, that means 1000 utterances
> for each speaker, how many utterances do you use, and these get from raw
> wav in original dataset or synthesized wavs getting from the system?
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/61?email_source=notifications&email_token=AHAF3TYFDIGAS7OFNP56Z23QAZY7XA5CNFSM4IFYKTJ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2RZB7Q#issuecomment-514035966>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AHAF3TYNUBVWRCR7VT6RAVLQAZY7XANCNFSM4IFYKTJQ>
> .
>
",check thesis believe wrote used mon wrote thank detail another question get eer speaker test use find thesis guess use author right however reading paper find author use test speaker totally get test speaker many use get raw original getting system thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
514035966,"Thank you for replying me in detail. And I have another question ,how do you get the EER 4.5% in your tests in speaker encoder, which test dataset(which speakers and which utterances) do you use? I haven't find this in your thesis, and I guess you use the same dataset as the author, right? However, after reading the paper, I find the author use 21 speakers from VCTK and Librispeech-clean using 100 test utterances each speaker, but  totally get 21,000 or 23,100 trials for test, that means 1000 utterances for each speaker, how many utterances do you use, and these get from raw wav in original dataset or synthesized wavs getting from the system?",thank detail another question get eer speaker test use find thesis guess use author right however reading paper find author use test speaker totally get test speaker many use get raw original getting system,issue,positive,positive,positive,positive,positive,positive
513973136,"I am running Linux on both computers. Ubuntu 18.04 with nvidia-430 and Cuda 10.2 (2gb VRAM) and Linux Mint 19.1 with nvidia-410 and Cuda 10.0 (4gb VRAM) on the other.

I have been playing around a bit with the code and it seems that in `vocoder/inference.py` (almost) any call to `numba.cuda` results in the error.",running mint around bit code almost call error,issue,negative,neutral,neutral,neutral,neutral,neutral
513813035,"The synthesizer/vocoder are trained on the clean sets (360 + 100). Including the other-500 set could be beneficial to generalization, not sure.

And yes the framework is very much limited by the ability of the synthesizer to generalize, which it's no doing so well. Voices with a non-English accent are very poorly cloned for example.",trained clean set could beneficial generalization sure yes framework much limited ability synthesizer generalize well accent poorly example,issue,positive,positive,neutral,neutral,positive,positive
513755997,"Seems to work with:

```
pip freeze | grep torch
torch==1.1.0
torchfile==0.1.0
torchvision==0.3.0
```",work pip freeze torch,issue,negative,neutral,neutral,neutral,neutral,neutral
513738988,"My computing memory is about 200G. I checked the training log. The data prepare time will take up more than 90% of the total time. Therefore, I consider reading the data into the memory first and reading it from the memory.
I also used the preload mode of nvidia's APEX framework to load, and the training speed is generally improved. I also used 3 card training, I found that the forward pass and the backward pass time have decreased, but there seems to be a problem, that is, the same batch_size=64 size is divided into 3 cards and the loss is calculated separately and the single card batch_size=64 is calculated. There is a difference in loss, do you think I am right?",memory checked training log data prepare time take total time therefore consider reading data memory first reading memory also used mode apex framework load training speed generally also used card training found forward pas backward pas time problem size divided loss calculated separately single card calculated difference loss think right,issue,negative,positive,positive,positive,positive,positive
513733635,"[My thesis, page 24](https://matheo.uliege.be/bitstream/2268.2/6801/5/s123578Jemine2019.pdf#page=25):
>While the “optimal” duration of reference speech was found to be 5 seconds, the embedding is
>shown to be already meaningful with only 2 seconds of reference speech (see table 4).
>We believe that with utterances no shorter than the duration of partial utterances
>(1.6s), the utterance embedding should be sufficient for a meaningful capture of the
>voice, hence we used utterance embeddings for training the synthesizer.

![image](https://user-images.githubusercontent.com/12038136/61625423-fd780680-ac7a-11e9-8c2b-401d0f0e5efe.png)

As for other requirements: the silences are removed automatically. There can be background noise, the encoder is robust. Unfortunately the synthesizer/vocoder aren't, so a voice that deviates a little too much from the training data will be cloned poorly.

See here for your last question: https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/41#issuecomment-510323973",thesis page optimal duration reference speech found shown already meaningful reference speech see table believe shorter duration partial utterance sufficient meaningful capture voice hence used utterance training synthesizer image removed automatically background noise robust unfortunately voice little much training data poorly see last question,issue,positive,positive,neutral,neutral,positive,positive
513730753,"Yeah so the encoder training is definitely something that I need to work on. It's quite unusual in deep learning to have a loss function of cubic complexity w.r.t. the batch size, so it's not a problem you see often. What I should do is add a minor and a major batch size where the minor batch size is the number of speakers and the major one is the number of batches of such speakers. That would help leveraging all the available GPU memory.

As for the data loading, it can be a problem if your disk is slow. It's not possible to read all data in memory, the dataset is huge. And due to the nature of the training you can't prepare batches in advance either. You can check if you are throttled by disk reading speed by enabling the profiler in this line:
https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/encoder/train.py#L67",yeah training definitely something need work quite unusual deep learning loss function cubic complexity batch size problem see often add minor major batch size minor batch size number major one number would help available memory data loading problem disk slow possible read data memory huge due nature training ca prepare advance either check disk reading speed profiler line,issue,negative,positive,neutral,neutral,positive,positive
513713486,"> Are you sure you have the latest commit and that your vocoder directoy is exactly what is currently on the master branch?

i change the pytorch verision into 1.1.0 and it solve the problem

",sure latest commit exactly currently master branch change solve problem,issue,negative,positive,positive,positive,positive,positive
513598363,"It may be possible, but you would have to change calls to cuda() to get that to work. I don't know how torch handles other GPUs. You would also need to change the device the weights are stored on.",may possible would change get work know torch would also need change device,issue,negative,neutral,neutral,neutral,neutral,neutral
513598200,"Of course there is. The project is meant to be run with 3.6 at minimum. See the readme:

> Python 3.7. Python 3.6 might work too, but I wouldn't go lower because I make extensive use of pathlib.

",course project meant run minimum see python python might work would go lower make extensive use,issue,negative,neutral,neutral,neutral,neutral,neutral
513450874,"You actually want the encoder dataset not to always be of good quality, because that makes the encoder robust. It's different for the synthesizer/vocoder, because the quality is the output you will have (at best)",actually want always good quality robust different quality output best,issue,positive,positive,positive,positive,positive,positive
513416435,"Oh I thought that was just a fork of Keithitos. Regardless, I'll look into using a different implementation and/or try to figure out whats wrong with rahayne's. Thanks for the help!",oh thought fork regardless look different implementation try figure whats wrong thanks help,issue,negative,negative,neutral,neutral,negative,negative
513404307,"I think the issue is elsewhere, as in most likely a bug from my end or rayhane's work. I've talked with someone else whose work also stems from rayahane's and he's got the same problem. Meanwhile, other implementations elsewhere (mozilla, nvidia, fatchord) of tacotron/tacotron2 do not have that issue.",think issue elsewhere likely bug end work someone else whose work also got problem meanwhile elsewhere issue,issue,negative,neutral,neutral,neutral,neutral,neutral
513390771,"Ah yes I see what you mean. That makes sense, I agree that it has to be another issue.

One idea that I had was annealing the level of teacher forcing that takes place during training. I suspect that the issue is that due to the synthesizer being autoregressive, any errors (deviation from true mel frame) are going to compound on each other as they get fed into the predictions for the next Mel frame. Teacher forcing accelerates training convergence because it removes the ability of these errors to propogate, but I would expect that the network would never learn to account for its own errors because it always was fed real data during training. Hence annealing the probability that the spectrogram frame is teacher forced might get the best of both worlds.

What do you think?",ah yes see mean sense agree another issue one idea level teacher forcing place training suspect issue due synthesizer deviation true mel frame going compound get fed next mel frame teacher forcing training convergence ability would expect network would never learn account always fed real data training hence probability spectrogram frame teacher forced might get best think,issue,positive,positive,positive,positive,positive,positive
513382728,"I wasn't talking about the vocoder. Tacotron's decoder being autoregressive, the first stop token above the threshold value will be predicted when the spectrogram is done being generated, by definition. Thus is has no impact on previous frames, in fact its output is not fed back to the model IIRC. I don't see how the stop token could be the issue.",talking first stop token threshold value spectrogram done definition thus impact previous fact output fed back model see stop token could issue,issue,negative,positive,neutral,neutral,positive,positive
513372558,"I was referring to the stop prediction in Tacotron 2 (synthesizer not vocoder), I wasn't aware that stop prediction was used in WaveRNN as it can just stop outputting when it runs out of spectrogram frames to condition on.

What do you mean by ""the stop prediction only occurs after the spectrogram is generated""?",stop prediction synthesizer aware stop prediction used stop spectrogram condition mean stop prediction spectrogram,issue,negative,negative,neutral,neutral,negative,negative
513322005,"I doubt it's because of the stop prediction. The stop prediction only occurs after the spectrogram is generated. Yes, this is an issue of the synthesizer. It would have to be replaced by a better one (eliminating other problems with that) such as fatchord's, but I just don't have the time to do it.",doubt stop prediction stop prediction spectrogram yes issue synthesizer would better one time,issue,negative,positive,positive,positive,positive,positive
513310617,"Does anyone have some five second sound clips that they ran through this implementation that they think are really good and be willing to share those?  I've tried some five second clips of several celebrities and words are frequently dropped from the output .wav and also there's a ""windy"" type noise that fills the gaps between words.",anyone five second sound clip ran implementation think really good willing share tried five second clip several frequently output also windy type noise,issue,positive,positive,positive,positive,positive,positive
513228908,That's not nearly enough to learn about the variations in speakers. Especially not on a hard language such as Chinese.,nearly enough learn especially hard language,issue,negative,negative,negative,negative,negative,negative
513228384,There's an open 12-hour Chinese female voice set from databaker that I tried with tacotron <https://github.com/boltomli/tacotron/blob/zh/TRAINING_DATA.md#data-baker-data>. Hope that I can gather more Chinese speakers to have a try on voice cloning. I'll update if I have some progress.,open female voice set tried hope gather try voice update progress,issue,positive,neutral,neutral,neutral,neutral,neutral
513187918,Are you sure you have the latest commit and that your vocoder directoy is exactly what is currently on the master branch?,sure latest commit exactly currently master branch,issue,positive,positive,positive,positive,positive,positive
513187629,"You'd want hundreds of speakers at least. In fact, LibriSpeech-clean makes for 460 speakers and it's still not enough.",want least fact still enough,issue,negative,negative,negative,negative,negative,negative
512766561,The vocoder is dependent on the synthesizer if you are training with GTA synthesized mel spectrograms (the default). This dependency is only lost if you train on ground truth spectrograms.,dependent synthesizer training mel default dependency lost train ground truth,issue,negative,neutral,neutral,neutral,neutral,neutral
512620000,"Thank you, for your response, I will close this now, but anyone can continue related discussions if they want to",thank response close anyone continue related want,issue,negative,neutral,neutral,neutral,neutral,neutral
512574702,Think I figured out my hardware can't run the nvidia stuff,think figured hardware ca run stuff,issue,negative,neutral,neutral,neutral,neutral,neutral
512087361,"> > You'll need a good dataset (at least ~300 hours, high quality and transcripts) in the language of your choice, do you have that?
> 
> Maybe it can be hacked by using audio book and they pdf2text version. The difficult come i guess from the level of expression on data sources. Maybe with some movies but sometimes subtitles are really poor. Firefox work on dataset to if i remember well

This is something that I have been slowly piecing together. I have been gathering audiobooks and their text versions that are in the public domain (Project Gutenberg & LibriVox Recordings). My goal as of now is to develop a solid package that can gather an audiofile and corresponding book, performing necessary cleaning and such.

Currently this project lives on my C:, but if there's interest in collaboration I'd gladly throw it here on GitHub.",need good least high quality language choice maybe hacked audio book version difficult come guess level expression data maybe sometimes really poor work remember well something slowly piecing together gathering text public domain project goal develop solid package gather corresponding book necessary cleaning currently project interest collaboration gladly throw,issue,negative,negative,neutral,neutral,negative,negative
511480095,"in this link  https://github.com/NVIDIA/tacotron2/issues/162 ,Tugstugi said
he could fine-tune a Mongolian TTS using pre-trained English models.,I
think this is a very interesting thing.,But I'm a beginner. I don't know
how to start.Can this give you some inspiration?

Corentin Jemine <notifications@github.com> 于2019年7月16日周二 上午12:37写道：

> Thanks.
>
> The quality of the final product is not that good, mostly due to the time
> limits that I had. The pauses are introduced by the synthesizer, not the
> vocoder. You can convince yourself of that by using griffin lim as vocoder.
>
> The output volume is a bug that is worth investigating. Essentially, the
> vocoder produces a few samples that are above 1 in absolute value so it
> screws with the normalization. Even when clamping all samples to [-1, 1],
> the issue persists.
>
> If you want to finetune to single speaker, you should use another repo
> such as fatchord's or rayahane's, because this one is about voice cloning.
> For training on a specific dataset, you will need to retrain both the
> synthesizer and the vocoder. If in another language, the speaker encoder
> will need to be retrained as well.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/48?email_source=notifications&email_token=ACRHKKDH4BXOFJQGMIA7OV3P7SRT7A5CNFSM4IDW6IPKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZ6IEXI#issuecomment-511476317>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ACRHKKDJAQ7NTFNCPFQDISLP7SRT7ANCNFSM4IDW6IPA>
> .
>
",link said could think interesting beginner know give inspiration thanks quality final product good mostly due time synthesizer convince griffin lim output volume bug worth investigating essentially absolute value normalization even issue want single speaker use another one voice training specific need retrain synthesizer another language speaker need well thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
511476317,"Thanks. 

The quality of the final product is not that good, mostly due to the time limits that I had. The pauses are introduced by the synthesizer, not the vocoder. You can convince yourself of that by using griffin lim as vocoder. 

The output volume is a bug that is worth investigating. Essentially, the vocoder produces a few samples that are above 1 in absolute value so it screws with the normalization. Even when clamping all samples to [-1, 1], the issue persists.

If you want to finetune to single speaker, you should use another repo such as fatchord's or rayahane's, because this one is about voice cloning. For training on a specific dataset, you will need to retrain both the synthesizer and the vocoder. If in another language, the speaker encoder will need to be retrained as well.",thanks quality final product good mostly due time synthesizer convince griffin lim output volume bug worth investigating essentially absolute value normalization even issue want single speaker use another one voice training specific need retrain synthesizer another language speaker need well,issue,positive,positive,positive,positive,positive,positive
511349973,I seem to solve this problem. It turned out that my original data has an illegal speaker folder. Thank you for helping me debug.,seem solve problem turned original data illegal speaker folder thank helping,issue,negative,negative,neutral,neutral,negative,negative
511345328,I confirmed this several times and the processed Librispeech file is there.,confirmed several time file,issue,negative,positive,positive,positive,positive,positive
511343081,Can you check the audio files in your datasets and check that they are indeed there?,check audio check indeed,issue,negative,neutral,neutral,neutral,neutral,neutral
511218063,"Thanks for the quick response! And that's a bit disappointing (but understandable), because this tool is very easy to set up and use, unlike the other ""voice conversion"" tools I've seen. If you know of a repo like this one (as in easy to setup and has pretrained models and stuff), please let me know. I've always wanted to try out something like that.",thanks quick response bit disappointing understandable tool easy set use unlike voice conversion seen know like one easy setup stuff please let know always try something like,issue,positive,positive,positive,positive,positive,positive
511217717,"This is called voice conversion and it's essentially a form of style transfer, usually applied at the spectrogram level. 

There are proofs of concept here and there:
https://github.com/leimao/Voice_Converter_CycleGAN
https://github.com/hujinsen/StarGAN-Voice-Conversion

It is however beyond the scope of this repo.",voice conversion essentially form style transfer usually applied spectrogram level concept however beyond scope,issue,negative,negative,negative,negative,negative,negative
511105118,"Thanks. You're supposed to gather all the parts of each VoxCeleb dataset, than you can extract them as if it was one big zip file. It's a feature of zip for large archives to allow you to split them in multiple sub archives.

As for how the training works, no, you're supposed to run the scripts. The toolbox is just for demonstration purposes. Refer to the [wiki](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training).",thanks supposed gather extract one big zip file feature zip large allow split multiple sub training work supposed run toolbox demonstration refer,issue,positive,positive,positive,positive,positive,positive
511104828,"Oh yeah it's a bit unclear there. It says that I experimented with them and that I report my results in section 3.5.2, but the experimenting part was more about seeing if I could get it to run faster by pruning, not improving the quality. In Kalchbrenner et al. they mention both improvements to speed and quality, although quality seems to be their main concern. My conclusion is this paragraph:

> Sparse tensors are, at the time of writing, yet an experimental feature in PyTorch. Their implementation might not be as efficient as the one the authors used. Through experiments, we find that the matrix multiply operation addmm for a sparse matrix and a dense vector only breaks even time-wise with the dense-only addmm for levels of sparsity above 91%. Below this value, using sparse tensors will actually slow down the forward pass speed. The authors report sparsity levels of 96.4% and 97.8% (Kalchbrenner et al., 2018, Table 5) while maintaining decent performances. Our tests indicate that, at best, a sparsity level of 96.4% would lower the real-time threshold to 7.86 seconds, and a level of 97.8% to 4.44 seconds. These are optimistic lower bounds on the actual threshold due to our assumption of constant time inference, and also because some layers in the model cannot be sparsified. This preliminary analysis indicates that pruning the vocoder would be beneficial to inference speed.

tl;dr: pruning might bring a small improvement. 

So I did not actually train a full model with pruning, what I did was replacing the layers in the alternative WaveRNN with sparse layers and I looked at the inference speed for various levels of sparsity. If you want a training script with pruning, I'd recommend looking at fatchord's repo.",oh yeah bit unclear experimented report section part seeing could get run faster pruning improving quality al mention speed quality although quality main concern conclusion paragraph sparse time writing yet experimental feature implementation might efficient one used find matrix multiply operation sparse matrix dense vector even sparsity value sparse actually slow forward pas speed report sparsity table decent indicate best sparsity level would lower threshold level optimistic lower actual threshold due assumption constant time inference also model preliminary analysis pruning would beneficial inference speed pruning might bring small improvement actually train full model pruning alternative sparse inference speed various sparsity want training script pruning recommend looking,issue,positive,positive,neutral,neutral,positive,positive
511098799,"@CorentinJ  you mentioned in your thesis that you tried to train the WaveRNN with pruning algorithm when doing this thesis. But you didn't get a complete and fine-trained model due to time limitation. I wonder if you are still doing this part? If not, have you ever thought about releasing related scripts?",thesis tried train pruning algorithm thesis get complete model due time limitation wonder still part ever thought related,issue,negative,negative,neutral,neutral,negative,negative
510887098,Thanks. What's your python version? Do your packages meet the minimum requirements in the requirements.txt?,thanks python version meet minimum,issue,negative,positive,positive,positive,positive,positive
510886753,"I tried every type of path.
From windows like to unix like.

Absolut filepath
c:\path\to\file
/path/to/file
also a mixture:
c:/path/to/file

also local file path
path/to/file
file
to/file",tried every type path like like also mixture also local file path file,issue,positive,neutral,neutral,neutral,neutral,neutral
510882677,@xw1324832579 the first error complains it cannot connect to display :0. have you shared the X11-unix socket as volume in the docker run command and adjusted your image build for GUI applications as I mentioned with a link at the end of my [comment](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/11#issuecomment-508438273)? this should solve the first & possibly the follow-up errors...,first error connect display socket volume docker run command image build link end comment solve first possibly,issue,negative,positive,positive,positive,positive,positive
510881590,"I can't reproduce this unfortunately, I'm running a win10 install on two computers (one with 2gb VRAM) and tensorflow-gpu=1.13.1. I'm going to need more information to look into it.",ca reproduce unfortunately running win install two one going need information look,issue,negative,positive,positive,positive,positive,positive
510872745,"Hi @DrStoop @CorentinJ   
I tested PyQt5 within a docker env, but the same error occurs to me. Details are showing in the following picture.
![image](https://user-images.githubusercontent.com/42728055/61128639-07fd0800-a4e5-11e9-9890-ddccac2639f2.png)
The errors are presented in the bellow:
![image](https://user-images.githubusercontent.com/42728055/61128675-29f68a80-a4e5-11e9-8fa4-98f3f3c2a388.png)
I succeeded in installing libxcb as your instructions. Could you give me some guidance?  ",hi tested within docker error showing following picture image bellow image could give guidance,issue,negative,neutral,neutral,neutral,neutral,neutral
510827799,"The loss was faster on CPU before it was properly vectorized, because it was a lot of back-and-forth operations with the device. The parameters that affect this are the number of speakers per batch and the number of utterances per speaker. Now that it has been vectorized, the loss should be just as fast on GPU, and for the sake of simplicity it would be much better to only have one device, but for reasons unknown the loss is incorrect (it is `None` IIRC) when computed on GPU, so I've left it to be computed on the CPU while the forward pass (a recurrent neural network) is much faster on GPU. I still have to figure out what that bug is.",loss faster properly lot device affect number per batch number per speaker loss fast sake simplicity would much better one device unknown loss incorrect none left forward pas recurrent neural network much faster still figure bug,issue,negative,positive,positive,positive,positive,positive
510354042,"When you get stucked on ""Interactive generation loop"",  you should input your own audio file, then the code will work, you can read the source code . But I don't know why the message didn't work.
",get interactive generation loop input audio file code work read source code know message work,issue,negative,neutral,neutral,neutral,neutral,neutral
510330564,"No. Either the model runs and it works as good as it can, either it doesn't run.",either model work good either run,issue,negative,positive,positive,positive,positive,positive
510326763,Could this be the reason that my cloned voice sounds different from the speaker?,could reason voice different speaker,issue,negative,neutral,neutral,neutral,neutral,neutral
510326176,"It shouldn't matter, but tensorflow can be very painful to install (and to use, really it's no surprise I want to move to a pytorch-only implementation).

My version of tensorflow is 1.13.1.",matter painful install use really surprise want move implementation version,issue,negative,negative,negative,negative,negative,negative
510323973,"> The speaker has a very thick voice, but the cloned result sounds like a normal person.

Yes, the synthesizer is only trained to output a voice at all times. Even if you input noise as reference or a piece of music, you will get an ""average human"" voice as output. That's normal. It also means that you're giving the model a reference audio it has not learned to generalize with. Either:

- The speaker encoder cannot create a meaningful embedding of that voice, because it has not generalized to it.

- The speaker encoder generates a meaningful embedding of that voice, but the synthesizer is not able to properly reproduce the voice in the embedding, because it has not generalized to it.

More often than not it's going to be the second case. The speaker encoder I've provided is excellent at its job, but the synthesizer and vocoder were trained on a limited dataset, and suffer from a few limitations (such as not using phonemes, using r=2, using location-sensitive attention - note, this limits the quality, not the voice cloning ability).

> May be you could try split the voice into five seconds pieces, and get embds of all these pieces, using the mean embd of embds,then synthesis.I don't know if it's help, may be you could have a try.

This is actually what happens internally, up to a normalizing constant. The resulting embedding is the average of the frame-level embeddings (called d-vectors) from a sliding window of 1.6 seconds (with a stride of 800ms, so 50% overlap):
![image](https://user-images.githubusercontent.com/12038136/61021890-ae40f480-a3a3-11e9-9594-c6fcadcb10ac.png)
If you pass a short input, you're going to have a small number of embeddings to average from, e.g. between 5 and 7. If you pass a very long input, it's going to make an average of a lot more embeddings, and that will be problematic if *there's a lot of variation of the voice in your audio*. If there isn't, and the voice is fairly consistent, then it's fine; but it's not likely going to improve the performance of the model due to what I said above.

This might slightly be improved by using speaker embeddings instead of utterance embeddings, it's something in my TODO list. Excerpt from my thesis:

> In SV2TTS, the embeddings used to condition the synthesizer at training time are speaker embeddings. We argue that utterance embeddings of the same target utterance make for a more natural choice instead. At inference time, utterance embeddings are also used. While the space of utterance and speaker embeddings is the same, speaker embeddings are not L2-normalized. This difference in domain should be small and have little impact on the synthesizer that uses the embedding, as the authors agreed when we asked them about it. However, they do not mention how many utterance embeddings are used to derive a speaker embedding. One would expect that all utterances available should be used; but with a larger number of utterance embeddings, the average vector (the speaker embedding) will further stray from its normalized version. Furthermore, the authors mention themselves that there are often large variations of tone and pitch within the utterances of a same speaker in the dataset, as they mimic different characters (Jia et al., 2018, Appendix B). Utterances have lower intra-variation, as their scope is limited to a sentence at most. Therefore, the embedding of an utterance is expected to be a more accurate representation of the voice spoken in the utterance than the embedding of the speaker. This holds if the utterance is long enough than to produce a meaningful embedding. While the “optimal” duration of reference speech was found to be 5 seconds, the embedding is shown to be already meaningful with only 2 seconds of reference speech (see table 4). We believe that with utterances no shorter than the duration of partial utterances (1.6s), the utterance embedding should be sufficient for a meaningful capture of the voice, hence we used utterance embeddings for training the synthesizer.

Funnily enough my ""interactions with the authors"" was in the youtube comment section:
![image](https://user-images.githubusercontent.com/12038136/61022479-3c1ddf00-a3a6-11e9-8f01-11b01268bb3e.png)
https://www.youtube.com/watch?v=AkCPHw2m6bY

I'm going to reopen the issue because, well, it's a limitation of the framework (see the original paper, the voice cloning ability is fairly limited), of my models and of the datasets; so it's going to be a long-lasting issue that other people will want to know about.",speaker thick voice result like normal person yes synthesizer trained output voice time even input noise reference piece music get average human voice output normal also giving model reference audio learned generalize either speaker create meaningful voice generalized speaker meaningful voice synthesizer able properly reproduce voice generalized often going second case speaker provided excellent job synthesizer trained limited suffer attention note quality voice ability may could try split voice five get mean know help may could try actually internally constant resulting average sliding window stride overlap image pas short input going small number average pas long input going make average lot problematic lot variation voice audio voice fairly consistent fine likely going improve performance model due said might slightly speaker instead utterance something list excerpt thesis used condition synthesizer training time speaker argue utterance target utterance make natural choice instead inference time utterance also used space utterance speaker speaker difference domain small little impact synthesizer agreed however mention many utterance used derive speaker one would expect available used number utterance average vector speaker stray version furthermore mention often large tone pitch within speaker mimic different appendix lower scope limited sentence therefore utterance accurate representation voice spoken utterance speaker utterance long enough produce meaningful optimal duration reference speech found shown already meaningful reference speech see table believe shorter duration partial utterance sufficient meaningful capture voice hence used utterance training synthesizer funnily enough comment section image going reopen issue well limitation framework see original paper voice ability fairly limited going issue people want know,issue,positive,positive,neutral,neutral,positive,positive
510289888,"> The speaker has a very thick voice, but the cloned result sounds like a normal person.

May be you could try split the voice into five seconds pieces, and get embds of all these pieces, using the mean embd of embds,then synthesis.I don't know if it's help, may be you could have a try. ",speaker thick voice result like normal person may could try split voice five get mean know help may could try,issue,positive,negative,negative,negative,negative,negative
510277667,"The speaker has a very thick voice, but the cloned result sounds like a normal person.",speaker thick voice result like normal person,issue,negative,negative,neutral,neutral,negative,negative
510267836,I got the same error and then a Failed to get convolution algorithm error so I gave up and just installed anaconda and got it working from there. took me 2 days trying and failing so I'd suggest just installing anaconda and be done with it. ,got error get convolution algorithm error gave anaconda got working took day trying failing suggest anaconda done,issue,negative,neutral,neutral,neutral,neutral,neutral
510114305,I still run into the same issue as well with the `--low_mem` flag.,still run issue well flag,issue,negative,neutral,neutral,neutral,neutral,neutral
510078814,"My voice works poorly with the model, others work nice. I would not recommend using a 30 mins audio file. While technically it should work, the framework is meant to operate with a short input speech. Try cutting only 5 seconds from that audio. If the speaker is not a native English speaker, there's a good chance the resulting voice will be poorly cloned.",voice work poorly model work nice would recommend audio file technically work framework meant operate short input speech try cutting audio speaker native speaker good chance resulting voice poorly,issue,positive,negative,neutral,neutral,negative,negative
510077472,"Are you using the toolbox to clone your voice? I tried to clone from a 30 mins audio file of a single speaker by loading this audio file to toolbox, but the resulting voice is not quite similar to the input.",toolbox clone voice tried clone audio file single speaker loading audio file toolbox resulting voice quite similar input,issue,negative,negative,neutral,neutral,negative,negative
510054957,"Given that I was short for the deadline, I didn't give it much of chance. I tried it once and gave up when I saw the loss plateaued with no alignments. I don't know for sure why it didn't converge, and I didn't want to waste time looking into it. I think LibriTTS is worth a shot, and I would also recommend using a different synthesizer implementation than mine if you're going that way, because it is fairly poor right now. 

I think I hadn't yet implemented the splitting on sentences when trying out LibriTTS. Logmmse (the noise removal algorithm) also came later.

Note that my commits before going public are [available](https://github.com/CorentinJ/Real-Time-Voice-Cloning/commits/master?after=2e8ef14bf7865c7e3828219236bac3b098d07584+69). You should be able to find more answers there. 9692bea9320cb18b988472b1c9aa0d195326c6a0 is the one where I changed back from LibriTTS to LibriSpeech. ",given short deadline give much chance tried gave saw loss know sure converge want waste time looking think worth shot would also recommend different synthesizer implementation mine going way fairly poor right think yet splitting trying noise removal algorithm also came later note going public available able find one back,issue,negative,positive,positive,positive,positive,positive
510015040,"New branch code without ""--low_mem"" flag get stucked on ""Interactive generation loop"" after ""all test passed"".
Output:

> W0710 12:49:35.993566 140170637424384 deprecation.py:323] From /Real-Time-Voice-Cloning/env/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
	Testing the vocoder...
All test passed! You can now synthesize speech.
This is a GUI-less example of interface to SV2TTS. The purpose of this script is to show how you can interface this project easily with your own. See the source code for an explanation of what is happening.
Interactive generation loop

With ""--low_mem"" flag same cuda init error.
Output:
https://pastebin.com/PFBEbEnz

",new branch code without flag get interactive generation loop test output removed future version use standard file check prefix testing test synthesize speech example interface purpose script show interface project easily see source code explanation happening interactive generation loop flag error output,issue,negative,positive,positive,positive,positive,positive
510004804,Can you try pulling from [this new branch](https://github.com/CorentinJ/Real-Time-Voice-Cloning/tree/low_mem_fix) and see what gives?,try new branch see,issue,negative,positive,positive,positive,positive,positive
509943857,"My implementation of the synthesizer and of the vocoder aren't that great, and I've also trained on LibriSpeech when LibriTTS would have been preferable. I think fatchord's WaveRNN is very good still, and I wouldn't change it for another vocoder right now.

If someone were to try to seriously improve on the quality, I would recommend using both his synthesizer and vocoder instead of the ones I currently have, and to train on LibriTTS.

While I would be willing to help this come to fruition in my repo, I cannot afford to work fulltime on it, unfortunately.",implementation synthesizer great also trained would preferable think good still would change another right someone try seriously improve quality would recommend synthesizer instead currently train would willing help come fruition afford work unfortunately,issue,positive,positive,positive,positive,positive,positive
509317519,"I believe the part of my implementation that would need work is the synthesizer. Unfortunately I haven't much time nor GPU power to work on a better implementation. I had planned to look at that mozilla implementation, maybe I'll find some interesting things there.",believe part implementation would need work synthesizer unfortunately much time power work better implementation look implementation maybe find interesting,issue,negative,positive,positive,positive,positive,positive
509312738,"Thanks for the clarification. I agree the mel-spectrogram should contain sufficient information without explicit speaker conditioning for vocoder.

Just found that a universal wavernn vocoder has been implemented here, trained on LibriTTS: https://github.com/mozilla/TTS/issues/221. With the same Fatcord WaveRNN model you used.

The out-of-sample audio samples sounds decently well: https://soundcloud.com/user-565970875/sets/universal-vocoder-with-wavernn However it still exhibit some small artifacts/noises, which hopefully with small samples fine-tune will fix.",thanks clarification agree contain sufficient information without explicit speaker found universal trained model used audio decently well however still exhibit small hopefully small fix,issue,positive,negative,neutral,neutral,negative,negative
509293411,"Thanks.

The vocoder is not conditioned on anything other than the mel spectrograms, as also stated in [the SV2TTS paper](https://arxiv.org/pdf/1806.04558.pdf), section 2.3:
> The network is not directly conditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer network captures all of the relevant detail needed for high quality synthesis of a variety of voices, allowing a multispeaker vocoder to be constructed by simply training on data from many speakers.

You can also see it on this diagram:
![image](https://user-images.githubusercontent.com/12038136/60825600-5c824980-a1ac-11e9-8036-5edc58bff9d3.png)

I don't know about an explicit conditioning on the vocoder, I think that all the necessary features are generated from the synthesizer alone. I did however think about indirect conditioning by finetuning all three models at once in a complete end-to-end training loop. Then I remembered that my implementation is Frankenstein-worthy with its mix of pytorch and tensorflow, and I quickly gave up on that idea.",thanks conditioned anything mel also stated paper section network directly conditioned output speaker mel spectrogram synthesizer network relevant detail high quality synthesis variety simply training data many also see diagram image know explicit think necessary synthesizer alone however think indirect three complete training loop implementation mix quickly gave idea,issue,negative,positive,positive,positive,positive,positive
509001953,"Same problem. 

""demo_cli.py"" output without ""--low_mem"" flag:
`Found 1 GPUs available. Using GPU 0 (GeForce MX150) of compute capability 6.1 with 2.1Gb total memory.
`
...
`Use standard file APIs to check for files with this prefix.
	Testing the vocoder...
All test passed! You can now synthesize speech.
`

""demo_cli.py"" output with ""--low_mem"" flag:

`Use standard file APIs to check for files with this prefix.
E0707 15:55:23.503854 139813249828608 driver.py:321] Call to cuInit results in CUDA_ERROR_NOT_INITIALIZED
`
...
`multiprocess.pool.RemoteTraceback: 
/python3.6/site-packages/numba/cuda/cudadrv/driver.py"", line 233, in initialize
    raise CudaSupportError(""Error at driver init: \n%s:"" % e)
numba.cuda.cudadrv.error.CudaSupportError: Error at driver init: 
[3] Call to cuInit results in CUDA_ERROR_NOT_INITIALIZED:
multiprocess/pool.py"", line 644, in get
    raise self._value
numba.cuda.cudadrv.error.CudaSupportError: Error at driver init: 
[3] Call to cuInit results in CUDA_ERROR_NOT_INITIALIZED:
`",problem output without flag found available compute capability total memory use standard file check prefix testing test synthesize speech output flag use standard file check prefix call line initialize raise error driver error driver call line get raise error driver call,issue,negative,positive,neutral,neutral,positive,positive
508966471,"Yes, I am running the exact code from master branch. And you are right that this is a warning. But is this warning normal? Any impact on the result performance?",yes running exact code master branch right warning warning normal impact result performance,issue,negative,positive,positive,positive,positive,positive
508966336,"Are you running the exact same code I provided or did you modify it? Also, if all test passed then what you're seeing are warnings, not errors.",running exact code provided modify also test seeing,issue,negative,positive,positive,positive,positive,positive
508966191,I got the same error when running `python demo_cli.py`. But it still says `All test passed!`,got error running python still test,issue,negative,neutral,neutral,neutral,neutral,neutral
508800424,"> You'll need a good dataset (at least ~300 hours, high quality and transcripts) in the language of your choice, do you have that?

Maybe it can be hacked by using audio book and they pdf2text version. The difficult come i guess from the level of expression on data sources. Maybe with some movies but sometimes subtitles are really poor. Firefox work on dataset to if i remember well",need good least high quality language choice maybe hacked audio book version difficult come guess level expression data maybe sometimes really poor work remember well,issue,negative,negative,neutral,neutral,negative,negative
508745317,"Thank you. However, I already have code to save audio and it's much simpler than that. E.g.: https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/vocoder/audio.py#L21
The TODO is just there to say that I need to put a button in the toolbox that does that.",thank however already code save audio much simpler say need put button toolbox,issue,positive,positive,positive,positive,positive,positive
508533885,"Without the low_mem flag, it says that all tests pass. With the low_mem flag, I get the same CUDA_ERROR_NOT_INITIALIZED.",without flag pas flag get,issue,negative,neutral,neutral,neutral,neutral,neutral
508470964,"I don't mind uploading the pretrained models somewhere else, but not on a website in a language I don't understand that requires me to create an account.",mind somewhere else language understand create account,issue,negative,neutral,neutral,neutral,neutral,neutral
508438273,"@MorganCZY, you got that right, [it](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/11#issuecomment-508425030) is exactly how I turned on the debug mode, just the output looked different :)...

To provoke the error I did not run the full demo_toolbox.py script but just a small code snippet what you also just tried. As I [tracked down the error to QApplication() before with `faulthandler`](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/11#issuecomment-504669708) I caused the errors in a Python console with this small code snippet from the description [above](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/11#issuecomment-504733590), so I didn't have to rerun the full toolbox (q&d):
```python
$ python
Python 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) 
[GCC 7.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from PyQt5.QtWidgets import QApplication, QLabel
>>> app = QApplication([])
```
Possibly you don't need GUI support for this snippet, but I cannot really tell.

Regarding your GUI-issue, I also can't really tell from here how your system looks like. In case you're using a Docker container, take a look at [this description](http://fabiorehm.com/blog/2014/09/11/running-gui-apps-with-docker/) to enable GUIs on your host desktop.
",got right exactly turned mode output different provoke error run full script small code snippet also tried tracked error python console small code snippet description rerun full toolbox python python python default type help copyright license information import possibly need support snippet really tell regarding also ca really tell system like case docker container take look description enable host,issue,negative,positive,positive,positive,positive,positive
508427242,"I wrote a piece of simple code to create a window, but getting an error""qt.qpa.screen: QXcbConnection: Could not connect to display     Could not connect to any X display.""
![image](https://user-images.githubusercontent.com/33675916/60659131-39dbf200-9e87-11e9-9a94-e308bd0046eb.png)
Does it mean this server has no tools for displaying GUI codes. If so, what should I install?",wrote piece simple code create window getting error could connect display could connect display image mean server install,issue,negative,negative,negative,negative,negative,negative
508425030,"@DrStoop Following your guidance, I didn't get more debug info.
![image](https://user-images.githubusercontent.com/33675916/60658694-49a70680-9e86-11e9-9736-1dce6cf72fba.png)
Do I correctly and fully get your point? ",following guidance get image correctly fully get point,issue,negative,neutral,neutral,neutral,neutral,neutral
508423012,"Hi @MorganCZY, have you set the environment variable in your terminal with the command `export QT_DEBUG_PLUGINS=1`? Your output still does not include any pyqt-debug messages which tell you what libraries couldn't be found and need installing (e.g. proposed [here](https://stackoverflow.com/a/39725335/9856685) or [here](https://forum.qt.io/topic/93247/qt-qpa-plugin-could-not-load-the-qt-platform-plugin-xcb-in-even-though-it-was-found)). Just set the qt-debug-flag in your terminal with the command 
```sh
$ export QT_DEBUG_PLUGINS=1
```
and rerun the demo_toolbox.py in the same terminal. This should return a lot of debug messages as listed above and you can proceed with the solution also described there (finding missing libs, installing missing libs, rerun, repeat). Hope that helps!",hi set environment variable terminal command export output still include tell could found need set terminal command sh export rerun terminal return lot listed proceed solution also finding missing missing rerun repeat hope,issue,negative,negative,negative,negative,negative,negative
508401936,"I am trying to run demo_toolbox.py on a server(Ubuntu 16.04.4 LTS), but the same problem occurs to me. 
![image](https://user-images.githubusercontent.com/33675916/60653285-545c9e00-9e7c-11e9-81d0-f0c4c13ceee8.png)
I didn't get the method to fix it even after reading your discussions. Could you simply tell me what should I do or what else should be installed?  ",trying run server problem image get method fix even reading could simply tell else,issue,negative,neutral,neutral,neutral,neutral,neutral
508392600,@CorentinJ Thanks. I fixed this issue by replacing PyQt5 5.12.3 version with 5.11.3 verison. It seems the error is caused by PyQt5 implementation itself.,thanks fixed issue version error implementation,issue,negative,positive,positive,positive,positive,positive
508390284,"I assume there's some conflict with another package. What about these threads:
https://forums.linuxmint.com/viewtopic.php?t=291157  
http://eab.abime.net/showthread.php?t=96856  ",assume conflict another package,issue,negative,neutral,neutral,neutral,neutral,neutral
508381648,"From [here](https://matheo.uliege.be/bitstream/2268.2/6801/5/s123578Jemine2019.pdf#page=12):

> A particularity of the SV2TTS framework is that all models can be trained
separately and on distinct datasets. For the encoder, one seeks to have a model
that is robust to noise and able to capture the many characteristics of the human
voice. Therefore, a large corpus of many different speakers would be preferable to
train the encoder, without any strong requirement on the noise level of the audios.
Additionally, the encoder is trained with the GE2E loss which requires no labels other
than the speaker identity. (...) For the datasets of the synthesizer and the vocoder, 
transcripts are required and the quality of the generated audio can only be as good 
as that of the data. Higher quality and annotated datasets are thus required, which 
often means they are smaller in size.

You'll need two datasets:
![image](https://user-images.githubusercontent.com/12038136/60648823-0c6c5580-9e41-11e9-9b3a-737379481824.png)

The first one should be a large dataset of untranscribed audio that can be noisy. Think thousands of speakers and thousands of hours. You can get away with a smaller one if you finetune the pretrained speaker encoder. Put maybe `1e-5` as learning rate. I'd recommend 500 speakers at the very least for finetuning. A good source for datasets of other languages is [M-AILABS](https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/).

The second one needs audio transcripts and high quality audio. Here, finetuning won't be as effective as for the encoder, but you can get away with less data (300-500 hours). You will likely not have the alignments for that dataset, so you'll have to adapt the preprocessing procedure of the synthesizer to not split audio on silences. See the code and you'll understand what I mean.

Don't start training the encoder if you don't have a dataset for the synthesizer/vocoder, you won't be able to do anything then.
",particularity framework trained separately distinct one model robust noise able capture many human voice therefore large corpus many different would preferable train without strong requirement noise level additionally trained gee loss speaker identity synthesizer quality audio good data higher quality thus often smaller size need two image first one large untranscribed audio noisy think get away smaller one speaker put maybe learning rate recommend least good source second one need audio high quality audio wo effective get away le data likely adapt procedure synthesizer split audio see code understand mean start training wo able anything,issue,positive,positive,positive,positive,positive,positive
508310086,I wanna train another language. How many speakers do I need in the Encoder? or can I use the  English speaker embeddings  to my language?,wan na train another language many need use speaker language,issue,negative,positive,positive,positive,positive,positive
508233123,I'd recommend so. The difference isn't going to be big unless you have a lot of utterances anyway.,recommend difference going big unless lot anyway,issue,negative,neutral,neutral,neutral,neutral,neutral
508224417,"""Embeddings are L2-normalized (this isn't important here, but if you want to make your own  # embeddings it will be).""

So if I have a new speaker where I want to use its embedding, do I L2 normalize it or not?",important want make new speaker want use normalize,issue,negative,positive,positive,positive,positive,positive
508140352,"You can read my thesis or the GE2E paper for a formal explanation, but what happens is that the mel spectrogram of an utterance is split in chunks of constant duration and an embedding is computed for each. The normalized average of these embeddings is the **utterance embedding**. You can average multiple utterance embeddings to form a **speaker embedding** (note that this speaker embedding is not normalized as per the GE2E paper, but if I ever implement them I would normalize them). A speaker embedding from a single utterance is exactly equal to that utterance's embedding, so I sometimes do not make the distinction. The code that handles this is `encoder/inference.py`. It is fairly well documented, so give it a look.

In the toolbox, the embedding is only from a single utterance (the currently selected one, which is usually the last one you loaded).",read thesis gee paper formal explanation mel spectrogram utterance split constant duration average utterance average multiple utterance form speaker note speaker per gee paper ever implement would normalize speaker single utterance exactly equal utterance sometimes make distinction code fairly well give look toolbox single utterance currently selected one usually last one loaded,issue,negative,positive,neutral,neutral,positive,positive
508010167,"Loading mono does not hang for me, are you also using something other than ffmpeg?",loading mono also something,issue,negative,neutral,neutral,neutral,neutral,neutral
508007457,For me it hangs on mono files whereas with stereo files everything loads up. ,mono whereas stereo everything,issue,negative,neutral,neutral,neutral,neutral,neutral
507959443,"You'll need a good dataset (at least ~300 hours, high quality and transcripts) in the language of your choice, do you have that?",need good least high quality language choice,issue,negative,positive,positive,positive,positive,positive
507910879,"Thanks for explaintation, I have big interest of adding other languages support, and would like to contribute.",thanks big interest support would like contribute,issue,positive,positive,neutral,neutral,positive,positive
507870363,"Oh yeah there's definitely supposed to be an `__init__.py` file there. I overlooked that, thanks. Fixing it now.",oh yeah definitely supposed file thanks fixing,issue,positive,positive,neutral,neutral,positive,positive
507864097,"You'll need to retrain with your own datasets to get another language running (and it's a lot of work). The speaker encoder is somewhat able to work on a few other languages than English because VoxCeleb is not purely English, but since the synthesizer/vocoder have been trained purely on English data, any voice that is not in English - and even, that does not have a proper English accent - will be cloned very poorly.",need retrain get another language running lot work speaker somewhat able work purely since trained purely data voice even proper accent poorly,issue,negative,positive,positive,positive,positive,positive
507844948,"Update: Inexplicably, this works on my server even without `__init__.py`. Both are anaconda 3.7, the only difference is the server is using CUDA, GPUS, and Ubuntu 18.04 and my laptop doesn't have GPUs and is MacOS. 

????",update inexplicably work server even without anaconda difference server,issue,negative,negative,negative,negative,negative,negative
507841384,"I have this same issue (py3.7). I didn't want to have to comment out the import, so I added an `__init__.py` file to the `utils` folder. @CorentinJ Does that sound like a reasonable fix to you? Python's import semantics are a mystery to me",issue want comment import added file folder sound like reasonable fix python import semantics mystery,issue,negative,positive,positive,positive,positive,positive
507641144,"Well there's that line in the error log:
```This may be caused by multiline strings or comments not indented at the same level as the code.```
It could be it.",well line error log may indented level could,issue,negative,neutral,neutral,neutral,neutral,neutral
507628473,"It is working if I load the new audio file using demo_toolbox. Is there something missing when I removed the UI part as in the code I attached?

I am trying to reproduce the result in Codlab, so cannot use the toolbox.",working load new audio file something missing removed part code attached trying reproduce result use toolbox,issue,negative,negative,neutral,neutral,negative,negative
507621873,"You need to use `pathlib.Path` for paths:

```
from synthesizer.inference import Synthesizer
from encoder import inference as encoder
from vocoder import inference as vocoder
from pathlib import Path
import numpy as np
import torch
import sys

enc_model_fpath = Path(""encoder/saved_models/pretrained.pt"")
syn_model_dir = Path(""synthesizer/saved_models/logs-pretrained/"")
voc_model_fpath = Path(""vocoder/saved_models/pretrained/pretrained.pt"")
low_mem = False

encoder.load_model(enc_model_fpath)
synthesizer = Synthesizer(syn_model_dir.joinpath(""taco_pretrained""), low_mem=low_mem, verbose=False)
vocoder.load_model(voc_model_fpath)

...
```

If you have modified the implementation to use something else, then I can't help you. This code works on my end.",need use import synthesizer import inference import inference import path import import torch import path path path false synthesizer synthesizer implementation use something else ca help code work end,issue,negative,negative,negative,negative,negative,negative
507102523,"@CorentinJ I see I have everything setup in colab but i think the problem is the demo_toolbox does not support non ui /running in cli yet?

I get this output running the demo_toolbox, demo_cli tests pass

WARNING: Logging before flag parsing goes to stderr.
W0701 03:24:00.135030 140347045406592 deprecation_wrapper.py:119] From /content/Real-Time-Voice-Cloning/synthesizer/models/modules.py:91: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.

Arguments:
    datasets_root:    /content/Real-Time-Voice-Cloning/UserAudio/speaker_01
    enc_models_dir:   encoder/saved_models
    syn_models_dir:   synthesizer/saved_models
    voc_models_dir:   vocoder/saved_models
    low_mem:          False

",see everything setup think problem support non yet get output running pas warning logging flag go name please use instead false,issue,negative,negative,negative,negative,negative,negative
507065190,"Not from me, but I would certainly not mind linking it if someone makes one. I think a notebook-ish demo is doable",would certainly mind linking someone one think doable,issue,negative,positive,positive,positive,positive,positive
507050592,"I really don't know why youtube is doing that. I assume this is based on your location. When I use online proxys it works fine.

I'll add that you should click on the picture in the readme.",really know assume based location use work fine add click picture,issue,negative,positive,positive,positive,positive,positive
507050392,"Yes I am talking about that link you've now posted. 
I will post a picture. Its in my ""Croatian language"" but you'll get the point
![Screenshot_1](https://user-images.githubusercontent.com/50591550/60399591-ed32a700-9b66-11e9-96d9-4ea5243b84ea.jpg)
.

P.S. And maybe it should be written somewere that to open video example you must click on a picture.
It took me a while to figure it out.",yes talking link posted post picture language get point maybe written open video example must click picture took figure,issue,negative,neutral,neutral,neutral,neutral,neutral
507050172,"Works fine for me. You're taking the link from the current readme, right (https://www.youtube.com/watch?v=-O_hYhToKoA)? The one that was initially put doesn't work anymore.",work fine taking link current right one initially put work,issue,negative,positive,positive,positive,positive,positive
507044502,Sorry I don't think that will happen. The current Tacotron2 implementation only supports GPU inference.,sorry think happen current implementation inference,issue,negative,negative,negative,negative,negative,negative
506988760,I dont see anything in demo_cli where you can load your own audio?,dont see anything load audio,issue,negative,neutral,neutral,neutral,neutral,neutral
506785361,looks like that library prefers gstreamer though.. kind of weird because for me (on Arch) gstreamer is a dependency of gnome-shell which would mean it's usually installed on Ubuntu as well,like library though kind weird arch dependency would mean usually well,issue,positive,negative,negative,negative,negative,negative
506784658,"Yes, ffmpeg is what I use and is typically the default framework people will use as well. I have an install on win10 and one on Ubuntu and in both cases I've been using ffmpeg.",yes use typically default framework people use well install win one,issue,positive,positive,positive,positive,positive,positive
506720671,"Looks like I can ""fix"" it by forcing librosa - audioread to use ffmpeg instead of gstreamer by commenting out this: https://github.com/beetbox/audioread/blob/6a8e7283f380498d28f94a60631b65d3fa623be9/audioread/__init__.py#L76-L78

Considering librosa.load still worked outside of your GUI though I think this is still an issue here and not with my installation of gstreamer etc.

Can you say which OS you are on and if you  have gstreamer installed? Maybe for you it always uses ffmpeg..",like fix forcing use instead considering still worked outside though think still issue installation say o maybe always,issue,negative,neutral,neutral,neutral,neutral,neutral
506714722,"It would definitely be a good idea to not run that kind of thing in the main UI thread, though I don't really know much about Qt so I can't tell if that would fix this..",would definitely good idea run kind thing main thread though really know much ca tell would fix,issue,positive,positive,positive,positive,positive,positive
506711619,"By the way, here is the stack of where it hangs within librosa (shows this when you press ctrl+c)

```
  File ""/home/tehdog/data/tmp/nobackup/pkg/Real-Time-Voice-Cloning/toolbox/__init__.py"", line 52, in <lambda>
    self.ui.browser_load_button.clicked.connect(lambda: self.load_from_browser())
  File ""/home/tehdog/data/tmp/nobackup/pkg/Real-Time-Voice-Cloning/toolbox/__init__.py"", line 110, in load_from_browser
    wav = Synthesizer.load_preprocess_wav(fpath)
  File ""/home/tehdog/data/tmp/nobackup/pkg/Real-Time-Voice-Cloning/synthesizer/inference.py"", line 111, in load_preprocess_wav
    wav = librosa.load(fpath, hparams.sample_rate)[0]
  File ""/home/tehdog/.local/lib/python3.7/site-packages/librosa/core/audio.py"", line 133, in load
    for frame in input_file:
  File ""/home/tehdog/.local/lib/python3.7/site-packages/audioread/gstdec.py"", line 371, in next
    val = self.queue.get()
  File ""/usr/lib/python3.7/queue.py"", line 170, in get
    self.not_empty.wait()
  File ""/usr/lib/python3.7/threading.py"", line 296, in wait
    waiter.acquire()
KeyboardInterrupt
```",way stack within press file line lambda lambda file line file line file line load frame file line next file line get file line wait,issue,negative,neutral,neutral,neutral,neutral,neutral
506685367,"My code also hangs for me even when the function doesn't do anything (just `queue.add(""hi"")`) - but when running the same thing in a script, (not from a Qt main thread) it works...",code also even function anything hi running thing script main thread work,issue,negative,positive,positive,positive,positive,positive
506685191,"This works:

```
def wrapped_load(*args, **kwargs):
    print(""starting load"")
    wav = librosa.load(*args, **kwargs)[0]
    print(""load done"")
    return wav.copy()

    ...

    @staticmethod
    def load_preprocess_wav(fpath):
        """"""
        Loads and preprocesses an audio file under the same conditions the audio files were used to
        train the synthesizer. 
        """"""
        print(fpath, hparams.sample_rate)
        wav = Pool(1).starmap(wrapped_load, [(fpath, hparams.sample_rate)])[0]
        print(f""loaded {wav.shape}"")
        if hparams.rescale:
            wav = wav / np.abs(wav).max() * hparams.rescaling_max
        return wav
```",work print starting load print load done return audio file audio used train print pool print loaded return,issue,negative,neutral,neutral,neutral,neutral,neutral
506684518,"Your code hangs for me too, regardless of whether I copy the output or not",code regardless whether copy output,issue,negative,neutral,neutral,neutral,neutral,neutral
506683365,"I've had multiprocessing hang for me as well, as soon as the task was over, but really it was for a different problem so I'm not sure it has anything to do with what we're seeing here. The problem that I had was that I was returning a numpy array that was shared elsewhere. I fixed it by copying the array and returning that instead. Don't know if that helps.",well soon task really different problem sure anything seeing problem array elsewhere fixed array instead know,issue,negative,positive,positive,positive,positive,positive
506681146,"I kind of expected this to fix it, but it just made it hang on `proc.join()`:

```diff
diff --git a/synthesizer/inference.py b/synthesizer/inference.py
index 99fb778..3f56d98 100644
--- a/synthesizer/inference.py
+++ b/synthesizer/inference.py
@@ -9,7 +9,13 @@ import tensorflow as tf
 import numpy as np
 import numba.cuda
 import librosa
+from multiprocessing import Process, Queue
 
+def wrapped_load(queue, *args, **kwargs):
+    print(""starting load"")
+    wav = librosa.load(*args, **kwargs)[0]
+    print(""load done"")
+    queue.put(wav)
 
 class Synthesizer:
     sample_rate = hparams.sample_rate
@@ -108,7 +114,15 @@ class Synthesizer:
         Loads and preprocesses an audio file under the same conditions the audio files were used to
         train the synthesizer. 
         """"""
-        wav = librosa.load(fpath, hparams.sample_rate)[0]
+        print(fpath, hparams.sample_rate)
+        queue = Queue()
+        proc = Process(target=wrapped_load, args=(queue, fpath, hparams.sample_rate))
+        proc.start()
+        print(f""proc started"")
+        proc.join()
+        print(f""proc joined"")
+        wav = queue.get()
+        print(f""loaded {wav.shape}"")
         if hparams.rescale:
             wav = wav / np.abs(wav).max() * hparams.rescaling_max
         return wav",kind fix made git index import import import import import process queue queue print starting load print load done class synthesizer class synthesizer audio file audio used train print queue queue process queue print print print loaded return,issue,positive,positive,positive,positive,positive,positive
506677953,"Yeah, yesterday when it worked for a short bit it loaded in maybe 3 seconds (both librispeech and my own .wav). Also, using recording from microphone works just fine. I'm guessing it's some race condition that has something todo with Qt. I'll try to see if it's fixed if i run librosa.load in a separate thread",yeah yesterday worked short bit loaded maybe also recording microphone work fine guessing race condition something try see fixed run separate thread,issue,negative,positive,positive,positive,positive,positive
506677447,I have to say I have no idea what it could be. I know that the encoder can take a very long time to load (25 seconds on my crappy laptop) but given what you're saying it doesn't seem to be it.,say idea could know take long time load given saying seem,issue,negative,negative,neutral,neutral,negative,negative
506676921,"Forgot to mention, demo_cli.py runs without problems. Also, I managed to get file loading in the GUI to work for a bit yesterday without really changing anything in the code, but after restarting the process it stopped working. Do you need any process output?",forgot mention without also get file loading work bit yesterday without really anything code process stopped working need process output,issue,negative,positive,positive,positive,positive,positive
506424486,"You need to run 

python demo_toolbox.py -d .

The dot indicates the current directory",need run python dot current directory,issue,negative,neutral,neutral,neutral,neutral,neutral
506416204,"Hi @CorentinJ,

Thank you for your answer.

I'm sorry, but maybe I'm a little confusing about that.

For example, I have the following project structure:

Real-Time-Voice-Cloning <-- project root
    - demo-toolbox.py
    - LibriSpeech <-- Dataset root
       - train-clean-100 <- Dataset directory

So, when you say that I need to pass the parent of that directory should I run like this?
python demo-tooltox.py -d Real-Time-Voude-Cloning

Since the root dataset is inside the project root folder and in that case 'Real-Time-Voice-Cloning' is the parent of it.

I'm sorry if I misunderstand something, but I'm trying to figure out how to use it since the video just show the demo-toolbox already opened and do now show how to execute it correct. 

Thanks",hi thank answer sorry maybe little example following project structure project root root directory say need pas parent directory run like python since root inside project root folder case parent sorry misunderstand something trying figure use since video show already show execute correct thanks,issue,positive,negative,negative,negative,negative,negative
506413302,Yes you need to pass the parent of that directory,yes need pas parent directory,issue,negative,neutral,neutral,neutral,neutral,neutral
506313827,"Read the source code of demo_cli.py, you'll find instructions there. I'm planning to add more really soon",read source code find add really soon,issue,negative,positive,positive,positive,positive,positive
506218605,"@CorentinJ  Hi, I try your new code and it worked! Thanks for the great job, I really like your art of coding and necessary comments.  I am now trying to manipulate the code to run fine-tuning. Maybe I can finally submit a version to run the full function as demo_toolbox.py does with CLI. ",hi try new code worked thanks great job really like art necessary trying manipulate code run maybe finally submit version run full function,issue,positive,positive,positive,positive,positive,positive
506131860,"You need to set the paths and all of that correctly. Type ""ffmpeg -version"" on your cmd prompt. If that is running then you installed ffmpeg correctly. Installing ffmpeg on windows in annoying. ",need set correctly type prompt running correctly annoying,issue,negative,negative,negative,negative,negative,negative
505761012,You're fine with 4gb if you don't put too long sentences. I'm currently experimenting with implementing low-memory inference to work around that but it's always a tricky thing to know in advance how much VRAM you'll need for an operation and how much is available...,fine put long currently inference work around always tricky thing know advance much need operation much available,issue,negative,positive,positive,positive,positive,positive
505730856,"@CorentinJ By the way, what's the total GPU memory this code consume? I currently run on a 4gb GPU but CUDA out of memory?",way total memory code consume currently run memory,issue,negative,neutral,neutral,neutral,neutral,neutral
505730511,"@CorentinJ Sorry for late reply, i will try it now and feed back the result",sorry late reply try feed back result,issue,negative,negative,negative,negative,negative,negative
505651437,"No, sorry. This is way beyond the scope of this repo.",sorry way beyond scope,issue,negative,negative,negative,negative,negative,negative
505402473,"I'd say one starting from the NVIDIA GTX600 series, but above would be better. I'm currently working on low VRAM support (for 2gb GPUs) but you would be better off with 4gb at least.",say one starting series would better currently working low support would better least,issue,positive,positive,positive,positive,positive,positive
505397709,You won't be able to run it properly until you have a GPU,wo able run properly,issue,negative,positive,positive,positive,positive,positive
505396828,"Yes, you won't be able to do anything without a GPU (an nvidia gpu is prefered)",yes wo able anything without,issue,negative,positive,positive,positive,positive,positive
505396124,"I can't install drivers for NVIDIA, I tried several times but without success. I've tried to run `python CLI` command, and I've got the following error:

```
C:\Users\admin\Downloads\Real-Time-Voice-Cloning_2>python demo_cli.py
WARNING: Logging before flag parsing goes to stderr.
W0625 07:59:13.038100  5128 deprecation_wrapper.py:118] From C:\Users\admin\Down
loads\Real-Time-Voice-Cloning_2\synthesizer\models\modules.py:91: The name tf.nn
.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell ins
tead.

This is a UI-less example of interface to SV2TTS. The purpose of this script is
to show how you can interface this project easily with your own. See the source
code for an explanation of what is happening.

Arguments:
    enc_model_fpath:   encoder\saved_models\pretrained.pt
    syn_model_dir:     synthesizer\saved_models\logs-pretrained
    voc_model_fpath:   vocoder\saved_models\pretrained\pretrained.pt
    no_sound:          False

Your PyTorch installation is not configured to use CUDA. If you have a GPU ready
 for deep learning, ensure that the drivers are properly installed, and that you
r CUDA version matches your PyTorch installation. CPU-only inference is currently not supported.
```

And the last one, can you show me the proper way to do pretrained data file (I'm little confused in these tutorials visdom). Please help me!!!! 

**PS:** I don't have graphic card, graphic card is build in MB. Feature of my computer are: CPU Intel® Core™ i7-7700, RAM 32GB, MB Asus Z170. Can be this why I couldn't install NVIDIA drivers?






",ca install tried several time without success tried run python command got following error python warning logging flag go name please use tead example interface purpose script show interface project easily see source code explanation happening false installation use ready deep learning ensure properly version installation inference currently last one show proper way data file little confused please help graphic card graphic card build feature computer registered core ram could install,issue,positive,negative,neutral,neutral,negative,negative
505377456,"If you're doing everything right, then it's just the ability of the model to clone a voice that is failing you. If you look at page 8 in the middle of [the SV2TTS paper](https://arxiv.org/pdf/1806.04558.pdf), you will see that the mean opinion score for voice resemblance is of 3 (where the min score is 1 and the max is 5). The framework has its limitations. For voices out of the distribution, it's not surprising to see poor performance. I noticed that for my voice, for example, I had to really make an effort to have a proper English accent (my native language is French) to get a voice somewhat close to mine. I also have all the datasets downloaded (VoxCeleb & LibriSpeech), and while most of the time the voice cloning is good (even on the test set of voxceleb, which has crappy quality), there are still a few uncommon voices for which the framework doesn't perform so well, and defaults to an ""average voice"" instead.

There isn't much I can do about that, these are the limitations of the SV2TTS framework and only new findings will allow for improvements in the domain of voice cloning.

Nonetheless, if you could try to send me a sample of your voice that would be great.",everything right ability model clone voice failing look page middle paper see mean opinion score voice resemblance min score framework distribution surprising see poor performance voice example really make effort proper accent native language get voice somewhat close mine also time voice good even test set quality still uncommon framework perform well average voice instead much framework new allow domain voice nonetheless could try send sample voice would great,issue,positive,positive,positive,positive,positive,positive
505374655,"Yes, i can hear my own voice nd other histogram data of the samples are also getting created. 
",yes hear voice histogram data also getting,issue,negative,neutral,neutral,neutral,neutral,neutral
505372618,Do you hear your own voice after recording it? Does the recording sound good?,hear voice recording recording sound good,issue,negative,positive,positive,positive,positive,positive
505346080,"@CorentinJ that's correct, I did not touch your code and I am running SV2TTS-toolbox in a docker container with the GUI through shared `X11` on my host desktop. Whereas I should mention that I connected the mic and speakers through shared `/dev/snd` with the sounddriver packages `alsa-base` and `also-utils` installed in the container (plus appending `audio`-group to the container user). So together with the speaker & mic I got full functionality... sure, feel free to link the comment :). ",correct touch code running docker container host whereas mention connected container plus audio container user together speaker got full functionality sure feel free link comment,issue,positive,positive,positive,positive,positive,positive
505325677,"Hey @DrStoop, sorry for the late reply, I was away for a while.

What I understand is that you managed to get the toolbox to work through X11? If so, I could link your comment in the readme for other users who might face the same issue. It looks to me that there isn't much that I can change to the code that would have helped, what do you reckon? @Interfish, does this solve your issue as well?",hey sorry late reply away understand get toolbox work could link comment might face issue much change code would reckon solve issue well,issue,negative,negative,negative,negative,negative,negative
504733590,"**Enable Qt-Debug `$ export QT_DEBUG_PLUGINS=1`  ==> reproduce error ==> re/install the `No such file or directory`-library listed in debug message ==> repeat!**

**System**

I am running my **PyQt5-application in a Docker container** with shared `/tmp/.X11-unix/` socket and display for GUI visualization:
```bash
$ nividia-docker run --interactive --tty --env DISPLAY=$DISPLAY --volume /tmp/.X11-unix/:/tmp/.X11-unix/ <docker_iamge>
```
**Error**

Initializing `PyQt5.QtWidgets.QApplication` led to following error:
```python
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from PyQt5.QtWidgets import QApplication
>>> app = QApplication([])
qt.qpa.plugin: Could not load the Qt platform plugin ""xcb"" in """" even though it was found.
This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.

Available platform plugins are: eglfs, linuxfb, minimal, minimalegl, offscreen, vnc, wayland-egl, wayland, wayland-xcomposite-egl, wayland-xcomposite-glx, webgl, xcb.

Aborted (core dumped)
```
In PyCharm Debug mode the error returned:
```
Process finished with exit code 134 (interrupted by signal 6: SIGABRT)
```
**Solution**

**General method:**

 - set Qt-debug environement variable in docker container terminal:
```bash
   $ export QT_DEBUG_PLUGINS=1
   ```
 - reproduce error in the docker terminal (or in the IDE), e.g.:
```python
$ python
Python 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) 
[GCC 7.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> 
KeyboardInterrupt
>>> from PyQt5.QtWidgets import QApplication, QLabel
>>> app = QApplication([])
``` 
 - read debug messages printed to the terminal, e.g.:
```
QFactoryLoader::QFactoryLoader() checking directory path ""/conda/envs/rapids/lib/python3.6/site-packages/PyQt5/Qt/plugins/platforms"" ...
QFactoryLoader::QFactoryLoader() looking at ""/conda/envs/rapids/lib/python3.6/site-packages/PyQt5/Qt/plugins/platforms/libqeglfs.so""
Found metadata in lib /conda/envs/rapids/lib/python3.6/site-packages/PyQt5/Qt/plugins/platforms/libqeglfs.so, metadata=
{
    ""IID"": ""org.qt-project.Qt.QPA.QPlatformIntegrationFactoryInterface.5.3"",
    ""MetaData"": {
        ""Keys"": [
            ""eglfs""
        ]
    },
...
...
...
Got keys from plugin meta data (""xcb"")
QFactoryLoader::QFactoryLoader() checking directory path ""/conda/envs/rapids/bin/platforms"" ...
Cannot load library /conda/envs/rapids/lib/python3.6/site-packages/PyQt5/Qt/plugins/platforms/libqxcb.so: (libxkbcommon-x11.so.0: cannot open shared object file: No such file or directory)
QLibraryPrivate::loadPlugin failed on ""/conda/envs/rapids/lib/python3.6/site-packages/PyQt5/Qt/plugins/platforms/libqxcb.so"" : ""Cannot load library /conda/envs/rapids/lib/python3.6/site-packages/PyQt5/Qt/plugins/platforms/libqxcb.so: (libxkbcommon-x11.so.0: cannot open shared object file: No such file or directory)""
qt.qpa.plugin: Could not load the Qt platform plugin ""xcb"" in """" even though it was found.
This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.

Available platform plugins are: eglfs, linuxfb, minimal, minimalegl, offscreen, vnc, wayland-egl, wayland, wayland-xcomposite-egl, wayland-xcomposite-glx, webgl, xcb.

Aborted (core dumped)
```
 - find the `<No such file or directory>.so.*` and `<coud not be loaded>`-packages, here e.g. `libxkbcommon-x11.so.0` and `libxcb`. Then re/install the corresponding packages/libraries (finding the packages works with `apt-file --package-only search <filename>` or `conda/pip search ...`). In my case the following libs were required:
```bash
### lib no.1 ###
$ sudo conda install --name <env_name> --force-reinstall libxcb    # or pip install ...
### lib no. 2 ###
$ apt-file --package-only search libxkbcommon-x11.so.0
libxkbcommon-x11-0
$ sudo apt install libxkbcommon-x11-0 
```
After repeating this process for all sequentially reproduced debug messages and installing the 2 libs I can now run PyQt5-apps from inside the Docker container on my local machine desktop.",enable export reproduce error file directory listed message repeat system running docker container socket display visualization bash run interactive display volume error led following error python type help copyright license information import could load platform even though found application start platform could application may fix problem available platform minimal aborted core mode error returned process finished exit code interrupted signal solution general method set variable docker container terminal bash export reproduce error docker terminal ide python python python default type help copyright license information import read printed terminal directory path looking found got meta data directory path load library open object file file directory load library open object file file directory could load platform even though found application start platform could application may fix problem available platform minimal aborted core find file directory loaded corresponding finding work search search case following bash install name pip install search apt install process sequentially run inside docker container local machine,issue,negative,positive,positive,positive,positive,positive
504669708,"Hi @CorentinJ, I am facing exactly the same problem as @Interfish on my local machine. and I tested your `demo_cli.py`. The test passes though, see attachment below.

I tracked the error with `faulthandler` down to `class UI(QDialog).__init__()`:
```
Connected to pydev debugger (build 191.7479.30)
Arguments:
    datasets_root:    /home/developer/data/datasets
    enc_models_dir:   data/models/encoder/saved_models
    syn_models_dir:   data/models/synthesizer/saved_models
    voc_models_dir:   data/models/vocoder/saved_models

Fatal Python error: Aborted

Thread 0x00007ff44b7c9700 (most recent call first):
  File ""/conda/envs/rapids/lib/python3.6/threading.py"", line 299 in wait
  File ""/conda/envs/rapids/lib/python3.6/threading.py"", line 551 in wait
  File ""/opt/pycharm-2019.1.3/helpers/pydev/pydevd.py"", line 128 in _on_run
  File ""/opt/pycharm-2019.1.3/helpers/pydev/_pydevd_bundle/pydevd_comm.py"", line 321 in run
  File ""/conda/envs/rapids/lib/python3.6/threading.py"", line 916 in _bootstrap_inner
  File ""/conda/envs/rapids/lib/python3.6/threading.py"", line 884 in _bootstrap

Thread 0x00007ff44bfca700 (most recent call first):
  File ""/opt/pycharm-2019.1.3/helpers/pydev/_pydevd_bundle/pydevd_comm.py"", line 383 in _on_run
  File ""/opt/pycharm-2019.1.3/helpers/pydev/_pydevd_bundle/pydevd_comm.py"", line 321 in run
  File ""/conda/envs/rapids/lib/python3.6/threading.py"", line 916 in _bootstrap_inner
  File ""/conda/envs/rapids/lib/python3.6/threading.py"", line 884 in _bootstrap

Thread 0x00007ff44c7cb700 (most recent call first):
  File ""/conda/envs/rapids/lib/python3.6/threading.py"", line 299 in wait
  File ""/conda/envs/rapids/lib/python3.6/queue.py"", line 173 in get
  File ""/opt/pycharm-2019.1.3/helpers/pydev/_pydevd_bundle/pydevd_comm.py"", line 460 in _on_run
  File ""/opt/pycharm-2019.1.3/helpers/pydev/_pydevd_bundle/pydevd_comm.py"", line 321 in run
  File ""/conda/envs/rapids/lib/python3.6/threading.py"", line 916 in _bootstrap_inner
  File ""/conda/envs/rapids/lib/python3.6/threading.py"", line 884 in _bootstrap

Current thread 0x00007ff460371740 (most recent call first):
  File ""/home/developer/toolbox/ui.py"", line 344 in __init__
  File ""/home/developer/toolbox/__init__.py"", line 38 in __init__
  File ""/home/developer/demo_toolbox.py"", line 31 in <module>
  File ""/opt/pycharm-2019.1.3/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18 in execfile
  File ""/opt/pycharm-2019.1.3/helpers/pydev/pydevd.py"", line 1147 in run
  File ""/opt/pycharm-2019.1.3/helpers/pydev/pydevd.py"", line 1752 in main
  File ""/opt/pycharm-2019.1.3/helpers/pydev/pydevd.py"", line 1758 in <module>

Process finished with exit code 134 (interrupted by signal 6: SIGABRT)
```

The error occurs in `File ""/toolbox/ui.py"", line 344 in __init__` due to `QDialog` or `QApplication` initialization:
```python
class UI(QDialog):
...
...
...
    def __init__(self):
        ## Initialize the application
        self.app = QApplication(sys.argv)
        super().__init__(None)
        self.setWindowTitle(""SV2TTS toolbox"")
```
I don't know PyQt5 so I couldn't debug it so far. Any ideas? Still working on it...

Cheers!

**Attachment:**
```
This is a UI-less example of interface to SV2TTS. The purpose of this script is to show how you can interface this project easily with your own. See the source code for an explanation of what is happening.

Arguments:
    enc_model_fpath:   data/models/encoder/saved_models/pretrained.pt
    syn_model_dir:     data/models/synthesizer/saved_models/logs-pretrained
    voc_model_fpath:   data/models/vocoder/saved_models/pretrained/pretrained.pt
    no_sound:          False

Found 1 GPUs available. Using GPU 0 (GeForce GTX 1080) of compute capability 6.1 with 8.5Gb total memory.

Loading the encoder, the synthesizer and the vocoder. This should take a few seconds. The synthesizer will output a lot of stuff. Tensorflow is like that.
Loaded encoder ""pretrained.pt"" trained to step 1564501
Constructing model: Tacotron
WARNING:tensorflow:From /home/developer/synthesizer/models/tacotron.py:86: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, use
    tf.py_function, which takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    
WARNING:tensorflow:From /conda/envs/rapids/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /home/developer/synthesizer/models/modules.py:112: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.
WARNING:tensorflow:From /home/developer/synthesizer/models/modules.py:421: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.conv1d instead.
WARNING:tensorflow:From /home/developer/synthesizer/models/modules.py:422: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.batch_normalization instead.
WARNING:tensorflow:From /home/developer/synthesizer/models/modules.py:425: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dropout instead.
WARNING:tensorflow:From /home/developer/synthesizer/models/modules.py:236: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
WARNING:tensorflow:From /conda/envs/rapids/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
WARNING:tensorflow:From /conda/envs/rapids/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
WARNING:tensorflow:From /home/developer/synthesizer/models/modules.py:305: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.
WARNING:tensorflow:From /home/developer/synthesizer/models/modules.py:269: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /conda/envs/rapids/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
initialisation done /gpu:0
Initialized Tacotron model. Dimensions (? = dynamic shape): 
  Train mode:               False
  Eval mode:                False
  GTA mode:                 False
  Synthesis mode:           True
  Input:                    (?, ?)
  device:                   0
  embedding:                (?, ?, 512)
  enc conv out:             (?, ?, 512)
  encoder out (cond):       (?, ?, 768)
  decoder out:              (?, ?, 80)
  residual out:             (?, ?, 512)
  projected residual out:   (?, ?, 80)
  mel out:                  (?, ?, 80)
  <stop_token> out:         (?, ?)
  Tacotron Parameters       28.439 Million.
Loading checkpoint: data/models/synthesizer/saved_models/logs-pretrained/taco_pretrained/tacotron_model.ckpt-278000
WARNING:tensorflow:From /conda/envs/rapids/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Loaded synthesizer ""pretrained"" trained to step 278000
Building Wave-RNN
Trainable Parameters: 4.481M
Loading model weights at data/models/vocoder/saved_models/pretrained/pretrained.pt

All models succesfully loaded!

Testing your configuration with small inputs.
	Testing the encoder...
	Testing the synthesizer...
	Testing the vocoder...
All test passed! You can now synthesize speech.

Process finished with exit code 0
```",hi facing exactly problem local machine tested test though see attachment tracked error class connected build fatal python error aborted thread recent call first file line wait file line wait file line file line run file line file line thread recent call first file line file line run file line file line thread recent call first file line wait file line get file line file line run file line file line current thread recent call first file line file line file line module file line file line run file line main file line module process finished exit code interrupted signal error file line due python class self initialize application super none toolbox know could far still working attachment example interface purpose script show interface project easily see source code explanation happening false found available compute capability total memory loading synthesizer take synthesizer output lot stuff like loaded trained step model warning removed future version instead use python function eager instead easy convert eager tensor call access eager use well differentiable gradient tape warning removed future version handled automatically placer warning removed future version class equivalent warning removed future version use instead warning removed future version use instead warning dropout removed future version use instead warning removed future version please use cell equivalent warning removed future version please use cell equivalent warning removed future version use instead warning removed future version class equivalent warning dense removed future version use instead warning calling dropout removed future version please use rate instead rate set rate done model dynamic shape train mode false mode false mode false synthesis mode true input device cond residual residual mel million loading warning removed future version use standard file check prefix loaded synthesizer trained step building trainable loading model loaded testing configuration small testing testing synthesizer testing test synthesize speech process finished exit code,issue,negative,positive,neutral,neutral,positive,positive
504557004,Yes that's a tensorflow classic. It's a mismatch between your cuda or cudnn dll and your tensorflow version IIRC.,yes classic mismatch version,issue,negative,positive,positive,positive,positive,positive
504422058,Try googling your errors first. I'm going to be busy for a while so I won't be able to help you much but I'll try later.,try first going busy wo able help much try later,issue,negative,positive,positive,positive,positive,positive
504421313,"But I'm not good at Python, be sincerely I don't know it, but I'll try to figure out. Can you help me with this issue?",good python sincerely know try figure help issue,issue,positive,positive,positive,positive,positive,positive
504420220,You should run it to debug issues. Your configuration seems broken and you need to fix it before doing anything.,run configuration broken need fix anything,issue,negative,negative,negative,negative,negative,negative
504417549,"Seems I forgot to include multiprocess in the requirements, I'll do that. I see that I also did not mention you had to run visdom before training, I've updated that [here](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#preprocessing-and-training).

As for Romanian, I did tell you that you need a Romanian dataset. Do you have one, a large one?

You do need to have your nvidia drivers installed. Try to run `demo_cli.py` before you run any other scripts. If it doesn't work, the rest of the project won't.",forgot include see also mention run training tell need one large one need try run run work rest project wo,issue,negative,positive,positive,positive,positive,positive
504182324,"Nice work.

Yes the semantics are not changed by black.",nice work yes semantics black,issue,positive,positive,positive,positive,positive,positive
504179812,"If you are doing this by hand then check out https://black.readthedocs.io

You could do just the worst files or the worst directories.",hand check could worst worst,issue,negative,negative,negative,negative,negative,negative
504178737,"Well thank you for looking into it. I've fixed those in 0908e4d30d3fae78b4cbd69cd09c20f0a98c4dd0.

I do think that some of the thousand style violations are problematic. E.g. I've been trying to fix (manually) the inconsistent indents in Rahayne's code and the character limits being broken everywhere, and I didn't think of using a tool that would automate that. I will look into using black, but I feel like I might regret it if I change a thousand lines at once and if I don't look at the diff after (I assume that the semantics are not changed, but still). Since you seem to have experience with that, do you think that's it's the right approach?",well thank looking fixed think thousand style problematic trying fix manually inconsistent code character broken everywhere think tool would look black feel like might regret change thousand look assume semantics still since seem experience think right approach,issue,negative,negative,neutral,neutral,negative,negative
504174328,"> I'm not religious on flake8

Me neither but I do like __--select=E9,F63,F72,F82__.  Most of the others are mere ""_style violations_"" that can be autofixed with a run of __python/black__.",religious flake neither like mere run,issue,negative,negative,negative,negative,negative,negative
504173324,Oh yeah these are functions from Rayahane's tacotron implementation that I've butchered away. I'm not religious on flake8 but I guess it wouldn't hurt to clean some things up. I'll see to it.,oh yeah implementation away religious flake guess would hurt clean see,issue,negative,positive,positive,positive,positive,positive
504166204,"Hey @Interfish, I've added [this stub](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/demo_cli.py) to allow for quick debugging without a GUI. Can you test and see if it runs? I'll implement an interactive way of doing inference later on.",hey added stub allow quick without test see implement interactive way inference later,issue,negative,positive,positive,positive,positive,positive
503702353,"> re you sure you're executing the scripts from the root of the repo as working directory (i.e. the directory that contains the scripts)? I suspect that's the problem

what do you mean i should not run it from the folder?
by removing the line it starts. Now I should investiagte cudnn that is supposed to be on my pc but failed. will chek that. but thanks for quick help",sure root working directory directory suspect problem mean run folder removing line supposed thanks quick help,issue,negative,positive,positive,positive,positive,positive
503688709,"Thank you. Are you sure you're executing the scripts from the root of the repo as working directory (i.e. the directory that contains the scripts)? I suspect that's the problem, but if it's not you can simply comment out the lines that use `utils.argutils` (the import statement and the print_args call) because they're simply there to display a fancy output. If that fixes it, let me know.

Maybe you have a package called utils, but even then my utils package should take priority.",thank sure root working directory directory suspect problem simply comment use import statement call simply display fancy output let know maybe package even package take priority,issue,negative,positive,positive,positive,positive,positive
503632443,"Yep, `CUDNN_STATUS_MAPPING_ERROR` either means your PyTorch version does not match your CUDA version or that your GPU is unsupported because it's too old. Given that the exact message `PyTorch no longer supports this GPU because it is too old.` appears up there I think you have no choice but to get a newer GPU. I'm planning for CPU inference maybe some day, but even then that would be very slow. I'm sorry but there's no simple solution here.",yep either version match version unsupported old given exact message longer think choice get inference maybe day even would slow sorry simple solution,issue,negative,negative,neutral,neutral,negative,negative
503629910,"I obtain:

Forwarding resnet on CPU
Forwarding resnet on GPU
/home/giorgio/anaconda3/envs/newenv/lib/python3.7/site-packages/torch/cuda/__init__.py:118: UserWarning: 
    Found GPU0 GeForce GT 750M which is of cuda capability 3.0.
    PyTorch no longer supports this GPU because it is too old.
    The minimum cuda capability that we support is 3.5.
    
  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))
Traceback (most recent call last):
  File ""script.py"", line 18, in <module>
    model.forward(torch.from_numpy(inputs).cuda())
  File ""/home/giorgio/anaconda3/envs/newenv/lib/python3.7/site-packages/torchvision/models/resnet.py"", line 192, in forward
    x = self.conv1(x)
  File ""/home/giorgio/anaconda3/envs/newenv/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/giorgio/anaconda3/envs/newenv/lib/python3.7/site-packages/torch/nn/modules/conv.py"", line 338, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: cuDNN error: CUDNN_STATUS_MAPPING_ERROR
",obtain forwarding forwarding found capability longer old minimum capability support name major capability recent call last file line module file line forward file line result input file line forward error,issue,negative,positive,neutral,neutral,positive,positive
503628467,"Could you try running the script I put here:
https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/3#issuecomment-502227424

If it fails before using the speaker encoder, then there's definitely something wrong with your PyTorch/Cuda installation.",could try running script put speaker definitely something wrong installation,issue,negative,negative,negative,negative,negative,negative
503619096,"Thanks for your answer. Sorry for my late reply, i was trying to solve the problem by installing cuda again, but it persists. Now the error is again:
**Exception: CUDA error: no kernel image is available for execution on the device**
I installed pytorch in this way: 
`conda install pytorch torchvision cudatoolkit=10.0 -c pytorch`

In order to be more accurate, i used test-other.tar.gz [328M]   (test set, ""other"" speech ) of the whole dataset corpus. The same error appears when i try to record an audio.
",thanks answer sorry late reply trying solve problem error exception error kernel image available execution device way install order accurate used test set speech whole corpus error try record audio,issue,negative,positive,neutral,neutral,positive,positive
503528245,Thank you. In what way did you install PyTorch? Did you compile it or did you use pip/conda?,thank way install compile use,issue,negative,neutral,neutral,neutral,neutral,neutral
503455387,"I had planned to make a GUI-less demo inference script for a while, but really its only purpose would be to show you how to interface the models with your code. Essentially it comes down to making calls to the three `<model>/inference.py` scripts I wrote for each model. You can read their current documentation and refer to how they're used in the toolbox to implement it yourself.

Either way I'll be writing that script in two days I think (currently have exams to pass). I'm still thinking about how I want to do it.

As for the GUI problem, I'm sorry but I just don't know enough on that topic to help you. Your guess is a good as mine.",make inference script really purpose would show interface code essentially come making three model wrote model read current documentation refer used toolbox implement either way writing script two day think currently pas still thinking want problem sorry know enough topic help guess good mine,issue,negative,positive,neutral,neutral,positive,positive
503453248,"You would need hundreds of hours, and you need the text that comes with that data.",would need need text come data,issue,negative,neutral,neutral,neutral,neutral,neutral
503452829,Last question if I have some records (romaine) can I use them to train the speech?,last question romaine use train speech,issue,negative,neutral,neutral,neutral,neutral,neutral
503451327,"No, I don't know about any substantial one. [M-AILABS](https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/) doesn't have any Romanian, and the only one I found is [SWARA](https://speech.utcluj.ro/swarasc/) which is way too small to do anything. Sorry.",know substantial one one found way small anything sorry,issue,negative,negative,negative,negative,negative,negative
503441548,"Could you suggest me some pre-trained reference of Romanian dataset source? or in other words, from what I should start?",could suggest reference source start,issue,negative,neutral,neutral,neutral,neutral,neutral
503343888,"You have to merge the pretrained models with the root of the repo. I think you put them in the wrong place. In the same folder you have `demo_toolbox.py` you should have the directory `encoder`, then `saved_models` under it and finally `pretrained.pt` under that. Right now, you don't have `pretrained.pt` there and that's causing the error.

As for retraining in Romanian, I think that's going to be difficult unless you find a good Romanian dataset. Even then that's a lot of work to interface the dataset with the project.",merge root think put wrong place folder directory finally right causing error think going difficult unless find good even lot work interface project,issue,negative,negative,neutral,neutral,negative,negative
503334321,"You have not downloaded the pretrained models. Look for them in the readme. Don't worry, you can run the toolbox without datasets first. What is your native language?",look worry run toolbox without first native language,issue,negative,positive,positive,positive,positive,positive
503331437,"I followed your indications, and I've got an error (see below). I understand something is related with dataset. How do I properly configure dataset, because before I tried to download VoxCeleb1 and VoxCeleb2 I couldn't, it asked me some authentication and just I skipped this step.

PS: How do I train for other languages, not English, where I can get the data set, or how can I do it?

C:\Users\admin\Downloads\Real-Time-Voice-Cloning>python demo_toolbox.py ""C:\User
s\admin\Downloads\Real-Time-Voice-Cloning\dataset_root""
WARNING: Logging before flag parsing goes to stderr.
W0619 00:22:47.159650  5648 deprecation_wrapper.py:118] From C:\Users\admin\Down
loads\Real-Time-Voice-Cloning\synthesizer\models\modules.py:91: The name tf.nn.r
nn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.

Arguments:
    datasets_root:    C:\Users\admin\Downloads\Real-Time-Voice-Cloning\dataset_root
        enc_models_dir:   encoder\saved_models
        syn_models_dir:   synthesizer\saved_models
        voc_models_dir:   vocoder\saved_models

Traceback (most recent call last):
  File ""demo_toolbox.py"", line 28, in <module>
    Toolbox(**vars(args))
  File ""C:\Users\admin\Downloads\Real-Time-Voice-Cloning\toolbox\__init__.py"", line 39, in __init__
    self.reset_ui(enc_models_dir, syn_models_dir, voc_models_dir)
  File ""C:\Users\admin\Downloads\Real-Time-Voice-Cloning\toolbox\__init__.py"", line 85, in reset_ui
    self.ui.populate_models(encoder_models_dir, synthesizer_models_dir, vocoder_
models_dir)
  File ""C:\Users\admin\Downloads\Real-Time-Voice-Cloning\toolbox\ui.py"", line 26
9, in populate_models
    raise Exception(""No encoder models found in %s"" % encoder_models_dir)
Exception: No encoder models found in encoder\saved_models


",got error see understand something related properly configure tried could authentication step train get data set python warning logging flag go name please use instead recent call last file line module toolbox file line file line file line raise exception found exception found,issue,negative,neutral,neutral,neutral,neutral,neutral
503314120,"The package is `Unidecode`, not unicode. It's only been added with a recent PR. Pull the repo and install the requirements again.",package added recent pull install,issue,negative,neutral,neutral,neutral,neutral,neutral
503232253,"I've changed it back that exceptions do not exit the program. I prefer it that way, usually it's nothing worth stopping execution.",back exit program prefer way usually nothing worth stopping execution,issue,negative,positive,neutral,neutral,positive,positive
503152379,"Thank you @CorentinJ, it works fine. Firstly I installed [BuldTools](https://github.com/felixrieseberg/windows-build-tools#examples-of-modules-supported) , and then webrtcvad and after that I ran pip install -r requirements.txt",thank work fine firstly ran pip install,issue,positive,positive,positive,positive,positive,positive
502821936,I see that PyQt5 can be installed through pip so I'm all for it. I've thus removed it from the readme and added to the requirements. Let me know if that is problematic.,see pip thus removed added let know problematic,issue,negative,neutral,neutral,neutral,neutral,neutral
502806933,"Thank you.

I've removed entirely PyAudio as it was unused in the project. I've kept the rest.",thank removed entirely unused project kept rest,issue,negative,neutral,neutral,neutral,neutral,neutral
502596627,I don't think torchvision has any role to play in the repo. It's a source for common datasets and models and I've only used it in the script I gave you. I suspect something else that you did fixed your setup. I'll still keep this issue in mind if someone else has the same problem. Glad this is fixed for you.,think role play source common used script gave suspect something else fixed setup still keep issue mind someone else problem glad fixed,issue,negative,positive,neutral,neutral,positive,positive
502589910,"> The models were trained and executed on pytorch 1.10.
> 
> At what point does the code I gave you fail?

Thank you!
I tried install pytorch and torchvision. It's work.
I think you should add requirements torchvision.",trained executed point code gave fail thank tried install work think add,issue,negative,negative,negative,negative,negative,negative
502581802,"The models were trained and executed on pytorch 1.10.

At what point does the code I gave you fail?",trained executed point code gave fail,issue,negative,negative,negative,negative,negative,negative
502578082,"> Do you think its because you have PyQt5 and not 4?

I don't think so. Because PyQt5 is frontend, it doesn't affect load models",think think affect load,issue,negative,neutral,neutral,neutral,neutral,neutral
502577068,"> Hey @hung96ad, any update?

I tried new code but it not work.
Can you tell me your pytorch version?",hey update tried new code work tell version,issue,negative,positive,positive,positive,positive,positive
502466361,"This is an error you will meet on windows with python packages that require compilation. Usually you can find precompiled binaries for these but in this case it's the `webrtcvad` package and I haven't found any myself. You need to keep trying to install webrtcvad until you manage to get it to work, then you should install all the requirements again. 

The command is `pip install webrtcvad`. It will give you the same error that appears at the end of the error message you pasted here:

```
ERROR: running install
running build
running build_py
creating build
creating build\lib.win-amd64-3.7
copying webrtcvad.py -> build\lib.win-amd64-3.7
running build_ext
building '_webrtcvad' extension
error: Microsoft Visual C++ 14.0 is required. Get it with ""Microsoft Visual
C++ Build Tools"": https://visualstudio.microsoft.com/downloads/
```

See [this thread](https://github.com/benfred/implicit/issues/76) to find the build tools. It's a bit of a pain to install. Check the other answers until that solves it. Then try to install webrtcvad again. 

If after that you get ""LINK : fatal error LNK1158: cannot run 'rc.exe'"", check [this](https://forum.qt.io/topic/90839/lnk1158-cannot-run-rc-exe/2).",error meet python require compilation usually find case package found need keep trying install manage get work install command pip install give error end error message pasted error running install running build running build running building extension error visual get visual build see thread find build bit pain install check try install get link fatal error run check,issue,negative,negative,neutral,neutral,negative,negative
502227424,"Can you put this script at the root of the project and run it, try to see at what point it fails? You will need a [torchvision](https://pytorch.org/get-started/locally/) version that matches your pytorch version.

```
from torchvision.models import resnet18
from encoder import inference as encoder
from pathlib import Path
import numpy as np
import torch


if __name__ == '__main__':
    # Test to see if your pytorch install works on CPU
    print(""Forwarding resnet on CPU"")
    inputs = np.zeros((4, 3, 224, 224), dtype=np.float32)
    model = resnet18()
    model.forward(torch.from_numpy(inputs))
    
    # Same but on GPU
    print(""Forwarding resnet on GPU"")
    model = model.cuda()
    model.forward(torch.from_numpy(inputs).cuda())

    # Test to see if the encoder works at all (on CPU) with a small input
    print(""Forwarding encoder on CPU"")
    weights_fpath = Path(""encoder/saved_models/pretrained.pt"")
    device = torch.device(""cpu"")
    encoder.load_model(weights_fpath, device)
    inputs = np.zeros((1, 100, 40), dtype=np.float32)
    encoder.embed_frames_batch(inputs)
    
    # Same but on GPU
    print(""Forwarding encoder on GPU"")
    device = torch.device(""cuda"")
    encoder.load_model(weights_fpath, device)
    encoder.embed_frames_batch(inputs)
    
```",put script root project run try see point need version version import import inference import path import import torch test see install work print forwarding model print forwarding model test see work small input print forwarding path device device print forwarding device device,issue,negative,negative,negative,negative,negative,negative
502221644,"> It looks like the encoder failed to be loaded. What is your version of PyTorch? Have you managed to run other PyTorch models with your setup before?
> 
> Also, if you could try to find which of [these lines](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/encoder/inference.py#L30-L38) is causing the problem, that would help.

Thanks for your answer! 
I tried with Pytorch 0.41 and Pytorch 1.1.0.
I change device with CPU on Line 39.
After  I change  [lines,](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/encoder/model.py#L18-L31) to CPU and can load model encode . 
However, it not work. 
I debug and discovered [line](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/encoder/inference.py#L55) About core dump ",like loaded version run setup also could try find causing problem would help thanks answer tried change device line change load model encode however work discovered line core dump,issue,negative,positive,positive,positive,positive,positive
502144379,"That's perfectly fine.

On Fri, Jun 14, 2019, 11:03 AM Corentin Jemine <notifications@github.com>
wrote:

> I can do that, but it will be one file at a time for now
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/2?email_source=notifications&email_token=ADFX425PTNZWF6CSB5LC7QLP2OXLRA5CNFSM4HX5LVB2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODXXBYIA#issuecomment-502144032>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ADFX427WJKAV4C7CT2JKJJTP2OXLRANCNFSM4HX5LVBQ>
> .
>
",perfectly fine wrote one file time thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
502139786,I think it would be easier to have a way to just import arbitrary audio files from anywhere on the PC through a file dialog.,think would easier way import arbitrary audio anywhere file,issue,negative,negative,neutral,neutral,negative,negative
502063661,"It looks like the encoder failed to be loaded. What is your version of PyTorch? Have you managed to run other PyTorch models with your setup before?

Also, if you could try to find which of [these lines](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/encoder/inference.py#L30-L38) is causing the problem, that would help.",like loaded version run setup also could try find causing problem would help,issue,negative,neutral,neutral,neutral,neutral,neutral
501908264,"Also, I have a question. How do you use audio clips from your PC to create the embeddings? If I'm saying that correctly. Like without the microphone.",also question use audio clip create saying correctly like without microphone,issue,positive,neutral,neutral,neutral,neutral,neutral
501392486,"You're free to use that implementation or [fatchord's Tacotron](https://github.com/fatchord/WaveRNN/) would work too, that is up to you.

First, you'll need to understand how Tacotron is modified to allow for voice conditioning. Refer to [section 2 of SV2TTS](https://arxiv.org/pdf/1806.04558.pdf). You can also check my code.

Then you need to ensure that you will use the same format of spectrogram and audio for the synthesizer and for the vocoder. Check the preprocessing scripts to see the what it done to the data. You will likely have to change the data loading routine of the vocoder so that it takes correct inputs.

The scripts that use the synthesizer are `synthesizer_train.py` and `vocoder_preprocess.py`. Ensure that your model correctly interfaces them. You have to provide an interface for inference as well.

And you will have to train all this, which is not a minor task! Good luck.",free use implementation would work first need understand allow voice refer section also check code need ensure use format spectrogram audio synthesizer check see done data likely change data loading routine correct use synthesizer ensure model correctly provide interface inference well train minor task good luck,issue,positive,positive,positive,positive,positive,positive
